# 8.8 — Updating Runbooks and Playbooks Based on Incidents

The incident resolved at 4:15 AM. The on-call engineer had spent two hours hunting through outdated runbooks, following steps that referenced infrastructure retired six months ago. By the time they found the correct mitigation procedure — buried in a Slack thread from a previous incident — the outage had cost the company forty minutes of unnecessary downtime. The runbook existed. It just hadn't been updated since the last time this failure mode appeared.

Every incident reveals what your documentation gets wrong. The runbook that said "restart the service" when the actual fix required "drain traffic, restart, then re-enable routing" just failed in production. The playbook that listed the wrong escalation contact just added fifteen minutes to your response time. The troubleshooting guide that didn't mention the new monitoring dashboard just forced your engineer to debug blind. These gaps are invisible until an incident exposes them. Then they become obvious, urgent, and immediately fixable.

The teams that improve reliability fastest are the ones that treat runbook updates as mandatory incident work, not optional cleanup. If you closed the incident without updating the documentation that failed you, you guaranteed the next engineer will face the same confusion. If you wait a week to update runbooks, the details that matter fade from memory. If you update runbooks but never test them, you document fiction instead of reality. The runbook improvement cycle happens in real time, while the incident is still fresh, or it doesn't happen at all.

## The Runbook Improvement Cycle

The improvement cycle starts during the incident. When an engineer hits a runbook gap — a missing step, an outdated command, an incorrect escalation path — they note it in the incident channel. Not after the incident resolves. During. The note is simple: "Runbook says restart service, actual fix required drain-then-restart." That note becomes a runbook update ticket the moment the incident closes.

Within 24 hours of incident resolution, the incident commander or on-call engineer opens the relevant runbook and makes the update. Not a week later. Not after the post-mortem. Within one day, while memory is sharp and the pain is recent. The update includes what failed, what actually worked, and any new context discovered during the incident. If the troubleshooting took two hours because the monitoring dashboard link was broken, the runbook now has the correct link. If the escalation path went through three people when it should have gone direct to the platform team, the runbook now has the right contact.

After the update, someone else tests it. Not the person who wrote it. Someone who wasn't involved in the incident. They follow the updated runbook step by step, as if responding to the same failure. If they get stuck, the runbook still has gaps. If they can follow it successfully, the runbook is ready. This test-before-merge discipline catches the updates that make sense to the incident responder but confuse everyone else. You're not updating the runbook for yourself. You're updating it for the engineer who will face this incident at 3 AM six months from now, with no context and no memory of what happened today.

## Identifying Runbook Gaps from Incidents

The most common runbook gap is the missing step. The runbook says "check the service health endpoint" but doesn't say what a healthy response looks like. The runbook says "restart the model server" but doesn't say how to verify it restarted correctly. The runbook says "roll back the deployment" but doesn't say which deployment identifier to use. These gaps force the on-call engineer to improvise, guess, or escalate. Every improvisation is a missing step. Every escalation that could have been self-service is missing documentation.

The second most common gap is the outdated step. The runbook references infrastructure that moved, commands that changed syntax, contact information that changed teams. A healthcare company's runbook for model inference failures still referenced an S3 bucket that had been replaced by a new bucket six months earlier. The on-call engineer spent twenty minutes investigating why the bucket was empty before realizing the runbook was wrong. The actual bucket was in the monitoring dashboard the whole time. No one had updated the runbook because the last incident happened before the migration. The documentation froze in time while the system evolved.

The third gap is the missing context. The runbook tells you what to do but not why. It says "disable the fallback model" without explaining that the fallback model has a known bug that triggers infinite retry loops. It says "check the rate limiter threshold" without explaining that the threshold was temporarily raised last month and never lowered. Context answers the question every on-call engineer asks: "Is this step still relevant, or is this outdated advice I can skip?" Without context, they either skip the step and miss the fix, or follow every step robotically and waste time on irrelevant checks.

## Updating Runbooks While Memory Is Fresh

The window for accurate runbook updates is 24 hours. After one day, details fade. You remember the general shape of the incident but not the exact command you ran. You remember escalating but not who you escalated to. You remember the fix but not the two things you tried before that didn't work. Those details matter. The failed attempts teach future responders what not to waste time on. The exact command prevents syntax errors. The escalation contact prevents delays.

The engineer who responded to the incident owns the initial update. They open the runbook, find the section that failed them, and rewrite it based on what actually worked. If the runbook said "restart the service" and the actual fix was "drain traffic, wait for active requests to complete, restart service, verify health checks pass, re-enable traffic," that full sequence goes into the runbook. If the troubleshooting guide said "check logs" and the actual path was "check logs, then check the dead letter queue, then check the upstream service status," that full path goes in.

The update includes timestamps and incident references. "Updated based on INC-4471, Feb 2026: the model server requires a traffic drain before restart or active requests fail." This timestamp tells future readers when the runbook was last validated. If they're reading it two years later, they know to verify the steps still apply. The incident reference lets them trace back to the original post-mortem if they need more context. You're not just updating the runbook. You're creating an audit trail of how the system's failure modes evolved.

## Testing Updated Runbooks

An untested runbook update is a theory. A tested runbook update is a procedure. The test is simple: hand the updated runbook to an engineer who wasn't involved in the incident and ask them to walk through it. Not execute it in production — walk through it step by step, verifying they understand each step, can find the referenced tools, and know what success looks like. If they hit a step that's ambiguous, the runbook needs more detail. If they can't find the monitoring dashboard, the runbook needs a direct link. If they're not sure what a healthy response looks like, the runbook needs an example.

For high-severity playbooks — the ones you use for user-facing outages or data incidents — the test is more rigorous. You simulate the incident in a staging environment and execute the updated runbook end-to-end. If the incident was "model server out of memory," you induce an out-of-memory condition in staging and follow the runbook. If the runbook works, it's ready. If it misses a step, you catch it before the next real incident. This level of testing is expensive. You only do it for the failures that matter most. But for those failures, untested runbooks are professional negligence.

Some teams run quarterly runbook drills. Every engineer on the on-call rotation picks a runbook, simulates the failure, and executes the documented response. The drill surfaces outdated steps, missing context, and broken links. It also trains the team. An engineer who has walked through the model rollback runbook in a drill responds faster and more confidently when the real incident happens. The drill isn't just testing the runbook. It's rehearsing the response.

## Version Control for Runbooks

Runbooks are code. They live in version control, not in wikis or Google Docs. Every update is a commit. Every commit has a message that references the incident that motivated the change. "Update model restart procedure based on INC-4471." "Add escalation path for rate limiter failures based on INC-4502." The commit history becomes a timeline of how your system's failure modes evolved and how your response improved.

Version control gives you rollback. If a runbook update introduces confusion — if the new steps are less clear than the old ones — you can revert the change and try again. You can compare versions to see what changed and when. You can trace a specific step back to the incident that introduced it. This traceability matters when you're debugging why a runbook has a step that seems unnecessary. The commit message and incident reference explain the context.

Some teams use pull requests for runbook updates. The incident responder opens a PR with their changes. A second engineer reviews it, tests it, and approves it before merge. This review step catches unclear language, missing steps, and assumptions that only make sense to the original author. It also spreads knowledge. The reviewer learns about the incident and the fix by reviewing the runbook update. When they face a similar incident, they already have context.

## Runbook Review Cadence

Even with incident-driven updates, runbooks decay. The system changes in ways that don't trigger incidents. A team moves to a new monitoring tool. A service migrates to a new region. A contact leaves the company. These changes don't cause outages, so they don't trigger incident-driven updates. But they make runbooks quietly obsolete. The next incident arrives, the engineer follows the runbook, and the monitoring link 404s.

The defense is a scheduled review cadence. Every six months, every runbook gets reviewed. Not rewritten — reviewed. An engineer who uses that runbook regularly opens it, walks through every step, and verifies it still matches reality. Links still work. Commands still have the right syntax. Contact information is still current. The monitoring dashboards still exist. If anything is wrong, they update it. If everything is correct, they add a "Last reviewed: August 2026" timestamp at the bottom. That timestamp tells future readers the runbook was recently validated.

For high-frequency runbooks — the ones you use monthly or more — the review happens more often. Every quarter. Every month, for the most critical procedures. For low-frequency runbooks — the ones you haven't used in two years — the review happens when someone joins the on-call rotation. New engineers are assigned a set of low-frequency runbooks and asked to review them as part of onboarding. This keeps the runbooks current and trains the new engineer on rare failure modes they haven't seen yet.

## Runbook Ownership

Every runbook has an owner. Not the person who wrote it. The person responsible for keeping it current. For service-specific runbooks, the owner is usually the team that owns the service. The model serving team owns the model restart runbook. The data pipeline team owns the ETL failure runbook. The platform team owns the Kubernetes node failure runbook. Ownership means that when the system changes, the owning team updates the runbook without being asked.

For cross-team runbooks — the ones that touch multiple services — ownership lives with the team most affected by failure. The incident escalation runbook is owned by the on-call coordinator. The data breach response playbook is owned by the security team. The customer communication runbook is owned by support operations. The owner is the person who answers "Is this runbook still accurate?" and "Who do I ask if I'm confused?" If no one can answer those questions, the runbook has no owner and will decay.

Ownership rotates when people leave. When a runbook owner changes teams or leaves the company, the ownership transfer is explicit. The old owner hands off the runbook to the new owner, walks them through any recent updates, and explains any context that isn't written down. The handoff is tracked in the runbook itself: "Owner: Platform Team (formerly owned by Reliability Team until Jan 2026)." This history prevents the runbook from becoming orphaned. An orphaned runbook is a runbook no one maintains. Within a year, it's fiction.

The runbook that failed you during the last incident is the runbook that needs updating today. The engineer who resolved that incident is the one who knows what the runbook should say. If they update it within 24 hours, test it with someone who wasn't involved, and commit it to version control, the next incident will go faster. If they don't, the next engineer will face the same confusion. The gap between incident resolution and runbook update is the gap between learning from failure and repeating it.
