# 8.10 — Structured Pre-Mortems: Assuming Failure Before It Happens

Three weeks before launching their AI-powered clinical decision support system, a healthcare company ran a pre-mortem. The engineering lead gathered the team in a conference room and asked a single question: "It's six months from now. The launch failed catastrophically. What happened?" The team spent two hours imagining failure. The infrastructure lead said: "The model inference latency spiked during peak patient load, and clinicians stopped trusting the system." The data scientist said: "We trained on data from urban hospitals, and the model performed terribly in rural settings." The compliance lead said: "We didn't document model limitations clearly enough, and a clinician relied on the model for a case it wasn't designed for."

The team left with twelve failure scenarios. Six of them were problems they hadn't considered. Three became immediate work items. They load-tested the inference service at 3x expected traffic. They audited the training data for demographic coverage. They rewrote the model limitation documentation. When the system launched, it didn't fail catastrophically. It had normal launch issues — a few edge cases, a few performance hiccups — but not the disasters the pre-mortem predicted. The pre-mortem didn't prevent all problems. It prevented the preventable ones.

Post-mortems teach you what went wrong after the damage is done. Pre-mortems teach you what could go wrong before you launch. The technique is simple: assume the project failed, then work backward to explain why. This inversion unlocks insight. When you ask "what could go wrong," people hedge. When you ask "the project failed — why," people speak freely. The failure is already assumed. There's no penalty for imagining disaster. The team that runs a structured pre-mortem before every major launch discovers vulnerabilities during development, when they're cheap to fix, instead of during incidents, when they're expensive.

## What Is a Pre-Mortem

A pre-mortem is a facilitated exercise where the team imagines a future failure and reverse-engineers the cause. The facilitator sets the scene: "It's three months after launch. The system failed badly enough that we had to roll it back. Users lost trust. Leadership is asking what went wrong." Then the facilitator asks: "What happened?" Each team member writes down their answer independently. After five minutes, everyone shares. The facilitator captures all the scenarios without debating them. The goal isn't to decide which failures are likely. The goal is to surface every failure mode the team can imagine.

The pre-mortem works because it bypasses optimism bias. When you ask "what risks should we mitigate," people list the obvious ones: latency, accuracy, uptime. They don't list the uncomfortable ones: "the model works well in testing but fails in production because our test data doesn't match real traffic," or "the product team will demand we remove safety constraints to improve engagement metrics." Framing the exercise as failure-already-happened removes the social pressure to sound optimistic. You're not being negative. You're explaining what already went wrong in this hypothetical future.

The exercise is time-boxed. Fifteen minutes for individuals to write scenarios. Thirty minutes to share and discuss. Fifteen minutes to prioritize. One hour total. If it takes longer, you're debating instead of brainstorming. The goal is breadth, not resolution. You'll decide which scenarios to act on later. During the pre-mortem, you're just collecting failure modes. The faster you move through the collection phase, the more scenarios you surface before people self-censor.

## Pre-Mortem Facilitation Techniques

The facilitator's job is to create psychological safety for imagining disaster. The first rule: no pushback during the brainstorming phase. If someone says "the model memorized training data and leaked PII," you don't respond with "we have differential privacy." You write it down. If someone says "the team got burned out and half of them quit," you don't respond with "we have reasonable working hours." You write it down. Pushback during brainstorming shuts down honesty. People stop sharing uncomfortable scenarios if they have to defend them immediately.

The second facilitation technique is prompting by category. If the team is stuck, the facilitator prompts: "What technical failure modes are possible? What organizational failures? What data failures? What adversarial failures? What failures caused by success — what breaks if adoption is 10x higher than expected?" These prompts unstick people. Someone who couldn't think of a failure mode suddenly remembers: "If adoption is 10x higher, the model serving costs blow past our budget and we have to throttle access."

The third technique is the outsider perspective. The facilitator asks: "If a new engineer joined the team today and looked at this project, what would concern them?" Or: "If an adversarial security researcher wanted to break this system, how would they do it?" Or: "If a journalist wrote a critical article about this launch, what would the headline be?" These prompts surface blind spots. The team has been working on the project for months. They've normalized risks that an outsider would find alarming. The outsider perspective makes those risks visible again.

## AI-Specific Failure Scenarios to Consider

AI systems have failure modes that traditional software doesn't. A structured pre-mortem for an AI project should explicitly prompt for these categories. The first category is silent degradation: the model's accuracy declines gradually, but no single day triggers an alert. Users start complaining. You investigate and discover the model has been degrading for six weeks, but your metrics didn't catch it because the decline was below your threshold per day. The pre-mortem scenario: "We launched with daily accuracy monitoring. The model drifted slowly enough that daily deltas looked normal, but six-week accuracy dropped 8 points."

The second category is distributional mismatch: the training data doesn't match production data in a way you didn't anticipate. The pre-mortem scenario: "We trained on historical data from 2024-2025. In production, user behavior changed because of a new regulation. The model's predictions no longer align with what users expect." This scenario prompts you to audit your training data coverage before launch, not after complaints arrive.

The third category is cascading failures from dependency changes: an upstream model, API, or data source changes behavior, and your model fails. The pre-mortem scenario: "We depend on an external embedding API. The API provider updated their model, and the new embeddings have different statistical properties. Our model's performance degraded, but we didn't detect it because we don't monitor embedding drift." This scenario prompts you to add dependency monitoring or build redundancy before launch.

The fourth category is adversarial use: someone uses your model in a way you didn't intend, and it produces harmful outputs. The pre-mortem scenario: "A user figured out how to prompt our customer support model to generate phishing emails. We didn't have output filtering for this use case. The misuse spread on social media before we could patch it." This scenario prompts you to red-team your model for adversarial prompts before launch.

The fifth category is cost explosion: the system works perfectly, but the cost per query is higher than expected, or adoption exceeds projections, and your inference costs blow past your budget. The pre-mortem scenario: "We estimated 10,000 queries per day. We got 50,000. Our model serving costs hit five figures per day, and we had to throttle access or shut down features to stay within budget." This scenario prompts you to model cost under high-adoption scenarios and build throttling or caching before launch.

## Prioritizing Pre-Mortem Findings

After collecting failure scenarios, you prioritize by impact and likelihood. Impact: how bad is this failure if it happens? Likelihood: how probable is it? A failure that's high-impact and high-likelihood goes to the top of the list. A failure that's low-impact and low-likelihood gets documented but not acted on. The middle cases — high-impact but low-likelihood, or low-impact but high-likelihood — require judgment.

Impact is measured in user harm, business damage, and recovery cost. A failure that leaks PII is high-impact. A failure that degrades model accuracy by two points is medium-impact. A failure that adds 50ms to response time is low-impact. Likelihood is estimated by how many things would have to go wrong. A failure that requires three independent systems to fail simultaneously is low-likelihood. A failure that could happen if a single upstream API changes is high-likelihood. The team estimates likelihood based on system complexity, dependency stability, and historical incident frequency.

The top five failure scenarios become work items. For each scenario, you define a mitigation: What do we build, test, or change to make this failure less likely or less damaging? The mitigation might be a new test, a new monitoring alert, a fallback mechanism, a documentation change, or a process change. The mitigation gets prioritized into the launch roadmap. If the failure is high-impact and high-likelihood, the mitigation is a launch blocker. If the failure is medium-impact and medium-likelihood, the mitigation is a pre-launch goal but not a blocker.

## Converting Pre-Mortem Insights to Preventive Action

The value of a pre-mortem comes from the actions it generates. A failure scenario without a mitigation is just anxiety. A failure scenario with a mitigation is risk management. For each prioritized scenario, you ask: What specific change would make this failure less likely or less damaging? The answer becomes a ticket, assigned to an owner, with a deadline before launch.

Example: Pre-mortem scenario: "The model performs poorly on rural patient data because training data was 90% urban." Mitigation: "Audit training data for demographic coverage. If rural coverage is below 20%, source additional rural data or document the limitation and restrict the model's deployment scope." Owner: Data scientist. Deadline: Two weeks before launch. Result: The team discovers rural coverage is 12%. They can't source more data in time. They document the limitation in the model card and restrict the initial launch to urban hospitals only. This isn't ideal, but it's better than launching without knowing the limitation and discovering it through user complaints.

Example: Pre-mortem scenario: "Inference latency spikes during peak traffic, and clinicians stop using the system." Mitigation: "Load test at 3x expected peak traffic. If latency exceeds 500ms at that load, add caching or scale the infrastructure before launch." Owner: Infrastructure engineer. Deadline: One week before launch. Result: Load testing reveals latency spikes to 800ms at 3x traffic. The team adds a Redis cache for frequent queries and re-tests. Latency stays under 500ms at 3x traffic. The system launches with confidence that peak traffic won't break the latency budget.

## Pre-Mortem Timing in Development Cycle

The pre-mortem happens late enough that the system is mostly designed, but early enough that you can still change course without major cost. For most projects, this timing is two to four weeks before launch. Any earlier, and the system isn't concrete enough to imagine realistic failures. Any later, and you don't have time to implement mitigations.

For phased launches, you run a pre-mortem before each phase. The pre-mortem before internal beta surfaces different failures than the pre-mortem before public launch. The internal beta pre-mortem focuses on functionality and performance: "The model doesn't handle edge cases we didn't test." The public launch pre-mortem focuses on scale and misuse: "The model gets 10x more traffic than expected" or "Users find a prompt injection that generates harmful content." Each phase has its own risk profile. Each pre-mortem tailors the failure scenarios to that profile.

Some teams run mini pre-mortems before smaller releases: a new model version, a new feature, a new data pipeline. The format is shorter — 30 minutes instead of an hour — but the technique is the same. Assume it failed. Explain why. Prioritize. Mitigate. The discipline of imagining failure before launch becomes muscle memory. The team stops launching with fingers crossed and starts launching with mitigations in place.

## Pre-Mortem vs Risk Assessment

A risk assessment is a checklist: data quality, model performance, infrastructure capacity, security, compliance. A pre-mortem is an imagination exercise: what could go wrong that we haven't thought of? Risk assessments catch known risks. Pre-mortems catch unknown risks. You need both.

The risk assessment asks: "Did we test the model on out-of-distribution data?" The pre-mortem asks: "The model failed on out-of-distribution data — what distribution did we miss?" The risk assessment confirms you followed the process. The pre-mortem imagines how the process might be insufficient. The teams that do both launch with fewer surprises than the teams that only do one.

The difference in output: A risk assessment produces a compliance checklist with green checkmarks. A pre-mortem produces a list of vulnerabilities with mitigation plans. The risk assessment reassures leadership that you followed best practices. The pre-mortem surfaces the failure modes that best practices don't cover. If you only have time for one, do the pre-mortem. The value is higher. But if you have time for both, the risk assessment ensures you didn't skip fundamentals, and the pre-mortem ensures you're prepared for the unexpected.

It's three weeks before launch. You gather the team and ask them to imagine failure. They list a dozen ways the system could break. Half of those ways are problems you hadn't considered. You turn three of them into work items and fix them before launch. The system still has problems when it launches — every launch does — but it doesn't have the catastrophic failures the pre-mortem predicted. That's the value: not preventing all problems, but preventing the worst ones. The teams that assume failure before it happens launch systems that survive contact with reality.
