# 10.13 — SLO Reviews and Adjustment

The quarterly SLO review surfaced three uncomfortable truths. The response time SLO of 400ms p95 was being met 99.8% of the time — far exceeding the 95% target. The team had over-invested in latency optimization. The error rate SLO of 1.0% was being missed 40% of weeks, but users had not complained. The threshold was tighter than user expectations required. The hallucination detection SLO did not exist at all, despite leadership repeatedly asking about hallucination rates. The SLOs were technically defined and actively measured. They were also wrong.

SLOs are not set-and-forget. They are hypotheses about what users need and what the system can deliver. Those hypotheses must be tested against reality and adjusted as systems evolve, user expectations change, and organizational priorities shift. Teams that treat SLOs as permanent commitments find themselves either over-investing in reliability that users do not value or under-investing in dimensions that matter. SLO reviews are the mechanism that keeps SLOs relevant, achievable, and aligned with actual needs.

## Why SLOs Need Regular Review

The world changes faster than SLOs. User expectations evolve as competitors set new standards. System capabilities improve as infrastructure and models mature. Business priorities shift as markets and strategies change. Product features change what users do and what they expect. The SLOs that made sense at launch rarely make sense two years later without adjustment.

User expectations rise over time. A response time that felt fast in 2024 feels slow in 2026 because competitors have set faster standards. Users who were forgiving of occasional errors in an early product become less forgiving as the product matures and they depend on it for critical workflows. The bar for what constitutes acceptable reliability increases. SLOs that were ambitious initially become table stakes.

System capabilities change as you invest in infrastructure and optimization. Multi-region deployment reduces incident impact. Better monitoring catches issues faster. Model improvements reduce error rates. These changes mean you can meet more aggressive SLOs than when the system was first built. Failing to raise SLOs when capabilities improve means you are not capturing the value of your investments.

Business priorities shift as products move through lifecycle stages. An early-stage product prioritizes availability over latency — better slow than down. A mature product serving critical workflows prioritizes both. A product under competitive pressure might prioritize features over reliability for a period. SLOs must reflect current priorities, not historical ones.

New dimensions become measurable or important. Initially, you might only track availability and latency because you had no way to measure quality. As you build evals and monitoring, you can define quality SLOs. These become as important as availability but were not possible to define initially. SLO reviews are when new dimensions are added.

## The Quarterly SLO Review Process

Most teams review SLOs quarterly, coinciding with planning cycles. The review involves engineering, product, and leadership. It examines each existing SLO for continued relevance, proposes new SLOs for dimensions that have become important, and adjusts thresholds based on recent performance and user feedback. The output is an updated SLO set that guides the next quarter's reliability investment.

The review starts with performance against current SLOs. For each SLO, examine compliance over the past quarter. Did you meet the SLO consistently? Did you miss it consistently? Did you exceed it by a large margin? Each pattern suggests different adjustments. Consistently meeting suggests the SLO is appropriate or possibly too loose. Consistently missing suggests the SLO is too tight or the system needs investment. Consistently exceeding by large margins suggests over-investment or that the SLO should be tightened.

User feedback provides critical context. Are users complaining about aspects of reliability not covered by SLOs? Are they satisfied despite SLO misses? A disconnect between SLO compliance and user satisfaction means the SLOs are measuring the wrong things or set at wrong thresholds. User feedback might reveal that p95 latency is not what matters — p99 latency is what users notice because the worst experiences create the strongest memories.

Incident analysis shows where SLOs failed to predict user impact. Were there incidents that did not trigger SLO violations but affected users significantly? Were there SLO violations that had minimal user impact? These disconnects reveal that SLOs are not calibrated correctly to actual user experience. The review uses incident data to realign SLOs with real impact.

Business context from product and leadership shapes priorities. Is the business prioritizing growth, which might tolerate slightly lower reliability to ship features faster? Is the business prioritizing enterprise sales, which requires tighter SLOs to meet buyer expectations? Is the business responding to competitive pressure on a specific reliability dimension? SLOs must serve business needs or they become academic exercises.

## Signs Your SLOs Are Wrong

Several patterns indicate SLOs need adjustment. Missing SLOs consistently despite effort suggests they are too tight for current system capabilities or traffic patterns. Exceeding SLOs consistently by large margins suggests they are too loose and not driving appropriate investment. Users complaining about reliability issues not covered by SLOs suggests missing dimensions. SLO violations that cause no user complaints suggest wrong thresholds.

The consistently missing pattern is common with initial SLOs. Teams set ambitious targets based on hopes rather than capabilities. The system never meets the SLO. The team feels perpetually behind. This creates demoralization and reduces SLO credibility. If you are missing an SLO 40% of the time, either the SLO is wrong or you need major investment to meet it. The quarterly review forces the decision.

The consistently exceeding pattern reveals over-investment. A latency SLO of 400ms that you meet at 200ms means you spent resources on latency optimization that could have gone elsewhere. The appropriate response is to tighten the SLO to 250ms or 300ms, capturing the improved capability, or to shift resources to other reliability dimensions that are not exceeding targets.

The missing dimension pattern emerges as products mature. Initially, you tracked only availability. Now users care about quality, hallucination rates, or citation accuracy. These dimensions affect user trust and satisfaction but are not reflected in SLOs. The review is when you add them.

The wrong threshold pattern happens when SLOs violate but users do not notice or when users complain without SLO violations. A 1% error rate SLO might be violated weekly, but if errors occur in edge cases that few users hit, user satisfaction remains high. Conversely, a 400ms latency SLO might be met, but if the p99 latency is 2 seconds, the worst user experiences are terrible. The thresholds need adjustment to align with actual impact.

## Adjusting SLOs Upward

Tightening SLOs captures improved system capabilities and rising user expectations. You adjust SLOs upward when you have consistently exceeded them, when competitive pressure demands better reliability, when enterprise buyers require tighter SLAs, or when user feedback indicates current performance is table stakes but not differentiating.

The adjustment process requires evidence that tighter SLOs are achievable. If your current p95 latency SLO is 400ms and you have achieved 250ms consistently for three months, you have evidence that a 300ms SLO is realistic. Tightening to 250ms might be ambitious. Tightening to 300ms captures improvement while leaving headroom. The new SLO should be achievable without major new investment.

Adjusting upward creates accountability for maintaining improvements. Teams often optimize a dimension, achieve great performance, then let it regress as attention shifts elsewhere. A tightened SLO prevents regression by making the improved performance a commitment. It converts a temporary achievement into a permanent standard.

Communication is critical when tightening SLOs. The team must understand why the SLO is changing and commit to meeting the new threshold. Stakeholders must understand what is driving the change — improved capabilities, competitive pressure, or user expectations. Tightening SLOs without buy-in creates resistance and increases likelihood of missing the new targets.

Tightening should be gradual unless there is urgent business need. A latency SLO of 400ms might move to 350ms this quarter, then to 300ms next quarter if performance supports it. Gradual tightening gives the team time to build supporting infrastructure, optimize incrementally, and avoid heroic efforts that are unsustainable.

## Adjusting SLOs Downward

Loosening SLOs is harder organizationally than tightening them. It feels like lowering standards. But sometimes SLOs are set unrealistically, the cost of meeting them exceeds their value, or business priorities shift to favor features over reliability for a period. Adjusting SLOs downward requires clear justification and stakeholder alignment.

Realistic recalibration happens when initial SLOs were aspirational rather than achievable. A team might launch with a 99.9% uptime SLO, then discover that their multi-provider architecture realistically supports 99.5% uptime without significant additional investment. Adjusting to 99.5% aligns the SLO with reality. The alternative is to invest heavily to reach 99.9% or to operate in perpetual SLO violation. Neither is sustainable.

Cost-value trade-offs sometimes justify loosening SLOs. Meeting a 200ms latency SLO might require infrastructure that costs $40,000 per month more than meeting a 300ms SLO. If user satisfaction data shows that users do not perceive the difference, spending $40,000 per month for imperceptible improvement is waste. Loosening the SLO and reallocating the budget to dimensions users care about increases overall value.

Business priority shifts can justify temporary SLO adjustments. A startup under competitive pressure might deliberately loosen reliability SLOs for two quarters to ship features faster, then retighten once market position is secure. This is a calculated risk. It requires clear communication that the loosening is temporary, specific timelines for retightening, and agreement across leadership that the trade-off is worth it.

The danger of loosening SLOs is that it becomes permanent. Temporary adjustments for strategic reasons turn into long-term acceptance of lower reliability. The quarterly review process prevents this by forcing reconsideration. If you loosened an SLO two quarters ago for strategic reasons, the review asks: do those reasons still apply, or should we retighten now?

## Stakeholder Involvement in SLO Changes

SLO changes affect multiple teams. Engineering must meet the SLOs. Product must communicate them to users. Support must handle escalations when SLOs are missed. Leadership must approve the reliability investment required. Changing SLOs without involving these stakeholders creates misalignment and reduces commitment.

Engineering involvement is obvious — they must believe the new SLOs are achievable with available resources. If engineering does not commit to new SLOs, they will not be met. The review process must include engineering validation that proposed SLOs are realistic given current architecture, traffic patterns, and planned investments.

Product involvement ensures SLOs align with user expectations and product strategy. Product understands what users care about, what competitors offer, and what reliability is required to meet business goals. Product input prevents engineering from tightening SLOs on dimensions users do not value while missing dimensions they do.

Support involvement provides ground truth about user impact. Support sees what users complain about. They know which reliability issues cause escalations and which go unnoticed. If support reports frequent complaints about slow response times but SLO data shows latency targets are met, the SLO threshold might be wrong or measuring the wrong percentile.

Leadership involvement is essential for SLO changes that affect business commitments or require significant investment. If you tighten SLOs in ways that flow through to customer SLAs, leadership must approve. If you loosen SLOs to free up resources for features, leadership must agree the trade-off serves business priorities. SLO changes are not purely technical decisions.

## Communicating SLO Changes

SLO changes must be communicated clearly to all affected parties. Internal teams need to understand what changed and why. Customers with SLAs need to know if their guarantees changed. Users need to understand if they should expect different performance. The communication strategy depends on who is affected and how.

Internal communication includes the rationale for changes, the new thresholds, the timeline for implementation, and implications for reliability investment. "We are tightening the p95 latency SLO from 400ms to 300ms based on consistently exceeding the previous target. This change is effective next quarter. We will shift some engineering time from feature development to latency optimization to maintain the new threshold." Everyone understands what changed and why.

Customer communication is necessary when SLO changes affect contractual SLAs. Tightening SLOs that improve customer SLAs is easy to communicate — "we are committing to better performance." Loosening SLOs requires careful handling. If you must loosen SLOs affecting customer SLAs, you need customer consent or risk contract violations. Often this means offering concessions — price adjustments, extended contracts, or commitments to retighten within a specific timeline.

User communication depends on whether changes are visible. Tightening latency SLOs makes the product faster — users benefit without needing explanation. Loosening error rate SLOs might increase error frequency — users might notice and deserve explanation. Transparency builds trust. If you must accept temporarily higher error rates for strategic reasons, telling users you are prioritizing features they requested over marginal reliability creates understanding rather than frustration.

Documentation must be updated to reflect new SLOs. Internal runbooks, public status pages, and customer-facing SLA documents all need updates. Outdated documentation creates confusion during incidents when stakeholders reference old SLOs. The SLO change is not complete until documentation reflects new reality.

## SLO Versioning and History

Maintaining a history of SLO changes provides valuable context. Why was this SLO changed? What was the performance before and after? What business context drove the decision? SLO version history answers these questions and prevents repeating past mistakes.

SLO versioning is simple: every SLO has a version number or effective date. "Error rate SLO v3, effective Q2 2026: 1.0% with 95% compliance" indicates this is the third version of the error rate SLO and when it became active. The versioning allows you to track performance across SLO changes and understand whether changes achieved their intended effects.

Historical context explains why SLOs changed. "We tightened the latency SLO in Q4 2025 because competitor launched with 200ms latency and we were at 400ms. The tightening required $30,000 in infrastructure investment but allowed us to match competitive positioning." This context helps future reviews understand whether the investment was worth it and whether similar changes are justified.

Performance trends across SLO versions show whether adjustments improved outcomes. If you loosened an error rate SLO from 0.5% to 1.0% in Q1 and error rates immediately jumped to 1.2%, the loosening may have removed helpful pressure. If error rates stayed at 0.6%, the loosening was appropriate — the old SLO was unnecessarily tight. The data informs future adjustments.

SLO history also reveals patterns in organizational behavior. Teams that constantly loosen SLOs might be systematically over-committing. Teams that never adjust SLOs might be treating them as fixed requirements rather than living standards. The history creates accountability and learning across quarters and years.

In February 2026, the best AI engineering teams treat SLOs as living documents that evolve with systems and expectations. They review SLOs quarterly, involving engineering, product, support, and leadership. They adjust SLOs based on performance data, user feedback, incident patterns, and business priorities. They communicate changes clearly to all stakeholders. They maintain version history to learn from past adjustments. SLOs that never change are probably wrong. SLOs that change thoughtfully keep reliability investment aligned with what actually matters.

The next chapter examines on-call, escalation, and organizational patterns — how teams structure themselves to maintain and improve AI system reliability.

