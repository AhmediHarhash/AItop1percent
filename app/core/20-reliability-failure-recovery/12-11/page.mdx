# 12.11 — Scheduled Refresh and Recalibration Systems

A fraud detection AI operated for 11 months without intervention. The team believed the model was stable — accuracy held at 94%, false positive rate stayed under 2%, no anomalies. In month 12, an audit revealed the model had drifted dramatically. It was missing new fraud patterns that emerged in months 8 through 11. The stable metrics were misleading — the model was still catching old fraud patterns while entirely new patterns went undetected. Fraud losses increased 18% year-over-year despite "stable" model performance. The team had no scheduled refresh process. They waited for metrics to degrade before acting. By the time metrics showed problems, the damage was done. They rebuilt a refresh pipeline, scheduled quarterly model updates, and automated recalibration. Fraud detection improved within six weeks. The preventable losses totaled $1.2 million.

Scheduled refresh prevents drift before it becomes severe. You do not wait for metrics to fail. You do not wait for alerts. You proactively update, recalibrate, and refresh components on a defined schedule. Prevention is cheaper than recovery.

## Why Scheduled Refresh Matters

AI systems age. The aging is predictable. You know the model will drift. You know the data distribution will shift. You know the feedback signal will change. You know embeddings will decay. The timing is uncertain, but the fact of aging is certain. Scheduled refresh acknowledges this reality.

Waiting for metrics to degrade before acting is reactive. Metrics are lagging indicators. By the time accuracy drops, drift has been accumulating for weeks or months. By the time user complaints increase, quality has already degraded noticeably. Reactive response means you are always behind. Scheduled refresh is proactive. You intervene before metrics fail. You prevent degradation instead of repairing it.

Scheduled refresh also reduces operational burden. If you only refresh when metrics fail, every refresh is urgent. Every refresh is an incident. Every refresh disrupts the team. If you refresh on a schedule, each refresh is planned. The team allocates time. The process is calm. The disruption is minimized. Scheduled work is less stressful than reactive fire-fighting.

Scheduled refresh creates predictability. Stakeholders know when updates will occur. Product can plan around them. Engineering can batch related work. Legal can schedule compliance reviews. Users can be informed in advance. Predictability reduces friction.

Scheduled refresh enables automation. If refresh happens on an unknown schedule triggered by metric degradation, automation is difficult — you must handle arbitrary timing and context. If refresh happens on a known schedule, automation is straightforward — the schedule is the trigger. The system runs without manual intervention.

## Components That Need Refresh

Not all components age at the same rate. Tailor refresh schedules to component type.

**Models** need refresh most frequently. Models learn from feedback. Feedback drifts. Models drift with it. Fine-tuned models drift faster than base models. Models in high-change domains — finance, news, social media — drift faster than models in stable domains. Refresh models quarterly as a baseline. Refresh monthly in high-change domains. Refresh weekly if your domain is extremely dynamic.

**Training data** needs refresh less frequently than models if the data is curated. If training data is static — a fixed golden dataset — it does not age. If training data is continuously updated with user interactions, it ages as fast as the model. Refresh curated training data annually. Review it quarterly for staleness. Refresh interaction-based training data whenever you refresh the model — they age together.

**Embeddings** need refresh when the corpus changes. If you are embedding a fixed document set, embeddings do not age. If you are embedding a dynamic corpus — support tickets, user messages, product catalog — embeddings decay as the corpus changes. Refresh embeddings monthly if the corpus updates frequently. Refresh quarterly if updates are infrequent.

**Evaluation datasets** need refresh semi-annually. Eval datasets measure current performance. As the world changes, the eval dataset should reflect current reality. An eval dataset from 18 months ago tests the model against outdated scenarios. Refresh eval datasets every six months. Add new examples that reflect current user behavior. Remove examples that no longer occur.

**Thresholds and calibration parameters** need refresh quarterly. Thresholds that worked at launch might be wrong six months later. Recalibrate classification thresholds. Recalibrate confidence thresholds. Recalibrate routing thresholds. Measure precision-recall curves on current data. Adjust thresholds to match current tolerance.

**Feature importance and model explanations** need refresh when model behavior changes. If you provide explanations based on feature importance calculated at training time, those explanations become stale as the model drifts. Recalculate feature importance quarterly. Update explanation systems to reflect current model behavior.

**Policy compliance filters** need refresh when policy changes. If policy evolves, filters must evolve with it. Review filters quarterly. Update them when policy updates. Test them on current data to ensure they still catch violations.

## Refresh Frequency Decisions

The optimal refresh frequency balances drift risk against operational cost. Refresh too often and you waste resources. Refresh too rarely and drift accumulates.

Start with domain velocity. High-velocity domains — breaking news, financial markets, social trends — require high refresh frequency. Monthly or even weekly refresh. Low-velocity domains — medical reference, legal precedent, historical analysis — tolerate lower frequency. Quarterly or semi-annual refresh.

Measure drift rate. Track how fast your metrics degrade without intervention. If accuracy drops 2% per month, quarterly refresh is too infrequent. Monthly refresh is appropriate. If accuracy drops 1% per quarter, quarterly refresh suffices.

Consider regulatory requirements. Regulated industries often mandate periodic model review and validation. Healthcare AI might require quarterly reviews. Financial AI might require semi-annual reviews. Match your refresh frequency to regulatory cadence. Do not let compliance audits find drift you should have already corrected.

Factor in operational cost. Each refresh requires compute, engineering time, and validation effort. If refresh costs $10,000 in compute and 40 hours of engineering time, monthly refresh costs $120,000 and 480 hours per year. Quarterly refresh costs $40,000 and 160 hours per year. The cost difference is significant. Balance it against drift risk. If drift causes more than $80,000 in annual damage, monthly refresh is worth it. If not, quarterly suffices.

Factor in user tolerance. Some applications tolerate drift better than others. A recommendation system can drift 5% without users noticing. A medical diagnostic system cannot. High-stakes applications require higher refresh frequency regardless of cost.

## Automated Recalibration Systems

Manual refresh does not scale. Automate refresh so it happens reliably without manual triggers.

Build a refresh pipeline. The pipeline runs on schedule. It pulls latest training data. It retrains or fine-tunes the model. It validates the new model on holdout data. It compares new model performance to current production model. If the new model improves or matches current model, it proceeds to deployment. If the new model degrades, it alerts engineers and halts deployment.

Build a recalibration pipeline. The pipeline runs on schedule. It samples recent production data. It calculates precision, recall, and ROC curves. It identifies optimal thresholds for current data distribution. It compares new thresholds to current thresholds. If new thresholds improve metrics, it updates configuration. If new thresholds degrade metrics, it alerts engineers.

Build an embedding refresh pipeline. The pipeline monitors corpus change rate. When the corpus changes by more than 10%, it triggers reembedding. It generates new embeddings for changed documents. It updates the vector database. It validates retrieval quality on test queries. If quality holds or improves, it completes the refresh. If quality degrades, it alerts engineers.

Build a validation gate. Every automated refresh runs through validation before deployment. The validation gate checks that accuracy, latency, policy compliance, and cost meet minimum thresholds. If any threshold fails, the refresh is blocked. Engineers investigate. The gate prevents automated refresh from deploying a degraded model.

Build rollback capability. If an automated refresh deploys successfully but causes problems in production, you need instant rollback. Store the previous model version. Store previous thresholds. Store previous embeddings. Rollback reverts to the stored versions within minutes. This makes automated refresh low-risk — you can always undo it.

## Refresh Testing and Validation

Refresh is not deployment. Refresh generates a candidate. Validation determines if the candidate is better than current production. Do not skip validation. Do not assume refresh always improves performance.

Validate on holdout data. Run the refreshed model on a dataset it has not seen. Measure accuracy, precision, recall, F1. Compare to current production model on the same data. If the refreshed model improves by at least 1%, it is a clear win. If it matches within 0.5%, it is neutral — deploy to benefit from recency. If it degrades by more than 0.5%, investigate before deploying.

Validate on recent production data. Holdout data might be stale. Sample the last 1,000 production inputs. Run both current and refreshed models. Compare outputs. Measure which outputs users preferred (if feedback exists). The refreshed model should perform as well or better on recent data. If it performs worse, it has not adapted correctly.

Validate policy compliance. Refresh might inadvertently degrade policy compliance. Run the refreshed model through your policy compliance test suite. Measure compliance rate. Compare to current production model. If compliance drops, the refresh learned policy-violating patterns from training data. Filter the training data and re-refresh.

Validate latency and cost. Refresh might change model size or inference characteristics. Measure latency on a sample of production queries. Measure cost per request. If latency increases by more than 10% or cost increases by more than 10%, investigate. The performance gain might not justify the operational regression.

Validate edge cases. Maintain a set of known difficult examples — edge cases, adversarial inputs, rare scenarios. Run the refreshed model on these examples. Compare outputs to current production model. The refreshed model should handle edge cases as well or better. If it fails on edge cases the current model handles correctly, it has forgotten important knowledge. Retrain with edge case augmentation.

## Coordinating Refresh Across Components

AI systems have multiple components. Refreshing one component affects others. Coordination prevents mismatches.

Refresh models and embeddings together. If you refresh embeddings but not the model, retrieval behavior changes but generation behavior does not. The model expects old retrieval patterns. It receives new patterns. The mismatch degrades quality. Refresh both in the same maintenance window. Validate the combination before deployment.

Refresh models and thresholds together. A refreshed model might output different confidence distributions than the old model. Old thresholds might no longer be optimal. Recalibrate thresholds immediately after model refresh. Do not deploy a refreshed model with stale thresholds.

Refresh models and eval datasets together. As you refresh the model, refresh the eval dataset to reflect current reality. An old eval dataset measures performance on outdated scenarios. A current eval dataset measures performance on real scenarios. Validate the refreshed model on the refreshed eval dataset.

Stagger refresh for dependent components. If you have a multi-stage system — retrieval, ranking, generation — do not refresh all stages simultaneously. Refresh and validate one stage. Deploy it. Monitor for 48 hours. Then refresh the next stage. Staggered refresh isolates problems. If quality degrades after a refresh, you know which component caused it.

Coordinate refresh with deployment windows. Refresh is a deployment. Treat it as one. Do not refresh during high-traffic periods. Do not refresh immediately before holidays. Schedule refresh during low-traffic windows. This reduces user-facing risk.

## Refresh Rollback Procedures

Not all refreshes succeed. Some degrade performance. Some introduce regressions. Rollback reverts to the previous version.

Automate rollback. Do not require manual intervention. If quality metrics degrade by more than 5% within 24 hours of refresh, trigger automatic rollback. The system reverts to the previous model, previous thresholds, previous embeddings. Manual rollback takes too long. Automate it.

Test rollback regularly. Run rollback drills quarterly. Execute a refresh, then immediately roll back. Verify rollback completes successfully. Verify metrics return to pre-refresh levels. Untested rollback procedures fail during incidents. Test them when stakes are low.

Store rollback artifacts safely. The previous model version must remain accessible for at least two weeks post-refresh. Store it in durable storage. Tag it clearly. Document its performance characteristics. If rollback is needed two weeks later, you need the artifact and the context.

Monitor post-rollback stability. After rolling back, monitor closely for 48 hours. Rollback should restore previous performance. If metrics remain degraded, the problem is not the refresh — it is something else. Investigate deeper.

Document every rollback. Why did refresh fail? What metric degraded? What pattern caused the degradation? How was it detected? Rollback documentation builds institutional knowledge. The team learns what kinds of refresh fail. They adjust the refresh pipeline to prevent similar failures.

## Measuring Refresh Effectiveness

Refresh is an operational cost. Measure whether it delivers value.

Measure drift prevented. Compare drift rate with scheduled refresh to drift rate without it. If your accuracy degrades 1% per quarter without refresh but stays stable with quarterly refresh, refresh prevents 4% annual drift. Quantify the value of prevented drift — reduced errors, better user experience, fewer compliance issues. Compare to refresh cost.

Measure incident reduction. Count production incidents related to drift. With scheduled refresh, incidents should decrease. If you had six drift-related incidents per year before scheduled refresh and two after, refresh prevents four incidents. Quantify the cost of incidents — engineering time, user impact, reputation damage. Compare to refresh cost.

Measure maintenance efficiency. Track engineering time spent on reactive drift correction before scheduled refresh. Track engineering time spent on scheduled refresh after implementation. Scheduled refresh should reduce total maintenance time. If reactive correction required 200 hours per year and scheduled refresh requires 150 hours, you saved 50 hours.

Measure user impact. Survey users before and after implementing scheduled refresh. Ask about quality consistency. Users should perceive more stable quality with scheduled refresh. Quantify satisfaction improvement. If NPS increases by 5 points, attribute part of the increase to refresh.

Measure compliance confidence. Scheduled refresh makes compliance audits smoother. You demonstrate proactive maintenance. Auditors see scheduled reviews. Compliance risk decreases. This is hard to quantify but valuable. Track audit findings before and after scheduled refresh. Findings should decrease.

Proactive refresh prevents drift from becoming crisis. It makes aging predictable and manageable. It turns maintenance from reactive fire-fighting into planned operations. Next: a complete playbook for maintaining aging AI systems, bringing together all the techniques into a coherent program.
