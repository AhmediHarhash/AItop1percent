# 7.7 — Gradual Traffic Restoration: The Canary Pattern

Most teams think that incident recovery ends when the fix is deployed. They restore traffic to 100 percent, mark the incident resolved, and move on. Then they discover that the fix was incomplete, or introduced a new problem, or works in testing but fails under production load. Now the incident cascades — the second outage affects all users because traffic was restored all at once. Gradual traffic restoration prevents this. You restore traffic in stages, monitoring metrics at each stage. If problems appear, you catch them when they affect 5 percent of users, not 100 percent. The canary pattern is borrowed from software deployment — send a small amount of traffic first, verify it succeeds, then gradually increase until all traffic is restored.

This matters because fixes deployed during incidents are deployed under pressure, often with incomplete testing, sometimes with incomplete understanding of the root cause. The risk of a bad fix is higher than the risk of a normal deployment. Gradual restoration is a safety valve. It gives you time to catch secondary failures before they become total failures.

## Why Instant Restoration Is Risky

Restoring all traffic immediately after deploying a fix assumes the fix is correct, complete, and will behave under production load exactly as it did in testing. That assumption is often wrong.

The fix may be incomplete. You fixed the primary failure mode, but a secondary failure mode remains. The system runs fine at low load but fails at production scale. The fix works for 90 percent of requests but fails for the remaining 10 percent due to edge cases you did not test. Instant restoration exposes all users to the secondary failure immediately.

The fix may introduce new problems. You deployed a workaround that bypasses a broken component, but the workaround has higher latency or higher cost or triggers rate limits in a downstream system. You rolled back to a previous version, but that version has a bug that only manifests under current data patterns. Instant restoration turns the new problem into a new incident before you have a chance to detect it.

The fix may work in testing but fail in production. Your test environment has 10 percent of production traffic volume, different data patterns, different user behavior, or different external dependencies. The fix passes all tests but fails under real conditions. Instant restoration means you discover production-only failures when all users are affected.

The fix may also overwhelm cold systems. Even if the fix is correct, the system may not be ready for full load. Caches are cold, queues are empty, autoscaling has not yet provisioned additional capacity. Instant restoration floods cold systems with full traffic, triggering cascading failures that would not occur if traffic ramped gradually.

A payment processing platform experienced a database connection pool exhaustion incident in October 2025. The team increased the pool size from 200 to 400 connections and restarted the service. They restored traffic to 100 percent immediately. Within 90 seconds, the system failed again. The increased pool size fixed the original problem, but the new pool size exceeded the database server's maximum connection limit, causing connection rejections and triggering a different failure mode. The team had to take the system down again, reduce the pool size to 350, and restart. If they had used gradual restoration, they would have caught the connection limit problem when only 10 percent of traffic was restored, limiting user impact.

## The Canary Restoration Pattern

The canary pattern restores traffic in stages, with monitoring and verification at each stage. You start with a small percentage — 1 percent or 5 percent — and verify that traffic succeeds before increasing to the next stage. If any stage shows elevated errors, latency, or anomalies, you stop restoration and investigate. If all stages succeed, you reach 100 percent and declare the incident resolved.

The stages are defined in advance as part of your incident response playbook. A typical progression: 1 percent, 5 percent, 10 percent, 25 percent, 50 percent, 100 percent. Each stage runs for a minimum duration — 2 minutes, 5 minutes, 10 minutes — before proceeding to the next. The duration gives metrics time to stabilize and gives you time to detect problems.

At each stage, you monitor key metrics: error rate, latency at p50 and p99, throughput, resource utilization, downstream dependency health, and user-visible success metrics like completed transactions or successful logins. If any metric exceeds its threshold, restoration pauses. The team investigates the anomaly, determines whether it is related to the fix, and decides whether to proceed, roll back, or deploy an additional fix.

The canary pattern requires traffic routing infrastructure. You need the ability to send a specific percentage of traffic to the fixed system and the remaining traffic to a fallback — either a backup system, a degraded mode, or a holding page. Load balancers, feature flags, and traffic management layers provide this capability. If your infrastructure cannot route traffic by percentage, you cannot execute gradual restoration.

A legal research platform uses a six-stage canary restoration: 2 percent, 5 percent, 10 percent, 25 percent, 50 percent, 100 percent. Each stage runs for 3 minutes except the 2 percent stage, which runs for 5 minutes to gather sufficient data for low-traffic metrics. At each stage, the on-call engineer reviews a dashboard showing error rate, p99 latency, and query success rate. If error rate exceeds 2 percent, latency exceeds 2 seconds, or success rate drops below 97 percent, restoration halts. The engineer investigates the anomaly, determines root cause, and either proceeds with caution or rolls back. The entire restoration process takes approximately 25 minutes from start to 100 percent. The 25-minute duration is acceptable because it prevents re-incidents and gives the team confidence that the fix is stable.

## Traffic Percentage Ramps

The traffic ramp — the sequence of percentages and durations — determines how quickly you restore service and how much user impact occurs if a secondary failure is discovered.

A fast ramp uses large percentage jumps and short durations: 5 percent, 25 percent, 100 percent, each stage running for 2 minutes. Fast ramps restore service in under 10 minutes but provide less safety. If a problem appears at the 25 percent stage, 25 percent of users are already affected.

A slow ramp uses small percentage jumps and long durations: 1 percent, 2 percent, 5 percent, 10 percent, 20 percent, 50 percent, 100 percent, each stage running for 5 minutes. Slow ramps take 35 minutes or more but provide maximum safety. If a problem appears at any stage, fewer users are affected.

A balanced ramp uses small jumps early and large jumps late: 1 percent for 5 minutes, 5 percent for 3 minutes, 10 percent for 3 minutes, 25 percent for 2 minutes, 100 percent. Early stages use small percentages and long durations to catch problems with minimal user impact. Later stages use larger percentages and shorter durations because confidence is higher.

The appropriate ramp depends on incident severity, fix confidence, and user impact tolerance. If the incident affected critical functionality and the fix is uncertain, use a slow ramp. If the incident affected a minor feature and the fix is well-tested, use a fast ramp. If the incident has been running for hours and users are demanding restoration, balance speed and safety with a balanced ramp.

A financial services company uses incident severity to determine the ramp. Severity 1 incidents — those affecting payments, transactions, or account access — use a slow ramp with seven stages over 40 minutes. Severity 2 incidents — those affecting reporting or analytics — use a balanced ramp with five stages over 20 minutes. Severity 3 incidents — those affecting non-critical features — use a fast ramp with three stages over 10 minutes. The ramp is specified in the incident response runbook, so the on-call engineer does not have to design a ramp during the incident.

## Monitoring During Restoration

Gradual restoration is only effective if you actually monitor metrics at each stage and catch problems before proceeding. Monitoring during restoration requires real-time dashboards, automated alerting, and clear thresholds.

The restoration dashboard shows: current traffic percentage, time in current stage, time remaining until next stage, error rate for current stage compared to baseline, latency at p50, p95, and p99 for current stage compared to baseline, throughput in requests per second, and resource utilization — CPU, memory, disk, network. The dashboard updates every 10 to 30 seconds to provide near-real-time visibility.

Automated alerts fire if any metric exceeds its threshold during restoration. Thresholds are tighter during restoration than during normal operation because you are actively watching for problems. A 1 percent error rate might be acceptable during normal operation, but during restoration it signals that the fix may be incomplete. A p99 latency increase of 500 milliseconds might be noise during normal operation, but during restoration it suggests a performance regression.

The on-call engineer watches the dashboard during restoration and approves each stage transition. Some teams automate stage transitions — if metrics are green for the minimum duration, traffic automatically increases to the next stage. Other teams require manual approval — the engineer reviews the dashboard and clicks "proceed" to advance. Manual approval is safer but slower. Automated advancement is faster but riskier.

A SaaS platform's restoration dashboard includes a side-by-side comparison of current stage metrics versus pre-incident baseline. The engineer can see at a glance whether error rate, latency, and throughput are within acceptable ranges. The dashboard also includes a "rollback" button that immediately reverts traffic to 0 percent and activates the previous fallback configuration. The rollback button has been used three times in the past year — each time catching a bad fix before it affected more than 10 percent of users.

## Automatic Rollback During Restoration

Gradual restoration can include automatic rollback if metrics degrade beyond acceptable thresholds. Automatic rollback acts as a safety net — if the fix is bad and the on-call engineer does not notice, the system protects itself.

Automatic rollback requires: clear rollback thresholds, a rollback mechanism that can revert traffic to 0 percent or to a fallback system, and alerting that notifies the team when rollback occurs. Rollback thresholds are stricter than alerting thresholds. You might alert at 2 percent error rate but rollback at 5 percent error rate. The gap gives the engineer time to investigate and decide before the system takes automatic action.

Automatic rollback is appropriate when the risk of a bad fix is high — during complex incidents, during fixes deployed under extreme time pressure, or during fixes that touch critical systems. Automatic rollback is less appropriate when false positives are common — if your metrics are noisy and rollback would trigger unnecessarily, manual monitoring is safer.

When automatic rollback triggers, the system should: immediately revert traffic to 0 percent or to the fallback configuration, send a high-priority alert to the on-call team, log the rollback event with the triggering metric and threshold, and lock further restoration attempts until the team manually resets the lock. The lock prevents automatic retry loops where the system repeatedly tries to restore traffic, fails, rolls back, and retries.

A healthcare platform uses automatic rollback during incident recovery. If error rate exceeds 4 percent or p99 latency exceeds 3 seconds during any restoration stage, traffic automatically reverts to 0 percent and the system activates degraded mode — a fallback configuration that serves cached responses and disables non-critical features. The rollback triggers a page to the on-call engineer and posts an alert to the incident channel. The engineer investigates the rollback, determines whether the fix needs revision, and manually initiates restoration again once the issue is resolved. Automatic rollback has triggered four times in the past eighteen months, each time preventing a secondary incident that would have affected all users.

## User Selection for Canary Traffic

Gradual restoration requires deciding which users see the restored service first. The selection strategy affects user experience and incident detection.

**Random selection** routes a random percentage of traffic to the restored system. This is the simplest approach and ensures a representative sample of users, query types, and usage patterns. Random selection works well for homogeneous user bases where all users have similar behavior.

**Geographic selection** routes traffic from specific regions first. You might restore traffic to a low-population region or time zone first, verify success, then expand to higher-population regions. Geographic selection limits blast radius — if the fix fails, only users in the initial region are affected. Geographic selection works well for globally distributed systems.

**Tier-based selection** routes traffic from specific user tiers first. You might restore traffic to internal users or free-tier users first, verify success, then expand to paid users and enterprise customers. Tier-based selection protects high-value users from bad fixes. It also allows internal users to act as canaries — they experience the fix first and can report problems before external users are affected.

**Synthetic traffic selection** routes synthetic test traffic first, then real user traffic. Synthetic traffic executes pre-defined test cases that exercise critical paths. If synthetic traffic succeeds, real user traffic follows. Synthetic traffic selection provides the safest canary because no real users are affected during initial verification. However, synthetic traffic may not catch problems that only appear with real user data or real usage patterns.

A B2B SaaS platform uses tier-based selection during restoration. Internal employee accounts receive traffic first — 100 percent of internal traffic is restored immediately. The team monitors internal usage for 5 minutes. If internal traffic succeeds, free-tier users are restored at 10 percent, then 50 percent, then 100 percent. Once free-tier restoration is complete, paid users are restored at 10 percent, then 50 percent, then 100 percent. Finally, enterprise customers are restored at 10 percent, then 100 percent. The multi-tier approach ensures that high-value customers are the last to experience any potential problems. The entire restoration takes approximately 35 minutes. Enterprise customers appreciate the conservative approach — they prefer a longer restoration time to the risk of a secondary failure affecting their production workflows.

## Duration of Gradual Restoration

How long should gradual restoration take? The trade-off is between speed and safety. Faster restoration returns users to service sooner. Slower restoration provides more time to detect problems.

The minimum duration per stage should be long enough to collect meaningful metrics. If your monitoring system aggregates data over 1-minute windows, a stage that runs for 30 seconds provides incomplete data. A stage that runs for 2 minutes provides two full data points. A stage that runs for 5 minutes provides five data points and allows you to detect trends — is error rate increasing, decreasing, or stable?

The total duration should be balanced against user expectations. If users have been waiting for 3 hours for service restoration, a 30-minute gradual restoration process is reasonable. If users have been waiting for 15 minutes, a 30-minute restoration feels too slow. Communicate the expected restoration duration so users know what to expect.

Some teams use fixed-duration stages — every stage runs for exactly 5 minutes. Other teams use confidence-based stages — each stage runs until the team is confident that metrics are stable, which might take 2 minutes or 10 minutes depending on what the metrics show. Confidence-based stages are slower but safer.

A logistics platform's gradual restoration takes approximately 18 minutes: 1 percent for 5 minutes, 10 percent for 4 minutes, 25 percent for 3 minutes, 50 percent for 3 minutes, 100 percent for 3 minutes. The initial 1 percent stage runs longer because low-traffic metrics take time to stabilize. Later stages run shorter because confidence is higher. The 18-minute duration is communicated to users via the status page: "We are gradually restoring traffic to ensure stability. Expected full restoration: 4:32 PM." The clear timeline manages expectations and reduces support inquiries.

## When to Skip Gradual Restoration

Gradual restoration is not always necessary. Some incidents and fixes are low-risk enough that instant restoration is appropriate.

Skip gradual restoration when: the fix is a configuration change with no code deployment, the fix has been tested extensively in staging and matches production exactly, the incident affected a non-critical feature with minimal user impact, the fix is a rollback to a known-good version that was previously stable, or the system has been down for many hours and users are demanding immediate restoration.

Skipping gradual restoration is a judgment call. The decision should involve the incident commander, the on-call engineer, and stakeholders who understand user impact and business risk. If there is any doubt, default to gradual restoration. The time cost of gradual restoration is small. The cost of a secondary incident is large.

An e-commerce platform experienced a configuration error that disabled product image loading. The fix was reverting a single configuration file to its previous value. The fix was tested in staging and confirmed to work. The incident affected a visual feature, not core transaction processing. The team decided to skip gradual restoration and restore traffic to 100 percent immediately. The fix worked correctly. Service was fully restored within 2 minutes of deploying the fix. In contrast, when the same platform experienced a database query performance regression requiring a code change, the team used gradual restoration with a six-stage ramp over 20 minutes. The different restoration strategies matched the risk profile of each incident.

## Communicating Restoration Progress

Users waiting for service restoration want updates. Gradual restoration takes longer than instant restoration, so communication is especially important.

The status page should show current restoration progress: "Traffic restoration in progress. Currently at 25 percent. Expected completion: 4:42 PM." Update the status page at each stage transition. Users can see that the system is recovering and know when to expect full service.

If restoration pauses due to an anomaly, communicate that: "Traffic restoration paused at 25 percent while we investigate an error rate increase. We will provide an update in 10 minutes." Transparency builds trust. Users prefer knowing that the team is being cautious to prevent further issues.

When restoration completes, announce it clearly: "Traffic fully restored. All services are operating normally. Thank you for your patience." Close the incident and update the status page to "resolved."

Internal communication should mirror external communication. The incident channel should show restoration progress, stage transitions, metric observations, and any anomalies. Stakeholders following the incident can see real-time progress and understand what the team is doing.

A SaaS platform's incident communication during restoration: "3:15 PM — We have deployed a fix and are beginning gradual traffic restoration. Starting at 5 percent." "3:18 PM — Metrics at 5 percent look good. Increasing to 10 percent." "3:22 PM — 10 percent stage complete. Increasing to 25 percent." "3:28 PM — Observing slightly elevated latency at 25 percent. Investigating before proceeding." "3:35 PM — Latency issue resolved. Resuming restoration. Increasing to 50 percent." "3:41 PM — Traffic restoration complete. All services operating normally." The detailed updates reassured users that the team was being careful and methodical, which reduced complaints and support load.

The next subchapter covers user re-engagement after recovery — how to bring users back after an extended outage and rebuild trust.

