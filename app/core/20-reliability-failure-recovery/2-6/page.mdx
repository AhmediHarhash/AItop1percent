# 2.6 — Alert Design: Sensitivity, Specificity, and Alert Fatigue

Most teams that skip this step typically find they've created a monitoring system that generates perfect data and useless alerts. The dashboards show every metric. The anomaly detectors catch every deviation. And the on-call engineer ignores all of it because 95% of alerts are false positives. An alert that nobody responds to is worse than no alert — it creates the illusion of safety while providing none of the protection. Alert design is not an afterthought. It's the interface between detection and response. Get it wrong and your entire reliability infrastructure becomes noise.

The core tension in alert design is sensitivity versus specificity. High sensitivity catches every real incident but drowns you in false positives. High specificity eliminates false positives but misses real incidents until they're catastrophic. You cannot optimize both simultaneously. You must choose based on the cost of missed incidents versus the cost of alert fatigue. For customer-facing production systems, miss a P1 incident and you lose revenue, trust, and credibility. For internal experimentation platforms, false positives waste engineer time but missed incidents have limited impact. The optimal balance differs by system criticality.

## The Alert Fatigue Epidemic in AI Teams

Alert fatigue is not a theoretical risk. It's the default outcome of untuned monitoring. A team ships monitoring with default thresholds. Thirty alerts fire in the first week. Twenty-seven are false positives. The engineer investigates each one, finds nothing wrong, and adjusts thresholds. Next week, 18 alerts. Thirteen false positives. More tuning. By week four, the engineer stops investigating. They glance at the alert, assume it's another false positive, and silence it. When a real incident occurs, the alert fires. The engineer ignores it. Users detect the problem first.

The psychological mechanism is Pavlovian. Alert sound plus investigation plus no real problem equals wasted time. Repeat this 50 times and the brain learns to associate the alert sound with false positives. The engineer's immediate reaction to an alert becomes annoyance, not urgency. They've been trained by experience to distrust the monitoring system. Reversing this requires weeks of consistent true positives. One real incident doesn't undo the conditioning. Ten consecutive real incidents might. The window to establish alert credibility is narrow — roughly the first 20 alerts. If most of those are false positives, you've lost the on-call engineer's trust permanently.

Alert volume amplifies fatigue. One false positive per day is tolerable. Ten per day is not. The engineer spends an hour daily investigating nothing. That hour comes out of feature development, bug fixes, or sleep if the alerts fire overnight. After two weeks, the engineer either disables the alerts or starts ignoring them. Either way, monitoring becomes decorative. The absolute number of alerts matters more than the false positive rate. A system with 90% precision generating 50 alerts per day produces 5 false positives per day — unmanageable. A system with 70% precision generating 3 alerts per day produces 1 false positive per day — sustainable.

The solution is not better anomaly detection. The solution is better alert design. You cannot eliminate false positives entirely. AI systems are too complex and too non-deterministic. Instead, design alerts so that false positives are quick to diagnose and dismiss. Provide enough context that the on-call engineer can determine within 30 seconds whether the alert is real. Attach runbooks that guide investigation. Route low-confidence alerts to lower-priority channels. Reserve urgent alerts for high-confidence, high-impact events. The goal is to make alert response efficient enough that false positives are annoying but not debilitating.

## Sensitivity Versus Specificity Tradeoffs

Sensitivity measures how well you catch real incidents. A detection system with 95% sensitivity catches 19 out of 20 real incidents. The 20th incident goes undetected until users complain or until secondary monitoring catches it. High sensitivity requires loose thresholds. If your error rate anomaly threshold is set at 1.5x baseline, you catch incidents early. You also catch traffic noise, deployment blips, and transient failures that self-resolve. Sensitivity prioritizes early detection at the cost of false positives.

Specificity measures how well you avoid false positives. A detection system with 90% specificity produces one false positive for every 10 alerts. High specificity requires tight thresholds. If your error rate anomaly threshold is set at 3x baseline, you only alert on severe incidents. You miss gradual degradations that never cross the threshold but accumulate over time. Specificity prioritizes precision at the cost of delayed detection. The tradeoff is inescapable. You cannot simultaneously detect every incident early and avoid every false positive.

The optimal balance depends on incident cost. For a healthcare application where AI failures risk patient harm, maximize sensitivity. Tolerate more false positives to catch every real incident early. For an internal document summarization tool, maximize specificity. Engineers can tolerate a slightly degraded summarizer for a few hours, but they cannot tolerate constant false alarms. The rule of thumb: customer-facing systems prioritize sensitivity, internal tools prioritize specificity. Revenue-critical systems prioritize sensitivity, experimental features prioritize specificity.

Tuning curves map the tradeoff explicitly. Run your detection system against historical data that includes known incidents and known stable periods. Sweep your threshold from very loose to very tight. At each threshold, calculate sensitivity — what percentage of known incidents would you have caught — and specificity — what percentage of alerts would have been true positives. Plot sensitivity versus false positive rate. This is your receiver operating characteristic curve. Choose the operating point that matches your cost structure. If missing an incident costs 100x more than a false positive, choose the high-sensitivity point. If they cost the same, choose the balanced point.

You can shift the curve, not just move along it. Better baselines, better feature engineering, and better signal selection improve both sensitivity and specificity simultaneously. Stratified baselines reduce false positives without sacrificing incident detection. Combining multiple signals — latency and quality metrics together — increases confidence. Filtering known noisy periods improves specificity. The goal is not just to pick a point on the curve but to push the entire curve toward the top-left corner: higher sensitivity and higher specificity. This requires continuous improvement in your detection logic.

## Tiered Alerting Strategies: P1, P2, P3

Not all incidents are equal. A complete service outage affecting all users is a P1 — immediate page, wake people up, all-hands response. A 5% increase in retry rate affecting one query type is a P3 — investigate during business hours, no urgent action required. Treating every alert as equally urgent creates fatigue. Treating every alert as low-priority causes delayed response to critical issues. Tiered alerting routes alerts to the appropriate response channel based on severity.

P1 alerts indicate customer-impacting outages or severe quality degradations. Define P1 as: error rate above 5%, latency above 10 seconds, task completion rate below 60%, or any total service unavailability. These trigger immediate pages to the on-call engineer, regardless of time of day. P1 alerts must have high specificity — false P1s destroy trust faster than anything else. Set P1 thresholds conservatively. Require multiple signals to confirm before paging. A single metric spike might be noise. Three correlated metrics spiking simultaneously is a real incident.

P2 alerts indicate significant but non-catastrophic degradations. Define P2 as: error rate 2-5%, latency 3-10 seconds, task completion rate 60-80%, quality metrics outside control limits but not critically failed. These trigger notifications during business hours and lower-urgency pages during off-hours. P2 alerts should be investigated within 30 minutes during work hours, within 2 hours overnight. They're real issues but not immediate emergencies. The engineer should assess whether the issue is escalating toward P1 or stabilizing at P2 severity.

P3 alerts indicate minor anomalies or early warning signals. Define P3 as: anomalies in non-critical metrics, single-signal deviations, issues affecting small user segments, or metrics approaching but not exceeding thresholds. These go to a low-priority Slack channel or daily summary email. They don't require immediate investigation but should be reviewed once per day. Many P3 alerts self-resolve. Some escalate to P2 or P1. The goal is visibility without urgency. P3 alerts answer the question "is anything unusual happening today?" without demanding immediate action.

Escalation logic moves alerts between tiers. An alert fires as P3. If it persists for 30 minutes, escalate to P2. If it worsens or additional signals confirm the issue, escalate to P1. Automatic escalation prevents alerts from being ignored. A P3 alert that the engineer dismisses as noise becomes a P2 alert half an hour later. If still unresolved, it becomes a P1 page. This structure rewards proactive response to low-severity alerts — fix the P3 and you avoid the P2 — while ensuring critical issues eventually force response even if early signals were missed.

Demotion happens too. A P1 alert fires. The engineer investigates and finds the issue has self-resolved or was transient. Demote to P2 or P3 to reflect current severity. This keeps the alert open for tracking — you want to know if the issue recurs — without maintaining high-urgency status. Demotion logic also handles cases where an initial alert was overly conservative. A threshold triggered prematurely. The engineer confirms the system is stable. Demote the alert and adjust the threshold to prevent recurrence.

## Alert Routing: Who Gets What, When

Alerts must reach the right person at the right time through the right channel. A P1 alert that goes to email is useless — the engineer won't see it for hours. A P3 alert that pages the CTO at 3 AM destroys trust in the monitoring system. Alert routing matches severity to response mechanism and responder role.

On-call engineers receive all P1 alerts immediately via phone call or SMS. Not Slack. Not email. These channels are too easy to miss. PagerDuty, Opsgenie, and similar on-call management platforms handle this routing. They escalate through multiple contacts if the primary on-call doesn't acknowledge within 5 minutes. P2 alerts go to the on-call engineer via push notification or Slack DM. These are urgent but not wake-up-worthy unless they escalate. P3 alerts go to a team Slack channel or email digest. Anyone can review them, no specific person is responsible.

Role-based routing sends alerts to the team with relevant expertise. Infrastructure alerts go to the platform team. Quality metric alerts go to the ML engineering team. Security alerts go to the security team. This prevents alert overload on any single person and ensures the responder has context to investigate. The risk is fragmented responsibility — each team assumes another team will handle it. Mitigate this with a primary on-call rotation that receives all alerts and can escalate to specialists as needed.

Time-based routing respects work-life balance. P1 alerts page 24/7. P2 alerts page during business hours, queue overnight unless they escalate to P1. P3 alerts queue always, no paging. This structure acknowledges that most issues can wait a few hours without causing permanent damage. The exceptions — true emergencies — justify waking someone up. Everything else respects the engineer's time. Teams that page for P2 issues at 2 AM burn out their on-call rotations within months.

Geographic distribution complicates routing. A US-based team with European customers must handle incidents that occur at 3 AM Pacific time but 11 AM European time — peak customer hours. The solution is follow-the-sun on-call rotations. A US engineer handles US daytime hours. A European engineer handles European daytime hours. An Asian engineer handles Asian daytime hours. Alerts route to whoever is currently on shift. This requires sufficient team size — at least six engineers to sustain three rotations. Smaller teams must accept overnight pages or accept delayed response to off-hours incidents.

## Runbook-Attached Alerts

An alert without a runbook is a riddle. "Task completion rate dropped to 68%." What does the engineer do? Check logs? Restart the service? Roll back the last deploy? Contact the model provider? The engineer must figure out investigation steps from scratch. This takes time, often 10-20 minutes of flailing before focused investigation begins. Runbook-attached alerts provide that structure immediately. The alert fires and includes a link to a wiki page or document that describes investigation steps, common causes, and remediation actions.

The runbook structure is consistent: symptom, likely causes, investigation steps, remediation actions, escalation contacts. Symptom describes what the alert detected. Likely causes lists the three most common root causes for this alert, based on historical incidents. Investigation steps provide a checklist — check these logs, query this dashboard, run this diagnostic command. Remediation actions list fixes for each likely cause. Escalation contacts identify who to involve if initial investigation doesn't resolve the issue.

Every alert should link to a runbook. Even simple alerts. "Error rate above threshold" links to a runbook that says: check recent deploys, check model provider status page, check downstream service health, query error logs for stack traces. This takes 2 minutes to write and saves 10 minutes every time the alert fires. Multiply by 50 alerts over a quarter and you've saved hours of investigation time. Runbooks also help new team members respond to alerts without requiring deep system knowledge. They can follow the checklist even if they don't understand the system architecture yet.

Runbook quality improves through incident review. After every incident, update the relevant runbook with what you learned. If the runbook said to check three things but the root cause was a fourth thing, add the fourth thing. If an investigation step was unclear, clarify it. If a remediation action didn't work, document that. Over time, runbooks become battle-tested incident response guides that incorporate institutional knowledge from dozens of incidents. Teams with mature runbooks resolve incidents 30-50% faster than teams without them.

Auto-generated runbook context makes alerts self-explanatory. Instead of just linking to a static wiki page, include key diagnostic information in the alert itself. "Task completion rate dropped to 68%. Recent deploys: model update 2 hours ago. Recent incidents: none. Affected user segments: enterprise tier. P95 latency: normal. Error rate: normal. Likely cause: model quality regression. Runbook link: ..." The engineer reads this and immediately knows where to focus. They don't waste time checking deploys or latency — the alert already shows those are normal. They jump straight to model quality investigation.

## The Snooze Problem: Why Silenced Alerts Kill You

Alert snoozing is a disaster. An alert fires. The engineer glances at it, decides it's not urgent, and hits snooze for 8 hours. They intend to investigate later. Later never comes. Eight hours pass. The alert fires again. The engineer snoozes it again. Meanwhile, the underlying issue slowly worsens. By the time someone actually investigates, a minor anomaly has become a major incident. Snoozing creates a false sense that someone is monitoring the issue while ensuring nobody actually acts on it.

The psychological trap is that snoozing feels responsible. The engineer didn't dismiss the alert — they acknowledged it and deferred action. They plan to investigate when they finish their current task. But context switching is hard and the current task expands. Four hours later, they've forgotten about the alert. The system sends a reminder. They snooze again. The pattern repeats until the issue forces itself into attention through user complaints. By then, the response is reactive rather than proactive. You've lost the benefit of early detection.

Alternatives to snoozing: acknowledge and assign, escalate, or auto-remediate. Acknowledge and assign means the alert doesn't disappear until someone owns it. An engineer must claim the alert and commit to investigating within a specific timeframe. If they don't, the alert escalates to their manager. This creates accountability. Escalation means the alert doesn't snooze — it gets louder. A P3 that's ignored for 30 minutes becomes a P2. A P2 ignored for an hour becomes a P1. The engineer can't defer forever. Auto-remediation means the system fixes known issues automatically without requiring human response. If logs show this alert corresponds to a known transient failure, auto-restart the service and resolve the alert. Only escalate to humans if auto-remediation fails.

Alert suppression is different from snoozing. Suppression says "this alert is expected, don't notify anyone." Snoozing says "this alert might matter, but I'll deal with it later." Suppression is appropriate during planned maintenance windows, during load tests, or after confirming an alert is a false positive. You suppress alerts for a defined period with a clear reason. After the suppression expires, alerting resumes. Snoozing has no defined reason and no clear endpoint. It's procrastination disguised as triage.

Disable snooze functionality if your team can't use it responsibly. Many PagerDuty configurations allow snoozing. Turn it off. Force engineers to either resolve the alert or escalate it. This creates short-term friction — engineers complain they can't defer alerts. It creates long-term reliability — issues get addressed when they're small instead of festering into incidents. If an alert fires frequently enough that engineers constantly want to snooze it, the alert is badly configured. Fix the threshold or demote it to P3 instead of enabling snooze.

## Alert Hygiene: Regular Review and Pruning

Alerting configurations decay. You add a new alert for a new feature. The feature changes. The alert is no longer relevant but nobody deletes it. Six months later, you have 80 alerts, 30 of which monitor features that no longer exist or metrics that no longer matter. Alert hygiene is the practice of regularly reviewing, updating, and deleting alerts to keep your monitoring relevant.

Quarterly alert review is the minimum cadence. Once every three months, the on-call team meets to review every alert. For each alert: has it fired in the past quarter? If yes, what was the true positive rate? If no, is it still relevant? Should thresholds be adjusted? Should the alert be demoted, escalated, or deleted? This review takes 1-2 hours for a team with 50 alerts. It prevents alert sprawl and ensures that every alert still serves a purpose.

True positive rate is the key metric. An alert that fired 10 times with 9 false positives and 1 true positive is broken. Either fix the detection logic or delete the alert. An alert that fired 10 times with 8 true positives is valuable. Keep it. An alert that didn't fire at all might be correct — the system was healthy — or it might be misconfigured and would never fire even during an incident. Test it. Inject a failure condition and verify the alert triggers. If it doesn't, fix or delete it.

Alert documentation must stay current. Every alert should have a short description: what it detects, why it matters, and who to contact. When you update an alert's threshold or logic, update the description. When you delete an alert, document why. This prevents the alert from being re-added later by someone who doesn't remember the reason for deletion. Alerts without documentation are often misconfigured because nobody understands their purpose well enough to maintain them.

Ownership assignment prevents orphaned alerts. Every alert should have an owner — a team or individual responsible for maintaining it. When the owner changes teams or leaves the company, reassign ownership. Alerts without owners become technical debt. Nobody adjusts their thresholds. Nobody updates their runbooks. Nobody deletes them when they're no longer needed. Over time, the monitoring system fills with orphaned alerts that fire occasionally, waste investigation time, and undermine confidence in monitoring.

## On-Call Burden and Sustainable Alerting Loads

On-call is exhausting. Even if alerts don't fire, the expectation of potential alerts disrupts sleep and prevents full disconnection from work. When alerts do fire, especially overnight, they fragment sleep and impair cognitive function. Sustainable on-call requires limiting alert volume to levels that humans can handle without burning out. The targets: no more than 2 pages per week during on-call shifts, no more than 1 overnight page per month per engineer.

Measure on-call burden explicitly. Track pages per on-call shift. Track overnight page frequency. Track time to resolution for paged incidents. If your team averages 5 pages per week per on-call shift, your alerting is too sensitive or your system is too unstable. Either tighten thresholds, invest in reliability improvements, or expand the on-call rotation to distribute load. Ignoring the data leads to burnout. Engineers leave. Knowledge walks out the door. Hiring replacements takes months. The reliability burden increases on remaining engineers, creating a death spiral.

Overnight pages are the most damaging. A single overnight page costs an engineer a full night's sleep. Two overnight pages in one week is unsustainable. Three means someone is heading toward burnout or resignation. If your system generates more than 4 overnight pages per month, you have a crisis. Either your P1 thresholds are too sensitive or your system reliability is unacceptable. Both require immediate attention. Temporary fixes: tighten P1 thresholds, queue P2 alerts overnight instead of paging. Permanent fixes: improve system reliability, implement auto-remediation, expand on-call rotation.

Rotation size matters. A team of 3 engineers doing on-call means each engineer is on call every third week. That's roughly 17 weeks per year — a third of their working life spent in an on-call state. This is too high. Aim for rotations of 6-8 engineers minimum. Each engineer is on call 6-8 weeks per year, roughly 12-15% of their time. This is sustainable. If your team is too small for this rotation size, either hire more engineers or reduce your on-call scope. Have fewer alerts. Accept higher risk. Acknowledge that not every issue requires immediate response.

Alert budgets enforce sustainability. Each team gets a monthly alert budget — say, 30 pages per engineer per month across the entire rotation. If the budget is exceeded, the team must either reduce alert volume or expand the rotation. This forces intentional tradeoffs. You can't add a new P1 alert without removing an existing one or improving system reliability to reduce existing alert frequency. The budget makes explicit what's usually implicit: alerting has a human cost, and that cost must be managed as rigorously as infrastructure cost.

Alert design determines whether your reliability infrastructure saves you from incidents or drowns you in noise. High sensitivity catches incidents early but risks fatigue. High specificity avoids fatigue but risks delayed detection. Tiered alerting routes severe issues to immediate response and minor issues to deferred investigation. Runbooks turn vague alerts into actionable guidance. Regular hygiene prevents alert sprawl. Sustainable on-call loads prevent burnout. None of these practices are optional. Together, they form the operational discipline that makes monitoring effective rather than decorative. The teams that invest in alert design detect incidents faster, respond more effectively, and maintain their engineering capacity over years rather than burning out in months. Next, we'll tackle the correlation problem — when five systems fail simultaneously, how do you determine which failure caused the others and where to focus remediation efforts?

