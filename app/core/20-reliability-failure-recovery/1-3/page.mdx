# 1.3 — The Taxonomy of AI Failure Modes

A fintech startup's AI financial advisor went down during market hours in September 2025. The postmortem identified seventeen distinct failure modes occurring simultaneously. The primary API provider rate-limited the account due to a billing issue — infrastructure failure. The fallback model started hallucinating SEC filing data because it was not fine-tuned for financial domain knowledge — quality failure. The safety classifier flagged legitimate investment advice as potentially harmful — safety false positive. The response latency spiked to 8 seconds because the retry logic was creating cascading requests — latency failure. Users started reporting inconsistent answers to identical questions — consistency failure. The estimated cost per query tripled due to the retry logic hitting expensive backup models — cost failure. And when Engineering finally killed the service to stop the bleeding, they discovered that the monitoring dashboard had been reporting green status the entire time because it only tracked infrastructure health, not output quality.

AI systems fail in categorically different ways than traditional software. A taxonomy is required. Name the failure modes. Classify them by cause, detection difficulty, and consequence severity. Prepare for them systematically rather than discovering them in production.

## Infrastructure Failures: When the System Cannot Respond

Infrastructure failures are the most familiar failure mode for traditional software engineers. These are failures of the machinery, not the intelligence. The model API returns a 500 error. The request times out after 30 seconds. The rate limit is exceeded and further requests are rejected. The authentication token is invalid. The network connection is dropped. These failures are loud. The system explicitly signals that it cannot complete the request.

Infrastructure failures are also the easiest to detect. Traditional monitoring catches them. Error rates spike. Latency crosses thresholds. Uptime drops. Alerts fire. Engineers respond. For AI systems, infrastructure failures are a solved problem from a detection standpoint. The challenge is mitigation: how do you gracefully degrade when the primary model is unavailable? Do you fail over to a backup model? Do you return a cached response? Do you queue the request for retry? Do you fail closed and tell the user the service is temporarily unavailable?

The mitigation strategy depends on the use case. A customer service chatbot might fail over to a smaller, cheaper model that handles common questions reasonably well. A medical diagnosis assistant might fail closed because serving a degraded response is worse than serving no response. A creative writing tool might queue requests and process them asynchronously. The failure mode is well-understood. The mitigation is a product decision, not a technical mystery.

The danger with infrastructure failures is assuming they are the only failures that matter. Teams that invest heavily in uptime, redundancy, and failover sometimes neglect quality monitoring because they assume "if the API is up, the system is working." This assumption is wrong. Infrastructure health is necessary but not sufficient for AI system reliability.

## Quality Failures: When the Response Is Wrong

Quality failures are when the model responds, but the response is incorrect, off-topic, nonsensical, or otherwise fails to meet the standard required for the use case. The API returns HTTP 200. The response is well-formatted. The model followed the prompt structure. But the content is wrong.

Hallucinations are the most common quality failure. The model generates information that sounds plausible but is factually incorrect. A legal assistant cites a case that does not exist. A medical bot describes a drug interaction that is not supported by clinical evidence. A customer service agent invents a return policy. These are not refusals. These are confident, fluent, wrong answers. The model does not know it hallucinated. It cannot signal the failure. Detection requires external validation: checking citations against databases, comparing medical claims against guidelines, auditing policy statements against documentation.

Off-topic responses are quality failures where the model ignores the user's intent and generates content unrelated to the request. A user asks for Python code, the model returns a recipe. A user asks for a summary, the model generates creative fiction. A user asks a factual question, the model responds with philosophical musing. These failures are easier to detect than hallucinations because relevance is measurable with semantic similarity or classifier-based evaluation. But they are still silent. The API does not return an error. The response is fluent. It is just useless.

Tone and style violations are quality failures where the content is correct but the presentation is wrong. A professional legal assistant responds in casual slang. A friendly customer service bot uses cold, bureaucratic language. A creative writing tool generates formulaic, repetitive prose. These failures matter more for some use cases than others. A code generation tool can be robotic. A mental health support chatbot cannot. The failure is context-dependent.

Quality failures are harder to detect than infrastructure failures because detection requires evaluating content, not just checking status codes. This requires either human review, automated evaluation models, or structured checks like citation verification. All three are expensive. All three are necessary for reliable AI systems. Without continuous quality evaluation, you will not know when the model is producing garbage until users tell you.

## Safety Failures: When the Response Is Harmful

Safety failures are when the model generates content that violates policies, causes harm, or exposes the system to liability. These are the highest-stakes failures. A chatbot that provides instructions for illegal activity. A content moderation system that allows violent threats through. A customer service agent that leaks PII. A medical bot that gives dangerous health advice. A hiring assistant that makes discriminatory statements.

Safety failures are different from quality failures because the consequence is not just a bad user experience. The consequence is harm: legal liability, regulatory action, reputational damage, physical danger. A hallucinated customer service policy costs money to remediate. A leaked social security number creates legal exposure and harms an individual. The severity is categorically different.

Safety failures are also the hardest to define. What counts as harmful depends on jurisdiction, cultural context, and use case. A political discussion bot that expresses opinions might be acceptable in one market and illegal in another. A creative writing tool that generates violent fiction might be appropriate for adults and unacceptable for children. A medical assistant that discusses end-of-life care might be providing valuable information or might be triggering psychological harm depending on the user's state. Safety is not a universal standard. It is a set of context-specific boundaries.

This makes safety evaluation uniquely difficult. You cannot just measure accuracy. You must define a policy, operationalize that policy into detectable violations, and build classifiers or review processes that catch violations before they reach users. And because adversarial actors are actively probing for ways to bypass safety measures, the evaluation must be continuous and adversarial. Static safety checks become obsolete as users learn to jailbreak the system.

Organizations that ship AI products without a dedicated Trust and Safety function are operating with unmitigated safety risk. Safety is not a launch checklist item. It is an ongoing operational discipline that requires people, process, and tooling. The cost of underinvesting in safety is measured in lawsuits, regulatory fines, and destroyed trust.

## Capability Failures: When the Model Cannot Do the Task

Capability failures are when the model attempts a task it fundamentally cannot do but does not refuse. Instead, it produces a response that appears to answer the question but actually does not. A math reasoning model that generates plausible-looking steps but arrives at the wrong answer. A translation model that produces fluent output in the target language but mistranslates nuanced meaning. A code generation model that writes syntactically valid code that does not solve the problem.

These failures are insidious because the model does not signal uncertainty. It generates tokens confidently. The response is well-formatted. It reads like success. But the underlying capability is missing. The model is approximating a task it cannot truly perform, and the approximation is good enough to fool casual review but not good enough to be correct.

Capability failures are particularly dangerous when users are not experts in the domain. A user who asks a legal question and gets a confident but incorrect answer has no way to know the answer is wrong unless they have legal expertise. A user who asks a medical question and gets a fluent but inaccurate response might act on that information. The model's confidence masks its incapability.

Detection requires task-specific evaluation. For math reasoning, check the final answer against ground truth. For translation, compare against reference translations or use back-translation to detect meaning drift. For code generation, run test cases. For factual questions, verify claims against authoritative sources. The evaluation must test actual capability, not just fluency.

The mitigation for capability failures is scope restriction. If the model cannot reliably perform a task, do not allow it to attempt that task. Build a refusal mechanism that triggers when the request is outside the model's capability range. This requires defining the capability boundary and enforcing it, which is hard because models do not self-report their limits accurately. But serving a refusal is better than serving a confidently wrong answer.

## Consistency Failures: When the Same Input Gives Different Outputs

Consistency failures are when the model produces different outputs for the same input on repeated trials. A user asks the same question twice and gets two substantively different answers. This happens because of temperature sampling, because of stochastic attention mechanisms, because of infrastructure-level load balancing across different model replicas with slightly different states.

Some level of inconsistency is inherent to generative models. If temperature is set above zero, the model is deliberately introducing randomness to avoid repetitive outputs. If you want consistency, you set temperature to zero or use deterministic sampling. But even at temperature zero, some models exhibit variation due to floating-point precision differences, hardware-level randomness, or subtle differences in how prompts are processed.

Consistency failures matter most for use cases where users expect deterministic behavior. A tax calculation tool that gives different answers for the same input is broken, regardless of how good the individual answers are. A legal research assistant that summarizes the same case differently on repeated trials confuses users. A content moderation system that allows the same post through on one trial and blocks it on the next is operationally incoherent.

Detection is straightforward: sample identical inputs, run them through the model multiple times, compare outputs. Measure consistency as the percentage of repeated trials that produce equivalent outputs. Define "equivalent" based on the use case: exact string match, semantic similarity above threshold, same numerical result, same categorical decision.

Mitigation depends on whether the inconsistency is acceptable. For creative tasks, inconsistency is often desirable. For factual tasks, it is a defect. You can reduce inconsistency by lowering temperature, using deterministic sampling, caching responses keyed by input, or using ensembles that vote across multiple trials. The choice depends on the product requirements.

## Latency Failures: When the Response Is Correct but Too Slow

Latency failures are when the model produces a correct response, but the response takes so long to generate that the user experience is degraded. A customer service chatbot that takes 12 seconds to respond is functionally useless even if the answer is perfect. A code completion tool that takes 5 seconds to suggest the next line is slower than just typing. A voice assistant that pauses for 3 seconds before responding feels broken.

Latency thresholds are use-case dependent. A research assistant that takes 30 seconds to generate a comprehensive literature review might be acceptable. A real-time translation tool that takes 30 seconds is not. A creative writing tool that takes 10 seconds to generate a paragraph might be fine. A chatbot that takes 10 seconds to greet a user is not.

Latency failures are usually infrastructure-related: model size is too large, batch size is too small, hardware is insufficient, network round-trip is too high. But they can also be architecture-related: prompts are too long, retrieval step is too slow, multi-step reasoning requires too many serial calls. The root cause determines the mitigation.

Detection is straightforward: measure end-to-end latency, alert when it crosses threshold. The challenge is defining the threshold and understanding what percentile to measure. Median latency is often acceptable even when p99 latency is catastrophic. If 99% of requests complete in 400 milliseconds and 1% take 8 seconds, the median is fine but the user experience for the tail is broken. High-reliability systems measure p95, p99, and p99.9 latency and set thresholds for each.

Mitigation strategies include model distillation, quantization, better hardware, caching, speculative decoding, streaming responses, and architectural changes to parallelize steps that are currently serial. The choice depends on the root cause and the available budget.

## Cost Failures: When the Response Is Correct but Too Expensive

Cost failures are when the model produces a correct response at a price that makes the product economically unviable. A customer service chatbot that costs $2.40 per conversation when the customer lifetime value is $15. A code generation tool that costs $0.80 per suggestion when users generate 200 suggestions per session. A summarization service that costs $0.12 per document when the market rate for the service is $0.05 per document.

Cost failures do not degrade user experience directly. The responses are good. The latency is acceptable. But the unit economics do not work. The product loses money on every interaction. This is unsustainable. Either the cost must be reduced or the product must be repriced or shut down.

Cost failures are common in AI products because teams optimize for quality and latency during development, then discover in production that the model they chose is too expensive at scale. A GPT-5.2 that performs beautifully in testing becomes a budget crisis when handling 10 million requests per month. The team has two options: switch to a cheaper model and accept quality regression, or find architectural efficiencies that reduce cost without sacrificing quality.

Detection requires cost tracking at the per-request level. Every API call has a token cost. Every fine-tuned model inference has a hosting cost. Every retrieval step has a compute and storage cost. Aggregate these costs and divide by volume to get cost per interaction. Compare against the revenue or value per interaction. If cost exceeds value, the product is underwater.

Mitigation strategies include model downgrading, prompt compression, caching, routing requests to cheaper models when quality requirements are lower, batch processing where latency allows, and quantization. The most effective mitigation is often architectural: redesigning the system to use fewer model calls, using smaller models for subtasks, or offloading deterministic logic to code instead of relying on the model.

## Cascade Failures: When One Component Failure Causes System-Wide Degradation

Cascade failures are when a failure in one component propagates through the system and causes failures in other components. The retrieval step times out, so the model generates a response without context, so the response is off-topic, so the user rephrases the question, so the system processes twice as many requests, so the API rate limit is exceeded, so all requests start failing. One timeout becomes a system-wide outage.

Cascade failures are particularly dangerous for multi-component AI systems. A typical RAG pipeline has a retrieval component, a reranking component, a prompt construction component, and a generation component. If any one of these fails, the downstream components either fail or produce degraded output. If the system does not have circuit breakers, retries can amplify the problem. A slow retrieval component causes generation requests to queue up. The queue grows. Timeouts increase. Retries fire. The load multiplies. The system collapses.

Detection requires observability across the entire pipeline. You cannot just monitor the final output. You must monitor each component: retrieval latency, reranking accuracy, prompt token count, generation latency, error rates at each step. When one component degrades, you need to see it before it cascades.

Mitigation requires defensive design. Circuit breakers that stop calling a failing component after a threshold of errors. Timeouts that prevent slow components from blocking downstream work. Fallback logic that degrades gracefully: if retrieval fails, generate without context; if the primary model is unavailable, use a backup; if reranking times out, skip it. The system should degrade incrementally, not collapse entirely.

Cascade failures are the reason why AI system reliability requires end-to-end thinking. A model that is 99% reliable in isolation can become 90% reliable in a system where five components must all succeed for a good outcome. Each component's failure rate multiplies. Reliability engineering for AI is systems engineering, not just model engineering.

## Using the Taxonomy to Prepare for Failure

This taxonomy is not theoretical. Every failure mode listed here occurs in production systems every day. Some occur in every system. Some occur only under specific conditions. Some are rare but catastrophic. The taxonomy is a checklist: which of these failure modes have you designed for? Which have you ignored?

For each failure mode, ask three questions. First, how would you detect this failure in production? Do you have monitoring for it? Do you have alerting thresholds? Would you learn about it from a dashboard or from a user complaint? Second, what is the consequence of this failure? Is it a bad user experience, a cost overrun, a safety incident, a legal liability? Third, what is your mitigation strategy? Do you fail closed, degrade gracefully, retry, fail over, cache, queue, or alert a human?

Organizations that systematically answer these questions for every failure mode in the taxonomy ship more reliable AI systems. Organizations that ignore the taxonomy discover failure modes in production and respond reactively. The choice is whether you prepare for failure or experience it unprepared. The next step is building an operational model that makes reliability a continuous discipline, not a launch checklist.

---

Next: **1.4 — The AI Reliability Stack Framework**
