# 12.4 — Retrieval Quality Decay Over Time

A legal AI product launched in March 2024 with 92% precision on contract clause retrieval. By September 2025, precision had dropped to 71%. The knowledge base had grown from 12,000 documents to 89,000. The team hadn't changed the retrieval pipeline. They hadn't modified the embedding model. They had simply kept adding documents — and every new document made retrieval worse.

The team discovered the problem only when a customer complained that the system was surfacing irrelevant precedents. By then, the damage was widespread. Investigation revealed that 40% of the knowledge base consisted of outdated templates, superseded policies, and archived contracts that should never have been in production retrieval. The index had become polluted. Retrieval quality had degraded silently for 18 months.

Retrieval-augmented generation systems do not maintain constant quality. They age. As knowledge bases grow, as content becomes outdated, as user queries evolve — retrieval precision degrades. This is not a failure of the retrieval algorithm. It is a structural consequence of how RAG systems accumulate information over time.

## Why Retrieval Degrades Over Time

Retrieval quality decay happens through four mechanisms. First, index bloat reduces precision. As knowledge bases grow, the retrieval system has more documents to choose from. More documents mean more noise. A query that once retrieved five highly relevant chunks now retrieves five relevant chunks and fifteen marginal ones. The signal-to-noise ratio declines.

Second, content aging erodes relevance. Documents written in 2023 reflect 2023 policies, 2023 product capabilities, and 2023 organizational structure. By 2026, those documents are historical artifacts. But if they remain in the knowledge base, the retrieval system surfaces them. Users ask about current policy. The system retrieves outdated policy. The model synthesizes an answer based on obsolete information.

Third, query pattern evolution creates mismatches. When the RAG system launched, users asked a certain type of question. Over time, their questions change. They develop more sophisticated queries. They ask about edge cases the original knowledge base never covered. They use new terminology. The retrieval system was optimized for the query distribution at launch. It performs poorly on the query distribution two years later.

Fourth, embedding model drift introduces instability. When you upgrade the embedding model — from text-embedding-ada-002 to text-embedding-3-large, from an older Sentence-BERT model to a newer one — the semantic space changes. Documents embedded with the old model may not align well with queries embedded with the new model. Retrieval quality can drop immediately after a model upgrade if embeddings are not regenerated.

## Content Aging and Relevance Decay

Content aging is insidious because it happens gradually. A policy document written in January 2024 is accurate in January 2024. In July 2024, the policy changes slightly. The new version is added to the knowledge base, but the old version remains. Now the knowledge base contains two versions of the same policy. Retrieval sometimes surfaces the old version. The model synthesizes answers that blend old and new policy. Users receive inconsistent information.

By January 2026, the knowledge base contains four versions of that policy across three years. Only one is current. The retrieval system has no way to know which version is authoritative unless you explicitly encode temporal metadata and build freshness scoring into the retrieval ranking.

Content aging affects different document types differently. Procedural documentation — how to file an expense report, how to request time off — ages rapidly. Organizational policies change every quarter. Reference documentation for APIs and products ages with every release. Conceptual documentation — what is GDPR, what is double-entry bookkeeping — ages slowly. Historical case studies and precedents may never age at all.

If your retrieval system treats all documents as equally fresh, it will surface outdated procedural documentation at the same rate as evergreen reference material. This is why many production RAG systems implement freshness scoring: recent documents receive a ranking boost, older documents are penalized. But freshness scoring only helps if you also implement document expiration policies that remove truly obsolete content.

## Index Bloat and Precision Loss

Index bloat is the silent killer of retrieval precision. Every document you add to the knowledge base increases the probability that retrieval will surface something marginally relevant instead of highly relevant. The problem compounds as the knowledge base scales.

A fintech RAG system started with 5,000 documents in early 2024. Retrieval precision at launch was 89%. By mid-2025, the knowledge base contained 42,000 documents. Precision had dropped to 68%. Analysis showed that 60% of the documents added after launch were duplicate content — the same policy rewritten for different audiences, the same FAQ answered in three different help articles, the same product description duplicated across marketing, sales, and support documentation.

The retrieval system was not broken. It was retrieving relevant documents. The problem was that "relevant" now included fifteen variations of the same information, and the ranking algorithm could not distinguish canonical content from derivative content. Users saw repetitive results. The model generated verbose, redundant answers because every retrieved chunk repeated the same point in slightly different words.

Index bloat also increases retrieval latency. Searching 5,000 documents is fast. Searching 50,000 documents is slower. Approximate nearest neighbor search algorithms scale well, but they are not immune to size. At some point, retrieval latency crosses the 200ms threshold that degrades user experience. The team then faces a choice: accept slower retrieval or reduce the knowledge base size through pruning.

## Query Pattern Evolution

Query patterns evolve because users evolve. When a product launches, users ask basic questions. As they become more sophisticated, their questions become more complex. A customer support RAG system initially received queries like "how do I reset my password?" Within six months, queries evolved to "why does the password reset email sometimes go to spam and how do I whitelist your domain in Outlook 365?"

The knowledge base was designed for the initial query distribution. It contains a straightforward article on password resets. It does not contain detailed documentation on email deliverability and domain whitelisting. Retrieval fails. The system cannot retrieve information that does not exist. The user receives a generic, unhelpful response.

Query evolution also happens through terminology drift. When the product launched, users asked about "reports." Over time, they started asking about "dashboards" and "analytics." The knowledge base uses the term "reports" consistently. The retrieval system struggles to match "dashboard" queries to "report" documentation. Semantic embedding models help, but they are not perfect. If terminology shifts significantly, retrieval precision degrades.

Some teams address query evolution through continuous knowledge base expansion. Every new query type that fails retrieval becomes a signal to add documentation. This works, but it requires a feedback loop from retrieval failures to documentation teams. Most organizations lack this loop. Retrieval degradation is noticed only when users complain loudly enough.

## Detecting Retrieval Decay

Retrieval decay is difficult to detect because it happens gradually. Precision drops from 92% to 89% to 85% over months. No single day shows a dramatic failure. But over a year, the system has become unreliable.

Detection requires continuous measurement. The most direct method is maintaining a golden eval set of query-document pairs. Run retrieval against this eval set weekly. Track precision at K — what percentage of the top K retrieved documents are actually relevant. If precision at 5 drops below a threshold, investigate.

The golden eval set must evolve with query patterns. If your eval set contains only the queries users asked at launch, it will not detect degradation on the queries users ask today. Refresh the eval set quarterly with real production queries sampled from recent traffic.

Another detection method is monitoring retrieval confidence scores. Most retrieval systems produce a similarity score for each retrieved document. Track the distribution of these scores over time. If the average score for top-ranked documents declines, it suggests that retrieval is finding fewer highly relevant matches. This could indicate index bloat, content aging, or query pattern drift.

User feedback provides a lagging but critical signal. Track thumbs-down rates, "this was not helpful" signals, and support escalations. If users increasingly report that the AI provided incorrect or outdated information, retrieval decay is a likely cause. Investigate retrieval logs for those specific queries to confirm.

## Measuring Retrieval Quality Trends

Measuring retrieval quality trends requires establishing baselines at launch and tracking deviations. The key metrics are precision at K, recall at K, mean reciprocal rank, and normalized discounted cumulative gain. These metrics tell you whether the retrieval system is surfacing the right documents in the right order.

Precision at K measures the fraction of retrieved documents that are relevant. If you retrieve 10 documents and 7 are relevant, precision at 10 is 70%. Track this weekly against your golden eval set. A declining trend indicates degradation.

Recall at K measures the fraction of all relevant documents that were retrieved. If there are 15 relevant documents in the knowledge base and your system retrieves 7 of them in the top 10, recall at 10 is 47%. Recall is harder to measure in production because it requires knowing all relevant documents for every query — which is only feasible for curated eval sets.

Mean reciprocal rank measures how quickly retrieval surfaces the first relevant document. If the first relevant document is ranked third, the reciprocal rank is one-third. Average this across all queries in your eval set. A declining MRR indicates that retrieval is pushing relevant documents lower in the ranking.

Normalized discounted cumulative gain accounts for both relevance and position. Documents ranked higher contribute more to the score. This is closer to how users actually experience retrieval — the first result matters more than the tenth. Track NDCG over time. A drop indicates retrieval quality decay.

## Refresh and Pruning Strategies

Retrieval quality decay is inevitable. The solution is not to prevent it but to manage it through refresh and pruning. Refresh means updating documents that have become outdated. Pruning means removing documents that are no longer relevant.

Refresh strategies depend on document lifecycle. For procedural documentation, implement scheduled reviews. Every quarter, audit all policy and process documents. Mark outdated versions as deprecated. Re-embed updated versions. For product documentation, tie refresh to release cycles. Every product release should trigger a documentation refresh that updates API references, feature descriptions, and troubleshooting guides.

For less structured content — blog posts, case studies, general knowledge articles — implement freshness metadata. Tag each document with a creation date and a review date. Surface this metadata in retrieval ranking. Penalize documents that have not been reviewed in 18 months. This encourages periodic review and prevents ancient content from dominating retrieval.

Pruning requires clear removal policies. Not every document belongs in production retrieval forever. Define expiration rules. Marketing content for a product that no longer exists should be removed. Support articles for a deprecated feature should be archived. Temporary announcements — office closures, scheduled maintenance — should be automatically removed after the event date.

Some organizations implement versioned knowledge bases. Only the current version is used for retrieval. Previous versions are archived but available for audit purposes. This prevents retrieval from surfacing outdated content while preserving historical records. The trade-off is increased operational complexity — someone must manage the versioning process.

## Retrieval Quality SLOs

Retrieval quality should have service level objectives just like latency and availability. Define a minimum acceptable precision at K. For example, precision at 5 must remain above 85%. If it drops below 85% for two consecutive weeks, trigger an incident review.

Define a maximum acceptable age for unreviewed documents. For example, no document in the production knowledge base should be more than 24 months old without a review. Implement automated monitoring that flags documents approaching the age threshold. Assign ownership for review and refresh.

Define a maximum knowledge base size relative to core content. For example, if your core knowledge base is 10,000 documents, the total knowledge base should not exceed 30,000 documents without explicit justification. This prevents unbounded growth that leads to index bloat.

Track retrieval quality as a dashboard metric alongside latency, error rate, and user satisfaction. Make retrieval degradation visible. When leadership sees a declining trend, they will allocate resources to refresh and pruning. When retrieval quality is invisible, it decays silently until a major incident forces attention.

Retrieval quality decay is not a one-time problem you solve and forget. It is an ongoing operational concern. RAG systems require continuous maintenance. The knowledge base must be curated, refreshed, and pruned. Retrieval metrics must be monitored. Query patterns must be analyzed. This is the maintenance tax for production RAG. Pay it consistently, or retrieval quality will collapse.

---

Next, we examine how prompts themselves degrade — the phenomenon of prompt entropy and template brittleness, where instructions that worked at launch fail silently months later.
