# 8.12 — Incident Metrics and Trends: Measuring Reliability Improvement

In January 2026, a logistics company tracked three reliability metrics: incident count, mean time to resolution, and mean time to detection. They had 14 incidents that month. MTTR was 47 minutes. MTTD was 18 minutes. In isolation, these numbers meant nothing. Was 14 incidents good or bad? Was 47-minute resolution fast or slow? The engineering VP asked: "Are we getting better?" No one could answer. They had data but no baseline, no trend, no comparison.

In February, they tracked the same metrics and added one more: recurring incident rate. February had 11 incidents. MTTR dropped to 41 minutes. MTTD dropped to 14 minutes. But three of the eleven incidents were recurrences of January incidents. Now they could answer the VP's question: "Incident count is down. Resolution is faster. Detection is faster. But we're not fixing root causes — recurring incidents are up." That was actionable. They shifted focus from fast resolution to deep fixes. By April, recurring incident rate dropped from 27% to 11%. Total incident count dropped to 7. The trend told a story the snapshot couldn't.

Incident metrics matter only when tracked over time. A single month's MTTR tells you how long incidents took to resolve. Six months of MTTR tells you whether your incident response is improving, stagnating, or degrading. A single quarter's incident count tells you how many failures happened. Four quarters of incident count tell you whether your reliability investment is working. The teams that improve reliability fastest are the ones that track a small set of metrics consistently, visualize trends, and adjust their work based on what the trend reveals. The teams that track nothing or track everything equally end up with dashboards that no one trusts and improvements that no one can measure.

## Key Incident Metrics

The foundational metric is incident count: how many incidents occurred in a given time period. You segment by severity: SEV1, SEV2, SEV3. SEV1 incidents are user-facing outages or data incidents. SEV2 incidents are degraded performance or partial failures. SEV3 incidents are internal issues with no user impact. If your total incident count is flat but SEV1 count is dropping, you're improving where it matters. If total count is dropping but SEV1 count is rising, you're getting worse where it matters.

The second metric is mean time to detection: how long between when the failure started and when the team noticed it. MTTD measures monitoring effectiveness. If MTTD is high, your alerts aren't firing or your team isn't looking at them. If MTTD is dropping, your monitoring is improving. For AI systems, MTTD often correlates with silent degradation risk. A model's accuracy drops gradually. If MTTD is hours or days, you're detecting the problem after users have already been affected. If MTTD is minutes, you're catching degradation early.

The third metric is mean time to resolution: how long from detection to full mitigation. MTTR measures response effectiveness. If MTTR is high, your runbooks are bad, your systems are hard to debug, or your team lacks the expertise to mitigate quickly. If MTTR is dropping, your incident response is improving. MTTR breaks into two sub-metrics: time to mitigation (when did you stop the bleeding) and time to resolution (when did you fully restore service). For user-facing incidents, time to mitigation is the metric that matters. For internal incidents, time to resolution matters.

The fourth metric is recurring incident rate: what percentage of incidents are recurrences of previous incidents. If recurring rate is high, you're not fixing root causes. If recurring rate is low, your post-mortem action items are working. Recurring rate is the most important metric for long-term reliability. You can have low incident count and fast MTTR, but if 40% of your incidents are recurrences, you're not learning. You're just getting faster at putting out the same fires.

## AI-Specific Reliability Metrics

AI systems need metrics that traditional software doesn't. The first is model performance degradation incidents: how many incidents were caused by the model's accuracy, latency, or output quality degrading. If this metric is rising, your model monitoring isn't catching drift early enough, or your models are fragile to distribution shift. If it's dropping, your model governance and retraining pipelines are working.

The second is data-related incidents: how many incidents were caused by bad input data, corrupted training data, or data pipeline failures. If this metric is high, your data quality gates are insufficient. If it's dropping, your data validation is improving. For some AI systems, data-related incidents are 50% of total incidents. If you're not tracking this separately, you're blind to your biggest reliability risk.

The third is dependency-related incidents: how many incidents were caused by upstream service failures, API changes, or external model updates. If this metric is high, your system is tightly coupled to dependencies you don't control. If it's dropping, you've added circuit breakers, fallbacks, or redundancy. AI systems often depend on external APIs for embeddings, moderation, or data enrichment. Every dependency is a failure vector. If you're not measuring dependency-related incidents, you can't quantify the risk.

The fourth is false positive alert rate: what percentage of alerts turned out to be non-incidents. If false positive rate is high, your team starts ignoring alerts. If it's dropping, your alerting is getting more precise. For AI systems, false positives are common because statistical thresholds trigger on noise. If your accuracy alert fires every time accuracy dips 0.5% but only 10% of those dips turn into actual incidents, your alert is 90% false positives. You either raise the threshold or add more context to the alert.

## Trend Analysis and Reporting

A trend is a metric tracked consistently over at least three months, preferably six. You plot the metric on a time series chart. You look for patterns: improving, stable, degrading, or volatile. An improving trend means your reliability work is paying off. A stable trend means you're holding the line. A degrading trend means something changed — system complexity increased, team turnover, or technical debt accumulated. A volatile trend means your system's reliability is unpredictable, which is worse than consistently bad because you can't plan for it.

Trend analysis answers three questions. First: Are we getting better or worse? If MTTR increased by 20% over six months, you're getting worse at incident response. That's a signal to invest in runbooks, training, or tooling. Second: What changed? If incident count spiked in a specific month, you correlate that spike with changes: a new model deployment, a new feature launch, a dependency update. The correlation doesn't prove causation, but it gives you a hypothesis. Third: Is the improvement sustainable? If MTTR dropped 40% in one quarter but then plateaued, you hit diminishing returns. Further improvement requires different investment.

You report trends monthly to the engineering team and quarterly to leadership. The monthly report is tactical: "MTTR dropped from 45 minutes to 38 minutes because we updated three critical runbooks." The quarterly report is strategic: "Incident count decreased 30% year-over-year. Recurring incident rate dropped from 25% to 12%. Model performance degradation incidents increased 15%, indicating we need better drift detection." The quarterly report includes the trend chart and the narrative. The chart shows the data. The narrative explains what it means and what you're doing about it.

## Benchmarking Against Industry

External benchmarks give you context for your metrics. If your MTTR is 45 minutes, is that good? It depends. For a consumer app, 45 minutes might be slow. For a healthcare AI system with complex dependencies, 45 minutes might be fast. Industry benchmarks — published by SRE survey reports, cloud providers, or analyst firms — give you a reference point. If the industry median MTTR for SaaS companies is 60 minutes and yours is 45, you're above average. If the industry median is 20 minutes, you're behind.

The challenge with AI-specific benchmarks is that they barely exist as of 2026. Traditional SRE metrics like MTTR and uptime have decades of benchmarking data. AI reliability metrics like model degradation incident rate or data-related incident percentage don't. The industry is too young. The benchmarks will emerge over the next few years, but in 2026, most AI teams are benchmarking against themselves — comparing this quarter to last quarter — or against peer companies in their network.

When external benchmarks don't exist, you create internal targets. You decide: "We want MTTR below 30 minutes for SEV1 incidents." That becomes your target. You track progress toward it. If you're at 45 minutes today and you get to 32 minutes in six months, you're making progress even if you haven't hit the target. The target is aspirational. The trend is what matters. A team that improves MTTR by 30% year-over-year will eventually hit 30 minutes, even if they're at 50 minutes today.

## Leading vs Lagging Indicators

Incident count, MTTR, and MTTD are lagging indicators. They tell you what already happened. They're essential for understanding reliability history, but they don't predict future reliability. Leading indicators predict whether your reliability will improve or degrade before incidents happen. The first leading indicator is postmortem action item completion rate: what percentage of action items from postmortems get completed within their target date. If completion rate is high, you're fixing root causes. If it's low, action items are getting deprioritized, and recurring incidents will rise.

The second leading indicator is test coverage for reliability scenarios: how many failure modes have automated tests or chaos experiments covering them. If you add tests for database timeout, pod failure, and upstream API degradation, you reduce the risk of those failures causing incidents. If test coverage is increasing, your proactive reliability work is increasing. If it's flat or decreasing, you're reactive, waiting for incidents to tell you what to fix.

The third leading indicator is runbook freshness: what percentage of runbooks have been reviewed or updated in the past six months. If runbook freshness is high, your documentation stays current. If it's low, runbooks decay, and MTTR will increase over time as on-call engineers follow outdated steps. Runbook freshness predicts MTTR six months out. A team with 90% runbook freshness today will have lower MTTR in six months than a team with 40% freshness, assuming all else is equal.

The fourth leading indicator is team training hours: how much time on-call engineers spend on incident response training, red-team exercises, or runbook drills. If training hours are increasing, MTTR will decrease. If training hours are zero, MTTR will increase as engineers forget procedures or new engineers join without training. Training is the leading indicator for incident response capability.

## Reporting to Leadership

Leadership cares about two things: Are we reliable, and are we getting more reliable? The first question is answered with availability metrics: uptime percentage, error rate, SEV1 incident count. The second question is answered with trend metrics: MTTR over time, incident count over time, recurring incident rate over time. The report you give leadership should fit on one page. Three to five metrics, each with a trend line and a one-sentence explanation.

Example report: "January 2026 Reliability Summary. Availability: 99.91% (up from 99.87% in December). SEV1 incident count: 2 (down from 4 in December). MTTR: 38 minutes (down from 47 minutes in December). Recurring incident rate: 15% (down from 27% in December). Key driver: Completed runbook updates from Q4 postmortems." That's it. No jargon, no deep technical detail, no excuses. Just the numbers, the trend, and the explanation.

If a metric is degrading, you explain why and what you're doing about it. "MTTR increased to 52 minutes in February (up from 38 in January). Root cause: Two incidents involved new infrastructure we don't have runbooks for yet. Fix: We're writing runbooks for the new infrastructure, targeted completion March 15." Leadership doesn't need to know every detail. They need to know you see the problem and you have a plan.

The monthly report is factual. The quarterly report is strategic. In the quarterly report, you add context: "Incident count decreased 30% year-over-year. This is the result of three investments: standardized pre-deployment testing (reduced deployment-related incidents by 50%), automated rollback on model degradation (reduced model performance incidents by 40%), and chaos engineering exercises (surfaced and fixed five latent failure modes)." The quarterly report connects the metrics to the work. It shows leadership that reliability investment has ROI.

## Setting Improvement Targets

A reliability target without a timeline is a wish. A reliability target with a timeline is a commitment. You set targets for the metrics that matter: "Reduce SEV1 incident count to fewer than 5 per quarter by Q4 2026." "Reduce MTTR for SEV1 incidents to below 30 minutes by Q3 2026." "Reduce recurring incident rate to below 10% by Q2 2026." The target is specific, measurable, and time-bound. You track progress monthly. If you're on track, you keep going. If you're behind, you adjust your work.

The target should be ambitious but achievable. If your MTTR is 60 minutes today, targeting 10 minutes by next quarter is unrealistic. Targeting 45 minutes is achievable. Targeting 30 minutes by end of year is ambitious but possible. The target stretches the team without demoralizing them. If you consistently miss targets, you're setting them too aggressively. If you consistently beat them, you're setting them too conservatively. Adjust every quarter based on actual progress.

Some targets are aspirational: "Achieve 99.95% availability." You might never fully hit it, but you get closer every quarter. Other targets are milestones: "Complete postmortem action items within 30 days for 90% of incidents." This target is binary. You either hit it or you don't. Both types of targets are useful. Aspirational targets give you a north star. Milestone targets give you checkpoints.

The incident metrics you track don't improve reliability. The work you do based on those metrics improves reliability. But without the metrics, you can't tell whether your work is helping. You can't prioritize the next investment. You can't report progress to leadership. The team that tracks incident count, MTTR, MTTD, and recurring rate every month for a year will improve reliability faster than the team that tracks nothing. Not because tracking is magic. Because tracking makes patterns visible, and visible patterns drive action. You measure not to prove you're good, but to get better.

In Chapter 9, we examine chaos engineering — the practice of deliberately injecting failures into production systems to validate resilience and discover unknown failure modes before they cause incidents.
