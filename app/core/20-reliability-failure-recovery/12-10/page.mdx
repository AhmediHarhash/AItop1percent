# 12.10 — Drift Detection vs Anomaly Detection

In March 2025, a healthcare AI team received two alerts on the same morning. Alert one: "Anomaly detected — response time spiked to 4.2 seconds." Alert two: "Drift detected — output sentiment shifted negative." They treated both alerts the same way — investigated urgency, assigned on-call engineer, escalated to incident channel. The anomaly resolved in 20 minutes. It was a transient API latency spike. The drift persisted for three weeks. It was a gradual shift in patient question patterns during flu season. They wasted resources treating drift as an anomaly. They missed the window to address drift as a trend. The confusion cost them two weeks of response time and a compliance flag.

Anomaly detection and drift detection are different monitoring disciplines. Anomalies are spikes. Drift is gradual change. Anomalies are temporary. Drift is persistent. Anomalies trigger immediate response. Drift triggers investigation. Confusing them wastes resources and misses problems.

## Anomalies vs Drift — The Key Distinction

An anomaly is a sudden, sharp deviation from baseline. A metric that was stable at 500 milliseconds jumps to 3,000 milliseconds. An error rate that was 0.2% jumps to 8%. A model that outputs balanced sentiment suddenly outputs 80% negative responses in one hour. The change is fast, large, and obvious. Anomalies are events.

Drift is a gradual, persistent change in baseline. A metric that was stable at 500 milliseconds increases to 520 milliseconds over two weeks, then 540 milliseconds the next week, then 565 milliseconds the week after. An error rate that was 0.2% increases to 0.4% over a month, then 0.7% the next month. A model that outputs balanced sentiment shifts to 55% negative over six weeks, then 58% the next six weeks. The change is slow, small, and hard to notice day-to-day. Drift is a trend.

Anomalies are usually caused by external events. An API goes down. A datacenter has a network issue. A user submits malicious input. A deployment bug introduces a regression. The cause is discrete and identifiable. Anomalies have clear start and end times.

Drift is usually caused by gradual environmental changes. User behavior shifts. Data distribution changes. Model weights degrade. Training data becomes less representative. The cause is diffuse and cumulative. Drift has a start time but no clear end time — it continues until you intervene.

Anomalies demand immediate action. Something broke. Fix it. Roll back. Investigate. Page on-call. Anomalies are incidents.

Drift demands investigation and planning. Something is changing. Understand it. Measure it. Decide if intervention is needed. Drift is not an incident. Drift is a signal.

## Detection Methods for Each

Anomaly detection uses threshold-based alerts. Define a baseline. Define acceptable deviation from baseline — usually two or three standard deviations. When a metric exceeds the threshold, trigger an alert. Anomalies are easy to detect because they are obvious. A 10x spike in error rate is unmistakable. A 500% increase in latency is unmistakable. Thresholds catch them.

Drift detection uses trend analysis. Define a baseline over a long window — 30 to 90 days. Measure the same metric over a short recent window — 7 to 14 days. Compare the two. If the recent average is statistically different from the baseline average, drift is occurring. The difference might be small — 3% or 5% — but it is persistent. Statistical tests like t-tests or Mann-Whitney U tests detect it.

Anomaly detection runs continuously. You check every data point against thresholds in real-time. If a single data point exceeds the threshold, alert. The goal is speed — detect and respond within minutes.

Drift detection runs periodically. You calculate trends daily or weekly. You compare windows. You track direction and magnitude. The goal is early warning — detect before the drift becomes severe.

Anomaly detection uses absolute thresholds. "Error rate above 2% is an anomaly." "Response time above 2 seconds is an anomaly." The thresholds are fixed or adapt slowly.

Drift detection uses relative thresholds. "Error rate increased by 40% compared to last month's baseline." "Response time is 12% slower than the 90-day average." The thresholds are always relative to recent history.

Anomaly detection focuses on short-term deviations. The detection window is seconds to hours. If a metric returns to normal within an hour, the anomaly is over. Detection stops alerting.

Drift detection focuses on long-term changes. The detection window is days to weeks. Even if a metric stabilizes at a new level, drift persists. The new level is different from the original baseline. Detection continues alerting until you reset the baseline or correct the drift.

## False Positive Patterns

Anomaly detection generates false positives from natural variation. A metric spikes briefly due to normal usage patterns — a batch job runs, traffic increases during peak hours, a single user submits an unusually large request. The spike exceeds the threshold. The alert fires. Investigation reveals no underlying problem. The spike was noise. These false positives are common. Reduce them by widening thresholds or adding context — only alert if the spike persists for 5 minutes, or if multiple metrics spike simultaneously.

Drift detection generates false positives from seasonal changes. User behavior changes during holidays. Traffic patterns change during weekends. Model behavior changes when input distribution shifts naturally. The drift is real — the metric changed — but it is expected and harmless. Investigation reveals no problem requiring correction. These false positives waste time. Reduce them by excluding expected variation — define seasonal baselines, track drift separately for weekdays and weekends, or exclude known change windows from drift calculation.

Both methods generate false positives from baseline misconfiguration. If your baseline is calculated during an unusual period — a launch week, a holiday, a datacenter migration — all future measurements compare against an unrepresentative baseline. Everything looks like an anomaly or drift. Recalculate baselines quarterly. Exclude unusual periods.

Both methods miss true positives when thresholds are too lenient. If you set anomaly thresholds at five standard deviations, you miss moderate anomalies. If you set drift thresholds at 50% change, you miss gradual drift. Tune thresholds to your tolerance for false positives and false negatives. Start strict. Relax if false positives overwhelm you.

## Combined Monitoring Approaches

Effective monitoring uses both methods. Anomaly detection catches sudden failures. Drift detection catches gradual degradation. Together, they cover the full failure spectrum.

Run anomaly detection on operational metrics. Response time, error rate, throughput, availability, cost per request. These metrics fail suddenly. Anomalies are the primary failure mode. Alert immediately when they occur.

Run drift detection on quality metrics. Accuracy, policy compliance, user satisfaction, output diversity, hallucination rate. These metrics degrade gradually. Drift is the primary failure mode. Alert when trends exceed thresholds.

Run both on hybrid metrics. Metrics that can fail suddenly or degrade gradually. Request success rate is hybrid — it can drop suddenly due to an API failure, or drift downward due to changing user behavior. Monitor with both methods. Anomaly detection catches the sudden drop. Drift detection catches the gradual decline.

Use anomaly detection as a gate for drift detection. If an anomaly occurs and resolves within an hour, do not include that data in drift calculations. Anomalies are noise from the drift perspective. Excluding them prevents transient events from skewing trend analysis.

Use drift detection as context for anomaly investigation. When an anomaly occurs, check if drift preceded it. If response time has been drifting upward for three weeks and then suddenly spikes, the spike is not isolated. It is the culmination of an underlying trend. The investigation should focus on the trend, not just the spike.

## When Drift Looks Like Anomaly (and Vice Versa)

Drift can look like an anomaly if you only monitor recent data. If you check metrics daily without comparing to long-term baseline, a gradual 15% increase over three weeks looks like a sudden 15% jump. You treat it as an anomaly. You look for a discrete cause. You find nothing. The real problem is drift. The solution is to widen your monitoring window.

Anomalies can look like drift if you only monitor aggregated data. If you calculate daily averages and a metric spikes for one hour each day, the daily average increases gradually. It looks like drift. You investigate trends. The real problem is a recurring anomaly. The solution is to monitor at finer granularity — hourly or per-minute instead of daily.

Repeated anomalies are drift. If a metric spikes above threshold ten times per week, every week, for a month, it is not ten separate anomalies. It is drift. The baseline is shifting. The system is less stable than it was. Treat it as drift. Investigate the underlying trend. Correct the baseline or fix the root cause.

Sudden drift is an anomaly. If a metric shifts 20% in two hours and stays at the new level, it is not a transient anomaly. It is a step-function change — sudden drift. A deployment changed behavior. A configuration updated. A new feature launched. Investigate as you would an anomaly — find the discrete change event — but recognize that the new behavior will persist unless corrected.

## Alert Design for Drift vs Anomaly

Anomaly alerts must be urgent. Use high-priority channels — PagerDuty, Slack alerts, SMS. Include current metric value, threshold, and deviation magnitude. Example: "ALERT: Error rate at 8.3%, threshold 2%, deviation 4.1x." Link to runbook. Expect immediate response.

Drift alerts must be informative, not urgent. Use medium-priority channels — email, Slack notifications, dashboard widgets. Include trend direction, magnitude, and timeframe. Example: "DRIFT: Response time increased 14% over 21 days. Baseline 520ms, current 593ms." Link to trend graph. Expect response within 24 hours.

Anomaly alerts should auto-resolve. When the metric returns to normal, send a resolution alert. Example: "RESOLVED: Error rate returned to 0.3%, within threshold." This prevents alert fatigue. Responders know the anomaly ended.

Drift alerts should not auto-resolve. Even if a metric stabilizes, the drift persists until you reset the baseline or correct the cause. Keep the drift alert active. Update it with latest trend data. Require explicit acknowledgment or baseline reset to resolve.

Anomaly alerts should fire on single breaches. One data point exceeding threshold is enough. Speed is critical.

Drift alerts should fire on sustained trends. Require at least three consecutive measurements showing the same trend direction. This prevents false positives from short-term noise.

Anomaly alerts should include recent context. Show the metric over the last hour. Responders need to see the spike in context of recent stability.

Drift alerts should include long-term context. Show the metric over the last 90 days. Responders need to see the trend in context of historical baseline.

## Response Procedures for Each

Anomaly response is incident response. Acknowledge the alert. Investigate the cause. If the anomaly is ongoing, mitigate immediately — roll back deployment, failover to backup, rate-limit traffic. If the anomaly resolved, investigate root cause to prevent recurrence. Document findings. Close the incident.

Drift response is investigative. Acknowledge the alert. Analyze the trend. Determine if the drift is expected (seasonal variation, intentional changes) or unexpected (degradation, misalignment). If expected, adjust the baseline or exclude the drift from alerting. If unexpected, determine severity. Minor drift requires monitoring. Moderate drift requires scheduled correction. Severe drift requires immediate intervention.

Anomaly response is fast. Time to resolution is measured in minutes to hours. The goal is restore normal operation.

Drift response is deliberate. Time to resolution is measured in days to weeks. The goal is understand the trend and make an informed decision about correction.

Anomaly response is reactive. Something broke. Fix it.

Drift response is proactive. Something is changing. Decide whether to adapt, correct, or reset baseline.

## Unified Monitoring Dashboards

The best monitoring systems display both anomaly and drift in a unified interface. A single dashboard shows:
- Real-time metrics with anomaly thresholds overlaid
- Trend lines with drift indicators
- Anomaly events marked as discrete points
- Drift trends marked as shaded regions
- Combined alert feed showing both types
- Separate sections for operational metrics (anomaly-focused) and quality metrics (drift-focused)

The dashboard makes the distinction clear. Responders see at a glance whether they are dealing with an anomaly requiring immediate action or drift requiring investigation. The visual separation prevents confusion. The unified view prevents silos — operational teams see quality drift, quality teams see operational anomalies, everyone understands system health holistically.

Anomalies are events. Drift is a trend. Both degrade AI systems. Both require monitoring. The difference determines response. Treat them differently. Next: how to build proactive refresh systems that prevent drift before it becomes severe.
