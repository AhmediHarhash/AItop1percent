# 14.2 — Level 1: Reactive Reliability (Firefighting Mode)

In November 2025, a legal tech startup learned that its AI summarization feature had stopped working when the CEO forwarded an angry email from a customer who had been waiting 72 hours for a response. Engineering investigated. The root cause was a third-party API rate limit that had been hit three days earlier. No one knew it had failed because no one was monitoring it. The team spent the weekend building a temporary workaround, restoring service, and apologizing to users. Two weeks later, a different failure occurred — the model started hallucinating case citations. Once again, the team learned about it from user complaints. Once again, they scrambled to fix it. This pattern repeated every few weeks. The team was constantly firefighting. They had no monitoring, no alerting, no incident process, and no time to build any of it because they were always fixing the last failure.

This is Level 1. Most teams start here. Some never leave.

## What Level 1 Looks Like

Level 1 is reactive reliability. You detect failures when users report them. You respond by fixing the immediate issue. You have no systematic monitoring, no alerting, no incident process, and no prevention culture. Every failure is a surprise. Every response is a fire drill. The same incidents recur because you fix symptoms, not root causes.

At Level 1, your primary detection mechanism is user complaints. A user emails support. Support escalates to engineering. Engineering investigates, discovers the failure happened hours or days ago, and scrambles to fix it. The failure might have affected hundreds or thousands of users before anyone on your team knew it existed. The lag between failure and detection is measured in hours, sometimes days. During that lag, users experience a broken product, trust erodes, and your reputation takes damage you cannot undo.

At Level 1, your response process is whoever is available tries to fix it. There is no on-call rotation. There is no incident commander. There is no escalation path. The engineer who happens to see the support ticket becomes responsible. If that engineer is on vacation or in a meeting, the incident waits. If the engineer does not know how to fix it, they ask around until they find someone who does. The response time is unpredictable. The quality of the fix is inconsistent. The communication to users is often nonexistent.

At Level 1, your prevention process is we will be more careful next time. After an incident, the team discusses what went wrong. Someone might write a quick postmortem. Action items are identified but rarely tracked and even more rarely completed. The same incidents recur weeks or months later because the root cause was never addressed. The team fixes the symptom — the broken API call, the misconfigured threshold, the missing fallback — but they do not fix the system that allowed the symptom to occur.

At Level 1, your culture treats reliability as someone else's problem. Product teams ship features without considering failure modes. Engineering teams deprioritize monitoring because it is not a roadmap item. Leadership does not allocate time or budget for reliability work because reliability does not appear in the OKRs. The only time reliability gets attention is during a crisis, and that attention evaporates as soon as the crisis ends.

## The Symptoms: Constant Firefighting and Recurring Incidents

The clearest symptom of Level 1 is constant firefighting. Every week brings a new incident. Every incident requires urgent manual intervention. Engineers are interrupted mid-task to investigate failures. Roadmap work is delayed because the team is always fixing something that broke. Morale suffers because no one can focus long enough to build anything new.

The second symptom is recurring incidents. The same failure types happen over and over. The model degrades in the same way. The API hits the same rate limit. The fallback mechanism fails in the same scenario. Each time, the team fixes the immediate instance but never addresses the underlying cause. The recurrence rate is high — 40 percent or more of incidents are repeat issues.

The third symptom is user-reported detection. If you ask the team how they learned about the last five incidents, the answer is almost always a user complaint. Monitoring might exist, but it is incomplete, misconfigured, or ignored. Alerts might fire, but they are buried in a flood of false positives that everyone has learned to ignore. The primary signal that something is wrong is an unhappy user.

The fourth symptom is unpredictable recovery time. Some incidents are fixed in 20 minutes. Some take six hours. Some linger for days because the right person is unavailable. There is no target for mean time to recovery because there is no measurement of mean time to recovery. The team does not track incident metrics because they are too busy responding to incidents.

The fifth symptom is hero culture. Reliability depends on individual heroics. One engineer stays up all night to fix the outage. Another engineer cancels their weekend plans to debug the hallucination issue. The team celebrates these heroes, which reinforces the behavior. But hero culture is not sustainable. Heroes burn out. And a system that requires heroics to function is a system that cannot scale.

## Why Teams Get Stuck at Level 1

Teams do not stay at Level 1 because they are lazy or incompetent. They stay at Level 1 because escaping Level 1 requires upfront investment, and when you are firefighting every day, you do not have time to invest. The incidents keep coming. The roadmap keeps demanding new features. Leadership keeps asking why reliability is not improving, but they do not give the team the space or resources to actually improve it.

This is the Level 1 trap. You are too busy fighting fires to build the systems that prevent fires. Every week, you tell yourself next week we will invest in monitoring. Next week arrives, and a new incident consumes your time. The cycle repeats. Months pass. The team is exhausted. The product is fragile. And you are still at Level 1.

The second reason teams stay at Level 1 is lack of organizational buy-in. Engineering wants to invest in reliability. Product does not see the ROI. Leadership wants the team to ship faster, not slow down to build monitoring. The roadmap is full of features that customers asked for. Reliability work is invisible. It does not show up in demos. It does not generate revenue. So it gets deprioritized, sprint after sprint.

The third reason is lack of clarity about what Level 2 requires. The team knows they need monitoring, but they do not know where to start. They know they need alerting, but they do not know what thresholds to set. They know they need an incident process, but they do not know what that process looks like. The gap between Level 1 and Level 2 feels insurmountable, so they do not start.

The fourth reason is fear of slowing down. Level 1 teams ship fast — at least in the short term. They skip testing. They skip documentation. They skip reliability work. They ship the feature, and if it breaks, they fix it later. Moving to Level 2 means slowing down to build the infrastructure that prevents breakage. That slowdown is temporary, but it is real. And in a culture that rewards speed above all else, the team fears they will be punished for slowing down, even if the slowdown makes them faster in the long run.

## The Cost of Staying at Level 1

Staying at Level 1 is expensive. The costs are often invisible in the moment, but they compound over time. By the time leadership recognizes the true cost, the damage is already done.

The first cost is user trust. Every user-reported incident erodes trust. Every delayed response erodes trust. Every recurring incident erodes trust. Users do not give infinite chances. After three or four bad experiences, they leave. And once trust is lost, it is nearly impossible to rebuild. You might fix the technical issues, but the user has already decided your product is unreliable. They do not come back.

The second cost is engineering velocity. Constant firefighting destroys velocity. Engineers cannot focus. Context-switching from roadmap work to incident response kills productivity. Technical debt accumulates because no one has time to address it. The codebase becomes more fragile over time, which increases the incident rate, which increases firefighting, which further destroys velocity. The cycle is vicious.

The third cost is team morale and retention. Engineers do not join your company to fight fires every week. They join to build interesting products. When firefighting becomes the primary activity, morale collapses. Burnout increases. Talented engineers leave. The remaining engineers are spread thinner, which increases the firefighting burden, which accelerates burnout. The best people leave first because they have options.

The fourth cost is opportunity cost. Every hour spent fixing incidents is an hour not spent building new features, improving the product, or growing the business. The roadmap slips. Competitors ship faster. Revenue growth stalls. Leadership blames engineering for being slow, but the real problem is that engineering is spending half their time on incidents.

The fifth cost is risk. At Level 1, you are one major incident away from existential damage. A healthcare AI that fails silently for three days exposes patient data. A financial AI that hallucinates transaction details causes monetary loss. A customer-facing AI that goes down during a high-traffic event destroys your reputation. At Level 1, you have no defense against these scenarios. You hope they do not happen. Hope is not a strategy.

## The Minimum Steps to Escape Level 1

Escaping Level 1 does not require a six-month transformation program. It requires focus, discipline, and a few critical capabilities. The goal is not perfection. The goal is to stop firefighting long enough to build the systems that prevent the next fire.

The first step is implement basic monitoring. Identify the five most critical failure modes in your AI system. For each failure mode, define a metric and a threshold. Instrument your system to track those metrics. Set up alerting when thresholds are crossed. This does not require a sophisticated observability platform. It requires deciding what matters, measuring it, and alerting when it breaks. Start with five metrics. Add more later.

The second step is define an incident response process. Write it down in one page. Who is on call? How do they escalate? Who communicates to users? Who investigates the root cause? Who tracks action items? The process does not need to be perfect. It needs to exist. When the next incident happens, follow the process. After the incident, revise the process based on what you learned. Repeat.

The third step is conduct postmortems for major incidents. Not every incident requires a postmortem. But every incident that affected users, cost money, or revealed a systemic weakness requires one. The postmortem is blameless. It focuses on what happened, why it happened, and what changes will prevent recurrence. The action items are tracked. Someone is responsible for each action item. The items are completed within 30 days or explicitly deprioritized with leadership buy-in.

The fourth step is track incident metrics. Mean time to detection. Mean time to recovery. Incident recurrence rate. Number of user-reported versus system-detected incidents. You do not need a dashboard. You need a spreadsheet. Track the metrics every week. Share them with the team. Use them to measure progress. If mean time to detection is decreasing, you are improving. If recurrence rate is increasing, you are regressing.

The fifth step is protect time for reliability work. Reserve 20 percent of engineering capacity for reliability. This is not negotiable. If the roadmap is too full, cut features. If leadership pushes back, show them the incident metrics and the cost of firefighting. Reliability work does not happen in the gaps between feature work. It happens when you make space for it.

These five steps are sufficient to move from Level 1 to Level 2. They do not require new headcount. They do not require expensive tools. They require discipline, organizational buy-in, and a recognition that staying at Level 1 is more expensive than investing in Level 2.

## What Success Looks Like When Exiting Level 1

You know you are exiting Level 1 when the majority of incidents are detected by your monitoring, not by user complaints. You know you are exiting Level 1 when you have an incident response process that the team actually follows. You know you are exiting Level 1 when postmortem action items are completed, not forgotten. You know you are exiting Level 1 when the incident recurrence rate is declining, not flat or rising.

Success at this stage is not zero incidents. Success is faster detection, faster response, and fewer recurring incidents. Success is a team that spends less time firefighting and more time building. Success is users who notice that reliability is improving, even if it is not perfect.

Exiting Level 1 is the hardest transition in the maturity model because it requires breaking the firefighting cycle. Once you break that cycle, the path to Level 2 is clear. The next subchapter defines Level 2 and the capabilities that distinguish it from Level 1.
