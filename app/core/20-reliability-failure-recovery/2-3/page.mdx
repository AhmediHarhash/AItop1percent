# 2.3 — Building Real-Time Quality Detectors

The data infrastructure team at a healthcare AI company built a comprehensive quality evaluation suite. It measured accuracy, relevance, safety, and factual consistency across 12 quality dimensions. It used state-of-the-art LLM-as-judge models, compared outputs against clinical guidelines, and validated every claim against medical databases. The evaluation was thorough, rigorous, and completely useless for production monitoring. Why? Because it took 18 seconds to evaluate a single response. Their symptom checker returned responses in 2 seconds. Users were not going to wait 18 seconds for a quality check. The team ran the evaluation suite nightly on sampled logs. By the time they detected quality degradation, it had been happening for 14 hours. Their detection latency was half a day. For a medical application, that was unacceptable.

Real-time quality detection is not the same as batch evaluation. Batch evaluation optimizes for thoroughness. Real-time detection optimizes for speed and cost while maintaining enough signal to catch regressions. The challenge is building quality checks that run fast enough to keep up with production traffic, cheap enough to run continuously, and sensitive enough to detect degradation before it compounds. This is a latency budget problem, a cost budget problem, and a statistical sampling problem all at once.

## The Latency Budget: How Much Time Do You Have?

Every production system has a latency budget. Users will tolerate some delay. Past a threshold, the experience feels broken. The latency budget determines whether quality checks can run synchronously — in the critical path of serving the response — or must run asynchronously — after the response is returned.

For synchronous detection, the quality check must complete within the remaining latency budget. If users expect a 500-millisecond response and your model returns in 350 milliseconds, you have 150 milliseconds for quality evaluation. That is tight. You can run lightweight checks: toxicity classifiers, PII pattern matching, basic relevance scoring. You cannot run heavyweight checks: multi-step fact verification, comprehensive hallucination detection, LLM-as-judge evaluation with large models.

For asynchronous detection, the quality check runs after the response is served. The user sees the response immediately. The evaluation happens in the background. Detection latency increases — you might not catch a bad response until seconds or minutes after it was served — but you can run more thorough evaluations without impacting user experience. This is the trade-off most production systems make: serve fast, evaluate thoroughly in parallel.

The healthcare company's 18-second evaluation could not run synchronously. Their latency budget was 2 seconds total. Even asynchronously, 18 seconds per response at their traffic volume would have cost more than their entire infrastructure budget. They needed faster evaluations, cheaper evaluations, or statistical sampling that allowed them to evaluate a subset of traffic rather than every response.

## Sampling Strategies: 100% Coverage vs Statistical Validity

The ideal is evaluating every response in real-time. The reality is that comprehensive quality evaluation is too expensive and too slow to run on 100% of traffic. The solution is sampling: evaluate a subset of responses and use statistical methods to infer quality across the full population.

Random sampling is the simplest approach. Evaluate X percent of responses chosen uniformly at random. If you evaluate 5% of traffic and detect that 8% of evaluated responses are hallucinations, you estimate the overall hallucination rate is 8% with confidence intervals based on sample size. Random sampling works when quality is distributed uniformly. It misses localized failures: a specific user query pattern that triggers failures, a specific time window when the model degrades, a specific demographic that experiences worse quality.

Stratified sampling addresses localized failures by ensuring the sample includes representation from different strata: high-traffic queries and low-traffic queries, new users and returning users, short prompts and long prompts, fast responses and slow responses. Stratified sampling costs more to implement — you need to classify traffic into strata in real-time — but it catches failures that random sampling misses.

Adaptive sampling adjusts the sampling rate based on risk. When quality metrics are stable, sample lightly to reduce cost. When metrics start to drift, increase sampling to get more signal. Adaptive sampling is the most efficient approach for mature systems. It requires automation: a monitoring loop that watches quality trends and adjusts sampling rates dynamically. Teams that implement adaptive sampling cut evaluation costs by 60% while maintaining detection sensitivity.

Triggered sampling evaluates specific responses that match risk patterns. If a response contains a medical claim, evaluate it. If a response refuses a request, evaluate whether the refusal was appropriate. If a response is unusually long, evaluate coherence. Triggered sampling is not statistically representative, but it focuses evaluation budget on high-risk outputs. It complements random sampling: use random sampling for baseline monitoring, use triggered sampling to catch edge cases.

The healthcare company adopted stratified sampling at 10% of traffic with triggered evaluation for any response containing drug names or diagnosis terms. This gave them statistical coverage and targeted coverage for high-risk outputs. Their evaluation latency dropped from 18 seconds to 4 seconds by using faster models for the sampled checks. Detection latency improved from 14 hours to 45 minutes.

## LLM-as-Judge for Quality Scoring

LLM-as-judge is the most versatile technique for real-time quality evaluation. The idea is simple: use a second model to evaluate the first model's output. The judge model receives the user's query, the response, and evaluation criteria. It returns a quality score or a binary judgment. The judge runs in parallel with production traffic, either synchronously if latency allows or asynchronously if not.

The advantage of LLM-as-judge is generality. You can evaluate any quality dimension by writing evaluation prompts: relevance, accuracy, tone, completeness, safety, coherence. You do not need labeled data. You do not need to train classifiers. You write the evaluation criteria in natural language, and the judge model applies them. This makes LLM-as-judge fast to deploy and easy to iterate.

The disadvantage is cost and latency. Running a second model call for every evaluation doubles the cost and adds latency. If your production model uses GPT-5.2 at three cents per thousand tokens and your judge model uses GPT-5-mini at 20 cents per million tokens, the judge is 150 times cheaper. But if you evaluate 10% of traffic, you add 1.5% to total cost. For high-volume systems, this is significant. The mitigation is using smaller, faster, cheaper judge models. GPT-5-nano, Claude Haiku 4.5, and Gemini 3 Flash are fast enough for real-time judging at manageable cost.

The accuracy of LLM-as-judge depends on the judge model's capability and the quality of evaluation prompts. A weak judge model will miss subtle failures. A vague evaluation prompt will produce noisy judgments. The best practice is using the strongest model you can afford as a judge, writing precise evaluation criteria with examples of good and bad outputs, and validating judge accuracy against human labels. If the judge agrees with humans 88% of the time, it is good enough for automated monitoring. If agreement is below 75%, the judge is too noisy to trust.

The healthcare company used Claude Haiku 4.5 as judge for relevance and coherence — dimensions where it performed well — and reserved GPT-5.1 for factual accuracy checks on the 10% of responses that contained medical claims. This hybrid approach balanced cost and accuracy. Their judge infrastructure cost $1,200 per month for 8 million evaluated responses.

## Embedding-Based Anomaly Detection

Embedding-based anomaly detection is a fast, cheap alternative to LLM-as-judge. The idea is to embed every production response into a vector space and measure distance from expected distributions. Responses that are far from the typical cluster are anomalies. Anomalies often indicate quality problems.

The setup is straightforward. Embed a baseline set of high-quality responses using a model like OpenAI text-embedding-3-large or Cohere embed-v3. These embeddings form your reference distribution. In production, embed each response and calculate its distance to the nearest centroid or its distance to the mean embedding. If the distance exceeds a threshold, flag the response as anomalous.

The advantage is speed and cost. Embedding inference is cheap — less than 0.002 cents per response — and fast — typically 50 to 150 milliseconds. Distance calculation is even cheaper. You can run embedding-based anomaly detection on 100% of traffic with negligible cost and latency impact. This makes it suitable for synchronous detection in the response path.

The disadvantage is interpretability and specificity. Embedding distance tells you a response is unusual. It does not tell you why. An anomalous response could be off-topic, could be hallucinating, could be unsafe, or could just be unusual in a harmless way. Anomaly detection is a signal, not a diagnosis. You need secondary evaluation to understand what the anomaly means.

Embedding-based detection is best used as a filter. Flag anomalous responses in real-time. Route flagged responses to deeper evaluation: LLM-as-judge, human review, or specialized classifiers. This two-stage approach is efficient. Most responses are normal and need no further checking. The anomalous 3% get scrutiny. Detection is fast. Diagnosis is thorough.

The healthcare company added embedding-based anomaly detection as layer one. Every response was embedded and checked. Responses more than two standard deviations from the centroid — about 4% of traffic — were flagged for LLM-as-judge evaluation. This caught off-topic responses, incoherent outputs, and unusual phrasings that often correlated with hallucinations. It added 60 milliseconds to median latency, which fit within their budget.

## Rule-Based Quality Gates

Rule-based quality gates are the fastest and cheapest detection method. The trade-off is limited coverage. Rules catch known failure patterns. They miss novel failures. But for specific high-risk patterns, rules are unbeatable for speed and reliability.

A rule-based gate is a deterministic check. If the response contains a banned phrase, reject it. If the response length is below threshold, flag it as potentially incomplete. If the response matches a regex for PII, block it. If the response repeats the same sentence three times, flag it as incoherent. Rules run in microseconds. They cost nothing. They trigger with certainty.

The challenge is defining useful rules without creating false positives. A rule that flags every response containing a number will fire constantly. A rule that flags responses containing email addresses will catch PII leakage but also catch legitimate responses that mention example email formats. Rules must be precise enough to avoid noise while broad enough to catch real issues.

The most effective rules are negative constraints: checks for things that should never happen. Responses should never contain your internal system prompts. Responses should never include specific placeholder text. Responses should never violate length limits. Responses should never be exact duplicates of training data. These are binary checks. Either the response violates the rule or it does not. False positives are rare.

The healthcare company implemented three rule-based gates. First, responses could not contain drug dosage instructions — those require pharmacist review, not AI generation. Second, responses could not include phrases like "I am not a doctor" because their product was a clinical decision support tool and disclaimers undermined trust. Third, responses could not exceed 600 tokens because anything longer was usually rambling. These rules ran synchronously, added less than 5 milliseconds of latency, and caught 2% of responses that would have been quality failures.

## The Latency Budget Allocation

Real-time detection requires allocating latency budget across detection methods. Different checks have different costs. You must decide which checks run synchronously in the response path and which run asynchronously after serving the user.

In the critical path, you can afford lightweight checks. Rule-based gates run first — microseconds. Embedding-based anomaly detection runs next — 50 to 150 milliseconds. Fast classifiers for toxicity or PII run if budget allows — 30 to 100 milliseconds. These checks must complete within your remaining latency budget after model inference. If your total budget is 500 milliseconds and the model takes 350 milliseconds, you have 150 milliseconds for synchronous checks.

Asynchronously, you can afford heavyweight checks. LLM-as-judge evaluation runs in parallel with serving the response. Factual accuracy verification runs after the response is returned. Multi-step hallucination detection runs on sampled traffic in background workers. These checks take seconds, but users do not wait for them. Detection latency increases — you might not catch a bad response until 5 seconds after it was served — but you can run thorough evaluations.

The decision of what to run synchronously depends on consequence severity. For safety-critical applications, you must catch policy violations before serving the response. That requires synchronous safety classifiers even if they add latency. For non-critical quality dimensions like tone or verbosity, asynchronous evaluation is fine. Users can tolerate a slightly verbose response. They cannot tolerate a response that leaks PII.

The healthcare company ran three synchronous checks: rule-based gates for dosage instructions and length, embedding anomaly detection for off-topic responses, and a toxicity classifier to catch inappropriate medical advice phrasing. These checks added 120 milliseconds total. Asynchronously, they ran LLM-as-judge for accuracy and relevance, hallucination detection on flagged responses, and human review on 1% of high-risk responses. The synchronous checks caught 8% of responses before serving. The asynchronous checks caught an additional 3% within 60 seconds.

## The Observer Effect: Monitoring Without Slowing Down Production

The observer effect in physics states that measuring a system changes the system. The same applies to production monitoring. Running quality checks in the response path adds latency. Adding latency degrades user experience. Degraded user experience affects business metrics. You must measure quality without materially impacting performance.

The mitigation is designing detection to be lightweight by default and heavyweight only when needed. Run cheap checks on all traffic. Run expensive checks on sampled traffic. Run the most expensive checks only on flagged responses. This cascading approach minimizes cost and latency while maximizing coverage.

Caching is another mitigation. If you evaluate a response for a specific query and the response is good, cache that evaluation. If the same query comes in again and produces the same response, skip re-evaluation. Caching works for deterministic responses or low-temperature sampling where responses are consistent. It does not work for creative use cases where every response should be unique. The healthcare company cached evaluations for common symptom queries. About 30% of their traffic hit the cache, eliminating evaluation latency entirely for those requests.

Batching is a third mitigation. Instead of evaluating each response individually, batch 100 responses and evaluate them together. LLM-as-judge can evaluate multiple responses in a single API call if the prompt is structured correctly. Embedding models can embed batches in parallel. Batching reduces per-response cost and latency by amortizing overhead. The trade-off is detection latency: you wait until the batch fills before evaluation. For asynchronous checks, batching is often a net win.

## Detection as a Continuous Discipline

Real-time quality detection is not a one-time implementation. It is a continuous discipline. Failure modes evolve. Adversaries discover new attacks. Models change. User behavior shifts. Detection infrastructure must evolve with the system.

The pattern is iterative. You start with basic detection: infrastructure monitoring, simple rule-based gates, sampling-based LLM-as-judge evaluation. You deploy to production. You observe what the detection catches and what it misses. You add new checks for the misses. You tune thresholds to reduce false positives. You refine sampling to focus on high-risk areas. The detection improves incrementally.

Teams that treat detection as static — deploy once and assume it works forever — discover gaps during incidents. Teams that treat detection as a living system — continuously tuning, expanding, and validating — catch failures earlier and respond faster. The difference is not the initial sophistication of the detection infrastructure. The difference is the discipline of maintaining and improving it. The next question is whether user behavior itself can be a detection signal.

---

Next: **2.4 — User Behavior as a Leading Indicator**
