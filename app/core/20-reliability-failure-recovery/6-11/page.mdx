# 6.11 — Incident Command for AI: Roles and Responsibilities

At 11:23 PM on a Friday in October 2025, a content moderation AI started flagging legitimate user posts as policy violations at five times the normal rate. By 11:30, four engineers had joined the incident channel. By 11:45, seven engineers were investigating, three were proposing different rollback strategies, two were arguing about whether this was a model problem or a data problem, and nobody was coordinating. By midnight, they had attempted two partial rollbacks that conflicted with each other, deployed a prompt change that made things worse, and accidentally took down the moderation queue entirely. What started as a quality incident became an availability incident because nobody was in command.

Large AI incidents need clear command structure. Without it, you get overlapping work, conflicting actions, and decision paralysis. The best incident responders in the world can't coordinate effectively if nobody is directing traffic. Incident command is not about hierarchy or authority — it's about coordination under pressure. Someone has to own the incident, someone has to own technical decisions, someone has to own communication, and someone has to understand AI-specific failure modes that general software engineers might miss. These roles have names and clear responsibilities. They can be filled by the same person in small incidents or by different people in large ones. But they must be filled.

## The Incident Command Structure

Incident command structure for AI systems adapts traditional software incident response with AI-specific roles. The core structure has four roles: incident commander, technical lead, communications lead, and domain expert. Smaller incidents might collapse these into two people. Larger incidents might expand to six or eight with multiple technical leads or regional communications coordinators. The structure scales to the incident, but the roles remain consistent.

The incident commander owns the incident from declaration to resolution. They coordinate all responders, make final decisions when the team is deadlocked, decide when to escalate or de-escalate, manage the timeline, and ensure handoffs happen cleanly. The IC does not need to be the most senior person on the call. They need to be the person best equipped to coordinate under pressure. In many organizations, the on-call engineer who detected the incident becomes IC by default. In others, IC is a trained role that rotates through senior engineers.

The technical lead owns diagnosis and remediation. They direct the technical investigation, propose fixes, evaluate rollback options, and coordinate implementation of the chosen solution. The technical lead is typically the person with the deepest understanding of the affected system. For AI incidents, this is often the ML engineer or AI product lead who built the model, understands its failure modes, and knows the deployment architecture. The technical lead reports to the IC but has autonomy over technical decisions within the IC's strategic direction.

The communications lead owns all messaging. They draft internal updates for leadership and customer support, write external messages for users and status pages, coordinate with PR or legal if needed, and ensure messaging is consistent across all channels. The communications lead is not a passive scribe. They actively manage what gets communicated, when, and to whom. In small incidents, the IC also handles comms. In large incidents with user impact or regulatory implications, comms becomes a dedicated role.

The domain expert provides AI-specific context that general engineers lack. They understand model behavior, training data provenance, eval methodology, and the subtleties of AI degradation that don't show up in standard metrics. For incidents involving hallucinations, bias, safety failures, or unexpected output patterns, the domain expert is critical. They help the technical lead interpret what's happening and guide the team toward AI-appropriate remediation strategies. In organizations without dedicated AI expertise, this role might be filled by the technical lead. In mature AI organizations, domain experts are on-call specifically for AI incidents.

## Incident Commander Role for AI Incidents

The IC's job is coordination, not heroics. They don't need to be the person who fixes the bug or identifies the root cause. They need to be the person who ensures everyone knows what they're working on, prevents duplicate work, makes decisions when the team can't reach consensus, and keeps the incident moving toward resolution. The IC runs the war room. Everyone else executes within that structure.

Decision authority is the IC's primary responsibility. When the team proposes three different rollback strategies, the IC picks one or asks for a recommendation with a 10-minute deadline. When engineers disagree about whether to roll back or push a forward fix, the IC makes the call based on available information and time pressure. These decisions might be wrong. That's acceptable. Indecision is worse than a wrong decision that you can reverse. The IC's job is to prevent analysis paralysis.

Timeline management keeps incidents from drifting. The IC sets checkpoints: "We're at 30 minutes in. Tech lead, what's your confidence we'll have a diagnosis in the next 15 minutes?" "We're at 90 minutes. If the forward fix isn't deployed and validated in the next 30 minutes, we're rolling back." These checkpoints force the team to assess progress and adjust strategy. Without timeline pressure, incidents expand to fill available time. Engineers keep investigating instead of implementing known-good mitigations.

Escalation decisions belong to the IC. When do you wake up the VP of Engineering? When do you notify legal? When do you loop in customer support? When do you declare a major incident versus keeping it contained? The IC evaluates severity, user impact, duration, and reputational risk. Escalation criteria should be pre-defined in runbooks, but edge cases require judgment. The IC makes the call.

Handoff coordination is critical for incidents that span shifts. If the incident starts at 11 PM and extends past 2 AM, the original responders need rest. The IC manages handoff to fresh responders: full context transfer, decision history, current status, and open questions. A clean handoff takes 15 to 20 minutes. A rushed handoff creates information loss and duplicated work. The IC ensures handoffs are thorough even when everyone is exhausted.

## Technical Lead: The Diagnostic Expert

The technical lead is the most technically proficient person on the incident response team for the affected system. For AI incidents, this usually means the person who trained the model, designed the prompt architecture, or built the eval pipeline. They understand the system's internals well enough to quickly narrow the problem space and propose credible fixes.

Diagnostic leadership means directing investigation without doing all the work yourself. The technical lead assigns investigation tasks: "Engineer A, check whether the retrieval system changed. Engineer B, pull a sample of the bad outputs and compare to the golden set. Engineer C, confirm the model version running in production matches the version we think we deployed." The technical lead synthesizes findings, identifies the most promising diagnostic paths, and kills dead ends quickly.

Hypothesis prioritization accelerates diagnosis. Early in an incident, you have multiple theories about what went wrong. Data corruption. Prompt regression. Model version mismatch. Infrastructure failure. The technical lead ranks these by likelihood and diagnosability, then assigns investigation in priority order. They don't investigate all theories in parallel. They investigate the most likely theory first, then move to the next if that's ruled out. This requires judgment and domain knowledge.

Fix evaluation is where the technical lead's expertise matters most. For AI incidents, rollback isn't always the right answer. Sometimes a prompt fix is faster and safer. Sometimes a traffic shift to a fallback model is correct. Sometimes you need to disable a feature entirely. The technical lead evaluates these options based on technical feasibility, risk, and time to implement. They recommend a course of action to the IC, who makes the final call.

Implementation coordination ensures the chosen fix deploys correctly. The technical lead doesn't necessarily implement the fix themselves, but they direct whoever is doing the implementation. They validate the fix in staging, approve production deployment, monitor post-deployment metrics, and confirm the fix resolved the issue. If the fix doesn't work or makes things worse, the technical lead owns the decision to revert and try a different approach.

## Communications Lead: Internal and External

The communications lead is the single voice of the incident to all non-responder audiences. They write updates for internal stakeholders, draft user-facing messages, coordinate with customer support, and escalate to PR or legal when needed. In large incidents with significant user impact, communications becomes a full-time role during the incident. In smaller incidents, the IC or technical lead also handles comms.

Internal updates happen at regular intervals. Every 30 minutes during active incidents. Every hour during lower-severity incidents. The update format is consistent: current status, what changed since last update, current ETA or uncertainty statement, and who to contact for questions. These updates go to leadership, customer support, and other engineering teams that might be affected. Internal updates can be more technical than external ones, but they still need to be clear to non-experts.

External messaging requires different tone and detail level. Users don't need to know about data pipeline failures or model version mismatches. They need to know what's not working, what they should do about it, and when it will be fixed. The communications lead translates technical incident details into user-facing language, gets approval from IC and legal if needed, and publishes to status pages, in-app banners, or social media. Every external message should answer three questions: what's wrong, what should I do, and when will it be fixed?

Customer support coordination is non-optional for user-facing incidents. Support teams need talking points before users start contacting them. The communications lead provides support with a brief summary of the issue, what users are experiencing, what support can tell users, and when the next update is expected. Support also needs to know what information to collect from users and how to escalate cases that indicate the problem is worse than initially assessed.

Legal and PR escalation happens when incidents involve data exposure, safety failures, regulatory violations, or significant media attention. The communications lead notifies legal immediately when these conditions appear, briefs them on the situation, and coordinates on what can and cannot be said publicly. For incidents that will draw media scrutiny, PR teams need advance notice and involvement in external messaging. The communications lead is the bridge between incident responders and these stakeholders.

## Domain Expert: Understanding AI-Specific Behavior

The domain expert is the person who understands AI failure modes that traditional software engineers don't recognize. They know what catastrophic forgetting looks like. They can distinguish between a retrieval failure and a generation failure. They understand when a bias issue is a model problem versus a data problem. They recognize the signs of prompt injection or adversarial inputs. For organizations without deep AI expertise, this role might be missing. That's a problem.

Interpreting model behavior requires AI-specific knowledge. When an engineer reports "the model is giving weird outputs," the domain expert asks the right follow-up questions. Are the outputs factually incorrect or just stylistically different? Are errors clustered in specific topics or evenly distributed? Are outputs longer or shorter than usual? Do errors appear more frequently for certain user demographics or query types? These questions guide diagnosis toward the actual failure mode instead of surface symptoms.

Failure mode identification separates AI incidents from infrastructure incidents. If latency is elevated but output quality is fine, that's an infrastructure problem. If latency is normal but outputs are hallucinating, that's a model problem. If both latency and quality are degraded, it might be either. The domain expert helps the team distinguish between these scenarios and direct diagnostic effort appropriately. Misidentifying the failure mode leads to investigation dead ends and wasted time.

Remediation options for AI failures are different from traditional software failures. You can't just restart a model and fix the problem. The domain expert knows whether the issue is fixable with prompt changes, retrieval tuning, threshold adjustments, or whether it requires model rollback or retraining. They guide the technical lead toward AI-appropriate solutions and warn when proposed fixes won't work for AI-specific reasons.

Eval validation during incidents is where the domain expert's role becomes critical. After deploying a fix, automated metrics might look fine but the fix might not actually resolve the underlying issue. The domain expert reviews a sample of outputs post-fix, validates that the failure mode is gone, and confirms that the fix didn't introduce new problems. This human-in-the-loop validation catches issues that automated metrics miss.

## When to Involve Leadership

Leadership escalation criteria should be pre-defined and known to all ICs. Typical criteria: incidents affecting more than X percent of users, incidents lasting longer than Y hours, incidents with potential regulatory or legal exposure, incidents with significant revenue impact, or incidents that will generate media attention. If an incident meets these criteria, notify leadership immediately even if you're actively working on a fix.

What leadership needs to know: user impact, business impact, current status, ETA or uncertainty, what resources are needed, and what decisions require their input. Leadership does not need technical details about database queries or model architectures during an active incident. They need strategic context. The communications lead provides these updates, not the technical lead. This frees technical responders to focus on fixing the problem.

When leadership should be in the war room versus when they should get updates is a judgment call. For critical incidents with ambiguous remediation strategies or decisions that affect customer commitments, having a senior leader in the war room helps. They can make calls that on-call engineers don't have authority to make. For most incidents, leadership should get regular updates but stay out of the war room. Too many people in the incident channel creates noise and slows decision-making.

Post-incident leadership briefings happen after resolution. This is where you provide the full technical details, root cause analysis, and timeline of decisions. Leadership needs to understand what happened, why, and what you're doing to prevent recurrence. This briefing informs whether additional investment in reliability, monitoring, or process is justified. It's also where leadership decides whether external communication beyond the standard postmortem is needed.

## Handoff Protocols Between Roles

Incident handoffs happen when incidents span multiple shifts or when responders need to be replaced due to fatigue. A clean handoff transfers full context: incident timeline, current diagnosis, remediation attempts and outcomes, current status, open questions, and who is responsible for what. The incoming IC, technical lead, or other roles need to be able to continue the incident with zero loss of information.

Handoff checklists prevent information loss. The checklist covers: incident start time, severity level, user impact assessment, timeline of key events, current working theory of root cause, remediation attempts so far, current status of deployed fixes, metrics being monitored, ETA or uncertainty level, and who to contact for additional context. The outgoing responder walks the incoming responder through this checklist. Both confirm understanding. The handoff is documented in the incident channel.

Handoff timing matters. Don't hand off during critical decision points or active deployments. Wait until you've reached a stable state: the rollback completed, the fix deployed, the metrics being monitored for the next hour. Handoffs during active changes create risk that the incoming responder won't have enough context to handle complications. If you must hand off during active work, have a longer overlap period where both responders are in the war room together.

Cross-role handoffs happen when a technical lead needs to hand off to someone with different expertise or when an IC hands off to someone in a different time zone. These handoffs are higher risk than same-role handoffs because the incoming person might approach the problem differently. Explicitly state what decisions have been made and why, what approaches have been ruled out, and what constraints the incoming team is operating under. This prevents the new team from revisiting decisions that have already been made.

## Training for Incident Roles

Incident response training should happen during peacetime, not during incidents. Run incident drills where engineers rotate through different roles. Declare a simulated incident, assign roles, and run through diagnosis and remediation under time pressure. The drill reveals gaps in runbooks, communication patterns, and role clarity. Engineers who have practiced incident command during drills are far more effective during real incidents.

Role-specific training focuses on the skills each role needs. IC training covers decision-making under uncertainty, timeline management, and coordination techniques. Technical lead training covers diagnostic frameworks, hypothesis prioritization, and fix evaluation. Communications lead training covers message drafting, stakeholder management, and legal considerations. Domain expert training covers AI failure mode recognition and eval validation techniques.

Incident postmortems are teaching opportunities. After every significant incident, review not just the technical root cause but also how incident response went. Did roles work as intended? Were there communication breakdowns? Did handoffs go smoothly? What would we do differently next time? These lessons feed back into training and runbook updates. The team that learns from every incident gets better at handling the next one.

---

Next: **6.12 — The Handoff Problem: Incidents That Span Multiple Shifts** — how to maintain context and momentum when incidents last longer than a single shift, preventing information loss and duplicated work during responder changes.
