# 9.4 — Retrieval Failure Simulation

In October 2025, a legal research platform ran a weekend chaos experiment. They simulated a vector database outage during low-traffic hours. Within six minutes, their entire AI system was down. Not degraded — down. The chatbot interface returned error messages. The document Q&A feature hung indefinitely. The smart search functionality fell back to keyword search that returned irrelevant results, confusing users more than helping them. The team thought they had built retrieval as a dependency. They discovered they had built retrieval as a single point of failure.

The incident revealed what most RAG systems hide: retrieval is not just a feature. It is the foundation. When it fails, everything built on top collapses.

## The Retrieval Dependency

Your RAG system has two critical paths: the model and the retrieval pipeline. You have spent months hardening the model path. You test model outages, model latency, model rate limits. But retrieval failures are different. They are silent. The system does not crash. It returns responses. They are just wrong. Or empty. Or fabricated. The user cannot tell if the failure is in retrieval or reasoning. They just know the answer is bad.

Most teams discover this in production. A vector database pod restarts. Queries return no results. The model generates responses based on zero context. The responses are confident, coherent, and completely ungrounded. Users trust them. Some make decisions based on them. The team does not notice until someone complains.

Chaos engineering for retrieval means deliberately breaking the retrieval pipeline to see what happens. You simulate the vector database going down. You simulate the embedding service timing out. You simulate indexes corrupting or going stale. You watch how your system behaves when retrieval stops working. Then you build the systems that prevent those failures from reaching users.

## Simulating Vector Database Outages

The simplest retrieval chaos experiment is a complete vector database outage. You shut down your Pinecone cluster, kill your Weaviate pods, or block network access to your Qdrant instance. Then you send queries through the system and watch what happens.

In well-designed systems, the application detects the outage immediately. It returns a clear error: "Search is temporarily unavailable. Please try again in a few minutes." The user experience degrades, but gracefully. Users know what is happening. They wait or come back later.

In most systems, something worse happens. The retrieval client times out after thirty seconds. The timeout propagates up through the stack. The user sees a generic 500 error or a spinner that never resolves. Or worse: the system interprets zero results as "no relevant documents" and generates a response anyway. The model hallucinates an answer based on the query alone. The user receives confident misinformation.

Outage simulation reveals how your system handles total retrieval failure. It shows you where timeouts are configured, how errors propagate, and whether your fallback mechanisms actually work. The goal is not to prevent vector database outages. Those will happen. The goal is to ensure that when they do, your system fails in a way that protects users.

## Simulating Embedding Service Failures

Retrieval depends on embeddings. Your user submits a query. You convert it to a vector using an embedding service. You search for similar vectors in your database. If the embedding service is down, you cannot convert the query. You cannot search. The entire pipeline stalls.

Embedding service chaos means simulating failures at the embedding layer. You configure your chaos tool to reject embedding API calls with 503 errors. You inject latency so embedding requests time out. You return malformed responses that break the embedding client. Then you watch how the system handles these failures.

Most systems do not handle them at all. The embedding request fails. The error bubbles up. The user sees a generic failure message. Some systems retry indefinitely, turning a fast failure into a slow one. Some cache the error and serve it to multiple subsequent users until the cache expires.

Better systems detect embedding failures and route around them. If your primary embedding service is OpenAI, your fallback might be Cohere or a locally hosted model. If embeddings fail entirely, you fall back to keyword search or return a clear message: "Search is temporarily limited. Keyword search only." The user gets a degraded experience, not a broken one.

Embedding chaos also tests your retry logic. If embeddings fail intermittently, does your system retry with backoff? Does it fail fast after a reasonable number of attempts? Does it cache failures to avoid hammering a down service? These questions have right answers. Chaos experiments reveal whether you have implemented them.

## Simulating Stale or Corrupted Indexes

Vector databases hold indexes. Indexes become stale. You update your knowledge base, but the index is not refreshed. Users query the old data. Or indexes corrupt. A pod restarts mid-write. The index is left in an inconsistent state. Queries return partial results or nonsense.

Staleness simulation means deliberately serving an outdated index. You roll back your vector database to a snapshot from last week. You query the system and see how it handles old data. Does it detect the staleness? Does it warn the user? Or does it confidently return outdated information?

Corruption simulation is harder. You cannot just corrupt an index in production. But you can simulate corruption by returning malformed query results. Your chaos tool intercepts the vector database response and scrambles the document IDs. The retrieval layer tries to fetch documents that do not exist. Errors propagate. You watch how the system handles missing or invalid document references.

Staleness and corruption are rare. But they happen. Cloud provider failures. Database bugs. Deployment mistakes. When they do, your system should detect them and degrade gracefully. Staleness detection might mean versioning your index and checking that the version matches your knowledge base version. Corruption detection might mean validating document IDs before attempting retrieval. Chaos experiments show you whether these checks exist and whether they work.

## Testing Retrieval Fallbacks

Most RAG systems claim to have fallbacks. "If primary retrieval fails, we fall back to keyword search." "If embeddings fail, we use a cached response." Chaos engineering tests whether those fallbacks actually work.

You simulate a primary retrieval failure and verify that the system switches to the fallback. You check the user experience. Is the fallback fast enough? Does it return reasonable results? Does the user know they are getting fallback behavior? Or does the system silently serve degraded results without warning?

Common fallback failures: the fallback is misconfigured and has never been tested. The fallback exists but is too slow to be useful. The fallback returns results in a different format, breaking downstream parsing. The fallback is triggered correctly but then fails itself, leaving the system with no options. Chaos experiments catch all of these before users do.

Effective fallback testing means simulating not just one failure but multiple cascading failures. Primary retrieval fails. The system falls back to keyword search. Keyword search times out. What happens next? Does the system return cached results? Does it fail fast with a clear error? Or does it hang indefinitely while the user waits?

The best systems have multiple layers of fallback, each with its own timeout and error handling. The worst systems have one fallback that has never been tested and does not work when needed.

## Partial Retrieval Failure

Complete outages are easy to detect. Partial failures are insidious. Your vector database cluster has three nodes. One node fails. Queries still work, but performance degrades. Or some queries succeed and some fail, depending on which shard they hit. Users experience intermittent errors. You see elevated latency but no clear incident.

Partial failure simulation means killing one node in a multi-node cluster. Or throttling traffic to one shard. Or injecting errors into a subset of queries. You create conditions where the system is neither fully up nor fully down. Then you watch how it behaves.

Most systems do not handle partial failures well. They treat any success as full health. Monitoring shows green because some queries are working. But user experience is terrible because half of queries are timing out. The team does not realize there is a problem until support tickets pile up.

Better systems detect partial failures through health checks that test all nodes, not just one. They route traffic away from unhealthy nodes. They raise alerts when error rates exceed thresholds, even if overall availability is above 99 percent. They degrade gracefully, reducing query parallelism or disabling expensive features to keep the system stable.

Partial failure chaos is closer to real production incidents than complete outages. Most failures are partial. Testing for them reveals the gap between "it works in the happy path" and "it works under realistic conditions."

## Chaos for Hybrid Retrieval Systems

Some RAG systems use hybrid retrieval: vector search plus keyword search, or vector search plus SQL queries, or multiple vector databases searched in parallel. Hybrid systems have more failure modes. Each retrieval path can fail independently. The combination logic can fail. The ranking or reranking layer can fail.

Hybrid retrieval chaos means testing each path independently and in combination. You shut down the vector database and verify that keyword search still works. You shut down the SQL database and verify that vector search compensates. You shut down both and verify that the system fails fast with a clear message, not slowly with confusing partial results.

You also test the combination layer. What happens if vector search returns results but keyword search times out? Does the system wait indefinitely for both? Does it return partial results? Does it rerank based on incomplete data and serve low-quality answers?

Hybrid systems promise resilience: if one path fails, the other compensates. But they also introduce complexity. Chaos engineering tests whether the complexity is managed or just hidden. The goal is to ensure that hybrid retrieval actually improves reliability, not just adds more failure modes.

## What Retrieval Chaos Reveals

Retrieval chaos experiments expose assumptions. The assumption that the vector database is always available. The assumption that embeddings are instant. The assumption that indexes are always consistent. The assumption that fallbacks work. In most systems, these assumptions are wrong. Retrieval chaos makes the wrongness visible before it becomes a production incident.

Teams that run retrieval chaos early build resilient systems. They add health checks for embedding services. They implement circuit breakers for vector databases. They test fallbacks in staging before relying on them in production. They monitor retrieval latency and error rates separately from model latency. They set alerts that fire when retrieval degrades, not just when it fails completely.

Teams that skip retrieval chaos learn the hard way. A vector database outage takes down the entire product. An embedding service rate limit causes cascading timeouts. A stale index serves incorrect data for hours before anyone notices. The team spends the weekend debugging, then spends the next month building the resilience they should have built from the start.

Retrieval is the foundation of RAG systems. Chaos engineering tests whether that foundation is solid or brittle. The answer determines whether your system survives its first real failure.

The next chaos experiment introduces a different kind of failure: not broken systems, but slow ones. Latency injection reveals how your system behaves when everything still works — just not fast enough.
