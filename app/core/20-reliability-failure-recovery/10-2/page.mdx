# 10.2 — The Quality SLO: Measuring What Users Actually Care About

Your system is up. Latency is under 500 milliseconds. Availability is at 99.95 percent. Users are furious. The legal chatbot is citing statutes from the wrong jurisdiction. The content moderation system is flagging product descriptions as hate speech. The customer support agent is confidently hallucinating return policies. Every technical metric looks green. The product is broken. This is the AI reliability trap: you can meet every traditional SLO and still deliver a terrible experience because you never measured whether the outputs were correct.

Users do not care if your system is up. They care if it gives them the right answer. A fast wrong answer is worse than a slow right answer. A highly available system that hallucinates is worse than one that admits it does not know. Quality is the dimension that determines whether your AI system is actually reliable, and it is the dimension most teams measure poorly or not at all.

## Quality as the Primary User Concern

For traditional software, availability and latency are the primary reliability concerns. If your database is down, users notice immediately. If your API is slow, users complain. Quality is usually a product concern, not a reliability concern, because the software does what you programmed it to do. If it returns the wrong result, that is a bug, not a reliability issue.

AI systems invert this. Quality is probabilistic, not deterministic. The same input can produce different outputs. The model can be correct on Tuesday and wrong on Wednesday with identical infrastructure. Quality degradation is not always accompanied by latency or availability changes. The system can be technically healthy while functionally useless.

This makes quality the most important reliability dimension for AI. If your availability drops from 99.9% to 99%, users experience a few extra errors per thousand requests. If your quality drops from 95% correct to 85% correct, users experience a 10 percentage point increase in wrong answers. The impact of quality degradation is immediate and severe. Users notice, trust declines, and they stop using the system.

Despite this, most AI teams do not have quality SLOs. They measure latency and availability because those are easy to instrument. They measure quality manually, sporadically, or not at all. They discover quality problems when users complain, not when their monitoring alerts. This is backward. If quality is the dimension users care about most, it should be the dimension you monitor most aggressively.

## Defining Quality Indicators for AI

The hardest part of quality SLOs is defining what quality means in a way you can measure automatically at scale. Quality is subjective, context-dependent, and expensive to evaluate. You cannot manually review every output. You need automated indicators that correlate with human judgment of quality.

The quality indicators you choose depend on your use case. For factual question answering, quality might mean "answer is accurate according to source documents." For summarization, quality might mean "summary captures key points without hallucinations." For classification, quality might mean "label matches expert annotation." For content generation, quality might mean "output passes editorial guidelines and safety checks."

In each case, you need a measurable proxy. Factual QA quality can be proxied by citation accuracy: do generated answers cite passages from the retrieved documents? Summarization quality can be proxied by semantic similarity: is the summary close in embedding space to the source? Classification quality can be measured directly: do predictions match labels? Content generation quality can be proxied by safety filter pass rate and coherence scores.

Good quality indicators share three properties: they correlate with human judgment, they can be computed automatically, and they are sensitive to the degradation modes you care about. Correlation means that when your automated indicator says quality is high, human reviewers agree. Automation means you can measure every request without human effort. Sensitivity means that when quality actually degrades, your indicator detects it.

The challenge is that most quality indicators are imperfect proxies. Citation accuracy measures whether the model quoted the context, not whether the answer is actually correct — the context itself might be wrong. Semantic similarity measures distributional closeness, not logical correctness. Safety filter pass rate measures absence of harmful content, not presence of useful content. You are always trading off measurement cost against measurement accuracy.

The solution is to use multiple indicators in combination and calibrate them against human evaluation regularly. If you track citation accuracy, semantic similarity, and length within bounds, and all three drop simultaneously, you have higher confidence that quality degraded than if only one dropped. If your automated indicators stay stable but human review shows declining quality, your indicators are miscalibrated and need adjustment.

## Quality Measurement Approaches

There are four main approaches to measuring AI quality at scale: automated metrics, model-based evaluation, user feedback, and sampled human review. Each has strengths and weaknesses. Most production systems use a combination.

Automated metrics are rule-based checks that run on every output: does the JSON parse, does the output contain toxic language, is the length within expected bounds, are required fields present, does the answer cite retrieved context. These metrics are cheap, fast, and deterministic. They catch obvious failures but miss subtle quality issues. A medical advice system can pass all automated checks while recommending the wrong treatment.

Model-based evaluation uses a second model to evaluate the first model's output. An LLM-as-judge approach might prompt GPT-5 to score outputs from your production model on accuracy, helpfulness, and safety. Embedding-based approaches measure semantic similarity between generated output and ground truth or retrieved documents. These methods scale better than human review and catch more subtle issues than rule-based metrics, but they inherit the biases and failure modes of the evaluation model. If your judge model hallucinates or has low agreement with humans, your quality SLO is built on sand.

User feedback is the most direct signal: did the user upvote or downvote the response, click the "this was helpful" button, report the output as incorrect, or escalate to a human agent? Feedback metrics are unambiguous — they measure what users actually think — but they are sparse and biased. Only a small fraction of users provide explicit feedback, and they are more likely to report negative experiences than positive ones. You cannot rely on feedback alone because silence does not mean satisfaction.

Sampled human review is the gold standard but the least scalable. You randomly sample a small percentage of outputs, send them to expert reviewers, and track the fraction that pass quality criteria. This gives you ground truth but at high cost. If you sample 1% of requests and each review takes 2 minutes, you need full-time reviewers for any system handling significant volume. Sampled review works best as a calibration mechanism: you use it to validate that your automated metrics are still correlated with human judgment.

## Sampling Strategies for Quality SLOs

You cannot measure quality for every request, so you sample. The question is how to sample in a way that gives you confidence in your quality SLO without reviewing everything.

Random sampling is the simplest approach: select 1% of requests at random and evaluate them. This works well for systems with uniform traffic. If your quality is evenly distributed across users, use cases, and time, random sampling gives you an unbiased estimate. The downside is that it misses rare but critical failures. If 0.1% of requests trigger a severe hallucination bug, you need to review 1,000 samples to see one instance. Random sampling at 1% means you might miss it entirely.

Stratified sampling ensures you evaluate quality across different slices of traffic: high-risk use cases, new users versus returning users, different input lengths, different times of day, different customer tiers. You sample more aggressively from high-risk slices and less from low-risk slices. This catches distributional issues that random sampling would miss. If your model performs well on short inputs but degrades on long inputs, stratified sampling by input length reveals it.

Triggered sampling evaluates requests that meet specific criteria: low confidence scores, long response times, user feedback flags, outputs that fail automated checks, requests from VIP customers, first-time users, or edge cases. This concentrates your review effort on the requests most likely to have quality issues. The downside is that triggered sampling gives you biased estimates — you are looking at the worst outputs, not a representative sample — so you cannot use it alone to estimate overall quality.

Most production systems use a hybrid approach: random sampling for baseline quality estimates, stratified sampling to ensure coverage across important segments, and triggered sampling to catch critical failures quickly. You might randomly sample 0.5% of requests, stratify to ensure at least 50 reviews per use case per week, and trigger additional review for any output flagged by automated checks or user reports.

Sample size matters. If you want to detect a 5 percentage point drop in quality with 95% confidence, you need hundreds of reviews per measurement period. If you only review 20 samples per week, your margin of error is so large that you cannot distinguish 90% quality from 95% quality. You need enough samples that your SLO threshold is statistically meaningful.

## The Quality Proxy Problem

Every automated quality indicator is a proxy for what you actually care about. The proxy is measurable. The actual thing is not. The danger is that you optimize the proxy and degrade the actual outcome.

A customer support chatbot might have a quality SLO of "95% of responses cite at least one retrieved document." This is a proxy for factual accuracy. The assumption is that cited answers are more accurate than uncited answers. But the model can learn to cite irrelevant documents to meet the metric. It can quote a sentence from a help article that has nothing to do with the question. Your SLO stays green while quality degrades.

A summarization system might measure quality as "semantic similarity above 0.85 between summary and source." This is a proxy for faithful summarization. But the model can copy sentences verbatim from the source to maximize similarity, producing summaries that are technically similar but not actually useful. Your SLO is met. The summaries are bad.

A classification system might have a quality SLO of "98% agreement with historical labels." This is a proxy for correctness. But if your historical labels have systematic errors or your input distribution shifts, high agreement with historical labels means you are consistently wrong. You meet your SLO by replicating bad data.

The proxy problem is fundamental to AI quality measurement. You cannot measure true quality at scale, so you measure signals that correlate with quality. The risk is that correlation breaks when the system adapts to the metric. The model, the data, or the process changes in a way that improves the proxy without improving the outcome.

The solution is to treat proxies as indicators, not definitions. Your quality SLO is "95% of responses cite relevant context," not "cite any context." You validate relevance through periodic human review. You track multiple proxies simultaneously, so if one is gamed, others catch it. You refresh your proxies regularly — every quarter, you re-evaluate whether the metrics you are tracking still correlate with what users care about.

## Quality SLO Thresholds

What percentage should your quality SLO target? 99%? 95%? 90%? The answer depends on the consequences of being wrong and the cost of achieving higher quality.

For high-stakes use cases, quality thresholds must be high. A medical diagnosis assistant should target 99% accuracy or better because a wrong answer can harm patients. A legal document review system should target 98% precision because missing a critical clause can cost millions. A financial fraud detection system should target 99.5% precision because false positives freeze legitimate accounts and damage customer trust. High-stakes use cases justify higher quality targets because the cost of failure exceeds the cost of achieving higher quality.

For low-stakes use cases, lower thresholds are acceptable. A content recommendation system might target 85% relevance because a bad recommendation is a minor annoyance, not a disaster. A creative writing assistant might target 90% user satisfaction because creative output is subjective and users can edit results. A spelling correction system might target 95% accuracy because the occasional wrong correction is easily noticed and ignored. Low-stakes use cases allow lower quality targets because the cost of achieving 99% exceeds the value of the improvement.

The key is aligning the threshold with user tolerance for errors. If users tolerate wrong answers 10% of the time without losing trust, a 90% quality SLO is appropriate. If one wrong answer in a hundred causes users to abandon the product, you need a 99% quality SLO. User tolerance depends on task criticality, availability of alternatives, and switching costs. Users tolerate more errors from a free tool than from a paid service. They tolerate more errors for creative tasks than for factual tasks.

Cost also constrains quality thresholds. Achieving 99% quality might require ensembling three models, running expensive verification steps, and maintaining large human review teams. If the incremental value of going from 95% to 99% quality is less than the incremental cost, the higher threshold is not economically rational. You set the threshold where marginal cost equals marginal value.

## Quality vs Availability Tradeoffs

Quality and availability often conflict. The choices that improve one can degrade the other. Managing this tradeoff is central to AI reliability engineering.

Failover to cheaper models improves availability at the cost of quality. If your primary model is down, you can route traffic to a backup model to avoid showing users an error. But the backup model is typically lower quality — otherwise it would be your primary model. Your availability SLO improves. Your quality SLO degrades. The question is whether users prefer a lower-quality answer over no answer at all.

Graceful degradation is another availability-quality tradeoff. When load spikes, you can reduce inference costs by switching to a smaller model, reducing context length, or lowering generation temperature. These changes keep the system responsive but may reduce quality. You are trading quality for availability and latency. Whether this is the right trade depends on whether users value speed over correctness in high-load scenarios.

Returning cached responses improves both availability and latency but risks stale or incorrect answers. If your model is overloaded, you can serve previously generated responses for repeated queries. This works well for static queries but poorly for time-sensitive or context-dependent queries. Your availability and latency SLOs improve. Your quality SLO might degrade if cached answers are no longer accurate.

The resolution is to make the tradeoff explicit in your SLO structure. You might define "99% availability, of which 95% must meet quality thresholds." This allows 4% of requests to return lower-quality responses during degraded states rather than errors. Or you might define separate SLOs for normal and degraded operation: "99% availability and 95% quality under normal load, 98% availability and 90% quality under high load." This acknowledges that quality may degrade when availability is stressed.

## Quality SLOs by Use Case Type

Different use case types require different quality SLO structures. What you measure and how you measure it depends on the task.

For question answering systems, quality SLOs focus on factual accuracy, citation correctness, and completeness. A good QA quality SLO might be "92% of answers are factually correct per human review" or "95% of answers cite at least two relevant sources." You measure by sampling responses, verifying facts against ground truth, and checking that citations support claims.

For classification systems, quality SLOs focus on precision and recall. A content moderation quality SLO might be "98% precision and 85% recall on harmful content detection." You measure by comparing model predictions to expert labels on a held-out test set, refreshed monthly to ensure it represents current production data.

For generation systems, quality SLOs focus on coherence, adherence to instructions, and safety. A creative content generation quality SLO might be "90% of outputs rated coherent and on-topic per human review" and "99% pass safety filters." You measure by sampling generated content, sending it to reviewers, and tracking automated safety check pass rates.

For summarization systems, quality SLOs focus on faithfulness and completeness. A document summarization quality SLO might be "95% of summaries contain no hallucinated facts per expert review" and "90% of summaries include key points identified by experts." You measure by comparing summaries to source documents and to expert-written reference summaries.

For agent systems, quality SLOs focus on task completion and action correctness. A customer service agent quality SLO might be "88% of user requests resolved without escalation" and "95% of actions taken are appropriate per review." You measure by tracking escalation rates and sampling agent action logs for review.

The structure of your quality SLO should match the failure modes of your use case. If the biggest risk is hallucination, measure citation accuracy. If the biggest risk is bias, measure fairness metrics. If the biggest risk is incoherence, measure readability and structure. Quality SLOs are not one-size-fits-all. They are tailored to the specific ways your system can fail and the specific outcomes users care about.

Users experience reliability as quality first, uptime second. Your SLOs must reflect that priority. The next subchapter addresses the complexity that arises when quality, latency, and cost constraints all interact: multi-dimensional SLOs and the tradeoffs they force.
