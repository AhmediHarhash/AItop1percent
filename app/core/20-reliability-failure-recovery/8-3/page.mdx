# 8.3 — The Incident Timeline: Reconstructing What Happened

Three engineers sat in a conference room trying to reconstruct the incident that happened 36 hours earlier. When did the alert fire? One engineer said 2:47am. Another said 2:52am. The third pulled up PagerDuty — the alert fired at 2:43am, acknowledged at 3:17am, resolved at 4:58am. Who deployed the change that triggered it? One engineer said it was an automatic deployment. Another said it was manual. The deployment logs showed a manual deploy at 1:14am by an engineer who was not in the room and had already left for vacation. What percentage of traffic was affected? One engineer said 10%. Another said 40%. The monitoring dashboard showed 23% of requests failed between 2:43am and 3:55am, then dropped to 8% until resolution. Every detail they remembered was wrong.

Human memory is unreliable under stress. When an incident is happening, you are not thinking about documentation — you are thinking about mitigation. You try five things that don't work, then something works, and by the time the incident resolves you remember the thing that worked but not the four failures before it. You remember the moment you realized the severity but not the 20 minutes you spent investigating the wrong hypothesis. You remember the decisions that feel important in retrospect but not the information you had when you made them. The only way to accurately reconstruct an incident is through timestamped data, not memory.

## Why Timelines Matter for Learning

The timeline is the foundation of every post-mortem. Without an accurate timeline, you cannot identify contributing factors, you cannot understand why people made the decisions they made, you cannot calculate mean time to detect or mean time to recover, and you cannot learn what to do differently next time. A post-mortem without a timeline is just a list of opinions.

The timeline shows **what information people had when they acted**. Thirty minutes into an incident, the on-call engineer decides to roll back. In retrospect, that was the right decision — but at the time, they did not know if it was right. They knew: the alert said latency spiked, the monitoring dashboard showed 15% request failures, the recent changes list showed three deployments in the last two hours, and the runbook said "if latency exceeds two seconds, consider rollback." They did not know: which deployment caused it, what the root cause was, or whether rollback would fix it. The timeline documents the uncertainty they operated under, which helps you design better runbooks and better monitoring.

The timeline shows **where the team wasted time**. The incident lasted three hours. The fix took 10 minutes. Where did the other two hours and 50 minutes go? The timeline shows: 30 minutes debugging the wrong subsystem because the alert was ambiguous, 45 minutes waiting for a senior engineer to respond because the on-call engineer wasn't sure if they had authority to roll back, 20 minutes rolling back the wrong deployment because the change log was unclear, 40 minutes waiting for the rollback to propagate, 15 minutes discovering the rollback didn't work, 10 minutes identifying the actual problem, 10 minutes applying the fix, 30 minutes waiting for monitoring to confirm resolution. Each of these delays is a learning opportunity. Better alerts reduce the 30 minutes of misdirection. Clearer escalation policies eliminate the 45 minutes of waiting. Better deployment tooling cuts the 40-minute propagation delay.

The timeline shows **cascading failures and dependencies**. AI incidents are rarely isolated. The model latency spike causes the API gateway to time out, which causes the load balancer to mark the service unhealthy, which causes traffic to shift to a different region, which causes that region to overload, which cascades to a different service. The timeline shows the sequence: model latency at 2:43am, gateway timeouts at 2:47am, region failover at 2:51am, secondary region overload at 2:54am, complete outage at 2:58am. Each step took minutes. If you had caught it at the first step, you could have prevented the cascade.

The timeline shows **what worked**. Post-mortems focus on failures, but timelines also document successes. The monitoring detected the issue within 60 seconds. The on-call engineer acknowledged within five minutes. The escalation process brought the right expert into the war room within 15 minutes. The rollback executed cleanly. These are process wins. The timeline shows which parts of your incident response are functional and should be reinforced, not just which parts are broken and need fixing.

## Timeline Construction Techniques

Building an accurate incident timeline is investigative work. You are reconstructing a sequence of events from fragmentary, imperfect data sources. The process is methodical, time-consuming, and non-optional.

Start with **automated data sources**. These are timestamped, immutable, and objective. Pull monitoring dashboards showing when metrics spiked. Pull alerting logs showing when alerts fired and who acknowledged them. Pull deployment logs showing when changes were deployed and by whom. Pull application logs showing error rates, latency percentiles, and throughput. Pull infrastructure logs showing CPU, memory, and network utilization. Pull load balancer logs showing traffic distribution and failover events. Every system you operate should emit timestamped logs. If it doesn't, you will not be able to reconstruct incidents accurately.

Then add **human communication logs**. Slack threads, PagerDuty comments, incident channel messages, email threads. These show what people knew, what they were thinking, what they tried, and what they decided. Humans timestamp their messages, so you can interleave communication with system events. At 2:51am, the monitoring showed latency at three seconds. At 2:53am, the on-call engineer posted in Slack: "seeing major latency spike, investigating." At 2:57am: "looks like the model is hanging on complex queries." At 3:04am: "trying rollback." The communication log shows the engineer's mental model evolving in real time.

Then fill gaps with **interviews**. Some decisions happen on Zoom calls that aren't recorded. Some debugging happens on a local terminal that doesn't log to a central system. Some realizations happen in someone's head before they communicate them. Schedule 30-minute interviews with everyone involved in the incident response within 24 hours while their memory is fresh. Ask: When did you first notice the issue? What did you think was happening? What did you try? What information did you wish you had? Record the interviews or take detailed notes. You will not remember the details a week later.

Then synchronize timestamps across systems. Your monitoring dashboard uses UTC. Your deployment system uses Pacific Time. Your Slack messages use the local timezone of whoever sent them. PagerDuty uses UTC but displays in local time depending on who is viewing it. Convert everything to UTC and document it clearly in the timeline. Mixing timezones is how you end up thinking a deployment happened before the alert when it actually happened after.

Then build the timeline document. Use a table or a chronological list. Each entry should have: timestamp in UTC, event description, data source, and any relevant links or quotes. The format should be scannable. Someone should be able to read the timeline in five minutes and understand the incident's arc: when it started, when it was detected, when mitigation began, when it resolved.

Example format:

2026-02-08 02:43:17 UTC — Model inference latency p99 spikes to 3.2 seconds. Source: Datadog dashboard. Link: dashboard URL.

2026-02-08 02:43:42 UTC — PagerDuty alert fires: "Model latency critical threshold exceeded." Alert acknowledged by Sarah Chen at 02:48:19 UTC.

2026-02-08 02:51:33 UTC — Sarah posts in Slack incident channel: "investigating latency spike, checking recent deployments." Link: Slack message.

2026-02-08 03:04:15 UTC — Sarah initiates rollback of deployment from 01:14 UTC. Source: deployment log.

2026-02-08 03:42:09 UTC — Rollback complete. Latency drops to 1.1 seconds. Source: Datadog dashboard.

2026-02-08 03:58:44 UTC — Sarah marks incident resolved in PagerDuty.

This is the skeleton. You will add detail in the post-mortem narrative, but the timeline gives everyone a shared foundation of fact.

## Data Sources for AI Incident Timelines

AI incidents require data sources beyond traditional software incident timelines because the failure modes are different. You need to reconstruct not just what the system did but what the model did, what inputs it received, and how its behavior changed over time.

**Model inference logs** show which inputs the model received, what outputs it generated, how long inference took, and what confidence scores or probabilities it returned. If the incident involved hallucinations or incorrect outputs, you need the actual inputs and outputs to analyze them. If the incident involved latency spikes, you need the inference duration for each request. Most inference serving platforms log this by default, but log retention is often short — 24 hours, maybe seven days. Pull the logs immediately after the incident before they expire.

**Evaluation metrics over time** show whether model quality degraded before the incident. Pull accuracy, precision, recall, F1, or whatever task-specific metrics you track, broken down by hour or day for the week leading up to the incident. If quality was slowly degrading for three days before the incident, the root cause might be data drift or model staleness, not a sudden deployment change. If quality was stable and then dropped suddenly at 2:43am, the root cause is more likely a deployment, configuration change, or infrastructure issue.

**Traffic distribution data** shows whether the incident correlated with changes in input patterns. Pull histograms of input length, topic distribution, user demographics, request rate, or any feature you track. If the incident started exactly when your product launched a new feature that generated different input patterns, you have evidence that the model did not generalize to the new distribution.

**Retrieval system logs** for RAG architectures show what documents were retrieved for each query, what the relevance scores were, and how long retrieval took. If the incident involved incorrect answers, the retrieval logs show whether the model received correct context. If retrieval was broken or slow, that is a contributing factor. If retrieval was correct but the model ignored the context, that is a different contributing factor.

**Guardrail and filter logs** show which safety mechanisms activated and which inputs or outputs they blocked. If the incident involved harmful outputs reaching users, the filter logs show whether the filters saw the outputs and failed to block them, or whether the outputs bypassed the filters entirely. This distinction changes the fix.

**User behavior data** shows how users reacted during the incident. Pull metrics on user retries, escalations to human support, session abandonment, and any in-product feedback or thumbs-down votes. If users immediately noticed the degradation and abandoned sessions, the incident had high user impact. If users didn't notice, the incident had low user impact even if technical metrics looked bad. User impact is what justifies incident severity, not technical metrics alone.

**Infrastructure metrics** show whether the AI system was constrained by infrastructure. Pull CPU utilization, memory usage, GPU utilization, network throughput, disk I/O, and queue depths for every service involved in the inference path. If the model was running on overloaded GPUs, latency spikes are expected. If the model was running on idle GPUs, the latency spike came from the model logic itself, not infrastructure.

Pull all of these data sources within 24 hours of the incident. Logs rotate. Metrics aggregate. Data disappears. You cannot reconstruct the timeline a week later if the underlying data is gone.

## Correlating Events Across Systems

AI systems are distributed. An incident that looks like a model failure might actually be caused by the retrieval system, the API gateway, the load balancer, the upstream data source, or the client application. The timeline has to correlate events across all of these systems to show causation, not just coincidence.

Start with the **alert timestamp** as your reference point. That is when the system detected the problem. Work backward from the alert to find the initiating event. The alert fired at 2:43am. The monitoring shows latency spiked at 2:43am. The deployment logs show a new model was deployed at 2:38am. The retrieval system logs show no changes. The infrastructure logs show no load spike. The timeline now has a hypothesis: the deployment at 2:38am caused the latency spike five minutes later. Check the hypothesis by looking at inference logs before and after 2:38am. If the latency spike correlates perfectly with the new model version, the hypothesis is confirmed.

Sometimes causation is not immediate. A configuration change at 1:00am causes no immediate problems, but as traffic ramps up between 2:00am and 3:00am, the system hits a resource limit and degrades. The timeline shows: configuration change at 1:00am, traffic increase starting at 2:00am, latency degradation starting at 2:30am, alert at 2:43am. The root cause is the configuration change, but it took 90 minutes to manifest because it required a traffic precondition.

Sometimes causation is indirect. The model did not change, but an upstream data source changed, so the retrieval system started returning different documents, so the model started generating different outputs, so the output quality degraded. The timeline shows: upstream API change at 1:14am, retrieval cache invalidation at 1:18am, retrieval quality degradation starting at 1:20am, model output degradation starting at 1:25am, alert at 2:43am. The alert came 90 minutes after the root cause because the degradation was gradual and took time to cross the alerting threshold.

The hard part is distinguishing **correlation from causation**. Two things happened at the same time. Did one cause the other, or did they coincide? The deployment happened at 2:38am. The latency spike happened at 2:43am. The traffic rate also increased at 2:40am. Was the latency spike caused by the deployment, or by the traffic increase, or by both? You need counterfactual reasoning. If the deployment had not happened, would the traffic increase alone have caused a latency spike? Look at historical data. Has this traffic level caused latency spikes before? No? Then the deployment is the primary cause, and the traffic increase is a contributing factor.

The timeline should explicitly label causation confidence. "2:38am — model deployment — **likely cause** of latency spike based on temporal correlation and absence of other changes." "2:40am — traffic increase — **contributing factor** based on historical analysis showing this traffic level did not cause issues on previous model version." "1:14am — upstream API change — **unrelated** based on lack of correlation between API response times and model latency." Do not pretend certainty when you are inferring causation from correlation.

## Handling Timeline Gaps

You will not have complete data. Logs will be missing. Monitoring will have gaps. People will forget details. The timeline will have holes. Your job is to acknowledge the gaps, document what you know, and avoid fabricating details to make the narrative cleaner.

**Missing log data** happens because log retention is short, because a service wasn't logging, or because the incident itself disrupted logging infrastructure. Document the gap explicitly. "2:43am to 2:58am — retrieval system logs unavailable due to log aggregator outage. We cannot confirm retrieval behavior during this period." Do not write "retrieval system was functioning normally" when you don't have data. That is fabrication. Write "retrieval system status unknown."

**Ambiguous monitoring data** happens because metrics are aggregated, because dashboards were misconfigured, or because the failure mode was not being measured. Document the ambiguity. "Model accuracy data unavailable during incident because eval pipeline only runs hourly, and incident lasted 90 minutes. We infer accuracy degradation based on user feedback and output samples, but we cannot quantify it precisely." This tells future readers that you have a hypothesis, not proof.

**Conflicting human recollections** happen because people misremember details under stress. Resolve conflicts using timestamped data. If two engineers disagree about when the rollback started, check the deployment logs. If the logs are missing, document the conflict. "Sarah recalls initiating rollback at approximately 3:00am. David recalls rollback at approximately 3:15am. Deployment logs unavailable due to retention policy. Best estimate: 3:00am to 3:15am." Do not pick one person's memory arbitrarily. Document the range.

**Unknown durations** happen because the start or end of an event is unclear. When did the model degradation actually start? The alert fired at 2:43am, but the degradation might have started earlier and taken time to cross the threshold. Check monitoring data leading up to the alert. If latency was normal at 2:30am and elevated at 2:43am, the degradation started sometime between 2:30am and 2:43am. Document the range. "Model latency degradation onset: 2:30am to 2:43am based on monitoring granularity." If you cannot narrow it further, accept the uncertainty.

**Unobservable events** happen when decisions or realizations occur in someone's head before they communicate them. The engineer noticed the problem at 2:50am but didn't post in Slack until 2:58am. What were they doing for eight minutes? Interview them. They were checking dashboards, running database queries, reading recent commit messages — all investigative work that was not logged. Add these events to the timeline with a note that they are reconstructed from interviews, not logs. "2:50am — Sarah noticed latency spike while reviewing monitoring dashboard. Source: interview." This explains the delay between detection and communication.

The goal is not a perfect timeline. The goal is an honest timeline that clearly distinguishes between facts, inferences, and gaps. A timeline with gaps is useful if the gaps are documented. A timeline that fills gaps with guesses is misleading.

## Multiple Concurrent Incidents

Sometimes two incidents happen at the same time. The model latency spikes. The retrieval system also goes down. Are these related, or is it bad luck? The timeline has to track both incidents separately and then analyze whether they are causally linked or independent.

**Independent incidents** happen by coincidence. The model latency spike was caused by a deployment. The retrieval system outage was caused by a database failover. They happened 10 minutes apart, but they are unrelated. The timeline shows both, but the post-mortems are separate. Each incident gets its own root cause analysis, its own action items, its own learnings.

**Cascading incidents** happen when one incident causes another. The model latency spike caused the API gateway to timeout, which caused the load balancer to mark the service unhealthy, which caused a failover, which caused a traffic surge on the backup service, which overloaded the retrieval system. These are causally linked. The timeline shows the cascade explicitly. "2:43am — model latency spike. 2:47am — API gateway timeouts due to model latency. 2:51am — load balancer failover triggered by timeouts. 2:54am — retrieval system overload due to failover traffic surge." The root cause is the initial model latency spike. Everything else is a cascading failure. The post-mortem addresses the root cause and also asks why each layer of the system did not contain the failure instead of propagating it.

**Correlated incidents** happen when a common cause affects multiple systems simultaneously. A network partition affects both the model serving infrastructure and the retrieval system at the same time. The timeline shows both failures starting simultaneously. The root cause analysis identifies the common cause. The action items address the shared vulnerability — maybe better network redundancy, maybe better partition tolerance in both systems.

The danger is **assuming correlation when incidents are independent**. Two things broke at the same time, so the team assumes they are related and wastes time looking for a connection that doesn't exist. The timeline helps here. If the two failures are truly causally linked, the timeline will show a sequence: A happens, then B happens shortly after, and B's failure characteristics correlate with A's behavior. If the two failures are independent, the timeline will show them starting simultaneously or with no causal pathway between them, and their failure characteristics will be uncorrelated.

When you have multiple concurrent incidents, track them in separate sections of the timeline until you understand the relationship. Do not merge them prematurely into a single narrative. Once you have established causation or independence, you can write a unified post-mortem that explains the relationship or separate post-mortems if the incidents are unrelated.

## Timeline Visualization and Sharing

The timeline exists to be read. It should be scannable, accessible, and linked from the post-mortem document. The format you choose affects whether people use it.

**Tabular timelines** work well for incidents with clear, sequential events. A table with columns: timestamp, event, source, owner, notes. Each row is one event. People can scan the timestamp column to understand the sequence. People can scan the event column to understand what happened. People can click the source column to see the underlying data.

**Narrative timelines** work well for complex incidents where events need explanation. Write the timeline as prose, but prefix every sentence or paragraph with a timestamp. "2:43am — The model latency alert fired. The on-call engineer, Sarah Chen, was notified via PagerDuty. She acknowledged the alert at 2:48am after waking up and checking her phone. 2:51am — Sarah reviewed the monitoring dashboard and saw that p99 latency had spiked to 3.2 seconds, well above the 500ms baseline..." This format is more readable but harder to scan.

**Visual timelines** work well for incidents with overlapping events or cascading failures. Use a tool like a Gantt chart or a swimlane diagram. The horizontal axis is time. Each system or component gets its own row. Events are plotted as boxes or markers. Causation is shown with arrows. This makes cascading failures visually obvious: you see the model latency spike, then an arrow to the API gateway timeouts, then an arrow to the load balancer failover, then an arrow to the retrieval system overload.

**Interactive timelines** work well for very complex incidents where different stakeholders care about different layers. Build a web page or dashboard where people can filter by system, by severity, by event type. Engineering wants to see every deployment and configuration change. Product wants to see when users noticed. Executives want to see when revenue impact started and when it ended. An interactive timeline lets everyone extract the view they need.

Regardless of format, the timeline should be **linked from the post-mortem document** so people can dive into details without cluttering the narrative. The post-mortem narrative summarizes: "The incident began at 2:43am when model latency spiked to 3.2 seconds. The on-call engineer acknowledged the alert within five minutes and initiated a rollback at 3:04am. The system recovered at 3:42am." The timeline has every minute-by-minute detail. Readers who want the summary read the post-mortem. Readers who want the full reconstruction read the timeline.

The timeline should be **version-controlled**. As you discover new information — someone finds a log you didn't know existed, an interview reveals a detail you missed — update the timeline and document what changed. "Updated 2026-02-10: added retrieval system logs from 2:43am to 2:58am, recovered from backup storage. Timeline now confirms retrieval latency spiked at 2:46am, three minutes after model latency spike." This shows the timeline is a living document, not a one-time artifact.

The timeline is not glamorous. It is not high-level strategy. It is painstaking investigative work that reconstructs what actually happened from imperfect data. But it is the foundation of every other learning activity. Without an accurate timeline, your root cause analysis is speculation, your action items are guesses, and your learnings are anecdotes. With an accurate timeline, you have facts — and facts are what turn incidents into reliability improvements.

---

Next: 8.4 — Identifying Systemic vs Episodic Failures
