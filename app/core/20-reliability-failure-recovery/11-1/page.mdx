# 11.1 — On-Call Design for AI Teams

The page went out at 2:47 AM. The on-call engineer woke up, opened the incident dashboard, saw "Model degradation — content policy violations up 340 percent" — and had no idea what to do. The runbook said "check model version and rollback if needed." But which metrics indicated whether rollback was safe? Which stakeholders needed to approve a rollback? How do you even rollback a fine-tuned model served through a provider API? The engineer escalated to the team lead, who escalated to the VP of Engineering, who called an emergency meeting at 4 AM. By the time the team understood the incident and executed a fix, six hours had passed. The root cause was not technical complexity — it was organizational unpreparedness. The on-call rotation was designed for traditional software incidents, not AI system failures.

On-call for AI systems is not the same as on-call for web services. The failure modes are different. The diagnostic tools are different. The escalation paths are different. The skills required are different. Teams that copy-paste their traditional on-call model onto AI systems discover this gap during their first serious incident. Designing effective on-call for AI requires understanding what makes AI incidents unique and building rotations, tooling, and runbooks specifically for those characteristics.

## How AI On-Call Differs from Traditional On-Call

Traditional on-call handles infrastructure failures, deployment issues, database outages, traffic spikes. The engineer pages the database team, rolls back the bad deploy, scales up the cluster, fixes the config error. The failure modes are well-understood. The diagnostic tools are mature. The fixes are often procedural.

AI on-call handles model degradation, content policy violations, retrieval failures, fine-tuning regressions, provider outages, cost spikes, data leakage incidents. The failure modes are often ambiguous. Is this a model problem, a data problem, a prompt problem, a retrieval problem? The diagnostic tools are immature. There is no "top" command for models. The fixes often require judgment, not just procedure. Should you rollback the model? Should you tighten the content filter? Should you disable a feature? These are product decisions disguised as engineering decisions.

The skills required are different. Traditional on-call engineers need to understand distributed systems, databases, networking, deployment pipelines. AI on-call engineers need to understand model behavior, eval metrics, prompt engineering, retrieval pipelines, content policy, and when a technical issue has legal or reputational implications. They need to know when an incident is an engineering problem and when it is a cross-functional crisis.

The timelines are different. A database outage is usually obvious within seconds. Model degradation can be subtle and gradual. Your content policy violation rate might drift from 0.02 percent to 0.18 percent over three days before anyone notices. By the time the alert fires, the damage might already be done. On-call for AI requires proactive monitoring and pattern recognition, not just reactive incident response.

## On-Call Rotation Design

A sustainable AI on-call rotation requires enough people to avoid burnout, enough expertise to handle incidents effectively, and enough documentation to make the rotation teachable. Most teams start with one or two people carrying the entire load. This works for three months, then someone quits. Build the rotation to scale from day one.

**Rotation size.** A healthy rotation has at least four people, preferably six to eight. Fewer than four means each person is on-call too frequently. More than eight dilutes expertise and makes knowledge transfer harder. If your team is smaller than four, consider sharing on-call with another team or hiring specifically for reliability coverage.

**Rotation length.** Most AI teams use one-week rotations. Long enough to build context, short enough to avoid exhaustion. Some teams use two-week rotations, which reduces handoff overhead but increases burnout risk. Daily rotations are too short — you spend all your time on handoffs. Monthly rotations are too long — one bad month destroys someone's life.

**Primary and secondary.** Always have a secondary on-call. The primary handles the first page. The secondary handles escalations when the primary needs help or when the incident requires more expertise. The secondary also provides coverage if the primary is unavailable. Single-person on-call means your engineer cannot take a shower without worrying about missing a page.

**Skill distribution.** Your rotation should include people with different expertise. At least one person should deeply understand model evaluation. At least one should understand retrieval pipelines. At least one should understand provider APIs and billing. At least one should understand content policy and legal implications. Not every person needs every skill, but the rotation as a whole needs coverage. Document who has which expertise so the primary knows who to escalate to.

**Compensation and recognition.** On-call is labor. Compensate it. Most companies pay on-call stipends — a flat amount per week on-call, plus additional pay for hours spent on incidents. Typical stipends range from 500 dollars to 2,000 dollars per week, depending on company size and incident frequency. Do not rely on "we are all in this together" sentiment. Uncompensated on-call breeds resentment and attrition.

## Required Skills for AI On-Call

Not everyone on your team needs to be on-call. On-call requires a specific skill set. Define the prerequisites clearly so people know what they need to learn before joining the rotation.

**Core technical skills.** On-call engineers must understand your monitoring stack, your incident management platform, your deployment pipeline, your rollback procedures. They need access to production logs, metrics dashboards, and eval dashboards. They need to know how to read eval metrics and interpret model behavior signals. They need to understand your circuit breaker logic, your fallback strategies, and your provider API retry policies.

**Model evaluation skills.** On-call engineers must know how to run spot-check evals during an incident. If precision drops from 94 percent to 78 percent, they need to look at sample outputs and determine whether this is a content policy issue, a retrieval issue, or a model behavior issue. They need to know which eval metrics matter most and which are lagging indicators.

**Cross-functional awareness.** On-call engineers need to know when an incident is not just an engineering problem. If your model starts generating content that violates your terms of service, that is a legal and product problem, not just a technical problem. If your cost spikes from 12,000 dollars per day to 140,000 dollars per day, that is a finance problem. If your model leaks PII, that is a compliance and PR problem. On-call engineers are often the first people to see these patterns. They need to recognize when escalation beyond engineering is required.

**Communication skills.** On-call engineers write incident updates, coordinate with other teams, explain technical issues to non-technical stakeholders. During a major incident, your VP of Product will ask "what happened and when will it be fixed?" The on-call engineer needs to answer clearly, without jargon, without deflecting.

## On-Call Tooling and Access

Your on-call engineer needs specific tools and access to respond effectively. If they have to ask for permissions during an incident, you have already lost time.

**Monitoring dashboards.** On-call engineers need access to all production monitoring dashboards — infrastructure metrics, model behavior metrics, eval metric trends, cost dashboards, content policy violation dashboards. They should not have to hunt for dashboards during an incident. Provide a single landing page that links to everything.

**Incident management platform.** Use a dedicated incident management platform — PagerDuty, Opsgenie, VictorOps, or similar. The platform should page the on-call engineer via multiple channels — phone call, SMS, push notification. The platform should track incident timelines, escalations, and post-incident reviews. Do not rely on email or Slack alone. People miss messages.

**Production access.** On-call engineers need read access to production logs, production databases, and production configuration. They often need limited write access — ability to toggle feature flags, adjust rate limits, trigger rollbacks. Define the access scope carefully. Too little access means they cannot fix incidents. Too much access increases security risk.

**Runbooks and playbooks.** On-call engineers need runbooks for common incidents and playbooks for complex scenarios. Runbooks are step-by-step instructions. Playbooks are decision trees. Both should live in your incident management platform or a dedicated wiki, not in someone's head.

**Communication channels.** On-call engineers need a dedicated incident channel in Slack or Teams where they can coordinate with other teams. The channel should have clear membership — engineering leads, product leads, support leads, legal if needed. During a major incident, this channel becomes the war room.

**Provider support contacts.** On-call engineers need escalation contacts for your model providers — OpenAI, Anthropic, Google, AWS, Azure. Not generic support emails — direct escalation paths for production outages. Negotiate these during your vendor contract.

## Runbook Requirements for AI Incidents

A runbook is not a novel. It is a checklist. Your on-call engineer should be able to follow it at 3 AM with minimal cognitive load.

**Incident detection.** Each runbook starts with detection — what alert fired? What metrics are abnormal? What user-facing behavior changed? The runbook should link directly to the relevant dashboards and logs.

**Initial triage.** Each runbook includes triage steps — is this a provider outage? Is this a deployment issue? Is this a model degradation issue? Is this a data issue? Triage determines the response path.

**Diagnostic steps.** Each runbook includes specific diagnostic commands or queries. "Check the last eval run." "Compare output samples from the last 24 hours to baseline." "Check the retrieval pipeline cache hit rate." "Query the content policy violation log for the last hour." Do not write "investigate the model." Write exactly what to investigate and how.

**Mitigation options.** Each runbook lists mitigation options in priority order. First try the least disruptive fix. If that does not work, escalate to the next option. For model degradation: first check if the provider is having issues, then check if a recent fine-tuning deploy changed behavior, then check if retrieval is returning bad context, then consider rollback.

**Escalation criteria.** Each runbook defines when to escalate. "If precision remains below 80 percent for more than 15 minutes, escalate to the model team lead." "If cost exceeds 50,000 dollars in a single hour, escalate to finance and engineering leadership." "If the model generates content that violates terms of service, escalate to legal immediately."

**Rollback procedures.** Each runbook includes rollback steps if applicable. "To rollback the model, run this script with the previous model version ID." "To disable the feature, toggle this feature flag." "To switch to the fallback model, update this config value and restart the service." Rollback should be a one-command operation, not a ten-step archaeological dig.

**Verification steps.** Each runbook includes verification — how do you know the fix worked? "Run the eval suite and confirm precision is above 90 percent." "Check the cost dashboard and confirm spend is back to baseline." "Spot-check ten user queries and confirm outputs meet policy."

## On-Call Handoff Procedures

Handoff is where incidents fall through the cracks. The outgoing on-call engineer thinks the issue is resolved. The incoming on-call engineer does not know it was happening. Three days later, the issue recurs and nobody remembers the context.

**Synchronous handoff.** Schedule a 15-minute handoff call at the start and end of each rotation. The outgoing engineer walks through active incidents, recent incidents, and anything that looks suspicious. The incoming engineer asks questions. This is not optional. Asynchronous handoff via written notes is not enough.

**Handoff document.** Maintain a shared handoff document — a running log of incidents, near-misses, and system health observations. Each on-call rotation adds to this document. Over time, it becomes a living history of your system's reliability patterns.

**Open incidents.** If there are any open incidents at handoff, the outgoing and incoming engineers should jointly review the incident timeline, the current mitigation status, and the escalation state. The incoming engineer becomes the new incident commander unless explicitly decided otherwise.

**Silent issues.** The most important part of handoff is the stuff that did not trigger an alert but looked weird. "The eval metrics dropped slightly on Tuesday but recovered by Wednesday." "The cost spiked for two hours but stayed within budget." These patterns often precede major incidents. Share them.

## Compensation and Recognition

On-call is work. Treat it as work. Compensate it, recognize it, and never take it for granted.

**On-call stipend.** Pay a flat weekly stipend for being on-call, regardless of whether incidents occur. The stipend compensates for availability — the fact that the engineer cannot travel, cannot drink, cannot fully disconnect. Typical stipends range from 500 dollars for low-incident systems to 2,000 dollars for high-incident systems.

**Incident pay.** Pay additional compensation for time spent on incidents. Some companies pay hourly rates. Some pay flat amounts per incident. Some pay time-and-a-half for weekend or overnight incidents. The key is that incident response time is compensated, not absorbed into "salaried work."

**Time-off recovery.** If an engineer spends significant time on incidents during an on-call week, give them recovery time off the following week. An engineer who worked 20 incident hours over a weekend should not be expected to work a full week afterward.

**Career recognition.** On-call is often invisible to promotion committees. Make it visible. Track on-call participation, incident response quality, and runbook contributions. Include this in performance reviews and promotion packets. Engineers who keep the system running deserve the same recognition as engineers who build new features.

## On-Call Metrics

Measure on-call effectiveness so you can improve it. Do not measure punitively — use metrics to identify process gaps, not to blame individuals.

**Time to acknowledge.** How long does it take the on-call engineer to acknowledge an alert? Most teams aim for under five minutes. Longer times suggest alert fatigue, unclear escalation, or inadequate tooling.

**Time to mitigation.** How long does it take to mitigate the incident — not fully resolve it, but stop the bleeding? Most teams aim for under 30 minutes for high-severity incidents. Longer times suggest missing runbooks, unclear procedures, or insufficient access.

**Incident recurrence rate.** How often does the same incident type recur? High recurrence suggests you are treating symptoms, not root causes. Track recurrence by incident category — model degradation, retrieval failures, cost spikes — and prioritize root cause fixes for the most common categories.

**Runbook coverage.** What percentage of incidents have runbooks? Aim for 80 percent or higher. If a significant number of incidents have no runbook, your on-call engineers are improvising too often.

**Escalation rate.** How often does the primary on-call engineer need to escalate? Low escalation rates suggest good runbook coverage and training. High escalation rates suggest either inadequate training or genuinely complex incidents that require multiple people.

**On-call load distribution.** Are incidents evenly distributed across the rotation, or does one person handle most of them? Uneven distribution suggests either bad luck or skill gaps. Track incidents per person per rotation and investigate imbalances.

**Burnout signals.** Track engineer retention and satisfaction within the on-call rotation. If people leave the rotation after three months, if survey scores drop, if people start declining to join the rotation — you have a burnout problem. Fix it before you lose your best engineers.

Your on-call design determines whether incidents are resolved quickly or slowly, whether your engineers burn out or build sustainable practices, whether your system reliability improves over time or stagnates. Invest in on-call design the same way you invest in architecture design. Both are infrastructure that everything else depends on.

Next, we cover escalation paths — when and how to escalate AI incidents beyond the on-call engineer.
