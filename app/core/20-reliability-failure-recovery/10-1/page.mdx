# 10.1 — Defining SLOs for AI Systems: Beyond Uptime

In October 2025, a B2B SaaS company reported 99.97% uptime for their AI-powered customer support routing system. Leadership celebrated. Then they ran the analysis: of the queries that did get responses, 34% were routed to the wrong team, and average resolution time had doubled. The system was up. It just wasn't working. Their SLO measured availability when users cared about accuracy. They spent three months redesigning their entire reliability measurement framework because they had optimized for the wrong thing.

Uptime is not enough for AI systems. A model can respond instantly with hallucinated data. A retrieval system can return documents with perfect latency but zero relevance. A classification system can label every input as the majority class and technically succeed at high throughput. The system is up, the API returns 200, and the user experience is garbage. Traditional reliability engineering focuses on availability — did the service answer? AI reliability engineering requires measuring whether the service answered correctly.

## The Three Dimensions of AI Reliability

AI systems operate across three reliability dimensions simultaneously: availability, latency, and quality. Availability is binary — did you get a response? Latency is continuous — how fast? Quality is probabilistic — was the response correct, useful, safe? Traditional SLOs handle the first two. The third dimension is what makes AI reliability different and harder.

Availability SLOs for AI look like traditional service SLOs: "99.9% of requests return a valid response within timeout." This measures whether your infrastructure is functioning. If your model serving layer crashes, your load balancer fails, or your API gateway goes down, availability drops. This dimension is necessary but insufficient.

Latency SLOs measure speed: "95th percentile response time under 800 milliseconds." For AI systems, latency includes model inference time, retrieval time for RAG systems, tool execution time for agents, and any post-processing. Latency matters enormously for user experience, but a fast wrong answer is not reliable.

Quality SLOs measure correctness: "98% of responses pass human review for factual accuracy" or "92% of classifications match expert labels." This is the dimension traditional reliability engineering never had to address. Your database either returned the data or it didn't. Your API either processed the request or it didn't. AI systems return something either way — the question is whether that something is right.

These three dimensions interact in ways that traditional SLO frameworks were not designed to handle. You can improve availability by adding failover to a cheaper, faster, lower-quality model. You can improve latency by reducing context length and degrading quality. You can improve quality by adding multiple verification passes and destroying latency. Every reliability decision in AI is a tradeoff between dimensions, and your SLO structure must make those tradeoffs explicit.

## What Makes a Good AI SLO

A good AI SLO is measurable in production, meaningful to users, and achievable with reasonable investment. Measurable means you can instrument it without human-in-the-loop for every request — you need automated signals that correlate with quality. Meaningful means users notice when you violate it. Achievable means you can hit the target without infinite cost or architectural rewrites every sprint.

The measurability constraint is why many teams start with proxy metrics instead of direct quality measures. You cannot manually review every response in a system handling millions of requests per day. You need automated quality indicators: semantic similarity to retrieved documents, confidence scores above thresholds, structured output validation, length within expected bounds, toxicity detector pass rates, internal consistency checks. These proxies are imperfect, but they let you measure quality at scale. The key is calibrating your proxies against human evaluation often enough to know when they drift.

The meaningfulness constraint is why "99% of responses have confidence scores above 0.7" is often a bad SLO — users do not experience confidence scores, they experience correctness. A meaningful SLO captures the user-facing outcome: "95% of customer queries are resolved without escalation," "90% of generated summaries contain no factual errors per spot-check," "98% of content moderation decisions are upheld on appeal." The closer your SLO is to what users actually care about, the better it guides reliability investment.

The achievability constraint prevents aspirational SLOs that sound good in planning meetings but require unrealistic resources. "100% of AI outputs are factually perfect" is not an SLO, it is a fantasy. If your SLO requires manually reviewing every output, rewriting your entire architecture, or spending more on reliability than your gross revenue, it is not achievable. Good SLOs stretch your system without breaking your budget.

## SLI Selection for AI Systems

Service Level Indicators are the metrics you measure to determine if you are meeting your SLOs. For AI systems, SLI selection is harder than for traditional services because the quality dimension has no universal metric. You have to define what "working" means for your specific use case, then instrument signals that indicate it.

Availability SLIs are straightforward: request success rate, non-error response rate, API availability percentage. These are the same SLIs you use for any HTTP service. If your model serving layer is up and returning responses, availability is high. If it is throwing 500 errors or timing out, availability is low.

Latency SLIs are also familiar: p50, p95, p99 response time, end-to-end latency including retrieval and generation, time to first token for streaming responses. These are continuous distributions, so you set thresholds: "p95 latency under 1 second" means 95% of requests complete in under a second. Latency SLIs for AI systems tend to have longer tails than traditional APIs because inference time varies with input complexity.

Quality SLIs are where you have to get creative. Common quality SLIs include: human review pass rate on sampled outputs, semantic similarity between generated output and retrieved context, confidence score distributions, structured output validation pass rate, safety filter pass rate, length within expected bounds, consistency between multiple generations for the same input, user feedback scores, task completion rate, escalation rate, correction rate. None of these are perfect. All of them correlate with quality when calibrated properly.

The art of AI SLI selection is choosing the smallest set of indicators that give you confidence the system is working without requiring unsustainable measurement effort. If you pick ten quality SLIs and they all move together, you probably only need two. If you pick one quality SLI and it stays green while user complaints spike, you picked the wrong one.

## Common AI SLO Mistakes

The most common mistake is defining SLOs only for availability and latency, ignoring quality entirely. Teams do this because quality is hard to measure and they already have availability monitoring. The result is that they hit their SLOs while the model degrades, hallucinates, or returns useless outputs. Users are unhappy, the SLO dashboard is green, and leadership is confused why reliability metrics look great but customer satisfaction is tanking.

The second most common mistake is defining quality SLOs that require human review for every request. "100% of outputs reviewed by experts before delivery" is not an SLO for a production AI system at scale — it is a manual process with AI assistance. If your quality SLI cannot be automated, you cannot measure it continuously, and you cannot use it to drive incident response. You need proxies that correlate with human judgment but run automatically.

Another mistake is setting SLOs that do not distinguish between use case criticality. A customer support chatbot and a medical diagnosis assistant have different quality requirements. Applying the same "95% accuracy" SLO to both is either over-investing in the chatbot or under-investing in the medical system. SLOs should reflect the consequences of failure, which vary by use case.

Teams also set SLOs without understanding the cost of meeting them. "99.99% availability with 99% quality and p95 latency under 200ms" sounds great until you calculate that it requires running three model replicas in each region, pre-warming inference capacity, maintaining hot failover to two providers, and keeping a human review team on-call 24/7. The reliability target is meaningless if the cost to achieve it exceeds the value of the service.

A subtler mistake is defining SLOs for individual components instead of end-to-end user experience. Your embedding model might have 99.9% uptime, your vector database might have p95 latency of 50ms, your generation model might have 98% quality, but if the end-to-end retrieval-augmented generation pipeline only works correctly 90% of the time, your component SLOs are misleading. Users experience the full pipeline, not the parts.

## SLO Precision and Granularity

How specific should your SLOs be? "The system works most of the time" is not an SLO. "99.9% availability and 95% quality" is better, but still incomplete. Precision and granularity matter because they determine what you measure, how you respond to violations, and where you invest in reliability.

Precision refers to the number of nines or the percentage threshold. The difference between 99% and 99.9% availability is the difference between 7 hours of downtime per month and 43 minutes. For quality, the difference between 90% and 95% correct responses might be the difference between acceptable and unacceptable user experience. Set precision based on user tolerance for failure, not based on what sounds impressive.

Granularity refers to how you slice your SLOs. Do you have one SLO for your entire service, or different SLOs for different endpoints, use cases, customer tiers, or time windows? A B2B AI product might have different SLOs for enterprise customers versus free tier users. A multi-use-case platform might have stricter quality SLOs for high-risk use cases like medical or financial applications and looser SLOs for low-risk use cases like content suggestions.

Time granularity also matters. Are you measuring SLOs over a day, a week, a month, a quarter? Shorter windows make SLOs more sensitive to incidents but also more volatile. Longer windows smooth out variation but delay detection of sustained degradation. Most teams measure SLOs over rolling 30-day windows for reporting and over 24-hour windows for alerting.

User-segmented SLOs are increasingly common for AI products. Your SLO might be "95% quality for English, 90% for Spanish, 85% for other languages" because your training data and evaluation effort are unequal across languages. Or "99% availability during business hours, 98% outside business hours" because user expectations differ by time of day. Granular SLOs let you make explicit tradeoffs instead of pretending all use cases are equal.

The risk of too much granularity is complexity. If you have 50 different SLOs, you cannot effectively prioritize reliability work. If you have one SLO, you cannot capture the diversity of your system. The right balance is usually 3-7 SLOs that cover your most critical user-facing outcomes.

## SLO Ownership and Accountability

An SLO without an owner is a wish without a plan. Every SLO needs a team responsible for meeting it, monitoring it, and responding when it is violated. For traditional services, ownership is usually clear: the team that owns the service owns the SLO. For AI systems, ownership is messier because quality SLOs often span teams.

Availability and latency SLOs are typically owned by the infrastructure or platform team. They control the serving infrastructure, autoscaling policies, failover logic, and model deployment pipeline. If availability drops, they respond by scaling up capacity, failing over to backup regions, or rolling back bad deployments.

Quality SLOs are harder to assign. Is the model team responsible? The data team? The product team? The answer depends on what drives quality. If quality degrades because the model drifts, the model team owns it. If quality degrades because input data distribution shifts, the data team owns it. If quality degrades because users are asking questions outside the intended scope, the product team owns it. Often, quality SLO ownership is shared, which means you need clear escalation paths when violations occur.

Shared ownership works when you have defined responsibility boundaries. The model team is responsible for model performance on the evaluation set. The data team is responsible for keeping the evaluation set representative of production. The product team is responsible for defining quality standards and acceptable tradeoffs. When quality SLO violations happen, you triage by root cause and assign to the team that owns that component.

SLO ownership includes defining the SLO, instrumenting the SLI, setting up alerting, responding to violations, and reporting on trends. The owner does not have to do all the work — they orchestrate the response. If a quality SLO drops because retrieval is returning irrelevant documents, the owner triages the issue, identifies the retrieval team as the resolver, and tracks the fix. Ownership is about accountability, not doing everything yourself.

## Starting Your First AI SLOs

If you do not have AI SLOs today, start with three: one for availability, one for latency, one for quality. Do not overthink it. You will refine them as you learn what matters. The goal is to start measuring so you can start improving.

For availability, start with "99% of requests return a valid response within timeout." This is basic but measurable on day one. Instrument request success rate, set an alert when it drops below 99% in a rolling 24-hour window, and assign the platform team to investigate violations.

For latency, start with "95th percentile response time under 2 seconds." Adjust the threshold based on your use case — real-time voice applications need under 500ms, asynchronous document processing can tolerate 10 seconds. Instrument p95 latency, set an alert, and assign the platform or model team to investigate when it is breached.

For quality, start with a proxy metric you can measure automatically. If you have user feedback, use "90% positive feedback rate." If you have structured outputs, use "95% validation pass rate." If you have retrieval, use "90% of responses cite retrieved context." If you have nothing, start with "manual review of 100 samples per week shows 90% acceptable quality" and evolve toward automation. The key is to start measuring something that correlates with quality, even imperfectly, so you have a baseline.

Once you have three SLOs running for a month, evaluate them. Are they realistic — are you meeting them most weeks? Are they meaningful — do violations correlate with user pain? Are they actionable — when they are violated, do you know what to fix? Adjust thresholds, refine SLIs, and add granularity where needed. SLOs are not set in stone. They evolve with your system, your users, and your understanding of what reliability means.

The most important step is making SLOs visible. Publish them to a dashboard, review them in weekly engineering meetings, report them to leadership monthly. SLOs only drive behavior when teams see them, care about them, and are held accountable for them. If your SLOs live in a document nobody reads, they are not doing anything.

Defining SLOs for AI systems requires expanding your definition of reliability beyond uptime to include quality. The next subchapter covers how to measure that quality dimension in practice: building SLOs around what users actually care about.
