# 3.11 — The Override Problem: When Humans Must Disable Protections

Every defensive mechanism eventually becomes an obstacle. Circuit breakers protect your system from cascading failures — until you need to test a fix and the circuit breaker keeps tripping before you can verify the repair. Rate limiters prevent overload — until a critical customer needs to process an urgent batch job and the rate limiter blocks it. Automated protections are essential for reliability. They are also, occasionally, wrong. The override problem is this: how do you give humans the ability to disable automated protections when necessary without making overrides so easy that the protections become meaningless.

In late 2025, a financial services company deployed a circuit breaker on their model endpoint. The breaker tripped during a latency spike. Engineering diagnosed the issue — a misconfigured autoscaling threshold — and fixed it within 15 minutes. The circuit breaker remained open. The half-open state allowed one request every 30 seconds. The production health check succeeded. The circuit closed. Traffic resumed. Twenty seconds later, the circuit tripped again — not from a real failure but from a single timeout caused by a slow client connection. The circuit breaker sensitivity was too high. Engineering wanted to temporarily disable the circuit breaker to verify the fix was stable. But there was no override mechanism. The breaker was controlled by a config file that required a code deploy. The deploy took 45 minutes. The service remained degraded for an hour because the protection mechanism that was supposed to prevent outages became the cause of one.

## Why Override Capability Is Necessary

Automated protections are tuned based on observed behavior. Circuit breaker thresholds are set based on historical error rates. Rate limits are set based on expected traffic. Timeout values are set based on P99 latency. These parameters are correct most of the time. They are incorrect during edge cases: load testing, incident recovery, unusual customer behavior, configuration errors, and false positives from monitoring systems.

Load testing requires overrides. You are intentionally pushing the system beyond normal capacity to find breaking points. Circuit breakers will trip. Rate limiters will engage. If you cannot override them, you cannot test the system's true limits. The override must be temporary and scoped to the testing environment. Disabling a circuit breaker in production to run a load test is dangerous. Disabling it in a staging environment for 30 minutes is appropriate.

Incident recovery requires overrides. A service fails. The circuit breaker opens. You fix the underlying issue. The circuit breaker stays open, allowing only limited test traffic through the half-open state. If the half-open threshold is too conservative — one success every 60 seconds — recovery takes too long. You need to override the circuit breaker temporarily to allow more test traffic, verify the fix, and close the circuit manually. Without an override, recovery is gated by the circuit breaker's built-in timers.

Critical customer requests require overrides. A customer has a contractual SLA for certain requests. A rate limiter blocks one of these requests because the customer hit their quota. But the request is part of a regulatory deadline or a time-sensitive business process. You need to allow the request through. The override is not about ignoring the rate limit permanently — it is about making a specific exception for a specific reason.

False positives require overrides. Your circuit breaker trips because of a transient network blip. The service is healthy. The circuit breaker is not. The half-open state will eventually close the circuit, but it might take minutes. If you have evidence that the service is healthy — direct health checks, manual testing, internal metrics — you should be able to override the circuit breaker immediately. Waiting for the automated recovery wastes time.

## Override Mechanisms: Manual, Time-Limited, and Request-Specific

Manual overrides are the simplest form. An operator sets a flag that disables a protection mechanism. The circuit breaker stops opening. The rate limiter stops blocking. The override remains in effect until the operator removes the flag. Manual overrides are appropriate for short-term testing or incident recovery. They are dangerous for long-term use because they require someone to remember to re-enable the protection. Incidents end. People forget. The override stays in place.

Time-limited overrides solve the memory problem. The override is configured with an expiration time. After 30 minutes, the protection re-enables automatically. Time-limited overrides are safer than manual overrides because they fail closed. If the operator forgets to remove the override, the system self-corrects. The downside: if the operator needs more time, they must extend the override or create a new one. This is acceptable. The friction is intentional. It forces periodic re-evaluation of whether the override is still necessary.

Request-specific overrides are the most precise. Instead of disabling the protection entirely, you mark specific requests as exempt. A customer request includes a token that grants rate limiter bypass. A load testing request includes a header that prevents circuit breaker evaluation. The protection remains active for all other traffic. This is the safest override pattern. The blast radius is limited to the requests that carry the override token.

Implement overrides as feature flags, not config file changes. Feature flags can be toggled in seconds without a deploy. Config file changes require a deploy, which adds latency and risk. If a circuit breaker needs to be disabled during an incident, waiting 20 minutes for a deploy to complete defeats the purpose. Feature flags should be managed through an admin UI or API, not hardcoded or controlled only through version control.

Override state must be visible in observability dashboards. If a circuit breaker is overridden, every engineer looking at the system dashboard should see that immediately. A banner, a prominent label, a distinct color — something that makes the override obvious. Otherwise, engineers will waste time debugging why the circuit breaker is not tripping when it should be or why rate limiting is not enforcing quotas. The override is temporary operational state. Treat it like an active incident.

## Audit Trails for Override Actions

Every override must be logged with a timestamp, the identity of the person who triggered it, the reason, and the scope. These logs are critical for post-incident review. If a circuit breaker override contributed to a cascading failure, you need to know who issued the override, why they thought it was necessary, and whether the reasoning was sound. Audit logs also provide accountability. If overrides are easy to issue and invisible, they will be overused. If overrides are logged and reviewed, engineers will think twice before issuing one.

The audit log should include the override duration. If an override was issued for 30 minutes but expired after 10 minutes because the engineer manually revoked it, that information matters. It tells you the override was temporary and deliberate, not forgotten. If an override was issued for 30 minutes and remains active for 6 hours, that is a red flag. Someone forgot to revoke it or extended it without documenting why.

Audit logs should be queryable. You need to answer questions like: how many circuit breaker overrides were issued in the last 90 days? Which engineers issue the most overrides? Which services have overrides most frequently? Are overrides correlated with incidents? A high override rate indicates one of three things: the protections are too sensitive, the system is unreliable, or engineers are using overrides as a workaround for deeper problems.

Audit logs should feed into incident postmortems. If an incident involved an override, the postmortem must address why the override was necessary, whether it was the right decision, and whether changes to the protection mechanism or the override policy are needed. Overrides are not inherently bad. They are a pressure relief valve. But if you are using the relief valve every day, something is misconfigured.

## The Danger of Permanent Overrides

A permanent override is an override that never expires. It is indistinguishable from disabling the protection mechanism entirely. Permanent overrides happen when a temporary override is not revoked and no expiration was set. They also happen when engineers decide a protection is too aggressive and disable it "until we have time to fix the threshold." The threshold never gets fixed. The override becomes permanent.

Permanent overrides are invisible debt. The circuit breaker config says it should trip at a 20 percent error rate, but it has been overridden for three months. New engineers joining the team assume the circuit breaker is active. It is not. They design their own systems assuming the circuit breaker provides protection. It does not. The first time a real failure occurs, the circuit breaker does not trip because it was disabled long ago.

Prevent permanent overrides by making all overrides time-limited by default. If an engineer issues an override without specifying a duration, the system assigns a default expiration — perhaps 1 hour. If the engineer needs more time, they must extend the override with a new expiration. Each extension requires a reason. After three extensions, the system requires approval from a manager or on-call lead. This escalation process forces conversation. If an override needs to be extended three times, maybe the protection threshold is wrong and should be adjusted permanently, not overridden repeatedly.

Review override state during on-call handoffs. When the on-call rotation changes, the outgoing engineer briefs the incoming engineer on active incidents, known issues, and active overrides. If an override is still in place, the incoming engineer needs to know why and whether it should be revoked. This prevents overrides from lingering after the reason for them has resolved.

Monitor override duration. If an override has been active for longer than 24 hours, alert the team. If it has been active for longer than a week, escalate to leadership. Long-lived overrides indicate a systemic problem. Either the protection is misconfigured and needs to be tuned, or the system is unreliable and the override is masking a deeper issue.

## Override Governance: Who Can Override What

Not all engineers should have override authority for all systems. A junior engineer running a load test in a staging environment should be able to override rate limiters in that environment. The same engineer should not be able to override circuit breakers in production without approval. Override authority must be role-based and environment-scoped.

Define override roles explicitly. A "service owner" can override protections for services they maintain. An "on-call lead" can override protections during active incidents. A "release engineer" can override protections during deployments. A "load testing operator" can override protections in non-production environments. These roles are assigned based on job function, not seniority. A senior engineer who is not on-call and is not the service owner does not have override authority just because they are senior.

Override authority should require authentication and authorization. The override API checks the identity of the requester and verifies they have the appropriate role. The override is logged with the requester's identity. This prevents accidental overrides — someone runs a script that issues an override without realizing it — and malicious overrides — a compromised account tries to disable protections to enable an attack.

Critical overrides should require dual authorization. Disabling a production circuit breaker for a tier-1 service should require approval from both the service owner and the on-call lead. This is the "two-person rule" from physical security applied to operational controls. It prevents a single person from making a high-risk decision under pressure without consultation. The downside: dual authorization adds latency. Use it only for high-risk overrides where the cost of getting it wrong is severe.

Some protections should be non-overridable. Security controls like rate limiting for authentication endpoints or circuit breakers for fraud detection models should not have an override mechanism at all. If these protections are too aggressive, the only way to change them is through a config change that requires review and approval. This is intentional. These protections are critical to the security posture of the system. Making them overridable introduces an attack vector. An attacker who compromises an operator account could disable fraud detection by issuing an override.

## Emergency Override Procedures During Incidents

During an incident, speed matters. If a circuit breaker is preventing recovery, waiting for a dual-authorization approval process is unacceptable. Emergency override procedures allow on-call engineers to bypass normal governance during active incidents. The trade-off: emergency overrides have elevated risk and require elevated scrutiny after the incident.

Emergency overrides should be self-service for on-call engineers. The on-call engineer issues the override through an API or admin UI. The override takes effect immediately. The override is logged with a flag indicating it was an emergency override. This flag triggers additional post-incident review. Emergency overrides are not subject to dual authorization or manager approval during the incident. Those checks happen after the fact.

Emergency overrides should be time-limited to the incident duration. The override expires automatically when the incident is marked resolved. If the incident lasts 3 hours, the override lasts 3 hours. If the incident is marked resolved and then reopened, the override can be reissued. This ties the override lifetime to the incident lifecycle.

Emergency overrides must be documented in the incident timeline. The moment the override is issued, an entry is added to the incident log: "Overrode circuit breaker on model endpoint X to allow test traffic during recovery validation." The reason is recorded. The expected impact is recorded. This documentation becomes part of the postmortem.

Emergency overrides should trigger alerts. If an on-call engineer issues an emergency override, the rest of the on-call team and the service owner are notified immediately. This is not to block the override — it is already in effect — but to provide visibility. If the override was issued in error or if it has unintended side effects, the team can respond quickly. The alert also serves as a check: if an override was issued but the incident was not severe enough to justify it, the team can discuss whether the override policy needs adjustment.

## Override Fatigue: When Teams Override Too Often

Override fatigue happens when protections are so aggressive or so misconfigured that engineers override them routinely. The circuit breaker trips multiple times per day. Engineers stop investigating why and start issuing overrides reflexively. The protection becomes a nuisance, not a safeguard. Once override fatigue sets in, the protection is effectively disabled.

Override fatigue is a signal that the protection is tuned incorrectly. If a circuit breaker is tripping on transient network blips that resolve themselves in 10 seconds, the half-open recovery threshold is too conservative. If a rate limiter is blocking legitimate customer traffic because the quota is too low, the quota should be raised. The overrides are telling you something. Listen to them.

Measure override frequency as a metric. Track overrides per day, overrides per service, overrides per engineer. If overrides are increasing over time, the protections are becoming less effective. If overrides are concentrated on a few services, those services have a configuration problem. If overrides are concentrated with a few engineers, those engineers might need training on when overrides are appropriate or they might be responding to a known issue that should be fixed rather than overridden.

Reduce override fatigue by making protections smarter, not weaker. Instead of disabling a circuit breaker because it is too sensitive, adjust the failure threshold or add failure type filtering so the breaker ignores transient blips but still trips on sustained errors. Instead of removing a rate limiter because the quota is too low, raise the quota or implement burst allowances. The goal is not zero overrides. The goal is that every override is a deliberate decision, not a reflex.

## Designing for Minimal Override Need

The best override policy is one that is rarely used. Design protections that rarely need overriding. This requires tuning thresholds based on real production behavior, not theoretical limits. It requires monitoring false positive rates — how often does the protection trigger when there is no actual problem? It requires making the protection adaptive so it adjusts to changing conditions automatically instead of requiring human intervention.

Circuit breakers should have adaptive thresholds. Instead of a fixed 20 percent error rate threshold, use a threshold based on recent history. If the normal error rate is 1 percent and it spikes to 10 percent, trip the breaker. If the normal error rate is 5 percent and it spikes to 10 percent, do not trip. The threshold adapts to the service's baseline behavior. This reduces false positives and reduces the need for overrides.

Rate limiters should have burst allowances. Instead of a hard limit of 100 requests per minute, allow short bursts above the limit as long as the average over a longer window stays within bounds. A customer who sends 150 requests in one minute and 50 requests the next is not abusing the system. A customer who sends 150 requests every minute is. Burst allowances make rate limiting less brittle and reduce the need for request-specific overrides.

Timeouts should be percentile-based, not fixed. Instead of a 5-second timeout for all requests, use a timeout based on the 99th percentile latency for the specific request type. A summarization request for a 100-page document gets a longer timeout than a two-sentence classification. This reduces false timeouts and reduces the need for overrides during legitimate high-latency requests.

Protections should degrade gracefully. Instead of a circuit breaker that is either fully open or fully closed, use a circuit breaker that gradually reduces traffic to the failing service. At 10 percent error rate, reduce traffic by 50 percent. At 20 percent error rate, reduce by 90 percent. At 30 percent, stop all traffic. This gives the service a chance to recover under reduced load instead of being completely cut off. Gradual degradation reduces the need for overrides because the service can often recover without human intervention.

## Post-Incident Override Review

Every override issued during an incident must be reviewed in the postmortem. The review answers three questions: Was the override necessary? Was the override issued correctly? What changes are needed to prevent this override in the future?

If the override was necessary, document why the protection failed to handle the situation correctly. Did the circuit breaker trip on a false positive? Was the rate limiter threshold too low? Was the timeout too aggressive? If the protection was working as designed but the situation required an override anyway, document the edge case. This becomes part of the protection's design documentation.

If the override was issued correctly, verify that the override governance process worked. Was the override issued by someone with appropriate authority? Was it time-limited? Was it logged and communicated? If any of these checks failed, update the override tooling or policy to prevent the same gap in the future.

If the override should not have been issued, investigate why the engineer felt it was necessary. Was there a misunderstanding about the state of the system? Was there pressure to restore service quickly without fully diagnosing the issue? Was the engineer unaware of the correct procedure? This is not about blame — it is about understanding the decision-making context and improving training or documentation.

Use override reviews to tune protections. If the same override is issued repeatedly for the same reason, the protection is misconfigured. Fix the threshold, change the logic, or redesign the protection. The override pattern is data. Treat it as a signal that something in your reliability architecture needs adjustment.

Circuit breakers, timeouts, and retries keep your system running when individual components fail. Overrides keep those protections from becoming obstacles when human judgment is needed. The next chapter addresses fallback chains — the strategies that answer the question "what do you do when the circuit breaker is open and the primary path is unavailable?"
