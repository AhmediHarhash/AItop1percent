# 6.1 — The AI Incident Response Lifecycle

At 2:47am on a Thursday in August 2025, the on-call engineer at a legal research platform received an alert. Not a page. An alert. Model quality score had dropped from 0.89 to 0.83 over the previous hour. The engineer looked at error rates, latency, throughput. All normal. They checked the model provider status page. No outages. They reviewed recent deployments. Nothing in the past three days. They went back to sleep. By 8am, customer support had logged forty-seven tickets about incorrect case law citations. By noon, three law firms had contacted their account managers threatening contract termination. The company later calculated that the six-hour delay between initial signal and response escalation cost them two hundred and thirty thousand dollars in credits, legal review, and one lost enterprise customer. The incident commander's post-mortem report included one line that captured the core problem: "We did not know we were in an incident until we were deep into it."

AI incidents do not announce themselves the way traditional software incidents do. A database failure produces a spike in errors. A network partition produces timeouts. A deployment bug produces crashes. An AI quality degradation produces... slightly worse outputs that users notice before your monitoring does. The incident response lifecycle that works for traditional software assumes clear failure signals, deterministic root causes, and verifiable recovery. AI breaks all three assumptions. You need a different lifecycle that accounts for delayed detection, probabilistic diagnosis, and uncertain recovery validation.

## The Traditional Lifecycle and Why It Fails for AI

Traditional incident response follows a well-established pattern. Detection happens when monitoring triggers an alert. Triage assesses severity based on error rates and user impact. Diagnosis identifies the root cause by examining logs, traces, and recent changes. Mitigation deploys a fix or rolls back the problematic change. Recovery confirms that metrics return to normal. Post-incident review documents what happened and what should change to prevent recurrence.

This lifecycle assumes that failure is binary. The system is either working or not working. An API either returns 200 or it returns 500. A database either commits transactions or it does not. A deployment either completes successfully or it fails. AI systems exist in a third state that traditional incident response does not account for: the system is working but producing degraded outputs. All health checks pass. All APIs respond. All infrastructure is operational. But the model is giving worse answers than it did yesterday, and you have no definitive signal that tells you when it crossed the threshold from acceptable to unacceptable.

The traditional lifecycle also assumes that diagnosis produces a clear root cause. A stack trace points to the failing line of code. A correlation between deployment time and error spike identifies the problematic change. A resource exhaustion pattern points to a capacity issue. AI diagnosis produces probabilistic hypotheses. Maybe the model provider silently updated their base model. Maybe a shift in user query distribution triggered behavior the model was never trained for. Maybe a dependency change affected prompt rendering in ways that degrade model reasoning. Maybe nothing changed at all and you are seeing natural variance in model outputs. You cannot point to a single root cause with confidence. You can only point to the most likely cause based on incomplete information.

The traditional lifecycle assumes that recovery is verifiable within seconds or minutes. You roll back the deployment, error rates drop to zero, latency returns to baseline, and you declare the incident resolved. AI recovery requires running evaluation suites that take twenty minutes. You roll back a prompt change, but you will not know if quality actually improved until you process enough production traffic to establish a new baseline, and even then you cannot be certain the improvement was caused by the rollback rather than by changes in query distribution. You cannot declare victory with confidence. You can only declare that the signal looks better and hope that user reports confirm it.

## The AI Incident Response Lifecycle

The AI-adapted lifecycle has seven phases that account for these differences. Detection, triage, diagnosis, mitigation, validation, monitoring, and learning. The names are the same, but the execution is fundamentally different.

**Detection** in AI incidents is slower and more ambiguous. You are not waiting for error rates to spike. You are waiting for quality metrics to degrade below thresholds that you may or may not have set correctly. Your monitoring might detect the degradation in minutes, hours, or days depending on sample size and metric sensitivity. In many cases, user reports will be your first detection signal, which means you are already behind. The detection phase in an AI incident often includes a sub-phase that traditional incidents do not need: confirming that the signal is real and not a false positive caused by natural variance or a shift in query distribution that does not actually represent quality degradation.

**Triage** in AI incidents requires multi-dimensional severity assessment. A traditional P1 incident is one that takes the service down or makes it unusable. An AI incident might be P1 even though the service is fully operational, because the model is producing outputs that violate safety policies, expose user data, or generate viral reputational damage. Severity is not a function of error rate. It is a function of quality degradation magnitude, affected user percentage, safety risk, compliance risk, and reputational risk. The triage phase must assess all five dimensions and determine severity based on whichever dimension produces the highest urgency.

**Diagnosis** in AI incidents is probabilistic and often incomplete. You generate hypotheses about root cause and rank them by likelihood based on available evidence. The model provider may have updated their base model. A prompt change may have interacted poorly with certain query types. A data distribution shift may have triggered edge case behavior. An infrastructure change may have affected input preprocessing in subtle ways. You will rarely have definitive proof of root cause during the incident. You will have the most plausible explanation based on correlation, timing, and domain knowledge. The diagnosis phase ends not when you have proven root cause but when you have enough confidence in a hypothesis to take mitigation action.

**Mitigation** in AI incidents may not involve code changes. It might involve rolling back a model version, reverting a prompt change, re-enabling a fallback model, adjusting a routing rule, or temporarily disabling a feature. The mitigation action you take is based on your best guess about root cause, and you will deploy it knowing that you might be wrong. Traditional incidents let you mitigate first and understand later. AI incidents require you to understand enough to mitigate intelligently, because naive mitigations like "roll back everything" might make the problem worse if the root cause was actually an external shift in user behavior or model provider behavior.

**Validation** in AI incidents is uncertain and slow. You deploy your mitigation and wait. You watch quality metrics trend upward. You check user reports. You run spot checks on recent outputs. You trigger on-demand evals. All of this takes time measured in minutes or hours, not seconds. You cannot refresh a dashboard and see immediate confirmation that the incident is resolved. You can only watch signals improve and gain confidence gradually. The validation phase has no clean endpoint. You declare validation complete when your confidence that the mitigation worked crosses some internal threshold, knowing that you might discover two hours later that it did not actually work and you are back in the incident.

**Monitoring** in the post-mitigation phase is more intense than in traditional incidents. You are watching for signal regression, secondary effects, and delayed failures. A rollback might have fixed the immediate problem but introduced a new problem in a different feature. A prompt change might have improved quality for one user segment while degrading it for another segment that you have not sampled yet. You keep the incident open and the responders engaged for hours after initial mitigation, watching dashboards and user reports until you are confident that the system has stabilized and no secondary effects have emerged.

**Learning** in AI incidents requires different post-incident analysis. You cannot simply point to a bug fix and declare that the problem will not recur. AI incidents often recur because the root cause was a model behavior pattern, not a code defect. The post-incident phase must identify whether the incident was caused by a systemic gap in your evaluation coverage, monitoring coverage, deployment process, or architectural resilience. The corrective action is often "add this failure mode to our continuous eval suite" or "build a new metric that detects this pattern" rather than "fix this bug." The learning phase produces process changes and monitoring changes, not just code changes.

## Phase Timing and Overlap

In traditional incidents, phases are mostly sequential. You detect, then triage, then diagnose, then mitigate, then recover. In AI incidents, phases overlap heavily. You might start mitigation before diagnosis is complete because waiting for certainty means accepting continued damage. You might declare the incident resolved while still monitoring for delayed effects. You might triage severity as P2, then escalate to P1 an hour later when you realize the quality degradation is worse than initial signals suggested.

A well-run AI incident response team expects phases to overlap and plans for it. The incident commander does not wait for complete diagnosis before authorizing rollback. The on-call engineer does not wait for perfect validation before declaring mitigation successful. The team accepts that they are operating under uncertainty and builds processes that allow them to act decisively anyway. The alternative is decision paralysis, where responders wait for signals that will never be definitive and watch damage accumulate while they wait.

The lifecycle phase that most often causes failure is the transition from detection to triage. An engineer sees a quality metric decline and does not escalate because it does not cross their mental threshold for "incident." They treat it as something to investigate during business hours. By the time they realize it was an incident, hours have passed and users have experienced degraded outputs the entire time. The fix for this failure is to establish clear escalation criteria that do not require perfect certainty. If a quality metric drops by more than a defined threshold, escalate. If user reports mention a pattern in outputs, escalate. If multiple weak signals appear simultaneously, escalate. Overescalation is cheaper than underescalation when you are dealing with silent failures.

## The Detection Lag Problem

AI incidents have a built-in detection lag that traditional incidents do not. In a traditional incident, the system fails and monitoring detects it within seconds. In an AI incident, the system continues operating and monitoring must accumulate enough samples to determine that quality has degraded. If you evaluate one percent of production traffic, you need enough traffic volume to collect statistically meaningful samples. If traffic is low, that might take hours. If your quality metrics have high variance, you need even more samples to distinguish true degradation from noise.

This detection lag is not a monitoring failure. It is a fundamental property of quality-based failure detection. You cannot fix it by adding more monitors. You can only reduce it by evaluating a higher percentage of traffic, using lower-latency evaluation methods, or incorporating user signals that detect problems faster than your automated metrics do. Many production AI systems now use user-reported issue rates as a leading indicator that supplements metric-based detection. A spike in user reports often precedes metric-based detection by minutes or hours, giving you earlier warning that something is wrong.

The detection lag means that by the time you enter the incident response lifecycle, damage has already occurred. Users have already seen bad outputs. The incident response goal is not to prevent damage. It is to stop ongoing damage and minimize total impact. This shifts your mental model. In traditional incidents, you are racing to restore service before users notice. In AI incidents, users have already noticed and you are racing to stop them from noticing more.

## Why Recovery Validation Takes So Long

The longest phase in most AI incidents is validation. You have deployed a mitigation. Now you need to confirm it worked. In a traditional incident, you watch error rates drop and latency normalize and you know within seconds that recovery was successful. In an AI incident, you watch quality metrics and hope they trend upward, but you will not have statistical confidence for minutes or hours depending on traffic volume and metric variance.

Some teams try to accelerate validation by running on-demand eval suites against recent production traffic. This helps, but it introduces a new problem: the eval suite might not cover the specific failure mode you are trying to validate. If the incident was caused by hallucinations about a specific entity type, and your eval suite does not include examples of that entity type, your eval will show normal quality even if the problem persists. Validation requires not just running evals but running the right evals, which requires understanding the failure mode well enough to select or construct eval cases that target it.

Other teams accelerate validation by sampling recent outputs and manually reviewing them. This works for incidents that produce obviously wrong outputs, but it fails for incidents that produce subtly degraded outputs where wrongness is not binary. A manually reviewed sample of fifty responses might look fine even if the underlying quality distribution has shifted in ways that will become apparent over hundreds of responses. Manual review gives you fast feedback but low confidence. Automated evals give you high confidence but slow feedback. Most teams use both and accept that validation will remain uncertain until enough time has passed to see the full effect of the mitigation.

## The Incident That Never Fully Resolves

Some AI incidents do not have clean resolution points. The mitigation improves quality, but not back to baseline. The team has exhausted obvious rollback options and deployed every mitigation they can think of, and quality is still ten percent below where it was a week ago. The incident commander faces a decision: declare the incident resolved at the new lower baseline, or keep the incident open indefinitely while the team searches for additional root causes.

Most teams declare resolution when they have exhausted mitigation options, even if quality has not returned to baseline. They document the new baseline as degraded, open a follow-up task to investigate further, and close the incident. This is the correct decision. Keeping an incident open for days or weeks while quality remains degraded does not help anyone. It burns out responders, reduces the perceived importance of future incidents, and does not change the fact that you have no more mitigation options available. Close the incident, document the degraded state, and shift the problem from incident response to longer-term improvement work.

The AI incident response lifecycle is not a recipe for perfect recovery. It is a framework for acting decisively under uncertainty, minimizing damage when failures are ambiguous, and learning from incidents even when root cause is never fully understood. The teams that handle AI incidents well do not wait for perfect information. They act on imperfect information and accept that some of their actions will be wrong. The teams that handle AI incidents poorly wait for certainty that never arrives and watch damage accumulate while they wait.

---

Next: 6.2 — AI-Specific Incident Severity Classifications
