# 12.3 — Embedding Drift and Vector Index Decay

In April 2025, a knowledge management platform upgraded their embedding model from OpenAI's text-embedding-3-small to text-embedding-3-large. The new model promised better semantic understanding and higher retrieval accuracy. They deployed the upgrade on a Friday. By Monday morning, retrieval quality had collapsed. User queries that previously returned relevant documents now returned nonsense. The vector index had not changed. The documents had not changed. But the embedding space had shifted, and the old embeddings were now incompatible with the new model.

Embedding drift is what happens when the model that creates vector representations changes, but the vector index that stores those representations does not. Embeddings are not universal. They are specific to the model version that created them. When you switch embedding models — even from one version to the next within the same model family — the vector space changes. An embedding vector created by model version one has no guaranteed compatibility with model version two. Your existing vector index becomes misaligned overnight.

This is one of the most dangerous forms of AI system aging because it is invisible until deployment and catastrophic when it happens. There are no gradual warning signs. The system works perfectly until the moment you switch models. Then it breaks completely.

## How Embedding Drift Happens

Embedding models are neural networks trained to map text or images to fixed-dimensional vectors such that semantically similar inputs produce similar vectors. The specific mapping — which dimensions correspond to which semantic features — is learned during training and is unique to that training run. Even if you train two embedding models on identical data with identical architectures, they will learn slightly different mappings. The vectors they produce live in different spaces.

When embedding model providers release new versions, they are releasing models with different internal representations. OpenAI's text-embedding-3-small and text-embedding-3-large were trained on different data, optimized for different objectives, and produce vectors in spaces with different dimensionalities. A vector from text-embedding-3-small is a 1536-dimensional vector. A vector from text-embedding-3-large is a 3072-dimensional vector. You cannot compare them directly. They are not compatible.

But even when dimensionality stays constant, the space changes. Anthropic released an updated version of their Voyage embedding model in late 2025. Both versions produced 1024-dimensional vectors. But the semantic structure of the space was different. The old model organized vectors such that "contract" and "agreement" were close together. The new model, trained on more recent legal text, learned that "contract" was closer to "SLA" and "NDA" than to the generic term "agreement." The new space was better — more aligned with how legal professionals actually use terminology in 2025. But it was incompatible with the old space.

A legal research startup used the original Voyage model to embed 400,000 legal documents and built a vector index for retrieval. When Anthropic released the updated Voyage model, the startup wanted to upgrade. They assumed that because dimensionality had not changed, the vectors would be roughly compatible. They swapped the embedding model in production without re-embedding the document corpus. Retrieval accuracy dropped from 84% to 41% within the first day. The old embeddings were optimized for the old model's semantic space. Queries embedded with the new model landed in different regions of the 1024-dimensional space. The nearest-neighbor search returned documents that were close in the old space but far in the new space — documents that were semantically unrelated to the query.

## Embedding Model Updates and Compatibility

Embedding model providers release new versions for good reasons: better performance, better multilingual support, reduced bias, improved efficiency. But every update creates a compatibility problem. You have three options when a new embedding model is released: stay on the old model, upgrade and re-embed your entire corpus, or maintain dual indexes during a transition period.

Staying on the old model is the safest short-term choice. Your existing index remains valid. But you fall behind. Newer models are better. Competitors using newer models get better retrieval quality. Users notice. Staying on an old embedding model is like staying on an old LLM version — it works, but you are aging in place while the ecosystem moves forward.

Upgrading and re-embedding is the correct long-term choice. But re-embedding a large corpus is expensive. If you have 10 million documents and embedding costs 0.01 cents per 1,000 tokens, re-embedding a corpus that averages 500 tokens per document costs around 5,000 dollars. That is manageable for a small corpus. For 500 million documents, re-embedding costs 250,000 dollars. And the cost is not just financial — it is operational. Re-embedding takes time. Rebuilding vector indexes takes time. Testing the new index takes time. You are offline or running degraded service during the transition.

Maintaining dual indexes is the safest but most complex approach. You keep the old index running in production while building a new index with the new embedding model. Once the new index is complete and tested, you switch traffic from the old index to the new index. If something breaks, you switch back. Dual indexes require double the storage and double the infrastructure, but they allow zero-downtime migrations. A fintech company migrating from Cohere Embed v2 to Cohere Embed v3 ran dual indexes for six weeks in late 2025. The old index served production traffic while the new index was built and validated in parallel. Once validation confirmed that the new index improved retrieval accuracy by 9%, they cut over production traffic in under five minutes.

## Vector Index Staleness

Even if you never upgrade your embedding model, vector indexes become stale. Staleness happens because the document corpus changes while the index does not. You add new documents. You update existing documents. You delete obsolete documents. Each change requires updating the index. If you delay index updates, the index drifts out of sync with the corpus.

A customer support knowledge base added 30 new troubleshooting articles per month. The articles were added to the document store immediately, but the vector index was only rebuilt once per month. For 29 days out of every month, the index was stale. User queries that should have retrieved new articles returned older, less relevant articles instead. The team did not notice the problem for five months because they did not measure retrieval quality continuously — they only spot-checked retrieval during index rebuilds. By the time they noticed, users had been experiencing degraded retrieval for half a year.

Index staleness compounds with corpus growth. A small corpus with 10,000 documents and 30 new documents per month grows by 0.3% per month. Staleness is negligible. A large corpus with 5 million documents and 30 new documents per month grows by 0.0006% per month. But if those 30 new documents cover recent product launches or recent regulatory changes, they may be disproportionately important. A 0.0006% corpus change can represent a 15% relevance change if the new documents cover high-traffic topics.

## Detecting Embedding Drift

Embedding drift is hard to detect before it causes retrieval failures. You cannot measure drift by comparing embeddings directly, because embeddings from different models live in different spaces. You cannot even measure cosine similarity across model versions — similarity scores are not comparable.

The most reliable detection method is retrieval quality benchmarks. Before upgrading an embedding model, you create a test set of 500 to 2,000 queries with known-relevant documents. You measure retrieval accuracy: for what percentage of queries does the top-five retrieved documents include at least one known-relevant document? You measure this with the old embedding model and the old index. Then you re-embed the corpus with the new model, rebuild the index, and measure retrieval accuracy again. If accuracy declines, the new model is worse for your specific corpus, and you should not upgrade. If accuracy improves, the new model is better, and the upgrade is worth the re-embedding cost.

A healthcare documentation platform maintained a benchmark set of 800 clinical queries with known-relevant articles. In November 2025, they tested an upgrade from their current embedding model to a newer version. The newer model was advertised as having 12% better performance on general-domain benchmarks. But when tested on the healthcare benchmark, the newer model had 7% worse performance. The reason: the newer model was trained on more diverse data, which diluted its performance on specialized medical terminology. The platform stayed on the old model. They re-tested every six months to see if newer models eventually surpassed the old one.

User-facing metrics detect drift after it has already caused harm. If retrieval accuracy was 84% last month and 76% this month, something changed. The change might be embedding drift from a model upgrade. It might be index staleness from corpus growth. It might be concept drift from changing query patterns. User-facing metrics tell you that drift exists. They do not tell you which type of drift or what caused it. Root cause analysis requires comparing multiple hypotheses.

## The Compatibility Matrix Problem

Large products often use multiple embedding models for different components. One model for document embeddings. Another for query embeddings. Another for image embeddings. A fourth for multilingual embeddings. Each model has its own versioning schedule. Each model gets updated independently. The number of compatibility states explodes.

A media company's content recommendation system used three embedding models: one for article text, one for user profiles, one for images. In mid-2025, they upgraded the article embedding model but delayed upgrading the user profile model because user profile re-embedding was more expensive. For three months, the system used mismatched embedding spaces. Articles were embedded in the new space. User profiles were embedded in the old space. Recommendations became incoherent because the system was comparing vectors from incompatible spaces. Article-to-user similarity scores were meaningless. The team did not catch the problem immediately because recommendation systems are hard to evaluate — there is no ground truth for what a user "should" have been recommended.

The compatibility matrix problem is worse when you use third-party APIs for embedding. You do not control when the provider updates their models. OpenAI released text-embedding-3-small in early 2024 but continued supporting the older text-embedding-ada-002 until late 2025. When they deprecated ada-002, any system still using it had to migrate. If you had embedded 50 million documents with ada-002 and built a product around that index, you faced a forced migration with a hard deadline. The alternative — staying on a deprecated model — meant accepting that the model could be shut down without warning after the deprecation period ended.

## Re-Embedding Strategies

Re-embedding a large corpus is expensive. The two dominant strategies are full re-embedding and incremental re-embedding.

Full re-embedding means taking the entire document corpus, running every document through the new embedding model, and rebuilding the vector index from scratch. This is straightforward but time-consuming. A corpus with 10 million documents, where each document takes 50 milliseconds to embed, requires 500,000 seconds of embedding time — roughly 139 hours. If you parallelize across 100 workers, that is 1.4 hours. But you also need to rebuild the index, which can take additional hours depending on the vector database. Full re-embedding works for small to medium corpora. For very large corpora, the cost and downtime become prohibitive.

Incremental re-embedding means re-embedding in batches while keeping the old index running. You re-embed 5% of the corpus, update the index with the new embeddings, and repeat until the entire corpus is re-embedded. Incremental re-embedding avoids downtime, but it creates a temporary state where some documents are embedded with the old model and some with the new model. If the embedding spaces are not compatible, retrieval quality degrades during the transition period. A SaaS company tried incremental re-embedding in early 2025 and abandoned the approach after two weeks. Mixing old and new embeddings in the same index caused retrieval to return incoherent results — some documents were semantically close in the old space but far in the new space, and the index could not distinguish which was which.

Hybrid strategies work better. You maintain the old index in production while building a new index in parallel with the new embeddings. Once the new index is complete, you switch traffic. This avoids the mixed-embedding problem but requires double the infrastructure during the transition.

## Index Refresh Scheduling

Even without model upgrades, indexes need periodic refreshes to incorporate new documents. The refresh frequency depends on corpus update rate and user tolerance for staleness.

High-velocity corpora need daily or real-time index updates. A news aggregation platform adds thousands of articles per day. Staleness longer than a few hours makes the index irrelevant. They update their vector index every two hours, re-embedding new articles and incrementally adding them to the index. The index is never perfectly current, but it is never more than two hours stale.

Low-velocity corpora can refresh weekly or monthly. A legal precedent database adds 50 new cases per month. A monthly index rebuild is sufficient. Staleness of a few weeks does not materially impact user experience because legal research involves decades of case law, and a three-week delay on new cases is negligible.

Medium-velocity corpora need a trade-off. A product documentation site updates 200 documents per month. Real-time indexing is overkill. Monthly indexing leaves the index stale for too long. The compromise: weekly index refreshes. New documents are added to the corpus immediately but are not searchable until the next weekly refresh. The team communicates this delay to users: "New articles may take up to seven days to appear in search results."

## Embedding Drift Monitoring

Monitoring embedding drift requires tracking retrieval quality over time. You cannot measure embedding similarity directly, but you can measure whether retrieved documents are relevant to queries.

A retrieval quality dashboard tracks metrics daily: precision at five, recall at ten, mean reciprocal rank. Baselines are established during the first month post-deployment. Alerts trigger when metrics drop below baseline by more than 10%. A travel booking platform monitored retrieval quality for their destination recommendation system. In June 2025, precision at five dropped from 0.78 to 0.69 over three days. Investigation revealed that a third-party embedding API they used had silently updated their model. The provider did not announce the update because they considered it a minor bug fix. But the embedding space had changed enough to degrade retrieval. The platform rolled back to a pinned version of the API and contacted the provider to negotiate a stable, versioned API endpoint.

User feedback signals drift indirectly. If users start saying "This search result is not relevant" more frequently, retrieval quality has degraded. A SaaS company tracked the percentage of searches followed by a "Refine Search" action. In July 2025, that percentage jumped from 18% to 31%. Users were not finding what they needed on the first search. Investigation traced the problem to index staleness — the index had not been refreshed in eight weeks due to an infrastructure migration that delayed scheduled maintenance.

Embedding drift is silent until it breaks retrieval. You cannot see the vectors shifting. You cannot feel the semantic space changing. You only notice when users stop finding relevant results. The only defense is continuous monitoring, regular index refreshes, and treating embedding model upgrades as breaking changes that require full regression testing before deployment.

In the next subchapter, we cover retrieval quality decay over time — how retrieval degrades even when embeddings and indexes remain stable.
