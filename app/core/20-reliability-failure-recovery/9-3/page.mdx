# 9.3 — Model Degradation Simulation

Provider outages are obvious. The API returns errors, your monitoring alerts fire, your engineers respond. Model quality degradation is subtle. The API returns 200 OK. Latency looks normal. Error rates are unchanged. But the outputs are worse. Hallucination rates increase from 2 percent to 7 percent. Instruction following drops from 0.94 to 0.87. Tone shifts from professional to casual. Users notice. Your monitoring does not. By the time you detect the problem, thousands of users have received degraded experiences.

Model degradation happens. Providers roll out new model versions that perform differently on your specific use case. Upstream training data changes affect model behavior in ways that are hard to predict. Models drift as real-world distributions shift away from training distributions. Your system must be able to detect degradation, quantify its severity, and respond — by failing over to a different model, by routing traffic to a safer fallback, or by alerting humans to intervene. The only way to verify that your detection and response systems work is to simulate degradation before it happens naturally.

## The Silent Degradation Problem

Silent degradation is the failure mode where model quality declines but your traditional monitoring does not catch it. Error rates are stable. Latency is stable. Throughput is stable. All the standard SRE metrics look healthy. But user satisfaction is dropping because outputs are less accurate, less coherent, or less appropriate for the context.

Silent degradation is dangerous because discovery is delayed. A provider outage is detected in seconds. Quality degradation can go undetected for days or weeks if you do not have active quality monitoring. By the time users complain loudly enough for escalation, the degradation has affected thousands of interactions. Trust is eroded. Reputation is damaged. The incident is expensive not because of its technical severity but because of its detection latency.

A healthcare AI company experienced silent degradation in August 2025 when their model provider released a new version that changed default temperature settings. Outputs became more creative and less deterministic. For a medical summarization system, creativity is a liability. Hallucination rates increased from 1.8 percent to 6.3 percent. Traditional monitoring showed everything normal. Detection came from a physician who manually reviewed outputs and noticed inconsistencies. By the time engineering investigated, the degraded model had been serving production traffic for nine days. The company implemented active quality monitoring and model degradation chaos experiments to ensure it never happened again.

Silent degradation is why passive monitoring is insufficient for AI systems. You cannot rely on error rates and latency alone. You must actively measure quality on an ongoing basis. But even with active quality monitoring, you need chaos experiments to verify that your monitoring actually catches degradation and that your response systems actually work when degradation is detected.

## Simulating Latency Degradation

Latency degradation is the simplest form of model degradation to simulate and the most common in production. Providers experience load spikes, infrastructure issues, or regional congestion that increases response times without causing outright failures. Your system must detect latency increases and respond before they accumulate into user-facing timeouts.

Simulating latency degradation means injecting artificial delays into model responses. A proxy or middleware layer intercepts requests to the model provider and adds a configurable delay before forwarding responses to your application. You can simulate sudden latency spikes — 200 milliseconds to 3 seconds instantly — or gradual latency increases — 200 milliseconds increasing by 100 milliseconds per minute over ten minutes.

The chaos hypothesis for latency degradation: "When model latency exceeds X milliseconds for Y consecutive requests, our circuit breaker will trip within Z seconds, routing traffic to a faster fallback model while maintaining quality above threshold Q." The experiment tests whether this hypothesis is true.

A logistics company ran a latency degradation chaos experiment that injected 4-second delays into their primary model responses. They expected their circuit breaker to trip after three slow requests, which should take roughly 12 seconds. Chaos revealed the circuit breaker never tripped. Investigation showed that their circuit breaker was configured to measure average latency over a 60-second window, which meant it required sustained degradation for a full minute before responding. During that minute, hundreds of requests timed out. They reconfigured the circuit breaker to trip on per-request latency and re-ran the experiment. The circuit breaker tripped after 12 seconds as designed.

Latency degradation experiments also test whether your fallback model is actually faster. You might assume your fallback model has lower latency because it is a smaller model or a different provider. Chaos testing verifies the assumption. If your fallback model is not actually faster, failing over does not solve the latency problem — it just changes which provider is slow. A fintech company discovered through chaos that their fallback model had higher latency than their primary model because it was hosted in a different region with worse network latency to their infrastructure. They changed their fallback to a co-located model and verified through re-testing that latency improved.

## Simulating Quality Degradation

Quality degradation is harder to simulate than latency degradation because quality is multidimensional and context-dependent. Latency is a single number. Quality includes accuracy, coherence, instruction following, tone, hallucination rate, and appropriateness. Simulating quality degradation requires modifying model outputs in ways that mimic real degradation patterns.

One approach: replace a percentage of real model outputs with outputs from a weaker model. If your production system uses GPT-5.2, simulate degradation by serving 10 percent of requests with GPT-5-mini outputs. GPT-5-mini is a capable model but produces lower-quality outputs on complex tasks. Your quality monitoring should detect the degradation. If it does not, the experiment revealed a monitoring gap.

Another approach: inject known errors into model outputs. Modify responses to include hallucinated facts, incorrect references, or tone shifts. For a customer support system, change friendly responses to curt responses. For a summarization system, inject factual inaccuracies. Your quality monitoring should detect these errors. If it does not, the experiment revealed that your monitoring is not sensitive enough.

A healthcare company simulated quality degradation by using a regex-based system to randomly replace medical terms with near-synonyms that were technically incorrect in context. "Hypertension" became "high blood pressure" in clinical notes where precision mattered. "Myocardial infarction" became "heart attack" in contexts where the distinction was clinically relevant. Their quality monitoring, which measured general fluency and coherence, did not detect the substitutions. Their domain-specific quality checks, which measured medical accuracy, caught the degradation within five minutes. The experiment validated that their domain-specific monitoring was necessary and effective.

Quality degradation experiments test whether your quality thresholds are correctly calibrated. If your threshold is too loose, you will not detect real degradation until users complain. If your threshold is too tight, you will generate false alarms that erode trust in your monitoring. Chaos experiments let you tune thresholds by observing how your system responds to controlled degradation of known severity.

## Simulating Rate Limit Scenarios

Rate limiting is a form of degradation where the provider throttles your requests instead of serving them at normal capacity. Rate limit errors are explicit — the provider returns HTTP 429 — but the user experience is still degraded because requests fail or are delayed until retries succeed. Your system must detect rate limiting and respond by shedding load, switching to a fallback provider, or queuing requests.

Simulating rate limits means injecting 429 responses for a percentage of requests. A proxy intercepts requests to the model provider and returns 429 errors according to the experiment design. You might simulate hard rate limiting where every request above a threshold fails, or soft rate limiting where requests succeed but with increasing delays. The experiment tests whether your retry logic, load shedding, and failover strategies work under rate limit pressure.

A customer support company simulated rate limiting by returning 429 errors for 30 percent of requests to their primary provider. They expected their system to retry failed requests with exponential backoff, then fail over to a secondary provider if retries failed. Chaos revealed that their retry logic was too aggressive. It retried immediately without backoff, which caused their requests to be rate-limited even more aggressively by the provider's anti-abuse mechanisms. The system entered a failure spiral where retries made the problem worse. They implemented exponential backoff with jitter and re-ran the experiment. Retry success improved by 60 percent.

Rate limit chaos also tests cost control. If your primary provider rate-limits you and you fail over to a secondary provider that costs 3x more, your costs spike. Do you have cost alerts that fire when spending exceeds thresholds? Do you have budget circuit breakers that stop failover when costs become unacceptable? Chaos experiments reveal whether your cost controls work or whether a rate limit event can cause runaway spending.

## Testing Detection Sensitivity

The purpose of model degradation simulation is to test whether your detection systems work. Detection sensitivity is the balance between catching real degradation quickly and avoiding false positives that create alert fatigue. Chaos experiments let you measure detection sensitivity by injecting degradation of known severity and measuring how quickly and accurately your monitoring responds.

Run chaos experiments that inject different levels of degradation. Inject 5 percent quality drop, 10 percent quality drop, 20 percent quality drop. Measure whether your monitoring detects each level and how long detection takes. If your monitoring detects 20 percent degradation but not 10 percent degradation, you know your sensitivity threshold is between 10 and 20 percent. You can tune your detection to be more sensitive or decide that 10 percent degradation is acceptable and does not require immediate response.

A fintech company ran chaos experiments with degradation levels from 3 percent to 25 percent. Their quality monitoring detected degradation above 12 percent within three minutes. Degradation below 12 percent went undetected for over an hour until batch eval runs completed. They decided that 12 percent was too high a threshold for a financial application and increased monitoring sensitivity. Follow-up chaos experiments confirmed that detection improved to 5 percent within four minutes.

Detection sensitivity also depends on sample size. If you evaluate quality on a small sample, your monitoring might miss degradation that affects only a subset of users. If you evaluate quality on a large sample, detection is more reliable but more expensive. Chaos experiments let you test whether your sample size is adequate by injecting degradation that affects only a subset of traffic — for example, degradation that only affects queries longer than 50 words. If your monitoring catches the degradation, your sample size is adequate. If it does not, you need more coverage.

## Verifying Alert Thresholds

Chaos experiments test whether your alert thresholds are correctly set. An alert threshold that is too sensitive fires constantly on normal variation, creating alert fatigue. An alert threshold that is too loose does not fire until degradation has caused significant user impact.

The chaos approach: inject controlled degradation, measure when alerts fire, and compare to your expectations. If you expect an alert when quality drops below 0.85, inject degradation that produces 0.83 quality and verify that the alert fires. If the alert does not fire, your threshold is misconfigured or your monitoring has a bug. If the alert fires immediately, your threshold is correctly calibrated. If the alert fires 20 minutes later, your monitoring has excessive lag.

A logistics company set quality alert thresholds based on historical data: fire when quality drops below 0.88. Chaos experiments revealed that normal variation caused quality to dip to 0.87 several times per week, producing false alarms. They tightened the threshold to 0.85 and added a requirement that the threshold be violated for two consecutive measurement windows before firing. Chaos re-testing confirmed that false alarms dropped to near zero while real degradation was still detected within acceptable time.

Alert thresholds should also account for severity. A 5 percent quality drop might warrant a low-priority alert. A 20 percent quality drop might warrant an immediate page. Chaos experiments test whether your tiered alerting works by injecting degradation at different severity levels and verifying that the correct alert priority fires.

## Chaos for Quality-Based Circuit Breakers

Quality-based circuit breakers are systems that automatically route traffic away from degraded models based on quality metrics. Unlike latency-based or error-rate-based circuit breakers, quality-based circuit breakers require continuous evaluation of model outputs. Chaos experiments test whether these circuit breakers work correctly and respond within acceptable time windows.

The chaos setup: inject quality degradation into your primary model. Measure how long it takes for the quality-based circuit breaker to detect degradation, make a routing decision, and fail over to a fallback model. Measure whether the fallback model maintains quality within acceptable bounds. Measure whether the circuit breaker re-opens when the primary model recovers.

A healthcare AI company built a quality-based circuit breaker that measured hallucination rate on a sample of outputs. If hallucination rate exceeded 4 percent for two consecutive minutes, the circuit breaker routed traffic to a more conservative fallback model. Chaos testing injected degradation that increased hallucination rate to 8 percent. The circuit breaker detected the degradation in 90 seconds, switched to the fallback model, and maintained hallucination rate below 2 percent. After the chaos experiment ended and the primary model recovered, the circuit breaker correctly re-opened after three minutes of sustained low hallucination rate. The experiment validated that their circuit breaker design worked.

Quality-based circuit breakers are more complex than traditional circuit breakers because quality measurement has latency. You cannot measure quality on every request in real time without significant cost and latency overhead. You measure quality on a sample. The sample introduces delay between when degradation begins and when detection occurs. Chaos experiments quantify that delay and let you decide whether it is acceptable.

## When Simulation Reveals Detection Gaps

The most valuable outcome of model degradation chaos is discovering that your detection does not work. If you run a chaos experiment that injects 15 percent quality degradation and your monitoring does not fire an alert, you have a detection gap. Fix the gap, re-run the experiment, and verify that detection now works.

Common detection gaps revealed by chaos: monitoring measures the wrong metric — you measure fluency when you should measure accuracy. Monitoring samples too infrequently — you evaluate every hour when degradation happens in minutes. Monitoring uses the wrong baseline — you compare to a static threshold when you should compare to recent historical performance. Monitoring lacks domain-specific checks — you measure general quality when domain-specific errors are what users care about.

A customer support company ran model degradation chaos and discovered that their monitoring measured response length and keyword presence but not semantic correctness. Chaos experiments that injected fluent but factually incorrect responses went undetected. They added semantic similarity checks between outputs and reference answers. Follow-up chaos experiments confirmed that detection improved dramatically.

Detection gaps are expensive to discover in production. Chaos experiments let you discover them in controlled conditions where the blast radius is limited and the cost is a learning opportunity, not a customer-facing incident. Fix the gaps revealed by chaos, and your system becomes genuinely resilient to degradation, not theoretically resilient.

Model degradation is the silent failure mode that traditional monitoring misses. You simulate latency degradation, quality degradation, and rate limiting. You test whether your detection systems are sensitive enough and whether your alert thresholds are correctly calibrated. You verify that quality-based circuit breakers work and respond within acceptable time windows. The gaps you find through chaos are gaps you fix before real degradation affects users. The confidence you gain through chaos is confidence you earn through testing, not through hope.

Next: 9.4 — Retrieval Failure Simulation
