# 2.10 — Detection SLOs: How Fast Must You Know

An AI-powered loan approval system degraded silently for eighteen minutes before anyone noticed. In that window, it approved 143 applications that should have been flagged for manual review. The bug was in the risk assessment prompt — a configuration change had removed a critical instruction. The system did not crash. It did not error. It just made wrong decisions for eighteen minutes. The cost of those eighteen minutes was $280,000 in potentially risky loans and three months of regulatory scrutiny.

The question is not whether failures happen. They do. The question is how fast you detect them. Every minute between failure and detection is a minute of damage accumulation. Users receive bad outputs. Downstream systems make decisions on bad data. Trust erodes. Money is lost. For high-stakes AI systems, detection latency is as important as mean time to repair. You cannot fix what you have not detected.

Detection latency is measurable, improvable, and should be managed with the same rigor as uptime or response time. This means setting detection SLOs — targets for how fast you must know when failures occur — and building monitoring systems that meet those SLOs. It means measuring your actual detection latency for every incident and treating detection latency as a reliability metric that teams are accountable for improving.

## Time-to-Detection as a Reliability Metric

Most reliability metrics focus on uptime or performance. Your SLO might be 99.9% of requests succeed within 500 milliseconds. This tells you how often the system works and how fast it responds. It does not tell you how fast you know when it does not work.

Time-to-detection is the gap between when a failure starts and when you become aware of it. For the loan approval system, the failure started the moment the bad configuration deployed. Detection happened eighteen minutes later when someone manually reviewed an approval and noticed anomalies. The detection latency was eighteen minutes. During those eighteen minutes, the system was failing but no one knew.

This metric is independent of time-to-repair. You might have excellent incident response — once alerted, you fix the issue in three minutes. But if detection takes thirty minutes, your total user-facing downtime is thirty-three minutes. Fast repair does not compensate for slow detection. Both matter. Both need SLOs.

The loan approval system had no detection SLO. They had an uptime SLO and a latency SLO. They had not considered how fast they needed to know about quality failures. When the postmortem asked why detection took eighteen minutes, the answer was that they had no monitoring for prompt content changes and no alerting on approval rate changes. They were measuring whether the system was online, not whether it was making correct decisions.

Setting a detection SLO forces you to build the monitoring that achieves it. If your SLO is detect failures within two minutes, you need metrics that update every thirty seconds and alerting logic that evaluates those metrics continuously. If your SLO is detect failures within ten minutes, you can use five-minute metrics and less frequent alerting. The SLO drives instrumentation requirements.

## Different Failure Types Require Different Detection SLOs

Not all failures need the same detection speed. A complete outage needs near-instant detection because every second without service is obvious to users. A subtle quality degradation can tolerate slower detection because individual users might not notice, but the aggregate effect accumulates over time. A security breach needs extremely fast detection because every minute of unauthorized access expands the blast radius.

For complete outages, your detection SLO should be under one minute. If your API stops responding, you need to know within sixty seconds. This is achievable with health checks that run every ten to thirty seconds. Most teams meet this SLO easily because infrastructure monitoring is mature. If your service is down, your monitoring fires almost immediately.

For quality degradation, your detection SLO depends on your use case. A customer-facing chatbot that hallucinates medical advice needs detection within two to five minutes. An internal document summarization tool that produces slightly worse summaries can tolerate ten to twenty-minute detection. The SLO should be proportional to the rate of damage accumulation. How much harm occurs per minute of undetected degradation?

For security failures — jailbreaks, prompt injections, data leaks — your detection SLO should be under five minutes and ideally under two. Attackers move fast. If someone discovers a jailbreak, they might script it and extract sensitive information from thousands of queries before you notice. Every minute of undetected security failure is a minute of potential data exfiltration or policy violation.

For silent model drift — the model gradually degrading over days or weeks — your detection SLO can be measured in hours or days. This is not an acute incident. This is chronic degradation. You need to detect it before it affects too many users, but you do not need real-time alerting. Batch evaluation that runs every few hours is sufficient.

The key is matching the detection SLO to the failure's urgency. Acute failures need acute detection. Chronic issues need monitoring but not instant alerts. Setting uniform detection SLOs across all failure types either over-invests in slow-moving problems or under-invests in fast-moving ones.

## The Cost Curve: Damage Accumulates While Failures Go Undetected

Detection latency matters because damage is a function of time. A hallucinating medical chatbot provides one bad answer per minute. If you detect after one minute, one user received bad information. If you detect after thirty minutes, thirty users did. The damage scales linearly with detection latency.

For some systems, the cost curve is steeper. A trading algorithm making wrong decisions loses money every second. A fraud detection system missing fraud cases costs more per minute as fraudsters exploit the gap. A content moderation system failing to catch policy violations exposes you to regulatory penalties that grow with the number of violations. In these cases, the damage curve is not linear. It accelerates.

Understanding your cost curve informs your detection SLO. If one minute of undetected failure costs ten dollars and ten minutes costs one hundred dollars, your detection SLO should be aggressive. If one minute costs ten dollars and ten minutes costs twenty dollars, you can tolerate slower detection. Map out the relationship between detection latency and damage for your specific system, and set SLOs that keep expected damage below acceptable thresholds.

A fintech company did this analysis for their credit scoring AI. They calculated that every minute of undetected wrong scoring cost an average of $1,200 in either missed revenue opportunities or risky approvals. Their acceptable damage threshold was $5,000 per incident. This gave them a detection SLO of four minutes. They built monitoring that could detect scoring anomalies within three minutes, meeting the SLO with margin.

This exercise also revealed which failures they cared about most. Complete outages cost the most per minute because they stop all revenue. Subtle accuracy drops cost less per minute but occur more frequently. They set different detection SLOs for each: one minute for outages, four minutes for accuracy drops, thirty minutes for latency increases. The monitoring investment matched the business impact.

## Measuring Detection Latency in Practice

Measuring detection latency requires knowing two timestamps: when the failure started and when you became aware of it. The second timestamp is easy — it is when your alerting fires or when a user reports the issue. The first timestamp is harder because failures do not announce themselves.

For failures triggered by deployments or configuration changes, the start time is the deployment timestamp. If you deploy at 14:22 and detect a failure at 14:30, your detection latency is eight minutes. This is the cleanest case because you have a ground truth start time.

For failures that emerge gradually — model drift, data pipeline degradation, traffic pattern shifts — the start time is ambiguous. Did the failure start when the first metric crossed the threshold, or when the aggregate metric became obviously bad? The practical answer is to define failure start as the first moment when metrics crossed the threshold you eventually alerted on. If you alert when error rate exceeds 5%, the failure started when error rate first exceeded 5%, even if you did not notice until later.

For user-reported failures, detection latency is the gap between the user's first experience of the failure and your first awareness. If a user encounters a broken feature at 09:15 and reports it at 09:20 and you see the report at 09:25, your detection latency is ten minutes from failure to awareness. This is the worst case because you rely on users to detect for you.

Tracking detection latency across incidents reveals patterns. You might discover that deployment-triggered failures are detected in three minutes on average, but data-pipeline failures take twenty minutes. This tells you where your monitoring has gaps. You might discover that failures during business hours are detected in five minutes, but failures at night take forty minutes. This tells you that you rely on human observation instead of automated alerting.

The goal is to measure detection latency for every incident, log it in your incident tracking system, and analyze it over time. Are you improving? Are certain failure types consistently detected late? Are certain teams faster at detection than others? This data drives monitoring investment decisions.

## Setting Realistic Detection SLOs

The detection SLO must be achievable with realistic monitoring infrastructure. Saying "we will detect all failures instantly" is aspirational but not actionable. Monitoring has latency. Metrics aggregate over time windows. Alerting logic evaluates periodically. You cannot detect faster than your monitoring's fundamental latency.

For most systems, a realistic detection SLO for quality degradation is two to ten minutes. This assumes metrics that update every thirty seconds to two minutes, alerting logic that evaluates every minute, and alert routing that delivers notifications within thirty seconds. Faster detection requires higher-frequency monitoring, which increases infrastructure cost and monitoring noise.

For outages and complete failures, a realistic SLO is thirty seconds to two minutes. Health checks run every ten to thirty seconds. If a health check fails, retry once to rule out transient issues. If the second check fails, alert. This gets you sub-two-minute detection for catastrophic failures.

For security failures, achievable detection SLOs depend on your detection method. If you are running guardrails synchronously on every request, you can detect violations within seconds. If you are using batch analysis of logs, detection might take five to fifteen minutes. The SLO must match your architecture. If you need faster detection, you must invest in lower-latency monitoring.

The SLO should also account for incident classification time. When an alert fires, someone must look at it and decide whether it is a real incident or a false positive. This adds one to five minutes to detection latency. Your SLO must include this human-in-the-loop time unless your alerting is accurate enough to auto-escalate without human review.

## Detection Latency vs Response Latency

Detection latency and response latency are different metrics that teams often conflate. Detection latency is how fast you know about a failure. Response latency is how fast you fix it. Both contribute to total downtime, but they require different investments to improve.

Improving response latency means better runbooks, faster rollback procedures, clearer escalation paths, more on-call training. Improving detection latency means better monitoring, more instrumentation, smarter alerting logic, faster metric pipelines. A team can be excellent at response but poor at detection, or vice versa. Both need attention.

Some teams optimize response at the expense of detection. They can fix issues in minutes once alerted, but they rely on users to alert them. This creates long detection latency and good response latency, resulting in mediocre total downtime. The better strategy is balanced investment. Detect fast and respond fast.

For the loan approval system, response latency was fine. Once engineers learned about the prompt bug, they rolled back the configuration in four minutes. But detection took eighteen minutes. If they had invested in approval rate monitoring and anomaly detection, they could have detected in two minutes. Total downtime would have dropped from twenty-two minutes to six minutes — a 73% improvement driven entirely by better detection.

This is the leverage of detection SLOs. Small improvements in detection latency create large improvements in total reliability. If you can detect failures four times faster, you cut damage accumulation by 75% even if your response time stays the same. For high-stakes systems, faster detection is often higher ROI than faster response.

## Continuous Improvement of Detection Speed

Detection latency should be a standing agenda item in incident postmortems. For every incident, ask: how long did detection take, and could we have detected faster? If detection took ten minutes and your SLO is five minutes, what monitoring gap prevented faster detection?

Common gaps include missing metrics, metrics with too much lag, alerting thresholds set too conservatively, alerting logic that does not cover this failure mode, and alert routing that delivers notifications too slowly. Each gap is addressable. Add the metric. Reduce metric lag. Adjust the threshold. Expand the alerting logic. Fix the routing. Then measure whether detection latency improves in the next incident.

Some teams track detection latency as a key performance indicator for their reliability program. They publish a monthly report showing average detection latency, detection latency by failure type, and detection latency trend over time. This creates organizational visibility and accountability. If detection latency is increasing, leadership asks why. If it is decreasing, the team gets credit for improving reliability.

Another improvement mechanism is red team exercises. Intentionally introduce failures in a controlled environment and measure how fast your monitoring detects them. If you inject a subtle prompt bug, how long until alerting fires? If you simulate a provider degradation, how fast do health probes catch it? These exercises reveal monitoring gaps before real incidents exploit them.

The best teams treat detection latency as a design constraint. When building new features, they ask: how will we detect if this feature fails? When choosing monitoring infrastructure, they ask: what detection latency can this infrastructure achieve? When setting SLOs, they ask: what detection latency do we need to meet our reliability goals? Detection is not an afterthought. It is a first-class requirement.

## Organizational Ownership of Detection SLOs

Setting and meeting detection SLOs requires ownership. Someone must define the SLOs, someone must build the monitoring to achieve them, and someone must be accountable when detection latency exceeds the SLO. In most organizations, this is split across multiple teams.

Product or leadership defines what detection latency is acceptable based on business risk. Engineering builds the monitoring infrastructure to achieve that latency. SRE or platform teams maintain the alerting and ensure it stays within SLO. Incident commanders measure actual detection latency during incidents and report gaps.

This requires cross-functional alignment. If product sets a two-minute detection SLO but engineering has only ten-minute metrics, the SLO is not achievable. If engineering builds one-minute metrics but SRE sets alert thresholds too conservatively, detection still takes ten minutes. The SLO drives requirements across all three functions.

The most effective approach is collaborative SLO-setting. Bring product, engineering, and SRE into the same room. Product explains the business cost of undetected failures. Engineering explains what detection latency is achievable with current and near-future infrastructure. SRE explains what alerting precision and recall they can achieve. Together, they set SLOs that balance business needs with technical reality, and they commit to the investment needed to meet those SLOs.

You cannot fix what you do not detect. You cannot detect instantly. But you can detect fast enough that damage is bounded. Set the SLO. Build the monitoring. Measure the latency. Improve continuously. That is how detection becomes a reliability practice instead of an accident.

---

With detection principles established, we now turn to circuit breakers and fail-safes — the mechanisms that automatically contain failures before they cascade.
