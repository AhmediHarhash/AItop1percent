# 6.4 — The First Five Minutes: Containment Before Diagnosis

In November 2025, a healthcare AI company's clinical documentation assistant started producing notes with inconsistent medication dosages. The on-call engineer spent eleven minutes investigating the root cause before taking any containment action. During those eleven minutes, the system generated 847 clinical notes. Thirty-two of them reached patient charts before the system was finally taken offline. The subsequent investigation found that a model provider's silent update had changed how the system handled numeric precision. The root cause took six hours to fully diagnose and fix. But the damage happened in the first eleven minutes — while the engineer was still trying to understand what was wrong.

The first five minutes of an AI incident are not for diagnosis. They are for containment. You stop the bleeding before you figure out why it's bleeding. Every second you spend investigating while the system continues to produce potentially incorrect output extends the damage radius. Every additional request processed increases the number of users affected, the amount of bad data generated, and the scope of cleanup required afterward. Containment is not about understanding. It is about limiting harm while you buy time to understand.

This principle runs counter to engineering instinct. Engineers want to understand before they act. The natural response to an alert is to pull logs, check dashboards, compare behavior to baseline, and form a hypothesis. This is the right approach for most software systems where the failure mode is binary — the service is up or down, requests succeed or fail, data is written or not. But AI failures are often gradual degradation. The system still responds. The outputs still look plausible. The failure is subtle, probabilistic, and hard to spot in individual requests. And while you are investigating whether there is actually a problem, the system is generating hundreds or thousands more outputs that you might need to remediate later.

## The Containment-First Protocol

The containment-first protocol is simple: within the first five minutes of detecting a potential AI quality incident, take action to limit exposure before you fully understand the problem. This does not mean blindly shutting down production systems. It means having pre-defined containment actions that reduce risk while maintaining some level of service, and executing those actions before you start deep diagnosis.

The most common containment action is traffic shifting. If you have multiple model versions in production — a primary model and a fallback — you shift traffic to the fallback. If you have a rule-based fallback system, you route requests there. If you have a human-in-the-loop option, you enable it for all requests. If you have a circuit breaker that can restrict the system to low-risk use cases, you activate it. The goal is to move from a potentially degraded high-risk state to a known safe lower-capability state. You accept reduced functionality to eliminate the risk of ongoing harm.

The second containment action is rate limiting. If you cannot shift traffic and you cannot take the system offline, you throttle it. Reduce the request rate to 10% of normal traffic. Route 90% of users to a "temporarily unavailable" message. This does two things: it limits the number of new potentially incorrect outputs being generated, and it buys you time to manually review a sample of recent outputs to confirm whether there is actually a problem. If your monitoring shows potential degradation but you are not certain, rate limiting lets you investigate with 90% less ongoing risk.

The third containment action is disabling specific features. If the incident appears isolated to one capability — summarization, translation, specific entity types, certain output formats — you disable that feature while leaving the rest of the system operational. This is surgical containment. It works when you have enough signal to localize the problem to a subsystem but not enough time to diagnose and fix it. You remove the broken piece from production, contain the damage to what has already happened, and investigate the disabled feature offline.

## When Containment Costs Too Much

Containment is not free. Every containment action reduces service quality or availability. Shifting to a fallback model means lower accuracy. Rate limiting means frustrated users. Disabling features means lost functionality. The decision to contain is a trade-off between the risk of ongoing incorrect output and the cost of reduced service. And that trade-off depends on the severity of the potential incident.

If your monitoring shows a 40% drop in user satisfaction scores, you contain immediately. The signal is clear, the impact is large, and the cost of waiting is high. If your monitoring shows a 3% shift in output length distribution — a weak signal that might be noise — you do not necessarily contain. You investigate first. The risk of a false positive containment, where you degrade service for a non-issue, outweighs the risk of generating a small number of additional outputs while you confirm the problem.

The key is pre-defining severity thresholds that trigger automatic containment. If precision drops below 0.85, shift traffic to fallback. If hallucination rate exceeds 5%, enable circuit breaker. If user feedback flags exceed 10 per hour, activate rate limiting. These thresholds are set based on your risk tolerance, your SLAs, and the cost structure of your containment options. They are documented, tested, and known to the on-call team before the incident happens. When the alert fires and the threshold is crossed, you do not debate whether to contain. You execute the playbook.

## The Containment Playbook

Every AI system in production should have a documented containment playbook. It is a one-page reference that lives in the on-call runbook and answers four questions: What are the containment options? What are the triggers for each option? What is the execution procedure? What is the rollback procedure if containment makes things worse?

For a customer support classification system, the playbook might read: If classification accuracy drops below 0.90, shift 100% of traffic to rule-based fallback classifier. Execute by setting environment variable FALLBACK underscore MODE equals true and restarting the routing service. Rollback by unsetting the variable and restarting. Notify product and customer support leads within 15 minutes. For a contract analysis system, the playbook might read: If any extraction field shows greater than 10% deviation from baseline, disable automated extraction and route all contracts to manual review queue. Execute by updating feature flag ENABLE underscore AUTO underscore EXTRACT to false. Rollback by re-enabling flag after confirming fix in staging.

The playbook is tested. Every quarter, during a scheduled maintenance window, the on-call team executes the containment procedures against production to confirm they work. You do not want to discover during a real incident that your traffic shifting logic has a bug or that your circuit breaker was never actually deployed to production. The playbook is also versioned. When you deploy a new model or add a new feature, you update the containment procedures. If the new model does not have a fallback, you document that risk and define an alternative containment action.

## Communication During Containment

The first five minutes of containment are tactically focused, but they are not silent. As soon as you execute a containment action, you communicate it. You post in the incident channel: "Containment executed. Shifted 100% traffic to fallback model. Investigating root cause." You notify stakeholders: "AI quality alert triggered. System currently operating in degraded mode. Expected resolution time unknown." You update the status page if your incident is user-visible: "Experiencing issues with AI feature X. Functionality temporarily reduced while we investigate."

This communication serves two purposes. First, it prevents redundant response. If three people are on-call and all see the same alert, the first person to contain posts that containment is done. The others shift to diagnosis and remediation instead of duplicating containment efforts. Second, it sets expectations. Product, customer support, and leadership need to know that the system is in a degraded state. They need to prepare for user questions, adjust messaging, and understand that this is a managed response, not an uncontrolled failure.

The communication is brief. You do not explain the root cause because you do not know it yet. You do not commit to a resolution timeline because you have not diagnosed the issue. You state the facts: what was observed, what action was taken, what the current state is. You update every 30 minutes until the incident is resolved, even if the update is just "still investigating." Silence during an incident is worse than lack of progress. Silence makes people assume the worst.

## Documenting Containment Decisions

After containment is executed, you document the decision. You record what signal triggered the containment, what action was taken, what time it was executed, and who made the call. This documentation lives in the incident record and is reviewed during the post-incident retrospective.

The documentation answers a specific question: Was containment the right call? If you contained based on a weak signal and it turned out to be a false alarm, you discuss whether the containment threshold was too sensitive. If you delayed containment and the incident escalated, you discuss whether the playbook was clear enough or whether the on-call engineer lacked the authority to act. If you contained but chose the wrong action — shifted traffic when you should have disabled a feature — you discuss what signal would have pointed to the better choice.

This documentation also feeds into containment playbook updates. If you discover a new failure mode that was not covered by the existing playbook, you add it. If you execute a containment action and it does not work as expected, you revise the procedure. The playbook is a living document, improved after every incident. The goal is that the next time a similar issue occurs, the containment is faster, more targeted, and more effective.

## When to Skip Containment

There are two situations where you skip containment and go straight to diagnosis. The first is when the containment cost exceeds the incident cost. If your system has no fallback, no circuit breaker, and the only containment option is full shutdown, and the signal suggests a minor quality issue, you diagnose first. Taking the entire system offline for a 2% drop in a secondary quality metric is not proportional. You investigate, confirm whether it is real, and then decide on a response.

The second situation is when you have strong evidence the incident is already over. If your monitoring shows a brief anomaly that lasted three minutes and has now returned to baseline, and you have no ongoing user reports, you skip containment. You investigate to understand what happened and prevent recurrence, but you do not disrupt current service to contain an incident that is no longer active. The key word is strong evidence. If there is any doubt whether the incident is ongoing, you contain first.

## The Post-Containment Handoff

Once containment is executed and confirmed, the incident shifts from response mode to resolution mode. The immediate crisis is over. The system is no longer generating potentially incorrect outputs at scale. Now you have time to diagnose, fix, and restore full service. But the transition from containment to resolution is a handoff, and handoffs fail if they are not explicit.

The on-call engineer who executed containment posts an explicit handoff message: "Containment confirmed. System stable in fallback mode. Handing off to diagnosis. Primary focus is identifying root cause and testing fix in staging." If the incident requires multiple people, you assign roles: one person owns diagnosis, one person owns remediation, one person owns communication. If the incident is complex and will take hours, you schedule a synchronous check-in every 60 minutes to align on progress and decisions.

The handoff also includes a containment sustainability check. If you shifted to a fallback that can run indefinitely with acceptable degraded quality, you have time to fix it properly. If you rate-limited to 10% traffic and users are blocked, you have hours at most before the business cost becomes unacceptable. The diagnosis and fix timeline must account for how long containment can hold. If the containment is not sustainable, you escalate immediately to expand the response team.

Containment before diagnosis is not a philosophy. It is a protocol. When an AI incident begins, you limit exposure first, understand second, and fix third. The first five minutes determine how much damage the incident causes. Everything after that determines how fast you recover.

---

Next: **6.5 — Diagnosis Under Pressure: Finding Root Cause Fast**