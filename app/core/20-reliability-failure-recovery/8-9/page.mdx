# 8.9 — The Recurring Incident Problem: When Fixes Don't Stick

The post-mortem from December 2025 listed five action items. All five were marked complete by January. In February, the same incident happened again. Same root cause, same impact, same mitigation. The team pulled up the December post-mortem and found that the fix — implementing a circuit breaker on the external API — had been deployed to production. The circuit breaker was live. It just didn't work the way the team expected. The incident recurred not because the team ignored the action item, but because the fix addressed a symptom instead of the root cause.

Recurring incidents are the most frustrating failure mode in reliability engineering. You do the post-mortem. You write the action items. You deploy the fix. The incident happens again. It feels like groundhog day. The team loses confidence in the post-mortem process. Leadership starts questioning whether you're learning from incidents at all. The on-call engineer who responded to the original incident feels like their work was wasted. Morale drops. Trust erodes. And the incident keeps happening.

The problem is rarely that teams ignore action items. Most teams complete post-mortem action items. The problem is that the action items fix the proximate cause, not the root cause. They add a monitoring alert but don't fix the resource leak. They restart the service but don't address why it needed restarting. They add a retry but don't fix the underlying race condition. The incident stops for a week, a month, a quarter — and then it comes back, often in a slightly different form. The teams that break the recurring incident cycle are the ones that ask why the first fix didn't stick, treat the recurrence as a meta-incident, and dig one layer deeper.

## The Recurring Incident Pattern

A recurring incident follows a predictable pattern. The first occurrence triggers a post-mortem. The post-mortem identifies a proximate cause: "The model server ran out of memory." The action item is a proximate fix: "Add memory monitoring and auto-restart on OOM." The fix deploys. The incident stops. Three months later, it happens again. The monitoring alert fired, the auto-restart triggered, but the restart failed because the underlying issue — a memory leak in the embedding cache — was never addressed.

The recurrence is often slightly different. The first incident was model server OOM. The second incident was model server OOM during peak traffic. The third incident was model server OOM after a model update. Each time, the proximate cause looks the same, but the context changes. The team treats each occurrence as a separate incident because the triggers differ. They don't recognize it as a recurring incident until the fifth or sixth occurrence, by which point the pattern is obvious and the team has wasted months on fixes that didn't stick.

The detection signal for recurring incidents is incident tagging. If you tag incidents by root cause — "memory exhaustion," "rate limit exceeded," "upstream timeout" — you can query for incidents with the same tag across time. If "memory exhaustion" appears four times in six months, you have a recurring incident. If "upstream timeout" appears twice in the same month, you might have one. The tags reveal the pattern that individual post-mortems miss. Without tagging, recurring incidents look like isolated failures until someone manually notices the repetition.

## Why Fixes Don't Stick

The most common reason fixes don't stick is that they address symptoms, not causes. The model server ran out of memory because the embedding cache wasn't bounded. The post-mortem action item was "add auto-restart on OOM." That fix contains the blast radius — the service recovers automatically instead of staying down — but it doesn't prevent recurrence. The memory leak still exists. The cache still grows unbounded. The next time traffic spikes, the server runs out of memory again. The fix made the incident less damaging. It didn't make it stop happening.

The second reason is that the fix only works under the original conditions. The December incident happened during off-peak hours. The fix — adding a retry on upstream timeout — worked fine for off-peak traffic. The February incident happened during peak hours, when the upstream service was saturated and retries made the problem worse by amplifying load. The fix that worked in December failed in February because the conditions changed. The team assumed the fix was universal. It was context-specific.

The third reason is organizational. The action item was assigned to a team that didn't have the authority or resources to implement it fully. The post-mortem said "refactor the data pipeline to handle duplicate events." The action item was assigned to the ML team. The ML team doesn't own the data pipeline. They opened a ticket with the data engineering team, the ticket sat in backlog for two months, and the incident recurred before the refactor started. The fix was correct. The ownership was wrong. No one had the responsibility and authority to ensure the fix actually shipped.

## Root Cause of Recurring Incidents

To break the recurrence cycle, you ask the Five Whys not once, but twice. The first Five Whys gets you to the proximate cause. The second Five Whys gets you to the root cause of recurrence. Example: A financial services company's model inference API timed out four times in six months. Each post-mortem identified a different proximate cause: high traffic, slow database query, upstream service delay, model load spike. Each post-mortem had a different fix: add caching, optimize the query, add a timeout, scale the model servers. None of the fixes prevented recurrence.

The second Five Whys asked: Why did we have four different proximate causes for the same symptom? Because the API has no request budget. Why does it have no request budget? Because every component assumes it can take as much time as it needs. Why does every component assume that? Because we never set latency requirements per component. Why not? Because we never decomposed the end-to-end latency budget into per-service budgets. That's the root cause: lack of a latency budget framework. The fix isn't adding caching or optimizing queries. The fix is establishing per-service latency budgets and enforcing them at design time.

The root cause of recurrence is often systemic. It's not a bug. It's a missing process, a missing capability, or a missing organizational structure. The incident keeps happening because the system lacks the defensive mechanism that would prevent it. The model servers keep running out of memory because you don't have structured capacity planning. The upstream timeouts keep causing cascading failures because you don't have circuit breakers or bulkheads. The data quality issues keep reaching production because you don't have automated validation in the data pipeline. The fix isn't patching the specific failure. The fix is building the missing layer of defense.

## Organizational Factors in Recurrence

Some recurring incidents persist because the organization doesn't prioritize the fix. The post-mortem identifies the root cause. The action item is clear. But the fix requires three weeks of engineering time, and the team has a roadmap deadline. The fix gets deprioritized. The incident happens again. The new post-mortem identifies the same root cause. The action item is the same. It gets deprioritized again. This cycle repeats until the incident causes enough damage that leadership overrides the roadmap and allocates time to fix it. By then, the team has lived through six occurrences of a preventable failure.

The defense is making recurrence a prioritization signal. If an incident recurs, the fix automatically moves to P0. Not P1. P0. You stop feature work and fix it. This policy forces a trade-off conversation: Is the recurring incident painful enough to justify stopping feature work? If yes, you stop and fix it. If no, you accept the recurrence as a known operational cost and stop pretending you're going to fix it. Both answers are defensible. What's not defensible is claiming you'll fix it, deprioritizing it, and letting it recur indefinitely.

The second organizational factor is diffusion of responsibility. The action item says "refactor the evaluation pipeline." But three teams touch the evaluation pipeline: ML, Data Engineering, and Platform. Each team assumes one of the other teams will do the refactor. No one does. The incident recurs. The new post-mortem assigns the same action item to the same three teams. It recurs again. The fix requires assigning the action item to a single owner with the authority to coordinate across teams or do the work themselves. Shared ownership is no ownership. Recurring incidents need single-threaded owners.

## Breaking the Recurrence Cycle

The recurrence cycle breaks when you treat the recurrence itself as an incident. When the same incident happens a second time, you don't just do a post-mortem on the second occurrence. You do a meta-post-mortem on why the first fix didn't work. The meta-post-mortem has three questions: What was the first fix? Why did we think it would prevent recurrence? Why didn't it? The answers reveal the gap between the first post-mortem's understanding and reality. That gap is the new root cause.

The meta-post-mortem produces meta-action items. Not "add more monitoring" or "optimize the query." Meta-action items address the process or capability gap. "Establish a latency budget framework for all user-facing APIs." "Implement automatic circuit breakers for all external dependencies." "Add pre-deployment resource utilization testing for all model updates." These action items take longer than proximate fixes. They require architectural work, not operational patches. But they prevent recurrence by eliminating the vulnerability, not by mitigating the symptom.

The hardest part of breaking the recurrence cycle is accepting that the first fix was insufficient. The engineer who implemented the first fix feels defensive. The team that approved the first action item feels like their judgment is being questioned. The meta-post-mortem must be blameless toward the original fix. The first fix was reasonable given the information available. The recurrence gave you new information. The meta-post-mortem uses that information to design a better fix. You're not criticizing the first response. You're learning from the fact that it wasn't enough.

## Escalation for Chronic Issues

Some recurring incidents persist despite multiple rounds of fixes. The incident has recurred five times. Each post-mortem identified a deeper root cause. Each fix addressed that root cause. The incident keeps happening. At this point, the problem isn't technical. It's organizational or architectural. The system has a fundamental design flaw that can't be fixed with incremental changes. The fix requires a redesign, a migration, or a strategic decision to replace a component.

These chronic issues require escalation beyond the incident management process. The incident commander escalates to engineering leadership: "This incident has recurred five times. We've implemented fixes at the application, infrastructure, and process layers. It keeps happening. The root cause is architectural: the model serving stack assumes synchronous upstream calls, but our upstreams have unpredictable latency. The fix is migrating to an async event-driven architecture. That's a six-month project. We need a decision: accept the recurrence, allocate resources for the migration, or redesign the product to reduce upstream dependencies."

The escalation forces the strategic trade-off into the open. Leadership can allocate resources, accept the risk, or change the product. What they can't do is keep expecting the on-call team to fix an unfixable architecture with operational patches. Escalation isn't admitting defeat. It's acknowledging that some problems require investment, not ingenuity. The teams that escalate chronic issues get resources or clarity. The teams that don't escalate burn out trying to patch an architecture that won't hold.

## When to Accept Recurrence Risk

Not every recurring incident needs to be fixed. Some incidents recur because the fix is more expensive than the damage. A media company's content recommendation model occasionally served stale recommendations for ten minutes during cache invalidation. The incident recurred monthly. The fix was migrating to a zero-downtime cache update strategy. The migration required four weeks of engineering time. The impact of the incident was minimal: ten minutes of stale recommendations, no user complaints, no revenue impact.

The team did the math. Four weeks of engineering time cost more than twelve occurrences of the incident. They documented the decision: "This incident will continue to recur approximately once per month. Impact is minimal. Fix cost exceeds damage cost. Accepted as operational risk. Will revisit if impact increases." They updated the runbook, added monitoring, and stopped doing post-mortems for this specific incident. The next time it happened, the on-call engineer followed the runbook, mitigated it in ten minutes, and moved on. No post-mortem, no action items, no guilt.

Accepting recurrence is a valid strategy when the impact is low and the fix is expensive. The key is making the decision explicit, documenting it, and revisiting it periodically. You don't silently tolerate the recurring incident. You explicitly accept it as a calculated trade-off. If the impact changes — if the ten-minute stale cache starts affecting revenue or user satisfaction — you revisit the decision. But until then, you allocate your engineering time to problems where the fix provides better return on investment. Not every fire needs to be extinguished. Some fires are cheaper to monitor and contain.

The post-mortem said the fix was deployed. The incident happened again anyway. That's the moment you stop treating incidents as isolated events and start treating them as symptoms of a deeper system weakness. The team that fixes recurring incidents is the team that asks why the first fix didn't stick, digs one layer deeper, and treats recurrence as a meta-problem requiring meta-solutions. The alternative is doing the same post-mortem every quarter and wondering why reliability never improves.
