# 11.9 — On-Call Load Balancing: Preventing Burnout

In early 2025, a machine learning team at a logistics company lost three senior engineers in four months. Exit interviews revealed the same pattern: all three cited burnout from on-call as a primary reason for leaving. The company's AI-powered routing system generated incidents at unpredictable intervals — sometimes quiet for weeks, then five alerts in one weekend. The on-call rotation included only seven engineers. Two were on parental leave. One was ramping up after joining recently and could not take primary shifts yet. That left four engineers covering 24/7 on-call for a system that paged multiple times per week.

Each engineer was on-call every fourth week. That meant one week per month where they could not make evening plans, could not travel without a laptop, could not drink alcohol, and could not sleep soundly. It meant being woken up at 2am to investigate a model accuracy drop, then working a full day the next morning because the incident did not count as a substitute for regular working hours. It meant canceling family dinners mid-meal because the alert fired and the SLA clock was ticking. After six months, the engineers were exhausted. After nine months, they started interviewing elsewhere.

The company eventually expanded the on-call pool to twelve engineers and implemented a compensation policy that paid extra for every incident response. The attrition stopped. But rebuilding the team took a year and cost the company institutional knowledge, productivity, and morale. The cost of fixing on-call load balancing was a fraction of the cost of ignoring it until people quit.

On-call is necessary for reliable AI systems. But on-call, when poorly managed, destroys teams. Load balancing is not optional. It is the difference between a sustainable rotation and a burnout factory.

## The Burnout Problem in AI On-Call

AI on-call has unique characteristics that make burnout more likely than traditional software on-call. Traditional systems fail in predictable ways. A database goes down. A service runs out of memory. An API dependency times out. These failures have clear root causes, clear mitigation steps, and clear resolution criteria. The on-call engineer investigates, applies a fix, and the incident is over.

AI systems fail in ambiguous ways. Model accuracy drops by 6 percentage points and you do not know why. The model generates outputs that feel wrong but do not violate any automated checks. The system works correctly on average but fails catastrophically on a narrow slice of inputs. These failures require investigation, hypothesis generation, experimentation, and often consultation with data scientists or domain experts. They are cognitively demanding. They take longer to resolve. And they often leave the on-call engineer uncertain about whether the incident is truly resolved or just temporarily mitigated.

This ambiguity creates a psychological burden that traditional on-call does not have. When the on-call engineer resolves a database failure, they know it is fixed — the database is back online, the queries are succeeding, the incident is closed. When the on-call engineer mitigates an AI failure, they are often left wondering: did I actually fix the root cause, or did I just stop the bleeding? Will this happen again tomorrow? Should I have escalated further? This uncertainty lingers after the incident ends. It makes it harder to disconnect from on-call even after the shift is over.

AI on-call also has unpredictable incident volume. Traditional systems tend to fail at consistent rates. You can predict that you will get paged once or twice per week based on historical patterns. AI systems can be quiet for a month and then fire ten alerts in three days because input distribution shifted or a data pipeline introduced subtle corruption. This unpredictability makes it harder to plan personal life around on-call. You cannot confidently schedule a weekend trip during your on-call week because you do not know if the system will stay quiet or explode.

Finally, AI incidents often require extended investigation. A traditional incident might take thirty minutes to resolve. An AI incident might take three hours — or eight hours if the root cause is buried in data drift that requires analyzing thousands of examples. An engineer who gets paged at 11pm and spends until 3am debugging a model accuracy drop is not going to be effective the next day. But most companies still expect that engineer to work their regular shift because the incident did not cause a full outage. This expectation — work all night, work all day — is unsustainable.

Burnout from on-call is not a personal resilience problem. It is a system design problem. If your on-call rotation is burning people out, the rotation is wrong.

## Measuring On-Call Load

You cannot balance load without measuring it first. Most teams track the wrong metrics. They count the number of incidents per person and declare the rotation balanced if everyone gets paged the same number of times. This is insufficient. Two incidents are not equivalent if one takes fifteen minutes to resolve and the other takes four hours. Three quiet pages during work hours are not equivalent to one middle-of-the-night page that requires immediate response.

Effective load measurement tracks three dimensions: frequency, duration, and disruption.

**Frequency** is the number of pages per on-call shift. This is the baseline metric. If one engineer is getting paged twelve times per week and another is getting paged twice, the load is not balanced. But frequency alone does not capture the full picture.

**Duration** is the total time spent responding to incidents per on-call shift. This includes investigation time, mitigation time, and follow-up communication. An engineer who receives three pages that each take twenty minutes to resolve has spent one hour on incident response. An engineer who receives one page that takes five hours to resolve has spent five hours on incident response. The second engineer has a heavier load even though they received fewer pages. Track duration for every incident. Measure it per engineer per week. If one engineer is averaging eight hours of incident response per on-call week and another is averaging two, the load is not balanced.

**Disruption** is the impact of the incident on the engineer's life. A page during work hours is disruptive but manageable. A page at 2am is severely disruptive — it interrupts sleep, degrades next-day performance, and creates anxiety about future nights. A page during a planned vacation is catastrophically disruptive. Most teams do not measure disruption quantitatively, but they should. One approach: assign a disruption weight to each incident based on when it occurred. Work hours: 1x weight. Evening: 2x weight. Middle of night: 4x weight. Weekend: 3x weight. Vacation: 6x weight. Multiply the incident duration by the disruption weight. This gives you a disruption-adjusted load metric.

An engineer who spends two hours on an incident at 2am has a disruption-adjusted load of eight hours. An engineer who spends two hours on an incident at 11am has a disruption-adjusted load of two hours. Both engineers spent the same amount of time on incidents, but the first engineer experienced four times the disruption. If you only track raw duration, you miss this difference.

Track all three dimensions. Frequency tells you who is getting paged most often. Duration tells you who is spending the most time on incidents. Disruption tells you who is experiencing the most stress. A balanced rotation minimizes all three.

## Rotation Design for Balance

The structure of your on-call rotation determines whether load is balanced or concentrated. The most common rotation structures are weekly primary, follow-the-sun, and tiered escalation. Each has trade-offs.

**Weekly primary rotation** assigns one engineer as the primary on-call responder for a full week. All pages go to that engineer first. A secondary engineer backs up the primary if they are unavailable or if the incident requires escalation. The primary engineer rotates weekly — this week it is Engineer A, next week it is Engineer B, and so on. This structure is simple and predictable. The primary engineer knows they are on-call for seven days straight. Everyone else knows they are off-call and can plan their lives accordingly.

The downside is concentration. If incidents cluster during one engineer's week, that engineer bears the full load. If the system is quiet during another engineer's week, that engineer experiences no load at all. Over a year, the average load per engineer may be balanced, but the week-to-week experience is highly variable. This variability makes it harder to plan personal life and increases the psychological burden of on-call.

**Follow-the-sun rotation** distributes on-call across multiple time zones. Engineers in New York cover daytime hours in the Americas. Engineers in London cover evening hours in the Americas and daytime hours in Europe. Engineers in Singapore cover nighttime hours in the Americas and daytime hours in Asia. No single engineer is on-call 24/7. Each engineer covers roughly eight to ten hours per day during their local working hours.

This structure eliminates middle-of-the-night pages, which drastically reduces disruption. It also requires a larger on-call pool — at least three engineers per follow-the-sun rotation, often more. Many companies do not have global teams large enough to implement this structure. But if you do have global distribution, follow-the-sun is the most humane rotation design.

**Tiered escalation rotation** splits on-call into tiers based on severity and complexity. Tier 1 on-call handles routine incidents that can be resolved with runbooks — restarting a model, adjusting a rate limit, acknowledging a known transient failure. Tier 2 on-call handles complex incidents that require investigation, hypothesis testing, or coordination across teams. Tier 1 pages escalate to Tier 2 if they cannot resolve the incident within a defined time threshold or if the incident meets escalation criteria.

This structure allows you to distribute load based on experience. Junior engineers can cover Tier 1 shifts and build familiarity with the system without being thrown into high-complexity incidents. Senior engineers cover Tier 2 shifts and handle the incidents that require deep system knowledge. Tier 1 engineers get paged more frequently, but the pages are shorter and less cognitively demanding. Tier 2 engineers get paged less frequently, but the pages are longer and require more expertise.

The downside is handoff latency. If a Tier 1 incident escalates to Tier 2, you lose time during the handoff. The Tier 1 engineer has to document what they tried, the Tier 2 engineer has to read that documentation and rebuild context, and the incident clock keeps ticking. Tiered escalation works well for systems where most incidents are routine and only a small fraction require deep expertise. It works poorly for systems where every incident is ambiguous and requires investigation.

Most teams use weekly primary rotation because it is simplest. If your incident volume is low — fewer than three pages per week on average — weekly primary works fine. If your incident volume is higher, or if your incidents frequently occur outside work hours, consider follow-the-sun or tiered escalation. The goal is to ensure that no engineer is carrying more than ten to twelve hours of disruption-adjusted load per month. Beyond that threshold, burnout risk rises sharply.

## Handling Incident Spikes

Even a well-designed rotation becomes unbalanced when incidents spike. A data pipeline bug introduces corruption that causes fifty alerts over three days. A model regression triggers cascading failures across multiple downstream systems. A compliance audit surfaces violations that require immediate remediation. These spikes are unpredictable and unavoidable. The question is how you handle them without destroying your on-call engineer.

The first mitigation is time-boxing. If an incident takes longer than two hours to resolve, the on-call engineer should escalate — not because they need help, but because they need relief. A senior engineer or an incident commander takes over coordination. The on-call engineer continues to contribute, but they are no longer solely responsible for driving the incident to resolution. This prevents a single engineer from spending eight hours straight on one incident.

The second mitigation is shift swapping. If the on-call engineer has been handling incidents continuously for more than eight hours in a 24-hour period, they should be relieved for the remainder of their shift. Another engineer takes over on-call, and the original engineer is given compensatory time off. This prevents the scenario where an engineer works all night on incidents and is then expected to work a full day the next morning.

The third mitigation is rotation compression. If incidents spike during one engineer's week, shorten their rotation. Instead of staying on-call for seven days, they rotate out after three days. The next engineer in the rotation takes over early. This distributes the spike load across multiple engineers instead of concentrating it on one.

The fourth mitigation is incident throttling. If the system is generating excessive alerts due to a known issue — for example, a model is failing consistently because a data pipeline is down — suppress the redundant alerts. The on-call engineer does not need to be paged fifteen times for the same root cause. Page once, create a ticket for remediation, and suppress further pages until the root cause is fixed. This reduces frequency load without hiding real incidents.

The fifth mitigation is temporary on-call pool expansion. If incidents spike and the spike is expected to last more than a few days, temporarily add engineers to the on-call rotation. This reduces the frequency with which any individual engineer is on-call and spreads the load across more people. Once the spike resolves, return to the standard rotation size.

Incident spikes will happen. The question is whether your team has a plan for handling them without burning out your on-call engineers. If your plan is "whoever is on-call deals with it," you do not have a plan.

## Compensation for Heavy On-Call Periods

On-call work is work. It requires availability, cognitive effort, and personal sacrifice. Most companies do not compensate on-call adequately. They treat on-call as an expected part of the job, like attending meetings or writing code. This is a mistake. On-call is categorically different from regular work because it imposes a burden even when no incidents occur — the engineer must remain available, stay sober, keep their laptop nearby, and maintain mental readiness to respond. This burden has value. Compensating it appropriately signals that the company respects the engineer's time and sacrifice.

Compensation structures vary. Some companies pay a flat stipend for carrying the on-call pager — typically $200 to $500 per week, regardless of whether any incidents occur. This compensates the availability burden. Other companies pay per incident — typically $50 to $200 per incident, depending on duration and time of day. This compensates the response burden. The most effective structures combine both: a flat stipend for availability plus per-incident pay for actual response.

Compensation should scale with disruption. An incident that occurs at 2am should pay more than an incident that occurs at 2pm. An incident during a holiday or vacation should pay substantially more. This acknowledges that not all incidents impose the same cost on the engineer's life.

Beyond financial compensation, time compensation matters. If an engineer spends four hours responding to incidents outside of work hours, they should receive four hours of compensatory time off during work hours. This prevents on-call from becoming unbounded work — you are not asking the engineer to work 40 hours per week plus on-call, you are asking them to work 40 hours per week including on-call. The total time commitment stays bounded.

Some companies resist compensating on-call because they fear it will become expensive. This fear is misplaced. If your on-call compensation budget is large, it means your system is generating excessive incidents and you have a reliability problem. The appropriate response is to fix the reliability problem, not to refuse compensation. Compensating on-call fairly does not create cost — it reveals cost that was already being paid by your engineers in the form of personal sacrifice and eventual burnout.

## When to Temporarily Remove Someone from Rotation

On-call is not universally appropriate for every engineer at every moment. Certain life circumstances make on-call untenable. If an engineer is experiencing a personal crisis — a family emergency, a health issue, a major life transition — they should be temporarily removed from the rotation. Expecting someone to remain on-call while managing a personal crisis is inhumane and counterproductive. They will not respond effectively to incidents, and the stress will compound their personal situation.

Temporary removal should be easy to request and non-stigmatized. An engineer should be able to say "I need to step out of on-call for the next two weeks" without needing to justify or defend the request in detail. The team adjusts the rotation to cover the gap. When the engineer is ready to return, they return. No questions, no judgment, no career penalty.

Temporary removal is also appropriate after a particularly severe incident. If an engineer just spent twelve hours coordinating a major outage, they should be removed from on-call for the following week. They need time to recover cognitively and emotionally. Expecting them to go back on-call immediately after a traumatic incident increases burnout risk.

New parents should be removed from on-call for at least three months after parental leave. A person who is waking up multiple times per night to care for an infant cannot also be expected to respond to middle-of-the-night pages. Sleep deprivation compounds. Returning to on-call rotation should be gradual — start with secondary on-call only, then transition to primary after the parent has stabilized their sleep schedule.

Finally, engineers who are ramping up on a new system should not carry primary on-call until they have completed incident response training and shadowed at least two on-call shifts. Throwing a new engineer into on-call before they understand the system is unfair to the engineer and risky for the system.

The ability to temporarily step out of on-call without penalty is a sign of a healthy engineering culture. Teams that make on-call mandatory regardless of circumstances create resentment, burnout, and attrition.

## Building Team Resilience

Individual load balancing is necessary but insufficient. Team-level resilience requires redundancy, knowledge distribution, and psychological safety.

**Redundancy** means your on-call rotation can absorb the loss of one or two engineers without collapsing. If your rotation includes five engineers and losing one engineer makes the rotation unsustainable, you do not have enough redundancy. A resilient rotation should have at least eight engineers. This allows for parental leave, vacation, temporary removal due to personal circumstances, and unexpected attrition without overburdening the remaining team.

**Knowledge distribution** means multiple engineers understand every critical subsystem. If only one engineer knows how to debug your model serving pipeline, that engineer will be paged for every serving incident regardless of who is officially on-call. This creates a shadow on-call rotation where certain engineers are always de facto on-call even during their off weeks. Knowledge distribution eliminates single points of failure. Every critical subsystem should have at least three engineers who can handle incidents involving that subsystem.

**Psychological safety** means engineers feel comfortable escalating, asking for help, and admitting uncertainty during incidents. A team with low psychological safety creates situations where on-call engineers try to resolve incidents alone because they fear looking incompetent if they escalate. This extends incident duration, increases error rates, and amplifies stress. A team with high psychological safety creates situations where engineers escalate early, collaborate openly, and resolve incidents faster with less individual burden.

Resilience is not a property of individuals. Resilience is a property of systems. A resilient on-call system is one where no single person is indispensable, no single failure is catastrophic, and no single incident destroys morale.

## On-Call Sustainability Metrics

Load balancing is effective only if you measure whether it is working. Sustainability metrics tell you whether your on-call rotation is healthy or deteriorating.

**Attrition rate among on-call engineers.** If engineers are leaving the team and citing on-call as a reason, your rotation is unsustainable. Track exit interview data. If more than 20 percent of departing engineers cite on-call burden, you have a problem.

**Self-reported stress scores.** After each on-call shift, survey the engineer. Ask them to rate the stress level of the shift on a scale of 1 to 10. Track these scores over time. If average stress scores are above 6, or if scores are rising, your rotation is unsustainable.

**Incident response error rate.** If on-call engineers are making more mistakes during incidents — missing escalation criteria, skipping runbook steps, misinterpreting metrics — it suggests cognitive overload. Track error rate per engineer per shift. If error rates rise during or after high-load periods, your engineers are burning out.

**Time to fill on-call shifts.** If you struggle to find engineers willing to take on-call shifts, or if engineers are requesting to be removed from rotation at increasing rates, your rotation is unsustainable. Voluntary participation in on-call is a signal of a healthy rotation. Reluctance to participate is a signal of dysfunction.

**Compensation cost as a percentage of total engineering cost.** If your on-call compensation budget is growing faster than your engineering headcount, it suggests incident volume is increasing or incident severity is increasing. Both are reliability problems that require systemic fixes, not just better load balancing.

Measure these metrics quarterly. If any metric is deteriorating, investigate. Sustainability is not a one-time fix — it is an ongoing practice.

Your on-call rotation is a system. Like any system, it can be designed well or designed poorly. A well-designed rotation distributes load fairly, compensates engineers appropriately, allows for temporary removal without penalty, and maintains team resilience. A poorly designed rotation burns people out, drives attrition, and creates a culture where on-call is feared rather than accepted as a necessary responsibility. The difference is not luck. The difference is intentional design.

---

The next subchapter covers reliability culture — how to make reliability everyone's responsibility, not just the on-call team's problem, and how to sustain that culture as your organization scales.
