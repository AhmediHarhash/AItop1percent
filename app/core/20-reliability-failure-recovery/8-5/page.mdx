# 8.5 — Action Item Design: Effective vs Performative Fixes

The post-mortem generated fourteen action items. "Improve monitoring." "Enhance testing." "Add documentation." "Review processes." The team assigned owners, set deadlines, and marked the post-mortem complete. Six months later, the same class of failure occurred. Every action item had been marked done, but none of them actually prevented recurrence.

This is the performative action item problem. Teams generate action items that look responsible, that satisfy the expectation that incidents produce work, but that do not address the root cause in a way that prevents the next failure. The difference between effective and performative action items is not effort or intent — it is specificity, measurability, and a direct causal link to preventing recurrence.

## The Performative Action Item Problem

Performative action items share common patterns. They are vague: "Improve monitoring" does not specify what metrics to add, what thresholds to set, or what alerts to create. They lack measurable completion criteria: "Enhance testing" does not define what tests, what coverage, or what passing means. They do not have clear owners: "Review processes" assigned to "the team" means no one is accountable. They do not link directly to the root cause: "Add documentation" might be useful, but if the root cause was a missing validation check, documentation does not prevent recurrence.

Performative action items exist because they satisfy the social expectation that incidents produce action. They let teams close the post-mortem feeling like they responded. They give management something to track in incident review meetings. But they do not change the system in ways that prevent the next failure.

A retail AI company experienced a model drift incident that affected product recommendations. The post-mortem generated the action item: "Improve model monitoring." Six engineers marked themselves as owners. Three months later, the action item was marked complete. The team had added a dashboard showing model latency and error rates — metrics that were already visible elsewhere. They had not added drift detection, performance metric tracking, or automated alerts on recommendation quality degradation. The next drift incident happened four months later. The performative action item had been completed, but the root cause had not been addressed.

The psychology of performative action items is understandable. Writing specific, measurable actions requires understanding the root cause deeply enough to know exactly what change will prevent recurrence. That takes time and thought. Performative action items can be written quickly during the post-mortem meeting without deep analysis. They allow the meeting to end on time with a list that looks complete.

But performative action items are worse than no action items. They create the illusion of progress while allowing the vulnerability to persist. They consume engineering time on work that does not prevent recurrence. They teach teams that post-mortem action items are bureaucratic rituals, not meaningful improvements. When the next incident happens despite completed action items, teams lose trust in the post-mortem process entirely.

## Characteristics of Effective Action Items

An effective action item has five properties: it is specific, measurable, owned, causal, and scoped. Specific means it names exactly what will change — not a category of work, but the actual artifact or system that will be different. Measurable means there is an objective test for whether it is complete. Owned means one person is responsible for ensuring it happens, even if they delegate execution. Causal means it directly addresses the root cause in a way that prevents recurrence. Scoped means it can be completed in a reasonable timeframe without blocking on dependencies outside the owner's control.

Compare two action items from the same incident:

Performative: "Improve data validation." Assigned to the data team. Deadline: end of quarter.

Effective: "Add schema validation to the customer upload pipeline that rejects files with missing required fields and logs rejection reasons to the ops dashboard. Validation must run before data reaches the model. Owner: Sarah. Completion criteria: schema validation deployed to production, tested with five malformed upload examples, ops dashboard showing rejection logs. Deadline: two weeks."

The effective action item specifies what system will change, what the change does, how to verify it works, who ensures it happens, and when it will be done. There is no ambiguity about what "complete" means. The root cause was malformed uploads reaching the model — the action item directly prevents that by adding validation before the model sees the data.

Effective action items often create new artifacts: a new validation layer, a new alert, a new runbook section, a new test suite, a new schema definition, a new release gate. If the action item is complete but you cannot point to the artifact that now exists, it was likely performative. "We improved monitoring" is performative. "We added a CloudWatch alert that triggers when model p95 latency exceeds 800ms for more than five minutes, notifies the on-call engineer via PagerDuty, and links to the latency troubleshooting runbook" is effective. The alert exists. You can click on it. You can test it by simulating high latency.

Effective action items are also falsifiable. You can test whether they work before the next real incident. A validation rule can be tested with malformed data. An alert can be tested by simulating the failure condition. A runbook can be tested by walking through it during a drill. Performative action items are not testable because they do not specify what behavior should change.

## SMART Criteria for Incident Action Items

The SMART framework applies directly to incident action items. Specific, Measurable, Achievable, Relevant, Time-bound. Incident action items often fail on Specific and Measurable, sometimes on Achievable, rarely on Relevant or Time-bound.

Specific: The action item names the exact change. Not "improve alerting" but "add a PagerDuty alert for eval suite pass rate dropping below 90 percent." Not "better documentation" but "add a runbook section for diagnosing model latency spikes with step-by-step commands and expected outputs." Not "enhance testing" but "add integration tests for the top five prompt failure modes identified in the incident."

Measurable: There is an objective test for completion. For a validation rule: does it reject the malformed input that caused the incident? For an alert: does it fire when the failure condition is simulated? For a runbook: can a new team member follow it and resolve the issue without help? For a test: does it fail before the fix and pass after? If you cannot test completion, the action item is not measurable.

Achievable: The owner can complete it in the given timeframe without dependencies they do not control. Action items that require other teams, or budget approvals, or tool purchases often stall. If an action item requires cross-team coordination, that coordination becomes part of the action item. "Request access to prod model metrics from Platform team and add metrics to our dashboard" is achievable because requesting access is within the owner's control. "Wait for Platform team to implement new metrics API" is not achievable by this team.

Relevant: The action item directly prevents recurrence of the root cause. Ask: if this action item had been completed before the incident, would the incident have been prevented or significantly mitigated? If the answer is no, the action item is not relevant to this incident. It might be useful work, but it does not belong in this post-mortem.

Time-bound: The deadline is near enough to create urgency but realistic given the scope. Most effective action items are completed within two to four weeks. If an action item requires three months, break it into smaller milestones. "Rebuild the entire data pipeline" is not a single action item — it is a project. "Add schema validation to the upload endpoint" can be done in two weeks and prevents the most common failure mode immediately.

## Ownership and Accountability

Each action item has exactly one owner. Not a team, not multiple people, one person. That person is responsible for ensuring the action item is completed, even if they delegate the actual work. The owner is the person who updates status, who escalates if blocked, who confirms completion, and who answers when leadership asks "Is this done?"

The owner does not have to do the work personally. A senior engineer might own an action item that a junior engineer executes. A manager might own an action item that three different engineers contribute to. But there is always one throat to choke. When the deadline passes and the action item is incomplete, one person is accountable for explaining why.

Ownership without authority fails. The owner must have the authority to prioritize the work, allocate resources if needed, and escalate if blocked. If the owner is a junior engineer and the action item requires senior approval or cross-team coordination, either assign the action item to someone with that authority or explicitly give the junior engineer the authority for this task.

Track ownership explicitly in your incident management system. Each action item has an owner field. The owner receives automated reminders as the deadline approaches. The owner is responsible for updating the status field: not started, in progress, blocked, completed. When an action item is blocked, the owner is responsible for escalating to whoever can unblock it, not for waiting silently.

A financial services company made ownership explicit by requiring the owner's name in the action item text itself. "Sarah will add schema validation to the upload pipeline" not "Add schema validation to the upload pipeline (Owner: Sarah)." This small change made ownership psychologically stronger. The action item was now a commitment Sarah made, not a task assigned to her. Completion rates improved from 60 percent to 85 percent within three months.

## Prioritization Frameworks

Not all action items are equally important. Some prevent severe incidents, some prevent minor issues, some are hygiene improvements. Prioritize action items by severity of the incident they prevent, likelihood of recurrence, and effort required.

A simple prioritization matrix: High severity, high likelihood of recurrence — do these immediately. High severity, low likelihood — do these soon but not at the expense of high-likelihood items. Low severity, high likelihood — do these if effort is small, otherwise batch them. Low severity, low likelihood — document them but do not schedule them unless you have spare capacity.

Severity is the impact if the incident recurs: user-facing failure, revenue loss, regulatory risk, data breach, safety issue. Likelihood is the probability of recurrence: does this failure mode depend on rare conditions or common conditions? Effort is the engineering time required to implement the fix.

A healthcare company used a prioritization rule: any action item that prevents a patient safety incident gets resourced immediately regardless of effort. Any action item that prevents a HIPAA violation gets resourced within one week. All other action items are prioritized by impact divided by effort — maximize risk reduction per engineering-hour. This rule made trade-offs explicit and defensible.

Beware the trap of only completing easy action items. If your completed action items are all documentation updates and your incomplete action items are all architectural changes, you are systematically avoiding the hardest, most impactful work. Review action item completion by type. If 90 percent of completed items are low-effort and 90 percent of incomplete items are high-effort, adjust your prioritization to ensure critical-but-hard items get done.

## Tracking Action Item Completion

Track action items in a system that supports status updates, deadline reminders, and completion verification. A spreadsheet works for small teams. A dedicated incident management tool works better for larger teams. The system should surface overdue action items, send reminders before deadlines, and require evidence of completion before marking items done.

Evidence of completion matters. The owner does not just mark an action item complete — they provide evidence. For a validation rule: link to the pull request and the test that verifies it works. For an alert: link to the alert configuration and a screenshot of a test firing. For a runbook: link to the runbook section and confirmation that someone unfamiliar with the incident successfully followed it. Evidence makes completion verifiable and prevents action items from being marked done without actually fixing anything.

Review overdue action items weekly in a standing meeting or async channel. The owner explains why the item is overdue and what the new timeline is. If the action item is blocked, escalate immediately. If the action item is no longer relevant, close it with a reason. If the action item is stalled because the owner is overloaded, reassign it or deprioritize other work. Do not let action items sit in "in progress" limbo for months.

A common failure mode: teams diligently track action items until the post-mortem meeting ends, then never look at them again. The action items live in a document that no one reads. Schedule explicit follow-up. One week after the post-mortem, check status. Two weeks after, check again. At the deadline, verify completion. This discipline ensures action items are not just generated but acted upon.

## Measuring Action Item Effectiveness

Completion is not the same as effectiveness. An action item can be completed without preventing recurrence if it was the wrong action item. Measure effectiveness by tracking whether similar incidents recur after action items are completed.

A logistics company tracked incident recurrence by failure category. After a data quality incident, they generated action items and marked them complete. They then monitored for data quality incidents over the next six months. If data quality incidents dropped to near zero, the action items were effective. If data quality incidents continued at the same rate, the action items were performative or incorrect.

This recurrence tracking revealed that 30 percent of their action items did not prevent recurrence. The action items were completed, but the same failure mode appeared again. They audited those action items and found common patterns: too vague, addressed symptoms rather than root causes, or assumed the root cause was X when it was actually Y. This feedback loop improved their action item quality over time.

Another metric: time to resolution for future incidents in the same category. If you complete action items after an incident and the next similar incident resolves faster because of those action items, they were effective. A model drift incident leads to action items that add drift detection and automated alerts. The next drift incident is detected within one hour instead of one week. That time reduction proves the action items worked.

Track action item completion rates over time. If your team completes 90 percent of action items within their deadlines, your action items are well-scoped and prioritized. If completion rates are 40 percent, your action items are too ambitious, too numerous, or poorly prioritized. Adjust by writing fewer, more focused action items or extending deadlines to match actual engineering capacity.

## When to Close Action Items

An action item is closed when it is completed with evidence, or when it is no longer relevant. Completed means the change is deployed to production, tested, and verified to work. Evidence means the owner can point to the artifact, the pull request, the alert configuration, the runbook section. No action item is closed without evidence.

No longer relevant means circumstances changed such that the action item is obsolete. The root cause was re-analyzed and the action item addressed the wrong cause. A different fix was implemented that makes this action item redundant. The system that the action item would modify has been replaced. Close these action items explicitly with a reason, not by letting them sit incomplete indefinitely.

Never close an action item because it is too hard or too time-consuming. If the action item is critical but expensive, break it into phases. Phase one might be a partial fix that prevents the most common failure mode. Phase two is the complete solution. Close phase one when it is done. Keep phase two open or create it as a separate tracked item. But do not close an important action item simply because it is inconvenient.

A pattern to avoid: teams that close 100 percent of action items by re-labeling incomplete items as "no longer relevant." This gaming of metrics destroys the value of tracking action items. Leadership sees high completion rates and assumes the team is responding well to incidents. Meanwhile, critical fixes never happen because they are marked irrelevant to meet completion targets.

Review closed action items in quarterly incident retrospectives. Look at the action items that were marked complete and ask: did we see similar incidents after these were completed? If yes, the action items were ineffective or incomplete. If no, they worked. This long-term view separates action items that genuinely prevented recurrence from those that merely satisfied the process.

Effective action items are specific, measurable, owned, causal, and scoped. They create artifacts that can be pointed to and tested. They prevent recurrence of the root cause. Performative action items are vague, unverifiable, and do not link to prevention. They satisfy the expectation of response without changing the system in meaningful ways. The difference is not effort — it is clarity about what will change and why that change prevents the next failure. Your post-mortems are only as valuable as the action items they generate, and action items are only valuable if they prevent recurrence.

Next: 8.6 — The Incident Database: Building Institutional Memory
