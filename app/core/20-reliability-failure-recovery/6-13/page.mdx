# 6.13 — Provider Coordination During Joint Incidents

At 3:47 AM, your model latency spiked from 450ms to 3.2 seconds. Your monitoring caught it immediately. Your team investigated. Your infrastructure was fine. Your prompt caching was working. Your request queuing was normal. At 4:15 AM, you checked the provider status page. Nothing. You opened a support ticket marked Sev-1. At 4:47 AM, you got a response: "We're investigating." At 5:30 AM, your CEO was asking why your product had been degraded for nearly two hours. At 6:10 AM, the provider posted a status update acknowledging a regional latency issue. At 7:45 AM, it resolved. You had no control. You had no visibility. You had no way to communicate accurate information to your users because you didn't know what was happening inside your provider's infrastructure.

This is joint incident reality. Your AI system depends on provider infrastructure. When that infrastructure fails, your incident involves two organizations with different incentives, different communication protocols, and different definitions of severity. A Sev-1 incident for you might be Sev-3 for your provider if you're a small customer. Managing joint incidents requires knowing when to coordinate, how to escalate, what information to share, and when to implement emergency provider switches.

## Recognizing When Provider Coordination Is Needed

The first question during any AI incident: is this our problem or their problem? The distinction determines your response. If it's your problem — bad prompts, incorrect configuration, application-level bugs — you own the resolution. If it's their problem — API downtime, model degradation, infrastructure issues — you need provider coordination.

The diagnostic pattern is systematic. Check your own infrastructure first. Are your services healthy? Are your request rates normal? Are your error rates consistent with past patterns? If yes, the issue likely lives in provider infrastructure or model behavior. Check the provider status page. Most major providers — OpenAI, Anthropic, Google, AWS Bedrock — maintain real-time status pages. If there's a posted incident, you have confirmation. If there's no posted incident but you're seeing issues, you're either hitting an edge case they haven't detected or you're early to a broader incident.

Next, check provider API errors. A 503 Service Unavailable means their infrastructure is overloaded or down. A 429 Too Many Requests means you're hitting rate limits, which is your configuration, not their incident. A 500 Internal Server Error without explanation means something broke on their side. A 200 success code with degraded output quality means model-level issues, not infrastructure.

Cross-validate with external sources. Check social media, check status monitoring services, check community forums. If other users are reporting the same symptoms at the same time, it's a provider issue. If you're alone, it might still be a provider issue — you might be the canary — but your evidence is weaker.

The determination matters because it changes your escalation strategy. If it's definitively a provider issue, you escalate aggressively. If it's ambiguous, you investigate further before escalating. False escalations damage your credibility with provider support teams. Real incidents that you don't escalate damage your users.

## Provider Support Escalation Paths

Every major AI provider has tiered support. Standard support for most customers. Priority support for paying enterprise customers. Dedicated support for strategic accounts. Your contract determines your access. Know what level you have before incidents happen.

Standard support means submitting a ticket and waiting. Response time SLAs are usually measured in hours, sometimes days. During a Sev-1 production incident, this is unacceptable. If you're on standard support, your incident response plan must assume you will get no meaningful provider help during the incident window. You need fallback strategies — secondary providers, cached responses, degraded mode operations — because you can't depend on fast resolution.

Priority support means faster ticket response and access to a support phone number. Response time SLAs drop to 30-60 minutes for critical issues. This is the minimum viable support tier for production AI systems serving external users. If you're running production workloads on standard support, you're accepting unmitigated availability risk.

Dedicated support means you have a named contact at the provider, often a technical account manager or customer success engineer. During incidents, you can reach them directly. They can escalate internally to engineering teams. They have visibility into internal incident status before it's posted publicly. This tier is expensive but essential for high-scale or high-stakes AI deployments.

Escalation within a support tier follows a pattern. You open a ticket with maximum severity. You provide clear reproduction steps, error logs, request IDs, timestamps. You state user impact in business terms, not technical terms. "Our product is unavailable to 50k users" is more compelling than "API returning 503s." If you don't get response within your SLA, you escalate to the next level — reply to the ticket demanding escalation, call the support number if you have one, reach out to your account contact if you have one.

If you're a large customer and you don't have dedicated support, you're under-supported. If you're generating six figures or more in annual provider revenue, you should have a direct escalation path. Negotiate it into your contract renewal.

## Sharing Diagnostic Information With Providers

When you open a provider support ticket during an incident, the information you provide determines response quality. Vague reports get slow responses. Precise reports with evidence get fast responses.

The essential diagnostic package includes six elements. First, the symptom: what's broken, how you're observing it, when it started. "Model latency increased from 450ms p95 to 3.2s p95 starting at 03:47 UTC, affecting 100% of requests." Second, your configuration: which model, which API endpoint, which region, which parameters. "GPT-5.1 via us-west-2 endpoint, temperature 0.7, max tokens 2048." Third, reproduction: specific request IDs or example requests that demonstrate the issue. "Request ID req_abc123 took 3.1s, normally 400ms."

Fourth, your investigation: what you've already ruled out. "Our infrastructure is healthy, request rate is normal at 200 req/s, no recent config changes, issue started abruptly without corresponding change on our side." Fifth, user impact: how many users, what functionality is broken, what business impact. "Affecting 50k daily active users, product search unavailable, estimated revenue impact $12k per hour." Sixth, your status page or communication: what you've told your users. "We've posted a status update attributing the issue to our AI provider and are monitoring."

This level of detail does three things. It shows you've done your homework. It gives the provider's support team the information their engineering team will ask for. It demonstrates severity through specificity. A support engineer who sees this package knows they're dealing with a competent customer experiencing a real incident, not a confused developer who didn't read the docs.

Avoid sharing sensitive data in support tickets. If your prompts contain user PII, customer business logic, or proprietary information, sanitize your examples. Replace real data with realistic synthetic data. "User John Smith asked about his account balance of $5,432" becomes "User asked about their account balance." The provider needs to see the structure and pattern, not your actual data.

## Joint Incident Calls

For severe incidents affecting multiple customers, providers sometimes run joint incident calls. You join a conference line with the provider's incident commander and engineering team. Other affected customers might be on the same call. These calls have specific etiquette and strategy.

Your role on a joint call is not to solve the provider's technical problem. Their engineers will do that. Your role is to provide information about your symptoms, answer questions about your configuration, and understand the expected timeline for resolution. You are a data source and a stakeholder, not a collaborator in their debugging process.

Come prepared with your diagnostic package. When asked what you're seeing, state it clearly and concisely. When asked about your configuration, have it ready. When asked about reproduction, provide request IDs. The call is not the time to investigate or discover new information about your own system. That should be done before the call.

Listen for timeline commitments. If the provider says "we expect resolution within 30 minutes," note the time and hold them to it. If 30 minutes pass without resolution, ask for an updated timeline. If the provider says "we don't have a timeline yet," ask what information they need to establish one. Your users are waiting. Vague promises are not sufficient.

Ask about workarounds. "Is there a different endpoint or region we can use?" "Can we roll back to a previous model version?" "Are there parameters we can adjust to mitigate impact?" Even partial mitigation is valuable. If you can reduce user impact by 50% while waiting for full resolution, do it.

Document the call in your incident timeline. Who was on the call, what was said, what commitments were made. This documentation protects you if the provider later disputes what was promised or if you need to escalate to account management or legal.

## The Their Fault Versus Our Fault Determination

During joint incidents, there's often a period of ambiguity. Is this the provider's problem or yours? The provider's support team might suggest it's your configuration. Your team might believe it's their infrastructure. This determination has contractual and reputational implications.

The evidence-based approach: run controlled experiments. If possible, test against a different provider's equivalent model. If the symptom disappears, it's provider-specific, suggesting their problem. If the symptom persists, it might be your prompts or application logic. Test against a different region or endpoint with the same provider. If the symptom is regional, it's their infrastructure. If it's global, it might be model behavior.

Test with known-good requests from your validation suite. If requests that passed QA yesterday now fail, something changed outside your control. If requests that always failed continue to fail, it's not a regression, it's a pre-existing issue you're newly noticing.

The provider will have their own investigation. They'll check their logs for your request IDs. They'll look for error patterns across customers. If you're the only customer reporting an issue and their internal metrics show healthy performance, they'll push back. If multiple customers are reporting similar issues, they'll acknowledge the incident.

In ambiguous cases, your status communication should be careful. Don't blame the provider publicly if you're not certain. "We're investigating a service degradation affecting model performance, working with our infrastructure partners to identify root cause" is accurate and doesn't assign fault prematurely. Once fault is clear, you can be more specific. "Our service degradation was caused by an infrastructure issue at our AI provider, now resolved" is appropriate after confirmation.

## When Provider Response Is Too Slow

Sometimes the provider's response is inadequate. They're not providing updates. They're not acknowledging severity. They're treating your Sev-1 as their Sev-3. You have options, but they're limited.

First option: escalate within the provider. If you have a technical account manager, contact them directly. If you have an account executive or sales contact, loop them in. Business relationships sometimes unlock faster technical response. Be direct: "Our product has been down for two hours, we have 50k affected users, and we've received no substantive update from support. We need executive escalation."

Second option: implement emergency mitigation without waiting for provider resolution. Switch to a backup provider if you have multi-provider architecture. Serve cached or degraded responses if you have that capability. Put your AI feature into maintenance mode and route users to non-AI alternatives if possible. These options require advance planning. You can't build multi-provider switching during an incident.

Third option: go public. Post on social media tagging the provider's official accounts. This is a last resort and should only be used when you have clear evidence the problem is theirs and their response is genuinely inadequate. Going public burns relationship capital. Use it only when the incident severity justifies it and other options have failed.

Fourth option: post-incident escalation. After the incident resolves, you schedule a post-mortem with your account team. You document the impact, the timeline, the communication gaps. You ask for service credits if your contract includes them. You ask for process improvements to prevent recurrence. This doesn't help during the current incident, but it improves future incidents.

The fundamental constraint: you're dependent on your provider. You can't force them to fix their infrastructure faster. Your incident response plan must account for this dependency. The best preparation is redundancy — architectural patterns that reduce single-provider dependency.

## Documenting Provider Involvement

Provider involvement must be documented in your incident timeline. Not just "contacted provider support" but the full interaction history. Ticket number, time opened, initial response time, updates received, resolution time, root cause provided.

This documentation serves three purposes. First, it's your evidence for service level credits if your contract includes availability SLAs. If the provider's infrastructure caused your downtime, you may be entitled to compensation. Your documentation proves the impact and timeline.

Second, it's your input for internal reliability analysis. If 30% of your incidents in the past quarter involved provider issues, you have a strategic dependency problem. You need architectural changes — multi-provider redundancy, more aggressive caching, fallback modes. Without documentation, you can't quantify the pattern.

Third, it's your leverage for contract negotiations. When you're renewing your provider contract, the history of provider-caused incidents informs your negotiation position. "We've had six P0 incidents in the past year caused by your infrastructure, totaling 14 hours of downtime. We need either better SLAs or lower pricing to account for this reliability risk." Your documentation makes this argument concrete.

Document not just the technical facts but the business impact. "Provider incident caused 4.2 hours of customer-facing downtime, affecting 85k users, estimated revenue loss $37k, 1,200 support tickets generated." This quantification turns an abstract technical event into a business cost that executives and procurement teams understand.

## Post-Incident Coordination With Providers

After a joint incident resolves, you need post-incident coordination with the provider. Request a post-mortem or incident review. Larger providers conduct these for significant incidents. Smaller providers might not have formal processes, but you can still request a written summary of root cause and remediation.

The post-mortem should answer five questions. What was the technical root cause? Why didn't the provider's monitoring catch it earlier? What's being done to prevent recurrence? What's the timeline for those preventive measures? What process improvements are being made to improve customer communication during future incidents?

If the provider's post-mortem is inadequate, push back. "Your root cause analysis states 'infrastructure issue' without specifics. We need technical detail to assess whether similar failures could occur with different services or configurations." You're entitled to enough information to make informed architectural decisions.

Use the post-mortem to inform your own resilience strategy. If the root cause was single-region failure, maybe you need multi-region request routing. If it was a specific model version issue, maybe you need version pinning with faster rollback capability. If it was rate limiting under unexpected load, maybe you need better provider capacity planning.

Document any commitments the provider makes. "Provider committed to implementing improved monitoring for this failure mode by end of Q2 2026." Follow up in Q2 to confirm delivery. Providers sometimes make commitments during post-incident reviews that are forgotten later. Your documentation creates accountability.

Joint incidents are a reality of AI system operations. You depend on provider infrastructure. When it fails, your system fails. The quality of your provider coordination — how fast you escalate, how clearly you communicate, how effectively you mitigate — determines how much your users suffer. Teams that build provider coordination into their incident response process recover faster. Teams that treat it as an afterthought extend outages while they figure out who to call.

Next, we address the incidents that trigger legal obligations — knowing when and how to notify regulators, customers, and legal authorities when AI incidents cross from operational failures to compliance events.
