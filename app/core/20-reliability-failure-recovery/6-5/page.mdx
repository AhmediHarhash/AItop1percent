# 6.5 — Diagnosis Under Pressure: Finding Root Cause Fast

Containment stops the bleeding. Diagnosis tells you why you were bleeding in the first place. And in the middle of an AI incident, with executives asking for ETAs and customers reporting issues and the system running in degraded mode, you need to find the root cause fast. But AI systems are not like traditional software. A web service fails because a database connection pool is exhausted or a dependency timed out or a deploy introduced a null pointer exception. The failure is discrete, the logs are clear, and the cause is usually obvious within minutes. An AI system degrades because the input distribution shifted or the model provider silently updated their API or a retrieval pipeline started returning stale documents or the user behavior pattern changed in a way the model was never trained for. The failure is probabilistic, the logs are ambiguous, and the cause is often invisible in individual requests.

Diagnosing AI failures under incident pressure requires a systematic approach. You cannot rely on intuition. You cannot chase symptoms and hope you stumble into the cause. You need a diagnostic framework that narrows the possibility space quickly, confirms hypotheses with evidence, and gets you to a root cause decision within 30 minutes. Not a perfect diagnosis, not a deep understanding of every contributing factor — a root cause decision. Enough clarity to choose the right fix and restore service. The full post-mortem can happen later. Right now, you need to know what to do.

## The Symptom Versus Cause Problem

The first diagnostic trap is confusing symptoms with causes. A symptom is what you observe: precision dropped, latency increased, user satisfaction scores fell, hallucination rate spiked. A cause is the mechanism that produced the symptom: a model version changed, a prompt template was updated, retrieval started returning off-topic documents, input traffic shifted to a demographic the model handles poorly.

Symptoms are easy to detect but do not tell you what to fix. Causes tell you what to fix but are hard to isolate. And in an incident, the pressure is to react to the symptom. "Precision is down, let's tune the threshold." "Latency is up, let's add more replicas." "Hallucinations are spiking, let's add a disclaimer." These are symptom responses. They might make the metric look better temporarily, but they do not address the underlying issue. Two days later, the symptom recurs or a different symptom appears because the root cause is still there.

The diagnostic discipline is to resist the urge to fix symptoms and instead ask: what changed? AI systems do not spontaneously degrade. Something changed in the inputs, the model, the retrieval pipeline, the prompt, the infrastructure, or the user population. Your job is to identify what changed and when. If you can pinpoint the change, you can test whether reverting it resolves the symptom. If reverting it works, you have found the cause. If it does not, you move to the next hypothesis.

## The What Changed Investigation

The what changed investigation is the most reliable diagnostic tool for AI incidents. It works because AI systems are deterministic given the same inputs, model, and infrastructure. If behavior changes, something in that stack changed. Your job is to walk backward through every layer and identify the deltas.

Start with the model. Did the model version change? Did your model provider push an update? Check your model version logs. If you are using an API, compare the responses you are getting now to responses you got last week for the same inputs. If you are self-hosting, check your model registry for any recent deployments. If the model changed, that is your leading hypothesis. Deploy the previous model version to a test environment, run your eval suite, and see if the degradation disappears.

Next, check the prompt. Did anyone update the system prompt or the user prompt template? Pull your prompt version control logs. If you do not have version control for prompts, check your deployment history for any changes to configuration files or environment variables that store prompt text. Even a small change — adding a sentence, reordering instructions, changing a single word — can shift model behavior. If the prompt changed, revert it in staging, run eval, and confirm the impact.

Next, check retrieval. If your system uses RAG, did the retrieval behavior change? Did your vector database get reindexed? Did your embedding model update? Did the underlying document corpus change — new documents added, old documents removed, metadata updated? Compare retrieval results from today to retrieval results from last week for the same queries. If retrieval changed, that is your hypothesis. Test with the old retrieval behavior.

Next, check inputs. Did the user population or query distribution shift? Pull request logs and compare the last 24 hours to the previous week. Are users asking different types of questions? Are they using different phrasing? Are they accessing features they rarely used before? A sudden influx of queries from a new geographic region or a new user segment can surface model weaknesses that were always there but never at scale. If input distribution shifted, the fix is not to revert inputs — users are users — but to acknowledge the shift and either adapt the model or adjust the feature scope.

Next, check infrastructure. Did compute resources, API quotas, or network topology change? Did you get rate-limited by a model provider? Did autoscaling rules kick in and reduce the number of inference replicas? Did a cloud region experience degraded performance? Check your infrastructure monitoring, your cloud provider status page, and your cost dashboards for anything unusual. Infrastructure rarely causes AI quality degradation directly, but it can cause latency increases that force timeouts, which then trigger fallback logic that produces lower-quality results.

## The Comparative Diagnosis Pattern

The comparative diagnosis pattern is simple: find two states where behavior differs, hold everything else constant, and isolate the variable. If production is degraded but staging is fine, compare the environments. What is different? If production behavior changed between Monday and Tuesday, compare Monday logs to Tuesday logs. What inputs, what model behavior, what retrieval results differ?

This pattern requires that you have a known-good baseline to compare against. If you do not have recent logs from when the system was working correctly, you cannot compare. If you do not have a staging environment that mirrors production, you cannot isolate variables. If you do not have versioned artifacts for your model, prompt, and retrieval configuration, you cannot test rollbacks. The time to build comparative diagnosis capability is not during the incident. It is in advance.

For AI systems, the most powerful comparison is running your eval suite against both the current degraded behavior and a suspected previous good state. If your eval suite ran on Friday and showed 92% precision, and today it shows 84% precision, you have a quantified baseline. You can test hypotheses by running the eval suite against candidate fixes and seeing if precision returns to 92%. If you do not have eval suite history, you run the eval now against the current state and then again after each attempted fix. The eval suite becomes your diagnostic tool, not just your quality gate.

## The What If It Is Not a Change Problem

Not all AI incidents are caused by something changing. Sometimes the system was always fragile, and you just hit a rare edge case at scale. A user submits a query that triggers a failure mode the model was never tested for. A sequence of interactions creates a context state that confuses the model. A rare document in your retrieval corpus dominates results in a way that breaks output quality.

These incidents are harder to diagnose because there is no change to identify. The diagnostic approach is to isolate the failure to specific inputs and then manually inspect those inputs for patterns. Pull the last 100 requests that triggered low quality scores. Read them. What do they have in common? Are they unusually long? Are they in a specific language? Do they reference niche topics? Do they contain formatting that the model struggles with? If you can characterize the failure pattern, you can build a test case, reproduce it in staging, and design a targeted fix — maybe a preprocessing filter, maybe a prompt adjustment, maybe a circuit breaker for that input type.

If you cannot isolate a pattern — if the failures seem random and irreproducible — you escalate. You bring in a second engineer to review your diagnostic work. You pull in a data scientist to analyze the failure distribution. You notify leadership that the root cause is unclear and you are expanding the investigation. This is not failure. This is appropriate escalation when the standard diagnostic approach does not yield answers.

## The Diagnostic Time Box

Diagnosis during an incident is time-boxed. You do not spend six hours searching for a perfect root cause explanation. You spend 30 to 60 minutes on systematic hypothesis testing, and then you make a decision. Either you have identified a likely cause and you test a fix, or you have not and you escalate to a different response strategy.

The time box is not arbitrary. It is based on how long your containment can hold and how much business cost the degraded service incurs. If you shifted to a fallback that can run indefinitely, you have more time. If you rate-limited traffic and users are blocked, you have less. The incident commander — the person running the response — enforces the time box. "We have been diagnosing for 40 minutes. What is our current hypothesis? Do we have a testable fix? If not, we escalate."

Escalation does not mean giving up. It means shifting strategy. If diagnosis is stalled, you might escalate to a broader rollback. "We do not know what changed, but we know the system worked on Friday. We are rolling back all changes deployed since Friday." You might escalate to extended containment. "We cannot diagnose the root cause quickly enough. We are keeping the fallback in place and treating this as a longer-term investigation." You might escalate to a workaround. "We cannot fix the model behavior right now, but we can add a post-processing filter that catches the failure mode." The key is not to stay stuck in diagnosis mode when diagnosis is not progressing.

## Diagnostic Tools for AI Incidents

The diagnostic tools you use during an incident are the same tools you built before the incident. You need request tracing that shows the full path from input to output. You need eval suite automation that can run on demand against any model version. You need prompt and model version control that lets you compare current state to any previous state. You need retrieval logging that captures what documents were retrieved and what scores they had. You need input distribution monitoring that shows whether query patterns shifted.

If you do not have these tools, you are diagnosing blind. You are reading tea leaves in production logs and hoping to spot a pattern. This works occasionally, but it is not systematic, and it burns time. The engineering investment to build diagnostic tooling is not optional. It is infrastructure for reliability. Every dollar spent on diagnostic tooling buys you faster incident resolution and smaller damage radius when things go wrong.

## The Diagnostic Hypothesis Log

During diagnosis, you document your hypotheses and test results. You do not do this in your head. You write it in the incident channel. "Hypothesis 1: model version changed. Testing rollback to previous version. Result: no improvement." "Hypothesis 2: retrieval embedding model updated. Checking logs. Result: no change detected in last 7 days." "Hypothesis 3: input query distribution shifted. Comparing query logs. Result: confirmed 18% increase in non-English queries starting Tuesday."

This log serves three purposes. First, it prevents duplicate work. If two engineers are diagnosing in parallel, the log shows what has already been tested. Second, it creates a record for post-incident review. After the incident is resolved, you go back and analyze whether your diagnostic process was efficient or whether you wasted time on low-probability hypotheses. Third, it forces discipline. Writing a hypothesis and a test plan before executing makes you think through what evidence would confirm or refute the hypothesis. It prevents random trial and error.

## When You Find the Root Cause

When you identify the root cause, you do not immediately deploy the fix to production. You test it. You apply the fix in staging, run your eval suite, and confirm that quality metrics return to baseline. You manually inspect a sample of outputs to verify that the fix does not introduce new issues. You check that the fix does not break unrelated functionality. Then you deploy to production with monitoring active, and you watch the metrics for 15 minutes before declaring the incident resolved.

This testing discipline feels slow when you are under pressure to restore service. But deploying an untested fix during an incident is how you turn one incident into two. The fix might not work. Worse, it might work for the failure mode you diagnosed but break something else. You contain, you diagnose, you test, and then you fix. The testing step is not optional.

Diagnosis under pressure is not about finding the perfect explanation. It is about finding an actionable cause fast enough to restore service before the business cost becomes unacceptable. You use systematic hypothesis testing, comparative analysis, and time-boxed decision-making to get from symptom to fix. And when diagnosis stalls, you escalate instead of guessing. The goal is not to be right. The goal is to restore service with evidence-based decisions.

---

Next: **6.6 — Decision Paralysis Under Ambiguous Quality Signals**