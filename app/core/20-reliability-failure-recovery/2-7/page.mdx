# 2.7 — The Correlation Problem: Distinguishing AI Failures from Traffic Changes

Your hallucination rate doubled overnight. The page fires. The war room assembles. Engineering digs through logs for two hours before someone in product mentions that you launched a new customer segment yesterday — enterprise legal teams asking contract questions instead of the usual consumer FAQ traffic. The model is fine. Your queries got harder. You just burned two engineer-hours and created organizational panic over a traffic change, not a failure.

This is the correlation problem. When metrics move, you need to know whether the system broke or the inputs changed. Most AI monitoring systems conflate these. They alert on output changes without understanding input changes. The result is false positives that erode trust in your alerting and mask real failures when they occur.

The difficulty is that both look identical from the outside. A spike in refusal rate could mean your content filter broke. Or it could mean a bot swarm just hit your API with policy-violating queries. A drop in task completion could mean your prompt regressed. Or it could mean your marketing team ran a campaign that brought in users who need a different workflow. Without input distribution awareness, every output change looks like a failure.

## Why Traffic Changes Masquerade as Failures

AI systems are more sensitive to input distribution than traditional software. A web server handles GET requests the same way regardless of whether they come from mobile or desktop, new users or returning users. An AI model behaves fundamentally differently when the query complexity changes, when the user intent shifts, when the domain changes.

This sensitivity creates monitoring problems traditional observability tools were not built to handle. If latency spikes, you check for infrastructure issues. If error rate increases, you check for code bugs. But if hallucination rate increases, the cause could be infrastructure, code, model, prompt, or just different questions. And the different questions are not an error — they are legitimate traffic that happens to be harder.

The enterprise legal team is not doing anything wrong. Their queries are longer, more ambiguous, require more context. The model's behavior on these queries is measurably different from its behavior on consumer FAQ traffic. If you monitor only the output metrics without segmenting by input characteristics, you see degradation when none exists. If you then roll back a deployment or switch models in response, you make changes for the wrong reasons and lose the ability to learn what actually causes failures.

This is not a corner case. Traffic composition changes constantly. New features launch. Marketing campaigns target different demographics. Seasonal patterns shift. Bots probe your API. Users discover new ways to use your product. Every change in traffic composition changes your output metrics, and most of those changes are not failures. If your monitoring cannot distinguish between the two, your alerts are noise.

## Input Distribution Shift vs Output Quality Degradation

The solution is monitoring input characteristics alongside output metrics. When output metrics change, you need to immediately see whether input metrics changed first. This requires defining and tracking input features that correlate with model behavior.

For text generation systems, these features include query length, query complexity, domain indicators, language, user segment, time of day, device type. For retrieval systems, they include document corpus version, query specificity, number of results requested. For agent systems, they include tool usage patterns, conversation length, task type. The exact features depend on your domain, but the principle is universal: track the inputs that affect model behavior.

When hallucination rate spikes, your monitoring should show whether average query length increased, whether a new user cohort started sending traffic, whether a particular domain suddenly dominates. If input metrics moved first, the output change is likely correlational. If output metrics moved without input changes, the output change is likely causal — something in your system broke.

This does not mean input changes are ignorable. If the enterprise legal queries have double the hallucination rate of consumer queries, you have a quality problem for that segment. But it is a different problem than a regression. A regression means something that worked stopped working. A segment quality gap means you have harder traffic that your system is not yet handling well. The mitigation for a regression is rollback or hotfix. The mitigation for a segment quality gap is prompt tuning, model selection, or feature development. Conflating the two leads to wrong responses.

Segmented monitoring makes this distinction automatic. Instead of one aggregate hallucination rate, you track hallucination rate per user segment, per query complexity bucket, per domain. When the aggregate rate spikes, you immediately see which segments drove the change. If it is all segments simultaneously, that is a system failure. If it is one new segment, that is a traffic composition change. The alert logic is different for each.

## The New User Problem

New users behave differently from existing users. They ask different questions. They misunderstand your product's capabilities. They trigger edge cases veterans learned to avoid. In consumer products, this effect is obvious — first-time users need onboarding, make mistakes, churn faster. In AI systems, it also changes model behavior.

A healthtech startup saw their symptom checker's accuracy drop five points over two weeks. Investigation revealed no code changes, no prompt changes, no model changes. What changed was user growth. They had launched a referral campaign. The new users were asking about rarer conditions the model was less confident about, and they were providing less detailed symptom descriptions. The model's behavior was appropriate for the inputs it received. The accuracy drop was real, but the cause was user composition, not system failure.

This creates a monitoring challenge. User growth is good. But it changes your baseline metrics, and if your monitoring treats those changes as degradation, you get false alerts during your best growth periods. The solution is cohort-based monitoring. Track metrics separately for users in their first session, first week, first month, and long-term users. When aggregate metrics move, check whether cohort metrics moved or whether cohort mix changed.

If your first-session accuracy is always lower than long-term accuracy, and your aggregate accuracy dropped because you grew first-session users faster than long-term users, that is not a failure. That is successful growth with a known quality gap for new users. The response is not incident mitigation. The response is new user onboarding improvements — better prompts, better examples, better guardrails for common new-user mistakes.

Cohort-based monitoring also helps detect real failures faster. If first-session accuracy drops for new users while staying stable for long-term users, something broke for the cold-start case specifically. If accuracy drops across all cohorts simultaneously, something broke system-wide. Without cohort segmentation, both scenarios look like generic accuracy drops, and you lose diagnostic information during the critical first minutes of an incident.

## A/B Test Contamination in Monitoring

A/B tests change traffic distribution by design. Half your users get treatment A, half get treatment B. If treatment B performs worse, your aggregate metrics degrade while the test runs. If your alerting does not know about active experiments, it fires false alerts on intentional changes.

This happens more often than it should. A fintech company ran an experiment testing a more cautious version of their credit advice model — fewer recommendations, higher confidence thresholds. Aggregate task completion dropped fifteen points. Monitoring alerted. On-call investigated. They discovered the experiment thirty minutes later. The alert was correct that metrics degraded, but incorrect that it represented a failure. It represented an experiment working as designed.

The solution is experiment-aware monitoring. Your alerting system needs to know which experiments are running, which user segments are in treatment vs control, and which metrics are expected to change. When an experiment launches, you suppress alerts on expected metric changes for the treatment population. You still alert on unexpected changes — if task completion drops forty points instead of fifteen, something is wrong. But you do not treat designed behavior as failure.

This requires integration between your experimentation platform and your monitoring platform. When an experiment starts, it registers with monitoring. When it ends, it unregisters. During the experiment, monitoring segments metrics by experiment arm. After the experiment, winning variants become the new baseline, and monitoring adjusts thresholds accordingly. Without this integration, every experiment risks false alerts, and teams start ignoring alerts during experiment periods, which masks real failures.

## Building Causal Inference into Detection

The correlation problem is ultimately a causal inference problem. You observe output changes and need to infer whether those changes were caused by system failures or input changes. Statistical correlation is not enough — you need causal models.

The most practical approach for production systems is comparative monitoring. When output metrics move, immediately compare them across multiple dimensions: treatment vs control groups, old cohort vs new cohort, high-complexity queries vs low-complexity queries, US traffic vs international traffic. If metrics moved in all dimensions simultaneously, the cause is likely system-wide. If metrics moved in only one dimension, the cause is specific to that dimension.

This is not perfect causal inference. It does not rule out confounders. But it is fast, automatable, and good enough for incident detection. When an alert fires, your runbook should show not just the degraded metric but also the segmentation analysis: which user populations are affected, which input characteristics correlate with the change, which system components show anomalies. The on-call engineer sees immediately whether this looks like a failure or a traffic change.

For deeper causal analysis, build intervention detection. Track known changes to your system — deployments, configuration updates, upstream dependency changes, feature flag flips — and correlate them with metric changes. If metrics degrade within minutes of a deployment, the deployment is the likely cause. If metrics degrade with no recent changes, the cause is external — traffic shift, provider issue, or silent dependency failure.

Intervention detection requires telemetry integration. Your deployment system reports to your monitoring system. Your feature flag system reports to your monitoring system. Your dependency health checks report to your monitoring system. When an alert fires, the context includes recent interventions. This does not prove causation, but it narrows the hypothesis space from infinite possibilities to a manageable set.

The goal is not perfect attribution during the alert. The goal is fast triage. When metrics degrade, you need to know within two minutes whether to investigate your system or your traffic. Correlation analysis gives you that. Then during the incident, you build the full causal model. But the initial detection must separate signal from noise, or your alerting becomes unusable.

## Organizational Ownership of Input Monitoring

Building input-aware monitoring is not just an engineering task. It requires partnership with product, data science, and operations. Product understands user segments and traffic composition. Data science understands feature engineering and distribution modeling. Operations understands alerting workflows and runbook design. Engineering builds the instrumentation and integration.

The most successful teams treat input monitoring as a shared responsibility. Product defines user segments and maintains segment definitions. Data science defines input features and validates their predictive power. Operations defines alerting rules and maintains runbooks. Engineering builds the pipeline that makes all of this queryable in real time during incidents.

Without this partnership, input monitoring either does not happen or happens in a way that is not operationally useful. Engineering builds input telemetry that no one uses. Product maintains segment definitions that never reach monitoring. Data science builds beautiful distribution models that do not integrate with alerting. The components exist but do not connect, and the correlation problem persists.

Your system broke, or your traffic changed. That question should be answerable in under two minutes for any alert. If it takes longer, you have a monitoring gap. If it is not answerable at all, you have a correlation problem. And if you are treating traffic changes as failures, you are training your organization to ignore alerts. Fix the correlation problem before it erodes trust in your entire observability stack.

---

Next, we examine provider-side versus application-side detection — distinguishing between failures in the AI provider and failures in your own system.
