# 5.11 — Testing Failover: Scheduled Provider Rotation

The incident began at 9:14 AM when the primary model provider experienced an API outage. The engineering team had built multi-provider failover eight months earlier. The architecture was clean. The runbooks were documented. The failover logic had been tested in staging. Confidence was high. The on-call engineer triggered failover. Traffic switched to the backup provider. Requests started timing out. The backup provider's API keys had expired three weeks earlier. No one noticed because the keys were only used during failover tests, and no failover test had run in four months. The team scrambled to regenerate keys, update configs, and redeploy. Failover took 28 minutes instead of the planned two minutes. Users experienced 26 minutes of unnecessary downtime because a backup system that looked ready was not actually ready.

Failover that is not tested is not failover. It is a plan. Plans decay. Credentials expire. APIs change. Dependencies drift. The only way to know that failover works is to invoke it regularly under controlled conditions.

## Why Untested Failover Is Not Failover

Failover systems have a half-life. The moment you deploy them, they start decaying. API keys expire. Provider rate limits change. Network configurations drift. Monitoring alerts get muted. Engineers who understood the system leave. Documentation becomes outdated. A failover path that worked perfectly six months ago may fail completely today, and you will not know until you need it.

The most common failure mode: expired credentials. Backup provider API keys are generated, stored in secrets management, and then forgotten. Many API keys expire after 90 days or require rotation. If you have not used the backup provider in 90 days, the keys may be dead. You discover this during an incident when failover triggers and every request returns 401 Unauthorized. Fixing this during an outage adds 10 to 30 minutes to recovery time.

Another common failure: configuration drift. Your application uses a specific model version on the primary provider. During a failover test, the backup provider is configured for the same model version. Three months later, you upgrade the primary to a newer model. The backup configuration is not updated. During a real failover, the backup provider serves responses from the old model. Quality degrades. Users complain. You spend 20 minutes diagnosing why the backup provider is behaving differently before realizing the config is stale.

Dependency changes are invisible until failover. The backup provider announces that they are deprecating a feature your system depends on. The announcement goes to a mailing list no one monitors. Three weeks later, you fail over and the feature does not work. The backup provider helpfully returns an error: "This endpoint was deprecated on January 15th." Your incident response now includes figuring out how to refactor code to work without the deprecated feature.

Provider rate limits are often misconfigured. Your backup provider account has a rate limit of 1,000 requests per minute. Your production traffic is 5,000 requests per minute. During normal operation, this does not matter—the backup receives no traffic. During failover, you immediately hit the rate limit. Requests are throttled. The system appears slow or broken. You have not tested at scale, so you did not discover that the rate limit is insufficient.

## Scheduled Failover Testing: The Game Day Pattern

The solution is scheduled failover testing. Once per quarter, you deliberately fail over to the backup provider for a defined period—30 minutes, one hour, or a full business day. You monitor quality, latency, error rates, and user feedback. You document issues. You validate that runbooks are accurate. You train the team on failover procedures. This is the reliability equivalent of a fire drill.

The game day is announced in advance. Engineering, product, support, and leadership know it is happening. Users may or may not be notified depending on whether you expect noticeable quality changes. The goal is to exercise the failover path under controlled conditions where failures are educational rather than catastrophic. If something breaks during a game day, you fix it before the next real incident.

The test plan includes specific success criteria. Failover triggers within 60 seconds. Backup provider serves traffic without errors. Latency stays below 2x baseline. Quality metrics stay above defined thresholds. Monitoring and alerting work for both providers. Rollback to primary completes within 60 seconds. On-call runbooks are accurate. If all criteria pass, the failover system is validated. If any fail, you have work to do before the next test.

Game days surface issues that staging tests miss. Staging environments do not have production traffic patterns, production data diversity, or production scale. A failover that works in staging with 10 requests per minute may fail in production with 5,000 requests per minute. Production testing is the only way to validate real readiness.

## Production Rotation Testing

A more aggressive version of scheduled testing is production rotation. Instead of failing over once per quarter for an hour, you rotate traffic between providers weekly or daily. Provider A handles Monday, Provider B handles Tuesday. Or Provider A handles 80% of traffic and Provider B handles 20%, switching weekly. This keeps both integrations fresh and ensures neither path becomes stale.

Rotation testing provides continuous validation. If a provider releases a breaking API change, you discover it within days instead of months. If credentials are about to expire, you hit errors during routine rotation rather than during an incident. If quality degrades, you see it in weekly metrics rather than discovering it during an emergency failover.

The cost of rotation testing is higher operational complexity. You are deliberately introducing provider variance into production. Users may notice quality differences. Support may receive questions about why responses seem different day to day. Monitoring becomes more complex because you need to track metrics separately for each provider and identify which provider is responsible for any quality regression.

Some teams use rotation testing only for non-critical traffic. Internal tools, staging environments, or low-stakes features rotate between providers continuously. Customer-facing production traffic stays on the primary provider with quarterly failover testing. This balances the validation benefit of rotation against the risk of user-visible quality variance.

## Gradual Rotation Versus Full Switch

Full switch testing means 100% of traffic fails over to the backup provider instantly. This validates that the backup can handle full load and that failover logic works under maximum stress. The risk: if the backup fails under load, all users experience an outage. Full switch is high-confidence, high-risk testing.

Gradual rotation means incrementally shifting traffic: start with 5%, then 10%, then 25%, then 50%, then 100%. At each step, you validate that quality and latency stay within bounds. If issues arise, you stop the rotation and roll back. Gradual rotation is lower risk but slower. It takes 30 to 60 minutes to fully fail over instead of 60 seconds.

The gradual approach is safer for first-time failover tests or after major changes to the backup provider. If you have never tested failover in production, start gradual. Once you have confidence, move to full switch testing. The trade-off: gradual rotation does not test whether your backup can handle 100% load instantly, which is the actual failure scenario. You need both: gradual tests to build confidence, full switch tests to validate real readiness.

Canary failover is a middle ground. During a test, route 10% of production traffic to the backup provider continuously for one hour. Monitor closely. If metrics stay healthy, declare the test successful. If metrics degrade, roll back immediately. Canary testing validates the backup provider under real load without risking full traffic exposure.

## Measuring Failover Test Success

A failover test is not successful just because it completes without errors. Success means meeting defined quality and performance thresholds. The metrics to track: error rate, latency at p50/p95/p99, quality scores on sampled outputs, user satisfaction feedback, task completion rate, and escalation rate to human support.

Error rate should stay below 1% during failover. If errors spike, investigate whether they are transient (rate limits, cold start latency) or systemic (broken API integration, missing feature support). Transient errors are acceptable if they resolve within minutes. Systemic errors require fixing before the next test.

Latency should stay below 2x baseline. If your primary provider serves requests in 600 milliseconds and your backup serves them in 1.8 seconds, users will notice but the system is functional. If backup latency is 8 seconds, the system feels broken. Either optimize the backup integration or accept that failover means degraded performance and communicate that to users.

Quality scores must stay above your minimum acceptable threshold. If baseline quality is 92% and your threshold is 85%, backup quality must exceed 85% during the test. Sample 100 to 500 outputs during the test, run them through your evaluation suite, and compare to baseline. If backup quality is 78%, you cannot rely on that provider for failover. Find a better backup or improve the integration.

User satisfaction is a lagging indicator but critical. Monitor support tickets, chat feedback, and social media mentions during and after the test. If users complain about weird responses, slow performance, or degraded quality, the test revealed a problem even if infrastructure metrics looked fine. User perception is reality.

## Finding Issues Before Real Incidents

The purpose of testing is to find problems when they are fixable, not during emergencies. Every issue discovered during a game day is an issue that will not surprise you during a real incident. Expired credentials, rate limit misconfigurations, quality gaps, latency spikes—finding these during controlled tests is a win.

Document every issue. After each test, write a report: what broke, why it broke, how it was fixed, and what changes prevent recurrence. Common issues from failover tests: API keys expired, rate limits too low, prompt formatting incompatible with backup provider, backup provider does not support a feature the app depends on, monitoring did not switch to backup provider dashboards, runbooks had outdated commands, on-call engineer did not know how to trigger rollback, cost spiked because backup provider pricing is higher than expected.

Each of these issues is solvable. Automate API key rotation. Negotiate higher rate limits. Refactor prompts for cross-provider compatibility. Build feature detection logic that gracefully handles missing capabilities. Update monitoring configs to track both providers. Refresh runbooks after every test. Train on-call engineers on failover procedures. Budget for backup provider costs. Testing exposes the gaps. Fixing them makes the next test smoother and the next real incident survivable.

## Building Organizational Confidence in Failover

Failover is not just a technical system. It is an operational capability that requires team confidence. Engineers must trust that triggering failover will not make things worse. Product must trust that users will not churn due to quality degradation. Leadership must trust that failover is worth the investment. Confidence comes from repeated successful tests.

After the first failover test, the team is nervous. After the fifth, they are confident. After the tenth, failover is routine. The goal is to make failover boring. Boring is reliable. Boring is predictable. Boring means the system works and the team knows how to operate it. Game days build this confidence.

Involve the entire team in game days, not just engineering. Product should observe quality metrics during failover. Support should monitor user feedback. Leadership should understand what degraded service looks like and whether it is acceptable. Marketing should know how to communicate if a real incident requires extended failover. The game day is cross-functional training.

Post-game-day retrospectives are critical. What worked? What broke? What surprised us? What should we change before the next test? The retrospective should produce action items with owners and deadlines. Testing without follow-through wastes the learning. Testing with follow-through builds a culture of reliability.

## Documentation and Runbooks from Tests

Runbooks are only accurate if they are tested. A runbook written six months ago and never used is probably wrong. Commands change. Dashboards move. Procedures evolve. Failover tests validate runbooks and surface necessary updates.

The runbook should be executable: step-by-step commands that an on-call engineer can follow during an incident. Not vague guidance like "switch to backup provider." Specific commands: "Run this script. Check this dashboard. Verify these metrics. If errors occur, escalate to this person." The more specific, the better. During an incident, cognitive load is high. Clear instructions reduce mistakes.

Update the runbook immediately after each test. If a command failed, fix it. If a step was missing, add it. If a verification check was unhelpful, replace it. The runbook should reflect the most recent test, not the original design. A runbook that is updated quarterly through testing is trustworthy. A runbook that was written once and never touched is not.

Include rollback procedures. Failover is not complete until you can switch back to primary. The rollback runbook should be as detailed as the failover runbook: how to verify primary is healthy, how to shift traffic back, how to monitor for issues after rollback, how to confirm that primary is serving traffic correctly. Rollback is often overlooked in planning but critical in practice.

## Testing the Monitoring and Alerting Layer

Failover is only useful if you know when to trigger it. That requires monitoring that detects primary provider failures and alerts the on-call engineer. The monitoring itself must be tested. During a game day, validate that alerts fire when primary goes down, that dashboards switch to show backup provider metrics, and that on-call engineers receive notifications through the correct channels.

A common failure: monitoring is configured for the primary provider but not the backup. When you fail over, you lose visibility. You are serving traffic through the backup but you cannot see latency, error rates, or quality metrics. You are flying blind. Configure monitoring for all providers before you need it. Test that the monitoring works during failover.

Alert fatigue is a risk during failover. If failover triggers dozens of alerts—primary is down, traffic shifted, latency increased, quality degraded—on-call engineers may miss the critical signal in the noise. Tune alerts to fire only for actionable issues. A single alert: "Failover to backup provider activated. Quality is within acceptable range. Primary provider recovery expected in 30 minutes" is more useful than 15 alerts about individual metrics.

Test the monitoring during every game day. Verify that dashboards update, that alerts fire, that metrics are accurate. If monitoring fails during a test, it will fail during a real incident. Fix it before it matters.

## The Continuous Improvement Loop

Each failover test improves the system. The first test finds 10 issues. You fix them. The second test finds 5 new issues. You fix them. The third test finds 2 issues. By the tenth test, failover is smooth and reliable. This is the continuous improvement loop: test, find issues, fix issues, test again.

The improvement is not just technical. It is operational, organizational, and cultural. The team learns how to execute failover calmly. Documentation becomes accurate. Monitoring becomes reliable. Confidence grows. Reliability becomes a capability, not an aspiration.

Some organizations schedule game days monthly. Others quarterly. The frequency depends on how often your dependencies change and how critical reliability is. High-stakes systems test more frequently. Lower-stakes systems test less often. The minimum is twice per year. Less than that, and skills atrophy, documentation decays, and systems drift.

## From Testing to Operational Readiness

Testing proves that failover can work. Operational readiness means failover will work when you need it. The difference is practice, documentation, automation, and confidence. A team that runs quarterly failover tests, updates runbooks after every test, automates credential rotation, monitors both providers continuously, and trains new engineers on failover procedures has operational readiness.

A team that built failover once, tested it in staging, and has not touched it in six months does not have operational readiness. They have a system that used to work and might work again if they are lucky. Luck is not a reliability strategy.

Operational readiness requires investment: time to run tests, engineering effort to fix issues, budget for backup provider costs, organizational commitment to reliability. The investment pays off during incidents. The team that invested in readiness responds in minutes. The team that skipped testing responds in hours and makes mistakes under pressure.

## The Transition to Incident Response

Multi-provider failover is one layer of reliability. It protects against provider outages. It does not protect against all failures. Models can degrade silently while providers are healthy. Quality can regress due to prompt changes or data drift. Security incidents can compromise systems regardless of which provider serves traffic. Failover is necessary but not sufficient.

The next chapter covers incident response: how to detect when AI systems fail, how to respond when failures occur, how to communicate during incidents, and how to learn from failures to prevent recurrence. Reliability is not just about having backups. It is about knowing when to use them, how to use them, and what to do when even backups are not enough.

---

Next: **Chapter 6 — Incident Response and Crisis Management**
