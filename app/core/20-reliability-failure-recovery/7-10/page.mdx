# 7.10 â€” Documentation and Evidence Preservation

The incident is over. The post-mortem meeting is scheduled. The team gathers to understand what happened. And then someone asks: "What were the exact latency numbers when the failure started?" No one knows. The logs rotated out. The dashboard is gone. The Slack thread is buried under 400 other messages.

A financial planning assistant running Claude Opus 4.5 experienced a cascading failure in October 2025. The incident lasted 2 hours. The team resolved it, scheduled a post-mortem, and two days later sat down to analyze what happened. They could not reconstruct the timeline. Logs had a 24-hour retention policy. Dashboard snapshots were not captured. The incident Slack channel had 600 messages, most irrelevant. Ninety minutes of the 120-minute incident was undocumented in any retrievable form. The post-mortem produced vague conclusions and no actionable insights.

Evidence preservation is not optional. If you cannot reconstruct what happened, you cannot learn from it. If you cannot learn from it, you will repeat it. Effective incident response includes capturing evidence as the incident unfolds, not after.

## What Evidence to Preserve

Preserve logs from the incident window, plus 30 minutes before and after. The 30 minutes before capture early warning signs. The 30 minutes after capture recovery stability. A recruiting platform captured logs starting exactly at incident detection. They missed the 12 minutes of gradual latency increase that preceded the failure. Root cause analysis was incomplete because the triggering conditions were not logged.

Preserve all logs, not just error logs. Request logs show traffic patterns. Access logs show user behavior. Application logs show decision paths. Infrastructure logs show resource usage. A customer service chatbot running Gemini 3 Flash preserved only error logs during a failure. The root cause turned out to be a gradual memory leak visible in infrastructure logs but not in error logs. They could not confirm it until the next incident because the evidence was deleted.

Preserve metrics and dashboards. Take screenshots of key dashboards every 5-10 minutes during an incident. Export raw metric data if your monitoring system supports it. Dashboards change over time. The view you see during the incident may not be available later if panels are modified or deleted. A legal research assistant lost all latency distribution data from an incident because the dashboard was reconfigured two days later and historical panel data was not preserved.

Preserve communication threads. Export the Slack incident channel, email threads, and video call transcripts if available. These capture decision-making context: why the team chose one mitigation over another, what hypotheses were tested, what information was ambiguous. A healthcare documentation system preserved all incident Slack messages and discovered during the post-mortem that the team had the correct diagnosis 40 minutes before they acted on it. The delay was due to miscommunication, not lack of information.

Preserve deployment and configuration state. Capture which model versions were running, what configurations were active, what feature flags were enabled. A code generation tool running Codex-Max could not determine whether a deployment 30 minutes before the incident was related to the failure because they did not capture configuration state. They suspected a correlation but could not prove it.

Preserve external evidence. If users reported issues publicly on social media, support tickets, or forums, capture those reports. They provide the user perspective: what users experienced, when they noticed it, how they described it. Sometimes user descriptions reveal failure modes that internal monitoring missed.

## Log Retention During Incidents

Standard log retention policies are often too short for incident analysis. A 7-day retention policy seems reasonable until an incident occurs on Friday and the post-mortem happens the following Thursday. By then, detailed logs are gone. A SaaS platform increased log retention to 30 days after losing critical incident data twice in six months.

During an incident, extend log retention immediately. If your logging system supports it, flag logs from the incident window for extended retention. A travel booking assistant running GPT-5 had a logging system that allowed tagging log streams with retention overrides. When an incident was declared, the on-call engineer tagged all relevant log streams to retain for 90 days. This took 2 minutes and preserved everything needed for analysis.

If manual log extension is not possible, export logs to long-term storage during the incident. A contract analysis platform ran a script that copied logs from the incident window to an S3 bucket with 1-year retention. The script ran automatically when an incident was declared. This added 15 seconds to incident response but saved multiple incidents from being unanalyzable.

## Screenshots and Recordings

Take screenshots of dashboards, alerts, and key system states every 5-10 minutes during an incident. Screenshots are insurance. If the monitoring system fails, if data is corrupted, or if retention policies delete data, screenshots remain. A financial advisory platform had a monitoring system outage during an AI service incident. The only evidence of the service incident was screenshots the incident commander took manually.

Record incident war room calls if possible. Video recordings capture tone, urgency, and decision-making dynamics that text logs miss. A customer support system recorded all incident calls and discovered during a post-mortem that a critical escalation decision was made based on a misunderstood metric. The misunderstanding was audible in the recording but not captured in any written log.

Screenshots and recordings should be stored in incident-specific folders, named with timestamps and incident IDs. A recruiting platform stored all incident evidence in folders named by date and incident number: "2025-10-12-INC-0047". Every screenshot, log export, and communication thread went into that folder. During the post-mortem, all evidence was immediately available.

## Timeline Documentation

Document the timeline as the incident unfolds. The incident commander or a designated scribe should maintain a running log of key events: when the incident was detected, when each hypothesis was tested, when each mitigation was applied, when traffic was restored. Timestamps must be precise to the minute.

A healthcare appointment scheduler used a shared Google Doc as a live timeline during incidents. Every significant action was logged with a timestamp. The format was simple: "14:23 - Applied rate limiting to embedding service. 14:31 - Latency reduced to 3.2 seconds. 14:40 - Restored traffic to 50%." This timeline became the backbone of every post-mortem.

Do not rely on memory to reconstruct timelines after the incident. Human memory is unreliable under stress. A legal document generator tried to reconstruct an incident timeline three days later based on team recall. The team disagreed on the order of events, the timing of decisions, and who made key calls. The post-mortem devolved into arguments instead of learning.

## Decision Documentation

Document why decisions were made, not just what decisions were made. During an incident, teams make judgment calls with incomplete information. Document the reasoning: what data was considered, what trade-offs were weighed, what alternative actions were discussed. A content moderation system documented every major decision during a 90-minute incident: "Decided to failover to secondary region instead of rolling back model deployment because rollback ETA was 25 minutes and failover ETA was 8 minutes."

Decision documentation protects against hindsight bias. After an incident, it is easy to criticize decisions that seem obviously wrong with full information. But during the incident, those decisions may have been reasonable given what was known. Documenting reasoning shows what information the team had and what they did not have.

A financial modeling platform had a decision during an incident to restart a service instead of investigating deeper. In hindsight, restarting caused a re-failure. But the decision log showed that at the time, monitoring indicated a transient issue, the incident was 70 minutes old, and restarting was the fastest path to user impact reduction. The post-mortem focused on improving monitoring, not blaming the decision.

## Communication Archives

Archive all incident communication: Slack threads, email chains, status page updates, customer notifications. These show how information flowed, how stakeholders were informed, and where communication breakdowns occurred.

A customer service chatbot had an incident where Engineering believed the issue was resolved but Product continued receiving customer complaints for 30 minutes. The communication archive revealed that Engineering posted an update in the wrong Slack channel. Product never saw it. The fix was a communication process change, not a technical change.

Communication archives also show response time. How long did it take to notify users? How long between detection and executive escalation? A recruiting platform discovered that their average time from incident detection to status page update was 23 minutes. They set a target of 10 minutes and tracked it in subsequent incidents.

## Evidence Chain of Custody

Treat incident evidence like legal evidence. Once captured, it should not be modified. Store evidence in immutable storage or with access logging so you know who accessed it and when. A legal research assistant had an incident where someone edited the Slack export before the post-mortem to remove an embarrassing comment. The post-mortem drew incorrect conclusions because the evidence was altered.

Access control matters. Incident evidence may contain sensitive data: customer information, API keys, internal architecture details. A healthcare documentation system stored all incident evidence in a restricted S3 bucket with access limited to the incident response team and leadership. Evidence was retained but protected.

Retention policies should balance learning with compliance. Keep incident evidence long enough to conduct post-mortems, track trends, and reference in future incidents. But respect data retention regulations like GDPR. A SaaS platform retained incident evidence for 2 years, then automatically deleted logs containing user data while preserving anonymized timelines and metrics.

## Automated Evidence Collection

Manual evidence collection is error-prone under stress. Automate as much as possible. When an incident is declared, a script should automatically capture logs, export metrics, take dashboard screenshots, and create a storage folder. A travel booking assistant built an incident evidence bot that triggered on incident declaration. The bot captured logs from the past hour, exported key metrics, took screenshots of 12 predefined dashboards, and posted links to an incident-specific folder. This took 90 seconds and ran without human intervention.

Automation ensures consistency. Every incident gets the same evidence collection process. A financial advisory platform had inconsistent evidence across incidents because different on-call engineers captured different data. After automating, every incident had complete, comparable evidence.

Automated collection does not replace human judgment. Engineers should still capture additional evidence if they notice something unusual. But automation provides a baseline that ensures nothing critical is missed.

## Using Evidence in Post-Mortems

Evidence is only valuable if it is used. Schedule post-mortems within 3-5 days of incident resolution while memory is fresh but after the team has rested. Distribute evidence to participants before the meeting. A recruiting platform sent post-mortem participants the timeline, key screenshots, and communication logs 24 hours before the meeting. Participants came prepared with hypotheses and questions.

During the post-mortem, reference evidence directly. Do not rely on memory. When someone says "I think latency was around 5 seconds," check the dashboard screenshot. When someone says "I think we detected the issue within 10 minutes," check the timeline. Evidence grounds the discussion in facts and prevents misremembering from distorting lessons learned.

A customer service chatbot running Claude Sonnet 4.5 conducted a post-mortem without evidence. The discussion was speculative. Action items were vague. A month later, they conducted a second post-mortem for a different incident, this time with full evidence. The discussion was precise. Action items were specific and tied to timeline moments. The quality difference was stark.

Evidence preservation is what separates organizations that learn from incidents from organizations that repeat them. Preserve as the incident unfolds. Store immutably. Use systematically. Every incident is a teacher, but only if you capture what it has to say.

When incidents affect SLAs, recovery includes more than restoring service. The next subchapter covers SLA clock management during and after incidents, and how to report SLA impact accurately.
