# 8.2 — Root Cause Analysis for Probabilistic Failures

Why do most fine-tuned models degrade in production within six months? The team runs a root cause analysis. They identify 14 potential causes: training data staleness, eval-prod distribution mismatch, catastrophic forgetting, overfitting to eval metrics, inadequate regularization, learning rate misconfiguration, insufficient training data, poor data quality, annotation drift, model architecture limitations, deployment configuration errors, infrastructure instability, user behavior changes, and seasonal traffic patterns. Every single one is true. Every single one contributed. There is no root cause. There is a system that allowed 14 failure modes to compound until the model became unusable.

Traditional root cause analysis fails for AI systems because it assumes deterministic causation. The bug happened because line 247 had incorrect logic. Fix line 247, bug never happens again. AI failures are probabilistic, multifactorial, and emergent. The model hallucinated because the input was adversarial AND the retrieval context was noisy AND the output filter was tuned too permissively AND the user's phrasing triggered a low-probability failure mode the eval suite never tested. You cannot fix "the" root cause because there is no single root cause. You can only reduce the probability of the contributing factors aligning again.

## Why Traditional RCA Doesn't Work for AI

Traditional root cause analysis uses the Five Whys method. The server crashed. Why? The process ran out of memory. Why? A memory leak in the session handler. Why? The code failed to deallocate objects after logout. Why? The engineer misunderstood the garbage collection contract. Why? The code review process didn't catch memory management errors. Root cause: inadequate code review. Fix: add memory leak detection to CI. Problem solved.

This model assumes linear causation, deterministic behavior, and reproducible failures. AI systems violate all three assumptions. AI failures are not linear — they emerge from the interaction of multiple subsystems that individually behave correctly but collectively produce failure. They are not deterministic — the same input might succeed 999 times and fail once depending on stochastic sampling, cache state, or load-dependent latency. They are not reproducible — you cannot reliably trigger the failure in a test environment because the production traffic distribution, the user interaction patterns, and the system state are too complex to replicate.

When you apply Five Whys to an AI failure, you get either shallow answers or infinite regression. The model generated a biased output. Why? The training data contained bias. Why? Human language contains bias. Why? Society contains bias. You have now blamed society for your product failure, which is philosophically accurate but operationally useless. Or you stop early: the model generated a biased output. Why? The output filter missed it. Fix: lower the filter threshold. You ship the fix. The model now refuses harmless requests. You have traded one failure mode for another.

The fundamental problem is that AI systems are **probabilistic by design**. A traditional bug is a deviation from intended behavior. An AI failure is often intended behavior taken to an unintended extreme. The model is supposed to generalize beyond its training data — that's the point. When it generalizes incorrectly, you cannot simply "fix the bug" because the generalization mechanism itself is the product. You can only shift the probability distribution toward better outcomes and accept that some failure rate is irreducible.

## Probabilistic Root Cause Models

If Five Whys doesn't work, what does? You need a root cause model that acknowledges multiple contributing factors, quantifies their relative importance, and identifies which interventions reduce failure probability most efficiently. This is not a single technique — it is a mindset shift from "find the cause" to "understand the system that produced this failure."

Start with **contributing factor mapping**. List every factor that made the incident possible or made it worse. Do not filter yet. Include technical factors — the model architecture, the training data, the eval suite, the deployment process. Include organizational factors — the deadline pressure, the team understaffing, the lack of domain expertise. Include environmental factors — the user behavior shift, the traffic spike, the upstream API change. Write them all down. You should have 10 to 20 factors for any serious incident.

Then estimate the **counterfactual**: if you removed this factor, would the incident still have happened? This is imprecise. You are guessing. But it forces you to think through causation. If the eval suite had covered this input type, would the failure have reached production? Maybe not — that's a preventative factor. If the on-call engineer had better documentation, would the incident have been shorter? Probably — that's a mitigation factor. If the training data had been cleaner, would the model have failed less often? Maybe — that's a probabilistic factor.

Then prioritize **intervention leverage**. Which contributing factors can you actually change, and which have the biggest impact? You cannot fix society's bias, but you can add bias detection to your eval suite. You cannot prevent all user behavior changes, but you can add monitoring that detects distribution shifts. You cannot eliminate all infrastructure instability, but you can add graceful degradation when latency spikes. The goal is not to fix everything. The goal is to fix the three things that most reduce failure probability.

Some teams use **failure mode and effects analysis** adapted from hardware reliability engineering. For each component in the AI system — the retrieval layer, the prompt template, the model, the output filter, the fallback logic — list the failure modes, estimate their probability, estimate their severity, and calculate a risk score. The highest-risk failure modes get the most investment. This works well for systems with clear component boundaries. It works less well for emergent failures where the components individually function correctly but the system-level behavior is wrong.

Some teams use **fault tree analysis** where you diagram the logical combinations of failures that produce the incident. The model generates harmful output AND the output filter fails AND the human review is skipped AND the user reports it publicly. Each AND gate represents a safeguard that failed. Each OR gate represents alternative paths to the same failure. This visualization helps you see which safeguards are single points of failure and where you need defense in depth.

The key insight across all these techniques is that you are modeling a **probability distribution over failure modes**, not identifying a single root cause. The incident happened because several low-probability events coincided. Your job is to reduce the probability of each contributing factor so that their coincidence becomes vanishingly rare. You will never get to zero — but you can get from "happens every month" to "happens every five years."

## Contributing Factors vs Root Causes

Teams that insist on identifying "the root cause" produce shallow post-mortems that miss systemic issues. Teams that identify contributing factors produce deep post-mortems that improve the entire system. The difference is not semantic. It changes what gets fixed.

**Root cause thinking** stops at the first satisfying answer. The model failed because the eval suite missed this input type. Fix: add this input type to the eval suite. The team closes the incident, adds 50 examples, and moves on. Six months later, a different input type causes an almost identical failure. The root cause was never "the eval suite missed this input type." The root cause was "the eval suite is not systematically designed to cover input diversity."

**Contributing factor thinking** asks why each factor existed and what systemic change would prevent similar factors in the future. The eval suite missed this input type. Why? The eval design process doesn't include adversarial input generation. Why? The team doesn't have a red-teamer role. Why? Leadership views red-teaming as a nice-to-have, not a requirement. The contributing factors are: inadequate eval coverage, no adversarial testing process, missing role in the team structure, organizational underinvestment in safety. You can fix all four, and the system becomes materially more resilient.

Contributing factors split into categories. **Technical factors** are the immediate causes: the prompt was brittle, the retrieval was noisy, the model hallucinated. **Process factors** are the missing safeguards: the deployment checklist wasn't followed, the eval results weren't reviewed, the cross-functional sign-off was skipped. **Organizational factors** are the systemic issues: the team was understaffed, the deadline was unrealistic, the oncall rotation had no coverage redundancy. **External factors** are the environmental changes: user behavior shifted, upstream APIs changed, traffic spiked unexpectedly.

Most teams only document technical factors. They write "the model failed because..." and stop. The best teams document all four categories and explicitly call out which factors are within their control and which require escalation. If the incident was caused by deadline pressure forcing the team to skip safety checks, that is an organizational factor that Engineering leadership needs to see. If the incident was caused by an upstream API change that broke your retrieval system, that is an external factor that requires better vendor management or circuit breakers.

The rule: **every incident has at least one technical factor, one process factor, and one organizational factor**. If your post-mortem only lists one category, you stopped investigating too early. Dig deeper. The technical failure is always enabled by a process gap, and the process gap is always enabled by an organizational decision or constraint.

## The "No Single Root Cause" Reality

The hardest thing for teams trained in traditional software debugging to accept is that many AI incidents genuinely have no single root cause. The failure emerged from the interaction of multiple factors that individually were not failures. The model was fine. The data was fine. The deployment was fine. The combination — at this traffic volume, with this user population, on this input distribution — was not fine.

A financial services company deployed a credit risk model in January 2025. The model performed well for eight months. In September, accuracy dropped by 12%. The team ran a root cause analysis. They found: the model was trained on 2022-2023 data and had never seen 2024 macroeconomic conditions. The retrieval system was pulling credit reports formatted differently than training data due to a vendor update. The user population had shifted because the company expanded to a new geographic market. The inference latency had increased due to infrastructure load, causing the fallback logic to activate more often. None of these factors alone would have caused noticeable degradation. Together, they compounded.

The team argued for a week about "the root cause." Was it data staleness? Distribution shift? Infrastructure issues? The answer was yes. The incident was not caused by any single failure. It was caused by the system's inability to handle the **coincidence** of multiple stressors. The fix was not to patch one component. The fix was to add systemic resilience: better monitoring for distribution shift, faster model retraining cycles, infrastructure capacity planning, fallback logic that degraded gracefully instead of abruptly.

This is the norm for AI systems, not the exception. Most serious AI incidents are **confluence failures** where multiple low-probability events coincided. The challenge is that humans are bad at reasoning about coincidence. We want a villain. We want a single mistake we can point to. When the post-mortem says "this incident required seven things to go wrong simultaneously," stakeholders resist. It feels like excuse-making. It feels like the team is avoiding accountability.

Your job as the post-mortem author is to make the confluence visible and credible. Show the timeline where each factor was introduced. Show the probability estimates for each factor failing independently. Show the multiplication: if each factor has a 5% failure rate and you need four to coincide, the combined failure rate is 0.00000625 — which sounds rare until you process 10 million requests per day, at which point it happens 62 times daily. The incident was not caused by negligence. It was caused by operating a complex probabilistic system at scale without sufficient defense in depth.

## Statistical Approaches to Understanding Failures

When the failure is probabilistic and non-reproducible, you cannot debug it by stepping through code. You have to analyze it statistically. This requires data — lots of data — and a willingness to accept uncertain conclusions.

Start with **failure rate characterization**. What percentage of requests failed during the incident? Not "the model failed" — how often? If 0.01% of requests failed, you are looking for a rare edge case. If 30% of requests failed, you are looking for a systemic issue. The failure rate tells you whether this is a tail risk or a mainline problem, which changes prioritization dramatically.

Then segment by input characteristics. Did failures occur uniformly across all inputs, or were they concentrated in specific patterns? Run clustering analysis on failing inputs. What do they have in common? Length? Complexity? Topic? Sentiment? User demographics? Language? If 80% of failures came from requests longer than 500 words, you have identified a testable hypothesis: the model degrades on long-context inputs. If failures were uniformly distributed, the root cause is more elusive.

Then compare to baseline. Was this failure rate higher than normal, or is this the background noise you always have? Pull the same metrics from the week before the incident. If the failure rate was 0.02% before and 0.03% during, this might not even be an incident — it might be normal variance. If the failure rate was 0.02% before and 2% during, you have a 100x spike and a real problem. Context matters.

Then look for temporal patterns. Did the failure rate spike suddenly and stay elevated? That suggests a deployment or configuration change. Did it ramp gradually over days or weeks? That suggests data drift or user behavior shift. Did it spike and then recover on its own? That suggests infrastructure instability or a transient load issue. The temporal pattern narrows the search space for contributing factors.

Then correlate with system metrics. Did the failure rate correlate with latency? With traffic volume? With cache hit rate? With upstream API response times? Correlation is not causation, but it generates hypotheses. If failures spiked exactly when latency exceeded two seconds, you have evidence that timeout logic or user impatience is involved. If failures spiked when traffic doubled, you have evidence that the system does not scale linearly.

This statistical analysis does not give you "the root cause." It gives you a probability distribution over hypotheses ranked by strength of evidence. The strongest hypothesis gets tested first. If you confirm it, you get a high-leverage fix. If you disprove it, you move to the next hypothesis. This is the scientific method applied to incident response. It is slower than "someone spots the bug and fixes it," but it is the only approach that works for probabilistic systems.

## When to Stop Investigating

You could investigate forever. Every contributing factor has upstream causes. Every upstream cause has organizational and historical context. At some point, you have to stop investigating and start fixing. The question is when.

Stop when you have identified **three to five actionable interventions** that would materially reduce the probability or impact of recurrence. Not 20 action items. Not one. Three to five. If you have fewer than three, you didn't investigate deeply enough — you found a surface-level cause and stopped. If you have more than five, you're either documenting low-value work or you've identified a systemic disaster that requires escalation beyond the post-mortem process.

Stop when **further investigation requires more time than the incident cost**. If the incident caused 20 minutes of degraded service and affected 0.1% of users, you should not spend 40 hours reverse-engineering the exact probabilistic sequence that caused it. Write up what you know, document the uncertainty, and move on. If the incident cost three hours of downtime and $500,000 in lost revenue, spend the 40 hours. The investigation effort should be proportional to incident severity.

Stop when you have reached **irreducible uncertainty**. Sometimes you genuinely cannot determine why the model behaved the way it did. The training data is too large to audit. The input was adversarial in a way you cannot replicate. The model's internal representations are not interpretable. You have theories, not proof. Document the theories. Implement the most plausible fixes. Add monitoring so you will catch it if it happens again. Accept that some AI failures are not fully explainable.

Stop when you have escalated systemic issues that are **beyond your team's authority to fix**. If the root cause is "Engineering leadership set an unrealistic deadline that forced the team to skip safety checks," the post-mortem should document that — but the post-mortem process cannot fix it. That requires an organizational intervention. Write the action item as "escalate to VP of Engineering for process review" and consider the investigation complete from your team's perspective.

The goal of root cause analysis is not perfect understanding. The goal is **sufficient understanding to take effective action**. Once you have that, stop investigating and start fixing. The system does not get more reliable from additional analysis. It gets more reliable from implemented improvements.

## Documenting Uncertainty in RCA

Traditional post-mortems pretend to certainty. "The root cause was X. We fixed X. It will not happen again." AI post-mortems must document uncertainty honestly. You believe the primary cause was X, you have 70% confidence, you implemented fixes that should reduce recurrence by 80%, and you added monitoring so you will know within 10 minutes if you are wrong. This level of honesty is uncomfortable. It is also essential.

When you document uncertainty, use calibrated language. Do not write "the root cause was likely X" — that is vague. Write "we estimate 70% probability that the primary cause was X based on correlation between failure rate and metric Y." Do not write "this should prevent recurrence" — that is a wish. Write "this intervention is expected to reduce failure rate from 2% to 0.2% based on historical analysis of similar changes."

When you have multiple hypotheses with uncertain probabilities, list them ranked by confidence. "Hypothesis 1 — data distribution shift — 60% confidence based on temporal correlation and input clustering. Hypothesis 2 — infrastructure latency spike — 25% confidence based on weak correlation with timeout rate. Hypothesis 3 — adversarial user behavior — 15% confidence based on absence of other evidence." This tells future readers which theories you believed and why, which helps them interpret the effectiveness of your fixes.

When your fix is uncertain, document the expected outcome and the failure mode. "We retrained the model on recent data. Expected outcome: failure rate drops to 0.1%. Failure mode: if the issue was not data staleness but a model architecture limitation, retraining will not help and we will see the same failure rate within two weeks." This sets up a testable prediction. If the failure rate does not drop, you know the hypothesis was wrong and you can pivot.

When the incident revealed a knowledge gap, document it explicitly. "We do not understand why the model generated this output. The input was within distribution. The model's internal reasoning is not interpretable. The failure is not reproducible. We have added monitoring to detect similar patterns in the future, but we cannot confidently prevent recurrence." This is not a failure of the post-mortem process. This is an honest acknowledgment of the limits of your understanding. It might prompt investment in interpretability tools, or it might justify accepting this risk as irreducible.

The teams that pretend to certainty in post-mortems create false confidence. Six months later, when the "fixed" incident recurs, stakeholders lose trust. The teams that document uncertainty honestly create realistic expectations. When an incident recurs, stakeholders see it as evidence that the problem is harder than initially believed, which justifies more investment. Honesty about uncertainty is not weakness. It is the foundation of continuous improvement.

---

Next: 8.3 — The Incident Timeline: Reconstructing What Happened
