# 6.9 — The Rollback Decision: When and How to Revert

The model had been in production for 11 days. Quality metrics looked strong. Then at 9:14 AM on a Tuesday in December 2025, user complaints spiked. A healthcare AI was generating discharge summaries that omitted critical medication instructions. The incident commander made the call: roll back to the previous version. Fifteen minutes later, the rollback completed. Twenty minutes after that, they discovered the previous version had a different critical bug that had prompted the upgrade in the first place. Rolling back solved one problem and reintroduced another. They now needed to roll forward to a third version that fixed both issues, deploy it under incident pressure, and explain to hospital administrators why their AI had cycled through three versions in 40 minutes.

Rollback is not always the answer. Sometimes it's the only answer. Sometimes it's the wrong answer that makes things worse. The decision to roll back an AI system during an incident is one of the highest-stakes calls an incident commander makes. You're trading a known bad state for a previous state that you hope is better but that you deployed weeks or months ago under different conditions with different traffic patterns. Rollback is a bet that the past is safer than the present. Sometimes that bet is wrong.

## When Rollback Is an Option and When It Isn't

Rollback is possible when you have a previous version that is deployable, compatible with current infrastructure, and known to have acceptable performance. This sounds obvious but eliminates many scenarios. If the previous model version required a data pipeline that no longer exists, rollback isn't possible without also rolling back infrastructure. If the previous version was trained on data that's now deleted for compliance reasons, you can't reproduce it. If the previous version had quality problems that prompted the upgrade, rolling back reintroduces those problems.

Model-only changes are the easiest rollbacks. You deployed a new model, kept the same prompt templates and API contracts, and can swap back to the previous model by changing a version pointer. These rollbacks are low-risk and fast. The healthcare company's initial rollback was this type. They reverted from model v2.3 to v2.2 in 15 minutes. The problem was that v2.2 had the medication instruction bug that v2.3 was supposed to fix. The rollback was technically successful but operationally disastrous.

Infrastructure-coupled changes resist rollback. If the new model version required changes to preprocessing pipelines, retrieval systems, or output formatting, rolling back the model also requires rolling back those components. Now you're coordinating a multi-system rollback under incident pressure with dependencies and timing risks. The window for rollback widens from minutes to hours. The risk of partial rollback states where the model version and infrastructure version are mismatched increases. Many teams decide forward fixes are safer than coordinated rollbacks.

Data-driven incidents often can't be rolled back. If the model degraded because training data was contaminated or if a data pipeline bug corrupted the fine-tuning dataset, rolling back the model doesn't fix the underlying data problem. The previous model version is fine, but you still need to fix the data pipeline before you can train and deploy a corrected version. Rollback buys you time but doesn't solve the incident. You're rolling back to a known-good state while you fix the root cause, then rolling forward to a new version that incorporates the fix.

## The Rollback Decision Framework

The rollback decision comes down to four questions. First: is the previous version known to be better than the current version for the failure mode you're seeing? If yes, rollback is likely the right call. If no, rollback might trade one set of problems for another. Second: can you roll back without introducing new risks? If rollback is technically complex or requires multi-system coordination, the rollback itself becomes a risky operation. Third: how long will rollback take versus how long will a forward fix take? If you can patch and redeploy a fix in 20 minutes and rollback takes 30, the forward fix wins. Fourth: what user impact happens during rollback versus what happens if you stay on the current version?

Impact severity drives urgency. If the current version is generating harmful outputs, exposing sensitive data, or violating regulatory requirements, you roll back immediately even if the previous version has minor quality issues. Safety failures trump quality failures. The healthcare discharge summary issue was a safety failure. Omitted medication instructions could harm patients. The team was right to roll back even though the previous version had its own problems. They could tolerate the medication instruction bug for hours while fixing it. They could not tolerate the discharge summary omission bug for minutes.

Rollback confidence depends on observability. If you have detailed monitoring and eval data for the previous version under recent traffic patterns, you know what you're rolling back to. If the previous version was deployed months ago and traffic has changed significantly since then, you're rolling back to an unknown state. The version might have worked well in August but fail badly in December. This is why teams maintain ongoing eval coverage for previous versions even after deploying new ones. You need current performance data on old versions to make informed rollback decisions.

The no-rollback decision is valid. If rollback risks are high, previous version quality is uncertain, and the forward fix is achievable within an acceptable time window, staying on the current version and fixing forward is the right call. This requires confidence in your ability to diagnose, fix, and redeploy quickly. It also requires acceptance of continued degraded performance during that window. Incident commanders who default to rollback aren't always making the safest choice. Sometimes the safest choice is controlled forward progress.

## Partial Rollback Strategies

Traffic splitting enables partial rollback with monitoring. Instead of rolling back 100% of traffic to the previous version, split traffic 50-50 or 20-80 between current and previous versions. Monitor quality metrics on both populations. If the previous version performs better, shift more traffic to it. If performance is similar, the incident cause might not be the model version at all. Partial rollback reduces blast radius while giving you empirical data about whether rollback actually solves the problem.

Regional rollback works for globally distributed systems. Roll back the impacted region first. Monitor whether the problem resolves in that region and whether other regions remain stable. If the regional rollback succeeds, expand to other regions. If it fails or introduces new issues, halt the rollback and investigate further. This strategy is slower than full rollback but much safer for large-scale systems where a bad rollback can take down production globally.

Feature-level rollback is possible when model changes are tied to specific features. If the new model version powers a feature that's causing problems, disable the feature and fall back to the previous implementation while leaving the new model in place for other features. This requires architecture that supports feature toggles and fallback paths. Not every system has this capability. But when it exists, feature-level rollback is much more surgical than full model rollback.

Rollback with override gives you both safety and time. Roll back the model version to restore baseline performance, but implement manual overrides or review gates for high-stakes decisions until you understand the root cause. This keeps the system operational while reducing risk from the previous version's known issues. The healthcare team used this strategy: they rolled back to v2.2, added a mandatory human review for any discharge summary missing medication instructions, and worked on v2.4 that fixed both the omission and instruction bugs.

## Rollback Risks: Regression to Previous Problems

Every rollback reintroduces the problems that prompted the original upgrade. If you upgraded to fix bias in loan decisions, rolling back reintroduces the bias. If you upgraded to improve refusal handling, rolling back reintroduces poor refusal handling. The incident commander must explicitly acknowledge these trade-offs. You're not rolling back to a perfect state. You're rolling back to a state with known problems that you've decided are less severe than the current problems.

Version amnesia is a real risk. Teams forget why they upgraded in the first place. The engineer who led the original upgrade might not be on-call during the incident. The context for why v2.2 was replaced by v2.3 is buried in a pull request from six weeks ago. Without that context, the rollback decision is made blind. This is why upgrade decisions need clear documentation: what problems did this version solve, what risks does it introduce, and what are the known limitations of the previous version?

Rollback can trigger cascading failures if downstream systems depend on new model behavior. If a downstream service was updated to parse outputs from the new model version and you roll back the model, that downstream service might break. If a monitoring system was tuned to detect anomalies based on new model output distributions, rollback invalidates that tuning. Large AI systems have dependencies in both directions. Rolling back one component can destabilize others.

## Safe Rollback Procedures

Safe rollback requires the same rigor as safe deployment. You don't just flip a switch and hope. You validate the rollback in staging, monitor the rollback in production, and have a plan to halt or reverse the rollback if it makes things worse. The process looks like this: first, confirm that the previous version deploys successfully in a staging environment. Second, deploy to a small percentage of production traffic. Third, monitor quality metrics and user impact for a defined period. Fourth, if metrics improve, expand rollback to full traffic. Fifth, if metrics don't improve or degrade further, halt rollback and reassess.

Rollback validation catches configuration drift. The previous model version might not work with current infrastructure if configuration files, environment variables, or dependency versions changed since the original deployment. Validating in staging before rolling back in production catches these mismatches. A rollback that fails mid-execution is worse than no rollback at all. You've now introduced a partial deployment state with unpredictable behavior.

Phased rollback timelines vary by system scale. A small system might validate rollback in staging for 10 minutes, deploy to 10% of production for 10 minutes, and expand to 100% over the next 20 minutes. A large system might spend an hour in staging, 30 minutes at 10%, another 30 minutes expanding to 50%, and another hour reaching 100%. The timeline trades speed for safety. Under incident pressure, teams want to move fast. The incident commander's job is to enforce the validation gates that prevent a bad rollback from becoming a worse incident.

## Validating Successful Rollback

Rollback success is not defined by "the deployment completed." It's defined by "the problem stopped happening and no new problems started." You validate rollback by monitoring the same metrics that detected the original incident and confirming they return to baseline. You also monitor for new anomalies that indicate the rollback introduced different issues.

Metric recovery timelines vary. Some metrics improve immediately after rollback. Refusal rates, error rates, and latency metrics typically respond within minutes. Quality metrics that depend on user feedback or human review might take hours or days to confirm. The incident commander needs to define success criteria before rolling back: what metrics need to recover, to what thresholds, within what timeframe. Without clear criteria, you don't know whether the rollback worked.

User feedback is the ground truth. Even if automated metrics recover, if user complaints continue or escalate, the rollback didn't solve the problem. The healthcare team saw this firsthand. After rolling back to v2.2, automated metrics looked fine. But within 30 minutes, hospital staff reported the medication instruction bug was back. The rollback succeeded technically but failed operationally. They had to roll forward to v2.4 to actually resolve the incident.

## When Not to Roll Back: The Forward Fix Alternative

Forward fixes are preferable when the issue is a configuration problem, a prompt engineering bug, or a small code defect that can be patched quickly. Rolling back is expensive. Forward fixing is precise. If you can identify the specific line of code or prompt template that caused the issue and fix it in 20 minutes, that's faster and safer than rolling back an entire model version.

Hot-patching works for prompt and configuration changes. If the incident is caused by a prompt that generates inappropriate outputs, you can update the prompt template without redeploying the model. If it's caused by a misconfigured threshold or routing rule, you can update configuration. These fixes deploy in seconds to minutes, carry minimal risk, and don't require rollback coordination. The forward fix is complete before the rollback would have finished.

Version-pinning with patches gives you the best of both worlds. You roll back to the previous model version but deploy a patch that fixes its known issues. This requires a CI/CD pipeline that supports rapid patch deployment and testing. It's more complex than a pure rollback but addresses the "trading one set of problems for another" risk. You get the stability of the previous version plus the fixes that prompted the upgrade.

## Rollback Authority and Approval

Who has authority to execute a rollback during an incident? In most organizations, the incident commander has rollback authority for their system. They don't need VP approval to roll back a model version during an active outage. But rollback authority has limits. If rollback requires database migrations, infrastructure changes, or dependencies on other teams' systems, coordination is required. If rollback affects customer contracts or SLAs, leadership might need to be involved.

Pre-approved rollback procedures reduce decision latency. Define rollback runbooks in advance with clear criteria for when rollback is appropriate. Train on-call engineers on rollback procedures during incident drills. When the real incident happens, the responder knows they have authority to roll back without waiting for approvals. The approval already happened during runbook review. This is how you achieve sub-30-minute rollbacks during incidents without skipping safety checks.

Post-rollback review is mandatory. Even if the rollback resolved the incident, you need to understand why the incident happened, why rollback was the chosen remediation, and what prevents recurrence. Rollback is not a long-term solution. It's a short-term mitigation that buys you time to build the real fix. The post-incident review should include a timeline for addressing the root cause so you're not rolling back and forth between two flawed versions indefinitely.

---

Next: **6.10 — User Communication During AI Incidents** — what to tell users when your AI is degraded, how to balance honesty with liability, and communication templates for common incident scenarios.
