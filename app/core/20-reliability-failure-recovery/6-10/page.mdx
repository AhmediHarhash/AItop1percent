# 6.10 — User Communication During AI Incidents

Your model is generating incorrect responses at a 12% rate. Higher than normal, not catastrophic. You've identified the issue. Fix ETA is 90 minutes. Do you tell users?

In March 2025, a legal research AI experienced exactly this scenario. They chose silence. No status page update. No in-app banner. Users noticed anyway. Lawyers reported incorrect case citations on social media. By hour three, the story was "AI legal tool giving bad advice and hiding it." The actual incident lasted 90 minutes. The trust damage lasted months. The company's next incident communication strategy was radically different: when in doubt, communicate.

AI incidents are different from traditional software outages because the degradation is probabilistic and hard to explain. When your API is down, the user experience is clear: nothing works. When your AI is degraded, the user experience is ambiguous: some things work, some things are wrong, and users can't tell which is which. This ambiguity makes communication both more important and more difficult. Users need to know when they can't trust outputs. But telling them "the AI is 12% less accurate right now" creates more questions than it answers.

## Why AI Incident Communication Is Different

Traditional software incidents have binary states. The service is up or down. The database is reachable or unreachable. The deployment succeeded or failed. AI incidents have gradient states. The model is performing at 88% of baseline quality. Refusal rate is elevated by 15%. Hallucination frequency is within acceptable bounds but higher than last week. These states are harder to communicate because they require users to understand probabilistic systems and make risk-adjusted decisions about whether to trust outputs.

User risk tolerance varies by use case. A lawyer using an AI research tool needs to know about any degradation that could affect case citations. An engineer using an AI code assistant can tolerate occasional errors because they review outputs anyway. A customer service agent using an AI response suggester needs to know when suggestions are unreliable but doesn't need technical details about why. Your communication strategy must account for these different risk profiles. One-size-fits-all messaging doesn't work.

Legal and regulatory exposure changes the calculation. If your AI makes medical recommendations, financial advice, or legal guidance, degraded performance might create liability. Communicating the degradation protects you legally but might trigger user panic. Not communicating it protects user experience but exposes you to claims that you knowingly let users rely on degraded outputs. This is why in-house legal counsel should review your incident communication templates before incidents happen, not during them.

The technical versus user-facing gap is wider in AI than in traditional systems. Engineers understand what "fine-tuning introduced bias in edge cases" means. Users hear "the AI is broken and biased." Engineers understand that "99th percentile latency increased from 800ms to 1.2 seconds" is a performance degradation. Users don't notice. Translating technical incident details into user-facing language that is accurate, non-alarming, and actionable is one of the hardest parts of AI incident communication.

## Status Page vs In-App Communication

Status pages work for system-wide outages and complete service disruptions. When your AI is entirely unavailable, the status page is the right channel. Post the incident, update with ETAs, mark as resolved when service restores. This is the traditional software playbook and it translates directly to AI systems when the failure mode is availability rather than quality.

Status pages fail for quality degradation. If your AI is available but producing lower-quality outputs, most users won't check the status page. They'll encounter bad outputs, assume it's normal variance or their fault, and either stop using the system or lose trust silently. Status pages are pull-based communication. Users have to go looking. Quality incidents need push-based communication where the warning reaches users in context.

In-app banners are the right channel for quality incidents. Display a message directly in the UI where users interact with the AI: "We're currently experiencing elevated error rates. Please review AI outputs carefully. ETA for resolution: 60 minutes." This message reaches every active user without requiring them to check external status pages. The warning is contextual. The user sees it before relying on potentially degraded outputs.

Banner design matters. The message needs to convey severity without causing panic, provide actionable guidance, and set expectations for resolution. Poor banner design: "AI performance degraded." This tells users something is wrong but not what to do about it. Good banner design: "AI responses may be less accurate than usual. Review outputs carefully before taking action. We expect to resolve this within 90 minutes." This tells users what's wrong, what to do, and when it will be fixed.

Real-time updates during extended incidents reduce user frustration. If your initial ETA was 60 minutes and you're now at 75 minutes with another 30 minutes expected, update the banner. Users who checked an hour ago and return now need current information. Stale ETAs erode trust faster than no ETAs. If you don't have a reliable ETA, say so: "We're actively working to resolve this. Updates every 30 minutes." Then follow through with updates even if the update is "still investigating."

## Communicating Degraded Quality vs Complete Outage

Complete outages are easier to communicate because the user experience is unambiguous. The AI isn't responding. Queries time out. The interface shows error messages. Users know the system is down. Your communication confirms what they already observe and provides ETA. This is straightforward incident communication.

Quality degradation is harder because the user experience is ambiguous. The AI is responding. Most responses look normal. Some responses are wrong in subtle ways. Users don't know whether the error is an isolated case or indicative of broader degradation. Your communication has to help them calibrate trust. The legal research company learned this the hard way. Users needed to know: "right now, you should verify every case citation against primary sources because citation accuracy is degraded." Without that message, they relied on outputs as if the system was operating normally.

Severity levels help users make decisions. Define incident severity in user-facing terms, not technical terms. Critical: do not rely on AI outputs without verification. Elevated risk: review outputs more carefully than usual. Minor: outputs may vary slightly from normal quality but remain within acceptable bounds. These severity levels give users a mental model for how to interact with the system during the incident. They can decide whether to continue using the AI with extra caution or wait until the incident resolves.

Transparent about uncertainty is better than false precision. If you're still diagnosing the incident and don't know the scope of degradation, say so: "We're investigating reports of incorrect outputs in financial summaries. If you're using financial summary features, please verify outputs until we complete our investigation." This is more trustworthy than waiting until you have complete information, by which time users have already encountered and potentially acted on bad outputs.

## The Honesty vs Liability Balance

You want to be honest with users about AI failures. You also don't want to admit liability for harm caused by those failures. These goals are in tension. Saying "our AI gave incorrect medical advice" is honest but might be used as evidence in litigation. Saying "we experienced a service disruption" is vague enough to avoid liability but doesn't help users understand what went wrong or whether they were impacted.

Legal templates balance these concerns. Work with legal counsel to draft pre-approved incident communication templates that acknowledge the issue without admitting fault. Example: "We've identified a technical issue affecting AI output quality in certain scenarios. We recommend users verify outputs during this period. We're actively working on a resolution." This says there's a problem, tells users what to do, and avoids language that admits negligence or liability.

What you never say during an incident: "We apologize for any harm caused." This implies you're accepting responsibility for downstream consequences. "The AI made an error that could affect your decisions." This attributes agency to the AI and suggests you're aware of specific harms. "We should have caught this sooner." This admits to process failures that could be used to argue negligence. These phrases might feel like the right way to express accountability, but they create legal exposure that can cost millions in settlements.

What you always say: "We take this issue seriously." "We're working to resolve this as quickly as possible." "We recommend users verify outputs until this issue is resolved." These phrases acknowledge the situation, demonstrate responsiveness, and provide actionable guidance without admitting fault. They're sincere without being legally risky.

Post-incident communication is where you can be more expansive. After the incident is resolved and you've completed internal review, you can publish a detailed postmortem that explains what happened, what you learned, and what you're changing to prevent recurrence. At this point, legal review has happened and you can be more transparent without the real-time pressure of an active incident. Postmortems build trust through honesty without creating the same liability risks as mid-incident statements.

## Template Communication for Common Scenarios

Quality degradation scenario: "We're currently experiencing elevated error rates in AI responses. We expect some outputs may be less accurate than usual. Please review all AI-generated content carefully before using it. Estimated resolution time: 90 minutes. Updates provided every 30 minutes."

Hallucination increase scenario: "We've identified an issue where the AI may generate plausible-sounding but incorrect information at higher rates than normal. If you're using AI outputs for any critical decisions, please verify against authoritative sources. We're actively working on a fix."

Bias or inappropriate output scenario: "We've paused AI functionality for a specific feature after identifying outputs that don't meet our quality and safety standards. We're working on a fix and will restore functionality once we've validated the solution. Estimated downtime: 2-3 hours."

Data exposure scenario: "We've identified and immediately addressed a security issue that may have affected a limited number of users. Out of an abundance of caution, we're conducting a full investigation. Impacted users will be contacted directly within 24 hours with specific details and next steps."

Latency degradation scenario: "You may experience slower than usual response times from AI features. All functionality remains available but responses may take 2-3x longer than normal. We're working to restore typical performance. Updates in 45 minutes."

Rollback scenario: "We've temporarily reverted to a previous version of our AI system while we address a quality issue with our latest update. You may notice some differences in response style and capabilities during this period. We expect to deploy an improved version within 48 hours."

## Internal vs External Communication

Internal communication during AI incidents needs to be more detailed and more frequent than external communication. Your engineering team, customer support, and leadership need different information than your users. They need technical details, diagnostic progress, internal ETAs that might not be customer-facing, and context about severity and scope. This communication happens in incident channels, team syncs, and direct updates to leadership.

Customer support teams are your front line. They need to know about incidents before users contact them. Provide support with talking points that align with external messaging but include additional context: what's broken, what's not broken, what support can do to help affected users, and when to escalate. Support agents who learn about incidents from angry users instead of from internal communication are set up to fail.

Leadership communication focuses on impact and risk. Executives don't need to know the technical details of what went wrong during an active incident. They need to know: how many users are affected, what business impact this has, what regulatory or legal risks exist, what's the ETA for resolution, and what resources are needed. Leadership comms should be brief, factual, and updated at defined intervals. If the incident escalates, leadership needs to know immediately. If it resolves ahead of schedule, leadership needs to know that too.

Post-incident internal communication is where you unpack everything. The blameless postmortem, the root cause analysis, the timeline of decisions made under pressure, and the action items to prevent recurrence. This communication is detailed, technical, and honest in ways that external communication cannot be. It's where the team learns. Internal postmortems are teaching documents. External postmortems are trust-building documents. The content overlaps but the goals are different.

## Post-Incident Communication

After the incident resolves, users need three things: confirmation that it's fixed, explanation of what happened, and assurance that you're preventing recurrence. The post-incident message should be concise but complete. "The incident affecting AI output quality has been fully resolved as of 4:37 PM UTC. Root cause was an issue in our data processing pipeline. We've implemented additional monitoring and validation to detect and prevent similar issues in the future. We apologize for any inconvenience and appreciate your patience."

Detailed postmortems are optional but valuable for building trust with technical audiences. If your users are developers, engineers, or technical decision-makers, they appreciate transparency. A detailed postmortem that explains what went wrong, why existing safeguards didn't catch it, and what you're changing demonstrates maturity and competence. Not every user will read it, but the ones who do will trust you more.

Do not publish detailed postmortems that expose security vulnerabilities, competitive information, or legally sensitive details. Have legal and security review external postmortems before publishing. The goal is transparency within safe bounds, not radical openness that creates new risks.

Follow-up communication weeks after the incident shows you followed through. If your postmortem promised new monitoring, new testing, or new processes, publish an update a month later confirming you implemented those changes. This closes the loop and demonstrates accountability. Most companies publish postmortems and then never mention the incident again. The ones that follow up with "here's what we built to prevent this" stand out.

---

Next: **6.11 — Incident Command for AI: Roles and Responsibilities** — defining clear command structure for AI incidents, including incident commander, technical lead, communications lead, and domain experts.
