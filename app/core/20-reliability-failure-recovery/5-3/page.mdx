# 5.3 — Prompt Portability and Adaptation Across Providers

A legal document analysis platform spent eight months optimizing prompts for GPT-5. They refined instructions, tuned examples, tested thousands of variations. Their final production prompt was 2,400 tokens of carefully structured instructions, examples, and output format specifications. Quality on GPT-5 reached 93% accuracy on their contract clause extraction task. When they implemented multi-provider redundancy and attempted to use the same prompt with Claude Opus 4.5 as backup, accuracy dropped to 76%. The prompt that worked beautifully on GPT-5 confused Claude. Instructions were interpreted differently. Examples were weighted differently. Output format compliance broke entirely.

The team had two options: maintain two completely separate prompts, one optimized for each provider, or find an abstraction layer that worked adequately on both. They chose the latter. They rewrote the prompt using simpler instructions, fewer examples, and more explicit output constraints. The new prompt achieved 91% accuracy on GPT-5 and 88% on Claude. They gave up two percentage points on their primary provider to gain portability. The trade-off was worth it. The alternative was maintaining two divergent prompt codebases and keeping them synchronized as requirements evolved.

Prompt portability is the degree to which a prompt written for one provider works without modification on another provider. Perfect portability is rare. Most prompts are provider-specific because models differ in instruction following behavior, example weighting, and output generation patterns. But you can design for portability. The techniques involve using simpler constructs that work reliably across providers, avoiding provider-specific features, and testing rigorously across your target provider set.

## Why Prompts Are Not Portable by Default

Language models are trained on different datasets with different training objectives and different reinforcement learning tuning. These differences create provider-specific prompt behaviors that break portability.

**Instruction interpretation varies.** GPT-5 might interpret "be concise" as "use 50 words or fewer." Claude might interpret it as "use 100 words or fewer." Gemini might ignore the instruction entirely if it conflicts with other instructions. The same words mean different things to different models. Portability requires instructions precise enough that interpretation variance does not matter.

**Example weighting varies.** Some models heavily weight few-shot examples and treat them as strong signals. Others treat examples as weak suggestions. If your prompt includes three examples showing a specific output format, GPT-5 might follow that format 95% of the time. Claude might follow it 70% of the time. Gemini might follow it 60%. You cannot rely on examples alone for cross-provider consistency.

**Output formatting compliance varies.** Asking a model to return JSON, or to structure output with specific headings, or to include metadata fields works differently across providers. GPT-5 has strong JSON mode support. Claude requires explicit instruction in the prompt. Gemini sometimes ignores format instructions under certain conditions. Portable prompts need format enforcement mechanisms that work across all target providers.

**Refusal behavior varies.** Models refuse to answer for different reasons. One provider refuses content it interprets as medical advice. Another refuses content it interprets as legal advice. A third refuses neither but refuses political analysis. If your prompt sometimes triggers refusals on one provider but not another, portability is broken. You need either to rewrite content to avoid refusal triggers or accept that some inputs cannot be processed by all providers.

**System message handling varies.** Some providers support system messages as a separate input field. Others append system messages to the user message. Others ignore system messages entirely. If your prompt architecture depends on system messages being processed in a specific way, it will not port cleanly.

## The Prompt Portability Spectrum

Prompts fall on a spectrum from fully portable to completely provider-specific.

**Fully portable prompts** work identically across all major providers. These are rare. They use simple instructions, no examples, no format constraints, and address tasks that all models handle similarly. A prompt that says "Summarize the following text in one sentence" is highly portable. The quality of summaries varies, but the prompt works everywhere.

**Mostly portable prompts** work across providers with minor quality variance. They use straightforward instructions, minimal provider-specific features, and explicit format constraints that all models understand. A prompt that asks for three key points from a document with bullet formatting works on GPT-5, Claude, and Gemini with 5-10% quality variance. You accept the variance in exchange for single-prompt maintenance.

**Adapted prompts** require provider-specific modifications but share a common structure. You maintain a base prompt template and provider-specific variants that adjust for known behavioral differences. The core logic is the same. The phrasing, examples, and format instructions are customized per provider. You maintain two or three prompt versions, but they are recognizably related.

**Provider-specific prompts** are optimized for one provider and do not work on others. These prompts use provider-specific features—GPT-5's function calling semantics, Claude's tool use API, Gemini's grounding extensions. They are not portable. You maintain completely separate prompts for each provider. When you switch providers, you switch prompts. This maximizes quality on each provider at the cost of maintenance complexity.

## Designing for Portability: Simplicity as a Strategy

The most portable prompts are the simplest. Complexity reduces portability because each additional feature—examples, constraints, format rules—is another place where provider behavior can diverge.

**Use declarative instructions over procedural ones.** Declarative instructions state what you want. "List the three main arguments in this document." Procedural instructions state how to do it. "Read the document, identify arguments, rank by importance, select the top three, format as a numbered list." Procedural instructions give models more room to diverge in interpretation. Declarative instructions are clearer and more portable.

**Minimize few-shot examples.** Examples increase prompt size and introduce variance in example weighting across providers. If you can achieve acceptable quality with zero-shot instructions, do that. If you need examples, use the minimum number required. One or two examples are more portable than five or ten.

**Make output constraints explicit and non-negotiable.** Do not say "please format as JSON." Say "You must return valid JSON matching this schema. If you cannot, return an error message." Explicit constraints with consequences are more reliably followed across providers than polite requests.

**Avoid provider-specific features.** Do not use GPT-5 function calling syntax in prompts intended for Claude. Do not use Claude's prefill feature in prompts intended for Gemini. Use the lowest-common-denominator feature set. If a feature is not available on all target providers, do not use it in portable prompts.

**Test across providers during development.** Do not optimize a prompt for one provider and then test portability later. Test on all target providers as you iterate. This surfaces portability problems early when they are easy to fix. If a change improves quality on GPT-5 but breaks Claude, you know immediately and can choose a different approach.

## Prompt Abstraction Layers

Some teams build prompt abstraction layers to manage provider differences programmatically rather than through manual prompt variants. The abstraction layer translates a canonical prompt representation into provider-specific prompts at runtime.

**Template-based abstraction** uses a prompt template with variables that are filled differently per provider. The template includes placeholders for instructions, examples, and format rules. The system substitutes provider-specific variants of those elements when generating the final prompt. This works when differences are localized—one provider needs stricter format instructions, another needs an extra example. It does not work when the entire prompt structure must change.

**Instruction rewriting** automatically modifies instructions based on known provider behavioral patterns. If Claude struggles with negations, the rewriter converts "do not include personal opinions" to "include only factual information." If Gemini ignores length constraints, the rewriter adds "this is critical: comply with the length limit." This requires deep knowledge of provider-specific quirks and ongoing maintenance as models change.

**Dynamic example selection** chooses few-shot examples based on which provider is being used. GPT-5 gets examples optimized for GPT-5 behavior. Claude gets examples optimized for Claude behavior. The core instruction set stays the same. Only examples vary. This works when instruction interpretation is consistent but example weighting differs.

Abstraction layers add complexity. You now maintain the abstraction layer code in addition to prompt content. For small teams or simple use cases, the complexity exceeds the value. For large teams managing hundreds of prompts across multiple providers, abstraction layers reduce duplication and centralize provider-specific logic.

## Maintaining Parallel Prompt Versions

When portability is not achievable, you maintain parallel prompt versions—one optimized for your primary provider, one for your backup. This doubles prompt maintenance but maximizes quality on each provider.

**Prompt versioning** tracks which version of each prompt is deployed for which provider. You tag prompts with provider identifiers and version numbers. When deploying a prompt update, you deploy separate versions for primary and backup. Rollback is per-provider. If a Claude prompt update causes regressions, you roll back Claude while keeping the GPT-5 version.

**Synchronized prompt changes** update both versions when core logic changes. If you add a new output field or change task instructions, you update both the GPT-5 prompt and the Claude prompt to reflect the change. The updates are not identical—you adapt to provider-specific needs—but they happen together. This prevents drift where one provider's prompt evolves and the other stagnates.

**Divergent prompt optimization** allows each version to evolve independently for provider-specific quality improvements. If you discover that Claude performs better with a specific phrasing, you update the Claude version without touching GPT-5. If GPT-5 benefits from an additional example, you add it to GPT-5 without adding it to Claude. Over time, the prompts diverge. They remain functionally equivalent—they perform the same task—but they are optimized for different models.

**Prompt testing parity** ensures both versions meet quality thresholds. Every time you update either prompt version, you run your eval suite against both. If one version regresses, you do not deploy it. Both versions must maintain quality above threshold. You do not let the backup prompt degrade just because it is not actively serving production traffic. The moment you need to fail over, backup quality matters as much as primary.

## The Prompt Debt Problem

Parallel prompt maintenance creates prompt debt—the accumulation of small differences between versions that over time become large divergences. What starts as "Claude needs one extra example" becomes "the two prompts solve the problem completely differently and we're not sure why."

**Prompt drift** happens when small provider-specific changes accumulate. Each change makes sense individually. Together, they create two prompts that are hard to compare. You can no longer reason about whether a logic change in one should be reflected in the other. The prompts have diverged beyond easy reconciliation.

**Testing coverage gaps** emerge as prompts drift. Your eval suite was designed for the original prompt structure. As prompts diverge, the eval suite tests commonalities but may miss provider-specific edge cases. You need provider-specific test cases. Those test cases grow over time. Testing effort increases.

**Ownership fragmentation** occurs when different engineers own different prompt versions. The engineer who optimized the GPT-5 prompt leaves. The engineer who optimized the Claude prompt moves to a different project. New engineers inherit prompt versions they did not write and do not fully understand. Knowledge about why specific phrasing or examples were chosen is lost.

Prompt debt is managed through regular reconciliation. Every quarter, review both prompt versions. Identify unnecessary divergences. Simplify where possible. Document intentional differences. Consolidate test coverage. Treat prompt maintenance as an ongoing discipline, not a one-time task.

## Automated Prompt Adaptation

Some teams use AI to adapt prompts across providers. You give a language model your optimized GPT-5 prompt and ask it to rewrite the prompt for Claude. The model analyzes provider-specific differences and generates a variant. This works surprisingly well for simple adaptations—adjusting phrasing, reordering instructions, tweaking examples.

**LLM-based rewriting** uses a meta-prompt that instructs a model to adapt prompts for different providers. The meta-prompt includes information about provider-specific behaviors: "Claude prefers explicit step-by-step instructions. Gemini requires strict format enforcement. Rewrite this GPT-5 prompt for Claude." The model generates an adapted version. You test the adapted version and iterate if necessary.

**Behavioral testing loops** automatically test generated adaptations and refine them. You generate a candidate adapted prompt, run it against your eval suite, measure quality, and feed results back to the adaptation model. The model adjusts the adaptation and you test again. After several iterations, you converge on an adapted prompt that meets your quality threshold.

This approach reduces manual work but introduces new risks. The adapted prompt may differ from the original in subtle ways you do not notice until production. The adaptation model may introduce behaviors you did not intend. Automated adaptation works best when followed by rigorous human review and testing.

## The Portability Trade-Off

Perfect prompt portability and maximum per-provider quality are incompatible goals. Portable prompts accept quality compromises on each provider. Provider-specific prompts maximize quality but double maintenance burden. The right choice depends on your constraints.

If your team is small, if prompts change frequently, or if maintaining synchronization is a burden, optimize for portability. Accept 2-5% quality loss on your primary provider in exchange for single-prompt maintenance. The operational simplicity is worth the quality trade-off.

If your team has dedicated prompt engineers, if quality thresholds are tight, or if you rarely change prompts, optimize per provider. Maintain parallel versions. Each version achieves maximum quality on its provider. The maintenance cost is justified by the quality gain.

Most teams start with portability and move to provider-specific optimization when quality gaps become unacceptable. The legal document analysis platform initially maintained separate prompts. As they scaled, the maintenance burden became unsustainable. They invested in building a prompt abstraction layer. The abstraction layer allowed them to maintain one canonical prompt representation with provider-specific rendering. This balanced quality and maintainability.

## What Comes Next

Even with portable or adapted prompts, you face a second challenge: quality calibration drift. Your backup provider may match your primary provider's quality today. Six months from now, your primary improves and backup does not, or backup regresses while primary stays stable. Quality baselines drift over time. The next subchapter covers how to detect and respond to quality calibration drift between providers before it creates unacceptable gaps during failover.

---

**Next: 5.4 — Quality Calibration Drift: When Backup Providers Diverge**
