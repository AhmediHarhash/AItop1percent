# 8.11 — Red-Team Reliability Exercises

The reliability team spent two hours trying to break the production AI system. They didn't inject malicious prompts. They didn't attack the API. They just simulated realistic failures: killed a Kubernetes pod, introduced 500ms latency to the database, throttled traffic from the upstream embedding service, doubled request volume for ten minutes. Within thirty minutes, they found three vulnerabilities. The model server crashed when the embedding service timed out. The fallback logic triggered correctly but returned empty responses instead of cached results. The auto-scaling kicked in too slowly, causing a two-minute latency spike before new pods were ready. None of these failures appeared during normal operation. They only surfaced when the system was stressed.

The team documented all three vulnerabilities. By the following week, they'd fixed two and added monitoring for the third. A month later, the embedding service actually did time out during a production incident. The model server didn't crash. The fallback logic returned cached results. The incident resolved in under five minutes with no user impact. The red-team exercise didn't prevent the upstream failure. It prevented the upstream failure from becoming a cascade.

Red-teaming for reliability is different from red-teaming for security. Security red teams try to exploit vulnerabilities. Reliability red teams try to induce failure. The goal isn't to breach the system. The goal is to break it in controlled conditions, understand how it breaks, and fix the failure modes before they happen in production. The exercise is part testing, part learning. You discover what breaks. You practice incident response. You validate that your monitoring actually detects the failure. And you find the gaps in your runbooks when you try to follow them under stress.

## Red Team vs Chaos Engineering

Red-teaming and chaos engineering overlap but have different goals. Chaos engineering injects failures randomly or continuously to verify the system tolerates them. You kill pods on a schedule, inject latency at random, and measure whether the system stays healthy. The goal is resilience: can the system absorb failure without degrading? Red-teaming for reliability injects targeted failures to discover vulnerabilities. You deliberately break a specific component — the model server, the cache, the rate limiter — and see what else breaks downstream. The goal is discovery: what don't we know about our failure modes?

Chaos engineering is continuous. You run chaos experiments weekly or daily in production. Red-teaming is episodic. You run red-team exercises quarterly or before major launches. Chaos engineering assumes the system should tolerate the injected failure. If it doesn't, that's a bug. Red-teaming assumes the system might not tolerate the failure, and that's useful information. If the model server can't handle a 5-second database timeout, you either fix it or document it as a known limitation.

Both techniques require production-like environments. Chaos engineering runs in production with low blast radius. Red-teaming runs in staging or a isolated production environment where failures won't affect real users. If your staging environment doesn't match production — different scale, different dependencies, different traffic patterns — the red-team exercise finds staging problems, not production problems. The value drops. The best red-team exercises run in production during low-traffic windows, just like the best chaos experiments do.

## Reliability Red Team Scope

A reliability red-team exercise has a defined scope: which system, which components, which failure modes, which constraints. The scope prevents the exercise from becoming a free-for-all where you break everything and learn nothing. You pick one subsystem — the model inference API, the data pipeline, the evaluation service — and focus the exercise there. You define the failure modes you'll test: latency, resource exhaustion, dependency failure, traffic spikes. You define the constraints: no failures that require manual recovery, no failures that affect real user data, no failures that take more than ten minutes to resolve.

For an AI model serving system, a typical red-team scope includes: upstream API failures (embedding service, rate limiter, authentication), downstream failures (database timeout, cache eviction, logging service down), resource exhaustion (CPU spike, memory leak, disk full), traffic anomalies (10x traffic spike, zero traffic, malformed requests), and infrastructure failures (pod killed, node failure, network partition). You don't test all of these in one exercise. You pick three to five failure modes per exercise and cycle through the full set over multiple quarters.

The scope also defines success criteria. What does it mean for the system to pass the red-team exercise? Typically: the failure is detected within your SLA, the system degrades gracefully instead of cascading, the monitoring alerts fire correctly, the runbook leads to mitigation, and the system recovers automatically or with minimal manual intervention. If the system meets all five criteria, it passes. If it fails any criterion, you've found a gap.

## Running Red Team Exercises

The exercise starts with a scenario briefing. The red-team facilitator describes the failure they'll inject: "At timestamp T, we'll introduce 2-second latency to all database queries." The responder team — usually the on-call engineer and a backup — prepares. They open the monitoring dashboard, pull up the runbook, and confirm they know how to mitigate. The facilitator injects the failure. The responder team detects it, mitigates it, and documents what happened. The entire exercise runs 15 to 45 minutes depending on complexity.

During the exercise, the facilitator observes and takes notes. How long did detection take? Did the monitoring alert fire? Did it fire accurately, or were there false positives? Did the runbook have the right steps? Did the responder team escalate appropriately? Did the system recover automatically, or did it require manual intervention? These observations become the exercise debrief. The debrief happens immediately after the exercise, while memory is fresh. The team discusses what worked, what didn't, and what to fix.

Some exercises include a surprise element. The facilitator injects a second failure while the team is mitigating the first. Example: The team is handling a database timeout. While they're investigating, the facilitator kills a model server pod. This tests whether the monitoring surfaces both failures, whether the team can triage multiple concurrent incidents, and whether the mitigations for one failure interfere with the other. Surprise elements are stressful. You don't use them every exercise. But once per quarter, a surprise element trains the team to handle the chaos of real incidents, where failures rarely arrive one at a time.

## Documenting and Acting on Findings

Every red-team exercise produces a findings report. The report lists the failure injected, the system response, the detection time, the mitigation actions, and the outcome. It also lists gaps: monitoring that didn't fire, runbook steps that were unclear, escalation paths that were wrong, recovery actions that failed. These gaps become tickets. High-severity gaps — the system failed to detect the failure, or the runbook led to the wrong mitigation — become P0 tickets. Medium-severity gaps — detection was slow, or the runbook was ambiguous — become P1 tickets. Low-severity gaps — minor monitoring improvements — get backlogged.

The findings report is shared with the broader engineering team. Not just the responders. Everyone who owns a component that was tested in the exercise reads the report and understands what broke. This transparency spreads knowledge. The data engineering team sees that the model server doesn't handle database timeouts gracefully and adds a note to their own runbooks. The platform team sees that pod restarts don't trigger alerts and fixes the monitoring configuration. The red-team exercise discovers one team's gaps and improves three teams' readiness.

Some findings are architectural. The red-team exercise reveals that the system has no fallback for a critical dependency. The upstream embedding service has no backup. If it fails, the model server fails. The finding isn't a bug you can fix with a patch. It's a design limitation. The team documents it as a known risk, estimates the cost to add redundancy, and escalates the decision to leadership. The red-team exercise didn't fix the problem. It made the problem visible and quantified the risk.

## Frequency and Timing of Exercises

Most teams run reliability red-team exercises quarterly. More frequent than that, and you're spending too much engineering time on exercises instead of building. Less frequent, and the exercises don't keep pace with system changes. A system that was resilient in January might be fragile in June after a major refactor. Quarterly exercises catch regressions before they become incidents.

The timing within the quarter matters. You don't run a red-team exercise the day before a major launch. You run it two to four weeks before the launch, so you have time to fix any gaps the exercise reveals. You also don't run an exercise during a high-traffic period or right after a real incident, when the team is already stressed. The ideal timing is mid-quarter, during a low-traffic week, when the team has bandwidth to focus and recover from any unexpected failures the exercise triggers.

Some teams run a red-team exercise after every major system change: a new model deployment, a new infrastructure migration, a new dependency integration. The exercise validates that the change didn't introduce fragility. If you migrated from one database to another, the red-team exercise tests whether the new database handles timeouts, failovers, and connection pool exhaustion the same way the old one did. If it doesn't, you discover the difference during the exercise, not during an incident.

## Red Team Skill Requirements

Running an effective reliability red-team exercise requires someone who understands the system deeply enough to know which failures are realistic and which aren't. The facilitator isn't trying to break the system in exotic ways. They're simulating the failures that actually happen in production: a pod gets killed by the orchestrator, an upstream service has a bad deploy and starts returning 500s, a network partition splits the cluster. If the facilitator injects failures that would never happen in production, the exercise trains the team for unrealistic scenarios.

The facilitator also needs the authority and tooling to inject failures safely. They need admin access to the Kubernetes cluster, the ability to introduce latency or packet loss, the ability to throttle or kill services. They need a rollback plan: if the injected failure causes something unexpected, how do they stop the failure and restore the system? They need coordination with the platform team: if the exercise accidentally affects real users, who do they notify, and how fast can they roll back?

For teams that don't have internal expertise, some organizations bring in external reliability consultants to facilitate the first few exercises. The consultant designs the scenarios, injects the failures, observes the response, and writes the findings report. The internal team learns the facilitation process by watching. By the third or fourth exercise, the internal team can run exercises independently. The consultant's value isn't just running the exercise. It's transferring the skill to the internal team.

## Measuring Red Team Effectiveness

A red-team exercise is effective if it finds gaps that you fix before they cause production incidents. The effectiveness metric is simple: How many production incidents in the past year would have been prevented or mitigated faster if we'd discovered the failure mode during a red-team exercise? If the answer is zero, your red-team exercises aren't finding realistic failure modes. If the answer is more than two, your exercises are providing value.

The second effectiveness metric is responder confidence. After a red-team exercise, you survey the responders: Do you feel more prepared to handle this failure in production? If yes, the exercise trained the team. If no, the exercise was too easy, too hard, or too unrealistic. The goal isn't to make the team feel good. The goal is to make them more competent. A hard exercise that reveals gaps is better than an easy exercise that everyone passes.

The third metric is runbook improvement rate. How many runbooks were updated based on red-team findings? If you run four exercises per year and update zero runbooks, the exercises aren't surfacing documentation gaps. If you update five runbooks per year based on red-team findings, the exercises are working. The runbook updates are a trailing indicator that the exercises revealed something the team didn't already know.

The red-team facilitator injects a database timeout. The model server crashes. The monitoring doesn't alert. The runbook has the wrong mitigation steps. The responder team escalates in five minutes, but by then the exercise has revealed three gaps. You fix the crash, add the alert, and update the runbook. A month later, the database actually times out. The model server doesn't crash. The monitoring alerts. The runbook works. The incident resolves in three minutes. That's the value of red-teaming for reliability: finding the gaps in controlled conditions, fixing them before they matter, and knowing your system can survive the failures you've already practiced.
