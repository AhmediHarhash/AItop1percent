# 6.3 — Runbooks for Common AI Failure Scenarios

The model provider's API started returning 529 errors at 3:14am. The on-call engineer had never seen a 529 before. They checked the provider's status page, which showed all systems operational. They searched internal documentation for incident response procedures. Nothing mentioned 529 errors specifically. They pinged the team Slack channel asking if anyone had seen this before. No response at 3am. They spent twenty minutes reading the provider's API documentation trying to understand what 529 meant. By the time they understood it was a transient overload error and implemented retry logic, forty minutes had passed and the service had returned hard errors to users for the entire duration. The post-incident review identified the core failure: the team had no runbook for provider outages. Every engineer was expected to diagnose and respond from first principles during an incident. This works during business hours when the full team is available. It fails at 3am when one engineer is alone and every minute of diagnosis is a minute of continued user impact.

Runbooks are pre-written response procedures for common failure scenarios. They tell the responder exactly what to check, exactly what actions to take, and exactly when to escalate. They eliminate the diagnosis phase for known failure modes. When a runbook scenario is detected, the responder follows the runbook steps without needing to understand root cause first. Understanding comes later. Response happens immediately. AI systems have recurring failure patterns that happen often enough to justify runbooks: provider outages, quality degradation, safety violations, hallucination spikes, latency degradation, and cost explosions. If your team has encountered a failure mode more than twice, you need a runbook for it.

## What Makes a Good AI Incident Runbook

A good runbook is specific, actionable, and complete. It starts with detection: what signals indicate this scenario is happening. It lists triage steps: what checks confirm that this is the right scenario and not something else. It provides immediate mitigation actions: what to do in the first five minutes to stop damage. It includes rollback procedures: how to revert to last known good state if mitigation does not work. It specifies escalation criteria: when to pull in additional responders or escalate severity. It ends with validation steps: how to confirm that the mitigation worked.

A bad runbook is vague and assumes context that the responder might not have. It says "check if the model is working correctly" without specifying what checks to run. It says "roll back if necessary" without specifying what conditions trigger rollback or how to execute it. It says "escalate to senior engineer" without specifying what signals indicate escalation is needed. Bad runbooks exist to satisfy a compliance checkbox. Good runbooks exist to enable fast, confident response by whoever is on-call, regardless of their experience level.

The test of a good runbook is whether a new team member could follow it successfully during their first on-call shift. If the runbook requires domain knowledge that only senior engineers have, it will fail when you need it most. If the runbook is written in a way that makes sense to the person who wrote it but confuses everyone else, it will not be followed. If the runbook has not been tested and validated by actually using it during incidents, it probably contains steps that do not work or skips steps that are critical.

Every runbook should be structured the same way so responders know where to look for the information they need. Start with a one-line description of the scenario. Follow with detection signals: the specific metrics, alerts, or user reports that indicate this scenario. Then triage steps: the checks that confirm this is the right scenario. Then immediate actions: what to do in the first five minutes. Then mitigation options: ranked from least disruptive to most disruptive. Then rollback procedures: how to revert if mitigation fails. Then escalation criteria: when to call for help. Then validation steps: how to confirm success. End with follow-up tasks: what to document and investigate after the incident is resolved.

## Runbook: Model Provider Outage

**Scenario:** Your primary model provider's API is returning errors, timing out, or behaving unexpectedly. This includes complete outages, elevated error rates, elevated latency, or silent behavior changes.

**Detection signals:** Error rate from provider API exceeds five percent. Latency from provider API exceeds 2x baseline. Alert from provider status page. User reports of service unavailability or degraded performance.

**Triage steps:** Check the provider's status page for announced outages or incidents. Check error logs for specific error codes. Check if the issue affects all traffic or only specific request patterns. Check if fallback models are available and healthy. Estimate affected user percentage based on traffic routing.

**Immediate actions within five minutes:** Enable fallback model if configured and if fallback quality is acceptable for the use case. If no fallback is configured, enable circuit breaker to return graceful error messages instead of timeout errors. Notify customer support team that the service is degraded due to provider issue. Post initial status update if the issue affects more than ten percent of traffic.

**Mitigation options ranked by disruption:** First, enable retry logic with exponential backoff if not already enabled. Second, route affected traffic to fallback model if quality degradation is acceptable. Third, route affected traffic to alternative provider if multi-provider setup exists. Fourth, enable degraded mode where non-essential features are disabled but core functionality remains. Fifth, enable maintenance mode with user-facing message explaining the outage.

**Rollback procedures:** If you enabled fallback model and it is performing worse than expected, revert to returning error messages. If you routed to alternative provider and quality is worse, route back to primary provider even if errors are elevated. If you enabled degraded mode and users report it is worse than full outage, switch to maintenance mode.

**Escalation criteria:** If the outage lasts more than fifteen minutes, escalate to engineering manager. If the provider status page shows no incident but your service is clearly affected, escalate to platform team to investigate whether the issue is on your side. If the outage affects a tier-one customer or high-value use case, escalate to account manager to coordinate customer communication.

**Validation steps:** Monitor error rate from provider API until it returns to baseline. Monitor quality metrics if you enabled fallback model to ensure quality is acceptable. Check user-reported issue rate to confirm that mitigation reduced user impact. Run on-demand eval suite if quality concerns exist.

**Follow-up tasks:** Document the outage timeline, affected traffic volume, and mitigation actions in the incident log. Review whether fallback model quality was acceptable and whether routing decisions were correct. Identify whether additional monitoring or automation could have detected the outage faster. If the provider has published a post-mortem, review it and assess whether your architecture could prevent similar impact in future outages.

## Runbook: Quality Degradation Detected

**Scenario:** Your primary quality metric has dropped below acceptable threshold or shows sustained downward trend. Outputs are measurably worse than baseline but the system is operationally healthy.

**Detection signals:** Primary eval metric drops below threshold or drops by more than ten percent from baseline. User-reported issue rate increases by more than 2x. Quality-focused alerts trigger. Customer support reports pattern of quality complaints.

**Triage steps:** Check recent deployments for prompt changes, model updates, config changes, or code changes that could affect outputs. Check model provider status and version to see if they updated their base model. Check input distribution for shifts in query types or user segments that could trigger different model behavior. Sample recent outputs and manually review to understand what "degraded quality" looks like in this incident. Check if degradation affects all traffic or specific segments.

**Immediate actions within five minutes:** If a deployment happened in the past 24 hours, prepare rollback but do not execute yet. If degradation is severe and affects core functionality, notify customer support team. If degradation affects safety-critical outputs, escalate severity to P1 and notify Safety team. Post initial status update if user reports are elevated.

**Mitigation options ranked by disruption:** First, roll back most recent prompt or config change if one occurred. Second, roll back most recent model version change if one occurred. Third, adjust routing rules to send affected traffic to fallback model or previous model version. Fourth, enable human review for affected traffic if volume is manageable. Fifth, disable affected feature if degradation is severe and no other mitigation is available.

**Rollback procedures:** Rollback requires deploying previous version of prompt, config, or model routing rules. Follow standard deployment process but mark as emergency rollback. After rollback, wait fifteen minutes for traffic sample to accumulate, then check whether quality metric improves. If quality does not improve within thirty minutes, the rollback did not fix the issue and you need to investigate other hypotheses.

**Escalation criteria:** If quality degradation has no clear cause based on recent changes, escalate to senior engineer to investigate. If quality degradation is confirmed but mitigation options have been exhausted, escalate to engineering manager to decide whether to accept degraded quality or disable feature. If degradation involves safety concerns, escalate to Trust and Safety team. If degradation involves regulated content, escalate to Legal.

**Validation steps:** Monitor primary eval metric for sustained improvement. Check that user-reported issue rate returns to baseline within one hour of mitigation. Run on-demand eval suite focused on the specific quality dimension that degraded. Sample and manually review recent outputs to confirm quality looks acceptable. Validate that rollback or mitigation did not introduce new quality issues in other dimensions.

**Follow-up tasks:** Identify root cause if not already known. Determine whether the incident exposed gaps in evaluation coverage or monitoring coverage. Assess whether similar degradation could happen again and what preventive measures would help. Document specific examples of degraded outputs to add to continuous eval suite.

## Runbook: Safety Violation in Production

**Scenario:** The model produced outputs that violate safety policies, expose sensitive information, recommend harmful actions, or otherwise cross safety boundaries.

**Detection signals:** Safety-focused eval detects policy violation. User reports outputs that are harmful, biased, or inappropriate. Internal review identifies outputs that should have been blocked. Media reports or social media posts highlight problematic outputs.

**Triage steps:** Identify the specific outputs that violated safety policies and what policy they violated. Determine if the violation was an isolated incident or part of a pattern. Check if the violation affects specific query types, user segments, or use cases. Assess severity using safety dimension of incident severity framework. Estimate how many users were exposed to violating outputs and whether those outputs are likely to be shared publicly.

**Immediate actions within five minutes:** If violation is severe, disable the affected feature immediately. If violation affects specific query patterns, implement query filtering to block those patterns. Notify Trust and Safety team. If outputs have been shared publicly or picked up by media, notify Communications team. Preserve examples of violating outputs for investigation but also scrub from production systems if they contain sensitive data.

**Mitigation options ranked by disruption:** First, implement output filtering to block specific violating patterns while keeping feature enabled. Second, add human review for high-risk queries if violation patterns are identifiable. Third, roll back recent prompt or model changes if violation started after recent deployment. Fourth, route affected queries to more conservative model with stricter safety boundaries. Fifth, disable feature entirely if no partial mitigation is possible.

**Rollback procedures:** If violation started after recent prompt or model change, roll back that change immediately. If rollback is not possible because the previous version had other issues, deploy emergency prompt fix that explicitly instructs model to avoid violating behavior. If violation is in base model behavior and not caused by your changes, you cannot roll back and must implement filtering or disable feature.

**Escalation criteria:** All safety violations escalate to Trust and Safety team within five minutes. If violation involves potential legal issues, escalate to Legal within fifteen minutes. If violation is generating public attention, escalate to Communications and executive team within fifteen minutes. If violation involves regulated content or protected user data, treat as P1 and initiate full incident response.

**Validation steps:** Test output filtering or mitigation by running queries that previously produced violations. Confirm that filtered or mitigated version blocks violating outputs without breaking legitimate use cases. Run safety-focused eval suite to validate that mitigation did not introduce new safety issues. Monitor production outputs for next two hours to confirm no additional violations occur.

**Follow-up tasks:** Conduct root cause analysis with Trust and Safety team to understand why existing safety measures did not prevent violation. Add specific violation patterns to continuous safety eval suite. Review whether safety policies need clarification or safety boundaries need adjustment. Assess whether similar violations could occur in other features or use cases.

## Runbook: Hallucination Spike

**Scenario:** The model is producing factually incorrect outputs, making up information, or citing non-existent sources at rates significantly higher than baseline.

**Detection signals:** Factual accuracy eval metric drops. User reports specifically mention incorrect facts, made-up citations, or nonsensical information. Internal review identifies pattern of hallucinated content. Customer complaints about incorrect information.

**Triage steps:** Sample recent outputs and identify specific hallucination patterns. Determine if hallucinations are domain-specific or general. Check if hallucinations involve specific entity types, date ranges, or query patterns. Review recent changes to prompts, retrieval logic, or context injection that could increase hallucination likelihood. Assess whether hallucinations are harmful or just inaccurate.

**Immediate actions within five minutes:** If hallucinations involve high-stakes domains like medical, legal, or financial advice, escalate severity to P1. If hallucinations are generating user complaints, notify customer support team. If hallucinations started after recent change, prepare rollback.

**Mitigation options ranked by disruption:** First, adjust prompt to include stronger instructions against hallucination, such as explicit "only use provided context" or "admit uncertainty rather than guess" instructions. Second, roll back recent changes to retrieval logic or context injection if hallucinations correlate with those changes. Third, reduce model temperature or adjust sampling parameters to make outputs more conservative. Fourth, enable citation validation where outputs are checked against source documents before delivery. Fifth, route queries to model with lower hallucination rates even if other quality dimensions are worse.

**Rollback procedures:** If hallucinations started after prompt change, roll back prompt. If hallucinations started after retrieval logic change, roll back retrieval logic. If hallucinations started after model version update, roll back to previous model version. Wait fifteen minutes after rollback to collect new samples and validate that hallucination rate decreases.

**Escalation criteria:** If hallucinations involve high-stakes domains and cannot be mitigated quickly, escalate to engineering manager to decide whether to disable feature. If hallucinations are generating significant user complaints or threatening customer relationships, escalate to product manager to coordinate customer communication. If hallucinations may have caused user harm, escalate to Legal.

**Validation steps:** Sample outputs after mitigation and manually verify that hallucination rate has decreased. Run factual accuracy eval suite. Check user-reported issue rate to confirm user complaints decrease. For high-stakes domains, enable temporary human review to validate that outputs are factually accurate.

**Follow-up tasks:** Analyze what conditions triggered hallucination spike. Review whether prompts adequately discourage hallucination. Assess whether retrieval quality issues contributed to hallucinations. Add hallucination examples to continuous eval suite. Consider whether architectural changes like citation validation should be permanent features.

## Runbook: Latency Degradation

**Scenario:** Model response time has increased significantly. Requests are timing out or users are experiencing slow responses.

**Detection signals:** P95 or P99 latency exceeds threshold. Timeout rate increases. User reports of slow responses. Latency alerts trigger.

**Triage steps:** Check model provider latency metrics to see if degradation is on provider side. Check infrastructure metrics to rule out resource exhaustion or network issues. Check if latency affects all requests or specific patterns. Review recent changes that could affect latency such as prompt length increases, context injection changes, or model version updates. Check if traffic volume has increased beyond provisioned capacity.

**Immediate actions within five minutes:** If latency is severe and approaching timeout thresholds, reduce timeout values to fail fast rather than letting requests hang. Notify customer support if user impact is significant. If latency is provider-side based on their status page, follow provider outage runbook.

**Mitigation options ranked by disruption:** First, increase timeout values if requests are timing out but eventually succeeding. Second, reduce prompt length or context size if those increased recently. Third, enable request queuing to smooth traffic spikes if latency is caused by burst load. Fourth, route traffic to faster model even if quality is slightly worse. Fifth, enable caching for repeated queries if applicable. Sixth, scale up infrastructure if resource exhaustion is detected. Seventh, enable degraded mode where non-essential features are disabled to reduce per-request latency.

**Rollback procedures:** If latency increased after prompt or context changes, roll back those changes. If latency increased after model version update, roll back to previous model version. If latency increased after infrastructure change, roll back infrastructure change.

**Escalation criteria:** If latency makes service effectively unusable, escalate to P1. If latency is provider-side and sustained, escalate to account team to pressure provider. If latency is caused by infrastructure issues that on-call cannot fix, escalate to platform team. If latency affects tier-one customer, escalate to account manager.

**Validation steps:** Monitor latency metrics until P95 and P99 return to baseline. Check timeout rate returns to normal. Check user-reported issue rate decreases. Run load test if mitigation involved infrastructure scaling to validate capacity.

**Follow-up tasks:** Identify root cause if not already clear. Review whether latency budgets need adjustment. Assess whether caching or other architectural changes could prevent future latency issues. Document what mitigation worked and whether it should be permanent.

## Runbook: Cost Explosion

**Scenario:** AI system costs are significantly higher than expected or budget. Token usage, request volume, or provider charges have spiked.

**Detection signals:** Cost monitoring alert triggers. Provider bill preview shows unexpected charges. Token usage metrics spike. Request volume metrics spike without corresponding business metric growth.

**Triage steps:** Identify which component or feature is driving cost increase. Check if cost increase correlates with traffic increase, prompt length increase, or model version change. Review recent changes that could affect token usage. Check for retry storms or error loops that could cause repeated requests. Assess whether cost increase represents attack, abuse, or legitimate usage pattern change.

**Immediate actions within five minutes:** If cost increase appears to be abuse or attack, enable rate limiting immediately. If cost increase is from specific feature, calculate burn rate and estimated time to budget exhaustion. If budget will be exhausted within hours, this is P1.

**Mitigation options ranked by disruption:** First, identify and disable any retry loops or error conditions causing repeated requests. Second, enable or tighten rate limiting. Third, reduce prompt length or context size if those increased recently. Fourth, disable non-essential features that contribute to cost. Fifth, route traffic to cheaper model if quality is acceptable. Sixth, enable aggressive caching to reduce redundant requests. Seventh, temporarily disable feature if cost cannot be controlled otherwise.

**Rollback procedures:** If cost increase started after prompt or model change, roll back that change. If cost increase started after feature launch, disable that feature.

**Escalation criteria:** If cost will exceed budget within 24 hours, escalate to engineering manager and finance. If cost increase appears to be abuse or attack, escalate to security team. If cost cannot be controlled with available mitigations, escalate to executive team to approve emergency budget increase or feature shutdown.

**Validation steps:** Monitor cost metrics and confirm burn rate decreases after mitigation. Check that quality has not degraded unacceptably if mitigation involved routing to cheaper model. Validate that rate limiting is not blocking legitimate users.

**Follow-up tasks:** Analyze root cause of cost increase. Review whether cost monitoring thresholds need adjustment. Assess whether architectural changes could prevent future cost explosions. Document what mitigation worked and whether cost controls need to be permanent.

## Maintaining and Updating Runbooks

Runbooks decay. A runbook written six months ago references tools you no longer use, mitigation options you no longer have, and escalation paths that no longer exist. A runbook that does not get updated becomes worse than no runbook because it gives false confidence and outdated guidance.

Update runbooks after every incident where the runbook was used. If a step did not work, fix it. If a step was missing, add it. If a step was confusing, clarify it. The person who responded to the incident is the best person to update the runbook because they just experienced what works and what does not.

Test runbooks during chaos engineering exercises or fire drills. Simulate the failure scenario and have an engineer who did not write the runbook follow it. Watch for steps where they get confused, steps that no longer work, and steps that are missing. Update the runbook based on what you learn.

Review runbooks quarterly even if they have not been used. Check if tools, systems, or processes have changed in ways that make the runbook outdated. Check if escalation contacts are still correct. Check if severity classifications still align with current priorities.

A runbook that is six months old and actively maintained is infinitely more valuable than a runbook that is perfectly written but never updated. The goal is not perfect runbooks. The goal is runbooks that actually help responders during incidents, and that requires continuous maintenance.

---

Next: 6.4 — The First Five Minutes: Containment Before Diagnosis
