# 7.1 — The Recovery Checklist: Beyond "It's Working Again"

In March 2025, a legal AI platform declared an incident resolved after rolling back a deployment that caused response latency to spike from 800 milliseconds to 9 seconds. The dashboard showed latency back to normal. The incident commander closed the incident channel. Four hours later, users began reporting that the system was giving them outdated information — answers that referenced documents uploaded weeks ago but ignored documents uploaded in the past six hours. The rollback had reverted the deployment, but the indexing pipeline was still catching up. The system was "working" in the sense that it responded quickly. It was not recovered in the sense that it was providing correct answers. The team reopened the incident, apologized to users, and spent another eight hours validating that all subsystems were genuinely restored. They learned that "working" and "recovered" are not synonyms.

The recovery checklist is what stands between premature victory declarations and actual system health. It is a structured validation procedure that ensures every component, every dependency, and every user-facing capability is genuinely restored before you declare recovery. For AI systems, this checklist is longer and more nuanced than for traditional software because AI systems have more state, more cascading dependencies, and more ways to appear functional while being subtly broken.

## The Premature Recovery Declaration Problem

The pressure to declare recovery is immense. Stakeholders want the incident closed. The incident channel is chaotic and exhausting. The on-call engineer wants to hand off to the next shift. The dashboard is green. The most recent test query returned a reasonable answer. It is deeply tempting to say "we're back" and close the war room. But premature declarations cause three predictable problems. First, users experience the failure twice — once during the incident, and again when the incomplete recovery causes a secondary failure. Second, the team loses credibility. Declaring victory then reopening the incident an hour later signals that you do not know what "recovered" means. Third, the real root cause often hides in the gap between "working" and "recovered." If you declare success before validating all components, you miss the evidence that reveals why the system failed in the first place.

AI systems are especially vulnerable to premature declarations because they degrade gracefully. A partially recovered RAG system might retrieve documents but retrieve the wrong ones. A partially recovered agent might respond to users but forget everything it learned in the past day. A partially recovered classifier might return predictions but at lower confidence. These are not hard failures. The system appears to work. The logs show requests completing. The dashboard shows green. Only careful validation reveals that the system is functional but not correct.

## The Core Recovery Checklist

A recovery checklist is not a single yes-or-no question. It is a structured set of validations covering every layer of the system. The checklist for an AI system includes technical validations, data validations, user-facing validations, and stakeholder sign-offs. Each item must be verified before moving to the next. Skipping items or parallelizing validations that have dependencies creates blind spots.

The technical validation layer confirms that infrastructure is healthy. Are all services running? Are all replicas healthy? Are all health check endpoints returning success? Are resource utilization levels normal — CPU, memory, GPU, disk? Are all caches populated? Are all background jobs running? Are all queues draining at normal rates? These are the basics, but they are not sufficient. A service can pass health checks while serving incorrect results. A queue can drain while processing stale data. Technical health is necessary but not sufficient for recovery.

The data validation layer confirms that the system is operating on current, correct data. For RAG systems, this means validating that the vector database reflects the latest document uploads, that embeddings are current, that retrieval is returning expected results for known queries. For fine-tuned models, this means validating that the correct model checkpoint is deployed, that model version metadata matches expectations, that inference results match spot-check examples. For agent systems, this means validating that memory stores are intact, that learned behaviors reflect recent feedback, that context windows contain the right information. Data validation is where most premature recovery declarations fail. The infrastructure is healthy, but the data is stale, incomplete, or corrupted.

The user-facing validation layer confirms that end-user capabilities are restored. Can users log in? Can they submit queries and receive responses? Are response times within normal ranges? Are responses correct for a sample of known-good test cases? Are any features degraded or unavailable? For conversational systems, can users resume conversations from before the incident, or does the system treat them as new users? For agent systems, do agents remember prior context, or do they start fresh? User-facing validation requires running real queries through the production system — not just checking dashboards. Dashboards show aggregate health. User-facing validation shows actual experience.

The stakeholder sign-off layer confirms that the teams responsible for each component agree that their component is recovered. Engineering signs off that the technical systems are healthy. Data Engineering signs off that pipelines are current. Product signs off that user experience is restored. Trust and Safety signs off that safety mechanisms are operational. Legal signs off that audit logging is intact. Customer Support signs off that they can handle inbound questions about the incident. Stakeholder sign-off is not theater. It is a forcing function that ensures every responsible party has validated their domain before the incident is closed.

## Validation Sequencing and Dependencies

The order of validation matters. You cannot validate user-facing functionality before validating data currency. You cannot validate data currency before validating infrastructure health. The recovery checklist is a directed graph, not a flat list. Some validations have prerequisites. Some can run in parallel. The incident commander owns the sequencing and ensures no validation is skipped because someone assumed another team already checked it.

A common sequencing error is validating user-facing functionality too early. A support engineer runs a test query, gets a reasonable response, and declares the system recovered. But they did not validate that the response used current data. They did not validate that the model version is correct. They did not validate that the agent's memory is intact. The single test query happened to work, but the system is not comprehensively recovered. The correct sequence is infrastructure first, data second, inference third, user-facing fourth, stakeholder sign-off fifth.

Another common error is assuming that if the primary service is healthy, all dependencies are healthy. A RAG system might be serving requests, but the background indexing job that updates the vector database might still be catching up from a backlog. The system appears recovered because queries complete, but users are seeing stale results. Recovery validation must walk the entire dependency graph, not just check the front door.

## The Difference Between Up and Recovered

A system is up when it responds to requests. A system is recovered when it responds correctly, with current data, with all features operational, and with confidence that it will not fail again imminently. The gap between up and recovered is where premature declarations live. For traditional software, the gap is often small — a web server that responds is usually recovered. For AI systems, the gap is large and dangerous. An AI system can respond confidently with outdated information, corrupted state, or degraded model quality. It looks up. It is not recovered.

The distinction shows up in how you communicate status. Telling stakeholders "the system is up" creates the expectation that everything is fine. Telling stakeholders "the system is responding, and we are now validating recovery" sets the correct expectation. It signals that more work remains before the incident is truly closed. It buys the team time to complete the checklist without pressure to declare victory prematurely.

## Time-Delayed Recovery Issues

Some recovery issues do not appear immediately. They surface hours or days after the system is declared recovered. These time-delayed issues are why recovery includes a stabilization period — a window of elevated monitoring after the checklist is complete. Common time-delayed issues include cache inconsistency, where stale cached results serve users for hours after the underlying data is corrected; memory leaks, where resource usage grows slowly and causes a secondary failure twelve hours later; and agent state drift, where agents appear to work correctly at first but gradually degrade as they process more requests with partially corrupted memory.

The stabilization period is explicitly part of recovery. It is not "post-recovery monitoring." It is the final phase of recovery. During stabilization, monitoring is more aggressive, alerts are tighter, and the on-call engineer expects anomalies. The incident is not closed until the stabilization period passes without issues. For complex AI systems, the stabilization period is often 24 to 48 hours. For agent systems, it can be longer — agents might interact with users sporadically, and corrupted state might not manifest until the agent encounters a specific type of query days later.

## Documentation During Recovery

Every step of the recovery checklist must be documented in the incident log. Who validated what, at what time, with what result. This documentation serves three purposes. First, it creates accountability — if a validation was skipped, the gap is visible. Second, it provides evidence for post-incident review — the timeline shows whether the checklist was followed and where delays occurred. Third, it creates a template for future incidents — the next incident commander can see what worked and what did not.

Documentation is not optional even when the team is exhausted. The incident commander enforces documentation discipline. A common pattern is to use a shared document or incident management tool where each checklist item has a checkbox, a timestamp, and a name. The item is not checked until someone has explicitly validated it and recorded the result. If validation fails, the failure is documented, the remediation is documented, and the item is re-validated.

## Recovery Metrics and Success Criteria

Recovery is not complete until the system meets predefined success criteria. These criteria are established before the incident, ideally as part of runbooks or incident response plans. Success criteria include technical thresholds like latency below a specific percentile, error rates below a specific percentage, and resource utilization within normal ranges. They include data thresholds like retrieval accuracy above a specific score, model output quality matching baseline eval metrics, and agent memory consistency above a specific percentage. They include user-facing thresholds like successful query completion rate above a specific level and user-reported issues below a specific rate.

The success criteria are objective. They remove ambiguity about when recovery is complete. If latency is above the threshold, recovery is not complete. If retrieval accuracy is below the threshold, recovery is not complete. The incident commander does not declare recovery based on intuition. They declare recovery when all success criteria are met and all stakeholders have signed off.

## Stakeholder Communication During Recovery

Stakeholders need to know recovery status, but they do not need to know every technical detail. The incident commander provides concise updates at regular intervals. The update format is: current status, what has been validated, what remains to be validated, expected time to completion. If a validation fails, the update includes what failed, what is being done to remediate, and the revised timeline. Stakeholders do not need to know that the vector database cache is warming or that the agent memory store is being revalidated. They need to know whether recovery is on track or delayed.

Internal stakeholders — Engineering, Product, Support — receive more detailed updates because they may need to take action. Support needs to know if certain features are still degraded so they can set user expectations. Product needs to know if recovery will take longer than expected so they can decide whether to communicate with users. Engineering needs to know if additional resources are required to accelerate recovery. The incident commander tailors communication to the audience.

## The Recovery Handoff

When recovery is complete, the incident commander hands off to the on-call rotation. The handoff is explicit and documented. It includes a summary of what failed, what was done to resolve it, what was validated during recovery, what the stabilization monitoring plan is, and what the escalation path is if anomalies appear during stabilization. The on-call engineer acknowledges the handoff and confirms they understand the monitoring expectations. A poor handoff is a common cause of re-failure — the next shift does not realize the system is fragile, misses an early warning signal, and the system fails again.

The handoff document lives in the incident log and is accessible to everyone who might be on-call during the stabilization period. It is not a verbal handoff that disappears when the shift changes. It is a written artifact that persists.

## When Recovery Fails Validation

If a checklist item fails validation, recovery is not complete. The team investigates why the validation failed, remediates the issue, and re-runs the validation. This is not a setback. It is the checklist working as designed — catching incomplete recovery before it is declared. The incident commander does not skip the failed item or declare recovery anyway. The failed validation is treated as new information that the system is not yet healthy.

A common failure mode is pressure from stakeholders to declare recovery even when validations are incomplete. "The system looks fine, can we just close the incident?" The answer is no. The checklist exists because "looks fine" is not a reliable heuristic. The incident commander protects the integrity of the recovery process, even when it is uncomfortable.

Recovery is not the end of the incident. It is the beginning of the post-incident review, the analysis of root causes, and the implementation of preventive measures. But it is a critical milestone. A complete, validated recovery means the system is safe to operate, users are no longer affected, and the team can shift from firefighting to learning. The checklist is what makes that shift possible.

Next, we examine the unique recovery challenges for conversational and agent systems, where state is not just infrastructure but memory, context, and learned behavior.
