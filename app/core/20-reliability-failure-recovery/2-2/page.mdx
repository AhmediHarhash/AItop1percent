# 2.2 — The Quality Signal Stack: What to Monitor Beyond Infrastructure

Most teams that realize traditional monitoring is insufficient respond by adding more infrastructure metrics. They track token usage, model inference time, prompt token count, completion token count, API provider response codes. These are useful operational metrics. They are still not quality metrics. They still do not tell you whether the model is producing good outputs. The engineering lead at a document processing company accumulated 47 distinct metrics across Datadog, CloudWatch, and their internal dashboards. When their summarization model started producing incoherent summaries, none of those 47 metrics detected it. They were measuring the right category of things — operations — but missing the category that mattered — quality.

AI reliability requires a layered approach. Infrastructure metrics form the foundation. Quality metrics sit on top. Safety metrics catch the tail risks. Business metrics close the loop. Together, these layers create a signal stack that detects failures at every level. Miss a layer and you have blind spots. Build all layers and you catch regressions before they compound.

## The Five-Layer Signal Stack

The framework has five layers. Each layer provides visibility into different failure modes. Each layer requires different instrumentation. Each layer has different latency between failure and detection. The stack builds from bottom to top: infrastructure enables operations, operations enable quality measurement, quality measurement enables safety assurance, safety assurance enables business outcomes.

**Layer 1: Infrastructure.** This measures whether the system is available. Uptime, error rates, latency distribution, throughput, API status codes. This is what traditional APM provides. It detects outages, service degradation, and network failures. It does not detect quality problems. It is necessary but not sufficient. If infrastructure is failing, nothing else matters. If infrastructure is healthy, you still do not know if the AI is working.

**Layer 2: Model Operations.** This measures how the model is being used and whether it is operating within expected parameters. Token consumption, prompt length distribution, completion length distribution, temperature settings, refusal rates, model version deployed, provider API response times. This layer detects operational anomalies: sudden spikes in token usage indicating a prompt injection attack, refusal rate increases suggesting overly aggressive safety tuning, token length drift indicating users changing behavior. It bridges infrastructure and quality.

**Layer 3: Quality.** This measures whether outputs meet defined standards. Relevance scores, factual accuracy rates, hallucination detection, coherence evaluation, instruction-following adherence, consistency across repeated inputs. This layer requires content evaluation. It cannot be measured with infrastructure metrics. It detects the failures that matter most to users: the model stopped being helpful, the model started making things up, the model stopped following instructions. This is the core of AI-specific monitoring.

**Layer 4: Safety.** This measures whether outputs violate policies or create harm. Toxicity scores, PII leakage detection, policy violation rates, adversarial success rates, jailbreak attempts, prompt injection detection. This layer is defense-focused. It assumes adversaries are probing the system and users will inadvertently trigger edge cases. It detects failures that create legal, reputational, or physical risk. Safety metrics lag quality metrics because they measure tail events, not typical performance.

**Layer 5: Business Outcomes.** This measures whether the AI is delivering value. Task completion rates, user satisfaction scores, conversation abandonment, feature adoption, conversion impact, retention impact, support ticket deflection. This layer is the ultimate validation. A model can have perfect quality metrics and fail to deliver business value if it is solving the wrong problem. Business metrics are lagging indicators — they tell you what already happened — but they close the loop between technical quality and product success.

Most organizations have layer one. Some have parts of layer two. Few have systematic layer three. Almost none have mature layers four and five. The result is that they detect outages quickly, detect operational anomalies slowly, detect quality failures late, detect safety failures after user reports, and detect business impact failures when leadership asks why the AI is not delivering ROI.

## Layer 1: Infrastructure Metrics You Already Have

Infrastructure monitoring is the most mature layer because it is identical to traditional software monitoring. You need uptime tracking, latency percentiles, error rate monitoring, and throughput measurement. These are table stakes. If your model API is returning errors, you need to know immediately. If latency spikes to 8 seconds, users are experiencing a broken product. If your provider is rate-limiting your account, requests are failing.

The key infrastructure metrics for AI systems are the same as for any API-driven service. Response time measured at p50, p95, and p99 percentiles. Error rate as a percentage of total requests. Throughput as requests per second. Availability as uptime percentage over time windows. These metrics catch outages, provider issues, network failures, and capacity problems.

The mistake is treating these as sufficient. They are necessary. They catch one category of failure. They do not catch the category that matters most for AI reliability: intelligence degradation. A model that is up 99.99% of the time but produces garbage 15% of the time is not reliable. Infrastructure metrics will show green. Users will experience failure.

## Layer 2: Model Operations Metrics That Bridge to Quality

Model operations metrics are the bridge between infrastructure health and quality measurement. They track how the model is being used, what it is being asked to do, and whether operational patterns are shifting in ways that indicate problems.

Token consumption is the most basic operational metric. Total tokens per request, prompt tokens, completion tokens, tokens per second. Tracking token consumption detects prompt injection attacks — sudden spikes in input tokens — and generation runaway — sudden spikes in output tokens. It also detects cost anomalies. A model that suddenly starts generating 3x the tokens it used to is either being used differently or malfunctioning.

Refusal rate is the percentage of requests where the model refuses to answer. Refusals happen when the model determines the request is unsafe, out of scope, or policy-violating. A baseline refusal rate is normal. A spike in refusals indicates either an attack — adversaries probing for weaknesses — or a safety tuning problem — the model became overly cautious. A drop in refusals can indicate a safety regression — the model stopped catching violations it used to block.

Prompt length distribution and completion length distribution show how users interact with the system. If prompt lengths suddenly increase, users might be trying more complex queries or attempting prompt injection. If completion lengths drop, the model might be generating shorter, lower-quality responses. If completion lengths spike, the model might be rambling or failing to converge. Distribution shifts indicate behavior changes that often precede quality problems.

Model version and provider API tracking are operational hygiene. You must know which model version is serving traffic and which provider you are using. When quality degrades, the first question is whether anything changed. If you cannot answer "which model version was running when the failure started," you cannot root-cause the issue. Version tracking also enables rapid rollback when a new version underperforms.

Temperature and sampling parameters are configuration metrics. If temperature drifts due to a configuration bug, consistency will degrade. If top-p changes unexpectedly, response quality shifts. These are operational parameters that directly affect quality. They belong in monitoring because configuration drift is a common root cause of silent failures.

## Layer 3: Quality Metrics That Measure What Users Care About

Quality metrics are the core of AI observability. This is where you measure whether the model is actually doing its job. Infrastructure tells you the model is running. Operations tell you the model is being used within normal bounds. Quality tells you the model is producing good outputs.

Relevance scoring measures whether the response is on-topic. This is detectable with semantic similarity: embed the user's query, embed the model's response, measure cosine similarity. If similarity is high, the response is likely relevant. If similarity is low, the model went off-topic. Relevance can also be measured with LLM-as-judge: another model evaluates whether the response addresses the query. Relevance degradation is one of the most common quality failures and one of the easiest to detect automatically.

Factual accuracy measures whether the response is correct. This is hard to automate because correctness is domain-specific. For structured tasks — math, code generation, data extraction — you can run test cases and check outputs. For unstructured tasks — summarization, question answering, advice — you need external validation. LLM-as-judge can estimate accuracy by checking claims against retrieved sources. Human review can sample outputs and label correctness. Accuracy is expensive to measure but critical for high-stakes use cases.

Hallucination detection measures whether the model is making things up. This is distinct from accuracy. A model can be inaccurate by omission or by fabrication. Hallucinations are fabrications: the model generates information that is not supported by sources, contradicts known facts, or is internally inconsistent. Detection methods include citation checking — does the claim match the retrieved document — and self-consistency checking — does the model give the same answer when asked again with different phrasing. Hallucination rate is the most watched quality metric for knowledge-intensive applications.

Instruction-following measures whether the model does what the prompt asks. This is detectable with classifiers or LLM-as-judge. If the user asks for a bulleted list, does the response use bullets? If the user asks for a formal tone, is the response formal? If the user asks for code in Python, is the response Python code? Instruction-following degradation often indicates prompt engineering issues or model version regressions.

Coherence and fluency measure whether the response is well-formed. Coherence means the ideas connect logically. Fluency means the language is grammatical and natural. These are measurable with perplexity scores, readability metrics, or LLM-as-judge evaluation. Coherence failures are less common than relevance or accuracy failures, but they are highly visible when they occur. A model that produces word salad is immediately obvious to users.

Consistency measures whether the model gives the same answer for the same question. This requires repeated sampling: send the same input multiple times, compare outputs, measure agreement. High consistency is expected for factual queries. Low consistency might be acceptable for creative tasks. Consistency degradation often indicates infrastructure issues — different model replicas giving different answers — or configuration issues — temperature settings changing unexpectedly.

## Layer 4: Safety Metrics That Catch Tail Risks

Safety metrics measure whether the model violates policies or creates harm. These are lower-frequency events than quality failures. Most responses are safe. But the unsafe responses in the tail can destroy a product. Safety monitoring is about catching the 0.5% of outputs that are catastrophically bad, not measuring the average performance.

Policy violation rate is the percentage of responses that violate defined content policies. Policies cover toxicity, hate speech, harassment, violence, illegal content, misinformation, and domain-specific rules. Detection requires classifiers trained on policy definitions. The violation rate should be near zero. Any sustained increase indicates a safety regression or an adversarial attack. This metric is non-negotiable for consumer-facing products.

PII leakage detection measures whether the model exposes personal information. This happens when the model is trained on data containing PII, when users input PII in prompts, or when the model hallucinates realistic-sounding PII. Detection uses pattern matching — social security numbers, credit card numbers, email addresses — and classifier models trained to recognize PII. Even a single PII leak can create legal liability. This metric must be continuously monitored for any system handling user data.

Adversarial success rate measures how often red-team attacks succeed. This is proactive safety monitoring. Security teams or automated systems send adversarial prompts designed to jailbreak the model, extract training data, generate harmful content, or bypass safety filters. The success rate should be low and stable. Increases indicate safety regression or emerging attack vectors. This metric requires maintaining a library of adversarial test cases and running them continuously.

Jailbreak detection measures whether users are attempting to bypass safety controls. This includes prompt injection, role-playing attacks, encoding tricks, and multi-turn manipulation. Detection uses pattern matching on user inputs and behavioral analysis. High jailbreak attempt rates indicate adversarial users or public disclosure of an exploit. Even if the jailbreaks are unsuccessful, the attempts signal that your safety perimeter is being tested.

Toxicity and harmfulness scores measure the content of responses on continuous scales. A toxicity classifier returns a score from 0 to 1 indicating how toxic the response is. These scores are noisier than binary policy violation flags, but they provide earlier warning. A model that is not yet violating policies but is trending toward more toxic language is degrading. The trend is detectable before it crosses a hard threshold.

## Layer 5: Business Metrics That Validate Value

Business metrics measure whether the AI is achieving its purpose. Technical quality is necessary but not sufficient. A model can be highly accurate, safe, and consistent while failing to deliver business outcomes if it is solving the wrong problem or if users do not trust it.

Task completion rate measures whether users accomplish what they came to do. For a customer service bot, this is the percentage of conversations where the user's issue is resolved without escalation. For a code assistant, this is the percentage of suggestions the user accepts. For a search tool, this is the percentage of queries where the user clicks a result. Task completion is the most direct measure of utility. If completion rates drop, the model is not helping users even if quality metrics look fine.

User satisfaction is measurable through explicit feedback — thumbs up, thumbs down, star ratings — or implicit signals — conversation length, feature re-use, time spent. Satisfaction is subjective and lagging, but it correlates with retention. Users who are satisfied continue using the product. Users who are dissatisfied churn. Satisfaction scores that decline over time indicate quality degradation that users are experiencing but metrics might not be catching.

Conversation abandonment measures how often users quit mid-conversation. High abandonment rates indicate the model is not providing value. Users who leave after two turns did not get what they needed. Abandonment rate is a leading indicator of dissatisfaction. It is detectable in real-time, unlike retention which is a lagging metric.

Conversion impact and retention impact measure whether the AI drives business outcomes. Does the recommendation engine increase purchases? Does the assistant reduce churn? Does the chatbot improve conversion rates? These are A-B testable. The AI should outperform the non-AI baseline on metrics that matter to the business. If it does not, the technical quality is irrelevant.

Support ticket volume and type distribution measure whether the AI is reducing operational load or increasing it. A customer service bot should deflect tickets from human agents. If ticket volume increases after deploying the AI, the bot is creating more problems than it solves. If ticket types shift toward "the AI gave me wrong information," the model is causing harm. Support volume is a lagging indicator but a clear signal of product-market fit or lack thereof.

## How the Layers Work Together

The layers are not independent. They interact. Infrastructure failures cause operational anomalies which cause quality degradation which cause safety incidents which hurt business metrics. The stack is causal. Monitoring the entire stack allows you to detect failures at the earliest possible layer.

A provider outage appears in layer one as increased error rates. It appears in layer two as dropped requests and spiking retry attempts. It appears in layer three as relevance degradation if the system falls back to a weaker model. It appears in layer four if the fallback model has weaker safety tuning. It appears in layer five as increased abandonment and decreased satisfaction. If you only monitor layer five, you learn about the outage when users start churning. If you monitor layer one, you detect it immediately.

A prompt change appears in layer two as shifted token distributions. It appears in layer three as relevance or accuracy changes. It appears in layer four if the new prompt weakens safety. It appears in layer five as changed task completion or satisfaction. Monitoring layer two catches it before users notice. Monitoring only layer five means you detect it after user impact.

A model version update appears in layer two as a version number change. It appears in layer three as quality metric shifts. It appears in layer four as safety regression. It appears in layer five as satisfaction decline. The earlier layer catches it faster, giving you more time to roll back before damage compounds.

## The Instrumentation Challenge

Building this stack is expensive. Layer one is cheap — traditional APM is a solved problem. Layer two is moderate — token tracking and operational metrics require custom instrumentation but are straightforward. Layer three is expensive — quality metrics require continuous evaluation, which requires compute, models, and annotation. Layer four is expensive — safety metrics require adversarial testing, classifiers, and human review. Layer five is moderate — business metrics are usually already tracked for product analytics.

The cost is front-loaded. You build the instrumentation once, then it runs continuously. The value is ongoing. Every failure you catch before users experience it pays back the instrumentation cost. Teams that underinvest in layers three and four detect failures late, respond slowly, and lose user trust. Teams that build the full stack catch regressions early, contain them quickly, and maintain reliability.

The next question is how to implement layer three in production without adding unacceptable latency or cost. That requires designing real-time quality detectors.

---

Next: **2.3 — Building Real-Time Quality Detectors**
