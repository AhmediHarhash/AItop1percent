# 11.5 — Vendor Escalation: Working with Provider Support During Outages

At 1:47pm on a Tuesday, every request to OpenAI started timing out. The engineering team checked their code — no recent changes. Checked their infrastructure — all green. Checked the OpenAI status page — showed all systems operational. They opened a support ticket. Standard priority. "Experiencing timeouts on GPT-5-mini, started 1:47pm UTC, error rate 100%." Three hours later, they received an automated response asking for more details. By then, they had burned through their fallback budget on Anthropic, their customer support queue had 400 tickets waiting, and the status page still showed green. At 6:30pm, OpenAI updated the status page — there had been an outage in us-east affecting GPT-5-mini from 1:45pm to 5:20pm. The engineering team learned about the resolution from Twitter, not from their support ticket.

When your AI provider is the problem, you are not in control of the fix. You cannot redeploy. You cannot roll back. You cannot debug. Your job is to escalate effectively, communicate clearly, and make decisions about what to do while you wait. Most organizations discover during their first major provider outage that they do not know how to escalate, do not have the right support tier, and do not have relationships with anyone who can help. By the time they figure it out, the outage is over.

## Recognizing Provider-Side Issues

The first challenge is distinguishing between your problem and the provider's problem. AI provider outages often look like your problem first — increased latency, elevated error rates, timeouts, or degraded quality. You check your own systems, find nothing wrong, and waste time investigating internal causes before realizing the provider is down.

The signals that point to provider-side issues: **simultaneous degradation across all models from the same provider**, even if you have not changed anything. **Error patterns that do not correlate with your traffic patterns** — you are sending the same requests you always send, but suddenly they are failing. **Discrepancies between the provider's status page and observed behavior** — the status page shows green, but your metrics show red. **Reports from other users on social media or community forums** — someone else is seeing the same problem at the same time.

Check multiple data points before escalating. Look at error logs for provider-specific error messages. Check if the issue is isolated to one model or all models. Check if the issue started at a time boundary — top of the hour, specific minute — which often indicates a provider-side deploy or configuration change. Gather enough evidence to know you are not wasting the provider's time with a problem that is actually yours.

## Provider Support Tiers and Response Times

Most AI providers offer tiered support. Standard support, which is free, gets you access to a ticketing system with response times measured in hours or days. Premium support, which costs thousands of dollars per month, gets you faster response times and access to higher-level engineers. Enterprise support, which is negotiated as part of large contracts, gets you dedicated support contacts, direct escalation paths, and sometimes joint incident response.

If you are running production AI systems at scale, standard support is not sufficient. During an outage, a response time of 24 hours is meaningless. You need a support tier that promises response within 30 minutes for critical issues. You need access to engineers who can check internal systems, not tier-one support reading from a script.

Many organizations run on standard support because they do not realize how bad it is until they need it. They assume provider reliability is high enough that outages will be rare, so they do not pay for premium support. Then an outage happens, they cannot get help, and they realize the cost of premium support is trivial compared to the cost of being down for hours with no communication.

Check your support tier now. If you are on standard support and you are running production AI systems, upgrade. The time to establish a better support relationship is not during an outage.

## Escalation Contacts and Procedures

Every AI provider has an escalation path. For critical incidents, there is usually a separate contact mechanism — a phone number, a high-priority ticket type, a Slack channel for enterprise customers, or a direct contact for dedicated account teams. Find out what yours is before you need it.

If you have an account manager or a customer success contact, ask them how to escalate during a production incident. Get the phone numbers, get the email addresses, get the escalation procedure documented. If you are on enterprise support, ask for a direct contact who can pull in engineering during outages. Get that person's name, their Slack handle if you share a Slack Connect channel, and their coverage hours.

Some providers have a concept of a Technical Account Manager or TAM — a person who knows your architecture, understands your use case, and can be your advocate inside the provider during incidents. If you can get one, do it. They are the difference between filing a ticket that disappears into a queue and having someone internally who can check on your issue and escalate if needed.

Document your escalation procedure in your runbooks. Not just the contact information — the exact steps someone should take when they suspect a provider outage. "Check status page. Check community forums. Check these three metrics. If all indicate provider issue, escalate using this contact, include this information." During an incident, no one should be searching Slack for the escalation email address or trying to remember whether you have a TAM.

## What Information Providers Need

When you escalate to a provider, you need to provide specific information that helps them identify the problem fast. Generic reports like "your API is slow" or "we are seeing errors" do not give them enough to work with.

Include the **time range** when the issue started and whether it is ongoing. Include the **specific models and endpoints** affected — GPT-5-mini text completions, Claude Opus 4.5 on the messages API, Gemini 3 Pro embeddings. Include **error rates** — percentage of requests failing, absolute number of failures, whether the rate is increasing or steady. Include **specific error messages** — the exact HTTP status codes, the error text returned, any error IDs or request IDs from responses. Include **request IDs** from failed attempts so the provider can look up those specific requests in their logs.

Include what you have already ruled out. "We checked our infrastructure, no changes in the last 48 hours, no elevated load, issue affects requests from multiple regions." This tells the provider you have done your homework and are not sending them a problem that is actually yours.

The faster you provide detailed information, the faster the provider can diagnose and respond. The worst escalations are vague, include no data, and require the provider to send follow-up questions before they can even start investigating.

## Joint Troubleshooting Approaches

During a provider incident, you are often troubleshooting jointly — you provide information about what you are seeing, the provider checks their systems, you try changes they suggest, and you report back. This loop needs to be fast.

If the provider asks you to try a specific change — switch regions, switch models, adjust request parameters — do it immediately and report the results. If they ask for additional data — logs from a specific time window, examples of requests that failed, metrics from a particular endpoint — provide it as fast as you can. The back-and-forth should be measured in minutes, not hours.

Some providers can see your request logs on their side. If they can, give them permission to look. This eliminates the need for you to export logs and send them over. They can see exactly what you sent, what they returned, and what their internal systems were doing at the time.

During long incidents, maintain a shared document or incident channel where the provider can see your status updates and you can see theirs. This prevents duplication, keeps everyone aligned, and ensures no one is waiting on information that has already been shared.

## When Provider Response Is Inadequate

Sometimes the provider response is too slow, too generic, or does not acknowledge the severity of the issue. You escalate with detailed information, and you get a template response asking for information you already provided. Or you escalate a critical production issue, and the ticket sits in a queue for hours.

Escalate higher. If you have an account manager, contact them directly. If you have a TAM, call them. If you have a Slack Connect channel with the provider, post in it. If you know anyone at the provider company — a solutions architect, a sales engineer, someone you met at a conference — reach out.

If you have a large contract, mention it. Not as a threat — as context. "We are a GPT-5 enterprise customer with over 20 million API calls per month, this outage is affecting our production customer-facing systems." Providers prioritize based on customer size and severity. If you are a significant customer and this is a critical incident, they need to know.

If none of that works, document everything. The ticket ID, the time you escalated, what information you provided, what response you received, how long you waited. After the incident, follow up with your account team and explain that the support response was not adequate for a production incident. Ask what should have been different. Ask for a commitment to faster response next time. If you are paying for premium support and you are not getting premium response, that is a contract issue.

## Post-Incident Provider Coordination

After a provider incident, request a post-incident review. Most providers will provide one if you ask, especially if you are on premium or enterprise support. The post-incident review should cover what happened on the provider side, why monitoring did not catch it earlier, why the status page was not updated, and what they are doing to prevent recurrence.

Use the post-incident review to improve your own response. Did you escalate fast enough? Did you have the right contact information? Did the provider get the information they needed from you quickly? Identify gaps and fix them.

If the provider incident exposed weaknesses in your architecture — over-reliance on a single provider, no fallback strategy, inadequate monitoring of provider health — fix those too. You cannot control provider outages, but you can control how much they affect you.

## Building Provider Relationships Before Incidents

The best time to establish a relationship with your AI provider is not during an outage. Organizations that handle provider incidents well have invested in the relationship during normal operations.

Attend the provider's office hours or technical workshops. Engage with their solutions architects. Join their Slack communities or customer forums. When you have questions about best practices, ask them. When you hit scaling challenges, tell them. The goal is not to become friends — the goal is to be a customer they recognize, whose use case they understand, and who they want to help when things break.

If you are a large enough customer, ask for a quarterly business review or technical review where you share how you are using their platform, what challenges you are facing, and what improvements you need. This puts you on their radar as a sophisticated user who takes the relationship seriously.

When an outage happens, you are not a stranger filing a ticket. You are a customer they have talked to, whose architecture they understand, and whose escalation they take seriously. That difference matters when the clock is running.

---

Next, you will learn about follow-the-sun models for global AI systems — how teams spanning multiple time zones can provide 24/7 coverage without requiring anyone to be on call overnight.
