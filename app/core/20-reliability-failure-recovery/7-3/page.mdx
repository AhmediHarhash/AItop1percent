# 7.3 — Agent State Corruption: When Memory and Context Go Wrong

The team responsible for a financial planning agent noticed something strange in early January 2026. The agent, which had been performing well for months, started giving answers that were subtly wrong — not hallucinated nonsense, but responses that reflected outdated tax law information from 2024 instead of current 2026 rules. The agent's memory had been poisoned during a failure three weeks earlier. During that failure, the agent had interacted with a batch of queries that referenced old documents while the retrieval system was degraded and returning incorrect results. The agent processed those bad results, updated its learned memory to reflect the incorrect information, and persisted the corrupted memory to its long-term store. When the retrieval system recovered, the agent continued confidently citing 2024 tax rules because its memory said those were correct. The team discovered the problem only when an accountant user flagged multiple incorrect answers in a single day. By that point, the agent had given wrong advice to 1,200 users over three weeks. The recovery process required identifying all corrupted memory entries, purging them, and re-validating the agent's learned behaviors against known-good data. It took nine days to complete.

State loss is a visible problem. The system forgets, users notice immediately, and recovery is a matter of restoring what was lost. State corruption is insidious. The system remembers, but what it remembers is wrong. It operates confidently based on corrupted memory, bad context, or poisoned learned behaviors. Users do not notice immediately because the agent appears functional. The corruption surfaces gradually as the agent's answers drift from correct to subtly incorrect to obviously wrong. By the time the corruption is detected, it has often spread — the agent used corrupted memory to inform new decisions, which generated new corrupted memory, which informed further decisions. Recovery is not just restoring lost state. It is identifying corrupted state, understanding how it became corrupted, tracing its downstream effects, and repairing or purging the damage.

## Types of Agent State Corruption

Agent state corruption falls into several categories, each with different causes and recovery patterns. Memory corruption is the most common. The agent's memory store — the record of facts it has learned, preferences it has inferred, or summaries it has created — contains incorrect information. This happens when the agent processes bad data during a failure and persists the result, when a bug in the memory update logic writes malformed data, or when external systems provide inconsistent information and the agent does not detect the inconsistency.

Context window corruption is more transient but equally damaging. The agent's context window — the information it considers when making a decision — contains stale, irrelevant, or incorrect data. This happens when the context assembly process retrieves the wrong documents, when the ranking algorithm surface outdated results, or when the agent's memory of prior turns in the conversation is incomplete and it fills gaps with plausible but incorrect inferences.

Learned behavior drift is the most difficult to detect. The agent adapts its behavior based on feedback, and over time that feedback pushes the agent toward incorrect behaviors. This happens when users provide feedback that is locally correct but globally inconsistent, when automated feedback loops reinforce bad patterns, or when adversarial users deliberately train the agent to misbehave. The agent is not hallucinating. It is following learned behaviors that are wrong.

Tool invocation corruption happens when the agent's memory of how to use external tools — APIs, databases, search engines — becomes incorrect. The agent remembers that calling a specific API with specific parameters worked in the past, but the API has changed and those parameters are no longer valid. The agent retries the old pattern, receives errors, and updates its memory based on the error responses. Over time, its understanding of the tool drifts from reality.

## Memory Poisoning from Failed Interactions

Memory poisoning is the most common corruption scenario. It begins with a failure that causes the agent to process incorrect information — a retrieval system returning wrong documents, a tool returning error messages that the agent interprets as factual responses, a user providing information that is incorrect but confidently stated. The agent processes the bad input, extracts what it believes are facts, and writes those facts to its long-term memory. The memory is now poisoned.

The poisoning spreads because the agent trusts its own memory. In future interactions, the agent retrieves the poisoned memory, uses it to inform its responses, and reinforces the incorrect information. If the agent uses the poisoned memory to process new inputs, it may generate additional incorrect facts that also get persisted. The corruption grows.

A classic example: a customer support agent interacts with a user asking about return policy during a database failure. The policy database is unreachable, so the agent's retrieval step fails silently. The agent, trained to always provide an answer, guesses based on vague prior knowledge and says "returns are accepted within 14 days." The actual policy is 30 days. The agent writes "14-day return policy" into its memory. For the next month, every user asking about returns is told 14 days. Some users miss the real deadline because they trusted the agent.

Memory poisoning detection requires comparing agent memory against ground truth. If the agent has learned that a fact is true, but the authoritative source says otherwise, the memory is poisoned. The challenge is that agents often do not have a single authoritative source. They learn from many interactions, many documents, many users. Determining which learned facts are poisoned requires either manual review or automated consistency checking against high-confidence sources.

## Context Window Corruption

Context window corruption is more transient than memory corruption, but it causes immediate incorrect behavior. The agent assembles context for each decision by retrieving relevant information from memory, from external documents, from prior conversation turns. If the retrieval process surfaces the wrong information, the context is corrupted and the agent makes decisions based on bad inputs.

This happens frequently during recovery from retrieval system failures. The retrieval index might be stale, returning documents from before a recent update. The ranking algorithm might be degraded, promoting irrelevant results above relevant ones. The embedding model might have changed, causing semantic drift in what "relevant" means. The agent receives these results, assumes they are correct, and proceeds.

A financial advice agent during a database lag retrieves documents from six months ago when queried about current market conditions. The agent synthesizes an answer based on outdated data and confidently cites six-month-old stock prices as if they are current. The user makes a decision based on stale information. The agent's memory is fine. The context it assembled for this specific interaction was corrupted.

Context corruption is harder to detect than memory corruption because it is ephemeral. The corrupted context is not persisted — it only exists for the duration of the interaction. Detection requires monitoring what documents the agent retrieves, validating that those documents are current and relevant, and alerting when retrieval returns results that fail validation. This monitoring must happen in real-time because by the time the user reports an incorrect answer, the corrupted context is gone.

## Learned Behavior Drift from Bad Feedback

Agents that learn from user feedback or automated quality signals are vulnerable to behavior drift. The agent adjusts its behavior based on what gets positive feedback and what gets negative feedback. If the feedback is consistently accurate, the agent improves. If the feedback is noisy, inconsistent, or adversarial, the agent drifts toward incorrect behaviors.

A common failure mode is feedback that is locally correct but globally inconsistent. A conversational agent learns that users prefer concise answers. It receives positive feedback when it gives short responses and negative feedback when it gives long ones. Over time, it learns to be maximally concise — answering complex questions with one-sentence responses that omit critical nuance. Each individual piece of feedback was correct — users did prefer the shorter answer in that moment. But the aggregate effect is that the agent now underfits, providing answers that are technically accurate but usefully incomplete.

Another failure mode is automated feedback loops that reinforce bad patterns. An agent uses engagement metrics — did the user click, did they follow-up, did they rate the response highly — as feedback signals. It learns that certain response styles get high engagement. But high engagement does not always mean high quality. An agent might learn that responses with bold claims and dramatic language get more engagement than nuanced, careful responses. It drifts toward sensationalism because that is what the feedback loop rewards.

Adversarial feedback is the most dangerous. A small number of users deliberately give feedback intended to corrupt the agent's behavior. They provide positive ratings for incorrect answers and negative ratings for correct ones. If the agent's learning rate is high enough, adversarial users can shift its behavior over time. A moderation agent might learn to approve content it should reject because adversarial users systematically provide positive feedback on rule violations.

Detecting behavior drift requires comparing the agent's current behavior against a baseline. If the agent's response style, tone, accuracy, or decision patterns diverge significantly from the baseline, drift is likely. The challenge is distinguishing drift from legitimate adaptation. The agent is supposed to learn. The question is whether it is learning the right lessons.

## Detection Methods for Corrupted State

Detecting corrupted state requires both automated monitoring and manual review. Automated detection uses consistency checks, comparison against ground truth, anomaly detection, and feedback analysis. Manual review uses spot-checking, user reports, and expert audit.

Consistency checks validate that the agent's state is internally consistent. If memory says the user provided their email address, but the extracted email field is empty, something is wrong. If the task plan says step three completed successfully, but the side effects that should result from step three are not present, the state is inconsistent. Consistency checks catch corruption caused by partial writes, race conditions, and bugs in state update logic.

Comparison against ground truth validates that learned facts match authoritative sources. If the agent has learned that the return policy is 14 days, but the policy document says 30 days, the agent's memory is wrong. This requires maintaining a set of ground truth facts and periodically validating agent memory against them. The challenge is scope — agents learn thousands of facts, and not all of them have a single authoritative source.

Anomaly detection identifies state that is statistically unusual. If the agent's memory of user preferences suddenly shifts dramatically, if its tool invocation patterns change without a corresponding change in the task distribution, or if its context retrieval starts returning systematically different documents, an anomaly exists. Anomalies are not always corruption, but they are always worth investigating.

Feedback analysis tracks whether the agent's performance on known-good test cases is stable over time. If accuracy on a fixed eval set degrades, the agent's state has drifted. If the agent starts failing test cases it previously passed, its learned behaviors have changed in a problematic way.

Manual review is necessary for subtle corruption that automated systems miss. Domain experts spot-check the agent's responses, review recent memory updates, and validate that learned behaviors align with expectations. User reports are a critical signal — users notice when the agent gives wrong answers, and those reports often identify corruption before automated systems do.

## State Reset Versus State Repair

When corrupted state is detected, the team must decide whether to reset or repair. State reset means purging all learned memory and reverting the agent to its initial state. This is the simplest and safest approach, but it throws away all legitimate learning. The agent loses everything it learned that was correct along with everything it learned that was wrong. For agents that have been learning for months, reset is a significant loss.

State repair means identifying the specific corrupted entries and purging or correcting them while preserving the rest. This is more complex and requires understanding what is corrupted and what is not. If the corruption is localized — a specific set of memory entries written during a specific time window — repair is feasible. If the corruption has spread through cascading effects, repair may be impossible without extensive manual review.

The decision between reset and repair depends on the scope of corruption, the value of the legitimate learned state, and the cost of repair. If corruption is widespread and repair would take longer than re-learning from scratch, reset is the right choice. If corruption is narrow and the agent has learned valuable behaviors that would take weeks to re-learn, repair is worth the effort.

Partial reset is a middle ground. The team identifies which domains of memory are corrupted and resets those domains while preserving others. An agent that learned customer preferences correctly but learned product information incorrectly resets its product knowledge but keeps its preference models. This requires the agent's memory to be structured in a way that allows domain-specific reset, which is not always the case.

## Quarantine Procedures for Corrupted Agents

When corruption is detected but not yet understood, the safest approach is to quarantine the agent. Quarantine means removing the agent from production traffic while the corruption is investigated and repaired. Users are routed to a backup agent, a fallback system, or a human operator. The corrupted agent continues to exist, but it does not interact with real users and cannot cause further damage.

Quarantine is appropriate when the scope of corruption is unknown, when the agent's responses are unpredictable, or when the risk of wrong answers is high. A financial advice agent with suspected memory corruption should be quarantined immediately because incorrect advice has serious consequences. A low-stakes conversational agent might remain in production with elevated monitoring while repair is attempted.

Quarantine procedures must be automated and fast. The decision to quarantine should not require a committee meeting or a VP approval. The on-call engineer sees evidence of corruption, follows a runbook, and triggers quarantine. The system automatically shifts traffic away from the corrupted agent within seconds. The investigation proceeds without user impact.

Restoring a quarantined agent to production requires validation. The agent is tested against known-good eval sets, its memory is reviewed by domain experts, and its behavior is compared to baseline. It is not restored just because the corruption was repaired. It is restored when the team is confident that repair was successful and no residual corruption remains.

## Prevention of Future State Corruption

Preventing state corruption is more effective than detecting and repairing it. Prevention strategies include input validation, write verification, state immutability, and memory versioning. Input validation ensures that data written to memory is plausible and consistent before it is persisted. If the agent attempts to write a fact that contradicts existing high-confidence memory, the write is rejected or flagged for review. If the agent attempts to write malformed data, the write fails before it corrupts the store.

Write verification checks that the data written to memory matches the data the agent intended to write. This catches bugs in serialization, database driver issues, and race conditions that corrupt writes. The agent writes data, reads it back, and verifies the read matches the write. If verification fails, the write is rolled back and an alert is raised.

State immutability prevents corruption by making memory append-only. Instead of updating a fact in place, the agent writes a new version of the fact with a timestamp. The agent always uses the most recent version, but prior versions remain accessible. If a corruption is detected, the team can identify when the incorrect version was written, roll back to the prior version, and investigate what caused the bad write. This does not prevent corruption, but it makes recovery much easier.

Memory versioning extends immutability with explicit version numbers. Each memory update increments a version counter. If the agent's behavior degrades, the team can roll back memory to a prior version where behavior was correct. This is similar to version control for code — every change is tracked, and any change can be reverted.

Agent state corruption is the dark side of agent adaptability. Agents learn, remember, and adapt, which makes them powerful. But when they learn the wrong lessons, remember incorrect facts, or adapt based on corrupted feedback, they become unreliable in ways that are difficult to detect and expensive to fix. The teams that build robust agent systems design for corruption from the start — they validate inputs, version memory, quarantine on suspicion, and repair with discipline. The teams that treat memory as an implementation detail discover corruption only when users lose trust.

Next, we examine the challenge of recovering from agent actions that have already been taken — the problem of undoing side effects in external systems after a failure.
