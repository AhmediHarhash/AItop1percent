# 5.7 — Redundant Failure Modes: When Both Providers Share Upstream Risk

A financial services company built what they believed was bulletproof multi-provider redundancy. Primary: Anthropic Claude running on AWS us-east-1. Fallback: OpenAI GPT-5 running on Azure East US. Two different providers. Two different clouds. Complete independence—or so they thought. On March 12th, 2025, a fiber cut in Virginia disrupted connectivity to multiple data centers. Both AWS us-east-1 and Azure East US lost connectivity for 90 minutes. Primary and fallback failed simultaneously. The company discovered during the incident that both regions relied on the same network backbone provider. Their "independent" providers shared critical infrastructure three layers down. The redundancy they paid for disappeared the moment they needed it most.

True redundancy requires independent failure modes. If two systems can fail for the same reason, they are not truly redundant.

## The Illusion of Independence

Multi-provider redundancy feels comprehensive. You are not locked into a single vendor. You have negotiated contracts with two or three providers. You have built integrations for each. You have tested failover between them. The architecture diagrams show separate paths. The procurement process congratulated you on avoiding vendor lock-in. Then a correlated failure happens and you discover the independence was surface-level.

The most common false independence: cloud region overlap. Your primary provider runs in AWS us-east-1. Your backup runs in GCP us-east1. These are marketed as different clouds, but they often share physical data center facilities or network interconnects in Northern Virginia. A regional power failure, a natural disaster, or a fiber cut can affect both simultaneously. The cloud brands are different. The physical infrastructure overlaps.

Another common false independence: shared upstream APIs. Your primary model uses one provider. Your fallback uses another. Both providers call the same safety API to screen outputs, the same embedding service to encode queries, or the same content moderation endpoint to filter responses. If that shared dependency fails, both providers degrade or fail together. The model providers are independent. The dependencies are not.

DNS is a subtle shared failure mode. Both your primary and fallback paths depend on DNS resolution. If your DNS provider experiences an outage—or if an upstream DNS resolver fails—neither provider is reachable. The systems themselves are healthy. The naming system that connects clients to them is broken. You need DNS redundancy, not just model provider redundancy.

Authentication services create shared failure modes. If both providers authenticate through your corporate SSO, and that SSO fails, neither provider is accessible. If both providers depend on OAuth tokens issued by a single identity provider, and that provider has an outage, your API keys stop working. Authentication is infrastructure, and infrastructure can fail.

## Mapping Dependency Graphs to Find Correlated Risk

The way to identify shared failure modes is to map the full dependency graph for each provider path. Start at the user request. Trace every component the request touches on its way to a response. Document each dependency: cloud infrastructure, network paths, DNS, authentication, rate limiting, load balancers, API gateways, retrieval systems, databases, caches, monitoring, logging. Then overlay the primary and fallback paths. Any component that appears in both paths is a shared failure mode.

A thorough dependency audit for a typical AI system reveals dozens of shared components. Both paths use the same Kubernetes cluster. Both use the same Postgres database for request logging. Both call the same Redis cache for session management. Both write to the same S3 bucket for audit logs. Both depend on the same internal network routing. Each of these is a potential single point of failure that eliminates the value of multi-provider redundancy.

Some shared dependencies are acceptable risks. If your internal logging system fails, your AI providers can still serve requests—you just lose observability. That is a degraded state but not a total outage. Other shared dependencies are unacceptable. If your authentication system fails and neither provider can validate API keys, your entire service goes dark. The audit must categorize dependencies by criticality: does failure of this component cause total unavailability, degraded service, or just lost telemetry?

The dependency graph also reveals distance from the failure. Some components are in your direct control: your application code, your Kubernetes cluster, your database. Some are in your provider's control: the model API, the provider's infrastructure. Some are in neither party's control: upstream internet routing, DNS root servers, cloud region power grids. The further a dependency is from your control, the less you can do to prevent correlated failures—but the more important it is to design for them.

## Cloud Region Geography and Physical Infrastructure

Cloud regions are marketed as independent availability zones, but geography matters. AWS us-east-1 and Azure East US are both in Virginia. They share regional power grids, weather risks, and fiber routes. An ice storm, a hurricane, or a regional power failure can affect both. A team relying on this pair for redundancy had both providers go offline during a 2025 winter storm that knocked out power to multiple Northern Virginia data centers. The cloud brands were different. The weather was the same.

True geographic redundancy requires physical distance. If your primary is in Virginia, your fallback should be in Oregon, Ireland, or Singapore—regions that do not share weather, power grids, or regional fiber routes. This introduces latency trade-offs. A fallback provider 3,000 miles away has higher baseline latency than one 50 miles away. But higher latency during failover is better than no failover at all.

Some teams use a three-region strategy: primary in one region, hot standby in a nearby region for fast failover with low latency, cold standby in a distant region for catastrophic regional failure. This triples infrastructure cost but provides both fast recovery from provider-level failures and protection from region-level disasters. The decision depends on your availability requirements and budget.

Physical infrastructure is also shared at the network level. Multiple cloud regions often connect through the same internet exchange points or fiber providers. A fiber cut at a major peering point can simultaneously disrupt connectivity to multiple clouds. In 2024, a backhoe accident in Chicago severed fiber lines affecting AWS, GCP, and Azure traffic simultaneously. Teams with multi-cloud redundancy all failed over to nothing. The solution: ensure fallback paths use different network providers or routes, not just different cloud brands.

## Shared Embedding and Safety Services

Many AI providers depend on shared upstream services. OpenAI models, Anthropic models, and Google models all integrate with common embedding providers for retrieval tasks. If you built your RAG pipeline to call a third-party embedding API, and both your primary and fallback models depend on that API, a failure in the embedding service breaks both paths.

Safety and content moderation APIs are another common shared dependency. Some providers outsource safety checks to third-party APIs. If your primary and fallback providers both call the same content filtering service, and that service goes down, both providers may refuse to return outputs—or may return unfiltered unsafe content. Either scenario is unacceptable. You need to know whether your providers share safety infrastructure and have a plan for if it fails.

The solution is to build independence into your auxiliary services. If your primary provider uses Embedding Service A and your fallback uses Embedding Service B, a failure in A does not affect B. If your primary provider uses an external safety API and your fallback uses on-device safety checks, they have independent failure modes. This requires deliberate architectural design—it does not happen by default.

Some teams build their own embedding and safety layers that sit in front of all providers. This centralizes the dependency, which means it becomes a single point of failure—but it is a single point you control. You can over-provision it, test it aggressively, and design it for reliability in ways you cannot control external services. The trade-off: you take on the operational burden of running a critical service. The benefit: you eliminate shared upstream risk from third parties.

## Correlated Failures: The Hidden Single Point

Correlated failures are the failure mode that redundancy cannot fix. You have two providers. Both are healthy. Then a systemic event affects both simultaneously. A global internet routing issue. A DNS root server failure. A payment processor outage that prevents both providers from validating your billing. A regulatory compliance change that causes both providers to shut down certain capabilities at the same time. These are low-probability, high-impact events—but they happen.

In January 2025, a widespread DDoS attack targeted major DNS providers. Multiple AI providers became unreachable not because their infrastructure failed, but because clients could not resolve their hostnames. Teams with multi-provider redundancy discovered their fallback providers were equally unreachable. The redundancy protected against provider-specific failures but not against shared DNS risk. The teams that stayed online were those with IP-based fallback routing and local DNS caching.

Another example: in mid-2025, the EU AI Act's compliance deadline caused multiple providers to simultaneously restrict certain model capabilities pending certification. Teams relying on those capabilities across multiple providers found themselves with no working path forward. The providers were operational. The feature that the system depended on was unavailable across all of them. Regulatory risk is a correlated failure mode.

The way to mitigate correlated failures is to design for degraded operation when all external providers are unavailable. This means having a final-tier fallback that depends on no external services: cached responses, rule-based systems, human escalation, or explicit error messages that preserve user trust even when you cannot serve the full feature. This tier is expensive to build and rarely invoked—but it is the difference between a total outage and a degraded service.

## The Independence Checklist

To validate true independence between providers, audit these dimensions. Infrastructure: do they run on the same cloud, the same region, the same data centers? Network: do they share fiber routes, internet exchange points, CDN providers? DNS: do they use the same DNS provider, the same upstream resolvers? Authentication: do they depend on the same identity provider, the same OAuth issuer? Upstream services: do they call the same embedding APIs, the same safety services, the same logging infrastructure? Regulatory: are they subject to the same jurisdictional rules, the same compliance deadlines?

If the answer to any of these is yes, you have shared failure modes. That does not mean your redundancy is useless—it means your redundancy protects against provider-specific failures but not against failures of the shared component. You need to decide whether the shared risk is acceptable or whether you need to invest in further independence.

For critical systems, true independence requires designing every layer of the stack for redundancy: multiple clouds, multiple regions, multiple network paths, multiple DNS providers, multiple authentication systems, multiple embedding services. This is expensive, complex, and operationally demanding. But for systems where downtime has catastrophic consequences—financial trading, medical diagnosis, safety-critical infrastructure—the investment is justified. For less critical systems, accepting some shared failure modes is a pragmatic trade-off.

## When Shared Risk Is Acceptable

Not every shared failure mode requires mitigation. Some dependencies are so reliable that shared risk is acceptable. If both your providers depend on the internet existing, that is technically a shared dependency—but the internet's reliability is high enough that you do not need to mitigate it. If both providers depend on TCP/IP, that is shared risk you accept.

The question is: what is the failure rate of the shared component, and what is the cost of mitigating it? If a shared component fails once every ten years and mitigating it requires doubling your infrastructure cost, you may accept the risk. If it fails once a quarter and mitigation is straightforward, you mitigate.

Some teams use a risk matrix: plot the likelihood of failure for each shared component against the impact of that failure. High-likelihood, high-impact risks require mitigation. Low-likelihood, low-impact risks are acceptable. High-likelihood, low-impact risks may require monitoring but not full redundancy. Low-likelihood, high-impact risks are judgment calls—some teams accept them, some mitigate them with insurance or business continuity plans rather than technical redundancy.

The goal is not to eliminate all shared risk. The goal is to know what risk you are accepting and have decided it is acceptable. The failure mode to avoid is discovering shared risk during an outage.

## Testing Correlated Failures

The only way to validate independence is to test correlated failure scenarios. In a controlled environment, simulate a failure of suspected shared components. Shut down the DNS provider and verify that fallback routing works. Disable the authentication service and confirm that cached credentials allow continued operation. Block network access to a shared region and test whether failover to a distant region succeeds.

Chaos engineering for multi-provider systems means deliberately breaking shared dependencies. If you suspect both providers share a network path, use network fault injection to sever that path and observe behavior. If both depend on a shared database, simulate database failure during load tests. These tests are disruptive, which is why they run in staging or during scheduled game days—but they are the only way to prove that redundancy is real.

A company that tested their multi-provider failover by shutting down their primary model provider was surprised to see their fallback succeed—but with 10x higher latency than expected. Investigation revealed that while the providers were independent, the fallback routing logic depended on a database query that was suddenly under load. The database became a bottleneck. The redundancy worked, but not well enough. They redesigned the routing logic to use cached provider health state instead of live database queries. The test revealed a failure mode that would have caused degraded failover in production.

## Building Toward True Independence

True independence is expensive and complex. It requires multiple clouds, multiple regions, multiple providers, independent network paths, independent DNS, independent authentication, and independent upstream services. Most teams start with partial independence—different model providers, same cloud—and evolve toward deeper independence as reliability requirements and budgets grow.

The maturity curve: Level 1 is single provider, no redundancy. Level 2 is multi-provider on the same infrastructure. Level 3 is multi-provider on independent infrastructure within the same region. Level 4 is multi-provider across different regions. Level 5 is multi-provider across different geographies with independent network paths and independent auxiliary services. Most production systems operate at Level 2 or 3. Level 5 is reserved for systems where availability is existential.

The key is knowing what level you are at and what failure modes remain unmitigated. Multi-provider redundancy is valuable even with shared failure modes—but only if you understand which failures it protects against and which it does not. The next question is how to manage the quality gap users experience when you switch from one provider to another.

---

Next: **5.8 — Cross-Provider Quality Inconsistency During Failover**
