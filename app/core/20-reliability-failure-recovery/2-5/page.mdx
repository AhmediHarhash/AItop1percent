# 2.5 — Anomaly Detection for AI Systems: Techniques That Work

Anomaly detection is the promise: a system that automatically identifies when AI behavior drifts from normal, alerting you to problems you didn't anticipate. The reality is messier. False positives drown teams in alerts. Baseline definitions shift with user behavior. Seasonal patterns look like anomalies. Statistical techniques designed for deterministic systems fail when applied to probabilistic models. But when implemented correctly, anomaly detection catches the subtle degradations that threshold-based monitoring misses — the slow drift in response quality, the gradual shift in semantic understanding, the creeping increase in edge case failures. The challenge is building anomaly detection that actually works in production without crying wolf every hour.

## Defining Normal for Non-Deterministic Systems

The fundamental problem with AI anomaly detection is that there is no single definition of normal. A traditional web service has a normal response time distribution — median 120ms, p95 at 280ms, p99 at 450ms. Deviations from this distribution indicate performance problems. An AI system has normal response time and normal semantic output and normal topic distribution and normal reasoning patterns and normal hallucination rate. Each dimension has its own distribution. Those distributions interact. A response that's semantically normal but unusually fast might indicate quality shortcuts. A response that's semantically unusual but perfectly valid might indicate rare but correct handling of an edge case.

You cannot define normal as a single metric. You must define normal as a multidimensional profile. Latency distribution. Token count distribution. Refusal rate. Task completion rate. Similarity to historical responses for similar queries. Embedding space position. Topic distribution. Sentiment distribution. Each dimension contributes signal. An anomaly in one dimension is not necessarily a problem. Anomalies in three dimensions simultaneously almost always indicate failure. The anomaly detection system must model these interactions.

Determinism makes this harder, not easier. If you run the same query twice and get two different responses, which one is normal? Both might be valid. Or one might be a quality regression. Temperature and sampling mean that even identical queries produce output variation. This variation is intentional — it prevents repetitive responses and increases creativity. But it complicates anomaly detection. You cannot compare outputs directly. You must compare statistical properties of output distributions. A single unusual response is not an anomaly. A systematic shift in the distribution of responses is.

Normal also changes over time. You ship a new model version. Normal shifts. User behavior evolves. Normal shifts. New query types arrive. Normal shifts. A seasonal event — tax season, holiday shopping, enrollment periods — changes query distribution. Normal shifts. Your anomaly detection system must adapt to these shifts without treating them as anomalies. The challenge is distinguishing intended shifts from unintended degradation. You shipped a new model — expect response time to change. But did it change by 15ms or 150ms? One is normal adaptation. One is a performance regression. Your system needs to know the difference.

## Statistical Process Control for AI Metrics

Statistical process control works by defining control limits based on historical variation. You calculate the mean and standard deviation of a metric over a baseline period. You set upper and lower control limits — typically three standard deviations from the mean. When the metric crosses these limits, you investigate. For manufacturing defects or server response times, this works well. For AI systems, it requires adaptation.

The baseline period must be stable. You cannot calculate control limits from data that includes incidents. Start with a clean week — a period where the system functioned correctly, no outages, no known quality issues. Calculate mean and variance for each metric you monitor. Latency, error rate, task completion, refusal rate, negative feedback rate. For each metric, establish upper and lower control limits. These become your anomaly thresholds. A metric crossing its control limit triggers investigation, not necessarily an alert. Some anomalies are expected. Others indicate real problems.

Rolling baselines handle drift better than static baselines. Instead of calculating control limits once and using them forever, recalculate them continuously using a trailing window — the past 7 days, the past 30 days. This allows normal to shift gradually as your system evolves. The risk is that slow degradation becomes normalized. If quality decreases by 1% per week, a 30-day rolling baseline absorbs that decrease. By week eight, you've lost 8% quality but the anomaly detector sees nothing wrong. Mitigate this with a secondary long-term baseline. If the 7-day baseline drifts more than 10% from the 90-day baseline, alert. This catches slow drift that rolling baselines miss.

Multivariate control charts detect correlated anomalies. Individual metrics might stay within control limits while their relationships break. Latency increases slightly. Token count decreases slightly. Neither crosses thresholds individually. But together, they indicate a quality regression — the model is producing shorter, faster, worse responses. Multivariate techniques like Hotelling's T-squared statistic measure whether the vector of all metrics has moved outside its normal range. Implementing this requires more sophisticated tooling, but it catches subtle degradations that univariate monitoring misses.

## Embedding Space Monitoring for Semantic Drift

Numeric metrics measure behavior. Embedding space monitoring measures meaning. You take a sample of recent outputs, embed them using a sentence transformer model, and compare their distribution to the baseline embedding distribution. If the semantic space shifts — responses cluster in different regions, cover different topics, use different language patterns — you have semantic drift. This catches problems that numeric metrics miss entirely.

Baseline embedding distribution comes from a reference period of known-good outputs. Sample 5,000 to 10,000 responses from a week when the system performed correctly. Embed each response. Store the resulting embedding vectors. Calculate the centroid and variance of this distribution. This becomes your semantic baseline. When monitoring new outputs, embed a rolling sample — say, the past hour's responses — and compare their distribution to the baseline. If the new sample's centroid has moved significantly, or if its variance has increased, semantic drift is occurring.

Distance metrics quantify drift. The simplest approach is centroid distance — measure the Euclidean or cosine distance between the baseline centroid and the current sample centroid. If this distance exceeds a threshold — say, 0.15 in normalized embedding space — semantic drift has occurred. More sophisticated approaches use distributional distance measures like Wasserstein distance or KL divergence, which compare entire distributions rather than just centroids. These catch cases where the mean hasn't shifted but the variance or shape has changed.

Clustering reveals failure modes. Instead of treating all embeddings as a single distribution, cluster them by topic or intent. Monitor each cluster separately. If the "medical query" cluster shifts, but the "general knowledge" cluster doesn't, you have a domain-specific degradation. If a new cluster appears that wasn't present in baseline, you're getting query types the model wasn't trained to handle. Clustering adds interpretability — when an anomaly fires, you can examine the cluster that drifted and see example outputs. This turns semantic drift from an abstract alert into a concrete diagnostic.

Implementation requires infrastructure. Embedding models must be fast — you're embedding thousands of responses per hour. Use lightweight models like all-MiniLM-L6-v2 or OpenAI's text-embedding-3-small. Store embeddings in a vector database or time-series database that supports high-dimensional data. Calculate rolling statistics in real time using stream processing. This infrastructure is non-trivial, but the signal quality justifies the cost. Embedding space monitoring catches hallucination increases, tone shifts, capability loss, and other semantic failures that traditional metrics miss.

## Distribution Shift Detection

Distribution shift is what happens when the input distribution changes. Users start asking different questions. A new user cohort arrives with different language patterns. A seasonal event shifts query topics. The model's outputs might be correct for the new distribution, but they differ from historical norms. Your anomaly detector fires. This is a false positive — the model adapted correctly to changed inputs. The challenge is distinguishing adaptive responses to distribution shift from failures caused by distribution shift.

Input monitoring prevents this. Monitor the distribution of incoming queries, not just outputs. Embed queries. Track topic distribution, query length, language distribution, entity types mentioned. When input distribution shifts, expect output distribution to shift correspondingly. If input shifts but output doesn't, the model may be failing to adapt. If output shifts but input doesn't, the model has a problem. The correlation between input and output drift tells you whether the anomaly is benign or critical.

Covariate shift is the most common form. The distribution of input features changes, but the relationship between inputs and correct outputs remains stable. Users switch from asking about Product A to asking about Product B. Your retrieval system retrieves different documents. Your model generates different responses. This is normal. Label shift occurs when the underlying task distribution changes. Users who previously asked informational questions start asking transactional questions. Your model must adapt its response style. If it doesn't, you have a capability failure. Concept drift occurs when the meaning of inputs changes over time. A term that meant one thing in your training data means something different now. The model produces outdated responses.

Adaptive baselines handle expected shifts. If you know tax season causes a query distribution shift every March, don't treat it as an anomaly. Use seasonal baselines — compare March 2026 to March 2025, not to February 2026. If you launch a new feature that changes query patterns, reset your baseline. The anomaly detector should flag unexpected shifts, not expected ones. This requires manual intervention — someone must tell the system when to adapt its baseline. Automate this through deployment hooks. When you ship a new model or feature, the monitoring system automatically starts a new baseline period.

Drift magnitude matters. A 5% shift in query topic distribution is noise. A 40% shift is signal. Set thresholds based on historical variation. Calculate how much your input distribution naturally varies day to day. Multiply that variation by 2 or 3 to get your drift alert threshold. Shifts beyond this magnitude require investigation. Often, they indicate data pipeline failures — a feed broke, a filter misconfigured, a user segment suddenly missing. These are incidents, not benign drift.

## The Baseline Problem: What Do You Compare Against?

Every anomaly detection system compares current behavior to a reference. That reference is your baseline. Choose it incorrectly and your anomaly detector is useless. Too narrow a baseline — one day of data — and normal variation looks anomalous. Too broad a baseline — six months including incidents — and real anomalies look normal. The baseline must be long enough to capture typical variation, short enough to exclude degradation, and representative of current system behavior.

Clean baselines require manual curation. Identify a period of known-good behavior. No incidents. No quality complaints. No unusual traffic patterns. Ideally one to two weeks of data. Extract metrics and outputs from this period. This becomes your reference. The challenge is that most teams don't have clean periods. They run continuous experiments, ship constant updates, and experience ongoing minor incidents. In this environment, you must define clean as "acceptable" rather than "perfect." Choose a period where quality was at target and incident count was typical. It's not pristine, but it's reality.

Versioned baselines handle model updates. When you ship a new model, the baseline shifts. Old normal is irrelevant. New normal must be established. Run the new model in shadow mode for 24-48 hours. Collect metrics and outputs. Establish a new baseline. Then promote to production. Your anomaly detector compares production behavior to the new baseline, not the old one. When you roll back to the previous model, revert to the previous baseline. This requires tracking which baseline corresponds to which model version. Most teams don't do this. They should.

Stratified baselines reduce false positives. Not all queries are equal. Simple queries have one normal. Complex queries have another. User cohort A behaves differently from user cohort B. Peak traffic hours differ from off-peak. Define separate baselines for each stratum — query complexity, user segment, time of day, query topic. Compare current behavior within each stratum to that stratum's baseline. This prevents false positives caused by natural variation in traffic mix. If complex query volume increases at night, aggregate metrics shift — but stratified metrics stay stable.

Baseline refresh cadence depends on system stability. For rapidly evolving systems — multiple model updates per week, continuous feature launches — refresh baselines weekly. For stable systems, monthly refresh is sufficient. The goal is to keep the baseline recent enough to reflect current behavior without absorbing gradual degradation. Automate baseline refresh. Manual updates don't happen consistently. Schedule a job that recalculates baselines every N days using the most recent clean period. Alert when baseline drift exceeds thresholds. This catches slow degradations that rolling windows miss.

## Seasonality and Expected Variation

Tax season changes query distribution. Holiday shopping spikes traffic. School enrollment periods shift user demographics. Elections flood the system with political queries. These are not anomalies. They're predictable seasonal patterns. Your anomaly detector must account for them or it will cry wolf every time a calendar event occurs. The solution is seasonal baselines and expected variation modeling.

Seasonal baselines compare like to like. Don't compare December to January. Compare December 2026 to December 2025. If Black Friday 2025 had twice the normal traffic, expect Black Friday 2026 to as well. Seasonal baselines require a full year of data. Most teams building in 2026 don't have that yet. Until you do, maintain a calendar of known seasonal events. When those events occur, increase your anomaly thresholds or pause anomaly detection temporarily. It's manual and imperfect, but it prevents false positives during predictable spikes.

Day-of-week variation is universal. Monday traffic differs from Saturday traffic. Query types shift. User cohorts shift. Model behavior shifts. Calculate separate baselines for each day of the week. Compare Monday to Monday, not Monday to Saturday. This eliminates a major source of false positives. Time-of-day variation matters for global products. Peak hours in Europe, Asia, and the Americas create three traffic surges per day. Each surge has different query patterns. Use time-of-day baselines if your traffic is globally distributed. For regional products, aggregate by local time.

Trend adjustment prevents false positives during growth. Your user base doubles over six months. Traffic doubles. Query volume doubles. Absolute metrics increase, but relative metrics stay stable. An anomaly detector comparing absolute counts to a six-month-old baseline will fire constantly. Detect anomalies in relative metrics — rates, percentages, ratios — rather than absolute counts. Task completion rate is more stable than absolute task completions. Retry rate is more stable than absolute retry count. Relative metrics normalize for growth and scale.

Expected variation should be learned, not guessed. Don't assume seasonal patterns based on intuition. Measure them. If you've been in production for six months, you have data. Analyze how your metrics vary by day of week, time of day, user cohort, query topic. Calculate variance for each segment. Use this variance to set anomaly thresholds. High-variance segments need wider thresholds. Low-variance segments can use tight thresholds. Learned thresholds adapt to your actual system behavior rather than theoretical models.

## False Positive Management: Anomaly Detection That Doesn't Cry Wolf

The death of anomaly detection is alert fatigue. The system fires 40 alerts per day. Thirty-eight are false positives. The on-call engineer learns to ignore them. When alert 39 represents a real incident, nobody investigates until users complain. False positive management is not optional. It's the difference between a useful detection system and a notification spam generator.

Precision targeting determines value. A detection system with 50% precision — half true positives, half false positives — is borderline usable if alerts are rare. Two alerts per day, one real incident. The team investigates both. But a system with 50% precision and 40 alerts per day is worse than useless. It trains the team to ignore alerts. Aim for 80% precision minimum. Four out of five alerts should represent real issues. Achieving this requires aggressive tuning, stratified baselines, and constant threshold adjustment.

Threshold tuning is continuous work. Start with conservative thresholds — only alert on extreme anomalies. Monitor false positive rate. If it's low and you're catching incidents, tighten thresholds gradually to catch smaller anomalies. If false positives spike, loosen thresholds. Track true positive rate and false positive rate over time. Aim for operating points that maximize true positives while keeping false positives below one per day. This is not a set-it-and-forget-it process. Your system evolves. Your baselines shift. Your thresholds must adapt.

Alert suppression during known issues prevents cascades. When an incident occurs, dozens of downstream metrics anomalize. Latency spikes. Error rate spikes. Task completion drops. Negative feedback increases. Each metric triggers its own alert. The on-call engineer receives 15 notifications about the same incident. Implement alert suppression — when a root cause alert fires, suppress related alerts for 30 minutes. If latency anomalizes, suppress task completion anomalies. If error rate spikes, suppress quality metric anomalies. This keeps incident response focused on the root cause rather than drowning in correlated symptoms.

Anomaly scoring prioritizes investigation. Not all anomalies are equally urgent. Rank them by severity, impact, and confidence. A 5-sigma anomaly in task completion rate affecting all users is P1. A 2-sigma anomaly in embedding centroid distance affecting 3% of queries is P3. Score anomalies based on magnitude, affected user percentage, and metric criticality. Route high-severity anomalies to immediate pages. Route low-severity anomalies to daily summary reports. This ensures critical issues get immediate attention while minor anomalies are tracked without causing alert fatigue.

## Tools and Platforms in 2026

Building anomaly detection from scratch is a multi-month engineering project. Most teams use platforms that provide anomaly detection out of the box. Arize, Langfuse, Weights and Biases, and LangSmith all offer AI-specific monitoring with built-in anomaly detection. These platforms understand LLM metrics, handle embedding space analysis, and provide drift detection for both inputs and outputs. They're not perfect — you still need to tune thresholds and define baselines — but they provide a foundation that would take months to build internally.

Arize specializes in drift detection and embedding monitoring. It ingests your model inputs, outputs, and ground truth labels, then automatically calculates drift metrics, PSI scores, and embedding space shifts. It flags when distributions change and provides drill-down tools to investigate which segments are affected. For teams that lack internal ML monitoring infrastructure, Arize provides production-grade anomaly detection without requiring you to build a data pipeline. The tradeoff is vendor lock-in and cost — Arize pricing scales with traffic volume.

Langfuse focuses on LLM trace analysis and user feedback integration. It tracks every LLM call, calculates quality metrics, and monitors user feedback signals. Anomaly detection is based on these metrics — when thumbs-down rate spikes or average score drops, Langfuse alerts. It's lighter weight than Arize and better suited for startups. The downside is less sophisticated statistical modeling. Langfuse detects obvious anomalies well but misses subtle drift.

Weights and Biases and LangSmith provide experimentation platforms with monitoring built in. They're designed for teams running continuous experiments who need to track model performance across versions. Anomaly detection is secondary to experiment tracking. If your workflow is already built around these platforms, their monitoring capabilities are sufficient for basic anomaly detection. For dedicated production monitoring, dedicated tools work better.

Custom-built systems remain common at scale. Large teams with specialized needs build anomaly detection into their internal monitoring platforms. They integrate with existing observability stacks — Datadog, Grafana, Prometheus — and add AI-specific metrics on top. This approach gives you full control over thresholds, baselines, and alerting logic. It also requires significant engineering investment. Build custom if you have the team and the requirements justify it. Use platforms if you need monitoring now and don't have six months to build it.

Anomaly detection is not a magic solution. It's a tool that catches certain kinds of failures — drift, degradation, distribution shift — that threshold-based monitoring misses. It requires careful baseline selection, threshold tuning, and false positive management. When done right, it detects incidents 20-40 minutes faster than manual review and catches subtle quality regressions that would otherwise go unnoticed for days. When done poorly, it generates alert spam that trains teams to ignore monitoring entirely. The difference lies in treating anomaly detection as an ongoing operational practice, not a one-time configuration task. Next, we'll examine how to design alerts that people actually respond to — balancing sensitivity and specificity to prevent alert fatigue while ensuring real incidents get immediate attention.

