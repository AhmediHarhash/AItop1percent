# 3.10 — Timeout and Retry Patterns That Complement Circuit Breakers

The circuit breaker tripped at 14:21. The system stopped sending requests to the failing model endpoint. By 14:23, Engineering had identified the issue — a memory leak in the serving infrastructure — and deployed a fix. The endpoint returned to health. The circuit breaker closed at 14:25, allowing traffic through again. Then at 14:26, the endpoint failed again. Not from the memory leak. From the thundering herd of retry attempts that had been queuing up during the four-minute outage. Every retry policy in every service upstream had been backing off, waiting. When the circuit closed, they all fired at once. The endpoint couldn't handle the load spike and crashed again. The circuit breaker reopened. The cycle repeated three more times before Engineering rate-limited the retry attempts and manually drained the queues.

Circuit breakers prevent your system from hammering a failing dependency. Timeouts prevent your system from waiting forever for a response that will never come. Retries give transient failures a second chance. These three mechanisms work together — or they create cascading failure loops. The interaction between them determines whether your system degrades gracefully or collapses under its own recovery attempts.

## Timeout Design for AI Model Calls

Timeouts are not a single number. They are a strategy for abandoning work that is taking too long. AI model inference is fundamentally different from database queries or API calls. A database query that takes 50 milliseconds or 5 seconds is behaving pathologically — something is wrong. A model inference call that takes 500 milliseconds or 5 seconds might just be processing a longer input. Setting the right timeout requires understanding what "too long" means for your specific workload.

Start with P99 latency under normal conditions. If 99 percent of your model calls complete in under 2 seconds, a timeout of 5 seconds gives a reasonable buffer for outliers without waiting indefinitely. But P99 is not enough. You also need to measure P99 by input length, by user tier, by model version. A summarization request for a 50-page document legitimately takes longer than a two-sentence classification. Your timeout strategy must account for this variance.

Timeouts must be applied at multiple levels. The client making the model call sets a timeout. The load balancer routing the request sets a timeout. The model serving infrastructure sets a timeout. These timeouts must be coordinated. If the client timeout is 5 seconds, the load balancer timeout is 10 seconds, and the model server timeout is 15 seconds, the client will abandon the request while the model server is still working on it. You have wasted compute and the user still gets no answer. The correct pattern: each layer's timeout is slightly shorter than the layer below it. Client timeout 5 seconds, load balancer 6 seconds, model server 7 seconds. The client decides when to give up. The infrastructure respects that decision.

Timeouts interact with circuit breakers. If your timeout is too short, you will trigger circuit breakers unnecessarily. If your model P99 latency is 2 seconds and your timeout is 2.5 seconds, every P99 request looks like a timeout failure. The circuit breaker sees a rising error rate and opens, even though the model is functioning normally — just slowly. The fix: set timeouts based on realistic worst-case latency, not optimistic averages. A timeout of 3 times P99 latency is a reasonable starting point. Adjust based on failure mode analysis. If timeouts correlate with circuit breaker trips, your timeout is too aggressive.

## Retry Strategies: Exponential Backoff and Jitter

Retries amplify load. One failed request that retries three times becomes four requests. If 10 percent of requests fail and every one retries three times, your actual request volume is 130 percent of the original load. If those failures are caused by overload, retries make the problem worse. The service is already struggling, and you are sending more traffic.

Exponential backoff solves this by spacing out retries. First retry after 1 second. Second retry after 2 seconds. Third retry after 4 seconds. Each retry waits longer than the previous one, giving the failing service time to recover. But exponential backoff alone is not enough. If 10,000 requests all fail at the same moment, they will all retry at the same moments — 1 second later, 2 seconds later, 4 seconds later. You have created synchronized retry waves.

Jitter breaks the synchronization. Instead of retrying after exactly 1 second, retry after 0.8 to 1.2 seconds, chosen randomly. Instead of 2 seconds, retry after 1.6 to 2.4 seconds. The retry attempts spread out over time instead of arriving in coordinated bursts. Full jitter means the retry delay is chosen uniformly at random between 0 and the exponential backoff value. Decorrelated jitter means each retry delay is based on the previous delay plus some randomness. Both work. Full jitter is simpler to implement and sufficient for most systems.

The backoff parameters matter. If your base delay is too short, retries happen too quickly and overwhelm the recovering service. If the base delay is too long, users wait unnecessarily for transient errors to resolve. A base delay of 500 milliseconds to 1 second works for most AI systems. The backoff multiplier — how much the delay increases each retry — controls how aggressive the backing off is. A multiplier of 2 is standard. A multiplier of 1.5 is gentler. A multiplier of 3 is more aggressive. Choose based on how quickly you want to give up versus how much load you want to impose during recovery.

## When Retries Help Versus When They Hurt

Retries are appropriate for transient failures. A network packet was dropped. A connection timed out. A server was momentarily overloaded but recovered. These failures resolve themselves quickly. Retrying after a brief delay succeeds. Retries are inappropriate for permanent failures. The model endpoint does not exist. The authentication token is invalid. The input is malformed. Retrying these requests will fail every time. You are wasting resources and delaying the inevitable error response.

Classify errors into retryable and non-retryable categories. HTTP 500 errors are retryable — the server encountered an internal error that might not recur. HTTP 503 errors are retryable — the service is temporarily unavailable. HTTP 429 errors are retryable — you are being rate-limited, and backing off will eventually allow the request through. HTTP 400 errors are not retryable — the request is malformed. HTTP 401 and 403 errors are not retryable — you lack authentication or authorization. HTTP 404 errors are usually not retryable — the resource does not exist. Some 404 errors are retryable if your system is eventually consistent and the resource might appear soon. Know your error semantics.

Timeouts are retryable in most cases but require special care. A timeout means the server did not respond within the allotted time. The request might have succeeded on the server side. The response might be on its way. If you retry a non-idempotent operation — like submitting a support ticket or charging a credit card — you might execute the operation twice. Retries must only be applied to idempotent operations or operations with built-in deduplication.

Circuit breakers change retry behavior. When a circuit breaker is open, retries should not attempt to reach the failing service. The circuit breaker is explicitly saying "do not send traffic here." Retrying against an open circuit wastes resources and delays the circuit breaker's recovery detection. Instead, retries should fail fast or use a fallback. When the circuit transitions to half-open, retries can resume cautiously. The retry logic must respect circuit breaker state.

## Retry Budgets: Limiting Total Retry Attempts

A retry budget is a limit on how much retry traffic your system will generate. Without a budget, a cascade of retries can overwhelm downstream dependencies. The budget is usually expressed as a ratio: retry traffic cannot exceed some multiple of original traffic. A retry budget of 1.5 means retry traffic is limited to 50 percent of original traffic. If you are handling 1000 requests per second, you can generate at most 500 retries per second.

Implement retry budgets at the client level, not per-request. Each client tracks how many retries it has issued in the last N seconds. If the retry count exceeds the budget, additional retries are rejected immediately. This prevents a failure in one part of the system from consuming all retry capacity. Retry budgets should be enforced per dependency. Your retry budget for the model serving endpoint is separate from your retry budget for the database. A failure in one should not exhaust retry capacity for the other.

Retry budgets interact with circuit breakers. When a circuit breaker opens, retry attempts drop to zero — the circuit is preventing traffic from reaching the dependency. When the circuit transitions to half-open, retry attempts resume. If the dependency is still struggling, retries can quickly exhaust the retry budget, causing the circuit to reopen. This is correct behavior. The retry budget prevents the retries from overwhelming the recovering service.

Monitor retry budget utilization. If you are consistently hitting your retry budget, either your budget is too low or your failure rate is too high. A healthy system uses a small fraction of its retry budget — maybe 5 to 10 percent under normal conditions. If you are using 80 percent of your retry budget, you have an availability problem. Investigate the root cause of failures rather than increasing the budget.

## The Interaction Between Retries and Circuit Breakers

Circuit breakers and retries serve complementary goals. Circuit breakers prevent cascading failures by stopping traffic to unhealthy dependencies. Retries give transient errors a second chance without requiring user intervention. The interaction is delicate. Retries can keep a circuit breaker closed longer than it should be by masking failures. Circuit breakers can cause retries to fail immediately instead of waiting for recovery.

The correct pattern: retries happen before circuit breaker evaluation. A request is sent. It times out. The client retries. The retry succeeds. From the circuit breaker's perspective, the request succeeded — it does not see the initial timeout. This is correct if the timeout was transient. It is incorrect if the timeout indicates a systemic problem. The circuit breaker is blind to patterns hidden by retries.

One solution: count both original attempts and retries toward circuit breaker error rates. If a request requires two attempts to succeed, that is logged as one success and one failure. The circuit breaker sees the failure. This prevents retries from masking an elevated error rate. The trade-off: you might trip circuit breakers more aggressively, even when retries are successfully handling transient errors. Tune the circuit breaker failure threshold to account for expected retry rates.

Another solution: track retry-corrected success rates separately. Your observability system records both raw success rate — the percentage of first attempts that succeed — and effective success rate — the percentage of requests that eventually succeed after retries. If raw success rate drops but effective success rate remains high, retries are handling transient issues. If both drop, you have a deeper problem. Circuit breakers can use effective success rate for health checks while monitoring raw success rate for early warning signals.

Circuit breaker state must be visible to retry logic. When a circuit breaker opens, retries should not attempt to reach the failing dependency. Instead, retries should fail fast with a clear error message or invoke a fallback path. When the circuit breaker is half-open, retries can attempt the primary path but should limit concurrency to avoid overwhelming the recovering service. This requires coordination between the circuit breaker implementation and the retry logic — they must share state or at least share a communication channel.

## Idempotency Requirements for Safe Retries

An idempotent operation produces the same result no matter how many times you execute it. Retrying an idempotent operation is safe — even if the original request succeeded and you never received the response, executing it again causes no harm. Retrying a non-idempotent operation is dangerous — you might charge a credit card twice, send two emails, or create duplicate records.

Model inference is usually idempotent. Sending the same input to the same model with the same parameters returns the same output. Retrying is safe. But there are exceptions. If your model uses sampling with temperature above zero, the output is non-deterministic. Each retry produces a different response. This is not a safety issue — you are not creating duplicate side effects — but it is a consistency issue. If the user sees different answers for the same question because you retried in the background, they lose trust. Set temperature to zero for idempotent inference or use a fixed random seed.

Operations with side effects are not idempotent. Logging an event, sending a notification, writing to a database, calling a third-party API — these actions have consequences beyond returning a response. If you retry these operations without deduplication, you create duplicates. The standard solution: idempotency keys. The client generates a unique identifier for each request. The server checks if it has seen that identifier before. If yes, it returns the cached response without re-executing the operation. If no, it executes the operation, caches the response, and returns it.

Idempotency keys must be stored durably. An in-memory cache is insufficient — if the server restarts, the cache is lost, and retries will re-execute operations. Store idempotency keys in a database or distributed cache with a time-to-live of at least 24 hours — long enough to handle retries from slow clients or clients that retry after extended delays. If storage is a concern, you do not need to store the full response, only a hash and a success or failure indicator. The client can decide how to handle cached failures.

AI systems often have long-running operations — fine-tuning jobs, batch inference, dataset annotation. These operations cannot complete within a single request-response cycle. Retrying the request does not retry the operation — it queries the operation's status. This pattern is naturally idempotent. The client submits the operation once, receives an operation ID, and polls for completion. Retrying the poll does not re-submit the operation. Design long-running AI workflows as asynchronous operations with idempotent status checks.

## Timeout Coordination Across Nested Calls

AI systems are rarely single-hop request-response. A user request triggers an API call. The API calls a prompt router. The router calls a model endpoint. The model endpoint calls an embedding service for RAG. Each hop adds latency. Each hop has its own timeout. If the timeouts are not coordinated, you get pathological behavior.

The innermost timeout must be the shortest. If the embedding service timeout is 10 seconds and the model endpoint timeout is 5 seconds, the model endpoint will time out before the embedding service finishes. The embedding service will continue working on a request that has already been abandoned. This is wasted compute. The correct pattern: each layer's timeout is the sum of the layer below it plus a small buffer for processing. If the embedding service timeout is 2 seconds and the model endpoint needs 1 second to process the embeddings, the model endpoint timeout should be 3 to 4 seconds. The API timeout should be 5 to 6 seconds.

Propagate timeout deadlines across service boundaries. The user request has a deadline — the time by which the response must be returned. Each downstream call inherits a portion of that deadline. If the user request has a 10-second deadline and the API call to the model endpoint happens 1 second into the request, the model endpoint call inherits a 9-second deadline. The model endpoint knows it has 9 seconds to respond. If it needs to call the embedding service, it allocates perhaps 2 seconds to that call, reserving 7 seconds for its own processing. This is called deadline propagation.

Implement deadline propagation with context objects. The user request creates a context with a deadline. Each service call receives the context and checks the remaining time. If the remaining time is insufficient to complete the operation, the service fails fast instead of starting work it cannot finish. This prevents wasted work and improves user-facing latency — the user gets an error quickly instead of waiting for a timeout.

Timeouts should account for queueing time. If your model endpoint has a queue of 50 requests and each request takes 2 seconds to process, a new request will wait 100 seconds before processing begins. A 5-second timeout will expire in the queue — the request never reaches the model. Monitor queueing depth and reject requests when the queue is too deep to meet timeout deadlines. This is admission control. It feels harsh — you are rejecting requests that might succeed if the user waited longer — but it prevents queue buildup and keeps latency predictable.

## The Cascading Timeout Problem

A cascading timeout happens when upstream timeouts are too aggressive relative to downstream latency. The user request has a 5-second timeout. The API has a 4-second timeout. The model endpoint has a 3-second timeout. The embedding service has a 2-second timeout. If the embedding service takes 2.1 seconds to respond — slightly over its timeout — the model endpoint times out. The API times out. The user request times out. The embedding response arrives 0.1 seconds too late, and the entire call chain fails.

Cascading timeouts cause false negatives. The underlying service is functioning correctly, just slightly slower than expected. The timeout thresholds are too tight. The solution: set timeouts based on realistic latency distributions, not ideal-case performance. If your P99 latency is 2 seconds, a 2-second timeout will fail 1 percent of requests. A timeout of 3 to 4 seconds gives headroom for variance while still catching pathological delays.

Cascading timeouts interact with retries. If the first attempt times out because the embedding service was slow, the retry might succeed if the embedding service speeds up. But if the timeout is propagated — the retry inherits the same deadline as the original attempt — the retry has even less time to complete. The second attempt is almost guaranteed to fail. Retries must receive fresh timeouts, not inherited deadlines. Each retry starts a new timeout clock.

Monitor timeout correlation across layers. If your API timeout rate is 5 percent and your model endpoint timeout rate is 5 percent and they are the same 5 percent of requests, you have cascading timeouts. If they are different 5 percent, you have independent timeout causes. Correlated timeouts indicate that timeout thresholds are too tight or that latency variance is too high. Independent timeouts indicate multiple different failure modes. Investigate the causes and adjust timeouts accordingly.

Circuit breakers prevent timeout cascades from taking down entire systems. If the embedding service is slow enough to trigger cascading timeouts, the circuit breaker on the embedding service will open, stopping traffic before the timeouts cascade. Requests will fail at the circuit breaker level — immediately, without waiting for a timeout. This is better than waiting 5 seconds for a timeout. Faster failure means faster recovery. The circuit breaker contains the blast radius of the slow service.

When timeouts, retries, and circuit breakers are coordinated correctly, your system handles transient failures without user impact, isolates failing dependencies before they cascade, and recovers quickly when services return to health. When they are not coordinated, you get thundering herds, wasted compute, false circuit breaker trips, and cascading failures. The next subchapter addresses the override problem — the moments when human judgment must disable these automated protections and the governance required to make overrides safe.
