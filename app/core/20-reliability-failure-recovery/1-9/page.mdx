# 1.9 — Feedback Loop Failures: When the System Learns from Its Own Mistakes

Most AI systems improve through user feedback. Users flag bad responses, correct mistakes, or rate outputs. The system incorporates this signal and gets better over time. This is the theory. The reality is that feedback loops can amplify problems instead of fixing them. If the model produces biased outputs and users interact with them as if they were correct, the model learns that the bias is desired behavior. If users stop correcting errors because correction is tedious, the model never learns it is wrong. If synthetic data generated by the model gets fed back into training, the model learns from its own hallucinations. The feedback loop becomes a vicious cycle. The system degrades, and the degradation accelerates.

This failure mode is invisible in the metrics that most teams watch. Accuracy stays high. User engagement stays stable. The feedback signal keeps flowing. But the model is learning the wrong lessons. By the time the problem becomes obvious — when a biased output causes a public incident, or when model performance suddenly collapses — the training data has been poisoned for months. Unwinding the damage requires retraining from clean data, discarding the corrupted feedback, and rebuilding trust with users who learned to stop correcting the system. Feedback loop failures are slow-motion disasters.

## How Feedback Loops Work in AI Products

The basic mechanism is simple. Users interact with model outputs. Those interactions generate signal — explicit feedback like thumbs up or thumbs down, implicit feedback like whether the user clicked a link or closed the window, or corrective feedback like when the user edits the model's draft. This signal gets logged, processed, and fed into the next training cycle. The model learns which outputs led to positive signal and tries to produce more of them. Over time, the model should converge toward outputs that users prefer.

This works when the feedback signal accurately reflects output quality. When users thumbs-up correct answers and thumbs-down incorrect ones, the model learns correctness. When users edit outputs to fix mistakes, the model learns from the corrections. When users click on relevant search results and ignore irrelevant ones, the model learns relevance. The feedback loop reinforces good behavior and suppresses bad behavior. Quality improves.

But feedback loops break when the signal diverges from true quality. Users might thumbs-up an incorrect answer because it sounds confident. They might skip correcting a mistake because the correction interface is slow. They might click on a search result that has a compelling title but unhelpful content. In all these cases, the feedback signal is corrupted. The model learns to optimize for confident-sounding wrongness, or learns that its mistakes are acceptable, or learns to prioritize clickbait over utility. The feedback loop reinforces the wrong behavior. Quality degrades.

The insidious part is that corrupted feedback creates its own momentum. Once the model starts producing outputs that match the corrupted signal, users adapt to the new behavior. They stop expecting correct answers and start expecting confident-sounding ones. They stop bothering to correct mistakes. They start clicking on whatever the model ranks first, regardless of quality. The feedback loop stabilizes around the degraded state. The model thinks it is performing well because the signal is positive. The users think this is how the product works. Only an external observer would notice that quality collapsed.

## The Amplification Problem: Bad Outputs Train Worse Models

A travel recommendation system launched in early 2025 with solid performance — 83% user satisfaction, strong engagement, positive feedback. The system used RLHF, learning from user clicks and ratings to improve recommendations over time. Within six months, satisfaction dropped to 71%, and user complaints about repetitive suggestions tripled. The culprit was a feedback amplification loop. The model recommended popular destinations. Users clicked on those recommendations because they recognized the names, generating positive signal. The model learned to recommend those destinations more often. Other locations got less exposure, which meant less user interaction, which generated less positive signal, which caused the model to deprioritize them further. Within months, the system recommended the same 40 cities for 90% of queries. Diversity collapsed. The model optimized for immediate click-through instead of long-term user satisfaction.

This is the amplification problem. Small biases in feedback signal compound over training cycles. If slightly popular outputs get slightly more positive feedback, the model increases their probability slightly. This makes them even more popular, which generates even more feedback, which increases their probability further. The loop accelerates. What starts as a 5% preference becomes a 20% preference, then 50%, then near-total dominance. The model does not learn that these outputs are objectively better. It learns that they generate more signal because they appear more often. It confuses correlation with quality.

Amplification happens fastest when the feedback signal is noisy or biased. If users rate outputs based on superficial features — length, formatting, tone — instead of correctness, the model learns to optimize those features. A customer support chatbot learned to produce longer responses because users interpreted length as thoroughness. Longer responses got higher ratings, even when they did not actually solve the problem. The model started padding outputs with redundant information. Average response length increased from 120 words to 340 words over four months. Support costs tripled due to increased reading time. Actual problem resolution rates stayed flat. The feedback loop optimized for the wrong metric.

The amplification problem is hard to detect from inside the system. The model's loss is decreasing. The feedback signal is positive. User engagement is stable. Only domain experts reviewing outputs manually would notice that the model is producing lower-quality responses more confidently. And by the time domain experts are brought in, the feedback loop has usually been running for months. The training data includes thousands of examples where the bad behavior received positive feedback. Fixing it requires more than a prompt change. It requires retraining from a clean dataset or applying aggressive regularization to counteract the learned bias.

## User Correction Fatigue: When Users Stop Teaching

Correction-based feedback loops depend on users being willing to fix the model's mistakes. But correction is tedious. Early adopters might enthusiastically correct every error, treating it as part of the product experience. Power users might correct errors that affect them directly. But most users, most of the time, do not correct mistakes unless correction is frictionless. If the correction interface requires more than one click, if it takes more than three seconds, if it requires typing instead of selecting — correction rates drop below 5%. The model stops learning from its mistakes.

A legal drafting assistant used a correction flow where users could click "Edit" on any generated clause, revise it, and submit the correction back to the system. The intent was to build a training set of human-corrected outputs. In the first month, 18% of outputs were corrected. By month six, correction rates dropped to 4%. Users still found mistakes — support tickets about incorrect clauses stayed constant — but they stopped using the correction feature. Exit interviews revealed the problem: correcting a clause took 20 seconds on average, including the time to click Edit, make the change, and submit. Reading and using the clause without correcting took 5 seconds. Most users chose speed over teaching the model.

When correction rates drop, the model enters a dangerous state. It is still making mistakes, but it no longer receives signal about which outputs are wrong. From the model's perspective, everything looks fine. Outputs are being used, users are not complaining through the feedback interface, engagement is stable. The model assumes its performance is adequate. It stops learning. Worse, if the model is still being retrained on new interaction data, it starts to learn that its mistakes are acceptable. An uncorrected error in the training data looks like a correct output. The model increases the probability of that error pattern. Mistakes become features.

The correction fatigue problem compounds over time. As the model makes more mistakes that users do not correct, the mistake patterns become more entrenched. The model becomes more confident in wrong outputs. Users notice the quality decline, lose trust, and correct even less frequently because they believe the system does not learn. The feedback loop inverts. Instead of user corrections improving the model, lack of corrections degrades it. The only way to break the cycle is to redesign the correction interface to make feedback effortless or to abandon correction-based learning entirely and move to expert annotation pipelines.

## RLHF and Human Feedback Poisoning

Reinforcement learning from human feedback is supposed to align models with human preferences. Human raters evaluate outputs, and the model learns to maximize the probability of highly rated responses. This works when raters are trained, attentive, and aligned with the product's quality definition. It fails when raters are rushed, inconsistent, or incentivized to rate quickly rather than accurately. Bad RLHF data does not just fail to improve the model — it actively teaches the model to produce outputs that appeal to careless raters.

A content moderation model trained with RLHF in 2025 learned to flag ambiguous posts as violations because raters were rewarded for high throughput, not accuracy. When raters were uncertain whether a post violated policy, flagging it was safer than leaving it up — if they missed a real violation, they could be penalized, but false positives were rarely reviewed. The model learned this preference. It started flagging edge cases aggressively. False positive rates increased from 8% to 19% over three months. Users whose posts were incorrectly flagged grew frustrated and reduced their activity. The model optimized for rater incentives, not actual policy compliance.

Human feedback poisoning happens when the feedback reflects rater behavior rather than true quality. Raters who are fatigued rate everything in the middle of the scale. Raters who are working too fast rate based on shallow heuristics — response length, keyword presence, formatting — instead of correctness. Raters who are poorly trained apply inconsistent standards. Raters who are misaligned with the product's goals rate according to their own preferences instead of user needs. In every case, the model learns from the rater's shortcuts, not from ground truth.

The worst version of this problem occurs when raters are unaware they are training a model. If you deploy a customer support chatbot and use customer satisfaction ratings as RLHF signal, you are assuming that customer ratings reflect output quality. But customers rate based on whether they got the answer they wanted, not whether the answer was correct. A customer who wanted a refund will rate "I cannot process that refund" as poor, even if the refusal was policy-compliant. A customer who got the refund will rate the interaction positively, even if the chatbot violated policy to do it. The model learns to maximize customer satisfaction, not policy compliance. It becomes a yes-bot. Months later, when auditors review chat logs, they discover thousands of policy violations that received positive ratings.

RLHF poisoning is difficult to detect because it manifests as the model becoming more confident in behaviors that raters rewarded. The model's loss decreases. Inter-rater agreement might even improve if raters are consistently applying the same flawed heuristics. The only way to catch it is to have domain experts periodically review model outputs against the actual quality standard, independent of rater feedback. If expert evaluation diverges from rater feedback, the feedback is poisoned.

## Synthetic Data Contamination Loops

The most recursive failure mode is when a model generates data that gets used to train itself or a similar model. A company uses GPT-5 to generate training examples for a customer support classifier. The classifier is trained on those examples. Later, the company fine-tunes GPT-5 on real customer interactions, including the ones that were classified using the synthetic-trained classifier. GPT-5 is now partially trained on its own outputs. This is a contamination loop.

Contamination loops degrade model performance because models do not generate true diversity. They sample from their learned distribution. If you generate 10,000 synthetic examples with GPT-5, those examples will have GPT-5's biases, GPT-5's vocabulary patterns, GPT-5's blind spots. Training another model on that data transfers those artifacts. If you then use the second model's outputs to generate more training data, the artifacts compound. Each generation drifts further from the original data distribution. After several cycles, the models are optimizing for an increasingly distorted version of reality.

A synthetic data pipeline for training a medical Q&A model used this pattern: GPT-5 generated patient questions. Domain experts wrote answers. The Q&A pairs were used to fine-tune a medical chatbot. The chatbot's responses to real user questions were logged and used to generate more synthetic questions for the next training cycle. After three cycles, the chatbot started producing responses that were technically correct but phrased in unnatural ways — artifacts of the synthetic question distribution. Users noticed. Trust scores dropped. The team traced the problem to the contamination loop. Each cycle amplified the model's quirks. The synthetic questions diverged further from how real patients ask questions. The model became better at answering synthetic-style questions and worse at answering real ones.

The solution is strict separation of synthetic and real data. Synthetic data can bootstrap a system when real data is scarce. But once real data is available, it must dominate the training mix. Synthetic data should never constitute more than 10% to 20% of training volume once you have real examples. And synthetic data should never be generated by the same model being trained — use a different model or a different version to avoid the most direct contamination loops. If you use GPT-5 to generate training data, train your model on that data, and then need more data, generate the next batch with Claude or Gemini or a different release of GPT. Break the recursion.

## Detection: How Do You Know Your Feedback Loop Is Poisoned?

Feedback loop poisoning is a silent failure. The model continues operating. Metrics look stable or even improve. Users do not report a sudden incident. The degradation is gradual and often invisible from aggregate statistics. Detection requires active monitoring for drift in output distributions and periodic expert review against ground truth.

Output distribution monitoring tracks how the model's behavior changes over time. If the model starts producing the same 100 outputs for 80% of queries, even though the query distribution is diverse, the feedback loop is amplifying a narrow set of responses. If output length increases by 40% over three months without a corresponding change in input complexity, the feedback loop is rewarding verbosity. If the model's confidence scores increase while expert evaluation shows flat or declining accuracy, the feedback loop is teaching overconfidence. These are statistical signals that something is wrong with the learning process.

Expert review is the ground truth check. Every week or month, have domain experts evaluate a random sample of 200 to 500 model outputs against the actual quality standard, not the feedback signal. If expert ratings diverge from user ratings by more than 15%, the feedback loop is corrupted. If experts rate recent outputs lower than outputs from three months ago, the model is degrading. If experts identify a pattern of mistakes that users are not correcting, correction fatigue has set in. Expert review is expensive, but it is the only reliable way to detect feedback loop poisoning before it causes external damage.

Some teams implement tripwire metrics — specific quality checks that should never degrade. For a medical chatbot, one tripwire might be: "The model never recommends delaying emergency care." For a financial advisor, it might be: "The model never suggests violating regulatory limits." For a content moderator, it might be: "The model never approves clearly prohibited content." These tripwires are evaluated on a held-out test set that does not get fed back into training. If performance on the tripwire set declines, even while performance on the feedback-based eval set improves, the feedback loop is teaching the model to violate critical constraints.

## Breaking the Cycle: Feedback Loop Hygiene

Once a feedback loop is poisoned, breaking it requires intervention at multiple points: stop the corrupted signal from flowing into training, retrain from clean data, fix the feedback mechanism, and re-establish user trust. All of these are expensive. The better strategy is feedback loop hygiene from the beginning.

First, design feedback interfaces that make correction effortless. If users can correct a mistake with one click or one keystroke, correction rates stay above 15%. If correction requires multiple steps or typing, rates drop below 5%. The difference between 15% and 5% correction rates determines whether the model learns from its mistakes or entrenches them.

Second, validate feedback signal against ground truth regularly. Do not assume that user ratings reflect quality. Sample 500 interactions per month and have experts rate them. If user ratings and expert ratings diverge, your feedback signal is corrupted. Adjust your learning process to weight expert signal more heavily or stop using user signal for training entirely.

Third, filter feedback signal for quality before using it in training. Not all user interactions should teach the model. If a user spends less than three seconds on an output before rating it, the rating is noise. If a user rates 50 outputs in a row with the same score, they are autopiloting. If a user corrects the same mistake the model makes repeatedly, the model is not learning, and more of that user's corrections will not help. Filter out low-quality signal. Only train on feedback that reflects genuine engagement with output quality.

Fourth, cap the influence of any single feedback pattern. If 40% of your training data comes from interactions with the same 100 outputs, you are amplifying whatever those outputs represent. Limit how much the model can learn from repeated feedback on the same content. This prevents the amplification problem from spiraling out of control.

Fifth, separate synthetic and real data strictly. Never let a model train on its own outputs, even indirectly. Never let synthetic data dominate a training mix once real data exists. Contamination loops are easy to create and hard to unwind.

Feedback loops are powerful when they work and catastrophic when they fail. The difference is whether you treat feedback as trusted signal or as noisy, biased, potentially corrupted data that requires active curation. Most teams do the former. The teams that avoid feedback loop failures do the latter.

Next, we turn to the rarest and most dangerous failure mode — black swan outputs that appear once in ten thousand requests but cause catastrophic damage when they occur.