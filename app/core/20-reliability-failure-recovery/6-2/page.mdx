# 6.2 — AI-Specific Incident Severity Classifications

The incident classification system will fail you. A customer service AI produced outputs that were technically correct but emotionally tone-deaf to grieving users. Error rate: zero. Latency: normal. Quality score: 0.87, which was within acceptable range. Traditional severity classification: P3 or maybe P4, non-critical issue to investigate during business hours. Actual severity: the company's social media was flooded with screenshots within two hours, major news outlets picked up the story by evening, and the CEO spent the next week on an apology tour. Traditional P1 incidents take down your service. AI P1 incidents take down your reputation while your service continues running perfectly.

Traditional incident severity classifications measure availability and performance. P1 means the service is down or critically degraded. P2 means a major feature is unavailable. P3 means a minor feature is impaired. P4 means an issue with minimal user impact. These classifications assume that severity correlates with error rates, latency, or throughput. AI incidents break this assumption. An AI system can be fully operational by traditional metrics and still be in a P1 incident because of output quality, safety violations, or reputational risk. You need severity classifications that capture the dimensions that actually matter for AI systems.

## Why Traditional Severity Fails for AI

Traditional severity classification answers one question: how many users are unable to use the service? If the answer is "all users," it is P1. If the answer is "some users," it is P2. If the answer is "very few users," it is P3. If the answer is "almost no users," it is P4. This works beautifully for traditional software because availability is the dominant failure mode. If users cannot access the service, nothing else matters.

AI systems fail in dimensions that do not map to availability. The service is available, but outputs are wrong. The service is fast, but outputs are biased. The service has zero errors, but outputs violate safety policies. The service passes all health checks, but outputs generate viral social media backlash. None of these failures produce traditional incident signals. They produce quality signals, safety signals, and reputational signals that your P1/P2/P3 classification was never designed to handle.

The mismatch creates decision paralysis. An engineer detects a quality degradation that drops the primary eval metric from 0.91 to 0.85. Is that P1? The service is still up. Is it P2? Most users are not reporting problems. Is it P3? The metric is still above the minimum acceptable threshold. The engineer does not escalate because traditional severity classifications provide no clear guidance. By the time the team realizes the degradation was serious, hours have passed and user impact has compounded.

Traditional severity also fails because it treats all dimensions as equally important. A P1 availability incident and a P1 performance incident both trigger the same response level even though their impact profiles are completely different. An AI system needs severity classifications that distinguish between quality incidents, safety incidents, compliance incidents, and reputational incidents, because each type requires different response urgency and different mitigation strategies.

## The Five Dimensions of AI Incident Severity

AI incident severity is multi-dimensional. An incident has severity on five independent axes: quality degradation, safety violation, compliance breach, reputational risk, and operational impact. An incident might be P1 on one axis and P4 on another. Overall severity is determined by the highest-severity dimension, not by averaging across dimensions.

**Quality degradation** measures how much worse outputs are compared to baseline. Minor degradation means the primary eval metric dropped by less than five percent and user-reported issue rates are slightly elevated. Moderate degradation means the primary eval metric dropped by five to fifteen percent and user-reported issue rates have doubled. Severe degradation means the primary eval metric dropped by more than fifteen percent or user-reported issue rates have increased by more than three times. Critical degradation means the primary eval metric has dropped below minimum acceptable threshold or user-reported issue rates indicate that a majority of outputs are problematic.

**Safety violation** measures whether outputs cause harm or violate safety policies. Minor safety issues involve edge cases where outputs are suboptimal but not harmful. Moderate safety issues involve outputs that are factually wrong in ways that could mislead users but are unlikely to cause direct harm. Severe safety issues involve outputs that give incorrect advice in high-stakes domains or expose potentially sensitive information. Critical safety issues involve outputs that actively recommend harmful actions, expose protected user data, or generate content that violates legal or ethical boundaries.

**Compliance breach** measures whether the incident creates legal or regulatory exposure. Minor compliance issues involve edge cases that technically violate policy but have minimal enforcement risk. Moderate compliance issues involve patterns that could trigger regulatory questions if discovered. Severe compliance issues involve clear violations of regulations like GDPR, HIPAA, or the EU AI Act that require disclosure. Critical compliance issues involve violations that expose the company to immediate legal action or regulatory penalties.

**Reputational risk** measures potential for viral negative attention. Minor reputational issues involve outputs that a small number of users find problematic but are unlikely to generate external attention. Moderate reputational issues involve outputs that could generate negative social media attention if shared widely. Severe reputational issues involve outputs that are being actively shared on social media or picked up by media outlets. Critical reputational issues involve outputs that have generated widespread negative coverage and require executive-level response.

**Operational impact** measures traditional availability and performance. Minor operational issues involve elevated latency or reduced throughput that affects user experience but does not block usage. Moderate operational issues involve partial service degradation where some features are unavailable. Severe operational issues involve service degradation that makes core functionality difficult to use. Critical operational issues involve complete service outages or performance so degraded that the service is effectively unusable.

## Multi-Dimensional Severity in Practice

You assess severity across all five dimensions independently, then set overall incident severity to the highest individual dimension severity. An incident with minor quality degradation, no safety issues, no compliance issues, no reputational risk, but critical operational impact is a P1 incident because of the operational dimension. An incident with moderate quality degradation, severe safety violations, no compliance issues, minor reputational risk, and no operational impact is also a P1 incident because of the safety dimension.

This approach prevents severity underestimation. An engineer cannot dismiss an incident as P3 because "the service is still up" when the incident has severe reputational risk. The incident commander cannot defer response because "quality is only slightly degraded" when the incident has critical safety violations. The highest-severity dimension determines response urgency regardless of how other dimensions look.

In March 2025, a healthcare AI had an incident that illustrates multi-dimensional severity. A prompt change caused the model to occasionally include medical advice that contradicted current treatment guidelines. Quality dimension: moderate degradation, eval score dropped from 0.89 to 0.84. Safety dimension: severe violation, incorrect medical advice could cause patient harm. Compliance dimension: severe breach, providing incorrect medical advice creates liability exposure. Reputational dimension: moderate risk, if patients shared screenshots on social media. Operational dimension: none, service was fully functional. Overall severity: P1, determined by the safety and compliance dimensions. The team rolled back the prompt change within twelve minutes of detection even though the quality degradation was only moderate, because safety and compliance severity demanded immediate response.

## Severity Assessment Under Uncertainty

The hardest part of AI incident severity assessment is that you often do not have complete information when you need to classify severity. You have detected a quality degradation, but you do not yet know if it causes safety violations. You have seen user reports of problematic outputs, but you do not yet know if those outputs violate compliance requirements. You have to assess severity based on incomplete signals and escalate appropriately.

The rule is: when severity is ambiguous across multiple dimensions, escalate to the higher severity until you have evidence to downgrade. If you detect moderate quality degradation and you cannot immediately rule out safety violations, treat it as severe on the safety dimension until you confirm that outputs are safe. If you see moderate reputational risk but the incident involves a regulated domain where compliance breaches are possible, treat it as severe on the compliance dimension until Legal confirms there are no violations.

This approach creates overescalation. You will have P1 incidents that turn out to be P2 after investigation. You will have P2 incidents that turn out to be P3. Overescalation is the correct tradeoff. The cost of overescalating a P2 to P1 is wasted responder time and heightened stress. The cost of underescalating a P1 to P2 is continued user harm, reputational damage, and potential regulatory action. When you are operating under uncertainty, err toward higher severity.

## Severity Escalation Triggers

Most AI incidents do not start at P1. They start at P3 or P4 and escalate as the team gathers more information. You need clear escalation triggers that tell responders when to escalate severity without waiting for perfect information.

**Escalate to P1 if:** the primary eval metric drops below minimum acceptable threshold, user-reported issue rate exceeds three times baseline, outputs violate safety policies in ways that could cause harm, the incident triggers mandatory legal or regulatory notification requirements, or the incident has generated viral social media attention or media coverage.

**Escalate to P2 if:** the primary eval metric drops by more than ten percent, user-reported issue rate exceeds two times baseline, multiple weak signals suggest safety issues even if not definitively proven, compliance concerns have been raised but not yet confirmed, or the incident has generated negative social media posts from more than ten users.

**Escalate to P3 if:** the primary eval metric drops by more than five percent, user-reported issue rate exceeds 1.5 times baseline, outputs show quality degradation that does not clearly violate safety or compliance policies, or the incident has generated isolated user complaints without broader pattern.

These triggers are not perfect. They will produce false positives. They will escalate incidents that turn out to be minor. But they prevent the failure mode where responders wait too long to escalate because they are trying to gather complete information. The escalation triggers give permission to escalate based on partial signals.

## Severity Downgrade Criteria

Just as incidents escalate, they can downgrade. You escalate to P1 based on initial signals, investigate further, and determine that the incident is actually P2. Or you escalate to P2, deploy a mitigation, and downgrade to P3 as quality improves. Downgrade decisions require more confidence than escalation decisions because downgrading means reducing response urgency and potentially releasing responders.

**Downgrade from P1 to P2 if:** the triggering metric has improved to above critical threshold, mitigation has been deployed and initial validation suggests it is working, safety or compliance concerns have been investigated and ruled out, and the incident commander has consulted with affected stakeholders and confirmed that reduced response urgency is acceptable.

**Downgrade from P2 to P3 if:** the triggering metric has returned to within ten percent of baseline, user-reported issue rate has dropped to near baseline levels, and enough time has passed since mitigation deployment to establish confidence that no secondary effects are emerging.

**Downgrade from P3 to P4 if:** the triggering metric has returned to baseline, user-reported issue rate has normalized, and the team has confirmed that no other dimensions are affected.

The key phrase in all downgrade criteria is "confirmed" or "investigated and ruled out." You do not downgrade based on absence of signal. You downgrade based on positive confirmation that the incident is less severe than initially assessed. If you escalated to P1 because you could not rule out safety violations, you do not downgrade until Safety has reviewed sample outputs and confirmed there are no violations. If you escalated to P2 because of elevated user reports, you do not downgrade until user report rates have measurably decreased.

## Cross-Functional Severity Assessment

Some severity dimensions require cross-functional input. Engineering can assess quality degradation and operational impact. Engineering cannot assess compliance breach or reputational risk without input from Legal and Communications. An AI incident that might be P2 from an engineering perspective might be P1 from a compliance perspective.

Build severity assessment into your incident response process as a cross-functional step. When an incident is detected, the engineering on-call assesses quality and operational dimensions immediately. If either dimension is severe or critical, escalate to P1. If not, but the incident involves domains where safety, compliance, or reputational concerns are possible, pull in the relevant stakeholders within fifteen minutes to assess those dimensions. Do not wait for scheduled business hours. Legal, Trust and Safety, and Communications need to be reachable during incidents even if they are not primary on-call responders.

A financial services company formalized this in their runbooks. Any incident involving outputs related to investment advice, account information, or transaction details requires Legal assessment within fifteen minutes. Any incident involving outputs that could be shared publicly requires Communications assessment within thirty minutes. These are hard requirements in the runbook. Engineering cannot unilaterally classify an incident as P2 and defer cross-functional review. The cross-functional assessment is part of the severity classification process, not a separate step that happens later.

## Severity Classification as Decision Framework

Severity classification is not just a label. It is a decision framework that determines response speed, responder assignment, communication requirements, and escalation paths. P1 incidents trigger immediate executive notification, full incident command structure, and public communication planning. P2 incidents trigger team-level response with management notification. P3 incidents can be handled by on-call engineers with manager awareness. P4 incidents are tracked but do not require dedicated responders.

If your severity classification system does not clearly drive different response patterns, it is not a useful system. The point of classification is to eliminate ambiguity about "how urgently should we respond to this." If an engineer sees a quality degradation and has to decide whether it warrants waking up the VP of Engineering, severity classification should answer that question definitively. If it does not, your classifications need refinement.

The teams that handle AI incidents best have severity classifications that are precise enough to drive clear action but flexible enough to account for the unique characteristics of AI failures. They do not force AI incidents into P1/P2/P3/P4 buckets designed for availability failures. They build classification systems that measure what matters for AI: quality, safety, compliance, reputation, and operations. Five dimensions, clear escalation triggers, and a rule that says "escalate to the highest severity dimension until proven otherwise." That is the classification system that prevents underresponse and enables fast, confident decision-making under uncertainty.

---

Next: 6.3 — Runbooks for Common AI Failure Scenarios
