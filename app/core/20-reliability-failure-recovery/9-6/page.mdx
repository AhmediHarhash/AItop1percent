# 9.6 â€” Game Days: Scheduled Chaos for AI Systems

In March 2025, a financial services company ran their first AI game day. They scheduled two hours on a Friday afternoon. The scenario: their primary model provider goes down. The team's job: keep the credit risk assessment system running. They switched to their backup provider. The system stayed up. They declared success and went home for the weekend.

Monday morning, users reported that 18 percent of assessments were flagged as high-risk when they should have been low-risk. The backup model had different calibration than the primary. The team had tested availability. They had not tested quality. They had practiced switching providers. They had not practiced validating that the switch worked correctly. The game day succeeded at the exercise. It failed at the goal.

Game days are scheduled chaos exercises where teams deliberately break their systems to practice responding. They are fire drills for production infrastructure. But unlike fire drills, the fire is real. The systems actually break. The team actually responds. The lessons are not theoretical. They are learned under pressure, in real time, with real tools. When done well, game days transform how teams think about reliability. When done poorly, they teach the wrong lessons and build false confidence.

## What Game Days Are

A game day is a time-boxed chaos experiment with a specific scenario, specific roles, and specific learning goals. It is not ad-hoc testing. It is not exploratory debugging. It is structured practice for incident response.

The scenario defines what breaks. Your vector database goes down. Your model API returns errors. Your embedding service times out. Your knowledge base becomes unavailable. The scenario is designed to test a specific failure mode that the team has prepared for but never experienced in production.

The roles define who does what. Incident commander coordinates response. Engineers debug and deploy fixes. Support handles user communication. Product decides whether to degrade features or keep them running with reduced quality. Each person knows their role before the game day starts. The game day tests whether those roles function under pressure.

The learning goals define success. A game day is not successful because the system stayed up. It is successful because the team learned something they did not know before. They discovered a missing runbook. They found a misconfigured alert. They realized their backup plan does not work. They practiced communication under stress. Success is measured by insights gained, not by problems avoided.

Game days should be realistic but not catastrophic. You break things in ways that could actually happen. You break them during controlled time windows when the team is ready. You set boundaries so the experiment cannot escape containment. You build kill switches so you can stop the chaos if it goes too far. The goal is to learn, not to create real incidents.

## Planning a Game Day

Planning starts weeks before the event. You choose the scenario. You define the scope. You communicate the plan to everyone involved. You prepare runbooks. You brief stakeholders. You set up monitoring. You schedule the time.

Scenario selection depends on your system's maturity. If you have never run a game day, start simple. Simulate a single service outage. If you have run several game days, escalate complexity. Simulate multiple cascading failures. Simulate failures that affect both infrastructure and data quality. Simulate scenarios that require cross-team coordination.

The best scenarios are based on real near-misses or postmortems from other companies. Your model provider had a regional outage last month. Simulate it. A competitor experienced embedding service rate limiting that cascaded into a full outage. Test whether you would handle it better. A public incident report described a stale cache causing incorrect responses. Recreate it. Reality-based scenarios feel urgent. They motivate preparation. Teams take them seriously.

Scope definition prevents chaos from escaping. You decide which environments are in scope. Staging always. Production sometimes, with controls. You decide which services can be affected. You decide how long the experiment runs. Two hours is typical. Four hours is ambitious. Never let a game day run indefinitely. Time pressure creates the stress that reveals how systems and people behave under real incidents.

Communication happens at multiple layers. The engineering team knows the game day is coming. They know the date and time. They do not know the exact scenario. This preserves realism. Support and product teams are warned that systems might behave oddly during the window. Customers are not told. If you are running chaos in production, users should not experience degraded service. If they might, you notify them in advance or run the game day during maintenance windows.

## Scenario Design for AI Systems

AI systems have failure modes that traditional systems do not. Game day scenarios must test these AI-specific risks.

Model provider outage is the simplest scenario. Your primary model provider returns 503 errors for all requests. The team must switch to a backup provider or degrade to a simpler model. The game day tests whether the failover works, whether the backup model has sufficient capacity, and whether quality remains acceptable after the switch.

Embedding service rate limiting is a more subtle scenario. Your embedding service starts returning 429 errors. Requests still work, but only slowly. The team must decide whether to queue requests, fall back to cached embeddings, or switch to a different provider. The game day tests whether rate limit handling is implemented, whether queuing scales, and whether the system recovers when the rate limit lifts.

RAG retrieval failure is a knowledge-layer scenario. The vector database becomes unavailable. Queries return no results. The model must respond without grounding. The team must decide whether to serve ungrounded responses with disclaimers, return error messages, or fall back to keyword search. The game day tests whether retrieval failures are detected, whether fallbacks exist, and whether users are informed when quality is degraded.

Fine-tuned model degradation is a data scenario. You deploy a corrupted version of a fine-tuned model. Responses are subtly wrong. Accuracy drops from 92 percent to 78 percent, but the system does not crash. The game day tests whether the team detects quality degradation, how quickly they respond, and whether rollback procedures work.

Multi-region failure is an infrastructure scenario. Your primary cloud region becomes unreachable. Traffic must fail over to a secondary region. The secondary region has stale model weights or incomplete knowledge base replication. The game day tests geographic failover, cross-region data consistency, and whether the team notices quality drift when switching regions.

Each scenario should have a clear inject point and a clear success condition. Inject: vector database returns errors starting at 2:00 PM. Success: system switches to fallback retrieval and continues serving users with acceptable quality. Without clear conditions, game days become aimless experiments. With clear conditions, they become focused learning opportunities.

## Roles During Game Day

Every participant needs a defined role. Undefined roles lead to chaos without learning.

The incident commander owns the response. They coordinate the team. They make go or no-go decisions. They declare when the incident is resolved. They ensure communication happens. In real incidents, the IC is often a senior engineer or SRE. In game days, rotating the IC role trains more people. Junior engineers learn command. Senior engineers learn delegation.

The engineer or engineering team investigates and fixes. They read logs. They check dashboards. They deploy changes. They test hypotheses. During game days, they practice incident response under time pressure. They learn which tools work and which do not. They discover gaps in their runbooks. They build muscle memory for diagnosis and mitigation.

The support liaison communicates with users. In production game days, they draft user-facing messages. They update status pages. They respond to tickets. In staging game days, they simulate user communication. They practice writing clear incident updates. They learn how to explain technical failures in user terms.

The product owner makes trade-off decisions. Should we disable the feature or serve degraded results? Should we wait for a full fix or deploy a partial mitigation? In real incidents, product decisions are rushed. In game days, product learns to make those decisions quickly with incomplete information. They practice balancing user experience against technical constraints.

The observer watches and takes notes. They do not participate in response. They document what worked and what did not. They capture lessons learned. After the game day, the observer leads the retrospective. Their detachment lets them see patterns the responders miss.

Role clarity prevents duplication and confusion. Everyone knows what they are responsible for. Nobody steps on anyone else's work. The team operates as a unit, not as individuals reacting independently.

## The Game Day Timeline

A typical game day runs two hours. The timeline is structured to maximize learning while minimizing disruption.

The first fifteen minutes are preparation. The incident commander briefs the team on the format. Observers ensure monitoring is ready. Engineers verify they have access to the right tools. The chaos engineer prepares to inject the failure. Everyone confirms they are ready. Then the exercise begins.

The next ninety minutes are response. At time zero, the failure is injected. Systems break. Alerts fire. The team responds. The incident commander coordinates. Engineers debug. Support drafts communication. Product makes decisions. The clock runs. Pressure builds. The team must resolve the incident before time expires.

During response, the chaos engineer escalates if the team resolves too quickly. You fixed the first failure. Here is a second one. The backup model provider also fails. How do you respond now? Escalation keeps the exercise challenging. It prevents the team from declaring victory prematurely. It tests depth of preparation, not just surface readiness.

The final fifteen minutes are wrap-up. The incident commander declares the exercise complete. The team stops responding. The chaos engineer reverts all injected failures. Systems return to normal. Everyone takes a breath. The immediate pressure is gone. The learning has just begun.

The retrospective happens the same day or the next day, never later. Memory fades. Details are lost. The retrospective reviews what happened, what worked, what did not, and what changes are needed. Action items are documented. Owners are assigned. Follow-through is tracked. Without retrospectives, game days are expensive theater. With retrospectives, they drive lasting improvements.

## Common Game Day Scenarios

Some scenarios work across nearly all AI systems. These are worth testing regardless of your architecture.

Primary model provider failure tests your most critical dependency. Can you switch providers? How long does it take? Does quality degrade? Do you have enough quota with the backup provider? This scenario reveals whether your multi-provider strategy is real or aspirational.

Embedding service timeout tests retrieval resilience. Embeddings slow to fifteen seconds per request. Does your system time out gracefully? Do you have fallbacks? Can you serve users without real-time embeddings? This scenario exposes tight coupling between embeddings and user experience.

Partial datacenter failure tests geographic distribution. Half your infrastructure becomes unreachable. Traffic shifts to the remaining half. Does the remaining capacity handle the load? Do you have cross-region data replication? This scenario shows whether your redundancy actually provides resilience.

Model quality regression tests detection mechanisms. You deploy a model that returns subtly wrong answers. Do your quality monitors catch it? How long does it take to notice? Can you roll back quickly? This scenario validates that quality monitoring works in practice, not just in theory.

Each of these scenarios has caused real incidents at real companies. Practicing them in game days ensures your team is prepared when they happen for real.

## Learning from Game Day Failures

Game days succeed when they reveal gaps. The goal is not flawless response. The goal is finding weaknesses before production does.

Common findings: runbooks are out of date. They reference commands that no longer work or services that have been renamed. Engineers waste time figuring out the correct procedure. Post-game day, the runbooks are updated.

Monitoring is incomplete. The failure happens, but no alert fires. Engineers discover the problem only when users complain or someone checks dashboards manually. Post-game day, new alerts are added.

Failover mechanisms are misconfigured. The backup provider is configured, but the API key expired. The fallback model is selected, but it has insufficient quota. Engineers fix the configuration under pressure. Post-game day, these configurations are tested regularly.

Communication is unclear. Engineers know what is happening, but support does not. Support cannot update users because they do not understand the technical details. Product cannot make decisions because they do not know the options. Post-game day, communication templates are written and roles are clarified.

Game days that go perfectly are suspicious. Either the scenario was too easy, or the team is not looking hard enough for problems. The best game days are messy. They expose cracks in systems and processes. They create discomfort. That discomfort drives improvement.

## Game Day Frequency and Evolution

One game day per quarter is a reasonable starting cadence. It is frequent enough to build muscle memory but not so frequent that the team stops taking it seriously. As the program matures, frequency can increase.

Each game day should be harder than the last. Your first game day tests a single service failure. Your second tests cascading failures. Your third introduces time pressure: you must resolve the incident in thirty minutes or the system goes fully down. Your fourth adds communication complexity: you must brief executives while responding to the incident. Evolution keeps game days challenging as the team's skills improve.

Scenarios should rotate. Do not test the same failure twice in a row. Test model failures, then retrieval failures, then infrastructure failures, then quality regressions. Rotation ensures the team is prepared for multiple failure modes, not just one they have practiced repeatedly.

Post-game day action items should be completed before the next game day. If you discovered missing alerts, add them. If you found broken runbooks, fix them. If you identified gaps in training, fill them. Game days are wasted if lessons learned are not applied.

Over time, game days build a culture of resilience. The team stops fearing failure. They start expecting it. They build systems that anticipate problems instead of reacting to them. Incidents become less frequent and less severe. When incidents do happen, response is faster and smoother. Game days are the practice that makes real incidents survivable.

The ultimate test of resilience is not staging or scheduled exercises. It is production. The next step is learning how to run chaos experiments safely in live systems where real users depend on every response.
