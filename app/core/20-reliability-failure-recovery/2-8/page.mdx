# 2.8 — Provider-Side vs Application-Side Detection

The Claude API is returning errors. Your application is broken. Or is it? Your application might be fine. Claude might be down. Or Claude might be up, but your request format is wrong. Or Claude changed their API, and your integration is now incompatible. Or your rate limits are exhausted. Or their rate limits are exhausted. Or your authentication is failing. Or their authentication service is failing. The symptom is identical — your requests fail — but the causes, the responsibilities, and the mitigations are completely different.

This is the provider-versus-application problem. When you build on third-party AI providers, failures can originate from two different systems. You need detection that distinguishes between them, attributes blame correctly, and routes incidents to the right teams with the right context. Get this wrong and your engineering team wastes hours debugging your code when the provider is down, or your support team tells customers the provider is down when your code is broken.

The difficulty is that most failures involve both sides. Pure provider outages are obvious — their status page is red, their API is unreachable. Pure application bugs are obvious — you just deployed, and requests immediately fail. But the vast majority of failures are hybrid. Your code is technically correct but incompatible with a provider change. Your authentication works but hits a new rate limit. Your requests succeed but receive degraded responses. These hybrid failures require layered detection that examines both systems.

## Provider Failures: What You Do Not Control

Provider failures come in five forms. Complete outages, where the API is unreachable. Partial outages, where some endpoints work and others fail. Degradation, where requests succeed but latency or quality degrades. Rate limit exhaustion, where you hit provider-imposed throughput caps. And silent model changes, where the provider updates the model without announcement and behavior shifts.

Complete outages are the easiest to detect. Your requests time out or return 503 errors. The pattern is obvious: zero successful requests, all timeouts or connection failures. Your detection is simple health checks to the provider API. If your health check fails, the provider is down. You do not need sophisticated monitoring. You need a basic reachability test that runs every thirty seconds.

Partial outages are harder. The provider's text generation endpoint might work while their embedding endpoint fails. Or US region works while EU region fails. Or small requests work while large requests fail. If your application uses multiple endpoints or multiple regions, you need per-endpoint, per-region health checks. Aggregate availability is not enough. You need to know exactly which components of the provider service are affected.

Degradation is hardest. The requests succeed, but they are slow or low-quality. Latency goes from 800 milliseconds to 3 seconds. Refusal rate goes from 2 percent to 12 percent. Hallucination rate doubles. The provider status page shows green. Your application is receiving responses. But your user experience is broken. This requires comparing provider behavior to historical baselines and alerting when metrics drift beyond acceptable ranges.

Rate limit failures are distinctive. You receive 429 status codes or explicit rate limit messages. But the cause can be either provider-side or application-side. Provider-side means the provider lowered your rate limits without warning or is enforcing new limits. Application-side means your traffic increased and you hit existing limits. The HTTP response is the same, but the diagnosis and mitigation are different. Provider-side requires negotiation or provider change. Application-side requires traffic shaping or scaling your quotas.

Silent model changes are the most insidious. The provider updates the model behind your API key. Your code is unchanged. Your requests succeed. But responses change. A classification model becomes more conservative. A generation model changes tone. A summarization model changes verbosity. You detect this the same way you detect your own regressions — quality metrics, eval suite results, user feedback. But the root cause is external, and the mitigation is prompt tuning or version pinning, not code fixes.

## Application Failures: What You Control

Application failures originate in your code, your infrastructure, or your integration logic. Prompt construction bugs, where you generate malformed requests. Integration errors, where you mishandle responses or retry logic. Data pipeline failures, where context passed to the model is incomplete or corrupt. Authentication failures, where your credentials expire or are misconfigured. And resource exhaustion, where your infrastructure cannot handle the load.

Prompt construction bugs are application failures that look like model failures. You concatenate user input without escaping. You exceed token limits. You send empty prompts. You construct JSON that is not valid. The provider rejects the request or returns an error, but the error is your fault. Detection requires validating your prompts before sending them and logging malformed prompts when the provider rejects them.

Integration errors are logic bugs in how you call the provider API. You do not handle pagination correctly. You retry on errors that should not be retried. You time out too quickly or too slowly. You parse responses incorrectly. The provider is working fine. Your integration is broken. Detection requires testing your integration logic against provider documentation and monitoring for patterns of failed requests that follow code changes.

Data pipeline failures corrupt the context you send to the model. You retrieve documents from your knowledge base, but some are empty or truncated. You fetch user history, but it is stale. You pull entity data, but the database query times out. The model receives incomplete information and produces poor responses. The provider is fine. Your data pipeline is broken. Detection requires monitoring the quality and completeness of context before it reaches the model.

Authentication failures are transient but critical. Your API key expires. Your token refresh logic fails. Your credentials are rotated but your application does not pick up the new credentials. Requests fail with 401 or 403 errors. This is not a provider outage. This is your configuration failing. Detection requires monitoring authentication error rates separately from other errors and alerting when they spike.

Resource exhaustion happens when your infrastructure cannot keep up. You run out of memory while processing responses. Your API gateway hits connection limits. Your retry queue overflows. The provider is fine. Your application cannot handle the load or handle the error conditions the provider returns. Detection requires infrastructure monitoring — memory, CPU, connection pools, queue depths — alongside application monitoring.

## Hybrid Failures: Responsibility Depends on Context

Most failures are hybrid. The provider changes their API. Your code is technically correct for the old API but breaks on the new one. The provider introduces a new rate limit. Your traffic pattern was acceptable before but now exceeds limits. The provider updates their model. Your prompts worked on the old model but fail on the new one. Whose fault is this?

The answer matters because it determines response. If the provider broke backward compatibility, you file a support ticket and potentially demand compensation. If your code assumed undocumented behavior, you fix your code. If both sides share responsibility, you fix what you can and negotiate with the provider on the rest. Fast blame attribution prevents misdirected effort.

Hybrid failure detection requires version tracking and change correlation. Track the provider's API version, model version, and any announced changes. Track your application version and deployment history. When failures occur, correlate them with recent changes on either side. If failures started immediately after a provider model update, the model update is the likely cause even if your code did not change. If failures started immediately after your deployment, your deployment is the likely cause even if the provider claims no changes.

This is harder than it sounds because providers do not always announce changes. They might update a model behind a stable version identifier. They might change rate limiting behavior without documentation updates. They might roll out infrastructure changes that affect latency without user-visible announcements. You cannot rely on provider communication. You need to detect provider-side changes through behavior observation.

The practical approach is baseline comparison. Continuously monitor provider behavior — latency, error rate, refusal rate, response length, response quality — and alert when it drifts from baseline even if your code did not change. If the drift correlates with a deployment, blame your deployment. If the drift happens with no deployment, blame the provider. If both changed simultaneously, blame is ambiguous and requires manual investigation.

## Provider Health Probes: Building Your Own Monitoring

Provider status pages are not enough. They update slowly, they cover only complete outages, and they do not reflect your specific usage patterns. A provider might show green on their status page while experiencing degradation for your specific use case. You need your own health probes that test the provider from your application's perspective.

A health probe is a synthetic request that runs continuously and tests the provider's behavior. It should resemble your real traffic — same endpoints, same request size, same complexity. It should run from the same infrastructure your application runs on, so it tests network paths and authentication the same way. It should run frequently enough to detect failures within your detection SLO — every thirty seconds if you need one-minute detection, every five minutes if you need ten-minute detection.

The probe should measure latency, error rate, and quality. Latency is straightforward — how long the request takes. Error rate is straightforward — whether the request succeeds or fails. Quality requires evaluating the response. For classification, check whether the classification is correct on a known test case. For generation, check whether the response meets basic quality criteria — length, coherence, lack of refusals. The health probe is a mini-eval running continuously in production.

When the health probe fails, you know the failure is provider-side or network-side. When the health probe succeeds but real traffic fails, you know the failure is application-side or specific to certain request patterns. This is fast blame attribution. You do not wait for your users to tell you the provider is down. You do not waste engineering hours debugging your code when the provider is broken. The health probe gives you ground truth about provider behavior independent of your application's behavior.

Some teams run multiple health probes targeting different provider capabilities. One probe for text generation, one for embeddings, one for multimodal input. One probe for short requests, one for long requests. One probe from each region you serve. This granularity helps during partial outages. If your US probe fails but your EU probe succeeds, you know the failure is region-specific. If your generation probe fails but your embedding probe succeeds, you know the failure is endpoint-specific.

## Blame Attribution Architecture

Fast blame attribution requires telemetry that separates provider behavior from application behavior. This means structured logging of every provider interaction: request sent, response received, latency, status code, error message, request ID. This means metrics that segment by provider response type: 2xx success, 4xx client error, 5xx server error, timeout. This means correlation with provider health probe results: real traffic failing while probes succeed indicates application issue.

When an incident occurs, your runbook should immediately show blame attribution. If real traffic fails and probes fail, the provider is down. If real traffic fails and probes succeed, your application is broken. If real traffic succeeds for some patterns and fails for others, you have a traffic-dependent failure that requires deeper investigation. The on-call engineer sees this context within seconds of the alert firing.

Blame attribution is not about avoiding responsibility. It is about routing the incident correctly. Provider incidents escalate to your provider relationship manager or your platform team. Application incidents escalate to your application engineers. Hybrid incidents escalate to both with context about what each side needs to investigate. Routing incorrectly wastes time and delays mitigation.

This requires organizational process. Your provider contract should include SLAs and escalation paths. Your incident response runbooks should include provider escalation procedures. Your on-call rotation should know when to escalate to the provider and when to fix internally. Your monitoring dashboards should show provider health alongside application health. Blame attribution is both technical and organizational.

## The Mental Model for On-Call

When an alert fires, the on-call engineer's first question should be: provider issue or application issue? The answer comes from three signals. First, did anything change on our side — deployments, configuration, traffic patterns? Second, are health probes passing or failing? Third, are errors consistent across all traffic or specific to certain patterns?

If nothing changed on your side, probes are failing, and errors are universal, it is a provider issue. Escalate to the provider and implement fallbacks. If something just deployed on your side, probes are passing, and errors correlate with the deployment, it is an application issue. Rollback or hotfix. If nothing changed, probes are passing, and errors are traffic-specific, it is a hybrid issue requiring deeper investigation.

This mental model is only possible if you have built the instrumentation to answer these questions. Without provider health probes, you cannot distinguish provider failures from application failures. Without deployment tracking, you cannot correlate failures with changes. Without traffic segmentation, you cannot identify pattern-specific failures. The monitoring architecture must support the diagnosis workflow.

The provider is a dependency, but it is an opaque dependency. You cannot debug their infrastructure. You cannot hotfix their code. Your only control is detection, escalation, and mitigation on your side. Make detection fast and attribution accurate. Your time-to-mitigation depends on it.

---

Next, we examine compound incidents — failures that involve multiple components simultaneously, complicating both detection and diagnosis.
