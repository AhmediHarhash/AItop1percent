# 6.7 — When You Do Not Know If It Is Broken: The Uncertainty Response

In March 2025, a legal research AI platform received three user reports over 90 minutes describing outputs as "slightly off." The on-call engineer reviewed the flagged queries. The outputs looked plausible. Precision metrics were stable. Latency was normal. Retrieval scores were within expected ranges. But the users who flagged the issues were senior attorneys who used the tool daily and had developed an intuition for when results felt wrong. The engineer spent 40 minutes investigating and found no technical anomaly. No deployment in the last week. No infrastructure changes. No obvious input shift. The question was simple and unanswerable: is the system broken or not?

The engineer escalated. The team pulled two more engineers into a war room. They manually reviewed 200 recent queries and compared outputs to expected behavior. After three hours, they identified the issue: a legal database vendor had updated case law metadata the previous night, and the updated metadata shifted how documents ranked in retrieval. The model was working correctly. The retrieval was working correctly. But the retrieval was returning subtly different documents than before, and those documents led to outputs that felt wrong to experienced users even though they were technically accurate. The system was not broken in any measurable way. But it was also not working the way it had been the day before. The fix required re-tuning retrieval weights and took six hours.

This is the uncertainty response: when you genuinely do not know if the system is broken, but you also cannot confidently declare it is working. The metrics say it is fine. Some users say it is not. Your diagnostics find nothing. But the intuition that something is wrong does not go away. This scenario is not rare. It happens when AI systems interact with external data sources that update silently, when user expectations shift faster than your metrics capture, when failure modes are too subtle to detect with automated monitoring but clear enough to frustrate experienced users. The uncertainty response is the protocol for acting when you cannot determine whether an incident is actually happening.

## Structured Uncertainty Assessment

The first step in uncertainty response is structured assessment. You do not just say "I do not know." You systematically document what you do know, what you do not know, and what evidence would resolve the uncertainty. You create a written assessment in the incident channel that other engineers can review and critique.

The assessment answers four questions. First: what is the observable signal? Not "users say it feels off" but "three reports from experienced users in 90 minutes, describing outputs as less relevant." Second: what have you checked? Not "I looked at some stuff" but "checked precision, recall, latency, recent deploys, input distribution, retrieval scores." Third: what is inconsistent? Not "nothing obvious" but "metrics show no degradation, but user reports are from credible sources who use the tool daily." Fourth: what hypotheses remain? Not "no idea" but "possible retrieval shift, possible external data update, possible model provider change, possible emerging input pattern not covered by eval."

This structured assessment forces clarity. It prevents the vague unease of "something might be wrong" from paralyzing the response. Once the assessment is written, it becomes a shared artifact that other engineers can evaluate. They might see a hypothesis you missed. They might have context about a recent external change that explains the signal. They might confirm that the signal is too weak to justify response. The assessment turns individual uncertainty into collective decision-making.

## Gathering Additional Signal Without Disrupting Service

When you are uncertain, you gather more data. But you do it without disrupting service. You do not deploy experimental monitoring. You do not enable verbose logging that triples your log volume and costs. You do not push a canary build that might make things worse. You work within the observability you already have and you focus on manual inspection.

You pull a statistically significant sample of recent requests. Not ten requests. At least 100. You manually review the inputs and outputs. You compare them to baseline behavior — outputs you know were good. You look for patterns. Are there categories of queries where outputs feel less relevant? Are there specific entity types or document types that seem to be handled differently? Are there users or user segments where satisfaction appears lower? Manual inspection is slow, but it surfaces patterns that automated metrics miss.

You also re-run your eval suite against current production behavior. Even if your monitoring shows no eval degradation, run the suite again with fresh eyes. Look at which individual test cases are passing and which are failing. Are there any test cases that used to pass reliably and now fail intermittently? Are there any that pass but feel borderline? The eval suite is your diagnostic instrument during uncertainty. It gives you a controlled environment to test hypotheses without touching production.

If you have access to the users who reported the issue, you engage them directly. You ask them to submit more examples of queries that feel wrong. You ask them to describe what they expected versus what they got. You ask whether the issue is consistent or intermittent. User feedback during uncertain incidents is gold. These are the people who detected the signal before your monitoring did. They have information you do not have.

## Conservative Versus Aggressive Response Postures

When you are uncertain whether the system is broken, you choose a response posture. Conservative posture: you assume the system might be broken and you reduce exposure while you investigate. Aggressive posture: you assume the system is working and you continue normal service while you investigate. The choice depends on the cost structure and the strength of the signal.

Conservative posture means activating partial containment. You do not shut down the system, but you reduce risk. You might enable additional human review for high-stakes outputs. You might shift 20% of traffic to a fallback to compare behavior. You might add a disclaimer to the UI indicating that the system is under review. You might throttle usage for new users while continuing service for established users. The goal is to limit the damage radius if it turns out the system is actually broken, while still serving most users and gathering production data to resolve uncertainty.

Aggressive posture means maintaining full service but increasing monitoring intensity. You do not change the system behavior, but you watch it more closely. You set up a live dashboard that updates every five minutes. You assign an engineer to manually review a sample of outputs every 30 minutes. You create a fast-response escalation path so that if additional signal emerges, you can shift to containment within minutes. You are betting that the system is fine, but you are prepared to act quickly if you are wrong.

The decision between conservative and aggressive posture depends on three factors: the credibility of the signal, the cost of false containment, and the cost of missed real degradation. If the signal comes from experienced users with a track record of accurate reports, you lean conservative. If the signal is a single outlier report from a new user, you lean aggressive. If false containment means lost revenue or user frustration, you lean aggressive. If missed degradation means safety risk or regulatory exposure, you lean conservative. There is no universal rule. You make the call based on context.

## Time-Boxing Uncertainty

You cannot stay in uncertainty indefinitely. You time-box it. You give yourself 60 to 90 minutes to gather additional signal and resolve whether there is a real issue. If you reach clarity — either confirming the system is broken or confirming it is fine — you act accordingly. If you do not reach clarity, you escalate or you choose a response based on the best available evidence.

The time box is enforced by a decision checkpoint. At the 60-minute mark, you post an update: "We have spent 60 minutes investigating uncertain quality signal. Current status: no technical anomaly found, but three credible user reports persist. Decision: shifting to conservative posture, enabling human review for 10% of outputs, continuing investigation for 30 more minutes before escalating." The decision checkpoint forces you to synthesize your findings and choose a path forward, even if the path is just "investigate a little longer with additional safeguards."

Time-boxing prevents the scenario where uncertainty lasts for hours or days while the team debates whether to act. It also prevents the scenario where the team dismisses the uncertainty because they cannot resolve it and then a real incident surfaces a week later. You do not need perfect clarity. You need enough clarity to act. The time box forces you to act on whatever clarity you have.

## The Assume Broken Until Proven Otherwise Default

For high-risk systems, the default under uncertainty is to assume broken until proven otherwise. If you cannot determine whether the system is working correctly, you treat it as if it is not. You activate conservative posture, you reduce exposure, and you continue investigation under the assumption that there is a real issue you have not yet identified. You only restore full service once you have affirmative evidence that the system is working as expected.

This default is appropriate for systems where the cost of undetected degradation is high. A medical AI that might be giving subtly incorrect clinical recommendations cannot stay in "we are not sure if it is broken" status for long. You reduce risk first, investigate under reduced risk, and restore once you have confidence. The cost of false caution — degraded service while you investigate a non-issue — is acceptable because the cost of missed real degradation is catastrophic.

For lower-risk systems, the default might be the opposite: assume working until proven broken. You maintain full service, you gather data, and you only act if you find affirmative evidence of degradation. The cost of false caution — unnecessary containment, user frustration, lost engagement — outweighs the cost of a few bad outputs slipping through while you investigate. The choice of default posture is a risk management decision made before the incident, not during it.

## Escalation Paths for Persistent Uncertainty

If you reach the end of your time box and you still do not know if the system is broken, you escalate. You notify leadership, expand the response team, and shift to a higher-effort investigation. Persistent uncertainty is not something a single on-call engineer can resolve alone. It requires more people, more time, and potentially outside expertise.

The escalation includes a clear summary: "We have been investigating a potential quality issue for 90 minutes. Three experienced users reported outputs as less relevant. We found no technical anomaly in metrics, logs, recent deploys, or input distribution. We manually reviewed 150 recent outputs and saw no obvious degradation. We re-ran eval suite with no failures. Uncertainty persists. Requesting decision on response posture and assignment of additional engineers to investigation."

The escalation might result in a decision to activate conservative posture and continue investigation with more resources. It might result in a decision to dismiss the signal and close the incident. It might result in a decision to engage the users directly and ask them to help identify specific failure cases. But the decision is made at a higher level, with more context, and with accountability for the trade-offs.

## Documenting Uncertainty in Incident Records

Uncertainty incidents are documented differently from clear incidents. You do not write a root cause analysis that identifies a single technical failure. You write an investigation summary that captures the signal, the investigation steps, the decision process, and the outcome. The goal is to learn from uncertainty: what made the signal ambiguous, what additional observability would have resolved it faster, and whether the response posture was appropriate.

The documentation might read: "Received three user reports of reduced relevance over 90 minutes. Investigated for 75 minutes, found no technical anomaly. Activated conservative posture by enabling human review for 10% of outputs. Continued investigation identified a retrieval metadata shift from external vendor update. Root cause confirmed after three hours. Fix deployed at 14:40. Retrospective action: add monitoring for external data source updates, improve user feedback integration into incident detection."

This documentation is valuable because uncertainty incidents often reveal gaps in observability. If you could not determine whether the system was broken, that means your monitoring is incomplete. The incident is an opportunity to close the gap.

## Post-Uncertainty System Improvements

Every uncertainty incident should lead to a system improvement that makes the next similar incident less ambiguous. If user reports were the only signal, you invest in better user feedback integration. If external data changes caused the issue, you add monitoring for those external dependencies. If the failure mode was too subtle for your eval suite, you expand the eval suite to cover it. If the diagnostic process took too long, you add tooling to speed it up.

The goal is not to eliminate uncertainty. Some incidents will always have weak initial signals. The goal is to reduce the time to clarity. If an uncertainty incident takes 90 minutes to resolve, and six months later a similar incident takes 30 minutes because you improved observability, you have succeeded. Over time, your ability to respond to ambiguous signals improves because you learn from each ambiguous incident and close the observability gaps.

When you do not know if the system is broken, you do not freeze. You assess what you do know, you gather additional signal, you choose a response posture based on risk, you time-box investigation, and you escalate when uncertainty persists. You act on the best evidence you have, knowing that waiting for perfect clarity is itself a decision — and often the wrong one. Uncertainty is not an excuse for inaction. It is a condition that requires structured response.

---

Next: **6.8 — The False Positive Incident and Alert Credibility**