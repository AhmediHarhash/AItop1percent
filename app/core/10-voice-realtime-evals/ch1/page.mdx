# Section 10 â€” Voice & Real-Time Evaluation Systems

## Chapter 1

### Plain English

Voice and real-time evaluation answers this question:

**"Did the system behave correctly, naturally, and safely while time was flowing?"**

Not after.
Not eventually.

But **in the moment**.

In real-time systems:
- lateness is failure
- interruptions matter
- silence has meaning
- users don't wait

Voice AI is not judged like text AI.
It is judged like **a human conversation**.

---

### Why Real-Time Systems Need Special Evaluation

Traditional AI assumes:
- static input
- unlimited thinking time
- delayed output

Voice systems break all of that.

Voice systems are:
- streaming
- interruptible
- latency-sensitive
- stateful
- emotionally perceived

A system can be:
- correct
- grounded
- safe
and still be **bad** because it responded too late or awkwardly.

---

### What "Real-Time" Means in 2026

Real-time systems include:
- voice assistants
- call center agents
- live copilots
- streaming transcription
- interruptible agents

Key properties:
- partial inputs
- partial outputs
- overlapping turns
- strict latency budgets

Evaluation must respect **time**, not just content.

---

### Core Voice & Real-Time Quality Dimensions

Voice systems are evaluated across **temporal + conversational dimensions**:

- Latency
- Turn-taking
- Interrupt handling
- Speech recognition accuracy
- Response timing
- Conversational flow
- Error recovery
- Safety under pressure
- User experience continuity

These dimensions are not optional.

---

### 1) Latency (The Hard Gate)

**How long does the system take to respond?**

Latency includes:
- audio capture
- STT partials
- model reasoning
- tool calls
- TTS generation
- playback start

In 2026 benchmarks:
- >700ms feels slow
- >1.5s feels broken
- silence >2s feels like failure

Latency is a **release gate**, not a metric to average.

---

### 2) Turn-Taking Accuracy

**Does the system know when to speak and when to listen?**

Failures include:
- talking over the user
- long awkward pauses
- responding mid-sentence
- missing end-of-turn cues

Turn-taking is evaluated via:
- interruption rate
- false start rate
- missed turn rate

Good turn-taking feels invisible.

---

### 3) Interrupt & Barge-In Handling

**Can the system be interrupted gracefully?**

Humans interrupt naturally.
Voice AI must too.

Evaluation checks:
- does the system stop speaking?
- does it adapt context?
- does it resume correctly?

Ignoring interruptions breaks trust immediately.

---

### 4) Speech Recognition Quality (STT)

**Did the system hear correctly in real conditions?**

Evaluated across:
- accents
- noise
- overlapping speech
- fast speakers
- silence gaps

Metrics include:
- word error rate (WER)
- correction recovery rate
- misunderstanding recovery

STT quality directly impacts downstream reasoning.

---

### 5) Response Timing (Not Just Speed)

**Did the response arrive at the right moment?**

Too fast:
- interrupts the user

Too slow:
- breaks flow

Evaluation focuses on:
- conversational timing
- natural pauses
- context-sensitive delays

Timing is conversational intelligence.

---

### 6) Conversational Flow

**Does the interaction feel coherent over time?**

Includes:
- remembering context
- following topic transitions
- not repeating questions
- logical progression

Flow failures feel "robotic" even if content is correct.

---

### 7) Error Detection & Recovery (Voice-Specific)

**What happens when things go wrong?**

Failures include:
- misheard inputs
- dropped audio
- tool failures
- silence

Elite systems:
- acknowledge uncertainty
- ask clarifying questions
- recover smoothly

Pretending nothing happened is worse than admitting failure.

---

### 8) Safety Under Time Pressure

**Does the system remain safe when rushed or stressed?**

Voice systems face:
- emotional users
- urgency
- manipulation attempts

Evaluation includes:
- refusal correctness
- escalation behavior
- calm tone under stress

Safety must survive speed.

---

### 9) User Experience Continuity

**Does the system feel consistent across the session?**

Checks:
- voice stability
- tone consistency
- behavior predictability

Sudden personality or behavior shifts break immersion.

---

### Evaluation Units for Voice Systems

You do NOT evaluate per message.

You evaluate:
- full conversations
- time-aligned transcripts
- audio + text streams
- interruption events

Voice evals are **temporal traces**, not snapshots.

---

### Types of Voice & Real-Time Evaluations

#### 1) Synthetic Scenario Evals

Simulated calls with:
- scripted interruptions
- noise injection
- timing constraints

Used for:
- regression testing
- release gates

---

#### 2) Replay-Based Evals

Replaying real conversations against:
- new models
- new prompts
- new pipelines

Allows safe comparison without live risk.

---

#### 3) Live Shadow Evals

Running new versions silently alongside production.

Used to:
- measure latency
- detect regressions
- avoid user impact

---

#### 4) Adversarial Voice Evals

Designed to:
- confuse turn-taking
- stress latency
- provoke unsafe responses

Mandatory before enterprise deployment.

---

### Automated Metrics vs Human Judgment

Automation measures:
- latency
- interruption counts
- silence duration
- error rates

Humans judge:
- naturalness
- comfort
- trust
- conversational quality

Both are required.

---

### Voice Evals in Production Monitoring

Live monitoring includes:
- p95 / p99 latency
- barge-in success rate
- call completion rate
- escalation rate
- user hang-up rate

Voice failures are immediate and visible.

---

### Enterprise Expectations

Enterprises expect:
- predictable latency
- graceful failure
- professional tone
- compliance under pressure

One bad call can lose a client.

---

### Founder Perspective

For founders:
- voice evals protect brand
- reduce churn
- prevent viral failures
- enable confident scaling

Voice mistakes spread fast.

---

### Common Failure Modes

- evaluating text only
- ignoring silence
- averaging latency
- skipping interruption tests
- assuming STT is "good enough"

These failures kill voice products.

---
