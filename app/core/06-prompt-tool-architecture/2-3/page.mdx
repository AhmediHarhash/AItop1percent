# 2.3 â€” Constraint and Format Enforcement Techniques

A government contractor lost their $4.7 million AI transcription contract in October 2024 when their system produced outputs that violated security classifications. The team of twelve engineers had built a meeting transcription service using GPT-4 that achieved 97% accuracy on speech-to-text. The system worked beautifully for technical accuracy.

The failure was format compliance. Government contracts required specific output formats with classification headers, participant role designations, and redaction markers for sensitive information. The team had included these requirements in their prompts: "Please include classification headers and use [REDACTED] for sensitive information." The model usually complied, but on 3% of documents it omitted headers, used inconsistent redaction markers, or formatted participant labels incorrectly.

In government work, "usually" means failure. Every document must match exact specifications. A single missing classification header is a security violation. The team tried more explicit instructions, few-shot examples, and stronger language. Compliance improved to 97%, then 98.5%, but never reached 100%. They lost the contract in March 2025.

The winning competitor used structured output APIs with JSON schemas that enforced exact formats at the model sampling level. Format compliance reached 99.97%. The remaining 0.03% failures were caught by validation layers and regenerated. The difference was moving from suggesting formats to enforcing them mechanically.

## Output Format Specification: From Request to Requirement

Format specification in prompts asks the model to structure outputs in particular ways. You write "Respond in JSON format with fields X, Y, Z" and hope the model complies. This works most of the time with frontier models. Most of the time is not good enough for production systems.

The failure mode is probabilistic non-compliance. Models generate tokens based on probability distributions. Even with clear format instructions, there's always some probability the next token won't match your schema. That probability might be 0.1% or 5% depending on model quality and prompt clarity, but it's never zero.

Traditional approaches treat format specification as prompt engineering. You refine instructions, add examples, use emphatic language: "IMPORTANT: You MUST respond in valid JSON format." This reduces non-compliance but can't eliminate it because you're working within the model's sampling process, not controlling it.

Modern approaches use structured output APIs that constrain sampling to valid formats. OpenAI's structured outputs, Anthropic's JSON mode, and similar features modify the token sampling process to only allow tokens that produce valid structured data. If the schema requires a field named "confidence" with type float, the model cannot generate text that violates this.

The shift from prompting to enforcement is fundamental. Prompting influences behavior; enforcement guarantees it. For applications where format violations cause failures, enforcement is the only reliable solution.

## JSON, XML, and Structured Format Instructions

JSON is the most common structured format for LLM outputs. It's machine-readable, has clear syntax rules, and integrates easily with application code. Most structured output features target JSON specifically because it's so widely used.

When using prompt-based JSON requests, provide explicit schemas. Don't just say "respond in JSON." Specify exact field names, types, and structures: "Respond with JSON containing: question (string), answer (string), confidence (float between 0 and 1), sources (array of strings)." This reduces ambiguity and improves compliance.

XML works when you need hierarchical data with attributes and namespaces. It's less common than JSON but handles complex nested structures well. The same principles apply: specify exact tag names, nesting structure, and required attributes. "Format as XML with root element analysis, containing summary, details, and recommendations tags."

Markdown structured formats work for human-readable outputs with defined sections. "Use markdown with exactly these sections: Background, Analysis, Recommendations, Next Steps. Each section must start with ## heading." This creates parseable structure while remaining readable.

YAML appears occasionally for configuration-style outputs. It's human-readable and supports complex structures, but model compliance is often worse than JSON. Models have seen far more JSON in training data than YAML, so JSON requests work more reliably.

The format you choose should match your downstream parsing needs. If you're feeding output into JSON-expecting code, request JSON. If humans need to read it, markdown or structured text works better. Don't request JSON and then convert to text for display; just request the final format you need.

## Structured Output APIs: Enforcing Schemas at Sampling Time

Structured output APIs represent a paradigm shift. OpenAI introduced structured outputs for GPT-5 in August 2024. Anthropic added similar features to Claude Opus 4.5 in late 2024. These systems modify the token sampling process to guarantee schema compliance.

The mechanism is constrained decoding. Instead of sampling from the full token vocabulary at each step, the system restricts sampling to tokens that could lead to valid schema-compliant outputs. If the schema requires a boolean field and the model has generated the field name, only "true" and "false" tokens are allowed next.

This works because JSON and XML have formal grammars. The system can parse the partially generated output, determine what tokens would keep it schema-compliant, and restrict sampling to that set. It's computationally more expensive than unconstrained sampling but provides deterministic compliance.

Structured outputs still allow model creativity within the schema. The model chooses what values to generate; the system only ensures those values produce valid structure. If your schema has a "summary" field, the model generates whatever summary content it deems appropriate, but the output will definitely have a "summary" field with the correct type.

Adoption of structured output APIs should be default for any application requiring reliable format compliance. The cost overhead is minimal (typically 5-10% latency increase) and the reliability gain is enormous. Moving from 98% prompt-based compliance to 99.97% API-enforced compliance eliminates an entire class of production failures.

## Length Constraints: Controlling Output Verbosity

Length constraints specify how much text the model should generate. Common patterns include word counts, sentence counts, token limits, and character limits. These help control costs, fit outputs into UI elements, and ensure responses match user expectations.

Soft length constraints use prompting: "Respond in approximately 100 words" or "Keep your answer to 2-3 sentences." Models approximate these reasonably well. GPT-5 asked for 100 words typically generates 90-110. Claude Opus 4.5 tends slightly more verbose. Variation is high; you might get 75 words or 130.

Hard length constraints use max_tokens API parameters. Setting max_tokens to 150 absolutely prevents outputs longer than 150 tokens. This works for upper bounds but creates truncation risk. If the model needs 200 tokens to complete a thought and you cap at 150, you get an incomplete response.

The truncation problem requires careful handling. Check if responses hit the token limit. If they do, either regenerate with higher limits or add logic to handle incomplete outputs. A summarization system might detect truncated summaries and regenerate with 50% higher token limits.

Minimum length is harder to enforce. You can prompt "respond in at least 200 words" but models often ignore minimum bounds. If you need guaranteed minimum length, check output length and regenerate if it's too short. A content generation system might require minimum 500 words and automatically retry until achieving it.

Relative length constraints work better than absolute ones. "Summarize this article in about 20% of the original length" gives the model more flexibility than "exactly 100 words." The model can scale its output to content complexity while staying roughly within your ratio.

## Vocabulary Restrictions: Controlling Word Choice

Vocabulary constraints limit what words the model can use. Common cases include reading level restrictions, technical versus plain language, brand voice compliance, and content safety.

Reading level constraints specify target audiences: "Explain this in language a 10-year-old would understand" or "Write at an 8th grade reading level." Models approximate these reasonably well through word choice and sentence structure. Quantitative validation requires readability scoring (Flesch-Kincaid, Gunning Fog) on outputs.

Technical versus plain language affects whether the model uses jargon. "Avoid technical jargon" or "Use domain-standard terminology" shapes vocabulary selection. Testing this requires domain expert review. A medical writing platform might define lists of acceptable and forbidden terms, then validate outputs against them.

Brand voice constraints maintain consistency with company communication styles. "Use our brand voice: professional but approachable, avoid corporate jargon, favor active voice" encodes style guidelines. This works better with examples: "Here's an example of our brand voice: [sample text]. Match this style in your response."

Forbidden word lists prevent specific terms. "Never use these words: [list]" works moderately well in prompts but isn't guaranteed. A children's content platform discovered this when GPT-4 occasionally used forbidden words despite explicit prohibitions. They added post-processing filters to catch violations.

The most reliable vocabulary control combines prompting with validation. Prompt for desired vocabulary, then check outputs against your criteria. Readability scores, forbidden word detection, and jargon density metrics can all be automated. Regenerate outputs that fail validation.

## Negative Constraints: Never Mention X

Negative constraints tell models what not to do. "Never mention competitors," "Don't reveal pricing," "Avoid discussing politics," "Never provide medical diagnoses." These protect against specific failure modes but are notoriously unreliable.

The problem is that prohibition can increase salience. Telling the model "never mention X" puts X in the context, making X-related tokens more probable. This is the "don't think of a pink elephant" problem. Explicit prohibition sometimes backfires by priming exactly what you want to avoid.

Positive framing works better than negative. Instead of "don't mention competitors," try "focus exclusively on our product features." Instead of "never discuss pricing," use "redirect pricing questions to the sales team." Positive instructions give the model alternative behaviors rather than just forbidden ones.

A customer service chatbot team tested this in December 2025. Negative constraints: "Never promise refunds or discounts without manager approval." Compliance: 91%. Positive alternative: "For refund or discount requests, say 'Let me connect you with a manager who can discuss options.'" Compliance: 97%. The positive version gave the model a clear path forward instead of just a boundary.

Layered constraints work for critical prohibitions. Use prompt-level constraints, add validation checks, implement filtering as a final safety net. A healthcare chatbot that must never provide medical diagnoses uses three layers: prompt prohibitions, output classification to detect diagnosis-like responses, and keyword filtering for diagnostic terminology.

## Constraint Stacking: How Many Rules Can You Enforce

Every constraint you add competes for model attention and increases the probability of conflicts. A prompt with one constraint is easy to follow. A prompt with fifteen constraints creates competing priorities and edge cases where satisfying all constraints simultaneously becomes difficult or impossible.

Constraint interaction is the core problem. "Be concise" conflicts with "Provide detailed explanations." "Use simple language" conflicts with "Use technical terminology." "Be creative" conflicts with "Follow this exact template." The model can't simultaneously maximize conflicting objectives.

A legal tech company discovered constraint limits in July 2025 when they built a contract review system. The prompt included: specific format requirements, length limits, vocabulary restrictions, tone guidelines, citation requirements, confidence thresholds, and seven domain-specific rules. Outputs were inconsistent and often violated at least one constraint.

They reduced to five core constraints that aligned with each other: format (enforceable via structured output API), length (enforceable via token limits), citation requirements (verified via post-processing), confidence thresholds (validated programmatically), and one critical domain rule. Compliance jumped from 73% to 96%.

Prioritize constraints explicitly when you must use many. "In order of importance: 1) Valid JSON format, 2) Include all required fields, 3) Keep responses under 200 words, 4) Use professional tone." This helps the model resolve conflicts by following priority order. If constraint 3 and 4 conflict, constraint 3 wins.

Testing constraint limits for your specific use case is essential. Add constraints one at a time and measure compliance. Find the point where adding another constraint degrades overall compliance. That's your practical limit for that model and task.

## Format Enforcement in Multi-Turn Conversations

Multi-turn conversations complicate format enforcement. The first response might comply perfectly, but subsequent turns can drift as the conversation context accumulates tokens unrelated to format specifications.

Persistent format instructions in system messages help. Set format requirements once at the conversation start: "In every response, use JSON format with fields: role, message, next_action." This creates a baseline that applies to all turns.

Reinforcement at strategic points maintains compliance. Every 5-10 turns, inject a reminder: "Remember to format your response as JSON." This prevents gradual drift. A customer service platform does this automatically whenever conversation length exceeds 2000 tokens.

Validation and regeneration catches failures. After each model response, validate format compliance. If validation fails, automatically regenerate with stronger format emphasis: "Your response must be valid JSON. Generate again, ensuring strict format compliance." Most models correct format errors on retry.

Context window limits force truncation in long conversations, which can drop format instructions if they're only in early messages. System messages avoid this by persisting outside the truncation window. For APIs without system messages, keep format instructions in a separate context section that you reattach to each request.

Structured output APIs make multi-turn format enforcement trivial. If every request uses schema enforcement, every response complies regardless of conversation length or context complexity. This is another strong argument for API-level enforcement over prompt-level requests.

## Validation Layers: Catching Format Violations

Format enforcement is never 100% reliable, even with structured output APIs. Validation layers catch the rare failures and provide graceful degradation. Every production system needs validation between model output and downstream consumption.

Schema validation for structured formats uses standard libraries. For JSON, jsonschema validators check outputs against your specified schema. For XML, XSD validators do the same. These run in milliseconds and catch any format violations the model produced.

Content validation checks field values, not just structure. A valid JSON response might have schema-compliant structure but nonsensical values. "confidence": -0.3 is type-valid (it's a number) but semantically invalid (confidence should be 0-1). Validate value ranges, enum memberships, and cross-field consistency.

Fallback strategies handle validation failures gracefully. Options include: regenerate with stronger format emphasis, use a default/safe response, escalate to human review, or try a different model. Choose based on your application's error tolerance and latency requirements.

A financial analysis platform uses tiered fallbacks. First failure: regenerate once with enhanced format instructions. Second failure: switch to Claude from GPT-5 (different model might succeed where the first failed). Third failure: return cached analysis from previous run with a staleness warning. Fourth failure: escalate to human analyst. This ensures the system always produces output, degrading gracefully through increasing effort and latency.

Monitoring validation failure rates reveals prompt quality issues. If 5% of responses fail validation, your prompts or schemas need work. Rates above 1% suggest reliability problems. Rates below 0.1% are typical for well-designed systems with structured output APIs.

## Practical Constraint Implementation Patterns

Separate must-have constraints from nice-to-have preferences. Must-have constraints are enforced via APIs, validated programmatically, and cause regeneration on failure. Nice-to-have preferences are requested via prompts and you accept imperfect compliance.

For a customer email system, must-have constraints might be: valid JSON structure, all required fields present, character limit compliance. Nice-to-have preferences: professional tone, specific vocabulary, reading level. The system enforces the first set mechanically and prompts for the second set.

Use structured outputs for format, prompts for content. Let the API enforce that you get JSON with fields A, B, C. Use prompts to shape what content goes in those fields. This divides labor appropriately: the sampling constraint system handles structure, the model's generation handles content.

Document constraints in code and tests. Each constraint should have: a clear specification, examples of compliant and non-compliant outputs, validation code, and test cases. Treat constraints like features that need testing and maintenance.

Version constraints alongside prompts. When you change format requirements or add new constraints, version the change and test thoroughly before deploying. A breaking change to output format can break every downstream system that consumes the outputs.

The next subchapter explores self-consistency and majority voting techniques, examining how generating multiple responses and selecting the best improves reliability at the cost of increased computation.
