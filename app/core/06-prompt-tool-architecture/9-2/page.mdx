# 9.2 — Prompt Cost Attribution and Chargeback Models

A Series B enterprise software company launched an internal AI platform in June 2025, offering Claude API access to all product teams. Engineering built the platform, finance funded it from central IT budget, and teams used it freely. Within three months, monthly API bills hit $94,000. Finance demanded that product teams fund their own AI usage but had no way to attribute costs. No one had tracked which team made which API calls. The platform team spent six weeks building cost tracking infrastructure while finance temporarily froze all AI spending. Two product teams missed launch deadlines because their AI features were blocked. The root cause was treating AI infrastructure costs as unmeasurable overhead instead of attributable resource consumption.

Most teams think about AI costs wrong. They see the monthly Anthropic or OpenAI bill as a single number. They know AI is expensive but cannot answer basic questions. Which features cost the most? Which teams are driving spending? Are we getting value proportional to cost? Without answers, you cannot make informed decisions about where to invest in optimization or which experiments to fund.

## Why Cost Attribution Matters Before Costs Become a Problem

Your first month with AI features costs $400. This seems cheap. No one asks detailed questions. Your second month costs $1,200. Still manageable. By month six you are at $15,000 and finance wants explanations. You discover that 60% of spending comes from one experimental feature that three users tried. You have no way to allocate costs to the team that built it because you never instrumented cost tracking.

Cost attribution gives you three critical capabilities before costs become crises. First is visibility into spending patterns. You can see that your document summarization feature costs $0.08 per summary while competitive intelligence costs $2.40 per report. This tells you which features need optimization urgency and which have acceptable unit economics.

Second is accountability for consumption. When teams know their features will be charged for AI usage, they make different architectural decisions. They add caching. They choose smaller models for simple tasks. They implement rate limiting. Without chargeback, teams have no incentive to optimize because someone else pays the bill.

Third is capacity planning and budgeting. You can project that launching summarization to all users will increase monthly costs by $8,000. You can allocate AI budget across teams based on their planned features. You can identify when to negotiate volume discounts or committed usage contracts. None of this is possible when costs are a mysterious aggregate.

The enterprise software company that froze AI spending had none of these capabilities. They knew total cost but not cost drivers. They wanted teams to optimize but teams had no visibility into their own spending. The six-week delay building attribution infrastructure cost more in missed revenue than they would have spent building it proactively.

## What to Track for Every Prompt Execution

Cost attribution starts with tagging every API call with context. You need to know who made the call, why they made it, and how much it cost. This requires capturing six categories of metadata for every prompt execution.

First is organizational attribution. Which team owns the feature that generated this prompt? Which product or service? Which cost center in your finance system? You tag each API call with team_id, product_id, and cost_center so you can roll up costs accordingly. This enables chargeback where each team pays for their own consumption.

Second is feature attribution. Within a product, which specific feature generated this prompt? Your chat product might have document Q&A, code generation, and creative writing features. Each has different usage patterns and unit economics. Tagging prompts with feature_name lets you analyze costs per feature and identify optimization targets.

Third is user attribution. Which user triggered this prompt? Not for chargeback—users do not pay—but for usage analysis. You can identify that 5% of users generate 60% of prompts, suggesting power users or potentially abusive behavior. You can measure cost per active user and set budgets per user to prevent runaway spending.

Fourth is request attribution. Each prompt gets a unique request_id that ties it to your application logs. When you investigate high-cost requests, you need to connect API costs to application behavior. Maybe a bug is causing retry loops that execute the same prompt 50 times. Maybe certain user inputs trigger expensive prompt chains. Request IDs make these connections possible.

Fifth is model and parameter metadata. You log which model was used—Claude Opus 4, Claude Sonnet 4, Claude Haiku 4. You log input and output token counts. You log any premium features like extended context windows. This metadata lets you calculate exact costs per request using published pricing and identify opportunities to use cheaper models.

Sixth is business context. You tag prompts with any relevant business dimensions. Is this a free user or paid subscriber? Is this a trial account or enterprise customer? Is this a high-value feature or experimental prototype? These dimensions help you understand whether costs align with business value.

## Calculating Real-Time Costs Per Execution

Token counts translate to costs through published pricing. As of January 2026, Claude Opus 4.5 costs $15 per million input tokens and $75 per million output tokens. Claude Sonnet 4.5 costs $3 per million input tokens and $15 per million output tokens. If your prompt uses 2,000 input tokens and generates 500 output tokens with Opus, the cost is (2000 * $15 / 1M) + (500 * $75 / 1M) = $0.0675.

You need to calculate and store this cost for every execution. Most teams add a cost field to their prompt logs. After each API call completes, they calculate cost from token counts and model pricing, then write it to their database. This gives you a queryable record of spending over time.

Calculating costs in real-time enables two important capabilities. First is cost-aware feature logic. You can check if a user has exceeded their monthly AI budget before executing an expensive prompt. You can show users estimated costs before they submit a large document for analysis. You can implement different rate limits for high-cost versus low-cost operations.

Second is anomaly detection. You can alert when a single prompt exceeds a cost threshold. If a request costs $5 when typical requests cost $0.10, something is wrong. Maybe a bug is injecting huge context. Maybe a user is trying to abuse the system. Real-time cost calculation lets you catch these issues immediately instead of discovering them in next month's bill.

The implementation challenge is keeping pricing up to date. Model providers change prices occasionally. They introduce new models with new pricing tiers. They add features like prompt caching that change cost calculations. You need a pricing configuration system that is easy to update without code changes. Most teams use a database table or configuration file mapping model names to current input and output token prices.

## Aggregating and Rolling Up Costs

Individual prompt costs tell you what happened. Aggregated costs tell you patterns and trends. You need both levels of analysis. This means building a cost aggregation pipeline that rolls up individual executions to team-level, feature-level, and time-based summaries.

The simplest aggregation is daily spending by team. Each day you sum all prompt costs tagged with team_id and store the result. This gives you a time series showing how each team's spending evolves. You can see that the search team's costs doubled last week when they launched a new feature. You can see that the content team's costs dropped after they implemented caching.

More sophisticated aggregations break down by multiple dimensions. Costs by team and feature. Costs by model and feature. Costs by user tier and time of day. These multidimensional views help you understand drivers of spending. Maybe enterprise customers generate 10x more prompts than basic customers but use simpler prompts that cost less per execution. Maybe costs spike during business hours and drop at night, informing scaling strategies.

You also need anomaly detection on aggregated costs. Alert when daily spending exceeds expected ranges. Alert when a team's week-over-week costs increase by more than 50%. Alert when costs per user suddenly jump. These alerts help you catch problems early. A bug that doubles costs gets noticed in hours, not weeks.

Most teams implement cost aggregation as a scheduled job. Every hour or every day, a script queries prompt logs, calculates aggregates, and writes them to a reporting database. The reporting database feeds dashboards showing current spending, trends over time, and breakdowns by dimension. Engineers and finance both use these dashboards, engineering for optimization decisions and finance for budget management.

## Chargeback Models and Internal Accounting

**Chargeback** is the practice of billing teams for resources they consume. Instead of central IT paying all AI costs, each product team pays for their own prompt executions. This creates accountability and incentives for optimization. It also complicates accounting and can create perverse incentives if implemented poorly.

The simplest chargeback model is direct passthrough. Whatever a team's prompts cost in API fees, that amount gets charged to their budget. If the search team spent $12,000 on Claude API calls this month, finance charges their cost center $12,000. This is transparent and easy to understand. The downside is that teams pay variable costs they cannot perfectly predict, which complicates budgeting.

The second model is markup on usage. You charge teams 120% or 150% of actual API costs. The extra covers platform team costs—building infrastructure, providing support, managing vendor relationships. This is honest about total costs and funds platform sustainability. The downside is that it makes internal AI more expensive than teams using APIs directly, creating incentives to bypass your platform.

The third model is tiered pricing. Light usage is free up to a threshold. Medium usage gets charged at cost. Heavy usage gets charged at a premium to discourage excessive consumption. This encourages experimentation while preventing runaway costs. The implementation complexity is higher because you need to track usage thresholds and tier logic.

The fourth model is credits and budgets. Each team gets an AI budget in credits. They spend credits on prompts. Teams can request more credits through an approval process. This prevents surprise overruns and gives finance predictable costs. The downside is that teams may game the system, spending their full allocation even on low-value uses to avoid losing budget.

Most large companies use direct passthrough or markup models. Startups often use free tiers with thresholds to encourage adoption. Regulated industries sometimes use approval-based models to ensure governance. Your choice depends on company size, culture, and cost pressures. The worst choice is no chargeback—it creates a tragedy of the commons where everyone overconsumes because no one pays.

## Building Cost Optimization Incentives

Chargeback creates incentives but does not guarantee optimization. Teams need tools and knowledge to reduce costs. Your platform must provide both. This means building cost visibility into developer workflows and making optimization techniques easy to adopt.

Start with cost visibility in development. Your SDK should optionally log estimated costs for prompt executions in development environments. Engineers see that their new prompt costs $0.50 per execution before deploying it. They can experiment with different models or prompt structures and see cost impacts immediately. This feedback loop encourages cost-conscious design.

Provide cost optimization guidance as part of your platform documentation. Explain when to use Haiku versus Sonnet versus Opus. Show how prompt caching reduces costs for repeated content. Demonstrate how batch processing saves money compared to individual requests. Give concrete examples with cost calculations. Engineers will optimize if you make it easy.

Build cost optimization features into your platform. Automatic fallback from expensive to cheap models when quality is sufficient. Prompt caching enabled by default for cacheable content. Rate limiting that prevents accidental cost explosions. These features let teams get optimization benefits without implementing everything themselves.

Create cost leaderboards and incentives. Show which teams have the best cost per user or cost per feature engagement. Recognize teams that significantly reduce costs through optimization. Make cost efficiency a positive signal, not just about blame for high spending. This cultural approach works better than pure financial pressure.

The enterprise software company that froze spending eventually implemented tiered chargeback with optimization support. Teams get $2,000 monthly free for experimentation. Additional usage is charged at cost plus 30% markup. The platform team provides cost dashboards, optimization tooling, and best practices. Average cost per prompt dropped 40% within two months as teams adopted caching and model selection strategies.

## Budget Alerts and Spending Controls

Cost attribution enables spending controls. You can set budgets at team, feature, or user level and enforce them programmatically. This prevents cost surprises and gives teams confidence that experiments will not accidentally consume their entire quarterly budget.

Team-level budgets are the most common control. The search team gets $15,000 monthly AI budget. Your platform tracks their spending in real-time. At 50% of budget, you send an informational alert. At 80%, you send a warning that requires acknowledgment. At 100%, you either block additional requests or require explicit approval for overages. This gives teams visibility and control.

Feature-level budgets help manage experiments. A team wants to prototype a new AI feature but is not sure it will work. They allocate $500 to the experiment. Once the feature hits $500 in costs, it stops executing until the team explicitly increases the budget. This prevents runaway costs from failed experiments while allowing controlled exploration.

User-level budgets prevent abuse and align costs with value. Free users might get $1 worth of AI usage per month. Paid users get $10. Enterprise customers get unlimited usage or very high limits. When users hit limits, you show a message explaining how to upgrade. This creates a natural conversion funnel from free to paid tiers.

Implementing budgets requires real-time cost tracking and enforcement. You cannot wait for nightly batch jobs to update spending. Every prompt execution must check current usage against budgets and enforce limits. This adds latency—typically 10-50ms for a budget check. Most teams consider this acceptable overhead for cost control.

Budget alerts need escalation paths. An alert that no one sees is useless. Route alerts to team Slack channels and email distribution lists. For critical overages, page on-call engineers. For routine warnings, send daily digests. The alert mechanism should match the urgency and give recipients clear action items.

## Cost Analysis for Optimization Decisions

Cost attribution data guides optimization investments. You have limited engineering time. Which optimizations give the best return on effort? Cost analysis answers this question with data instead of guesses.

Start by identifying highest-spend features or prompts. If 40% of costs come from one feature, optimizing that feature has 10x more impact than optimizing something that represents 4% of spending. Look at absolute costs and growth rates. A small feature growing 50% month-over-month might be a better optimization target than a large stable feature.

Analyze cost per value metric. Calculate cost per active user, cost per engagement, cost per conversion. Compare across features. If feature A costs $0.02 per engagement and feature B costs $0.50 per engagement, feature B is the obvious optimization target. If feature A has higher engagement and revenue impact, maybe feature B should be reconsidered entirely.

Break down costs by component. In a RAG pipeline, how much do you spend on query rewriting versus retrieval versus synthesis? If synthesis is 70% of cost, optimize synthesis first. If retrieval costs are high because you are retrieving too many documents, improve filtering. Cost breakdown shows where effort matters.

Model and run experiments. You think switching from Opus to Sonnet for certain prompts will cut costs 75% with acceptable quality loss. Tag some requests with experiment flag, run both models in parallel, compare outputs and costs. If quality holds up, you have data to justify the migration. If quality degrades, you know to try different optimizations.

The enterprise software company used cost analysis to prioritize optimization. Their document summarization feature was highest spend at $31,000 monthly. Analysis showed 80% of cost was using Opus when Sonnet would work fine for documents under 10 pages. They implemented model selection based on document length. Costs dropped to $9,000 monthly for the same usage. Return on one week of engineering: $22,000 monthly savings.

## Integrating Costs with Business Metrics

AI costs only matter in context of business value. A feature that costs $50,000 monthly but generates $500,000 in revenue is a great investment. A feature that costs $500 monthly but no one uses is waste. You need to connect cost data with usage, engagement, and revenue metrics.

Build dashboards that show costs alongside business metrics. Track cost per active user and show it next to user retention rates. Track cost per conversion and show it next to conversion revenue. Track feature costs and show them next to feature engagement scores. These paired views help teams make informed tradeoffs.

Calculate unit economics for AI features. How much does it cost to acquire a user with AI-powered onboarding? How much does it cost to retain a user with AI-powered support? How much does AI content generation cost per piece versus human creation? These calculations inform build-versus-buy and feature investment decisions.

Set cost efficiency targets tied to business goals. Maybe your target is keeping AI costs under 5% of revenue. Maybe it is achieving cost per conversion below $2. Maybe it is maintaining cost per active user under $0.50. These targets guide optimization priorities and help teams understand what good looks like.

Review cost efficiency regularly in business reviews. Do not treat AI spending as pure cost. Treat it as investment with expected returns. Show how cost optimization efforts increased margins. Show how new AI features grew engagement faster than costs. This frames AI spending as strategic investment instead of budget problem.

## Building Cost Attribution Infrastructure

Cost attribution is not an afterthought. It is infrastructure you build before deploying production AI features. The first prompt you deploy gets tagged with team and feature IDs. The second prompt gets cost calculation. The third prompt gets budget enforcement. You build cost consciousness into your platform from day one.

This requires standardizing how teams integrate with your AI platform. Teams do not call model APIs directly. They use your SDK that automatically tags requests with context. The SDK calculates costs and checks budgets. The SDK logs everything for analysis. This centralization makes attribution automatic and consistent.

It also requires investing in cost analysis tooling. Build dashboards showing spending by team, feature, model, and time. Provide APIs that let teams query their own costs programmatically. Create reports that finance understands. These tools make cost data accessible and actionable across the organization.

The enterprise software company rebuilt their AI platform with cost attribution as a core feature. Their SDK requires team_id and feature_name for every request. Costs get calculated and logged automatically. Dashboards show real-time spending with drill-down by dimension. Teams can query costs through APIs and build monitoring into their own systems. Six months later, they have clear cost accountability, optimized spending, and confidence to expand AI usage because costs are predictable and controllable.

Cost attribution transforms AI spending from mysterious overhead into manageable resource consumption. You know what costs, why it costs, and whether it is worth it. Teams optimize because they see the impact. Finance supports AI investment because they understand the economics. This visibility is essential for scaling AI features sustainably.

The next subchapter covers how to respond to prompt and tool incidents, from classification and runbooks through post-incident review and prevention.
