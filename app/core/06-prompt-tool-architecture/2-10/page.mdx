# 2.10 â€” Retrieval-Augmented Prompting Patterns (Prompt-Side, Not RAG Pipeline)

A legal tech startup deployed a contract analysis AI in December 2025 that retrieved relevant clauses from a database of 340,000 legal documents. Their RAG pipeline worked well, returning accurate results from vector search. But their users complained that the AI would confidently cite clauses that did not exist in the retrieved documents, or worse, cite real clauses while completely misrepresenting their meaning. The hallucination rate on contract questions reached 23 percent despite retrieval precision exceeding 90 percent.

The engineering team investigated and found their prompt simply dumped retrieved text with a generic instruction: "Use the following context to answer the question." The model had no guidance about how to use the context, when context was insufficient, or how to handle conflicting information across sources. It freely mixed retrieved facts with generated reasoning, making hallucinations indistinguishable from grounded responses.

They redesigned their prompt structure with explicit retrieval handling. Context appeared in marked sections with source IDs. Instructions required citation of specific source IDs for every claim. The prompt instructed the model to state explicitly when context was insufficient to answer. Conflicting sources required explicit acknowledgment with quotes from each source. Within two weeks, hallucination rates dropped to 4 percent and user trust scores improved 34 percentage points.

The team learned that retrieval quality means nothing if your prompt does not tell the model how to use retrieved information properly.

## The Grounding Problem in Retrieved Context

Models have two knowledge sources: parametric knowledge from training and retrieved context from your system. Without explicit instruction, models blend these sources freely. You cannot distinguish which claims come from which source.

This blending creates hallucinations that sound authoritative. The model generates plausible-sounding information from parametric knowledge, formatted identically to information from your retrieved context. Users cannot tell the difference.

Retrieved context should dominate parametric knowledge for your domain. If your system retrieves product documentation, that documentation should be treated as ground truth even when the model's training included different information about your product.

The prompt structure determines whether models respect this priority. Clear demarcation of retrieved content, explicit instructions about source precedence, and formatting requirements for citations all influence grounding behavior.

You measure grounding through attribution accuracy. Can you trace every factual claim in the output back to a specific retrieved document? Poor attribution indicates weak grounding regardless of how good your retrieval pipeline is.

## Structuring Retrieved Context for Maximum Grounding

Place retrieved context in clearly marked sections distinct from instructions and the user question. Use XML-style tags, markdown sections, or other visual separators that make boundaries obvious.

Label each retrieved chunk with a unique identifier. "Source 1:", "Document A:", or "Chunk_42:" gives you handles to reference specific sources. The model can cite these identifiers in outputs, enabling verification.

Order retrieved chunks by relevance when possible. Place the most relevant information first. Models exhibit recency and primacy biases, paying more attention to content at the beginning and end of context. Put your best results where they receive maximum attention.

Include metadata with each chunk beyond just content. Source document title, publication date, section headers, and relevance scores provide additional grounding signals. This metadata helps models assess source credibility and relevance.

Preserve formatting from source documents when meaningful. If your retrieved content has headings, lists, or emphasis, maintain that structure. Formatting conveys semantic information that aids comprehension.

Truncate or summarize individual chunks to manageable size. A 5,000-token retrieved chunk overwhelms the model and wastes context window. Summarize long chunks to 200-500 tokens while preserving key information.

## Placement Strategies Within Prompts

Context placement relative to instructions and questions affects model behavior significantly. Different placements optimize for different outcomes.

The sandwich structure places context between instructions and questions. System instructions come first, establishing task framing and requirements. Retrieved context follows, providing information. The user question comes last, focusing attention on what to answer. This structure works well for question-answering tasks.

The context-first structure puts retrieved information at the beginning after brief task framing. This maximizes attention on your retrieved content before the model sees the question. Use this when grounding is more important than complex instruction following.

The context-last structure places retrieved content immediately before the output position. This works well for tasks requiring reasoning over context, as the information is freshest in the model's attention when generating responses. It risks lower attention on earlier instructions.

The interleaved structure alternates instructions with context. "Here is how to analyze contracts... Here is contract clause 1... Now extract key terms... Here is contract clause 2..." This works for complex multi-step tasks but adds structural complexity.

Test placement variations on your specific task. Optimal placement depends on task characteristics, model choice, context length, and instruction complexity. Empirical testing beats theoretical optimization.

## Source Attribution Instructions That Work

Require explicit citations for every factual claim. "Support all statements with citations in the format [Source N]" makes attribution mandatory rather than optional. Models follow explicit formatting requirements more reliably than vague guidance.

Provide citation examples in your instructions. Show the exact format you want: "According to the Q3 financial report [Source 1], revenue increased 23 percent." This few-shot demonstration of citation style improves compliance.

Distinguish between direct quotes and paraphrasing in citation formats. "Use [Source N, quoted] for direct quotes and [Source N, paraphrased] for restatements" enables verification of how closely outputs match sources.

Instruct models to state when information is not in the context. "If the context does not contain information to answer the question, state explicitly: 'The provided context does not address this question.'" This prevents gap-filling with parametric knowledge.

Require source-specific quotes alongside citations. "When citing, include a relevant quote from the source: 'According to Source 2: [quote]...'" This forces the model to ground claims in actual text rather than synthesizing across sources without attribution.

## Handling Conflicting Retrieved Sources

Conflicts between retrieved documents are common in real-world systems. Sources may contradict each other, provide different dates for events, or offer incompatible recommendations. Your prompt must handle this explicitly.

Instruct models to acknowledge conflicts rather than choosing one source silently. "If sources disagree, present both perspectives with citations: 'Source 1 indicates X, while Source 2 states Y.'" This transparency is more valuable than forced resolution.

Provide a precedence hierarchy when appropriate. "If sources conflict, prefer primary sources over secondary sources, and recent data over older data." This gives the model decision criteria for conflicts.

Require explicit quotes when presenting conflicts. "For conflicting information, quote each source: 'Source A says [quote]. Source B says [quote].'" This prevents the model from misrepresenting the nature of disagreement.

Distinguish between genuine conflicts and differences in scope. Sources might address different aspects of a question without truly contradicting each other. Instruct the model to synthesize when appropriate: "If sources address different aspects, integrate their perspectives."

Flag unresolved conflicts in outputs. When conflicts cannot be resolved within the prompt's guidelines, mark them for human review: "Note: Sources disagree on this point and human review is recommended."

## Context Window Allocation for Retrieval

Your context window is finite. Claude Opus 4.5 provides 200,000 tokens, but you must allocate these between instructions, retrieved content, conversation history, and output buffer.

Budget your token allocation explicitly. Reserve tokens for each component before filling them. Example budget: 2,000 for instructions, 20,000 for retrieved context, 10,000 for conversation history, 5,000 for output, leaving 163,000 margin for long conversations.

Prioritize retrieved content over long instructions. A 5,000-token instruction set with 1,000 tokens of context provides little information to ground on. Flip this: 1,000 tokens of focused instructions with 5,000 tokens of relevant context produces better grounding.

Implement dynamic allocation based on retrieval results. If retrieval returns three highly relevant chunks, allocate more tokens to context. If returns are marginally relevant, allocate fewer and note context limitations to the model.

Truncate conversation history before truncating retrieved context. For the current question, fresh retrieved context matters more than distant conversation turns. Keep recent turns and sacrifice older ones when token budgets tighten.

Monitor actual token usage in production. Log how many tokens each component consumes per request. Optimize your budget based on real usage patterns rather than theoretical estimates.

## Grounding Instructions That Reduce Hallucination

Explicit grounding instructions significantly reduce hallucination rates. The model needs clear guidance about the boundary between retrieved facts and generated inference.

Start with a strong framing instruction: "You must base your response solely on the provided context. Do not use outside knowledge for factual claims." This establishes context supremacy up front.

Define what counts as the provided context explicitly. "The context is everything between the CONTEXT_START and CONTEXT_END markers." This prevents ambiguity about what material is grounding versus what is instruction.

Require acknowledgment of knowledge boundaries. "If information is not in the context, say 'not addressed in provided context' rather than generating an answer." This explicit opt-out path reduces pressure to fill gaps.

Distinguish factual claims from reasoning. "You may reason about facts in the context, but all factual premises must cite sources." This allows inference while grounding facts.

Use confidence calibration instructions. "Express uncertainty when context is ambiguous or incomplete. Use phrases like 'the context suggests' or 'based on limited information' when appropriate." This produces more calibrated outputs.

## Multi-Document Retrieval Patterns

When retrieving from multiple documents, structure becomes crucial. Unorganized chunks from different sources create confusion about document boundaries and relationships.

Group chunks by source document. Present all chunks from Document A, then all from Document B, rather than interleaving based purely on relevance scores. This preserves document coherence.

Provide document-level metadata before each document's chunks. Include title, author, date, and relevance summary. This context helps the model assess each document's credibility and applicability.

Number chunks hierarchically to show provenance. "Doc1_Chunk1, Doc1_Chunk2, Doc2_Chunk1" makes source relationships clear. The model can see which chunks come from the same document.

Include document relationship information when available. "Document A is a later revision of Document B" or "Document C cites Document A" helps the model understand how sources relate.

Limit the number of source documents unless necessary. Ten documents with one chunk each is harder to process coherently than three documents with multiple chunks each. Prioritize depth over breadth when possible.

## Instruction Clarity for Context Usage

Vague instructions like "use the context" underspecify the task. Be explicit about how the model should engage with retrieved material.

Specify reading strategy for long contexts. "First scan all sources to identify which contain relevant information. Then extract specific details from those sources." This structured approach prevents the model from fixating on the first source.

Define synthesis expectations. "Synthesize information across sources to create a comprehensive answer" versus "Identify the single most relevant source and base your answer primarily on it." These are different tasks requiring different instructions.

Clarify handling of partial information. "If context answers part of the question but not all, answer what you can and explicitly note what is missing." This manages user expectations.

Provide examples of good and bad context usage. Show an example where the model correctly cites and grounds, and an example where it hallucinates or misattributes. This few-shot demonstration of the meta-task (using context correctly) improves compliance.

## Format Requirements for Verification

Structure output formats to enable easy verification of grounding. If humans or automated systems can quickly check that claims match sources, you catch errors faster.

Require claims and citations to be separable. "First state the claim, then provide the citation: [claim]. [Source N]" works better than "According to Source N, [claim]" because you can parse claims independently.

Use consistent citation formats throughout. Varying formats (sometimes footnotes, sometimes inline, sometimes at the end) make verification harder. Pick one format and enforce it.

Number or bullet separate claims when answers contain multiple points. "1. [First claim] [Source X]. 2. [Second claim] [Source Y]." This enumeration makes checking easier than prose paragraphs.

Include direct quotes for key claims when possible. "The report states [quote] [Source 3]" provides text you can directly verify against the source.

Generate structured outputs when the task allows. JSON or YAML formats with explicit "claim" and "source" fields make programmatic verification trivial.

## Handling Insufficient Context

Retrieval systems sometimes return poor results. Your prompt must handle cases where context does not contain the answer.

Explicitly permit "I don't know" responses. "If the context is insufficient, say so directly. Do not attempt to answer from general knowledge." This removes pressure to generate when grounding is absent.

Define insufficiency criteria. "Context is insufficient if it does not contain specific facts required to answer the question, only tangentially related information." This helps models recognize when context falls short.

Suggest alternatives when context is insufficient. "If you cannot answer from context, suggest what information would be needed: 'To answer this, I would need information about X.'" This is more helpful than a bare "I don't know."

Provide example insufficient-context responses. Show what good acknowledgment of limitations looks like: "The provided documents discuss Y, but do not contain information about X specifically." This models the desired behavior.

## Metadata-Enhanced Grounding

Retrieved chunks often have useful metadata beyond raw text. Source credibility, freshness, and relevance scores inform how the model should weight information.

Include relevance scores when available. "Source 1 (relevance: 0.94): [content]" tells the model which sources are most pertinent. It can prioritize highly relevant sources in reasoning.

Provide temporal information for time-sensitive content. "Source 2 (published: Jan 2026): [content]" helps models prefer recent information when appropriate and recognize potentially outdated content.

Include source type or credibility indicators. "Source 3 (official documentation): [content]" versus "Source 4 (user forum post): [content]" guides the model toward more authoritative sources when claims conflict.

Add section or context information about where chunks appear. "Source 5 (from: Limitations section): [content]" helps models understand whether information is central or peripheral.

Provide relationship metadata. "Source 6 (supersedes Source 2): [content]" explicitly indicates version relationships that affect which information to prefer.

## Production Debugging for Grounding Issues

When outputs show weak grounding, systematic debugging identifies whether the problem is retrieval, prompt structure, or instruction clarity.

Check retrieval quality first. Does retrieved context actually contain information to answer the question? If not, the problem is upstream of prompting. Fix retrieval before adjusting prompts.

Validate citation accuracy in failed cases. Do citations point to real sources? Do quoted passages appear in those sources? This identifies whether the model is fabricating versus misusing real sources.

Compare outputs with and without strong grounding instructions. If adding explicit citation requirements fixes the problem, your original prompt lacked clarity. If it does not help, retrieval quality or context structure might be the issue.

Test with perfect retrieval. Manually provide exactly the right context and see if grounding improves. This isolates prompt effectiveness from retrieval quality.

Analyze failure patterns by source characteristics. Do problems cluster with certain document types, lengths, or content styles? This reveals specific context structures that confuse the model.

Next, you will learn how to integrate tools into your prompt architecture, enabling models to take actions and access external systems reliably.
