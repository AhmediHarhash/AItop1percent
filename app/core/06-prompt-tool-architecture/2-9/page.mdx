# 2.9 â€” Conditional and Branching Prompt Logic

An enterprise software company built a customer support chatbot in November 2025 that handled 2,300 inquiries daily across billing, technical support, and account management categories. They used a single comprehensive prompt trying to handle all scenarios. The prompt sprawled to 3,400 tokens of instructions covering every possible case. Response quality was mediocre across all categories: 73 percent resolution rate, with users frequently complaining the bot misunderstood their needs.

A product engineer analyzed the failure patterns and found they clustered by intent category. Billing questions failed when the bot tried to debug technical issues. Technical questions failed when the bot focused on account status. The single monolithic prompt forced the model to consider irrelevant context and instructions for each specific user need.

She restructured the system with routing logic. A lightweight classifier identified whether the user needed billing help, technical support, or account management. Each category had a specialized 800-token prompt optimized for that domain. Resolution rate jumped to 89 percent within three weeks. Response latency dropped 40 percent because specialized prompts were shorter and more focused. The team had discovered that conditional routing to specialized prompts outperforms one-size-fits-all approaches.

The lesson: branching logic in prompt architecture is not optional for production systems handling diverse inputs. It is fundamental to quality and efficiency.

## Why Monolithic Prompts Fail at Scale

A single prompt handling multiple scenarios accumulates instructions for every edge case. Each new requirement adds lines to cover that specific situation. Within months, your prompt becomes an unmaintainable instruction manual.

These mega-prompts confuse models by providing conflicting guidance. Instructions for scenario A contradict instructions for scenario B. The model must guess which apply to the current input. This guessing introduces errors and inconsistency.

Token limits create hard constraints. Claude 3.5 Sonnet accepts 200,000 tokens of context, but your prompt competes with input data and conversation history for that space. A 4,000-token prompt leaves less room for the actual content you need to process.

Cognitive load matters even for models. While LLMs can technically process long contexts, attention degrades over distance. Instructions buried on line 2,847 of your prompt receive less attention weight than instructions near the input or output positions.

Maintenance becomes impossible as prompts grow. Multiple engineers edit different sections, creating inconsistencies. Version control diffs become meaningless when every change spans hundreds of lines. No one understands the whole prompt anymore.

## Routing Logic as Prompt Architecture

Routing logic classifies inputs into categories and directs them to specialized prompts. This separates concerns: one component decides what to do, others do it well.

The router can be a traditional classifier if you have labeled training data. Fine-tune a small, fast model on intent classification. This trades upfront labeling effort for low-latency, low-cost routing at inference time.

Zero-shot LLM routing works when you lack training data. Use a simple prompt asking the model to classify the input into predefined categories. Modern models excel at this task. The routing call is fast because it outputs only a category label.

Rule-based routing handles clear boundaries. If the input contains specific keywords, route to specific prompts. If input length exceeds thresholds, use prompts optimized for long documents. Rules are deterministic and debuggable.

Hybrid routing combines approaches. Use rules for obvious cases where logic is simple. Use LLM classification for ambiguous cases requiring semantic understanding. This optimizes for both cost and accuracy.

Implement fallback routing for unrecognized inputs. When your router cannot confidently classify an input, route to a generalist prompt or human escalation. Never let routing failures break the user experience.

## Designing Effective Router Prompts

Your router prompt should output structured, machine-parseable category labels. Request JSON output with a category field and confidence score. This enables downstream logic to act on the classification.

Define categories that align with meaningfully different processing strategies. "Technical support" and "billing questions" deserve separate prompts because they require different knowledge and output formats. "Short questions" and "long questions" might not, unless your processing actually differs.

Provide clear category definitions in the router prompt. Describe what distinguishes each category with examples. Ambiguous boundaries cause misrouting, which cascades into poor outputs from mismatched specialized prompts.

Include an "other" or "unclear" category for inputs that do not fit your taxonomy. This prevents forced misclassification when inputs are genuinely ambiguous or outside your system's scope.

Keep router prompts lightweight. The router runs on every request, so minimize tokens and complexity. You want routing decisions in milliseconds, not seconds. Save the heavy lifting for specialized prompts.

## Specialized Prompts for Different Input Types

Create specialized prompts for meaningfully different task characteristics. Questions versus commands. Short inputs versus long documents. Structured data versus natural language. Each type benefits from tailored instructions.

Short-input prompts can be more directive and example-heavy. With brief inputs, you have token budget for extensive few-shot examples and detailed instructions. These prompts compensate for limited input context with strong prior guidance.

Long-document prompts should emphasize structured processing. Instruct the model to scan systematically, extract key sections, and process incrementally. Long inputs overflow attention windows, so guide the model's focus explicitly.

Structured-data prompts need clear schema instructions. Specify exactly how to interpret each field. Provide examples showing edge cases in data formatting. Models often misinterpret structured inputs without explicit guidance.

Conversational prompts maintain context across turns. Include instructions about referencing previous messages, maintaining consistency, and detecting topic shifts. These multi-turn considerations do not apply to single-shot tasks.

Domain-specific prompts encode specialized knowledge. Medical, legal, financial, and technical domains each have terminology, reasoning patterns, and compliance requirements that justify specialized prompts rather than generic ones.

## Implementing If-Then Logic Within Prompts

You can embed conditional logic directly in prompt text. "If the input is a question, answer it directly. If it is a statement, acknowledge it and ask a clarifying question." These inline conditionals handle simple branching without external routing.

Use clear conditional markers that models recognize. "If... then..." and "When... do..." structures work reliably. Bullet lists with condition prefixes also work: "For questions: [instructions]. For commands: [instructions]."

Limit conditional depth to avoid confusion. One level of if-then logic works well. Nested conditions (if A then if B then if C) confuse models and produce unpredictable behavior. Complex logic belongs in external routing code.

Provide examples showing each branch. If your prompt has three conditional paths, include few-shot examples demonstrating each path. This shows the model how to execute branch logic correctly.

Test that models actually follow your conditionals. Many prompts include branch logic that models ignore, always taking one path. Validate on inputs triggering each branch and verify the model's behavior matches expectations.

## Decision Trees in Prompt Chains

Complex workflows require multi-step decision trees. The first prompt makes an initial classification. Subsequent prompts branch based on that classification. Each prompt in the chain makes progressively more refined decisions.

Design decision trees breadth-first. Start with broad categories at the root level. Branch into subcategories only when the distinction matters for processing. Avoid premature granularity that creates unnecessary complexity.

Pass classification metadata forward through the chain. Each prompt should receive the input plus all prior routing decisions. This context helps specialized prompts understand why they were invoked.

Implement confidence thresholds at branch points. If a routing decision is uncertain (confidence below threshold), route to a more general prompt or request human judgment. This prevents error propagation through the tree.

Track the path through your decision tree for debugging. Log which prompts were invoked for each input. When outputs fail, trace the routing path to find misclassifications or prompt mismatches.

Limit tree depth to 3-4 levels. Deeper trees create maintenance nightmares and multiply failure modes. If you need more depth, you probably need to redesign your category structure.

## Handling Edge Cases Through Branch Logic

Edge cases often require special handling that clutters general-purpose prompts. Branch logic lets you route edge cases to specialized handlers without contaminating your main prompts.

Create explicit branches for known edge cases. If certain input patterns consistently cause problems, detect them in routing and send them to prompts designed for those patterns specifically.

Use negative routing to catch exceptions. "If input contains personal information, route to PII handler. Otherwise, use standard processing." This keeps sensitive data handling isolated.

Implement fallback chains for errors. If a specialized prompt fails, retry with a more general prompt. If that fails, try an even more robust fallback. This graceful degradation prevents total failures.

Monitor which edge case branches activate frequently. If an "edge case" route handles 15 percent of traffic, it is not an edge case anymore. Promote it to a primary branch and optimize accordingly.

Document why each edge case branch exists. Link branch logic to specific incidents or requirements. This prevents accumulation of mysterious special cases no one understands.

## Dynamic Prompt Selection Based on Input Features

Select prompts based on measurable input characteristics beyond just categories. Input length, complexity, domain markers, and user context all inform prompt selection.

Length-based selection uses different prompts for different input sizes. Short inputs (less than 100 tokens) get concise prompts with quick processing. Long inputs (more than 2,000 tokens) get prompts optimized for document-level reasoning.

Complexity detection routes sophisticated inputs to advanced prompts. Use heuristics like vocabulary diversity, sentence structure, or topic mixing as complexity signals. Complex inputs might need chain-of-thought prompts while simple ones use direct answering.

Domain detection switches prompts based on subject matter. Scan inputs for domain markers (medical terms, legal citations, code syntax). Route to domain-specialized prompts that encode relevant expertise.

User-context routing personalizes based on who is asking. Power users might get detailed technical responses. Novice users get simplified explanations. This requires maintaining user profiles or inferring expertise from input quality.

Time-sensitive routing handles urgent versus routine requests differently. Urgent inputs route to fast, reliable prompts optimizing for speed. Routine inputs can use more sophisticated prompts with higher latency.

## Multi-Stage Processing Pipelines

Some tasks require sequential prompts where each stage feeds the next. Document analysis might extract structure, then classify sections, then summarize each section, then synthesize a final summary. Each stage is a specialized prompt.

Design stage interfaces carefully. Each prompt should output in a format the next prompt expects as input. Standardize these interfaces to enable pipeline composition.

Include validation between stages. Check that each stage's output meets quality criteria before passing to the next stage. Failed validation triggers retry or error handling rather than propagating garbage.

Pass forward only necessary context. Each stage does not need the full history of prior processing. Send the most recent output plus minimal metadata about prior stages. This conserves tokens and focuses attention.

Implement stage skipping for efficiency. If early stages detect that later stages are unnecessary, allow short-circuiting the pipeline. Not every input needs every stage.

Monitor stage-level metrics. Track success rates, latency, and quality for each pipeline stage independently. This pinpoints bottlenecks and quality problems more precisely than end-to-end metrics alone.

## Conditional Output Formatting

Different downstream consumers need different output formats. Your prompt logic should adapt format based on who or what will consume the output.

Human-facing outputs prioritize readability. Use natural language, clear structure, and explanatory text. Markdown formatting helps. Avoid technical jargon unless the user is expert.

API-facing outputs prioritize parseability. Use strict JSON or XML schemas. Minimize explanatory text. Include explicit error codes and status fields. Validate format compliance before returning.

Downstream-model outputs optimize for the next model's input expectations. If another prompt will process this output, format it the way that prompt works best. This might mean structured bullets, numbered points, or labeled sections.

Conditional verbosity adjusts detail level. For summaries, detect whether the user wants brief highlights or comprehensive coverage. Route to prompts tuned for the appropriate detail level.

Format validation should happen before returning outputs. Check that the generated format matches what the consumer expects. This prevents integration failures from format mismatches.

## Testing and Validation for Branching Logic

Test each branch independently before integrating into the routing system. Verify that specialized prompts work well on inputs they are designed for. This isolates prompt quality from routing quality.

Test routing accuracy with labeled examples. Create a validation set where you know the correct branch for each input. Measure routing precision and recall. Poor routing undermines even excellent specialized prompts.

Perform end-to-end testing on realistic input distributions. Your routing logic should match production traffic patterns. If 60 percent of real inputs need technical support, your test set should reflect that distribution.

Test edge cases at branch boundaries. Inputs that could plausibly go to multiple branches are where routing fails. Create adversarial examples that stress boundary decisions.

Monitor routing decisions in production. Log which branch each input takes. Track whether downstream prompts succeed or fail. High failure rates on specific branches indicate routing or prompt problems.

## Maintenance Strategies for Branched Architectures

Version specialized prompts independently. Technical support prompts can evolve without touching billing prompts. This isolation prevents changes from having unexpected side effects across your system.

Maintain a clear catalog of your branching structure. Document which prompts exist, what inputs they handle, and how routing works. This map is essential as complexity grows.

Implement canary deployments for prompt changes. Roll out updated prompts to a small percentage of traffic first. Monitor metrics. Expand gradually if quality improves.

Retire unused branches aggressively. If a branch handles less than 1 percent of traffic and shows no quality advantage, merge it into a more general prompt. Complexity has maintenance costs.

Conduct regular architecture reviews. As your product evolves, your prompt structure should evolve too. Categories that made sense at launch might not fit current reality. Refactor ruthlessly.

Next, you will learn how to structure retrieved context within prompts to maximize model grounding while managing token budgets and handling source conflicts.
