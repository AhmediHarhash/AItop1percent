# 2.11 — Prompt Compression and Token Optimization

A healthcare AI startup ran into a crisis in July 2025 when their medical documentation assistant started failing 18% of requests during peak hours. The product team had built an elaborate prompt system with detailed medical terminology guidelines, extensive examples, and comprehensive formatting instructions. Their average prompt consumed 3,200 tokens before the user's actual medical note even began. At $0.015 per 1K input tokens for Claude Opus 4.5, they were burning $847 per day just on prompt overhead across their 180,000 daily requests.

The engineering team's first instinct was to compress everything aggressively. They removed all examples, shortened instructions to bullet points, and eliminated what they considered "redundant" context. Token count dropped to 890 tokens per prompt. Cost savings looked impressive on paper. Then accuracy collapsed. Their extraction accuracy for medication names fell from 94% to 71%. Dosage errors tripled. Clinical staff started filing bug reports faster than the team could triage them.

The real problem wasn't prompt length itself. The team had conflated verbosity with token count, assuming all reduction was good reduction. They removed high-value examples that anchored the model's understanding while keeping verbose transitional phrases that added no semantic value. They compressed critical domain constraints while preserving generic politeness instructions. They had optimized the wrong thing. What they needed was strategic compression that preserved information density while eliminating genuine waste.

## The Token Budget Mindset

You need to think about prompt tokens as a budget, not a cost to minimize at all costs. Every project has a different optimal token allocation based on task complexity, domain specificity, and accuracy requirements. A simple sentiment classifier might work brilliantly with 120 tokens. A medical coding assistant might need 2,800 tokens to maintain acceptable accuracy. The question isn't "how few tokens can I use" but "what's the minimum token count that preserves the performance I need."

Start by establishing your baseline performance with your current prompt. Measure accuracy, consistency, and edge case handling with real production data, not cherry-picked examples. This baseline becomes your constraint. Now you can experiment with compression, but only changes that maintain performance within 2-3% of baseline are candidates for production. Anything that saves tokens but degrades quality is technical debt, not optimization.

Track cost per successful outcome, not cost per request. If compression reduces your per-request cost from $0.048 to $0.019 but increases your failure rate from 3% to 12%, your cost per successful outcome actually increased. You're now paying for failures, retries, and customer support tickets. The math favors higher-quality prompts with higher token counts in most production systems.

Token budgets vary wildly across industries and use cases. A fintech company running fraud detection might allocate 4,500 tokens per prompt because a single missed fraud case costs $15,000 in losses. An e-commerce company running product categorization might cap prompts at 300 tokens because categorization errors cost almost nothing. Your budget should reflect your error economics, not arbitrary limits.

## Identifying Genuine Redundancy

Real redundancy comes in specific forms that you can hunt systematically. Repetitive phrasing is the easiest target. If your prompt says "You must follow the format exactly as specified. The format is critical and must be followed precisely. Do not deviate from the specified format," you're spending tokens to say the same thing three times. Consolidate to "Follow this format exactly" and move on. The model heard you the first time.

Transitional fluff is another common source of waste. Phrases like "Now that we've covered X, let's move on to Y" or "It's important to note that" or "Please keep in mind that" add tokens without adding information. Cut them. The model doesn't need conversational hand-holding. Your prompt is a specification document, not a friendly chat.

Look for inherited instructions that no longer apply. Many production prompts started as templates or examples and accumulated instructions through copy-paste evolution. You might find instructions about code formatting in a prompt that only handles text, or JSON schema requirements in a prompt that outputs plain English. Audit every instruction against current requirements and delete anything that doesn't serve your actual use case.

Watch for redundancy between examples and instructions. If your instruction says "extract dates in YYYY-MM-DD format" and then every example shows dates in that format, the examples are reinforcing the instruction effectively. But if you then add another instruction saying "remember, dates must be four-digit year, dash, two-digit month, dash, two-digit day," you're being redundant. The instruction plus examples already covered this. Delete the redundant restatement.

Redundancy also appears in constraint specifications. You might tell the model "never include personal opinions" and then later say "remain objective" and then again say "do not insert subjective commentary." These are three ways of saying the same thing. Pick the clearest phrasing and delete the rest.

## Compression Through Structure

Structured formats often use fewer tokens than verbose prose to convey the same information. Compare these two approaches to specifying classification categories. The verbose version uses 89 tokens: "You should classify customer inquiries into one of the following categories. Technical Support includes issues with product functionality, bugs, or performance problems. Billing inquiries involve questions about charges, refunds, or payment methods. General Questions covers everything else including product information and general assistance."

The structured version uses 31 tokens: "Classify into: TECHNICAL (bugs, performance), BILLING (charges, refunds, payments), GENERAL (other)." Same information density, 65% fewer tokens. The model doesn't need full sentences to understand category definitions. It needs clear boundaries and distinguishing features.

This pattern extends to examples. Instead of writing "For example, if the user says 'my payment was declined', you should respond with 'I apologize for the inconvenience. Let me help you resolve this payment issue,'" you can write "Input: 'payment declined' → Output: 'I'll help resolve this payment issue.'" The essential input-output mapping is preserved with half the tokens.

Be careful with structure compression in domains where nuance matters. Medical guidelines, legal constraints, and safety requirements often need the precision that full sentences provide. Abbreviating "patients under 18 require parental consent" to "u18: parent OK" might save tokens but introduce ambiguity that leads to compliance violations.

List formats compress well. Instead of "You should check for errors including spelling mistakes, grammatical issues, factual inaccuracies, and formatting problems," you can write "Check for: spelling errors, grammar issues, factual errors, formatting problems." The list format is clearer and shorter.

Table structures can compress complex relationships. If you need to specify different handling for different user types, a compact table format beats prose paragraphs. The model parses structured information efficiently, often better than it parses dense paragraphs.

## Strategic Example Selection

Examples are the highest-value tokens in most prompts, but you're probably using too many or choosing them poorly. The healthcare startup's original prompt included 18 examples of medication extraction. When they cut to zero examples, performance tanked. When they tested different example counts systematically, they found 4 carefully chosen examples gave them 93% of their original accuracy with 72% fewer example tokens.

The key is diversity, not volume. Each example should demonstrate a distinct pattern or edge case. If you have three examples that all show the same basic pattern, you're wasting tokens on redundancy. One clear example of that pattern is sufficient. Your additional examples should cover different structures, ambiguous cases, or common failure modes.

Test your examples in isolation. For each example in your prompt, temporarily remove it and measure whether accuracy drops. If removing an example has no measurable impact on performance, that example is dead weight. Keep only examples that prove their value through ablation testing. This sounds tedious, but for prompts that run millions of times, the engineering time investment pays for itself in weeks.

Consider example compression through partial examples. Instead of showing complete input-output pairs, show just the parts that matter for disambiguation. If your task is extracting company names from news articles, you don't need to show entire articles as examples. Show the relevant sentences with the extraction highlighted. The model will generalize the pattern without needing full context in every example.

Examples should target actual confusion points, not hypothetical ones. Build your example set by analyzing real production failures. Where does your prompt actually fail? What patterns confuse the model? Use examples to address observed failure modes, not imagined edge cases. This approach produces leaner, more effective example sets.

Counter-examples can be powerful. Instead of just showing what to do, show what not to do. A single negative example ("NOT: extracting 'vitamin C' as a medication") can prevent entire classes of errors. Use counter-examples sparingly but strategically for high-impact distinctions.

## Abbreviation Strategies That Work

Domain-specific abbreviations can dramatically reduce token count if applied consistently. In a financial application, you can establish abbreviations at the start of your prompt: "Use: amt (amount), acct (account), txn (transaction), bal (balance)." Then use these abbreviations throughout your instructions and examples. The model adapts quickly to consistent abbreviation schemes within a single prompt.

The critical word is consistent. If you abbreviate "transaction" as "txn" in one section and spell it out in another, you're not saving tokens and you're introducing confusion. Commit to your abbreviation scheme across the entire prompt. Consider maintaining a project-level abbreviation glossary that all prompts use so your team doesn't reinvent abbreviations for every new prompt.

Avoid abbreviations that create ambiguity. Don't abbreviate "customer" to "cust" and "customization" to "cust" in the same prompt. Don't use "rec" if it could mean "record," "recommend," or "recent." When in doubt, spell it out. The token savings from an ambiguous abbreviation are wiped out by the error costs from misunderstanding.

Technical domains often have standard abbreviations that models know well because they appear frequently in training data. API, JSON, HTTP, SQL, HTML—these don't need definition. Domain acronyms like HIPAA, GDPR, or ISO standards are also safe to use without expansion. But your internal company abbreviations or product-specific shorthand need clear definition, at least once.

Context-specific abbreviations work when the domain is narrow. In a prompt that only handles e-commerce order processing, abbreviating "shipping" to "ship" and "handling" to "hdl" works because the context is constrained. In a general-purpose prompt, these abbreviations introduce confusion. Match abbreviation aggressiveness to domain specificity.

Abbreviation dictionaries at the prompt start can establish conventions efficiently. "Throughout this prompt: usr=user, msg=message, req=request, resp=response." This one-time token investment enables compression throughout the rest of the prompt. The net savings can be substantial for long prompts.

## When Compression Hurts Performance

Compression becomes counterproductive when it crosses into information loss. If your original prompt says "Extract medication names, but exclude vitamins, supplements, and over-the-counter drugs unless they interact with prescribed medications," that's not redundant verbosity. That's a complex constraint with multiple clauses. Compressing it to "Extract prescription medications" changes the specification and degrades accuracy on edge cases.

Tasks requiring nuanced judgment need breathing room. If you're building a content moderation system, your guidelines about context-dependent violations can't be abbreviation-compressed without losing critical nuance. The difference between harassment and heated debate, or between artistic nudity and pornography, requires careful explanation. Token savings that compromise these distinctions create moderation errors that erode user trust.

Watch for compression that makes prompts harder for humans to maintain. If your team needs a decoder ring to understand what a prompt is asking for, you've over-compressed. Prompts are code. They need to be readable, debuggable, and modifiable by multiple team members over time. Extreme abbreviation creates technical debt that costs more in engineering time than it saves in token costs.

New model versions sometimes change how much compression they tolerate. GPT-5 handles heavily compressed prompts better than GPT-4, and Claude Opus 4.5 improved compression tolerance over Claude 3 Opus. When you upgrade models, retest your compression assumptions. You might find you can compress further, or you might find that what worked before now causes quality regression.

Domain complexity drives minimum viable token counts. Legal contract analysis can't be compressed below a certain threshold without losing critical distinctions between clause types. Medical diagnosis assistance needs enough tokens to specify safety constraints. Compression can't fight fundamental task complexity. Know when you've hit the floor.

## Measuring Compression Impact

You need quantitative measurement, not gut feelings, to evaluate compression changes. Start by establishing your baseline: current token count, current accuracy on your validation set, current cost per request. Make one compression change at a time and measure all three metrics again. Only compress further if accuracy holds within acceptable bounds.

Build a compression testing pipeline that automates this measurement. Your pipeline should run your validation set through both the original and compressed prompts, calculate accuracy metrics, count tokens, and report the trade-offs. Without automation, compression testing becomes too tedious and teams stop doing it. With automation, you can test compression hypotheses in minutes and make data-driven decisions.

Track edge case performance separately from overall accuracy. Compression often degrades rare cases while maintaining performance on common inputs. If your overall accuracy stays at 94% but your accuracy on medical abbreviations drops from 91% to 78%, that's a problem even though aggregate metrics look fine. Break down your validation set by input categories and monitor category-specific accuracy.

Cost-per-outcome is your ultimate metric. Calculate the total cost to achieve one successful result, including retry costs for failures. If your original prompt costs $0.035 per request with a 96% success rate, your cost per success is $0.036. If compression reduces cost to $0.022 per request but success rate drops to 88%, your cost per success rises to $0.025. The compressed version looks cheaper but delivers worse economics.

Run A/B tests in production for significant compression changes. Deploy the compressed prompt to 10% of traffic and compare results to the original prompt on the other 90%. Monitor for a week. Look for performance differences, error rate changes, and unexpected failure modes. Production testing catches issues that validation sets miss.

Document compression decisions with data. When you compress a prompt, record the token reduction, the accuracy impact, and the business justification. Six months later, when someone asks "why is this prompt structured this way," you have evidence showing the trade-offs. Compression without documentation becomes archaeology that wastes future engineering time.

## Prompt Compression vs Context Window Size

Larger context windows make compression less urgent but don't eliminate its value. Claude Opus 4.5's 200K token window means you're not usually hitting hard limits. But you're still paying for every input token, and you still face latency costs as prompts grow. A 4,000-token prompt takes measurably longer to process than a 400-token prompt, even when both fit comfortably in the context window.

The real opportunity with large context windows isn't eliminating compression—it's being strategic about what you choose to include. With 200K tokens available, you can afford rich examples and detailed constraints where they matter. This makes cutting genuine waste even more valuable. Every token you save on redundant fluff is a token you can spend on high-value examples or domain-specific guidelines.

Think of context windows as enabling better compression, not eliminating the need for it. With a small context window, you're forced to make painful cuts between valuable content. With a large context window, you can keep all valuable content and focus compression efforts purely on waste elimination. This is a better optimization problem with clearer right answers.

Some teams use large context windows as an excuse to avoid prompt discipline entirely. They paste in entire documentation sets, wall-of-text guidelines, and dozens of examples without curation. This approach works until it doesn't—when costs scale with usage, when latency becomes user-visible, or when prompt maintenance becomes impossible because no one knows what's actually necessary anymore.

Context window size also affects where compression matters most. In a 4K context window, every token counts. In a 200K context window, spending 500 extra tokens on clarity in a high-stakes medical prompt is trivial. Compression effort should focus on high-frequency prompts that run millions of times. Low-frequency prompts can prioritize clarity over token efficiency.

## Compression Benchmarks by Task Type

Different task types have dramatically different optimal token budgets. Simple classification tasks typically work well with 100-300 token prompts. You're defining categories, providing brief descriptions, and maybe showing one example per category. Anything beyond 400 tokens for basic classification suggests you're carrying waste or over-constraining the model.

Extraction tasks need more room, typically 400-800 tokens. You're specifying what to extract, how to handle ambiguous cases, output format requirements, and examples of tricky inputs. If you're over 1,200 tokens for extraction, audit carefully. You might have legitimate complexity, or you might have accumulated instruction bloat over time.

Generation tasks span the widest range, from 200 tokens for simple content generation to 3,000+ tokens for specialized domains like legal writing or technical documentation. The key variable is domain constraint density. If you're generating blog posts, you need minimal constraints. If you're generating medical discharge summaries, you need extensive compliance and accuracy constraints.

Multi-step reasoning tasks often justify larger prompts, from 800 to 2,500 tokens. You're not just specifying what to do but how to think through the problem, what to check at each step, and how to validate reasoning. These prompts are legitimately complex. But even here, watch for redundancy. Many reasoning prompts explain the same checking step three different ways when once would suffice.

Transformation tasks—reformatting, translation, summarization—usually sit in the 200-600 token range. You're specifying input format, output format, and transformation rules. These specs are typically compact. If your transformation prompt exceeds 800 tokens, you're probably over-specifying or including unnecessary examples.

Industry benchmarks vary. Legal tech prompts average 1,800-3,200 tokens. Healthcare prompts average 1,200-2,400 tokens. E-commerce prompts average 300-900 tokens. Financial services prompts average 800-1,600 tokens. Your prompt should fall within industry norms for similar tasks unless you have specific reasons for deviation.

## Version Control for Prompt Compression

Treat prompt compression like any other refactoring. Use version control, create feature branches, and require review before merging compression changes to production prompts. Your commit messages should include the token reduction and the accuracy impact: "Compress medication extraction prompt from 2,100 to 1,450 tokens, accuracy 93.2% to 93.1% on validation set."

Tag your prompt versions with performance metadata. When you compress a prompt, record not just the new token count but the accuracy, cost per request, and P95 latency. Six months later, when someone asks "why did we structure the prompt this way," you have data showing the trade-offs that led to current design.

Consider maintaining both compressed and verbose versions during transition periods. Run them in parallel with a small percentage of production traffic to each, measure outcomes, and make the switch only when you have confidence in the compressed version's production performance. This A/B testing approach catches issues that don't show up in validation sets.

Document your compression decisions. When you choose not to compress something that looks redundant, leave a comment explaining why. "Seems redundant but ablation testing showed 7% accuracy drop without this constraint" saves future maintainers from repeating your experiments. Prompt archaeology is expensive. Make decisions discoverable.

Compression creates technical debt if done carelessly. Over-compressed prompts become hard to extend. When you need to add new features or handle new edge cases, you have to decompress first, losing all the optimization work. Design compression strategies that leave room for future growth. Don't optimize yourself into a corner.

## Compression and Model Instruction Following

Different models respond differently to compression. GPT-5 generally handles terse, abbreviated instructions well. Claude models often perform better with slightly more context and natural phrasing. Gemini 2.0 falls somewhere between. Your compression strategy needs to account for your target model's instruction-following characteristics.

Test compression changes with your actual deployment model, not a proxy. Don't compress a prompt against GPT-5 and assume it'll work the same way with Claude Opus 4.5. Model-specific quirks matter. Some models need explicit format specifications where others infer format from examples. Some handle abbreviations smoothly while others stumble on ambiguous shorthand.

When you support multiple models, you might need model-specific compression strategies. This sounds like maintenance overhead, but in practice you're often already maintaining model-specific prompts to account for different strengths. Adding model-appropriate compression to your existing model-specific prompt management isn't a large additional burden.

Watch for model version changes that affect compression tolerance. Providers usually announce major releases, but minor version updates that change instruction-following behavior often go unnoticed. Monitor your production metrics across model version updates and revalidate compression assumptions if performance shifts.

Some models handle implicit instructions better than others. GPT-5 can often infer what you want from brief examples. Claude models may need more explicit instruction alongside examples. Gemini models vary by version. Match compression aggressiveness to model instruction-following capability.

## The Compression Testing Loop

Effective compression requires a disciplined testing loop. Identify a compression opportunity, make the change in isolation, measure accuracy and cost impact, and decide whether to keep it. Don't batch multiple compression changes into one test. You won't know which change caused a performance regression, and you'll waste time debugging the interaction.

Your testing loop should include adversarial examples, not just happy path validation. Compression often degrades performance on edge cases and ambiguous inputs while maintaining accuracy on clear examples. Build a validation set that's heavy on the weird cases—ambiguous phrasings, malformed inputs, boundary conditions. If compression holds up on adversarial data, it'll work in production.

Time compression testing during natural development cycles. When you're already updating a prompt for feature changes, that's the right moment to audit for compression opportunities. You're already running validation and have the prompt loaded in working memory. Bundling compression with feature work is more efficient than dedicated compression sprints.

Set a compression cadence for stable prompts. Even prompts that aren't actively being updated can often benefit from compression as you accumulate more production data and understanding of what matters. A quarterly compression review for high-volume prompts catches accumulated cruft and keeps token costs from drifting upward over time.

Build compression testing into your CI/CD pipeline. Automated tests should flag prompts that exceed token budgets, verify that compression doesn't degrade accuracy below thresholds, and alert when compressed prompts show performance variance. Automation prevents compression regressions from reaching production.

## Cost-Performance Frontier Analysis

Every prompt sits somewhere on a cost-performance frontier. At one extreme, minimal prompts cost almost nothing but perform poorly. At the other extreme, exhaustive prompts perform well but cost a fortune. Your job is to find the point on that frontier that optimizes for your business objectives.

Map your prompt's position on this frontier by testing multiple compression levels. Start with your current prompt as baseline. Create progressively more compressed versions: 90% of original tokens, 75%, 60%, 45%, 30%. Measure accuracy at each level. Plot cost vs accuracy. The curve shows you where compression yields diminishing returns.

Most prompts show a sharp performance cliff somewhere on the compression curve. Above that cliff, compression removes waste with minimal accuracy impact. Below that cliff, compression removes essential information and accuracy plummets. Find the cliff and stay safely above it.

Different business contexts have different optimal points on the frontier. A high-stakes legal analysis tool should operate near the high-accuracy, high-cost end. An internal content tagger should operate near the low-cost end. Match your frontier position to your use case economics, not to arbitrary token minimization goals.

Frontier analysis also reveals where to focus optimization effort. If your prompt is far from the efficient frontier, compression can yield major wins. If you're already near the frontier, further compression requires painful trade-offs. Know where you stand before investing engineering time in compression.

## Compression as Ongoing Practice

Prompt compression isn't a one-time optimization. It's an ongoing practice. Prompts accumulate cruft over time as features are added, edge cases are handled, and constraints evolve. Without regular compression maintenance, prompts grow bloated and expensive.

Establish compression as part of your prompt maintenance rhythm. Monthly reviews for high-frequency prompts. Quarterly reviews for moderate-frequency prompts. Annual reviews for low-frequency prompts. These reviews catch drift before it becomes expensive.

Track compression metrics over time. If your average prompt size is growing month over month, you're accumulating bloat. If it's shrinking, you're successfully managing cruft. Treat token count per prompt as a key metric alongside accuracy and cost. What gets measured gets managed.

Build compression awareness into your team culture. Engineers should default to concise prompts and justify verbosity, not the other way around. Code review should include prompt review with compression as an evaluation criterion. Celebrate successful compression that maintains quality.

Compression expertise develops through practice. Your first compression attempts will be cautious and conservative. As you build intuition for what compression preserves vs degrades performance, you'll become more aggressive and effective. Invest in building this capability across your team, not just in individual contributors.

Now that you understand how to compress prompts strategically while preserving performance, you need systems for building prompts dynamically from reusable components at runtime.
