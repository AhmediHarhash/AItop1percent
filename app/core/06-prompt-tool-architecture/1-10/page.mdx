# 1.10 â€” The Role of Examples: Zero-Shot, Few-Shot, and Many-Shot

A healthcare SaaS company deployed a medical coding assistant in October 2024 using Claude Opus 4 with a carefully crafted zero-shot prompt. The prompt described ICD-10 coding rules in detail across 850 tokens. Their validation set showed 91 percent accuracy, meeting their launch threshold. They chose zero-shot to avoid the overhead of maintaining example sets across 12,000 procedure types.

By December 2024, they had processed 45,000 real patient encounters. Accuracy had dropped to 84 percent in production. The failure analysis revealed a clear pattern: rare procedures and ambiguous clinical notes triggered most errors. Their quality team manually created 30 examples covering the most common failure modes and added them to the prompt. Accuracy immediately jumped to 94 percent, exceeding their original target.

The team had assumed that detailed instructions could replace examples. They learned that examples teach patterns that are difficult to specify in prose. Examples show the model what good judgment looks like in ambiguous situations. The examples also compressed 850 tokens of instructions down to 600 tokens plus examples, reducing both complexity and latency while improving accuracy.

The company spent $28,000 on the quality investigation and example creation effort, plus another $12,000 in customer credits for coding errors. The lesson was expensive but clear: instructions tell the model what to do, but examples show the model how to do it. Sometimes showing beats telling.

## Zero-Shot Performance as a Baseline

**Zero-shot** prompting provides only task instructions without any examples of inputs and outputs. Modern large language models can perform many tasks zero-shot because they learned similar patterns during training. Claude 4, GPT-4.5, and Gemini 2 can summarize text, extract entities, classify sentiment, and answer questions without ever seeing task-specific examples.

Zero-shot works when your task aligns closely with common training patterns. Summarizing news articles works zero-shot because models saw millions of articles during training. Extracting company names from press releases works zero-shot because this pattern is common in training data.

Zero-shot fails when your task requires domain-specific judgment, uses specialized formats, or involves subtle distinctions. Medical coding is rare in training data. Your company's specific ticket categorization scheme did not appear in any training data. The distinction between a billing error and a feature request in your support queue requires context the model does not have.

You establish zero-shot as a baseline before adding examples. If zero-shot achieves acceptable performance, adding examples might not justify the maintenance cost. If zero-shot performance has clear gaps, examples become your primary tool for closing them.

The healthcare coding system achieved 91 percent accuracy zero-shot on their validation set. This seemed acceptable. Production revealed that 91 percent was an average across easy and hard cases. Accuracy on common procedures exceeded 95 percent. Accuracy on rare procedures fell below 80 percent. The average hid a bimodal distribution that examples would address.

Zero-shot also serves as a control in experiments. When you add examples and measure improvement, you need to know the zero-shot baseline. A jump from 70 percent to 88 percent justifies the examples. A jump from 88 percent to 89 percent might not. The baseline quantifies the value that examples provide.

## Few-Shot Example Selection Strategies

**Few-shot** prompting adds a small number of examples to demonstrate the desired input-output mapping. Few-shot typically means 3-10 examples. This range works across most tasks because modern models generalize well from limited examples.

Example selection matters more than example quantity. Three well-chosen examples often outperform ten mediocre ones. Good examples are diverse, representative, and instructive. They cover the main task variations, represent the actual distribution of inputs you will see, and teach the model something it could not infer from instructions alone.

Diversity means examples should span the task space. If you are classifying support tickets into five categories, include at least one example per category. If inputs vary in length, include both short and long examples. If tone varies, show formal and informal examples.

Representativeness means examples should reflect production data characteristics. If 70 percent of your inputs are straightforward and 30 percent are ambiguous, your example set should roughly match that ratio. Examples drawn from an idealized or sanitized dataset teach the model patterns it will not encounter in production.

Instructiveness means examples should teach boundary cases and judgment calls. Include examples where the correct answer is not obvious from instructions alone. Show cases where similar inputs require different outputs due to subtle distinctions. Demonstrate how to handle missing information or ambiguous inputs.

The healthcare system chose examples covering rare procedure codes, ambiguous clinical language, and common documentation errors. One example showed a procedure note saying "wound closure" when the actual procedure was a complex multi-layer repair requiring a different code. Another showed how to handle notes with missing anatomical locations. A third demonstrated coding for procedures performed as part of larger surgeries.

These examples taught judgment that instructions could not specify. How do you code when documentation is incomplete? What level of detail distinguishes basic from complex procedures? When do you infer information versus refusing to code? Examples answered these questions through demonstration rather than explanation.

## Many-Shot Learning in Long-Context Models

**Many-shot** prompting uses dozens or hundreds of examples when models have sufficient context windows. Claude Opus 4 supports 200,000 token contexts. GPT-4.5 supports 128,000 tokens. These windows enable example sets that would have been impossible with earlier models.

Many-shot makes sense when task complexity exceeds what you can specify in instructions. Instead of writing rules for every edge case, you show the model 100 examples covering diverse scenarios. The model learns implicit patterns from the example distribution.

The returns to additional examples diminish after a point. The jump from zero-shot to three-shot is usually dramatic. The jump from three-shot to ten-shot is meaningful. The jump from ten-shot to 100-shot is often marginal unless your task has high intrinsic complexity.

You validate whether many-shot justifies its cost by measuring performance gains per example added. If adding examples 30-40 improves accuracy by 2 percentage points but adding examples 60-70 improves it by 0.2 percentage points, you have hit diminishing returns. The optimal point balances performance gains against latency and cost increases from longer prompts.

Many-shot also enables dynamic task specification. Instead of maintaining explicit rules for how to handle regional variations in address formats, you include 50 examples from different countries. The model learns regional patterns implicitly. When new edge cases emerge, you add examples rather than updating rules.

For the healthcare system, many-shot would mean including hundreds of coded encounters spanning the full range of specialties, procedure types, and documentation styles. This comprehensive coverage would teach nuances that few-shot cannot capture. The trade-off is increased inference cost and latency from the longer context.

A practical many-shot strategy uses tiered prompts. Common cases get a lightweight few-shot prompt. Rare or complex cases detected through preliminary classification get a heavyweight many-shot prompt. This balances cost and quality by applying expensive processing only where it provides value.

## Example Ordering Effects

Example order influences model behavior more than most teams expect. Models are sensitive to recency, so later examples often carry more weight than earlier ones. Models also learn sequence patterns, so if all your positive examples appear before negative examples, the model may learn that order matters.

Randomizing example order during development helps you discover order dependencies. If performance varies significantly across different random orderings, your examples may be teaching unintended patterns. Stable performance across orderings suggests robust example selection.

Strategic ordering can improve performance when you understand the effect. Placing the most instructive or challenging examples last gives them more weight. Grouping similar examples together helps the model learn category patterns. Alternating between categories prevents sequence bias.

For production systems, you typically want consistent ordering to ensure reproducible behavior. Choose an ordering that performs well across your test set and lock it. Document why examples appear in that order so future maintainers understand the reasoning.

The healthcare system ordered examples from simple to complex, mirroring the way medical students learn coding. Early examples showed straightforward single-procedure encounters. Later examples introduced complications: multiple procedures, unusual documentation, and ambiguous clinical notes. This progression helped the model build understanding incrementally.

Ordering by difficulty also improved error analysis. When the model failed on an example similar to one late in the sequence, you knew the example set did not cover that pattern adequately. When it failed on patterns similar to early examples, you knew the model was not generalizing properly.

## When Zero-Shot Outperforms Few-Shot

Adding examples can hurt performance in specific scenarios. If your examples are not representative of production data, they teach the model the wrong patterns. If your examples contain subtle errors, the model learns to reproduce those errors. If your examples are too narrowly focused, they reduce the model's ability to generalize.

Zero-shot also wins when task requirements change frequently. Maintaining high-quality examples across rapidly evolving requirements creates ongoing overhead. If your product team ships new features weekly that change task semantics, the cost of updating examples may exceed the benefit.

Some tasks are easier to specify than to demonstrate. Simple transformations like "convert all dates to ISO 8601 format" or "extract all email addresses" work perfectly with instructions alone. Examples add tokens without adding information.

You measure whether examples help by A/B testing. Run the same inputs through zero-shot and few-shot variants. If few-shot does not improve accuracy by at least 3-5 percentage points, the examples are not earning their cost in latency and maintenance.

The healthcare system discovered that examples hurt performance on extremely rare codes appearing once per 10,000 encounters. For these outliers, the model's general medical knowledge from training data outperformed pattern matching to examples. The examples biased the model toward more common codes, causing misclassification of rare ones.

The solution was dual-prompt strategy. Standard encounters used the example-rich prompt. Encounters flagged as potential rare-code cases used a zero-shot prompt emphasizing careful consideration of unusual procedures. This hybrid approach leveraged the strengths of both methods.

## Negative Examples and Boundary Specification

**Negative examples** show the model what outputs to avoid. They are particularly valuable for tasks where the boundary between acceptable and unacceptable outputs is subtle. A customer service prompt might include examples of responses that are technically accurate but have the wrong tone.

Negative examples work by showing contrasts. You provide an input, a bad output, and a good output. The model learns to distinguish the patterns that separate them. This is more effective than instructions saying "avoid being robotic" because the model sees concrete instances of what robotic means in your context.

The ratio of positive to negative examples matters. Too many negative examples can confuse the model or make it overly conservative. A typical ratio is 3-4 positive examples for every negative example. The negative examples should represent common failure modes, not random bad outputs.

Boundary cases benefit especially from negative examples. If you are classifying bug reports versus feature requests, show examples near the boundary where the distinction is unclear. One example might be a user describing unexpected behavior that turns out to be intended functionality. Show both how to recognize this case and how to respond appropriately.

The healthcare system used negative examples to prevent over-coding, where coders assign more complex procedure codes than warranted to maximize reimbursement. One negative example showed a simple wound repair incorrectly coded as complex reconstruction. Another showed prophylactic medication incorrectly coded as therapeutic intervention. These examples taught ethical coding practices.

Negative examples also address systematic errors. If the model consistently confuses two categories, include examples contrasting them. If it regularly outputs malformed JSON, include an example of malformed output with explanation of what makes it invalid. Target your negative examples at observed failure patterns rather than hypothetical problems.

## Dynamic Example Selection

Static example sets work for stable tasks. Dynamic selection adapts examples to each input. If you are classifying support tickets, you might retrieve the three most similar historical tickets and use them as examples for the current classification.

Dynamic selection requires infrastructure for similarity search, typically using embedding-based retrieval. You maintain an example database with embeddings. For each new input, you embed it, retrieve the nearest neighbors, and include them in the prompt. This is the foundation of **retrieval-augmented generation** applied to example selection.

Dynamic examples provide better coverage than static examples when your input distribution is diverse. A static set of ten examples cannot cover all variations. A dynamic system draws from hundreds or thousands of examples, always providing relevant context for the current input.

The cost is added latency from retrieval and potentially longer prompts if you include many dynamic examples. You need to measure whether the performance gain justifies the added complexity. Dynamic selection makes most sense for high-value tasks where accuracy improvements directly impact revenue or user experience.

The healthcare system considered dynamic example selection but rejected it due to latency constraints. Coding must happen in real-time during patient checkout. Adding 200 milliseconds for similarity search would have violated their 2-second total latency budget. They optimized their static examples instead, accepting slightly lower accuracy for predictable performance.

Dynamic selection also requires careful curation of the example database. Low-quality examples hurt more in dynamic systems because you cannot manually review which examples appear for each input. You need quality thresholds, regular audits, and automated filtering to ensure only high-quality examples enter the pool.

## Example Contamination Risks

Example contamination occurs when examples leak information that should not be available during task performance. If you are testing a model's ability to classify medical images, including examples from the test set contaminates the evaluation. The model succeeds not because it learned the task, but because it memorized the examples.

Contamination also appears when examples include information from the future. A fraud detection prompt might include examples where the fraud label is known but would not be available in production until days later. The model learns to use signals that will not exist when you actually deploy it.

You prevent contamination through temporal splitting and strict data hygiene. Training examples must come from periods before your test period. Examples must use only information that would be available at prediction time. If your model makes decisions in real-time but examples include information available only after batch processing, the examples are contaminated.

Contamination is particularly dangerous because it creates false confidence. Your test metrics look excellent, but production performance collapses because the model relied on information it will not have. Regular production monitoring catches this quickly, but prevention through proper example selection is better than detection.

The healthcare system ensured temporal validity by using only examples from encounters at least six months old where all coding had been audited and confirmed correct. Recent encounters might still have pending audits or corrections. Using them as examples would teach patterns that had not been validated. The six-month lag ensured example quality at the cost of slightly stale representation of current documentation practices.

## Example Maintenance and Versioning

Examples require ongoing maintenance as tasks evolve and model capabilities improve. An example set created for Claude 3.5 Sonnet in June 2024 may not be optimal for Claude Opus 4 in January 2026. The newer model might need fewer examples or different examples to achieve the same performance.

Version control for example sets allows rollback when updates degrade performance. Each example set version is tagged with creation date, target model, and performance metrics. When you update examples, you A/B test the new version against the current production version before deploying.

Example drift occurs when production data shifts but example sets stay static. If your customer support tickets gradually shift toward mobile app issues but your examples focus on web platform issues, the model's performance will degrade over time. Regular example refresh cycles prevent this drift.

Automated example mining helps maintain freshness. You identify production cases where the model performed well, validate them manually, and add them to your example pool. This creates a feedback loop where production experience improves the prompt continuously.

The healthcare system reviewed their examples quarterly, comparing example distribution to production encounter distribution. If certain procedure categories became more common in production but remained underrepresented in examples, they added new examples from that category. If documentation styles evolved, they updated examples to reflect current practices.

Example versioning also documents why specific examples exist. Each example has metadata: when it was added, what problem it solved, what performance improvement it provided. This documentation prevents future maintainers from removing critical examples because they do not understand their purpose.

## Balancing Example Cost and Coverage

Each example adds latency and token cost. If your task runs thousands of times per day, example overhead compounds quickly. A prompt with ten examples might use 2,000 more tokens than a zero-shot prompt. At scale, this impacts both response time and inference cost.

You optimize this tradeoff by measuring the marginal value of each example. Start with one example and measure performance. Add a second and measure the lift. Continue until the performance gain from the next example is smaller than the cost increase you are willing to accept.

Example compression techniques reduce token overhead. Instead of showing complete examples with all context, show only the essential parts. Instead of verbose natural language examples, use structured formats that convey the same information more efficiently. Instead of including explanation in examples, let the pattern speak for itself.

Some teams maintain multiple example sets for different scenarios. Common cases get a lightweight prompt with 2-3 examples. Edge cases detected through preliminary classification get a heavier prompt with 8-10 examples. This tiered approach balances cost and coverage.

The healthcare system calculated that each example added 80 tokens on average. At their inference volume, each example cost $1,200 monthly in API fees. They needed each example to prevent at least $1,200 in coding errors to break even. This economic analysis justified their 30-example set but prevented them from expanding to 100 examples without clear ROI.

Cost-benefit analysis also considers indirect costs. Examples require maintenance time. They need periodic review and updates. They add complexity to prompt development. These operational costs compound over time and must factor into the decision about example quantity.

## Cross-Model Example Portability

Examples created for one model family often transfer to others, but with varying effectiveness. Claude and GPT-4 use different training data and architectures. Examples optimized for one may be suboptimal for the other.

Model-agnostic example selection focuses on fundamental task patterns rather than model-specific quirks. Choose examples that teach the task clearly regardless of which model processes them. Avoid examples that exploit known model behaviors like specific phrase sensitivities.

When you switch models, revalidate your example set rather than assuming it will transfer. Run your full test suite with the new model using the existing examples. Measure any performance changes. If performance drops significantly, investigate whether different examples would work better for the new model.

Some teams maintain model-specific example variants when they deploy to multiple model providers. The core examples are the same, but formatting or emphasis might differ. This adds maintenance overhead but ensures optimal performance across providers.

The healthcare system tested their examples across Claude Opus 4, GPT-4.5, and Gemini 2 Pro. They found that examples emphasizing step-by-step reasoning worked best on Claude. Examples with more varied formatting worked better on GPT-4.5. Examples with explicit negative cases improved Gemini performance. They maintained one base example set with model-specific variations for 6 of the 30 examples.

Cross-model portability also means designing examples that do not depend on model-specific knowledge cutoffs or training idiosyncrasies. Examples referencing events after a model's training cutoff will confuse that model. Examples assuming familiarity with obscure medical literature might work on one model but not another.

## Examples as Documentation and Knowledge Transfer

Beyond improving model performance, examples serve as documentation of task requirements and expected behavior. New team members read examples to understand what the system should do. QA engineers use examples to design test cases. Product managers reference examples when discussing requirements.

Well-commented examples include not just input and output but also explanation of why that output is correct. This explanatory layer helps humans understand the reasoning without affecting model behavior. The comments live in your prompt repository but not in the actual prompt sent to the model.

Examples also facilitate knowledge transfer when you migrate between models or rebuild prompts. The examples capture institutional knowledge about edge cases, domain-specific conventions, and quality standards. This knowledge survives team turnover and system evolution.

The healthcare system treated their example set as a training resource for new medical coders joining the team. Junior coders studied the examples to learn coding principles before reviewing real encounters. The examples compressed years of coding expertise into teachable patterns.

Examples also enabled better collaboration between clinical and technical teams. When clinicians wanted to change coding behavior, they proposed new examples rather than trying to modify prompt instructions. This concrete format facilitated communication between people with different expertise.

Understanding how to leverage examples effectively reduces prompt complexity while improving output quality. The next subchapter addresses how to manage complexity through decomposition when single prompts become unwieldy.
