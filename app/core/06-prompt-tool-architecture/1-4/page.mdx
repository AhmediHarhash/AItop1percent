# 1.4 â€” Temperature, Top-P, and Sampling Strategy as Architecture Decisions

A Series A legal tech startup launched a contract clause extraction service in September 2025. Their product team set temperature to 0.7 during prototyping because that value appeared in most tutorial code they found online. The setting came from a blog post about creative writing assistants. No one questioned whether it made sense for legal document processing. The service worked well in testing with a curated set of 200 sample contracts.

In production, the same contract uploaded twice would sometimes extract different clause interpretations. Law firms noticed discrepancies during audits. One client discovered that liability clauses were being parsed differently across identical uploaded contracts, creating compliance risk for their document management system. The client threatened litigation over the inconsistency. The startup spent $95,000 on incident response, legal review, and system rebuild. They changed temperature to 0 and implemented deterministic output validation. Post-incident analysis revealed that 18 percent of contracts had been processed with non-deterministic variations that created liability exposure.

The root cause was treating sampling parameters as arbitrary knobs to tune for performance rather than product requirements that define acceptable behavior variance. Temperature is not a performance optimization parameter like database connection pool size. It is a product decision about whether your application tolerates variability in outputs for identical inputs. Setting it wrong does not just hurt performance metrics. It violates user expectations and creates legal and operational liability.

This failure pattern repeats across industries. A financial services firm set temperature to 0.8 for investment research summaries in October 2025, believing higher values would produce more insightful analysis. Instead, they generated inconsistent summaries that confused clients and violated internal compliance policies requiring reproducible outputs. A healthcare documentation system used temperature 0.5 for clinical note generation, creating variance in medical terminology that triggered quality assurance flags. Each case stemmed from the same fundamental error: treating sampling parameters as technical details rather than product specifications.

## Temperature Controls Randomness and Is a Product Requirement

**Temperature** is a sampling parameter that controls output randomness by modifying the probability distribution over next tokens. At temperature 0, the model always selects the most likely next token deterministically. Outputs are functionally deterministic within the constraints of floating-point precision and API implementation details. At temperature 1, the model samples from the probability distribution according to the actual learned probabilities without modification. Higher temperatures flatten the distribution, making unlikely tokens more likely and increasing diversity at the cost of coherence.

The product question is simple but fundamental: does your use case tolerate variation in outputs for identical inputs. If the answer is no, temperature must be 0. If the answer is yes, you need to specify how much variation is acceptable, which determines your temperature value. This is not a technical question for engineers to decide unilaterally. It is a product requirement that should appear in your specification documents.

Deterministic tasks require temperature 0 without exception. Data extraction, classification, structured output generation, mathematical reasoning, code generation for production use, legal document processing. These tasks have correct answers or at least deterministic interpretations. Variation is not creativity. It is incorrectness or unreliability. A contract clause extraction tool that gives different results on identical inputs is fundamentally broken regardless of how high its average accuracy might be.

Creative tasks allow and often benefit from higher temperatures. Content generation, brainstorming, story writing, artistic style transfer, marketing copy variations. These tasks have no single correct answer. Variation is desirable because it provides options and explores creative space. A marketing copy generator that produces identical output every time is less useful than one that offers diverse options users can choose from.

The legal startup's failure came from not asking this product question during design. They copied temperature 0.7 from examples about creative applications without considering whether their product tolerated variance. Contract extraction is deterministic by nature. Legal documents have precise meanings that must be interpreted consistently. Temperature should have been 0 from the first line of code.

The mental model for temperature selection is simple: if variance creates risk, use temperature 0. If variance creates value, use higher temperatures. Risk includes legal liability, compliance violations, user confusion, audit failures, or operational inconsistency. Value includes creative exploration, content diversity, or option generation. Most enterprise applications involve risk more than value, which means most enterprise applications should use temperature 0.

## Temperature Zero Does Not Guarantee Identical Outputs

Setting temperature to 0 makes the model deterministic in the mathematical sense that it always chooses the highest probability token at each step. This does not mean you get byte-for-byte identical outputs across API calls to the same endpoint with the same prompt. Several implementation factors introduce variation even at temperature 0 that you must account for in your system design.

Floating-point arithmetic has rounding errors that depend on hardware architecture. Different GPU architectures round probability calculations differently at levels below 10 decimal places. API providers occasionally update their serving infrastructure, which changes hardware and introduces small probability differences. These changes are usually invisible but occasionally cause token selection to flip between near-equal probability options. Two tokens with probabilities 0.24001 and 0.23999 might switch ordering depending on rounding.

Prompt changes also matter more than teams expect. If you modify the prompt even slightly, the probability distribution changes, and temperature 0 may select different tokens for downstream positions. This includes changes you think are irrelevant, like adding whitespace, reordering examples, or changing punctuation. The model processes every single token including formatting. Every change affects probability distributions in non-obvious ways that cascade through generation.

API providers also make model updates that change behavior. What they call "GPT-5" today may be a different model checkpoint than "GPT-5" six months from now. The API identifier stays constant but the underlying model improves. Temperature 0 gives you consistent behavior within a model version. It does not give you consistent behavior across model versions that happen transparently. You need API version pinning and model checkpoint tracking for that level of stability.

The practical implication is that temperature 0 gives you functional determinism, not absolute determinism. You can trust that the same prompt will produce the same answer in the same request and in rapid succession. You cannot trust that it will produce the same answer byte-for-byte across months of production use. For most applications, functional determinism is sufficient. For applications requiring cryptographic-level reproducibility, you need additional controls like model version locks and output hashing.

A financial auditing platform discovered this in November 2025 when they needed to reproduce analysis from six months prior for regulatory review. They had used temperature 0 but the model provider had updated their infrastructure twice. Outputs differed slightly from historical records. The platform implemented model version pinning and output checksums to ensure future reproducibility, but the incident cost them $40,000 in audit delays.

## Seed Parameters Provide Reproducibility for Testing

Modern LLM APIs offer **seed parameters** that enable reproducible outputs even at non-zero temperatures. You provide an integer seed value with your request. The API uses that seed to initialize its random number generator. Identical prompts with identical seeds produce identical outputs, even at temperature 1 or higher. This determinism is useful for testing but comes with caveats about when it applies.

Seeds solve the reproducibility problem for testing and debugging workflows. You run your evaluation set with seed 12345. You get a specific set of outputs. You modify your prompt. You re-run with seed 12345. You get a different set of outputs that you can directly compare to the original baseline. This enables proper A/B testing of prompt variants with controlled randomness. Without seeds, comparing prompts at temperature 0.7 requires running each variant hundreds of times and comparing distributions.

Seeds also help with user-reported issues. A user reports unexpected or problematic output. Your logs include the prompt and seed. You replay the exact request with the same seed and reproduce the issue perfectly. Without seeds, you can only approximate reproduction at non-zero temperatures by running the same prompt multiple times and hoping to hit the same random state.

The limitation is that seeds only guarantee reproducibility within the same model version and API infrastructure. If the API provider updates their model or serving stack, your seed will produce different outputs even with identical prompts. Seeds are session-level reproducibility, not long-term versioning. This is usually fine for short-term debugging but insufficient for long-term regression testing across model updates.

You use seeds in testing environments and controlled experiments. You typically do not expose seed control to end users because it adds interface complexity without value for most use cases. The exception is creative tools where users want to explore variations but lock in a specific output they like. Seed becomes a "save this variation" feature that lets users bookmark specific random states.

An e-learning platform used seeds effectively in December 2025. They generated quiz questions at temperature 0.9 for variety. When teachers found questions they liked, they could "save" them, which stored the seed. The same seed reliably regenerated the same question for reuse across classes. This gave teachers both creative variety and selective reproducibility.

## Top-P and Top-K Are Alternative Sampling Strategies

**Top-p sampling** (also called nucleus sampling) is an alternative to temperature for controlling randomness. Instead of adjusting the probability distribution with a temperature parameter, top-p truncates it by considering only tokens whose cumulative probability mass reaches p. If p is 0.9, the model samples from the smallest set of tokens that collectively represent 90 percent probability mass. Tokens outside this nucleus are excluded from consideration entirely.

Top-p is more stable than temperature for controlling creativity without generating nonsense. Temperature affects all tokens equally by scaling probabilities. Setting temperature to 1.5 might turn reasonable variation into gibberish because it amplifies very low probability tokens that should never be selected. Top-p prevents this by cutting off the tail distribution. Even with aggressive top-p settings like 0.7, the model only samples from tokens that had non-negligible probability in the original distribution.

You use top-p when you want variability but need to avoid nonsensical outputs that pure temperature allows. Creative writing with top-p 0.9 gives diverse outputs while filtering extreme randomness. The same task at temperature 1.5 might produce incoherent text that wanders into impossible grammar or contradictory statements. Top-p provides creativity with guardrails that temperature does not offer.

**Top-k sampling** is a cruder version of top-p that was common in earlier systems. The model considers only the k most likely tokens at each step. Top-k 40 means the model samples from the 40 highest probability tokens and ignores everything else regardless of cumulative probability mass. This is less adaptive than top-p because the cutoff is a fixed count, not a probability mass that adjusts to distribution shape.

Top-k is rarely used in modern APIs because top-p is strictly superior for most use cases. Sometimes the top 10 tokens represent 95 percent of probability mass. Sometimes you need the top 50 to reach 90 percent. Top-p handles both cases correctly by adapting to the shape of the distribution. Top-k applies a blunt cutoff that ignores distribution shape and can either be too restrictive or too permissive.

A content generation platform tested temperature vs top-p in January 2026 for blog post generation. Temperature 0.8 produced creative content but 15% of outputs contained grammatical errors or nonsensical phrases. Top-p 0.9 at temperature 1.0 produced equally creative content with only 3% error rate. The adaptive cutoff prevented low-probability garbage tokens while preserving creative variety.

## Frequency and Presence Penalties Reduce Repetition

**Frequency penalty** and **presence penalty** are parameters that discourage token repetition during generation. Frequency penalty reduces the probability of tokens proportionally to how many times they have already appeared. Presence penalty applies a flat reduction to any token that has appeared at least once, regardless of repetition count. Both parameters help avoid the repetitive outputs that language models tend to produce in long-form generation.

You apply frequency penalty when the model tends to repeat phrases or ideas in a way that degrades output quality. Long-form content generation often shows this pattern. The model finds a phrase structure it likes and uses it repeatedly across paragraphs. "This is important because..." might appear five times in a 500-word response. Frequency penalty breaks this pattern by making repeated tokens progressively less likely each time they appear.

Presence penalty is blunter but sometimes more effective. It just discourages reusing any token that has appeared at all, regardless of how many times. This pushes for vocabulary diversity across the entire output. You use it when outputs feel monotonous or when you need the model to explore more varied phrasing and avoid getting stuck in local patterns.

Both penalties are typically small values between 0 and 2. Values above 1 create strong anti-repetition pressure that can hurt coherence. The model might avoid necessary repetition of key terms or proper nouns. Values below 0.5 are subtle nudges that reduce repetition without forcing unnatural variation. The right value depends on your use case and how repetitive the model's default behavior is for your prompts.

The architectural consideration is whether repetition is a bug or a feature for your application. Technical documentation benefits from consistent terminology, which means repetition is good. You want to use the same term for the same concept every time. Creative fiction benefits from varied language, which means repetition is bad. You want diverse descriptions and fresh phrasing. You set penalties based on product requirements, not based on random experimentation.

A technical writing platform tested frequency penalties in September 2025. Without penalties, their model used "utilize" instead of "use" inconsistently, creating confusing documentation. With frequency penalty 0.3, the model locked onto one term early and maintained consistency. This improved documentation clarity without requiring explicit style guides in the prompt.

## Combining Parameters Creates Complex Behavioral Spaces

Temperature, top-p, frequency penalty, and presence penalty can be combined, creating a large configuration space. Most of this space is meaningless or redundant. A few combinations are meaningful for specific use cases. Understanding which combinations make sense prevents cargo-culting settings from examples.

Temperature 0 with any other parameters set is nonsensical. Temperature 0 makes sampling deterministic by always selecting the highest probability token, which means top-p, frequency penalty, and presence penalty have no effect on token selection. If you see code that sets temperature to 0 and top-p to 0.9, someone copied settings without understanding them. The top-p setting does nothing.

Temperature greater than 0 combined with top-p is a valid combination. You use temperature to set overall randomness level and top-p to cap maximum randomness. Temperature 0.8 with top-p 0.9 gives creative sampling that still filters extreme outliers from the tail of the distribution. This is a common setting for content generation where you want variety but not chaos.

Frequency penalty with presence penalty is usually redundant. Both discourage repetition through different mechanisms. Setting both creates compounding anti-repetition pressure that often hurts coherence. The model starts avoiding reasonable repetition of proper nouns or key concepts. Pick one based on whether you want proportional discouragement (frequency) or flat discouragement (presence), but do not use both unless you have strong empirical evidence it helps.

The right mental model is that you have one primary randomness parameter (temperature or top-p, usually temperature) and optionally one anti-repetition parameter (frequency or presence penalty). Anything more complex is likely unnecessary and definitely untested in your specific domain.

## Sampling Settings Belong in Version Control with Prompts

Sampling parameters are part of prompt architecture. They define behavior as much as prompt text does. A prompt that works well at temperature 0 may fail at temperature 0.7 because the task requires determinism. When you version control your prompts, you must version control their associated sampling parameters as part of the same artifact.

The implementation is straightforward. Your prompt definitions in code include both the prompt text and a configuration object with sampling parameters. You never set sampling parameters in application logic separate from prompt definitions. This creates hidden dependencies that break when someone modifies one without the other. The prompt and its parameters are a single unit.

You also document why each parameter is set to its value. "Temperature 0 because contract extraction requires deterministic outputs for legal compliance" explains the reasoning. Future engineers understand the decision and its business justification. "Temperature 0.7 because examples used it" is technical debt. It reveals that no one made a conscious decision based on requirements.

Sampling parameters also require testing as part of prompt evaluation. Your evaluation set should test with the production sampling parameters. If you evaluate at temperature 0 for speed and consistency but deploy at temperature 0.7, your evaluation tells you nothing about production behavior. This mistake is common and dangerous. Teams optimize prompts at temperature 0, ship at 0.7, and wonder why production quality is lower than test quality.

A customer support platform made this error in October 2025. They evaluated response quality at temperature 0, achieving 95% accuracy. They deployed at temperature 0.5 to add "personality," but accuracy dropped to 87% in production. Users complained about inconsistent responses. The team reverted to temperature 0 and added personality through prompt design instead of sampling randomness.

## Different Tasks Within One Application Need Different Settings

A complex AI application has multiple prompts for different tasks. Each task may need different sampling parameters based on its requirements. Your legal document analysis application might use temperature 0 for clause extraction, temperature 0.5 for summarization where some variation is acceptable, and temperature 0.9 for generating plain-language explanations where creativity helps. This is correct architecture.

The mistake is using global sampling configuration. You set temperature once in an environment variable or config file and apply it to all prompts. This assumes all tasks have identical randomness requirements. They do not. Extraction needs determinism. Creative explanation needs variation. Summary writing might benefit from moderate randomness. One setting cannot serve all needs appropriately.

You implement per-task sampling by defining parameters with each prompt as part of its configuration. Your prompt for extraction includes temperature 0 in its metadata. Your prompt for summarization includes temperature 0.5. Your LLM client reads these settings and applies them per request. This makes sampling parameters explicit and task-specific rather than hidden global state.

This also enables independent optimization. You can experiment with different temperatures for summarization without affecting extraction. You can update extraction prompts without considering how temperature changes affect other tasks. Each prompt is a self-contained unit with its own behavioral contract defined by text and parameters together.

## Sampling Parameters Are Rarely the Solution to Prompt Problems

When a prompt produces poor outputs, the first instinct is often to adjust temperature or other sampling parameters. This is usually wrong. Sampling parameters cannot fix prompts that have unclear instructions, insufficient examples, or structural problems. They can only adjust randomness in an already-functional prompt that works but needs behavior tuning.

If your extraction prompt sometimes misses clauses, lowering temperature will not help. The problem is prompt design, not randomness. You need clearer instructions, better examples, or explicit validation steps in the prompt. Temperature 0 just makes the model confidently wrong in the same way every time instead of wrong in varied ways.

If your creative content generation feels repetitive at temperature 0.7, raising temperature might help by introducing more variety. But it might also produce nonsense. The better solution is usually improving the prompt to encourage varied structure or providing more diverse examples that teach the model different patterns. Temperature is a last resort after prompt quality is optimized.

The architecture principle is that you design prompts first to be as good as possible, then set sampling parameters to match the task requirements. You do not iterate on sampling parameters to compensate for prompt weaknesses. This creates superstition where you believe temperature 0.73 works better than 0.7 for mysterious reasons. The real reason is usually that you have a mediocre prompt and you got lucky with that specific setting.

## Production Systems Need Sampling Parameter Monitoring

Once you deploy prompts with specific sampling parameters, you need to monitor whether those parameters remain appropriate as your system evolves. User behavior changes over time. Input distributions shift. Model updates alter default behavior and probability distributions. Sampling parameters that worked perfectly at launch may need adjustment six months later.

You monitor output diversity metrics to detect when parameters may need tuning. For deterministic tasks at temperature 0, you should see identical outputs for identical inputs. If you start seeing variation, something changed: model update, prompt drift, or infrastructure change. For creative tasks at higher temperatures, you measure output uniqueness over time using similarity metrics. Decreasing uniqueness suggests you need higher temperature or different prompts.

You also monitor user satisfaction correlated with sampling parameters through A/B tests and feedback mechanisms. If users increasingly report that outputs feel repetitive, your frequency penalty may be too low or your temperature too constrained. If users report nonsensical or incoherent outputs, your temperature may be too high. These signals guide parameter adjustments based on real usage.

The key is having telemetry that connects sampling parameters to outcomes. You log which parameters were used for each request. You track which outputs users accepted versus rejected. You correlate these datasets to identify when parameter changes might improve results. Without this data, parameter tuning is superstition based on hunches.

## Sampling Strategy Is a Product Specification

The legal startup's mistake was treating temperature as an implementation detail that engineers set during development. It is not. Temperature defines whether your product gives consistent or varied outputs. This is a core product characteristic that users experience directly. It belongs in product requirements documents, not hidden in code as a magic number.

Your product spec should state: "Contract extraction must produce identical results for identical inputs to ensure legal compliance and audit consistency." This requirement translates directly to temperature 0 with no room for interpretation. The spec should also state: "Marketing copy generation should produce at least five distinct options for each request to give users choice." This requirement suggests higher temperature or multiple samples at lower temperature.

Making sampling parameters explicit in product specs ensures alignment between product intent and implementation. It also makes testing requirements clear. If the spec says outputs must be deterministic, your test suite verifies determinism. If the spec says outputs should be diverse, your test suite measures diversity using similarity metrics and checks that outputs meet minimum divergence thresholds.

Sampling parameters stop being mysterious tuning knobs when you frame them as product requirements. Temperature is not a performance optimization you tweak for marginal gains. It is the answer to the fundamental question: should this feature be consistent or creative. That is a product question with a clear answer based on user needs and use case requirements. You make the architectural decision accordingly and document it as part of the feature specification.

The next subchapter transitions from sampling strategies to model-specific prompt design, examining how Claude, GPT, Gemini, Llama, and Mistral require different approaches for optimal performance.
