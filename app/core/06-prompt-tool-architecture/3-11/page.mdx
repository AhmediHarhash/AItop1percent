# 3.11 — Video and Temporal Sequence Prompting

A retail security company wasted $220,000 between August and November 2025 deploying an AI system that analyzed surveillance footage for shoplifting detection. The team of nine engineers used GPT-5's video understanding capabilities to process footage from 140 store locations, replacing human monitors for night shifts. Initial tests on curated 30-second clips showed 91% accuracy at identifying theft events.

The system failed catastrophically in production. It flagged ordinary shopping behavior as theft, missed actual theft when perpetrators moved slowly, and generated thousands of false positives that overwhelmed security staff. The root cause was treating video as a collection of independent frames rather than a temporal sequence. Prompts asked "Is theft occurring in this video?" without specifying what temporal patterns constitute theft versus normal browsing. A customer picking up an item, examining it, and setting it down appeared identical to someone concealing merchandise when analyzed frame by frame without temporal context. When the team rewrote prompts to explicitly track object movement patterns over time, identify suspicious temporal sequences, and distinguish completion of actions from intermediate states, accuracy improved to 87% with 90% fewer false positives. They learned that video understanding requires temporal reasoning, not just visual recognition.

## The Temporal Dimension of Video Understanding

Video is fundamentally different from images because it encodes change over time. A single frame shows a state; a sequence of frames shows a process. Understanding video requires reasoning about how entities move, how scenes evolve, and how actions unfold across temporal spans.

Modern multimodal models process video by sampling frames at intervals and analyzing each frame's visual content. GPT-5, Claude Opus 4.5, and Gemini 2.0 can accept video inputs, but their temporal reasoning capabilities vary. They see what's in each frame but must infer relationships between frames through prompted reasoning.

Your prompts must explicitly guide temporal analysis. A prompt that says "describe this video" produces a list of observations about individual frames. A prompt that says "describe how the scene changes from the beginning to the end of the video, noting the sequence of events and how each event leads to the next" produces temporal reasoning.

The challenge scales with video length. A 10-second clip with 30 frames per second contains 300 frames, but models typically sample far fewer frames for processing. If the model samples 10 frames, it sees snapshots at roughly 1-second intervals. Your prompts must work with this temporal granularity, not assume the model perceives smooth motion.

Frame sampling strategies affect what the model can detect. Uniform sampling at fixed intervals works for continuous processes but might miss brief events. Event-based sampling that clusters frames around motion or scene changes captures key moments but requires preprocessing. Your prompt design must account for the sampling strategy your system uses.

## Frame Analysis and Scene Segmentation

Before reasoning about temporal sequences, you need effective frame-level analysis. Each frame must be interpreted correctly for temporal reasoning to work. Your prompts should structure frame analysis systematically.

For surveillance applications, specify what to look for in each frame. "In each frame, identify: number of people visible, location of each person (left/center/right, near/far), what objects each person is holding, what actions each person is performing, any objects on shelves or counters." This structured extraction creates a foundation for temporal tracking.

Scene segmentation divides videos into meaningful chunks. A 5-minute video might contain multiple distinct scenes: a person entering a store, browsing products, approaching the checkout, completing a transaction. Your prompts should identify scene boundaries. "Watch for scene transitions marked by: camera angle changes, major subject changes, significant time gaps indicated by timestamps, or completion of activities."

Object persistence tracking connects the same entity across frames. "Track each person across frames. Assign consistent identifiers. Note when people enter or exit the frame. If a person visible in frame 10 also appears in frame 15, confirm they are the same individual based on clothing, position, and movement continuity."

Background versus foreground separation matters for focusing attention. "The background contains shelves, walls, and fixtures that remain static. The foreground contains people and objects that move. Focus analysis on foreground elements and their changes. Only note background changes if they indicate significant events."

Lighting and quality variations across frames can confuse analysis. "Video quality may vary due to lighting changes, camera auto-adjustment, or compression artifacts. Maintain object tracking even if appearance varies slightly between frames due to these technical factors."

## Temporal Reasoning Across Frame Sequences

Once you have frame-level understanding, temporal reasoning connects frames into meaningful sequences. This requires prompts that explicitly specify how to infer causality, progression, and completion of actions.

Action recognition needs temporal context. "Identify actions by analyzing frame sequences, not individual frames. A person reaching toward a shelf in one frame might be picking up an item or returning it. Only determine the action by seeing the full sequence: hand approaches object, grasps object, object moves, hand returns."

Direction and trajectory analysis reveals intent. "Track the movement path of each person and object. Note: where did they come from, where are they going, is the path direct or wandering, do they approach high-value items, do they move toward exits." Movement patterns differentiate browsing from purposeful navigation.

Temporal anomaly detection identifies deviations from normal patterns. "Normal shopping sequences follow patterns: enter store, browse multiple areas, select items, proceed to checkout, exit. Flag sequences that deviate: entering and moving directly to exits, concealing objects, leaving without checking out, returning repeatedly to the same location."

State change tracking monitors how situations evolve. "Track the state of objects: item on shelf → item in hand → item in bag → item at checkout or item concealed. Missing states in the expected sequence indicate anomalous behavior."

Temporal windowing defines how much history to consider. "When evaluating whether current behavior is suspicious, consider the previous 30 seconds of activity. Isolated actions may appear innocuous but become suspicious in context. A person looking around constantly for 30 seconds before picking up an item suggests different intent than someone who immediately grabs an item."

## Event Detection and Classification

Events are temporal patterns that matter for your application. Detecting events requires defining them in terms of temporal features: what happens, in what order, over what duration, with what frequency.

For security applications, define threat events precisely. "A shoplifting event consists of: person picks up merchandise, looks around checking for observers, conceals item in clothing or bag, walks past checkout area without paying, exits store. All steps must occur in sequence within a 5-minute window."

Sports analytics require sport-specific event definitions. "In basketball footage, detect scoring events: player takes shot motion (arm raises with ball), ball leaves hand, ball travels toward basket, ball enters basket or misses. Classify shot type based on player distance from basket and motion pattern."

Manufacturing quality control events are equipment-state based. "On the assembly line, detect defect events: component approaches station, robotic arm positions component, alignment check fails (indicated by component shifting), arm repeats positioning attempt more than twice, component marked for rejection."

Medical procedure events follow protocol sequences. "In surgical video, identify procedural milestones: incision made, target tissue exposed, instrument inserted, procedure performed (varies by surgery type), instrument removed, closure initiated. Flag protocol violations where steps occur out of sequence or expected steps are skipped."

Event boundaries often blur. A person might gradually transition from shopping to theft-related behavior. Your prompts should handle ambiguous boundaries. "If behavior appears to transition from normal to suspicious, note the approximate frame where suspicion begins. Don't force a precise boundary if the transition is gradual."

## Temporal Summarization Strategies

Long videos require summarization to extract key information without overwhelming downstream systems or human reviewers. Effective summarization preserves critical temporal information while reducing volume.

Hierarchical summarization works for structured content. "Create a three-level summary: Level 1 (5 seconds): List major events in sequence. Level 2 (2 sentences): Describe overall narrative arc. Level 3 (1 phrase): Categorize video type and outcome." This gives consumers of your output options for detail level.

Keyframe extraction identifies representative frames. "Select 3-5 keyframes that capture critical moments: establishing shot showing initial scene, frames showing key action initiation, peak action frame, resolution frame, final state frame. Provide timestamps for each keyframe."

Event timeline construction creates structured temporal representations. "Generate a timeline with entries for each significant event: timestamp, event type, participants, outcome, confidence score. Format as JSON array sorted chronologically."

Motion-based summarization focuses on change. "Summarize periods of significant motion or activity in detail. Compress periods where the scene remains static into brief descriptions. A person standing still for 45 seconds becomes 'Subject remained stationary near entrance for 45 seconds.'"

Comparative summarization highlights differences. "This video shows before and after states. Describe: initial state at video start, major changes that occurred during the video, final state at video end, time required for the transformation."

## Handling Long Videos Through Frame Sampling

Processing constraints limit how many frames models can analyze. A 10-minute video at 30 fps contains 18,000 frames. Current models might analyze 100-200 frames at most. Your prompts must work within these limitations.

Uniform sampling takes frames at fixed intervals. "This video was sampled at 1 frame per second. You are seeing a subset of frames with approximately 1-second gaps between them. Infer motion and transitions that occurred during gaps based on state differences between consecutive frames."

Adaptive sampling varies frame density based on content. "Frame density is higher during periods of activity and lower during static periods. Timestamps indicate real-world time, not uniform sampling. When you see dense frame sequences, significant activity is occurring."

Multi-pass processing analyzes long videos in stages. "Pass 1: Analyze every 10th frame to identify periods of interest where activity occurs. Pass 2: For periods of interest identified in Pass 1, analyze frames at higher density to extract details."

Sliding window analysis processes videos in overlapping chunks. "Process this video in 30-second windows with 10-second overlap. For each window, identify events and track entities. Reconcile entity identities across overlapping regions to maintain consistent tracking throughout the full video."

Temporal super-resolution infers missing information. "Based on the sampled frames provided, estimate what occurred between frames. If a person appears at position A in frame 10 and position B in frame 20, infer that they moved continuously between these positions during the intervening seconds."

## Motion Pattern Analysis and Trajectory Tracking

Motion reveals intent and identifies anomalies. Effective motion analysis requires prompts that specify what motion patterns to look for and how to interpret them.

Velocity and acceleration patterns distinguish behavior types. "Track how fast each person moves. Sudden acceleration toward exits suggests fleeing. Very slow, deliberate movement while handling merchandise suggests concealment attempts. Normal browsing involves varied speeds with frequent stops."

Path analysis reveals navigation intent. "Track the path each person takes through the space. Direct paths to specific locations suggest purposeful navigation. Circuitous paths with backtracking suggest browsing. Paths that systematically avoid camera lines of sight suggest evasion."

Proximity tracking monitors interactions. "Note when people come within 2 meters of each other. Track how long they remain in proximity. Identify if they interact, exchange objects, or communicate. Distinguish coordinated movement (people moving together) from coincidental proximity."

Loitering detection identifies prolonged presence. "Flag individuals who remain in the same general area for more than 3 minutes without clear purpose. Consider purpose indicators: examining merchandise, waiting in line, talking on phone, meeting someone."

Exit pattern analysis detects suspicious departures. "Track paths toward exits. Normal exit patterns: approach checkout, complete transaction, proceed to exit at normal walking pace. Suspicious patterns: approach exit without checkout, frequent backward glances, sudden acceleration toward exit, exiting immediately after concealment behavior."

## Audio-Visual Temporal Integration

Many videos include audio tracks that provide temporal context beyond visual information. Integrating audio analysis with visual analysis creates richer understanding.

Sound event synchronization connects audio events to visual events. "Identify audio cues and correlate with visual events: alarm sounds → check for triggered security events, breaking glass → look for dropped items or vandalism, loud voices → identify conflict or emergencies in visual frames."

Speech transcription adds semantic context. "Transcribe audible speech. Associate transcribed text with the speaker visible in video frames. If speech content contradicts visible behavior (person saying 'just looking' while concealing items), flag the inconsistency."

Audio-based scene segmentation detects transitions. "Use audio transitions to identify scene changes: background music changes indicate scene shifts, silence after continuous noise suggests event completion, sudden loud sounds indicate incidents requiring visual analysis."

Temporal audio patterns reveal events. "Background noise levels indicate activity levels. Sustained elevated noise suggests crowds or busy periods. Sudden silence after noise suggests everyone stopped and turned attention to something. Use these audio patterns to guide visual analysis priority."

Off-screen event detection uses audio cues. "When audio suggests events outside the camera's view (sounds of crashing, shouting, alarms), note these as off-screen events. Describe what you can infer from audio even without visual confirmation."

## Quality Issues in Real-World Video

Production video data contains quality problems that affect analysis reliability. Your prompts must account for compression artifacts, poor lighting, occlusion, and camera issues.

Compression artifacts create temporal inconsistencies. "Video compression may cause blurring during motion, blockiness in low-light areas, or color banding. Don't interpret compression artifacts as meaningful scene changes. Focus on consistent features that persist across multiple frames."

Lighting variations affect visibility. "Scene lighting varies throughout the video. In dark frames, detail is limited; make inferences based on shapes and motion. In overexposed frames, bright areas lose detail; focus on properly exposed regions. Maintain entity tracking despite appearance variations due to lighting."

Occlusion breaks visual continuity. "Objects and people may be partially or fully occluded by foreground elements, other people, or camera blind spots. When an entity disappears behind occlusion, track its last known position and velocity. When it reappears, confirm identity based on appearance and expected trajectory."

Camera movement and shake affect analysis. "The camera may move or shake, causing the entire frame to shift. Distinguish between camera movement (entire frame shifts) and subject movement (subject position changes relative to background). Stabilize your analysis by tracking relative positions."

Low frame rate creates motion blur and jumped positions. "At low frame rates, fast motion appears as blur or sudden position changes. Infer smooth motion between frames rather than interpreting jumped positions as teleportation. Acknowledge uncertainty about exact motion paths."

## Cost Management for Video Processing

Video processing costs scale rapidly with video length and frame sampling density. A 10-minute video at 1 frame per second costs roughly 10 times more to process than a single image prompt.

Token economics vary by model. Gemini 2.0 offers competitive pricing for video inputs. GPT-5 charges per frame with each frame consuming tokens equivalent to image processing. Claude Opus 4.5's video handling capabilities have their own pricing structure. Calculate costs based on your specific model choice and sampling strategy.

Selective processing reduces costs. "Pre-filter videos to identify segments containing activity using lightweight motion detection. Only send active segments to the multimodal model for detailed analysis. Compress inactive periods into simple descriptions generated by cheaper methods."

Resolution reduction trades quality for cost. "Process videos at 480p instead of 1080p when high detail isn't critical. Lower resolution reduces tokens per frame while preserving enough detail for most tasks. Use high resolution only for videos requiring fine detail analysis."

Frame count optimization balances comprehension and cost. "Test your use case with different sampling rates: 1 frame per 5 seconds, 1 per 2 seconds, 1 per second. Find the minimum sampling rate that maintains acceptable accuracy. Many applications work fine with 1 frame per 3-5 seconds."

Batch processing amortizes costs. "When processing multiple videos, batch them into single requests when possible. Models may offer better per-frame pricing for larger batches. Balance batch size against latency requirements."

## Use Case Specific Patterns

Different applications require different video analysis approaches. Tailoring prompts to your specific use case improves accuracy and efficiency.

Retail security needs theft-specific temporal patterns. "Focus on: hands touching merchandise, items moving toward concealment locations (bags, clothing, pockets), individuals avoiding staff line of sight, checkout avoidance, exit paths. Build suspicion scores incrementally as suspicious behaviors accumulate."

Sports analytics requires game-specific event libraries. "For soccer, detect: passes (ball moves between players), shots (ball travels toward goal at high speed), goals (ball crosses goal line), fouls (irregular contact between players causing play stoppage). Time each event precisely."

Manufacturing inspection tracks product states. "Monitor each product as it moves through the assembly line. Track: arrival at station, operation performed, quality check, pass/fail determination, exit from station. Flag products with dwell times outside expected ranges."

Healthcare monitoring watches for patient events. "In patient room video, detect: patient movement from bed, fall events (rapid downward motion ending in prone position), staff entry and exit, emergency situations (sudden activity, multiple staff entering)."

Traffic analysis measures flow and incidents. "Count vehicles crossing screen regions per time unit to measure flow rates. Detect incidents: stopped vehicles in travel lanes, collision events, pedestrians in roadways, unusual congestion patterns."

The next subchapter examines when to use long context windows versus retrieval augmented generation, providing a decision framework for choosing the right architecture for your specific needs.
