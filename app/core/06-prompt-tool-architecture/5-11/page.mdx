# 5.11 â€” Prompt Documentation Standards

A Series C healthcare company deployed an AI-powered patient intake system in February 2024 that collected symptoms, medical history, and insurance information. The system worked well for nine months. In November 2024, a regulatory audit discovered that the prompt had been modified in June to expedite certain insurance verifications. Nobody remembered making the change. The Git log showed engineer Sarah Chen committed it with the message "update intake flow." Sarah had left the company in August. The diff showed 47 lines changed, but without context, auditors could not determine whether the changes complied with HIPAA requirements.

The company had no prompt documentation standard. Some prompts had detailed comments explaining every decision. Others had one-line descriptions. Most had nothing. When regulators asked why the system collected certain information in certain orders, nobody could answer definitively. The original product requirements from 2023 did not match current behavior. The company spent 180,000 dollars reconstructing prompt intent from git history, Slack messages, and interviews with former employees. They paid 1.2 million dollars in fines for documentation failures. The prompt itself was compliant. But they could not prove it.

The lesson was brutal: **prompt documentation** is not optional metadata you add if you have time. It is the institutional memory that preserves intent, the audit trail that demonstrates compliance, and the map that guides future engineers when you are gone. Without it, every prompt becomes a black box where changes are dangerous, debugging is guesswork, and regulatory exposure is unbounded.

## Why Documentation Fails by Default

Most engineering teams treat prompt documentation like code comments: nice to have, easily skipped, inevitably stale. This fails for prompts in ways it fails for code, plus additional ways unique to natural language instructions.

Code has executable semantics. You can read the code and understand what it does, even without comments. A function named calculate discount that multiplies price by percentage is self-explanatory. Prompts have no such clarity. Natural language is ambiguous. A prompt instruction "respond professionally" means different things to different people. Without documentation explaining what professionally means in your specific context, the instruction is interpretation-dependent.

Prompts encode business logic that exists nowhere else. In traditional software, business rules live in requirements documents, design specs, or at minimum in variable names and function logic. Prompts often contain rules that exist only in the prompt text itself. "For claims over 50,000 dollars, require two levels of approval unless the customer tier is platinum and the claim type is weather-related." This rule might not be documented anywhere else. If you delete that sentence without understanding why it exists, you break compliance.

Documentation drift is worse for prompts than code because prompts change more frequently. Code changes are usually localized: modify one function, update its tests, done. Prompt changes often involve rewording large sections, adding examples, or adjusting tone. The documentation needs to explain not just what changed but why. Without this discipline, documented behavior diverges from actual behavior within weeks.

Engineers also skip documentation because they do not know what to document. Should you document every word choice? Every example? The temperature setting? Without standards, engineers either document everything (creating unreadable walls of text) or nothing (creating mystery prompts). Both fail.

Accessibility compounds the problem. Documentation living in wikis disconnected from code becomes outdated immediately. Documentation in code comments is better but still easy to skip. Documentation that is not version-controlled alongside prompts is documentation that will lie to you.

The final failure mode is treating documentation as a post-deployment afterthought. You build the prompt, tune it, deploy it, then maybe write docs if you have time. This produces documentation that describes current behavior without explaining how you got there, what alternatives you tried, or what constraints you must maintain.

## The Specification Template

Effective prompt documentation starts with a required template that every prompt must complete before deployment. This template is not optional. It gates promotion to production.

The metadata section captures who, when, and what. Prompt name, version number following semantic versioning, author, creation date, last modified date, and link to the originating requirement or ticket. This section answers basic provenance questions. Who has context about this prompt's design? When was it created? What drove its creation?

Version numbering must follow a consistent scheme. Semantic versioning is common: major version for breaking changes, minor version for new features, patch version for bug fixes. A change from 2.3.1 to 3.0.0 signals breaking behavior changes. A change from 2.3.1 to 2.4.0 signals new capabilities. A change from 2.3.1 to 2.3.2 signals a bug fix. This signal helps downstream consumers understand update risk without reading diffs.

The author field is not about blame. It identifies who has institutional knowledge about design decisions. When documentation proves insufficient, you need to ask someone why certain choices were made. When that person leaves, documentation becomes the only knowledge source. This makes comprehensive documentation even more critical.

The intent description explains why this prompt exists and what business problem it solves. Write this for product managers, not engineers. Describe desired behavior in domain terms. Specify success criteria that define correct operation. "This prompt classifies customer support tickets into seven categories to route them to appropriate specialist teams. Success means 95 percent accuracy on the validation set, with less than 2 percent of tickets misrouted to teams that cannot help."

Include examples of successful outcomes. If the prompt extracts structured data, show what correct extraction looks like. If it generates responses, show examples of excellent responses. These concrete examples transform abstract intent into observable behavior that anyone can verify.

The input schema documents every variable the prompt accepts. For each variable, specify type, format, required versus optional, valid ranges or enumerations, and examples. If the prompt expects JSON, provide JSON schema. If it expects free text, describe length limits and content expectations. "customer name: string, required, 1 to 100 characters, example: John Smith. transaction amount: decimal, required, 0.01 to 999999.99, example: 1234.56. transaction currency: string, optional, ISO 4217 code, example: USD."

Input validation requirements belong here. If the prompt assumes inputs have been sanitized, document that assumption. If it breaks when inputs exceed certain lengths, document those limits. If it requires specific preprocessing, document what must happen upstream. These constraints prevent misuse and guide system design.

The output schema documents expected response format. Specify whether outputs are JSON, markdown, plain text, or another format. Define required fields, optional fields, and validation rules. Provide examples of valid outputs and examples of edge case outputs. "Output is JSON with fields: classification (string, one of seven values), confidence (decimal 0 to 1), reasoning (string, 50 to 500 characters), escalation flag (boolean). Example: classification: billing issue, confidence: 0.87, reasoning: Customer mentioned invoice discrepancy and payment confusion, escalation flag: false."

Document failure modes. When the prompt cannot complete the task, what does it output? An error structure? A partial result with flags? A sentinel value? Downstream systems need to detect and handle failures. The schema must specify how failures manifest. "If input is malformed, output: classification: ERROR, confidence: 0, reasoning: input validation failed, escalation flag: true."

## Behavioral Constraints Documentation

Constraints are often the most important and most neglected part of prompt documentation. Every explicit constraint in your prompt must be documented with rationale.

If the prompt must refuse certain requests, document what triggers refusal and what the refusal message should contain. "Prompt must refuse requests to disclose patient medical records without proper authorization. Refusal message: I cannot provide medical records without proper authorization. Please contact our records department at 555-0123." This makes the constraint testable and verifiable.

Document why constraints exist. "This refusal behavior is required by HIPAA regulations 45 CFR 164.508. See compliance doc reference HIPAA-2024-08." Future engineers need to know which constraints are nice-to-have preferences and which are legal requirements. The documentation must make this distinction explicit.

Constraints often embed in subtle phrasing that is invisible to casual readers. "Respond professionally and concisely" is a constraint, but it is vague. Your documentation must operationalize it. "Professionally means: use formal language, avoid slang, address users as Mr. or Ms. unless they specify otherwise, never use profanity. Concisely means: responses under 100 words for simple questions, under 300 words for complex questions, use bullet points for lists of three or more items."

Some constraints interact in non-obvious ways. Requiring both speed and thoroughness creates tension. Requiring both conciseness and completeness conflicts. Document these tensions and explain how the prompt balances them. "We prioritize speed over thoroughness for tier 1 support. Responses should be fast and actionable, even if they do not cover every edge case. Tier 2 support prioritizes thoroughness. Same prompt, different parameters, different tradeoffs."

Compliance constraints are especially critical. If regulations dictate behavior, link to the specific regulatory text. If safety testing revealed that certain phrasings produce harmful outputs, document what testing was done and what constraints were added. "After red team testing in March 2025, we added explicit refusal of requests to generate misleading financial advice. Red team report: RT-2025-03-15. This constraint prevents SEC violations under regulation FD."

Document constraint evolution. When an incident leads to a new constraint, link the constraint to the incident. "Added constraint refusing to provide dosage information without physician confirmation after incident INC-2024-11-08 where incorrect dosage was nearly administered. This constraint is load-bearing. Do not remove without medical team approval and updated risk assessment."

## Known Limitations and Edge Cases

Every prompt has limitations. Documenting them honestly protects you when they cause issues and guides improvement efforts.

For each known limitation, describe the failure mode, estimate occurrence frequency, document workarounds if they exist, and link to tickets tracking fixes. "Prompt struggles with addresses in rural areas lacking standard street names. Failure rate approximately 3 percent of rural addresses. Workaround: flag these for human review. Tracking: TICKET-2847."

Be specific with examples. Do not write "struggles with ambiguous inputs." Write "struggles with addresses like RR 2 Box 45 where rural route numbers are ambiguous. Example input that fails: RR 2 Box 45 Smithville. Prompt extracts RR 2 as street name instead of recognizing rural route format. Expected: route: rural route 2, box: 45, city: Smithville. Actual: street: RR 2 Box 45, city: Smithville."

Document performance limitations. "Prompt requires 4 to 8 seconds latency at p95 due to complex reasoning. This is acceptable for batch processing but not for synchronous API responses." Or "Prompt achieves 94 percent accuracy but requires Claude Opus 4, costing 0.08 dollars per request. Lower-cost models degrade to 87 percent accuracy."

Scope limitations matter. "Prompt works well in English. Accuracy degrades to 78 percent in Spanish, 71 percent in French due to training data imbalance. International expansion requires language-specific prompt variants."

Track limitation history. "Known limitation: prompt incorrectly classifies sarcasm as literal sentiment. Discovered: August 2024. Attempted fix: added 15 sarcasm examples, accuracy improved from 81 to 84 percent but still inadequate. Accepted limitation: sarcasm detection requires separate specialized model. Workaround: flag ambiguous sentiment for human review."

Edge cases reveal hidden assumptions. "Prompt assumes US phone number format with area code. International phone numbers fail validation. Discovered when customer in Germany could not complete signup. This exposes assumption that all users are US-based. International expansion requires input validation updates and prompt modifications to handle international formats."

## Changelog as Living Documentation

The changelog is not supplementary. It is core documentation that shows how the prompt evolved and why.

Structure entries with date, version number, change type, description, and rationale. Change types might include: bug fix, feature addition, performance improvement, safety enhancement, compliance update, or deprecation. "2025-11-15, v2.3.0, feature addition: Added support for Canadian postal codes. Rationale: expanding service to Canada per TICKET-3421. Testing shows 96 percent accuracy on Canadian addresses."

The description must be specific enough that someone reading it six months later understands exactly what changed. Bad: "Updated prompt." Good: "Modified tone from formal to conversational for retail tier customers while maintaining formal tone for enterprise tier. Changed greeting from Dear valued customer to Hi there and simplified phrasing throughout response generation section."

Rationale explains why the change was necessary. "Change driven by customer feedback showing 73 percent of retail customers found formal tone off-putting. A/B test in October 2025 showed conversational tone increased customer satisfaction scores from 6.8 to 8.1. See experiment results: EXP-2025-10-12."

Reference before-and-after behavior with examples. "Before: Input query asking about return policy produced 280-word formal response starting with Dear Valued Customer, we appreciate your inquiry. After: Same query produces 95-word conversational response starting with Hi, happy to help with that. Response time decreased from 6 seconds to 3 seconds due to reduced output length."

Cross-reference related changes. "This change affects prompts v2.1.0, v2.2.0, v2.3.0 in the customer service family. All updated to maintain consistency. Related changes: CS-GREETING-v1.1, CS-CLOSING-v1.2."

Document what you tested. "Tested on 500 sample queries from production logs. Accuracy maintained at 94 percent. Tone shifted as intended per blind review by product team. No regressions detected in refusal behavior, classification accuracy, or output format compliance."

Changelogs accumulate institutional knowledge. A new engineer reading the changelog learns what works, what fails, why certain decisions were made, and what has been tried before. This prevents wasted effort re-exploring failed approaches and provides context for future changes.

## Failure Investigation Documentation

When prompts fail, documentation determines debug speed. The documentation should enable someone unfamiliar with the prompt to understand what it should do, what it did, and where the discrepancy occurred.

Include a troubleshooting section listing common failure modes and diagnostic steps. "Failure mode: output JSON missing required fields. Diagnosis: check input validation logs to confirm all required inputs were present. If present, check for input values outside expected ranges. Example: negative transaction amounts cause extraction to fail. Resolution: add input validation rejecting negative amounts upstream."

Document dependencies explicitly. "Prompt assumes inputs have been sanitized for SQL injection by the web application layer. If sanitization is bypassed, prompt may produce outputs containing malicious code. Dependency: input sanitization service v3.2 or higher."

Assumption documentation prevents hidden failures. "Prompt assumes customer tier field is always present and contains one of five valid values: free, basic, premium, enterprise, platinum. If tier is missing or invalid, classification logic breaks. Assumption added: January 2024. Violated: March 2025 when legacy customer records migrated without tier mapping. Resolution: added tier validation with default to free for missing values."

Severity classifications guide triage. "Critical failures: prompt discloses PII without authorization, violates regulatory requirements, produces harmful medical advice. Major failures: classification accuracy below 90 percent, response time exceeds 10 seconds. Minor failures: formatting inconsistencies, tone deviations, verbose outputs. Cosmetic issues: typos, spacing inconsistencies."

Include remediation steps for common failures. "If prompt produces malformed JSON: verify all examples in prompt are valid JSON, check for unescaped quotes in variable injection, validate output schema against JSON schema validator. If issue persists, add explicit format enforcement instruction in first 200 tokens of prompt."

Failure pattern analysis should feed back into documentation. "Analysis of 47 failures in Q4 2025 showed 38 involved inputs exceeding 2000 characters. Added limitation: prompt designed for inputs under 2000 characters. Inputs exceeding this should be truncated or summarized upstream. Tracking truncation implementation: TICKET-4521."

## Automated Documentation Validation

Documentation quality should be enforced automatically, not left to human discipline.

Build checks verifying documentation exists and follows the template. "Pre-commit hook checks: all prompts have metadata section with author, version, date. All prompts have intent description exceeding 100 words. All prompts have input schema with at least one documented parameter. All prompts have output schema with examples. All prompts have changelog entries for version increments."

Flag documentation drift. "Automated check: if prompt file modified but changelog not updated in same commit, block merge. Exception: patch version changes under 50 characters allowed without changelog for minor typos."

Version consistency validation. "Check: prompt file declares version 2.3.0, changelog contains entry for 2.3.0, Git tag v2.3.0 exists. If mismatch detected, fail CI build."

Schema validation ensures examples match specifications. "Input schema declares transaction amount as decimal. Check all examples in prompt and documentation to verify they are valid decimals. Output schema declares classification as one of seven values. Check all examples contain only those seven values."

Linking validation prevents broken references. "Documentation references TICKET-3421, HIPAA-2024-08, EXP-2025-10-12. Validate these references exist in ticket system, compliance database, experiment tracker. Flag broken references in CI."

Coverage metrics track documentation completeness. "Calculate documentation coverage: percentage of prompts with complete metadata, intent, schema, changelog. Team goal: 100 percent coverage for production prompts, 90 percent for experimental prompts. Dashboard shows current coverage, blocks deployment if production prompt coverage below 100 percent."

## Peer Review of Documentation

Code review should specifically evaluate documentation quality, not just code changes.

Reviewers verify intent matches implementation. "Prompt claims to classify into seven categories but implementation shows eight. Documentation is wrong or implementation has undocumented category. Reviewer rejects pending clarification."

Example validation is critical. "Documentation shows example output with confidence field as decimal 0 to 1. Prompt sometimes produces confidence as percentage 0 to 100. Examples do not match actual behavior. Reviewer requires fixing examples or implementation to achieve consistency."

Rationale review catches insufficiently explained changes. "Changelog says: updated tone. Reviewer asks: why? What feedback or data drove this? What alternative approaches were considered? What testing was done? Insufficient rationale means insufficient institutional knowledge. Reviewer requires expanded explanation."

Limitation honesty is reviewed. "Prompt documentation claims 96 percent accuracy but validation testing shows 91 percent. Documentation is misleading. Reviewer requires updating limitations section with accurate performance data and explanation of gap."

Compliance linkage verification. "Prompt documentation claims compliance with HIPAA but provides no specific regulatory references. Reviewer requires linking to specific HIPAA provisions and explaining how each constraint maps to regulatory requirements."

Accessibility review ensures documentation is understandable. "Documentation uses jargon unfamiliar to new team members. Reviewer requires plain language explanations or glossary entries. Documentation should be comprehensible to engineer joining the team six months from now."

## Integration with Systems

Prompt documentation should live close to prompts, ideally in the same repository with the same version control.

Co-location ensures synchronized updates. "Prompt lives in prompts/customer-service/classification.txt. Documentation lives in prompts/customer-service/classification.md. Both files version-controlled together. Changes to prompt require updating documentation in same commit."

Generated reference documentation extracts structured information from specifications. "Documentation generation tool parses all prompt documentation, extracts metadata, intent, schema, and generates searchable reference site. Engineers search for prompts by name, purpose, input type, output type. Reference auto-updates on every commit."

Linking to related documentation provides context. "Prompt documentation links to: product requirements PRD-2024-08, system design doc DESIGN-CS-v3, compliance assessment COMPLIANCE-2025-Q1, related prompts in same workflow."

Multi-audience documentation generation serves different needs. "Engineering documentation includes technical details, schema definitions, implementation notes. Product documentation focuses on behavior, use cases, success criteria. Compliance documentation emphasizes constraints, refusal behavior, regulatory mapping. All generated from single source of truth."

Version-aware documentation prevents confusion. "Documentation site shows current version 2.3.0. Historical versions 2.2.0, 2.1.0, 2.0.0 remain accessible. Diff view shows exactly what changed between versions. This supports rollback scenarios and compliance audits requiring specific version review."

Search integration makes documentation discoverable. "Engineers search: prompt handling credit card verification. Search returns three prompts with intent descriptions mentioning credit cards. Engineers browse to find right one. Without search, they would need to know exact prompt names or browse through hundreds of files."

## Documentation as Contract

Treat prompt documentation as the authoritative specification of behavior. When questions arise about what a prompt should do, the documentation is the source of truth.

Product managers should be able to read prompt documentation without reading prompts. "PM asks: how does the system handle refund requests over 500 dollars? Documentation answers: prompts classify refund requests by amount, requests over 500 dollars require manager approval per policy FIN-2023-07, prompt generates approval request and routes to manager queue."

Compliance teams audit behavior against documentation. "Auditor asks: does system ever disclose customer financial data to unauthorized parties? Documentation shows: all prompts include explicit refusal instruction for unauthorized data requests, refusal tested in test suite TEST-SECURITY-04, refusal validated in production monitoring MONITOR-PII-DISCLOSURE with zero violations in past 12 months."

Future engineers inherit documentation, not tribal knowledge. "Engineer joins team in 2026. Original prompt author left in 2024. Documentation explains: prompt uses formal tone because customer base is primarily enterprise users who expect formality per user research UR-2023-03, conversational tone was tested and rejected in A/B test EXP-2023-05, do not change tone without new user research."

Documentation quality correlates with system maintainability. Well-documented prompts can be modified by any engineer. Poorly documented prompts become tribal knowledge owned by whoever wrote them. As prompt libraries grow, documentation determines whether they become reusable assets or unmaintainable liabilities.

The investment in comprehensive documentation pays dividends every time someone debugs a failure in minutes instead of days because the documentation explained exactly what should happen and why. It pays off when compliance audits pass because every constraint has documented rationale. It pays off when new engineers onboard and understand the system from documentation instead of months of code archaeology.

Build documentation habits early, before your library outgrows your ability to remember why each prompt exists and how it works. The documentation you write today determines whether your AI system can be debugged, audited, and evolved tomorrow.

The next section examines experimentation platforms and tooling, exploring infrastructure that supports systematic prompt development and evaluation at scale.

