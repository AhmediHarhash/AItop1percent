# 2.4 â€” Self-Consistency and Majority Voting

A legal technology company deployed an AI contract review system in August 2025 that analyzed commercial agreements and flagged risky clauses. The system used Claude Opus 4 with carefully tuned prompts achieving 92 percent accuracy in testing. The product launched to enterprise customers who relied on it for million-dollar deals. Within six weeks, three customers reported that the same contract uploaded twice produced different risk assessments. One upload flagged an indemnification clause as high risk. The same contract uploaded an hour later marked it as acceptable. The variance was not a bug. It was the inherent stochasticity of language model sampling.

The company's VP of Engineering ran an experiment. He took 50 contracts from their validation set and processed each one ten times with identical prompts. Thirty-two contracts produced identical outputs across all ten runs. Eighteen contracts produced varying outputs. Some variance was minor: confidence scores differed by a few percentage points. Some variance was material: risk ratings changed from low to medium, or flagged clauses appeared in some runs but not others. The variance correlated with complexity. Simple contracts were consistent. Ambiguous contracts with edge case clauses varied.

The team implemented majority voting across five independent samples. For each contract, they generated five separate analyses with different random seeds, then selected the most common conclusion for each flagged clause. Accuracy improved from 92 percent to 96.8 percent. More importantly, consistency improved. The same contract uploaded ten times now produced identical outputs in nine or ten runs, versus five or six runs before. The remaining inconsistency came from genuine 3-2 vote splits, which the system flagged for human review. Clients accepted this. What they could not accept was unpredictable variance with no explanation.

## The Statistical Foundation

Self-consistency exploits a fundamental property of probabilistic models: correct answers tend to appear more frequently than incorrect answers when you sample multiple reasoning paths. This is not obvious. It requires that errors be diverse while truth is convergent.

Think about a math problem with one correct answer and infinite ways to be wrong. If you ask a model to solve it five times with different random seeds, errors will scatter across different wrong answers. One attempt might make an arithmetic mistake. Another might misread the problem. A third might use flawed logic. These errors rarely produce the same wrong answer. But correct reasoning paths, while varying in details, converge on the right answer. Three of five samples say 42, one says 38, one says 47. The majority answer is statistically more likely to be correct.

This principle holds for many task types: classification with definitive categories, extraction with ground truth data, reasoning with verifiable conclusions, decision-making with clear criteria. It works less well for creative tasks with multiple valid outputs, subjective judgments with no ground truth, or open-ended generation where diversity is valuable.

The technique requires independence between samples. You must generate each response separately, ideally with different random seeds or temperature variation. If you generate five responses within one context window where later responses can see earlier ones, independence breaks down. The model might anchor on its first response, reducing meaningful diversity.

The mathematical foundation comes from ensemble methods in machine learning. Random forests aggregate many decision trees to outperform individual trees. Boosting combines weak learners into strong learners. The same principle applies to language model outputs: aggregating diverse samples produces more reliable results than single samples. The statistical advantage compounds when error rates are moderate and sample count is sufficient.

## Implementation Mechanics

Implementing self-consistency means making multiple API calls per input and aggregating results. The straightforward approach is parallel requests with identical prompts but different random seeds.

Most API providers support concurrent requests. OpenAI allows thousands of requests per minute on paid tiers. Anthropic provides similar capacity. You dispatch five requests simultaneously, collect five responses, and aggregate them. The latency cost is minimal if requests run in parallel. Serial generation would multiply latency by five. Parallel generation adds only the latency variance between fastest and slowest samples.

Random seed control varies by provider. Some APIs let you specify explicit seeds for reproducibility. Others generate seeds internally but guarantee independence across concurrent requests. Check your provider's documentation. The goal is ensuring that your five samples explore different parts of the model's probability space rather than producing near-identical outputs.

Temperature is the primary diversity control. Temperature governs randomness in token sampling. Low temperature (0.1 to 0.3) produces focused, deterministic outputs. High temperature (0.9 to 1.2) produces diverse, creative outputs. For self-consistency, moderate temperature (0.6 to 0.8) balances diversity with quality. You want samples that differ meaningfully but remain high-quality.

Too low temperature defeats the purpose. At temperature 0.2, all five samples might be nearly identical. If they are all wrong, voting provides no benefit. You wasted four API calls. Too high temperature produces noise. At temperature 1.1, samples vary wildly, including obvious errors. Low-quality samples corrupt the voting process. The optimal temperature depends on your task and model. Most applications find 0.6 to 0.8 works well.

A financial services company tested temperature effects in November 2025. For credit risk assessments, temperature 0.3 produced five nearly identical outputs, providing no diversity benefit. Temperature 1.0 produced high variance including nonsensical risk scores. Temperature 0.7 hit the sweet spot: meaningful reasoning diversity with consistent quality. They standardized on 0.7 for voting-based workflows.

## Voting Mechanisms

The aggregation strategy depends on your output type: categorical, numerical, or structured.

Simple majority voting works for categorical outputs. If you are classifying documents, selecting from multiple choice options, or making binary decisions, count votes for each category and select the winner. Three samples say "approved," two say "rejected," output is "approved." This is straightforward and works well.

Tie handling requires explicit rules. With five samples, you might get 2-2-1 splits across three categories. Options include: default to the safest choice, generate a sixth tiebreaker sample, escalate to human review, or use secondary criteria like confidence scores. The right choice depends on your domain's error tolerance and the cost of indecision.

Weighted voting assigns different importance to samples based on quality indicators. If you extract confidence scores, samples with higher confidence get more weight. A 90 percent confidence sample counts more than a 60 percent confidence sample. This improves accuracy when confidence is well-calibrated but adds complexity. You must validate that model confidence correlates with actual correctness before trusting weighted voting.

Averaging works for numerical outputs: prices, probabilities, quantities, scores. Generate five estimates and average them. This works better than simple voting for continuous values. A refinement is trimmed averaging: drop the highest and lowest values, then average the middle three. This reduces the impact of outlier errors. A real estate appraisal system uses median of seven estimates, automatically discarding the two highest and two lowest.

Threshold voting requires supermajorities instead of simple majorities. Instead of 3 out of 5, require 4 out of 5 agreement. Cases below threshold escalate to human review. This trades automation rate for accuracy. You process fewer cases automatically, but the cases you do process have higher confidence. A medical diagnostic system requires 4 out of 5 agreement for automatic triage; 3-2 splits go to clinicians.

Confidence-weighted averaging combines numeric predictions with uncertainty estimates. Each sample provides a value and confidence. The final output is a weighted average where high-confidence samples dominate. This is sophisticated but requires reliable confidence calibration, which is challenging with current models.

## Task Type Considerations

Self-consistency benefits different tasks differently. Understanding where it helps most lets you apply it selectively.

Mathematical reasoning shows dramatic improvements. Tasks requiring multi-step calculations, logical deductions, or algebraic manipulations benefit from voting because errors tend to appear at different steps across samples. Research on GSM8K and MATH benchmarks shows 5 to 10 percentage point accuracy gains with 5-sample voting.

Factual question answering benefits moderately. When questions have clear correct answers, voting helps. When questions are ambiguous or have multiple defensible answers, voting helps less. A question like "What is the capital of France?" has one answer and voting is unnecessary. A question like "What were the main causes of World War I?" has nuanced answers where voting might average out important details.

Classification tasks with clear categories benefit well. Sentiment analysis, document categorization, intent detection, and similar tasks see meaningful gains. The clearer the category boundaries, the better voting works. Ambiguous boundary cases might split votes, which is actually useful information: the split signals inherent ambiguity worth reviewing.

Structured extraction benefits when extraction rules are clear but inputs vary. Extracting names, dates, amounts, and entities from documents works well with voting because correct extractions tend to match while errors vary. Extraction of subjective attributes like tone or sentiment benefits less because multiple interpretations may be valid.

Creative generation benefits least. When you want diverse outputs, aggregating them destroys value. Blog post generation, creative writing, brainstorming, and similar tasks should not use voting. Diversity is the goal, not a problem to eliminate. Some creative tasks use voting in reverse: generate many samples and have humans select the best, rather than algorithmically aggregating.

Code generation shows mixed results. Simple code tasks benefit from voting because there is usually one correct implementation. Complex code tasks might produce different valid solutions, making voting counterproductive. The model might generate three different algorithms that all work; averaging them produces nonsense.

## Cost Analysis

Self-consistency multiplies API costs approximately by sample count. Five samples cost roughly five times more than one sample. At scale, this is significant.

Exact cost multiplier is slightly less than N because you can optimize output length. If you need only the final answer for voting, not the full reasoning chain, you can use lower max tokens settings or ask for terser outputs. A full explanation might use 800 output tokens, but "Answer: Option C" uses 10 tokens. If input tokens dominate cost (80 percent of total) and you reduce output tokens by 90 percent, per-sample cost drops by 18 percent.

Prompt caching reduces input token costs when you process multiple items sharing context. If you analyze many contracts with the same legal framework, the framework context can be cached and reused across requests. Claude's prompt caching and OpenAI's context caching both support this. Caching reduces the cost multiplier because repeated context is not re-processed.

Model selection affects cost tradeoffs. A smaller model with voting might match a larger model without voting at lower total cost. If GPT-5-mini with 5-sample voting achieves the same accuracy as GPT-5 with single sampling, and mini is 10x cheaper, the 5x voting cost still leaves you ahead. Test whether this pattern holds for your tasks.

Cost-benefit analysis requires knowing your error costs. If errors cost 1 dollar and voting reduces error rate from 6 percent to 3 percent, you save 0.03 dollars per query (3 percent of 1 dollar). If 5-sample voting costs 0.02 dollars extra, you save 0.01 dollars net. The investment pays off. If errors cost 0.10 dollars, the same improvement saves 0.003 dollars, less than the 0.02 dollar voting cost. Voting loses money.

Volume matters. At 10,000 queries per day, saving 0.01 dollars per query is 100 dollars per day, 36,500 dollars per year. At 100 queries per day, it is 365 dollars per year. Whether these savings justify engineering effort to implement voting depends on your team's capacity and priorities.

## Diminishing Returns

Accuracy gains from voting follow a logarithmic curve. Early samples provide large gains. Later samples provide smaller incremental benefits.

A healthcare analytics company measured this in October 2025. They evaluated medical code assignment accuracy across sample counts from 1 to 20. Results: 1 sample (88.7 percent), 3 samples (92.4 percent), 5 samples (93.9 percent), 7 samples (94.5 percent), 10 samples (94.8 percent), 15 samples (95.0 percent), 20 samples (95.1 percent). The curve flattened sharply after 5 samples. Going from 5 to 20 cost 4x more for 1.2 percentage points.

The optimal sample count depends on your error cost versus API cost tradeoff. If errors are catastrophically expensive, 20 samples might be justified despite diminishing returns. For most applications, 3 to 7 samples hit the sweet spot. Five samples is common because it provides good accuracy improvement at reasonable cost and allows clear majority votes without ties.

Adaptive sampling optimizes cost by varying sample count based on difficulty. Start with 3 samples. If all 3 agree, stop. If they split 2-1, generate 2 more samples (total 5). If those split, generate more. If the first 3 samples unanimously agree, you saved the cost of samples 4 and 5. Easy cases use fewer samples, hard cases use more.

Adaptive sampling requires careful implementation. You need early stopping logic that detects consensus, retry logic that handles splits, and maximum sample limits to prevent runaway costs. But the savings can be substantial. If 60 percent of cases reach consensus at 3 samples, your average sample count is 3.6 instead of 5, a 28 percent cost reduction.

Statistical confidence increases with sample count. With 3 samples, a 3-0 vote is suggestive but not definitive. With 7 samples, a 6-1 vote shows strong consensus. With 20 samples, an 18-2 vote is very high confidence. If your domain requires statistical certainty, larger samples are necessary. For practical systems balancing cost and quality, 5 to 7 usually suffices.

## Combining with Other Techniques

Self-consistency composes well with other prompting techniques.

Chain-of-thought prompting with voting is particularly powerful. Each sample generates a reasoning chain before concluding. You vote on final conclusions, not on reasoning steps. This combines the accuracy benefits of explicit reasoning with the reliability benefits of aggregation. Research shows this combination works especially well for complex reasoning tasks.

The medical triage system described earlier used chain-of-thought with voting. Each of five samples reasoned through symptoms, medical history, severity indicators, and decision criteria before outputting a triage level. The reasoning paths varied in details but often reached the same conclusion, increasing confidence. When reasoning paths diverged, the vote split signaled genuine ambiguity worth human review.

Few-shot examples enhance voting effectiveness. Examples teach the model the task, reducing the base error rate before voting even starts. Lower base error means voting amplifies a stronger signal. If your single-sample accuracy is 70 percent, voting might improve to 75 percent. If your single-sample accuracy is 90 percent, voting might improve to 95 percent. Better prompts make voting more effective.

Structured output enforcement remains critical. All samples must produce parseable outputs for voting to work. Use structured output APIs or strict format instructions to guarantee every sample returns usable data. A single malformed response breaks voting unless you have robust error handling that excludes invalid samples.

Reflection can follow voting. Generate five samples, vote for majority answer, then ask the model to critique that answer. If reflection reveals problems, regenerate samples or escalate to human judgment. This combines aggregate wisdom with critical analysis. It catches cases where the vote is confidently wrong.

Different temperatures per sample might increase diversity. Instead of five samples at temperature 0.7, try temperatures 0.5, 0.6, 0.7, 0.8, 0.9. This explores more of the probability space. But it also risks low-quality high-temperature samples corrupting the vote. Test whether temperature variation helps or hurts for your specific tasks.

## Failure Modes

Self-consistency fails when certain conditions hold. Recognizing these conditions prevents wasted effort.

Systematic errors defeat voting. If all samples make the same mistake, voting confirms the error instead of correcting it. A prompt with a subtle flaw might consistently cause misunderstanding. All five samples confidently agree on the wrong answer. Voting provides false confidence by making the error seem reliable.

A tax software company discovered this in January 2026. Their prompt for dependent exemptions had an edge case bug affecting divorced parents. All five samples calculated the same wrong value. Voting made the error seem authoritative. Single sampling would have been equally wrong but cheaper. The lesson: fix your prompts before adding voting.

Low sample diversity undermines voting. If samples are too similar, voting adds cost without benefit. This happens with very low temperature, with prompts that constrain outputs tightly, or with tasks that have only one plausible approach. Monitor sample diversity metrics. If samples differ trivially, voting is not helping.

Latency constraints can make voting impossible. Even with parallel requests, you must wait for the slowest sample to complete before aggregating. If your p95 latency is 8 seconds but your latency budget is 2 seconds, voting does not fit. Interactive applications with strict latency requirements often cannot afford voting.

Cost at scale can be prohibitive. A content moderation system processing 100 million items daily at 0.0005 dollars per request costs 50,000 dollars daily for single sampling. Voting costs 250,000 dollars daily, an extra 200,000 dollars. That 73 million dollars per year might exceed the entire AI budget. High-volume, low-value tasks cannot afford voting.

Task ambiguity makes voting meaningless. If legitimate answers differ based on interpretation, voting might select one arbitrarily. A question like "Is this comment positive or negative?" might have genuine ambiguity. Samples split 3-2 not because of model errors but because the comment is genuinely mixed. Voting produces an answer, but averaging out nuance might lose important information.

## Evaluation and Metrics

Measuring self-consistency effectiveness requires rigorous evaluation.

Accuracy with and without voting is the gold standard. Take a test set with ground truth labels. Measure single-sample accuracy. Measure 3-sample voting accuracy. Measure 5-sample voting accuracy. Plot the curve. This shows whether voting helps and how much.

Test sets must match production distribution. Academic benchmarks might show different voting benefits than real user data. Evaluate on realistic data that represents what your system will actually process. Differences in difficulty distribution, ambiguity levels, or edge case frequency affect voting effectiveness.

Vote margin distribution reveals voting dynamics. Plot how often votes are 5-0, 4-1, 3-2, etc. Highly skewed distributions (almost all 5-0 or almost all 3-2) suggest the task is too easy or too hard for voting to help. Balanced distributions with many 4-1 and 3-2 results show you are in the regime where voting adds value.

Error rates by vote margin show calibration. Do 3-2 votes have higher error rates than 5-0 votes? They should. If error rates are similar across vote margins, voting is not working as expected. The vote margin should correlate with confidence and correctness.

Cost per correct answer is a better metric than raw accuracy. Single sampling at 91 percent accuracy and 0.01 dollars per query costs 0.0110 dollars per correct answer. Voting at 95 percent accuracy and 0.05 dollars per query costs 0.0526 dollars per correct answer. Voting is more accurate but 4.8x more expensive per correct answer. Which is better depends on your priorities.

Sample disagreement identifies edge cases. When samples split 3-2 or produce no majority, those inputs are genuinely difficult or ambiguous. Collect these cases for analysis. They reveal where your system struggles and where prompts need improvement. High disagreement rate might indicate prompt problems rather than fundamental task difficulty.

Longitudinal monitoring tracks voting effectiveness over time. As models improve, voting benefits might change. As prompts evolve, voting dynamics shift. Track voting metrics in production. Alert when vote margin distributions change significantly, suggesting model updates or data drift.

## Production Implementation Patterns

Implementing self-consistency in production requires more than just making five API calls.

A voting service layer abstracts the pattern. Your application calls a voting function that handles parallel generation, aggregation, and error handling. This keeps voting logic separate from business logic. The voting service is reusable across different use cases and testable independently.

Structured outputs make aggregation reliable. If samples return JSON with a "classification" field, extract that field from all samples and vote on it. If samples return free text, you need parsing logic to extract votable elements. Design prompts to produce structured outputs suitable for programmatic aggregation.

Logging all samples, not just final votes, enables analysis. Store each sample's output, the vote result, and metadata like sample count and vote margin. This creates an audit trail and enables retrospective analysis. When votes are wrong, you can examine individual samples to understand what went wrong.

Vote margin as confidence signal drives downstream logic. 5-0 votes are high confidence, process automatically. 3-2 votes are low confidence, route to human review. 2-2-1 ties indicate severe ambiguity, escalate immediately. Use vote margin to calibrate automation versus human involvement.

Timeout handling prevents latency outliers from breaking systems. Set per-sample timeouts and aggregate timeout. If one sample takes 30 seconds, timeout and vote with remaining samples. Four successful samples might suffice. Or regenerate the timed-out sample. Don't let one slow sample delay the entire workflow.

Partial failure handling decides what to do when samples fail. If 2 of 5 samples return API errors, do you vote with 3 samples or fail the entire request? The answer depends on whether 3 samples provide sufficient confidence for your use case. Make this tradeoff explicit in your implementation.

A/B testing validates voting in production. Route a percentage of traffic to voting-based processing and compare outcomes against single sampling. Measure accuracy, latency, cost, and user satisfaction. This validates that theoretical accuracy gains translate to real-world value and acceptable cost.

## Strategic Deployment

Self-consistency is not an all-or-nothing choice. Deploy it strategically where benefits exceed costs.

High-stakes decisions justify voting costs. Medical diagnoses, financial approvals, legal analyses, and safety-critical systems have high error costs. A wrong medical triage might cost tens of thousands in liability. Spending 0.15 dollars instead of 0.03 dollars per decision is trivial. Apply voting to high-stakes cases even if costs seem high.

Low-stakes, high-volume tasks often cannot afford voting. Content recommendations, spam classification, routine customer routing, and similar tasks process millions of requests daily. Multiplying costs by 5x is prohibitive. Use voting selectively or not at all.

Confidence-based triggering optimizes costs. Run single sampling by default. If model confidence is low or validation checks fail, invoke voting. If outputs fail sanity checks, generate additional samples and vote. This gives cost efficiency of single sampling for clear cases and reliability of voting for ambiguous cases.

A document classifier uses adaptive voting: single sample for documents with model confidence above 0.85, 3-sample voting for confidence 0.65 to 0.85, 5-sample voting for confidence below 0.65. This balances cost against difficulty. Easy documents process cheaply, hard documents get extra scrutiny.

User-facing versus batch processing contexts have different constraints. Interactive features need low latency; voting might not fit. Batch processing can tolerate latency and optimize for accuracy and cost. Apply voting to batch workloads where latency is not critical.

Critical path versus background tasks differ in requirements. Voting might be too slow for synchronous API responses but perfect for asynchronous background jobs. A customer service system might use single sampling for real-time chat responses but voting for post-conversation quality analysis.

Understanding self-consistency and voting transforms your ability to improve reliability through statistical aggregation. The next section explores reflection and self-critique techniques that ask models to review and improve their own outputs.

