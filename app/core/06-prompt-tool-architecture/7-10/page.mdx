# 7.10 â€” Tool Hallucination: When Models Invent Tools or Parameters

In November 2024, a financial services chatbot confidently told a user it would "use the instant_wire_transfer tool to send $50,000 to your beneficiary within the hour." The system had no such tool. It had wire_transfer_request, which created a request for manual approval with a 24-hour processing window. The user expected immediate transfer and made business decisions accordingly. When the transfer didn't complete for 36 hours, the resulting contract breach cost the company $125,000 in penalties and destroyed the client relationship.

Models hallucinate tools the same way they hallucinate facts. When users request something that seems like it should be possible, models sometimes invent plausible-sounding tools to accomplish it. They generate tool calls with fictional tool names, invent parameters that don't exist, or use wrong parameter types that break tool execution. This is tool hallucination, and it's insidious because the model's confidence makes fictional capabilities seem real.

## Models Call Tools That Don't Exist

The most obvious tool hallucination is inventing tool names entirely. You provide the model with get_account_balance and transfer_funds. The user asks to schedule a recurring payment. The model calls schedule_recurring_payment, which doesn't exist. The tool call fails, but the model might tell the user "I've set up your recurring payment" before it realizes the tool doesn't exist.

This happens because models generalize from patterns in their training data. They've seen thousands of API specifications and tool definitions during training. When you describe a few tools, the model infers what other tools might exist based on patterns. If you have get_X and set_X tools, the model might assume delete_X or list_X tools exist too.

The model's training also includes conversations about capabilities it doesn't actually have. Users in training data asked GPT to do things it couldn't do, and sometimes GPT attempted them anyway. This trained behavior carries over to your tools. The model attempts plausible tool calls even when those tools aren't in the list you provided.

Detection is straightforward. Your tool execution layer receives a tool call with a tool name, checks it against registered tools, and rejects calls to nonexistent tools. The challenge is what happens after rejection. The model needs to receive an error message and recover gracefully, not just hang or hallucinate that the fictional tool succeeded.

Error messages matter enormously. Don't return "Tool not found." Return "Error: No tool named schedule_recurring_payment exists. Available tools: get_account_balance, transfer_funds, get_transaction_history." Explicitly listing available tools helps the model recover by attempting a valid tool or explaining to the user what's possible.

## Inventing Parameters

Even when models use real tools, they often invent parameters. You define transfer_funds with parameters amount, recipient_account, and memo. The model calls it with amount, recipient_account, memo, and priority="urgent". The priority parameter doesn't exist. Your tool execution fails because it receives an unexpected parameter.

This happens for similar reasons as tool name hallucination. Models see parameter patterns in training and generalize. Financial transfer APIs often have priority flags, so the model assumes yours does too. The model wants to be helpful. The user said "urgent," so the model tries to convey urgency to the tool, even if the tool doesn't support it.

Parameter invention is harder to detect than tool name hallucination because you need to validate the parameter schema, not just the tool name. Your tool execution layer should validate that incoming parameters exactly match the expected schema for the tool. Extra parameters should trigger errors, not be silently ignored.

Ignoring unexpected parameters is dangerous. If the model adds priority="urgent" and you ignore it, the model might tell the user "I've marked this as urgent" when the system did no such thing. Explicit validation and error responses keep the model's understanding aligned with actual system behavior.

Parameter name variations are a related problem. You define recipient_account but the model calls it with recipient_id or to_account. The model recognizes these are semantically similar and assumes your API accepts variants. Strict schema validation catches this, but you might also want to implement parameter name normalization if common variations are predictable.

## Wrong Parameter Types

Models hallucinate parameter types surprisingly often. You define amount as a number, the model passes it as a string: "100.00" instead of 100.00. You define date as ISO 8601 string, the model passes "January 15, 2026". You define recipient_account as a string, the model passes a nested object with account_number and bank_code fields.

Type confusion happens because models don't have strong type systems in their architecture. They generate text that represents tool calls. Whether a value is a number or a string representing a number isn't always clear in natural language. The model's training includes APIs with inconsistent type handling, so it doesn't always know which systems require strict types.

Your tool execution layer must perform type validation. Before executing a tool, check that each parameter matches the expected type. Numbers should be numbers, strings should be strings, objects should have the right structure. Type mismatches should fail with clear error messages that include the expected type.

Error messages for type issues need to be specific. "Parameter amount has invalid type" doesn't help the model fix the problem. "Parameter amount must be a number, received string '100.00'" tells the model exactly what's wrong and hints at the fix. The next tool call attempt will likely use a number.

Some systems implement automatic type coercion. If amount is defined as a number but the model passes "100.00", you parse it to 100.00 automatically. This makes the system more forgiving, but it's risky. Type coercion hides misunderstandings. The model thinks your API accepts strings, and you're silently converting them. Later changes might break this assumption invisibly.

## Causes of Tool Hallucination

Understanding why models hallucinate tools helps prevent it. The primary cause is underspecification in tool descriptions. If your tool description is vague or incomplete, models fill gaps with assumptions. A tool described as "transfers money" might be assumed to have scheduling, priority, and approval parameters because real-world transfer APIs often do.

Comprehensive tool descriptions reduce hallucination. Specify every parameter explicitly with types, descriptions, and whether they're required or optional. Show examples of valid tool calls. The more precisely you specify tool behavior, the less room models have to invent extensions.

Ambiguous naming contributes too. If you name a tool process_transaction, the model might assume it handles many transaction types and invent parameters for specifying type, category, or method. Specific names like charge_credit_card are clearer and leave less room for hallucinated parameters.

Models also hallucinate when users request something that seems like it should be possible but isn't. User says "send this urgently" but transfer_funds has no priority parameter. The model wants to satisfy the user's request, so it invents a priority parameter. The alternative is telling the user the system can't do what they asked, which feels unhelpful.

Training the model to acknowledge limitations helps. System prompts should say "If users request capabilities that your available tools don't support, explain the limitation clearly. Do not attempt to use parameters or tools that don't exist." This sets expectations that the model should fail gracefully rather than hallucinate capabilities.

## Detection Strategies

Detecting tool hallucination requires multiple layers. At the tool execution level, strict schema validation catches obvious problems. Tool names must match registered tools exactly. Parameters must match the defined schema exactly. Types must match expected types exactly. Any deviation fails with a detailed error.

Logging every tool call attempt, including failures, gives you visibility into hallucination patterns. Review logs for repeated attempts to call nonexistent tools or use nonexistent parameters. These patterns indicate where your tool descriptions are unclear or where the model consistently misunderstands capabilities.

Pre-execution validation catches hallucination before it affects users. Before executing a tool call, validate it against schemas and potentially flag suspicious patterns. If the model calls the same fictional tool three times in one conversation despite errors, pause execution and alert a human or trigger a fallback response.

Some systems implement a tool call review step where the model explains what it's about to do before executing. "I'm going to call transfer_funds with amount=100, recipient_account=12345, and memo='Payment'. This will transfer $100 to account 12345." If the explanation reveals nonexistent parameters or misunderstood behavior, you can catch it before execution.

User feedback is a critical signal. When users say "that's not what happened" or "that feature doesn't exist," investigate whether tool hallucination is the cause. Users experience the effects of hallucination directly, and their confusion often indicates the model promised something it didn't deliver.

## Prevention Through Tool Design

Tool design affects hallucination rates. Simple tools with few parameters hallucinate less than complex tools with many parameters. If transfer_funds requires ten parameters, models are more likely to guess at values or invent additional parameters. If it requires only three, hallucination opportunities are limited.

Required versus optional parameters matters. If most parameters are optional, models might omit important ones or add fictional optional ones. Clear documentation of which parameters are required reduces this. Consider making more parameters required if the tool behaves unpredictably without them.

Default values reduce hallucination too. Instead of an optional priority parameter where the model might invent values, build priority into the tool name. Have transfer_funds_standard and transfer_funds_urgent as separate tools. This eliminates the parameter entirely, removing the hallucination opportunity.

Structured enums help for parameters that accept limited values. Instead of a string status parameter where the model might generate any text, define status as an enum with allowed values: pending, completed, failed. Your schema validation rejects any other values, and the tool description shows exactly what's valid.

Tool composition reduces complex tool hallucination. Instead of one complex tool with many parameters and optional behaviors, create several simple tools that do one thing each. Models hallucinate less when tools are focused and unambiguous. Composition also makes tool calling more transparent to users.

## Handling Hallucination Gracefully

When tool hallucination occurs, recovery matters as much as detection. The model receives an error about a nonexistent tool or invalid parameter. What happens next determines whether the interaction succeeds or fails.

Good error messages enable recovery. "Tool schedule_recurring_payment does not exist. Available tools: get_account_balance, transfer_funds, get_transaction_history. To set up recurring payments, use transfer_funds multiple times or contact support for scheduling options." This gives the model information to either retry with valid tools or explain limitations to the user.

Some systems implement error-correction prompts. When a tool call fails due to hallucination, the system adds a message like "The tool call failed because that tool or parameter doesn't exist. Review the available tool specifications and try again with a valid tool." This triggers the model to reconsider rather than blindly retrying the same invalid call.

Retry limits prevent hallucination loops. If the model attempts invalid tool calls three times, stop retrying and return a generic error response to the user. Models sometimes get stuck in retry loops, repeatedly calling fictional tools despite errors. Timeout mechanisms prevent this from degrading user experience.

Fallback responses help when the model cannot accomplish the user's request with available tools. After failed tool calls, the system responds "I'm unable to complete that action with the available tools. I can do X, Y, or Z instead. Would you like to try one of those?" This redirects the conversation productively rather than letting it fail.

User transparency matters. When tool calls fail, some systems explain this to users rather than hiding it. "I attempted to schedule recurring payments, but that feature isn't available in this system. I can help you with one-time transfers instead." This manages user expectations and prevents frustration from unfulfilled promises.

## Training Models to Avoid Hallucination

System prompts can reduce tool hallucination with explicit instructions. "Only call tools from the provided tool list. Do not invent new tools or parameters. If a user requests something not supported by available tools, explain the limitation rather than attempting fictional tools." Clear guardrails help, though they don't eliminate hallucination entirely.

Few-shot examples demonstrating proper tool use and graceful limitation acknowledgment improve model behavior. Show examples where the user requests something unsupported, and the model explains what's possible instead of inventing tools. "User: Can you schedule this? Assistant: The available tools don't support scheduling. I can do an immediate transfer or you can contact support for scheduling."

Reinforcement learning from human feedback specifically targeting tool hallucination can reduce it. When models invent tools or parameters, human reviewers mark this as incorrect. The model learns that tool hallucination receives negative feedback and adjusts behavior. This requires significant data collection but creates measurable improvements.

Some teams fine-tune models on their specific tool sets. Training the model on thousands of examples of correct tool usage with your exact tools reduces hallucination because the model learns your specific tool schemas and naming conventions. This is resource-intensive but effective for production systems with stable tool sets.

Regular model evaluation on tool hallucination rates tracks progress. Run test suites with prompts designed to trigger hallucination: requests for unsupported features, ambiguous requests, edge cases. Measure how often the model invents tools or parameters versus gracefully explaining limitations. Track this metric over time as you improve prompts and training.

## Schema Enforcement as a Safety Net

Regardless of prevention efforts, assume models will hallucinate tools. Your tool execution layer is the safety net. Strict schema enforcement prevents hallucinated tool calls from executing and causing damage.

JSON Schema or equivalent validation frameworks work well. Define each tool's parameters as a JSON Schema. Before execution, validate incoming tool calls against schemas. Validation failures produce detailed error messages that help the model correct itself.

Type validation goes beyond just checking parameter presence. Validate that numeric ranges make sense, strings match expected patterns, enums contain valid values, and nested objects have required fields. Comprehensive validation catches subtle hallucinations that might otherwise slip through.

Version your tool schemas and enforce version matching. If a model tries to call transfer_funds with a parameter that existed in version 1.0 but not in version 2.0, catch this as a version mismatch rather than silently failing or ignoring the parameter.

Whitelisting is safer than blacklisting. Rather than trying to catch all invalid tool calls, only allow tool calls that exactly match registered tools with valid parameters. Everything else fails by default. This makes your system secure by default rather than requiring you to anticipate every possible hallucination.

The next chapter examines how to evolve tools over time without breaking existing systems or causing models to call outdated tool versions.
