# 1.8 â€” When Prompts Fail: Taxonomy of Prompt Failures in Production

A legal technology startup deployed a contract analysis system in August 2025 powered by GPT-5. The system extracted key terms, identified obligations, and flagged potential risks across 50 contract types. Testing showed 91 percent accuracy across 5,000 annotated contracts. The product launched to 200 law firms processing 12,000 contracts per week.

Within three weeks, customers reported strange failures. Some contracts returned hallucinated party names that did not appear in the source text. Others triggered safety refusals claiming the contracts contained harmful content. Some responses broke mid-sentence with JSON syntax errors. A few contracts caused context overflow errors despite being under the advertised token limit. Different contracts failed in completely different ways with no obvious pattern.

The engineering team initially treated all failures as a single problem requiring a single solution. They added more examples, clarified instructions, and increased temperature settings. Failure rates barely changed because they were fighting six distinct failure modes with overlapping symptoms but different root causes. Only after building a failure taxonomy and mode-specific detection did they reduce overall failure rates from 14 percent to 3 percent.

The cost of undifferentiated failure handling was substantial. They spent $180,000 in engineering time chasing symptoms instead of causes. Customer complaints triggered $60,000 in credits and refunds. Two major accounts threatened cancellation. The breakthrough came when they stopped treating "failure" as monolithic and started classifying failure modes systematically. Each mode had distinct signatures, causes, and remediation strategies. Generic fixes failed because they addressed the wrong problems.

## Why Failure Taxonomies Matter

Generic monitoring that tracks "success" versus "failure" provides insufficient signal for diagnosis and remediation. A system with 8 percent failure rate tells you nothing about whether failures are hallucinations, format breaks, refusals, or context issues. Different failure modes require different solutions, making classification essential.

Building a failure taxonomy forces you to think systematically about what can go wrong. The exercise reveals failure modes you had not considered and helps prioritize monitoring investments. It also enables granular SLAs: you might tolerate 5 percent format breaks (easily detected and retried) but zero percent hallucinations (silent and dangerous).

The taxonomy should be specific to your application and prompts while building on common patterns observed across production AI systems. The six major categories are hallucination, refusal, format break, instruction drift, context overflow, and ambiguity collapse. Most production failures fall into one of these categories or a combination.

Each mode has characteristic symptoms. Hallucinations produce plausible-sounding but factually incorrect content. Refusals produce apologetic non-responses. Format breaks produce syntactically invalid output. Instruction drift produces outputs that ignore some prompt requirements. Context overflow produces truncated or incomplete responses. Ambiguity collapse produces outputs that resolve ambiguity incorrectly.

The legal tech startup built a classification system that analyzed each failure and tagged it with one or more failure modes. Hallucinations accounted for 35% of failures. Format breaks were 25%. Refusals were 20%. Instruction drift was 15%. Context overflow was 3%. Ambiguity collapse was 2%. This distribution guided remediation prioritization. They focused first on hallucinations as the highest-volume and highest-risk mode.

Understanding the distribution revealed something deeper. Different contract types had different failure profiles. Employment contracts showed high refusal rates due to termination language triggering safety filters. Complex merger agreements had elevated context overflow. Standard NDAs rarely failed. This insight enabled type-specific handling: route employment contracts through safety-aware prompts, chunk merger agreements, use standard processing for NDAs. One-size-fits-all approaches had masked these type-specific patterns.

## Hallucination: Confident Fabrication

Hallucinations occur when the model generates plausible-sounding content that has no basis in provided context or reality. The legal contract analyzer hallucinated party names by generating realistic-sounding company names that never appeared in the contract text. These hallucinations are particularly dangerous because they look correct and the model expresses high confidence.

Hallucinations split into several subtypes. Factual hallucinations invent facts, dates, or statistics. Attribution hallucinations cite nonexistent sources or quotes. Extrapolation hallucinations extend beyond provided information with invented details. Confabulation hallucinations fill gaps in ambiguous inputs with made-up content that seems reasonable.

Detection requires comparing outputs to ground truth when available. For extraction tasks, verify that every extracted value actually appears in the source text using string matching or semantic similarity. For summarization tasks, use NLI models to check whether summary sentences are entailed by source documents. For generation tasks, use fact-checking models or knowledge base lookups to verify claims.

The legal startup implemented hallucination detection in September 2025. They required all extracted entities to have exact or fuzzy matches in source documents. Outputs without valid source matches were flagged for review. This caught 80% of hallucinations automatically. The remaining 20% involved paraphrasing or inference that looked legitimate but was actually unsupported. They added NLI scoring to catch these cases, bringing detection to 95%.

The subtlety of hallucination became clear through error analysis. The model extracted "Acme Corporation" from a contract that mentioned "the Company" throughout but never specified a name. The model hallucinated a plausible company name to fill the gap. This was not random noise; it was the model applying learned patterns about how contracts typically work. The hallucination was context-appropriate but factually wrong. Standard string matching caught it, but semantic similarity almost missed it because "the Company" and "Acme Corporation" are conceptually related.

## Hallucination Mitigation Through Prompt Design

You can reduce hallucination rates through defensive prompt design. Add explicit instructions forbidding fabrication: "Extract only information that explicitly appears in the provided text. Do not infer, guess, or generate information not present in the source. If information is not available, return null for that field." This instruction reduces hallucinations by 30 to 50 percent in extraction tasks.

Require the model to cite sources for extracted information: "For each extracted value, include the exact text span from the source where you found it." This forces the model to ground outputs in actual content and makes hallucinations easier to detect. When the model cannot provide a valid citation, it often returns null instead of hallucinating.

Use retrieval augmentation correctly by ensuring retrieved context actually contains relevant information. Hallucinations often occur when the model tries to answer questions using irrelevant retrieved documents. Improve retrieval quality and add instructions: "Answer only if the provided context contains relevant information. If the context is insufficient, respond with a status indicating insufficient context."

Structured outputs with explicit confidence fields help surface hallucinations. When the model must output a confidence score for each extracted value, it signals uncertainty on weak extractions. Values with low confidence likely indicate hallucinations or ambiguity. You can filter or flag these automatically.

A healthcare records platform reduced hallucinations by 60% in October 2025 by requiring citation spans and confidence scores. The model learned to express uncertainty when information was ambiguous or absent. Outputs with confidence below 0.7 triggered human review, catching most remaining hallucinations before they reached downstream systems.

The combination of techniques created a defense-in-depth strategy. Explicit anti-hallucination instructions reduced base rates. Citation requirements provided verifiable grounding. Confidence scores surfaced remaining uncertainty. Automated validation caught failures that slipped through. No single technique eliminated hallucinations, but layered defenses reduced them to acceptable levels.

## Refusal: Inappropriate Safety Triggering

Refusals occur when the model declines to process legitimate requests because safety filters incorrectly flag content as harmful. The legal contract analyzer triggered refusals on employment contracts containing termination clauses because the model interpreted "termination" as violent content. These false positive refusals block valid use cases and frustrate users.

Refusals typically include explanatory text: "I cannot help with that request" or "This content appears to violate safety guidelines." Some models return error codes instead of explanatory refusals. Detection is straightforward: parse responses for refusal patterns using regex or classifier models.

The challenge is distinguishing appropriate refusals (actual policy violations) from inappropriate refusals (false positives). Logging refused requests with human review reveals patterns. If 90 percent of refusals on employment contracts involve the word "termination," you have identified a specific false positive pattern needing remediation.

Prompt engineering can reduce refusal rates when false positives are high. Add context explaining the legitimate purpose: "You are analyzing legal contracts for a law firm. The following contract contains standard legal language including terms like termination, liability, and damages. This is professional legal content, not harmful material." This framing helps models distinguish legal language from actual harmful content.

For known false positive triggers, add explicit override instructions: "The text may contain words like termination, lawsuit, or breach. These are legal terms in proper context, not harmful content. Process them normally." This works because refusal systems often check both the content and the surrounding context when making decisions.

A financial compliance platform faced refusal issues in November 2025 when analyzing fraud investigation reports. Words like "fraud," "theft," and "laundering" triggered safety filters despite being the core vocabulary of their legitimate business domain. They added explicit context framing and saw refusal rates drop from 40% to 2% on fraud-related content.

The pattern repeated across domains. Medical platforms saw refusals on injury documentation. Security platforms hit refusals on threat analysis. HR platforms triggered refusals on harassment complaint investigations. The solution was always the same: explicit context framing that distinguished professional domain language from genuinely harmful content. Models have the capability to make this distinction; they just need clear signals about context.

## Format Break: Schema Violations and Malformed Outputs

Format breaks occur when model outputs do not match expected schemas or formats. The contract analyzer requested JSON but received responses with trailing text after the closing brace, breaking the parser. Other format breaks include missing required fields, incorrect field types, or responses wrapped in markdown code blocks when raw JSON was specified.

Format breaks are easy to detect through automated validation. Parse responses against expected schemas and flag failures. For JSON outputs, use schema validators. For structured text, use regex patterns or grammar parsers. For classifications, verify that returned categories exist in your allowed set.

The frequency of format breaks varies significantly across models and prompt patterns. Claude models rarely break JSON format when properly instructed but sometimes add explanatory text before or after the JSON. GPT models follow format instructions more literally but occasionally generate invalid JSON with syntax errors. Gemini models sometimes mix formats, returning partial JSON wrapped in markdown.

Specify format requirements with extreme clarity. Instead of "return JSON," use "return valid JSON only, with no markdown formatting, no explanatory text, and no content before or after the JSON object." This reduces ambiguity. Include a format example showing the structure with field names and value placeholders.

Use format forcing techniques when available. Some APIs allow you to specify response schemas that the model must follow. OpenAI's function calling and structured outputs feature guarantees JSON schema compliance. Anthropic's tool use system enforces format for tool responses. These API features eliminate format breaks for supported use cases.

Implement retry logic with format-specific error messages. When a response fails validation, retry with an appended instruction: "Your previous response was invalid JSON due to [specific error]. Generate a valid JSON response matching the required schema." This self-correction often succeeds on the second attempt.

A data extraction platform reduced format breaks from 12% to 0.3% in December 2025 by combining three techniques. They used OpenAI's structured outputs feature for GPT models, providing guaranteed schema compliance. For Claude, they used extremely explicit format instructions with examples. For edge cases that still failed, they implemented intelligent retry with specific error feedback. The combination eliminated format breaks as a significant failure mode.

## Instruction Drift: Losing the Thread

Instruction drift occurs when models stop following prompt instructions partway through a response or across conversation turns. The contract analyzer sometimes started well-formatted JSON responses but drifted into prose explanations before completing all required fields. In multi-turn conversations, models sometimes forget system message constraints after several exchanges.

Instruction drift manifests as responses that start correct but degrade, responses that follow some instructions but ignore others, or responses that work for simple inputs but fail on complex inputs. Detection requires analyzing response structure and comparing against instruction requirements throughout the entire output.

Long prompts with many requirements are particularly susceptible to instruction drift. When you specify 15 different constraints, models often satisfy 12 to 13 of them but miss 2 to 3. The missed constraints vary across responses, making debugging difficult. Each individual constraint has high compliance, but simultaneous compliance with all constraints is low.

Reduce prompt complexity by separating concerns. Instead of one prompt with 15 constraints handling all cases, create 3 prompts with 5 constraints each and route inputs to the appropriate prompt. Simpler prompts have higher instruction adherence.

Prioritize instructions explicitly: "Most important: return valid JSON. Secondary: include confidence scores. Optional: add explanatory metadata." This hierarchy helps models make correct trade-offs when they cannot satisfy all requirements simultaneously.

Use structural cues that reinforce instructions throughout the response. For JSON outputs, include the full expected schema in the prompt so the model can reference it while generating. For multi-step tasks, restate key constraints at each step: "Remember: continue to use formal language" or "Maintain JSON format throughout."

A customer service platform tracked instruction drift across 50,000 interactions in January 2026. They found that single-constraint prompts had 96% adherence. Three-constraint prompts achieved 89% adherence. Ten-constraint prompts dropped to 62% adherence. The relationship was non-linear: each additional constraint reduced compliance disproportionately. Their solution was constraint budgeting: keep core prompts under 5 constraints, use separate validation layers for additional requirements, and accept that complex multi-constraint prompts will always have partial compliance.

## Context Overflow: Exceeding Window Limits

Context overflow occurs when the combined length of prompt plus conversation history plus retrieved context exceeds the model's context window. The contract analyzer encountered overflow on complex contracts approaching 100,000 tokens despite GPT-5's advertised 128,000-token window. In practice, effective windows are smaller than advertised limits due to internal buffering and tokenization overhead.

Overflow manifests in several ways. Some APIs return explicit errors before processing. Others truncate input silently, causing the model to work with incomplete context. Some models degrade gracefully, prioritizing recent context over older context. Without explicit error handling, overflow causes mysterious failures where inputs work individually but fail when combined.

Detection requires tracking token counts for all prompt components: system message, user message, conversation history, and retrieved context. Log actual token counts (not character counts) using the model's tokenizer. Alert when total tokens approach 80 percent of the advertised limit because effective limits are often lower.

Implement dynamic context management that prioritizes important information when budgets are tight. For conversations, keep the system message and most recent turn while summarizing or dropping middle turns. For RAG systems, rank retrieved documents by relevance and include only the top N that fit within budget.

Use prompt compression techniques that preserve information density. Replace verbose phrasing with concise equivalents. Summarize background context while keeping task-specific details intact. Remove redundant examples when you have limited budget.

Consider chunking strategies for large documents. Instead of processing a 100,000-token contract in one prompt, split it into logical sections (parties, terms, obligations, warranties) and process each section separately. Combine results in a final aggregation step. This approach trades single-pass simplicity for reliable multi-pass processing.

A document processing platform implemented intelligent chunking in November 2025. They analyzed document structure, identified semantic boundaries, split at section breaks, processed chunks in parallel, and aggregated results. Processing time increased from 3 seconds to 8 seconds, but failure rates dropped from 15% to 1% on large documents. The performance trade-off was acceptable because reliability mattered more than speed.

The chunking strategy revealed unexpected benefits. Parallel processing of chunks was faster than sequential processing of the full document. Error isolation was better because chunk failures did not corrupt the entire analysis. Token costs decreased because smaller contexts generated more focused outputs with less redundant reasoning. What started as a failure mitigation strategy became a performance optimization.

## Ambiguity Collapse: Defaulting to Wrong Assumptions

Ambiguity collapse occurs when prompts contain implicit ambiguities and models resolve them incorrectly. The contract analyzer encountered contracts using "party" to mean both legal parties and social gatherings. Without disambiguation, the model sometimes extracted "party" as an entity when it appeared in clauses about company events.

Ambiguity also appears in instruction interpretation. A prompt requesting "key terms" might mean important terms, contractual terms, or technical terminology depending on context. Models choose one interpretation and execute confidently, even if they chose wrong.

Detection requires semantic analysis of outputs compared to inputs. If you extract "party" from a clause about holiday celebrations in an employment contract, the extraction is technically correct but semantically wrong. This requires understanding domain semantics, not just pattern matching.

Replace ambiguous instructions with precise specifications. Instead of "extract key terms," use "extract defined terms from the Definitions section where terms appear in title case followed by definitions." This eliminates interpretation ambiguity.

Provide disambiguation examples in your prompt: "Party refers to legal entities involved in the contract: Company A and Company B. Do not extract party when it refers to social gatherings or events." This explicit disambiguation prevents common mistakes.

Use structured input formats that separate different content types. If contracts include both legal clauses and metadata, format them distinctly with clear delimiters. This structure helps models maintain correct context for each section.

An insurance claims platform encountered ambiguity collapse in December 2025 when processing incident reports. The word "impact" appeared in both physical collision contexts ("vehicle impact") and financial contexts ("business impact"). The model inconsistently categorized incidents based on which sense of "impact" it activated first. They added explicit disambiguation: "Impact in this context refers to physical collision force, measured in G-forces or damage severity. Business impact and financial impact are handled separately in the cost assessment section." Categorization accuracy improved from 76% to 94%.

The deeper insight was that ambiguity is domain-specific. General language understanding is insufficient for specialized domains where common words have technical meanings. Medical "acute" versus conversational "acute." Legal "consideration" versus casual "consideration." Financial "position" versus spatial "position." Domain expertise requires explicit disambiguation of domain vocabulary, even for words that seem unambiguous in general usage.

## Cascading Failures: When Multiple Modes Interact

The most difficult production failures involve multiple failure modes interacting. A contract that triggers context overflow might get truncated, causing the model to hallucinate missing information, which produces invalid JSON that breaks parsing. The root cause is overflow, but the observed failure is a format break with hallucinated content.

These cascading failures require root cause analysis to fix. Surface symptoms mislead you into treating format breaks when the real issue is context management. Implement monitoring that detects all failure modes and logs their co-occurrence. Patterns like "90 percent of format breaks occur on inputs exceeding 80,000 tokens" reveal cascading relationships.

Address root causes rather than symptoms. If context overflow causes hallucinations, fixing overflow prevents downstream hallucination failures. If ambiguous instructions cause format breaks, clarifying instructions prevents both issues. Symptom-focused fixes create whack-a-mole dynamics where fixing one failure mode exposes another.

The legal startup discovered a three-way cascade in October 2025. Long contracts (root cause: context overflow) were getting truncated, causing the model to hallucinate missing party information (secondary failure: hallucination), which was formatted incorrectly because the hallucinated names didn't fit the expected schema (tertiary failure: format break). They spent two weeks fixing format validation before realizing the real issue was document length. Once they implemented chunking for long contracts, both hallucinations and format breaks dropped by 80%.

Cascade detection requires correlation analysis across failure modes. Track which modes co-occur more frequently than random chance. High correlation suggests causal relationships. The startup built a correlation matrix showing that context overflow preceded hallucinations in 78% of cases and hallucinations preceded format breaks in 64% of cases. This revealed the cascade pattern: overflow leads to hallucinations leads to format breaks. Fix overflow, prevent the cascade.

## Building Failure Mode Detection Systems

Automated failure detection requires mode-specific classifiers. Build detection logic for each failure mode using rules, heuristics, or small classification models. Run all detectors on every response and log which modes triggered.

For hallucinations, compare outputs to inputs using semantic similarity and fact-checking. For refusals, pattern match on refusal phrases and error codes. For format breaks, validate against schemas. For instruction drift, check compliance with each specified constraint. For context overflow, monitor token counts. For ambiguity collapse, use domain-specific semantic validators.

Aggregate detection results into dashboards showing failure mode distribution over time. Track which modes are increasing or decreasing, which correlate with specific input patterns, and which cause the most customer impact. This visibility enables data-driven prioritization of remediation efforts.

The legal startup built a comprehensive detection system in November 2025. Each failure was analyzed by six mode-specific detectors running in parallel. Detection results were logged to a time-series database and visualized in real-time dashboards. They tracked failure rates by mode, contract type, customer, and time of day. The system revealed that refusals spiked on Mondays (employment contract processing batch), hallucinations correlated with contract complexity, and format breaks were random noise. This granular visibility enabled targeted interventions that reduced overall failure rates by 79%.

## Failure Mode Remediation Strategies

Each failure mode requires different remediation approaches. Hallucinations need grounding mechanisms and citation requirements. Refusals need context framing and safety threshold adjustment. Format breaks need stricter format specifications and validation. Instruction drift needs simplified prompts and structural reinforcement. Context overflow needs dynamic budgeting and chunking. Ambiguity collapse needs disambiguation and specificity.

Build a remediation playbook mapping each failure mode to recommended interventions. When monitoring detects elevated failure rates for a specific mode, the playbook guides rapid response. This systematization prevents ad hoc debugging and ensures consistent quality.

Test remediation changes against held-out failure examples. If hallucination rate is 8 percent and you implement grounding mechanisms, verify on a test set of historical hallucinations that the new approach reduces failures. A/B test in production to ensure remediation does not introduce new failure modes or degrade other quality dimensions.

The startup's remediation playbook contained specific interventions for each mode. Hallucinations: add citation requirements, implement NLI validation, require confidence scores. Refusals: add domain context framing, whitelist professional vocabulary, escalate persistent false positives to model provider. Format breaks: use structured output APIs, provide explicit format examples, implement retry with error feedback. Instruction drift: reduce constraint count, prioritize requirements, add structural reinforcement. Context overflow: implement chunking, compress prompts, rank and filter retrieved context. Ambiguity collapse: add disambiguation examples, use precise terminology, provide domain glossaries.

The playbook was living documentation. Each successful remediation was documented with before/after metrics. Failed remediation attempts were recorded to prevent repeated mistakes. New failure patterns triggered playbook updates. Over six months, the playbook grew from six generic entries to 47 specific interventions mapped to detailed failure signatures. The knowledge base became the team's most valuable asset, encoding hard-won expertise about failure modes and their solutions.

The next subchapter examines how to version control prompts and manage prompt evolution as your system scales, building on failure mode insights to create systematic improvement processes.
