# 3.2 — Designing Prompts for 100K–1M+ Token Context Windows

A Series B fintech company spent $380,000 on a December 2025 sprint to migrate their compliance analysis pipeline to Claude's 200K context window, believing that dumping entire regulatory document sets into single prompts would eliminate their multi-stage retrieval system and cut latency by 70 percent. The sixteen-person engineering team rewrote their prompt templates to include full text of all relevant regulations—often 150,000 to 180,000 tokens per request—plus case law excerpts, internal policy documents, and the transaction data being analyzed. They launched the new system to their premium tier customers on December 18th.

Within four days, the VP of product was fielding escalations from their three largest enterprise clients. The new system was slower than the old one, not faster. Response times had increased from 12 seconds to 41 seconds. Cost per analysis had tripled from $1.20 to $3.80. Worst of all, output quality had not improved—in several cases, it had degraded. The model was missing regulatory citations that the old system caught reliably. It was sometimes generating vague summaries instead of specific compliance findings.

The team had assumed that more context automatically meant better answers. They had hit diminishing returns and architectural breakdown. The root cause was context overload. The model was spending compute on processing irrelevant regulations, skimming through redundant policy documents, and attending to background material that added no value to the specific analysis task. The old retrieval system had been selective, surfacing only the three to five most relevant regulatory sections. The new system dumped everything and hoped the model would figure out what mattered. It could not, or at least not efficiently enough to justify the cost and latency penalty.

By December 28th, they had rolled back to the old system. The $380,000 sprint had produced nothing. The engineering team spent January 2026 building a hybrid architecture that combined selective retrieval with targeted long-context usage for only the most complex cases. This time they measured diminishing returns, tested attention distribution, and optimized for value per token rather than context window bragging rights.

## Long Context as Capability, Not Universal Solution

Long context windows are a tool for specific problems, not a replacement for good information architecture. When Claude Opus 4 and GPT-4 Turbo launched with 200K token windows, many teams treated them as "throw everything in and let the model sort it out" black boxes. This works for narrow use cases—analyzing a single long document, comparing two complete codebases, or processing an entire conversation history—but it breaks down when you try to use long context as a substitute for retrieval, filtering, and relevance ranking.

The core issue is that **attention is finite**. A model with a 200K context window can process 200K tokens, but it cannot attend equally to all 200K tokens when generating each output token. Transformer architectures distribute attention across the context, and as context length grows, the attention allocated to any single token shrinks. The model can see all 200K tokens, but it cannot hold all of them in active working memory simultaneously. This is not a limitation of current models. It is a fundamental property of how attention mechanisms scale.

This creates a trade-off. Long context windows allow you to include more information, which increases the chance that relevant information is present somewhere in the context. But they also increase the chance that relevant information gets lost in the noise, especially if that information appears in the middle of a long context rather than at the beginning or end. You gain recall—more relevant facts are somewhere in the context—but you risk precision—the model might not reliably extract those facts when they matter.

Research published in late 2025 demonstrated this effect conclusively. When critical information appeared in the first 10,000 tokens of a 150,000-token context, models retrieved it correctly 94 percent of the time. When the same information appeared between tokens 60,000 and 90,000, retrieval accuracy dropped to 71 percent. The information was present. The model had access to it. But attention dilution made reliable extraction impossible.

You use long context windows when the task genuinely requires access to large amounts of information that cannot be easily filtered or ranked in advance. You avoid them when you can achieve the same result with selective retrieval and smaller contexts. The question is not "can I fit this in the context window" but "does including this improve the answer enough to justify the cost."

## What to Include vs Exclude in Long-Context Prompts

You include information that the model needs to generate a correct, complete, and grounded answer. This is not "everything that might be relevant." It is "everything that is necessary." The distinction matters because every additional token you include increases cost, latency, and the risk of attention dilution. Each token must earn its place.

You include complete documents when the task requires cross-referencing, coherence checking, or full-document synthesis. If your prompt asks the model to summarize a 50,000-token legal contract, you include the full contract because the model needs access to all clauses to identify conflicts and dependencies. If your prompt asks the model to compare two versions of a technical specification and identify all changes, you include both full versions because the model needs complete context to catch subtle edits that only make sense in relation to surrounding text.

You exclude documents that provide background context but are not directly referenced in the task. If your prompt asks the model to analyze a specific regulatory compliance issue, you include the regulations that apply to that issue. You do not include the entire regulatory code just because it exists in your knowledge base. If your prompt asks the model to debug a specific function, you include that function and its direct dependencies. You do not include the entire codebase unless the bug involves cross-module interactions that cannot be understood in isolation.

You exclude redundant information aggressively. If five documents all restate the same policy with minor phrasing variations, you include one canonical version and reference the others by name if needed. The model does not need to read the same policy five times to understand it. Redundancy wastes tokens and increases the chance of lost-in-the-middle errors where the model attends to the first instance and misses critical details in later instances.

You exclude low-relevance information even if it is topically related. If your prompt asks the model to analyze a contract dispute, you include the contract and directly relevant case law. You do not include a general primer on contract law unless the dispute involves an obscure legal principle that the model is unlikely to know. The model already has extensive knowledge of contract law basics from training. You are paying for tokens that teach it nothing new and distract it from the specific facts of the case.

A pharmaceutical company learned this in November 2025 when their drug interaction analysis system was producing vague warnings. They had been including full pharmacology textbook sections as context, reasoning that more background knowledge would improve accuracy. When they stripped the textbooks and included only the specific drug monographs and interaction databases, accuracy improved by eight percentage points and latency dropped by 60 percent. The textbooks had been noise.

## Diminishing Returns of Context Length

Context length exhibits diminishing marginal returns in almost every real-world application. The first 10,000 tokens of context often provide the bulk of the value. The next 50,000 tokens add incremental value. The next 100,000 tokens add very little unless the task specifically requires access to that much information. This pattern appears consistently across domains.

This happens because most tasks have a natural information boundary. A legal analysis task might require 5,000 tokens of contract text, 3,000 tokens of case law, and 2,000 tokens of internal notes—call it 10,000 tokens total. Adding another 50,000 tokens of tangentially related case law does not improve output quality because the model already has the information it needs to answer the question. The additional context is noise that dilutes attention and slows processing.

You measure diminishing returns by running controlled experiments where you vary context length and measure output quality using objective metrics. Start with a minimal context that includes only essential information. Measure quality using your evaluation framework. Add another 10,000 tokens of potentially relevant information and measure again. Keep adding context in increments until quality stops improving or begins degrading. That inflection point is your point of diminishing returns for that task type.

Most teams find that quality plateaus somewhere between 20,000 and 60,000 tokens for structured analysis tasks like contract review, compliance checking, or technical documentation. For summarization tasks, quality might plateau at 80,000 tokens because longer source documents contain more diverse information to synthesize. For creative tasks like story writing or brainstorming, quality might keep improving up to 150,000 tokens because the model benefits from diverse examples and background material that spark novel connections. The plateau point depends on task type and domain.

Once you hit the plateau, additional context is pure waste. It costs money, adds latency, and increases the risk of attention errors without providing value. You cut it ruthlessly. A document analysis platform discovered their average optimal context was 34,000 tokens despite having access to 200,000-token windows. They restructured their retrieval to target that length and cut costs by 72 percent with no quality loss.

Beyond the plateau, quality sometimes degrades. In contexts exceeding 150,000 tokens, models occasionally start ignoring critical information that would have been caught in smaller contexts. Lost-in-the-middle effects compound. Attention distribution becomes so thin that even important signals get missed. At extreme context lengths, you can achieve worse results than with focused, smaller contexts.

## Structuring Massive Inputs for Model Consumption

When you do need to include 100,000-plus tokens of context, you structure it to make the model's job easier. Unstructured dumps of text force the model to infer relationships, identify key sections, and navigate without signposts. Structured inputs provide roadmaps that guide attention and improve extraction reliability.

You use explicit section markers that create clear boundaries between documents. Instead of concatenating ten documents into one undifferentiated blob, you separate them with distinctive delimiters: "=== Document 1: Employment Contract ===", "=== Document 2: Employee Handbook ===", "=== Document 3: Severance Policy ===". The model can then reference specific documents in its reasoning chain using these labels. Without markers, the model generates vague statements like "according to the policy" without specifying which policy among the ten included documents.

You use hierarchical structure for nested information that has internal organization. If you are including a 60,000-token regulatory document, you preserve its section numbering and headings rather than stripping formatting. The model can then reference "Section 4.2.1" instead of "somewhere in the middle of the regulations." Hierarchical structure also helps the model navigate efficiently. If it needs information about data retention, it can scan section headers instead of reading every sentence sequentially.

You front-load critical information wherever possible. If there is a specific clause, regulation, or code snippet that your task hinges on, you place it near the beginning of the context. This exploits the primacy effect—models tend to attend more strongly to information near the start of the context window. You do not bury critical information on line 80,000 of a 120,000-token context and hope the model finds it. Attention probability distributions are not uniform. They skew toward the edges.

You use metadata tags to signal importance and document relationships. If certain documents are authoritative primary sources while others are supplementary background material, you label them: "Primary Source: IRS Publication 15," "Background Reference: Company Tax Policy." This helps the model weight information correctly when sources conflict. Without tags, the model treats all documents as equally authoritative, which causes problems when background material contradicts primary sources.

You compress low-priority information that must be included for completeness but is unlikely to be directly relevant. If you need to include a 40,000-token reference document that is only tangentially related to the task, you include a 2,000-token summary instead of the full text. The model gets the key points without wading through details it does not need. You include the full text only if the task requires fine-grained access to specific details that summaries might omit.

A legal research platform implemented this structured approach in October 2025. They included case law with explicit markers for holding, reasoning, and facts sections. They tagged precedential value and jurisdiction. They front-loaded the most relevant cases. Median accuracy improved from 82 percent to 91 percent on relevance judgments, and the model's citations became specific enough that lawyers could verify them without re-reading entire cases.

## Cost Implications of Long Contexts

Long context windows are expensive in ways that surprise teams accustomed to short-context pricing. Input tokens cost less than output tokens on a per-token basis, but at 100,000-token context lengths, input costs dominate your bill. If your model charges $3 per million input tokens and you run 10,000 requests per day with 150,000-token contexts, you are spending $4,500 per day on input tokens alone—$135,000 per month before output costs.

You calculate long-context costs by multiplying average context length by requests per period by input token price. If you are considering a migration to long-context prompts, you run this calculation before you write any code. You compare projected costs to your current architecture's costs. You estimate how much quality improvement you need to justify the cost increase. Most teams discover that long-context approaches are viable only if they reduce requests, improve outcomes enough to drive revenue, or replace even more expensive human labor.

You compare long-context costs to multi-stage retrieval costs explicitly. A two-stage system that retrieves 5,000 tokens in stage one and 15,000 tokens in stage two makes two API calls but processes only 20,000 tokens total. A single-stage long-context system that processes 150,000 tokens costs 7.5 times as much in input tokens despite requiring only one API call. The two-stage system's additional API call overhead is dwarfed by token savings. You choose the architecture that minimizes total cost while meeting quality requirements.

You use caching to reduce repeated context costs when the same documents appear in many requests. If you are running multiple analyses on the same regulatory document set, you cache the document embeddings or use provider-specific prompt caching so you do not re-process the same 100,000 tokens on every request. As of January 2026, Claude offers prompt caching where repeated context prefixes are billed at 90 percent lower rates. OpenAI offers similar features. These caching mechanisms can reduce costs by 50 to 80 percent for workloads with high context reuse.

You monitor context length distributions in production to optimize costs across your workload mix. If 80 percent of your requests use fewer than 30,000 tokens but 20 percent use 150,000 tokens, you split your system into two tiers. The high-volume, low-context tier uses a cheaper model or tighter retrieval filters. The low-volume, high-context tier uses long-context windows. This optimizes cost for the common case while maintaining capability for edge cases.

A customer support platform implemented this tiered approach in December 2025. Simple queries used 8,000-token contexts with Claude Opus 4.5 Haiku. Complex escalations used 120,000-token contexts with Claude Opus 4. The tier split reduced average cost per query by 64 percent while improving resolution rates because difficult cases got appropriate context depth.

## When Long Context Beats Multi-Stage Retrieval

Long context windows win when the task requires global coherence, cross-referencing, or synthesis across many sections of a document that cannot be identified through retrieval heuristics. If your prompt asks the model to identify all conflicting clauses in a 50,000-token contract, long context allows the model to hold the entire contract in memory and check every clause pair. A retrieval-based approach would struggle because you cannot predict which clauses conflict without reading them all and understanding their interactions.

Long context windows win when retrieval is expensive, brittle, or operationally complex. If your retrieval system requires embedding thousands of document chunks, maintaining a vector database with constant updates, and tuning relevance thresholds that drift as data distributions change, the operational cost might exceed the token cost of just processing full documents. Long context simplifies architecture at the cost of higher per-request token usage. For some teams, that tradeoff makes sense.

Long context windows win when the information need is unpredictable or exploratory. If your task is open-ended—"analyze this document and tell me what is interesting"—you cannot pre-filter relevant sections because you do not know what the model will find interesting until it processes the entire document. You include the full document and let the model explore. Retrieval requires knowing what you are looking for. Exploration requires seeing everything.

Long context windows win when latency is more important than cost per request. A single long-context API call completes faster than a multi-stage retrieval pipeline that makes three or four sequential API calls with retrieval operations between them. If your SLA requires sub-three-second responses and your retrieval pipeline takes four seconds, long context might be your only option even if it costs three times as much per request. User experience sometimes justifies premium cost.

A contract analytics company chose long context for their conflict detection feature despite 4x higher costs per analysis. The feature required comparing arbitrary clause pairs that retrieval could not predict. Multi-stage retrieval had 23 percent false negative rate. Long context with full contracts achieved 96 percent detection rate. The improved accuracy drove enough subscription upgrades to justify the higher costs.

## When Multi-Stage Retrieval Beats Long Context

Multi-stage retrieval wins when the task has a narrow information need and you can reliably identify relevant sections through semantic search, keyword matching, or structured queries. If your prompt asks the model to check whether a contract includes a non-compete clause, you retrieve the section on post-employment restrictions and send only that section to the model. You save 95 percent of the token cost and lose no quality because the answer is localized.

Multi-stage retrieval wins when most of your context is irrelevant most of the time. If you are running a Q&A system over a 500,000-token knowledge base and each question needs only 5,000 tokens of context on average, long context is absurd. You retrieve the relevant 5,000 tokens and ignore the rest. This applies to most customer support, documentation search, and knowledge base applications where queries are specific and answers are localized.

Multi-stage retrieval wins when your workload is heterogeneous and some requests need far more context than others. You use retrieval to normalize context length across requests. Every request gets the tokens it needs, not a fixed long-context budget that is wasteful for simple queries and insufficient for complex ones. This cost-optimizes across workload diversity.

Multi-stage retrieval wins when you need to compose information from multiple sources that are updated independently and cannot be pre-concatenated. If your system pulls data from a CRM, a knowledge base, and a vector database, and each source updates on different schedules, you retrieve from each source dynamically and compose contexts at request time. Long context does not help because you cannot maintain a static long context that stays current with three independently updating data sources.

Multi-stage retrieval wins on cost for most production systems. Unless your task genuinely requires access to 100,000-plus tokens per request, retrieval is cheaper. A customer support system processing 100,000 queries per day with 5,000-token retrieved contexts costs $1,500 daily. The same system with 150,000-token full-document contexts would cost $45,000 daily. The 30x cost difference funds a sophisticated retrieval system with plenty left over.

## Hybrid Architectures: Retrieval Plus Long Context

The best production systems combine retrieval and long context strategically. You use retrieval to filter down to a manageable set of relevant documents from a large corpus. Then you include the full text of those documents in a long-context prompt. This gives you the precision of retrieval and the global coherence of long context.

A typical hybrid flow: user submits a query, your retrieval system identifies the top five relevant documents from a corpus of 500 candidates, you include the full text of those five documents totaling 60,000 tokens in your prompt, the model generates an answer grounded in those five documents. You did not process all 500 documents, so you saved enormous cost. You did not truncate the five relevant documents to snippets, so you preserved coherence and avoided missing critical context that snippets might exclude.

You tune the retrieval threshold to balance cost and recall. If you retrieve only the top two documents, you minimize cost but risk missing relevant information in the third and fourth most relevant documents. If you retrieve the top twenty documents, you maximize recall but approach the cost of processing everything. You run experiments to find the sweet spot where additional retrieval candidates stop improving answer quality.

You use retrieval to handle the common case and long context to handle edge cases. Your system has a default mode that uses 30,000-token contexts from retrieval, optimized for the 85 percent of queries that are straightforward. If the model indicates that it needs more information to answer confidently—detected through confidence scoring or explicit statements—your system retrieves additional documents and makes a second request with a 100,000-token context. Most requests stay in the cheap mode. Difficult queries escalate to the expensive mode.

You use long context to handle cases where retrieval fails. If retrieval does not surface the relevant document because the query uses unexpected terminology or the document is poorly indexed, you fall back to a long-context prompt that includes a broader set of documents. This ensures robustness at the cost of occasional high-token requests. One financial services company uses this pattern: retrieval first, long context fallback when retrieval confidence is low. Fallback triggers on 8 percent of queries but catches most of their hardest cases.

A healthcare documentation system implemented a sophisticated hybrid in late 2025. First-stage retrieval identified relevant clinical guidelines. Second-stage retrieval added patient history records. These were concatenated into an 80,000-token context with full document text preserved. The system achieved 94 percent accuracy on diagnostic support tasks while keeping costs at $2.20 per analysis, compared to $8.40 for naive long-context approaches that included entire guideline databases.

## Latency Characteristics of Long-Context Prompts

Long-context prompts are slower than short-context prompts, all else equal. The model must process more input tokens before it begins generating output. For a 150,000-token context, this processing phase can add 10 to 30 seconds of latency depending on model architecture, provider infrastructure, and current system load. If your SLA requires sub-five-second responses, long context might be incompatible with your requirements no matter how much you optimize other factors.

You reduce latency by using prompt caching where available. If you are reusing the same 100,000-token regulatory document across multiple requests throughout the day, you cache it so the model does not re-process it every time. Cache hits reduce input processing latency by 70 to 90 percent because the provider can skip the encoding phase for cached content. This makes repeated long-context requests nearly as fast as short-context requests.

You reduce perceived latency by streaming output even when input processing is slow. Even if input processing takes 20 seconds, the user sees the first output tokens within a second or two after processing completes. Streaming makes long-context prompts feel faster even when total latency from request to completion is high. Users tolerate latency better when they see progress rather than waiting for a single delayed response.

You reduce latency by preprocessing documents offline when you know you will need specific documents for a category of requests. If customer support queries about product returns always need the returns policy document, you preprocess that document into a cached format during off-peak hours. At request time, you load the preprocessed content instantly. This shifts latency from request time to preprocessing time.

You set timeout policies for long-context requests to prevent worst-case latency from degrading user experience. If a request exceeds your latency budget, you terminate it and fall back to a faster, lower-context approach or return a partial answer with a flag indicating full context was not processed. This prevents occasional slow requests from creating unacceptable user experience.

A search application measured P99 latency at 41 seconds for 200,000-token contexts and 3.2 seconds for 20,000-token contexts. They implemented a timeout at 5 seconds that fell back to retrieval-based shorter contexts. The fallback triggered on 12 percent of requests but kept P99 latency under their 6-second SLA. Most users got long-context quality. All users got acceptable latency.

## Quality Monitoring for Long-Context Systems

You monitor long-context systems differently than short-context systems because the failure modes are different. Short-context systems fail when they lack information to answer correctly. Long-context systems fail when they have too much information and lose track of what matters. Your monitoring must detect attention failures, not just knowledge gaps.

You track citation accuracy by verifying that the model cites the correct documents and sections when grounding its answers. Long-context systems sometimes cite Document 7 when the information came from Document 3, or cite a document that was not included in the context at all. You detect this by parsing citations from outputs and cross-referencing them against your included documents. High citation error rates indicate attention problems.

You track lost-in-the-middle errors by creating synthetic test cases where the critical information appears at different positions in the context—beginning, middle, end—and measuring whether the model reliably finds it. If the model catches information at the beginning and end but misses information in the middle 30 percent of the time, you have confirmed position-dependent attention failure. You restructure your contexts to avoid placing critical information in the middle.

You track output specificity by measuring how often the model generates vague answers versus specific, grounded answers. Long-context systems sometimes generate vague statements like "according to the policies provided" instead of specific references like "according to Section 4.2 of the Employee Handbook dated January 2025." Vagueness suggests the model is not attending to specific sections and is instead producing generic summaries. You measure specificity by counting concrete references and checking citation precision.

You track cost per request and compare it to quality metrics. If your long-context system costs $4 per request and your old retrieval system cost $1 per request, you need to demonstrate that quality improved by enough to justify the 4x cost increase. If quality is only marginally better—say 91 percent accuracy versus 89 percent—the cost increase might not be justified unless those two percentage points have high business value.

A compliance monitoring system tracked all four metrics in production. They discovered citation accuracy was only 78 percent despite overall accuracy of 93 percent. The system was getting right answers but attributing them to wrong sources. For regulatory compliance, this was unacceptable. They restructured their contexts to front-load authoritative sources and added explicit citation requirements to their prompts. Citation accuracy improved to 94 percent.

## Long-Context Prompt Design Patterns

You design long-context prompts to guide the model's attention explicitly rather than assuming the model will automatically focus on the right information. You do not assume uniform attention distribution. You structure prompts to exploit known attention patterns.

You use a context map at the beginning of your prompt that lists what is included before including the full content. "This context includes: one, Full text of Employment Contract, 12,000 tokens; two, Employee Handbook excerpt, 8,000 tokens; three, Relevant case law, 15,000 tokens. Total context: 35,000 tokens." The model knows what it is working with and can plan its approach. Context maps are especially valuable when contexts exceed 50,000 tokens and the model cannot easily track what is included by scanning.

You place your task instructions at the end of the context, after all documents, to exploit the recency effect. Models attend more strongly to information near the end of the context window. If your task instructions are buried at the beginning and followed by 120,000 tokens of documents, the model might lose track of what you asked it to do by the time it finishes processing. End-placement keeps instructions fresh at generation time.

You use explicit focus directives when you need the model to pay special attention to specific sections. Instead of hoping the model notices that Section 4.2 is important, you write "Pay special attention to Section 4.2 of the contract, which governs termination conditions and is most relevant to this analysis." This is not prompt hacking. It is architectural guidance that helps the model allocate attention appropriately.

You ask the model to cite its sources explicitly in your instructions. "For each claim you make, cite the specific document and section where you found the information." This forces the model to ground its reasoning in the context and makes it easier for you to verify correctness. Citations also reveal when the model is attending to wrong sections or hallucinating information that is not present in the context.

The next subchapter examines prompt architecture for attention distribution across ultra-long contexts, covering techniques for placement, ordering, and structural design that maximize reliable information extraction from 100,000-plus token inputs.
