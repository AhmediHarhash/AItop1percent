# 7.13 â€” Custom Tool Design for Domain-Specific APIs

In April 2024, a healthcare platform built an AI assistant by exposing their existing REST API directly as tools. The API had 147 endpoints designed for web and mobile apps. The model consistently called wrong endpoints, mixed up parameters, and produced responses that required five or six API calls when two should have sufficed. User satisfaction scores were 3.2 out of 10. After redesigning their tool layer with domain-specific tools that wrapped API complexity, satisfaction jumped to 8.7. The difference wasn't model capability. It was tool design.

Your APIs were built for software engineers or frontend applications. They prioritize flexibility, backward compatibility, and fine-grained control. Models need something different: clear abstractions, obvious purposes, and minimal decisions per tool. Translating between these requirements is custom tool design. You create a model-friendly interface over your domain-specific APIs, handling complexity and ambiguity so the model doesn't have to.

## Building Tools for Your Specific Domain

Generic tools like web_search or calculate work across domains. Your competitive advantage comes from domain-specific tools that encode your business logic and data. A real estate platform needs tools like find_properties, calculate_mortgage, and schedule_showing. A logistics company needs track_shipment, optimize_route, and check_warehouse_inventory. These tools represent your unique capabilities.

Start by identifying the core operations users need in your domain. What questions do they ask? What actions do they take? Map user needs to backend capabilities. If users frequently ask "What homes are available in my budget?", you need a tool that combines property search with financial eligibility. If they ask "Where's my package?", you need shipment tracking with clear status communication.

Domain tools should match domain concepts, not database schemas. Users think in terms of orders, not order tables joined with order_items and customer_addresses. Your tool might be called get_order_details, and internally it queries five tables. But from the model's perspective, it's one coherent operation retrieving one domain entity.

Tool granularity matters enormously. Too fine-grained and the model needs many tool calls to accomplish simple tasks. Too coarse-grained and tools become inflexible Swiss Army knives trying to do everything. A get_customer tool that only returns name and ID is too granular. A manage_customer tool that handles retrieval, updates, deletion, and associated orders is too coarse.

The right granularity usually aligns with user-facing operations. If users view customer profiles as single operations, make get_customer_profile a single tool that returns comprehensive data. If users separately view contact info and order history, make them separate tools. Model the tool interface after user mental models, not system implementation.

Domain expertise should inform tool design. People who understand your business should participate in tool specification. Engineers understand the API, but domain experts understand what capabilities matter. A mortgage expert knows that debt-to-income ratio is critical for loan qualification. They'll ensure calculate_loan_eligibility includes DTI, where an engineer might miss it.

## Wrapping Existing APIs as Tools

Most organizations have existing APIs that need tool wrappers. The wrapper translates between model-friendly tool interfaces and your actual API. This translation layer is where you handle complexity, enforce constraints, and optimize for model use.

Direct API exposure rarely works well. Your API endpoint might be POST /api/v2/orders/search with a complex JSON body supporting dozens of parameters. The tool interface should be search_orders with clear, minimal parameters: status, customer_id, date_range. The wrapper constructs the complex API request from simple tool parameters.

Parameter simplification reduces model cognitive load. Your API might accept dates as Unix timestamps. Your tool should accept dates as "2026-01-30" strings and convert to timestamps in the wrapper. The model handles human-readable formats better than numeric timestamps. Let the wrapper handle format translation.

Default values in wrappers make tools easier to use. Your API requires specifying sort_order, page_size, and include_details on every call. Your tool wrapper provides sensible defaults: sort by relevance, 20 results per page, include basic details. The model can override these when needed, but most calls succeed with defaults.

Response transformation is as important as request transformation. Your API returns verbose JSON with nested objects, null fields, and internal IDs. Your tool should return cleaned, flattened data with human-readable labels. Transform customer ID, first name, and optional last name fields into readable text like "Customer: Alice (ID: 12345), Last name: Not provided."

Some APIs require authentication tokens, API keys, or request signing. Hide this entirely in the wrapper. The model calls get_account_balance without worrying about OAuth flows or HMAC signatures. The wrapper handles authentication transparently using credentials from the execution context.

## API Abstraction Strategies

Effective tool design requires abstracting API complexity. Multiple API calls might compose into one tool. Error conditions might need handling before reaching the model. Rate limits might require queueing or retry logic. These abstractions make tools reliable and usable.

Multi-call composition is common. To get comprehensive customer data, your API requires calling the customers endpoint, the customers orders endpoint, and the customers preferences endpoint, each parameterized by customer ID. Your tool get_customer_profile makes all three calls internally and returns unified results. The model makes one tool call, not three.

Pagination abstraction hides API pagination from models. Your API returns 50 results per page with next page tokens. Your tool can either return all results by following pagination automatically, or expose pagination as a simple page parameter. Either way, the model doesn't deal with cursors or continuation tokens.

Error normalization turns diverse API errors into consistent tool errors. Your various APIs return different error formats: some use HTTP status codes, some return error objects, some return success=false flags. Your tool wrapper normalizes these into standard error responses with clear messages.

Retry logic belongs in wrappers. If an API call fails with a transient error like 503 Service Unavailable, the wrapper retries with exponential backoff. The model never sees transient failures that will succeed on retry. Only persistent failures get returned as tool errors.

Rate limit handling prevents models from hitting API quotas. If your API allows 100 calls per minute, the wrapper tracks usage and queues requests that would exceed limits. The tool might respond slightly slower during bursts, but it never fails with rate limit errors.

## Domain-Specific Parameter Design

Generic parameters like query or id are ambiguous. Domain-specific parameters make tool purpose and usage obvious. Instead of query, use property_address or customer_email. Instead of id, use order_number or tracking_id. Specificity reduces model confusion.

Type constraints on parameters encode domain rules. A tool accepting price_max should specify it's a positive number. A tool accepting email should validate email format. A tool accepting state_code should enumerate valid two-letter codes. Build validation into parameter specifications so invalid calls fail fast with clear messages.

Enum parameters work better than free text for constrained values. Order status isn't a string field where the model might generate "delivered", "Delivered", "DELIVERED", or "shipment delivered". It's an enum: pending, processing, shipped, delivered, cancelled. The tool specification lists valid values explicitly.

Parameter relationships need documentation in descriptions. If your tool accepts both zipcode and city, but only one is required, specify this clearly. "Provide either zipcode or city, not both." If end_date must be after start_date, document that constraint. Clear parameter requirements prevent invalid tool calls.

Optional versus required parameters should match real-world usage. If 90% of searches use default sort order, make sort_order optional. If every order lookup requires order_number, make it required. Don't make parameters optional just to be flexible. Required parameters catch missing data early.

Parameter names should be self-documenting. price_range_min is better than min_price because it clarifies this is the minimum of a range, not minimum valid price. ship_to_address is better than address2 because it clarifies purpose. Verbose names are fine. Clarity matters more than brevity.

## Balancing Granularity

Tool granularity is the hardest design decision. Every operation could be its own tool, or you could have one mega-tool that does everything. The right balance depends on your domain and usage patterns.

Single-purpose tools are easiest for models to understand. calculate_shipping does one thing. The model knows when to use it. There's no ambiguity about parameters or behavior. Single-purpose tools also compose cleanly. The model chains get_cart, calculate_shipping, and apply_discount to show total cost.

Multi-purpose tools reduce tool count but increase complexity. An update_order tool might handle status changes, address updates, item modifications, and cancellations through a type parameter. This is fewer tools, but now the model must understand what type values mean and what parameters each type requires.

The guideline is one tool per user-facing operation. If users think of "checking order status" and "canceling an order" as different operations, make them different tools. If users think of "updating order details" as one operation covering multiple field changes, one tool might be appropriate.

Breaking up complex tools improves reliability. A multipurpose manage_inventory tool that handles receiving, shipping, adjustments, and audits is error-prone. Models confuse which operation needs which parameters. Breaking into receive_inventory, ship_inventory, adjust_inventory, and audit_inventory makes each tool's purpose obvious.

Composition is better than combination. Instead of one complex tool with many modes, create simple tools that models can chain together. This matches how models naturally break down complex tasks. "Check inventory, and if low, create reorder" becomes two tool calls: check_inventory_level and create_purchase_order.

Tool count does matter. If you have 200 tools, models struggle to select the right one. Context limits constrain how many tool descriptions you can provide. Group related functionality or implement tool namespaces. Instead of 20 product-related tools, maybe you have product.search, product.details, product.inventory, product.pricing, making relationships clear.

## Response Formatting for Model Consumption

How tools return data affects how well models use it. Return formats should prioritize clarity and structure. Models process well-structured data more reliably than informal text or deeply nested objects.

Flat structures beat nested structures. Instead of returning deeply nested customer name objects, return flat fields like customer_first_name and customer_last_name. Flatter data is easier for models to extract and cite.

Labeled values beat unlabeled values. Instead of returning just an array of order IDs, return both the order_numbers array and an order_count field. Labels make data interpretation obvious. Including metadata like counts helps models understand what they're looking at.

Consistent formatting across tools helps models learn patterns. If one tool returns dates as "2026-01-30" and another returns "1/30/2026" and another returns Unix timestamps, models struggle. Standardize formats across your tool set. ISO 8601 dates, USD currency formatting, consistent timezone handling.

Status and metadata fields provide context. A tool returning order details should include not just the data but also "status": "success", "retrieved_at": timestamp, "data_source": "production_database". This helps models understand data freshness and reliability.

Error responses need structure too. Don't return plain text errors like "Order not found". Return structured objects with status, error_code, error_message, and suggestion fields. Structured errors enable models to handle failures intelligently.

## Handling Domain Complexity in Tools

Real-world domains are complex. Tools must handle this complexity without exposing all of it to models. The goal is encapsulation: the tool manages complexity internally and presents simple interfaces externally.

Business rules belong in tool implementation, not model prompts. If orders over $1000 require manager approval, the place_order tool enforces this and returns appropriate responses. The model doesn't need instructions about approval thresholds. The tool handles it and communicates results.

Data validation happens in tools. If shipping addresses must include valid ZIP codes, the update_shipping_address tool validates this. Invalid addresses fail with clear messages. The model doesn't need zip code validation logic in its prompt. The tool prevents invalid data.

Multi-step workflows can hide in tools. Canceling an order might require checking eligibility, processing refund, updating inventory, and sending notifications. The cancel_order tool does all this atomically. The model makes one call, and complex workflow happens behind the scenes.

Conditional logic simplifies model responsibilities. A schedule_delivery tool might behave differently for same-day, next-day, and standard delivery. The tool handles complexity: checking availability, validating time windows, selecting carriers. The model just specifies desired delivery date.

Domain expertise gets embedded in tools. A calculate_loan_payment tool incorporates amortization formulas, interest calculation, and payment schedules. The model doesn't need financial expertise. It calls the tool with loan amount, rate, and term, and gets back a complete payment schedule.

## Iterative Tool Refinement

First-version tools are never perfect. You learn how models use tools by watching production usage. Iterate based on actual usage patterns, not theoretical design.

Monitor which tools are used frequently and which are rarely used. Unused tools might be designed poorly, documented unclearly, or simply not needed. Investigate why models aren't using them. Maybe they're calling two other tools instead of the one you expected.

Track tool call success rates. If a tool fails 30% of the time, something's wrong. Maybe parameter constraints are unclear, defaults are bad, or the underlying API is unreliable. High failure rates indicate tools need refinement.

Analyze parameter usage patterns. If a tool has five parameters but models only use two, maybe the other three should be removed or made internal. If a parameter is always set to the same value, make it a default. If two parameters are always used together, maybe they should be combined.

User feedback about tool-based features reveals tool problems. If users say "the AI couldn't find my order" but the order exists, maybe the search_orders tool has bad defaults or unclear parameters. User complaints map to tool improvements.

Model confusion patterns show design problems. If logs show the model repeatedly calling wrong tools or trying fictional tools, your tool set has gaps or ambiguities. Maybe you need a new tool, clearer names, or better descriptions.

A/B testing tool designs validates improvements. When redesigning a tool, deploy both versions to different traffic percentages. Measure success rates, user satisfaction, and error rates. Data-driven decisions beat theoretical arguments about tool design.

## Documentation and Examples in Tool Descriptions

Models learn tool usage from descriptions and examples. Invest in clear, comprehensive tool documentation. This isn't user-facing documentation. This is model-facing specifications that appear in prompts.

Tool descriptions should state purpose clearly and concisely. "Search for products in the catalog by name, category, or price range" is clear. "Product search API" is vague. Be specific about what the tool does and when models should use it.

Parameter descriptions need more detail than you think. Don't just say "The order number". Say "The unique order number, usually 8 digits, found in order confirmations and receipts. Example: 12345678." Examples in parameter descriptions help models understand expected formats.

Constraints and validation rules must be explicit. "price_max: maximum price in USD, must be positive number, maximum value 1000000" tells the model exactly what's valid. Vague descriptions lead to invalid tool calls that waste time and tokens.

Examples of valid tool calls are enormously helpful. Show a few realistic examples of calling the tool with parameters and what comes back. "Example: search_orders(customer_id=789, status='pending') returns list of pending orders for customer 789." Models learn patterns from examples.

Common mistakes and how to avoid them can be included. "Note: Do not use order status 'completed', use 'delivered' instead" prevents predictable errors. "If no results found, try broader search criteria" guides model recovery from empty results.

The next chapter examines emerging standards for tool interoperability, particularly the Model Context Protocol and how it enables tools to work across different models and platforms.
