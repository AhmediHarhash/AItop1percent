# 5.3 â€” Prompt Unit Tests: Input-Output Assertions

A fintech startup lost $620,000 in March 2025 when their fraud detection prompt started flagging legitimate transactions from international customers. The prompt had been working reliably for eight months. A routine update to improve detection of synthetic identity fraud inadvertently tightened criteria around address verification. The team tested the change on 15 recent fraud cases and saw improvement. They did not test on legitimate international transactions because those cases were not in their manual test set. Within three days, 2,847 valid transactions were incorrectly blocked, leading to customer churn, support costs, and payment processor penalties. The team had no systematic unit test suite that would have caught the regression. They had no assertions that verified the prompt correctly handled international addresses. They discovered the issue through customer complaints, not proactive testing.

This failure demonstrates why ad-hoc testing cannot protect production prompts. You need structured test suites with explicit assertions about expected behavior, comprehensive coverage of input variations, and automated execution on every change.

## What Prompt Unit Tests Accomplish

Unit tests for code verify that individual functions produce correct outputs for specified inputs. The same principle applies to prompts. A prompt unit test provides a specific input, executes the prompt, and asserts that the output meets expectations.

The difference from code unit tests is determinism. A function with input X always returns output Y. A prompt with input X might return slightly different outputs each time due to model randomness. This means assertion strategies must account for variance while still catching meaningful regressions.

Prompt unit tests serve multiple purposes. They document expected behavior through examples. They catch regressions when prompts change. They validate that prompts work across diverse input types. They establish quality baselines for new prompt development. They enable confident refactoring by proving behavior preservation.

Without unit tests, you validate prompts through manual inspection. This works for the first version. It fails as prompts evolve. Each modification might subtly change behavior in ways you do not notice until production failures emerge. Tests encode your expectations explicitly and verify them mechanically.

The fintech startup should have had unit tests asserting: legitimate domestic transactions with complete addresses pass, legitimate international transactions pass, transactions with mismatched billing and shipping addresses are reviewed but not auto-blocked, and known synthetic identity patterns are blocked. The third test would have failed after their update, catching the regression.

## Designing Effective Prompt Test Cases

Test case design determines test suite effectiveness. Good test cases cover representative inputs, edge cases, boundary conditions, known failure modes, and critical business logic. Poor test cases provide false confidence by testing only happy paths.

Representative inputs reflect your production distribution. If 60% of your traffic is Type A requests and 40% is Type B, your test suite should have similar proportions. If you test exclusively on Type A, you have no confidence about Type B. Sample real production data to build representative test sets.

Edge cases test boundary conditions and unusual inputs. For a document classification prompt, edge cases include: empty documents, documents in unexpected languages, documents with unusual formatting, very short documents (1-2 words), very long documents (at token limit), and documents that fit multiple categories equally well. Edge cases are where prompts break.

Known failure modes from production should become permanent tests. When you discover a prompt failure in production, add that specific case to your test suite before fixing the prompt. This creates regression tests that prevent the same failure from recurring. Track production failures systematically and convert them into tests.

Adversarial inputs test prompt robustness. These are inputs designed to break the prompt through injection, manipulation, or unexpected patterns. A customer support router might face adversarial inputs like: messages containing prompt injection attempts, messages with mixed languages, messages designed to trigger specific routing through keyword stuffing, and messages with emotional manipulation attempts.

Critical business logic must have explicit test coverage. If your prompt implements rules like "transactions over $10,000 require additional verification" or "medical questions trigger disclaimers," those rules need dedicated tests. These tests document business requirements and catch when prompt changes violate them.

## Assertion Types for Non-Deterministic Outputs

Traditional unit tests use exact equality assertions: assert output equals expected value. This works poorly for prompts because outputs vary. You need assertion types that tolerate acceptable variance while catching unacceptable changes.

Exact match assertions work when outputs should be deterministic. With temperature 0 and highly constrained prompts, many outputs are reproducible. A classification prompt that returns a single category name might reliably produce the same category. Test this with exact match: assert output equals "Billing Issue". Use exact match sparingly because it creates brittleness.

Contains assertions verify that outputs include required elements. A product recommendation prompt should include product IDs in the output. Assert output contains "SKU-12345". This tolerates variation in formatting and ordering while ensuring critical information is present. Contains assertions are robust to harmless changes.

Regex assertions verify structural patterns. If outputs should be JSON with specific fields, assert the structure with regex or JSON schema validation. If outputs should be formatted dates, assert the pattern matches ISO 8601. Regex assertions catch format violations without requiring exact text matches.

Semantic similarity assertions measure whether outputs convey the same meaning. Use embedding models to compute cosine similarity between actual and expected outputs. Assert similarity greater than 0.85. This tolerates paraphrasing and minor wording changes while catching semantic drift. The trade-off is that semantic similarity requires additional API calls and is less precise than exact matching.

Model-based evaluation assertions use another LLM to judge output quality. Provide the evaluator model with criteria and ask it to score the output. "Does this response answer the user's question accurately? Yes/No" or "Rate the helpfulness of this response from 1-10." This approach is flexible but adds cost and latency to test execution.

Multiple choice assertions work well for classification tasks. Assert output is one of the valid categories. "Output must be in ['Approved', 'Denied', 'Review']." This catches invalid outputs while allowing for any valid response.

The fintech fraud detection prompt should have used assertions like: for legitimate transactions, assert decision in ["Approved", "Review"] and assert decision not equals "Denied". This would have caught cases where legitimate transactions were incorrectly denied.

## Building Comprehensive Test Suites

A comprehensive test suite provides coverage across all important dimensions of prompt behavior. Coverage is not just quantity of tests but diversity of scenarios tested.

Input coverage ensures you test different types of inputs your prompt handles. For a text summarization prompt, test: news articles, research papers, conversation transcripts, product descriptions, legal documents, and social media threads. Each input type has different characteristics that might trigger different failure modes.

Output coverage ensures you test different types of outputs your prompt should produce. For a sentiment analysis prompt, test inputs that should return: positive sentiment, negative sentiment, neutral sentiment, mixed sentiment, and unclear sentiment. Verify the prompt handles the full range of expected outputs.

Parameter coverage tests different configurations if your prompt uses dynamic parameters. If you inject different few-shot examples based on input type, test each example set. If you adjust temperature based on use case, test each temperature setting. Configuration variations can introduce bugs.

Length coverage tests inputs and outputs of different sizes. Test with: minimal inputs (1-2 sentences), typical inputs (1-2 paragraphs), long inputs (multiple pages), and inputs at context window limits. Test that outputs stay within required bounds when you have length constraints.

Language coverage matters for multilingual applications. If your prompt should handle English, Spanish, and French, test all three. Language-specific errors are common. A prompt that works perfectly in English might fail in Spanish due to grammar differences that change model behavior.

Time coverage captures changes in real-world distributions. If your prompt references current events or time-sensitive data, test with different dates. A news classification prompt might behave differently on January content versus July content if seasonality matters.

Aim for at least 50-100 test cases for production prompts handling non-trivial tasks. Critical prompts warrant 200-500 tests. This seems like a lot compared to code unit tests, but prompts have more surface area for variance. More tests provide better confidence.

## Handling Flaky Tests in Prompt Test Suites

Flaky tests pass sometimes and fail sometimes without changes to the prompt. They erode confidence in test suites. Engineers start ignoring test failures when they cannot trust results. Addressing flakiness is essential for effective testing.

Temperature is the primary source of flakiness. Set temperature to 0 for test execution to maximize determinism. Even at temperature 0, some models show occasional variation, but it is minimal. If your production prompt uses temperature greater than 0, test at temperature 0 for regression detection and run a smaller set of smoke tests at production temperature.

Model API flakiness causes intermittent failures. APIs occasionally return errors or unusually slow responses. Implement retry logic in test execution: if a test fails, retry up to 3 times before marking it failed. This filters out transient API issues while catching real prompt problems.

Evaluation flakiness happens when your assertions are too strict. An exact match assertion on generated text is likely to be flaky even at temperature 0 due to minor variations. Loosen assertions to semantic similarity or contains checks. Balance precision with robustness.

Multiple sampling reduces flakiness when temperature greater than 0 is necessary. Run each test 3-5 times and aggregate results. If 4 out of 5 runs pass, mark the test as passed. If 0 or 1 out of 5 pass, mark as failed. If 2 or 3 out of 5 pass, mark as flaky and investigate. This provides statistical confidence.

Isolate flaky tests in a separate suite. When a test is consistently flaky, move it to a "flaky" category that runs but does not block merges. Investigate the root cause: is the test itself poorly designed, or is the prompt genuinely unstable? Fix or remove flaky tests rather than tolerating them indefinitely.

Track flaky test rates as a metric. If 20% of your tests are flaky, your test suite is not providing value. Aim for less than 1% flaky rate. This requires investment in deterministic testing infrastructure and assertion design.

## Writing Maintainable Test Cases

Test maintenance cost determines whether teams keep test suites updated. Tests that are hard to understand or modify get stale. Tests that break with every minor prompt change get deleted. Design for maintainability from the start.

Use descriptive test names that explain what is being tested. "test_international_address_approved" is better than "test_case_47". Good names serve as documentation and make failures easier to interpret. When a test fails, the name should immediately suggest what behavior broke.

Group related tests into logical suites. Organize by input type, output type, or business functionality. A customer support router might have test suites for: billing questions, technical support questions, account management questions, and adversarial inputs. Grouping makes it easier to understand coverage and identify gaps.

Keep tests independent. Each test should set up its own input, execute the prompt, and assert on the output without depending on other tests. Shared state between tests creates cascading failures and makes debugging harder. Independence allows running tests in parallel and in any order.

Document test intent with comments when the purpose is not obvious. If a test validates a specific edge case from a production incident, include a comment linking to the incident report. If a test validates a regulatory requirement, cite the regulation. Context helps future maintainers understand why tests exist.

Use test data factories or fixtures to generate inputs programmatically. Hard-coding 100 test inputs is brittle. A factory function that generates test cases from templates or schemas is maintainable. When input format changes, update the factory instead of 100 individual tests.

Parameterize tests when you need to run the same assertion logic on multiple inputs. Instead of writing 20 tests that differ only in the input value, write one parameterized test that runs on 20 inputs. Most test frameworks support parameterization: pytest's parametrize, unittest's subTest, etc.

## Integrating Unit Tests with CI/CD

Unit tests for prompts must run automatically in your CI pipeline. This provides continuous validation and prevents regressions from reaching production.

Trigger test execution on every commit or pull request that modifies prompts. Your CI workflow should: detect prompt changes, execute relevant test suites, report results, and block merge if tests fail. Integration with GitHub Actions, GitLab CI, or Jenkins makes this straightforward.

Optimize test execution speed through parallelization. Run tests concurrently across multiple workers. A suite of 200 tests that takes 20 minutes serially might complete in 2 minutes with 10 parallel workers. Fast feedback cycles improve developer productivity.

Cache test results when prompts have not changed. If a PR modifies code but not prompts, reuse previous test results instead of re-running. If a PR modifies prompt A but not prompt B, only test prompt A. Selective testing saves time and money.

Report test results clearly in PR comments. Show: total tests run, passed, failed, flaky; metrics like accuracy or latency across the test suite; and specific failures with input, expected output, and actual output. Make it easy for reviewers to understand test outcomes without digging through logs.

Set appropriate failure thresholds. You might allow 1-2 failed tests out of 200 if they are known flaky tests, but 10 failures should always block. Define these thresholds explicitly in your CI configuration.

Separate fast unit tests from slow integration tests. Unit tests run on every commit and should complete in under 5 minutes. Integration tests run on PRs marked ready-for-review or nightly and can take 30-60 minutes. This balances comprehensiveness with feedback speed.

## Test Data Management and Privacy

Test cases require input data. For many prompts, this means real user data or realistic synthetic data. Managing test data properly is essential for both quality and compliance.

Use production data samples when possible, but scrub personally identifiable information first. Real production data has authentic distributions and edge cases that synthetic data misses. Anonymize by removing or hashing user IDs, replacing names with placeholders, and redacting sensitive fields.

Synthetic data fills gaps where production data is insufficient. Generate synthetic test cases that cover edge conditions not yet seen in production. Use data generation libraries or LLMs to create realistic synthetic inputs. Document which tests use synthetic versus real data.

Versioning test data alongside prompts ensures reproducibility. Store test inputs and expected outputs in your repository or a test data warehouse. When a test fails, you can reproduce the exact scenario by checking out the same data version. Commit test data changes through the same PR process as prompt changes.

Compliance and privacy regulations affect test data usage. If you handle regulated data like healthcare records or financial information, test data must meet the same compliance standards. Do not use real production data if regulations prohibit it. Invest in high-quality synthetic data generation instead.

Refresh test data periodically to prevent staleness. Real-world distributions shift over time. An evaluation set built in January 2025 might not reflect July 2025 reality. Review and update test data quarterly. Add new production samples and remove obsolete cases.

## Measuring Test Suite Quality

A large test suite is not necessarily a good test suite. You need metrics that assess whether your tests effectively catch regressions and cover important behaviors.

Code coverage metrics do not apply directly to prompts, but you can measure input space coverage. Track: what percentage of input types are represented, what percentage of output types are tested, what percentage of edge cases have explicit tests, and what percentage of production failure modes are covered. Aim for high coverage across these dimensions.

Mutation testing reveals test suite effectiveness. Deliberately introduce bugs into your prompt: remove instructions, change examples, modify constraints. Run your test suite. Did tests catch the bugs? If a significant prompt change does not cause test failures, your tests are not sensitive enough. Good test suites catch most prompt mutations.

Historical effectiveness measures whether tests actually prevent production issues. Track: how many regressions were caught by tests before production, how many regressions escaped to production despite tests, and the severity of escaped issues. If tests catch 90% of regressions, your suite is effective. If tests catch 30%, your suite needs improvement.

Test maintenance burden indicates sustainability. Measure: how often tests break due to prompt changes, how long it takes to update tests after prompt changes, and what percentage of tests are flaky. High maintenance burden suggests over-fitted tests that are too brittle. Loosen assertions or redesign tests.

Developer confidence is a qualitative metric but matters. Survey your team: do you trust the test suite to catch problems? Do you feel comfortable refactoring prompts because tests provide safety? High confidence indicates an effective test suite. Low confidence suggests tests are seen as theater rather than valuable validation.

## Common Anti-Patterns in Prompt Testing

Several anti-patterns undermine prompt test effectiveness. Avoid these common mistakes.

Testing only happy paths creates false confidence. If all your tests use valid, well-formed inputs, you have not tested how prompts handle errors or edge cases. Production inputs include garbage, attacks, and corner cases. Test those scenarios.

Over-fitting tests to current implementation makes tests brittle. If you assert on exact phrasing that happens to be how the current prompt responds, any rephrasing breaks tests even if meaning is preserved. Test behavior and semantics, not implementation details.

Under-asserting allows regressions to pass. A test that executes a prompt and only checks that it returns any output at all provides minimal value. Assert on correctness, format, safety, and other quality dimensions. Weak assertions catch only catastrophic failures.

Testing in isolation from production conditions gives misleading results. If production uses temperature 0.7 but tests use temperature 0, you are not testing what you run. If production uses Claude 3.5 Sonnet but tests use GPT-4o, results do not generalize. Match test conditions to production.

Ignoring flaky tests normalizes unreliable testing. When tests fail intermittently and engineers shrug and rerun, the test suite loses credibility. Fix or remove flaky tests. Tolerate zero flakiness.

Never updating tests after production failures is a missed opportunity. Every production incident should generate new test cases. If a prompt failed in a specific way, create a test that would have caught it. Build your regression test suite from real failures.

## Test-Driven Development for Prompts

Test-driven development (TDD) for prompts means writing tests before writing or modifying prompts. This inverts the typical workflow and provides several benefits.

The TDD cycle for prompts is: write a test that defines expected behavior for a new capability or bug fix, run the test and verify it fails (the prompt does not yet have the capability), modify the prompt to make the test pass, run all tests to ensure no regressions, and refactor the prompt for clarity while keeping tests passing.

TDD forces clear specification before implementation. Writing the test requires articulating exactly what you expect the prompt to do. This often reveals ambiguities in requirements that you would otherwise discover later through failed implementations.

TDD builds regression tests automatically. Every capability you add comes with a test that prevents future breakage. You never have the situation where a feature exists in production but has no test coverage.

TDD works better for prompts with clear specifications than for exploratory prompt development. If you know exactly what a classification prompt should return for different inputs, TDD is natural. If you are experimenting with creative generation where "good" is subjective, TDD is forced.

Adopt TDD selectively. Use it for bug fixes: write a failing test that reproduces the bug, then fix the prompt. Use it for well-specified new features: write tests defining success criteria, then build the prompt. Skip it for early-stage experimentation where requirements are fuzzy.

## Evaluating Test ROI

Tests have costs: development time, execution time, and maintenance burden. They provide value by catching bugs and enabling confident iteration. Evaluate ROI to ensure you are testing appropriately.

Development time cost is the engineer hours spent writing tests. A test that takes 5 minutes to write is cheap. A test that requires 2 hours of data preparation is expensive. Prioritize high-value tests. Critical business logic deserves expensive tests. Minor edge cases might not.

Execution time cost is the latency and API costs of running tests. A fast test suite that completes in 2 minutes costs little developer time. A slow suite that takes 30 minutes disrupts workflow. Balance coverage with execution speed.

Maintenance cost is the effort required to update tests when prompts change. Brittle tests that break with every minor modification are expensive. Robust tests that tolerate harmless changes are cheap. Design for low maintenance cost.

Bug detection value is the severity and frequency of bugs the test catches. A test that would have prevented the fintech startup's $620,000 fraud detection failure has massive value. A test that catches a typo in a non-critical output has minimal value.

Confidence value is harder to quantify but real. Tests that enable fearless refactoring and rapid iteration provide value beyond specific bug catches. Measure this through development velocity: do comprehensive tests accelerate feature development by reducing debugging and production incidents?

The fintech startup now has 340 unit tests for their fraud detection prompt. Development took two weeks. The tests cost $200 monthly to execute. They have caught 12 regressions over six months that would have reached production. Even one prevented incident paid for the entire test suite investment.

Unit tests are not optional for production prompts. Build comprehensive test suites, automate execution, and maintain tests as prompts evolve. Tests are the foundation of confident iteration and reliable AI systems.

The next subchapter examines regression testing, exploring techniques for detecting silent quality degradation when prompts or models change.
