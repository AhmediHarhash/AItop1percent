# 8.6 — Code Output: Generation, Formatting, and Execution Patterns

In July 2025, a workflow automation platform allowed users to create custom integrations by describing them in natural language. The AI generated Python code that was automatically saved to the user's workspace and executed on the next workflow run. Within two weeks, they had a security incident: a user had pasted malicious instructions into the natural language description, causing the AI to generate code that exfiltrated environment variables containing API keys to an external server. The code executed successfully because there were no validation, sandboxing, or security review layers. The company had focused on making code generation work but hadn't considered that generated code is both structured output and executable instructions. After the incident, they implemented multi-layer validation: static analysis for dangerous patterns, sandboxed execution environments with network restrictions, runtime monitoring for anomalous behavior, and human review gates for production code. Generated code incident rates dropped to near zero, but the engineering complexity increased 10x. The lesson: **code as output** is fundamentally different from data as output because code has side effects.

Code generation is structured output where correctness means both syntactic validity and semantic safety. The code must parse, execute without errors, produce correct results, and not do anything harmful. This requires validation layers that go beyond schema checking to include static analysis, security scanning, test execution, and sandboxed environments.

## Code Generation Correctness Dimensions

When AI generates code, multiple dimensions of correctness must be satisfied simultaneously.

**Syntactic correctness** means the code parses according to the language's grammar. No missing parentheses, unmatched braces, invalid keywords, or malformed syntax. This is the minimum bar. Most modern language models handle syntax well for popular languages (Python, JavaScript, Java) but struggle with less common languages or complex syntax (Haskell, Scala, nested SQL).

Syntactic validation uses language parsers: Python's ast module, JavaScript's esprima or babel parser, Java's javac parser. Parse the generated code. If parsing succeeds, syntax is valid. If parsing fails, the code is unusable. Syntax errors are usually regeneration triggers because they're hard to repair automatically without changing semantics.

**Semantic correctness** means the code does what it's supposed to do. It implements the specified logic, handles edge cases, returns correct outputs, and doesn't have off-by-one errors, null pointer issues, or logical mistakes. Semantic correctness is harder to validate than syntax because it requires understanding intent.

Semantic validation uses tests. Generate test cases based on the code's specification, execute the code against those tests, verify outputs match expectations. If tests pass, semantic correctness is likely (but not guaranteed—tests might not cover all edge cases). If tests fail, the code is incorrect and needs regeneration or debugging.

**Runtime safety** means the code doesn't crash, hang, or consume excessive resources. No infinite loops, no unhandled exceptions, no stack overflows, no memory leaks. Runtime safety requires executing the code in a controlled environment and monitoring behavior.

Safety validation uses sandboxed execution with resource limits: timeout after N seconds, kill if memory exceeds M megabytes, restrict file system and network access. If execution completes within bounds, runtime safety is acceptable. If execution exceeds bounds, the code is unsafe for production.

**Security safety** means the code doesn't contain vulnerabilities or malicious behavior. No SQL injection, no path traversal, no arbitrary code execution, no credential exfiltration, no unauthenticated API calls. Security validation uses static analysis and pattern matching to detect dangerous constructs.

Security validation scans for known vulnerability patterns: eval() or exec() in Python, innerHTML or eval() in JavaScript, system() or shell_exec() in PHP, concatenated SQL queries, hardcoded credentials, unrestricted file operations. Presence of these patterns triggers warnings or blocks execution pending review.

**Style conformance** means the code follows formatting standards, naming conventions, and idioms for the target language. This doesn't affect correctness but affects readability, maintainability, and integration with existing codebases. Style validation uses linters and formatters: pylint and black for Python, eslint and prettier for JavaScript, checkstyle for Java.

Style violations are usually non-blocking but should be flagged and ideally auto-fixed. Running black or prettier on generated code before saving it ensures consistent formatting. Running linters identifies code smells (unused variables, overly complex functions, missing docstrings) that reduce quality.

## Prompting for Code Generation

Effective code generation prompts balance specificity with flexibility.

**Language and version specification** removes ambiguity. "Generate Python 3.10 code" is clearer than "generate Python code." Different language versions have different syntax (Python 2 vs 3), different standard libraries, different idioms. Specifying versions ensures the model generates code that's compatible with your runtime environment.

**Functional requirements** describe what the code should do. "Write a function that takes a list of integers and returns the sum of all even numbers" is specific. "Write a function that processes numbers" is vague. Functional requirements should include input types, output types, edge case handling, and error conditions.

**Constraints and context** specify what libraries are available, what the code can assume about its environment, and what it should avoid. "Use only Python standard library, no external dependencies. Assume input is always a non-empty list. Do not use deprecated functions." Constraints prevent the model from generating code that won't work in your environment.

**Style requirements** request specific formatting and conventions. "Follow PEP 8 style guidelines. Use type hints for all function parameters and return values. Include docstrings in Google style. Prefer list comprehensions over explicit loops where readable." Style requirements make generated code consistent with your codebase.

**Test generation** requests that the model generate tests alongside code. "Include pytest test cases covering normal inputs, edge cases (empty list, single element, all negative numbers), and error cases." Tests provide immediate validation that the code works and serve as documentation of intended behavior.

**Security guardrails** explicitly prohibit dangerous patterns. "Do not use eval, exec, os.system, or subprocess. Do not read or write files. Do not make network requests. All data processing should be pure functions." Security guardrails reduce (but don't eliminate) the risk of generating vulnerable code.

## Static Analysis and Security Scanning

Before executing generated code, static analysis identifies issues without running the code.

**AST-based analysis** parses code into an abstract syntax tree and searches for dangerous patterns. Python's ast module lets you walk the tree looking for specific node types: ast.Call nodes with func.id == "eval", ast.Import nodes importing dangerous modules, ast.Attribute access to sensitive objects.

For example, scanning for eval usage: parse code with ast.parse(), walk the tree with ast.walk(), check each node, if it's ast.Call and func.id is "eval", flag as dangerous. This catches explicit eval calls but might miss indirect calls (getattr(__builtins__, "eval")).

**Regex-based pattern matching** catches simple dangerous constructs without full parsing. Search for strings like "eval(", "exec(", "os.system(", "subprocess.run(", "open(", "__import__". Regex is faster than AST parsing but less precise—it generates false positives (flagging eval in comments or strings) and false negatives (missing obfuscated calls).

**Third-party security scanners** like Bandit (Python), ESLint security plugins (JavaScript), or FindSecBugs (Java) provide comprehensive vulnerability detection. These tools have extensive rule sets for common vulnerabilities: hardcoded secrets, SQL injection, XSS, insecure randomness, weak crypto. Integrate these scanners into your validation pipeline.

**Dependency analysis** checks what libraries the code imports. If the code imports libraries not on your allowlist, block it. If it imports deprecated or vulnerable libraries, flag for review. If it tries to import non-existent libraries, the code won't run.

Static analysis catches many issues but not all. It can't detect logic bugs, performance problems, or subtle security issues that require understanding program semantics. Static analysis is a necessary but not sufficient validation layer.

## Sandboxed Execution Environments

Executing generated code is risky. Sandboxing limits what code can do if it's malicious or buggy.

**Containerization** runs code in isolated containers (Docker, Firecracker) with no access to host system resources. The container has restricted filesystem (no access to sensitive directories), no network access (or only allowlisted endpoints), limited CPU and memory, and a timeout. If code tries to exfiltrate data or consume excessive resources, the container limits prevent harm.

Setting up sandboxes: create a minimal container image with the required language runtime, mount code as read-only, set resource limits (ulimit, cgroups), disable network (or use network policies to restrict), run code with non-root user, capture stdout and stderr, enforce execution timeout. If execution exceeds timeout or resource limits, kill the container and report failure.

**Language-level sandboxing** uses language features to restrict code capabilities. Python's RestrictedPython provides a restricted execution environment where dangerous builtins are disabled. JavaScript's vm module creates isolated contexts. JVM security managers restrict Java code capabilities. These are lighter-weight than containers but easier to bypass.

**Web Assembly sandboxing** compiles code to WASM and executes in a sandboxed WASM runtime. WASM has strong isolation guarantees: no direct memory access outside the sandbox, no file system access, no network access unless explicitly granted. This works for languages that compile to WASM (Rust, C, C++, and increasingly Python and JavaScript).

**Cloudflare Workers or AWS Lambda** provide serverless sandboxes where generated code runs in isolated, short-lived execution environments with built-in resource limits. These are convenient for small code snippets but add latency and cost for high-volume execution.

Sandbox choice depends on your threat model and performance requirements. High-security applications need container or WASM sandboxing. Low-stakes applications might accept language-level sandboxing. Evaluate trade-offs between isolation strength, execution overhead, and implementation complexity.

## Test-Driven Code Validation

Tests are the most reliable way to validate that generated code is semantically correct.

**Specification-based test generation** creates tests from the code's requirements. If the specification says "function takes a list of integers and returns the sum of even numbers," generate tests: sum_even([2, 4, 6]) should return 12, sum_even([1, 3, 5]) should return 0, sum_even([]) should return 0, sum_even([-2, -4]) should return -6.

Test generation can be done by the same model that generated the code or by a separate model. Using a separate model reduces the chance that the tests are biased toward the code's actual behavior rather than the specification's intended behavior.

**Execution and assertion** runs the generated code against tests in the sandbox. Capture outputs, compare to expected values, report pass or fail. If all tests pass, the code is likely correct. If any test fails, the code is incorrect and needs regeneration or debugging.

Test execution should handle exceptions gracefully. If the code throws an exception during testing, that's a test failure. Capture the exception, log it, and report which test case triggered it. This helps diagnose whether the problem is a code bug or an invalid test.

**Property-based testing** generates random inputs that satisfy certain properties and verifies that outputs satisfy expected properties. For example, for a sorting function, property-based tests generate random lists, verify the output is sorted, verify output has the same length as input, verify output contains the same elements. Property-based testing catches edge cases that specification-based tests miss.

**Regression testing** for iteratively generated code compares new versions to previous versions. If you regenerate code after a failed test, run all tests that passed before plus new tests. Ensure that fixes don't break previously working functionality. This prevents regressions during iterative debugging.

Test-driven validation is expensive—requires test generation, sandboxed execution, result analysis—but it's the most reliable correctness check. For production code generation, the cost is justified.

## Code Formatting and Style Enforcement

Generated code should be formatted consistently and follow best practices.

**Auto-formatters** normalize code style mechanically. Run black (Python), prettier (JavaScript), gofmt (Go), or rustfmt (Rust) on generated code before saving. Auto-formatters remove style variability and make code readable according to community standards.

Auto-formatting is deterministic and fast. It never changes semantics, only layout. There's no downside to auto-formatting every generated code snippet.

**Linters** identify code smells and potential bugs. Run pylint (Python), eslint (JavaScript), clippy (Rust), or golint (Go) on generated code. Linters flag unused variables, overly complex functions, missing error handling, deprecated API usage, and style violations.

Linter warnings are advisory. Some warnings indicate serious problems (unused variable might indicate a logic bug). Others are stylistic (line too long). Configure linters to match your project's standards: which warnings are errors, which are informational, which are disabled.

**Type checking** verifies type correctness for languages with type systems. Run mypy (Python with type hints), TypeScript compiler (JavaScript/TypeScript), or javac (Java) to catch type errors. Type errors usually indicate semantic mistakes—passing the wrong argument type to a function, returning the wrong type from a function.

Type checking requires that generated code includes type annotations. Prompt the model to include type hints: "Include Python type hints for all function parameters and return types." Or add type hints in post-processing based on analysis of how variables are used.

**Documentation generation** creates API docs from code. Tools like pydoc (Python), JSDoc (JavaScript), or javadoc (Java) extract docstrings and generate documentation. If generated code will be part of a library or API, generating documentation ensures it's usable.

Prompt the model to include docstrings: "Include docstrings for all functions, describing parameters, return values, and behavior." Or generate docstrings separately by prompting the model to document already-generated code.

## Iterative Debugging of Failed Code

When generated code fails validation or tests, you have several debugging strategies.

**Error message feedback** includes validation errors in a retry prompt. "The generated code failed with this error: [error message]. Fix the code to address this error." The model uses the error message to understand what went wrong and generates corrected code.

This works well for simple errors (syntax errors, type errors, test failures with clear assertions). It works less well for complex logic bugs where the error message doesn't clearly indicate the fix.

**Incremental fixing** regenerates only the problematic part. If a function has a bug but the rest of the code is correct, prompt the model to regenerate just that function: "Fix the calculate_total function. It currently fails this test: [test case]. The function should [specification]."

Incremental fixing is more efficient than regenerating entire programs. It reduces token usage and preserves working code. But it requires identifying which specific part is broken, which might not be obvious for integration bugs.

**Test-driven debugging** adds failing tests to the prompt for regeneration. "Here's the code you generated: [code]. It fails these tests: [test cases]. Generate corrected code that passes all tests." The model sees both the code and the failing tests, giving it context to understand the problem.

**Explanation-based debugging** asks the model to explain the code before fixing it. "Explain what this function does step by step. Then identify why it fails this test case." Explanation forces the model to reason about the code, increasing the chance that it identifies the bug correctly.

Debugging is iterative and expensive. Limit iteration count (3-5 attempts) before escalating to human review. Track which debugging strategies work best for which error types to optimize your debugging pipeline.

## Human Review Gates for Production Code

Automatically generated code that will run in production or access sensitive resources should have human review.

**Code review workflow** routes generated code to developers for approval. The review interface shows the specification, the generated code, test results, static analysis reports, and execution logs. Reviewers verify the code does what it should, follows standards, and is safe.

Code review adds latency but catches issues that automated validation misses: subtle logic bugs, poor design decisions, missed edge cases, security vulnerabilities not caught by scanners. For high-stakes code (financial transactions, security-sensitive operations, data processing), human review is essential.

**Approval-based deployment** prevents generated code from running until reviewed. Code is generated, validated, and staged, but not executed. A human reviews and approves. Only approved code runs. This is the safest pattern but the slowest.

**Tiered review** applies different review levels based on risk. Low-risk code (read-only operations, internal tools, non-production environments) gets automatic approval if validation passes. High-risk code (writes to databases, external API calls, production deployments) requires human review. This balances safety and velocity.

**Post-deployment monitoring** watches generated code in production for anomalies. If execution times spike, error rates increase, or behavior diverges from patterns, alert for review. Monitoring catches issues that validation missed and that manifest only under production load or data.

## The Code Generation Production Stack

Production code generation systems combine multiple layers for safety and quality.

**Prompt engineering** that specifies functional requirements, constraints, style, security guardrails, and test generation.

**Syntax validation** that parses code and rejects syntactically invalid outputs.

**Static analysis** that scans for security vulnerabilities, dangerous patterns, and code smells.

**Sandboxed test execution** that runs generated code against tests in isolated environments with resource limits.

**Formatting and linting** that normalize style and identify quality issues.

**Human review gates** for production-bound or high-risk code.

**Runtime monitoring** that watches code in production and alerts on anomalies.

Each layer provides safety and quality guarantees. Each layer catches issues the others miss. Code generation is complex, expensive, and risky, but when implemented with proper validation and safeguards, it's a powerful capability that enables automation of previously manual tasks.

The final section in this chapter examines output post-processing pipelines: transforming model outputs before serving them to users or downstream systems, handling format conversions, content enrichment, and output normalization to meet production quality standards.
