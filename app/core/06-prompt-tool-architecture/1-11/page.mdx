# 1.11 â€” Prompt Decomposition: Breaking Complex Tasks Into Prompt Chains

A healthcare technology company deployed an AI claims processor in October 2024 that analyzed medical bills, verified insurance eligibility, checked for coding errors, calculated patient responsibility, identified fraud patterns, generated denial letters, and flagged appeals in a single 6,800-token prompt. The system represented eighteen months of business logic development. The prompt contained 127 distinct conditional branches, 43 examples, and instructions written by seven different product managers who each added requirements without reading what came before.

The system failed in ways no one could debug. A claim for physical therapy would sometimes calculate correct copays but generate denial letters citing mental health coverage limits. A claim with minor documentation issues would sometimes trigger fraud alerts that should have required three separate red flags. When engineers tried to fix one error category, they created new errors elsewhere. After four months of attempting to patch the monolithic prompt, accuracy had improved from 81 percent to 84 percent while token costs had grown by 40 percent.

The CTO made the call to decompose. The team broke the monolith into nine sequential steps with clear boundaries: intake validation, eligibility verification, procedure code validation, pricing calculation, fraud scoring, payment determination, letter generation, appeal detection, and audit logging. Each step ran as a separate model call with explicit inputs and outputs. Within three weeks of deployment, accuracy reached 94 percent. Error diagnosis became trivial because each step's logs showed exactly where processing failed. Token costs dropped 35 percent because most steps used focused prompts under 600 tokens. The team learned that complexity scales through composition, not through bigger prompts.

## The Appeal of Monolithic Design

You build monolithic prompts because they feel natural. When you explain a complex task to a human colleague, you describe the whole workflow in one conversation. You provide all the context, explain all the rules, mention all the edge cases, and expect them to handle it. This mental model transfers directly to prompt design. You write everything the model needs to know into one comprehensive instruction set.

Monolithic prompts also avoid coordination complexity. You do not need to manage data passing between steps. You do not need to worry about error propagation across boundaries. You do not need to version multiple components. Everything lives in one place, visible in one file, debuggable in one context window. For small tasks, this simplicity is valuable.

The transition from simple to complex is gradual enough that you do not notice when monolithic design stops working. You start with a 400-token prompt that works well. Requirements expand, so you add another paragraph. Then another example. Then a special case. Then a clarification. Six months later you have 3,000 tokens and you cannot remember why half of it exists or what would break if you removed any particular instruction.

The breaking point arrives when the prompt contains multiple distinct cognitive functions competing for the model's attention. Extraction logic interferes with generation logic. Validation rules conflict with formatting requirements. The model must simultaneously parse inputs, apply business rules, generate outputs, and maintain consistency across unrelated requirements. This cognitive overload produces unpredictable failures.

Testing becomes impossible because you cannot isolate failures. Is the error in how the model parsed the input, how it applied business logic, how it formatted the output, or how it handled an edge case? The entire process is opaque. You change one instruction and observe whether overall accuracy improves, but you have no mechanistic understanding of what you fixed or what you might have broken.

## Decomposition by Natural Boundaries

The most effective decomposition follows natural task boundaries that separate distinct cognitive functions. One step extracts structured data from unstructured text. Another validates that extracted data against business rules. A third makes decisions based on validated data. A fourth generates natural language outputs communicating those decisions. Each function has different failure modes, different accuracy requirements, and different optimization strategies.

Data extraction fails when inputs are ambiguous, incomplete, or malformed. The model must handle variations in formatting, missing fields, and unclear references. Optimizing extraction means adding examples of difficult inputs and teaching the model robust pattern matching. Extraction prompts focus on comprehension and structure.

Business rule application fails when rules conflict, edge cases are unclear, or priority is ambiguous. The model must apply formal logic consistently. Optimizing rule application means clarifying edge cases, providing decision trees for complex scenarios, and testing boundary conditions. Rule prompts focus on precision and consistency.

Natural language generation fails when tone requirements are complex, audience expectations vary, or context needs to be adapted. The model must produce human-appropriate communication. Optimizing generation means providing style guides, audience profiles, and examples of excellent outputs. Generation prompts focus on clarity and appropriateness.

Mixing these functions in one prompt means you cannot optimize any of them effectively. An instruction that helps extraction might interfere with generation. An example that clarifies rule application might confuse data parsing. Decomposition allows specialized optimization.

The boundaries often align with how domain experts describe the task. Ask a claims processor how they work and they will describe steps: receive claim, check eligibility, validate codes, calculate amounts, identify issues, generate correspondence. These natural steps are your decomposition points. Your prompt architecture should mirror how humans conceptualize the workflow.

## Identifying When to Split

You know a prompt needs decomposition when you find yourself writing meta-instructions about instruction order. Phrases like "first do X, then do Y, then do Z" indicate sequential steps that should be separate prompts. If you are numbering steps within your prompt, those numbers are telling you where to draw boundaries.

Long prompts with distinct sections separated by headers are begging for decomposition. If your prompt has sections titled "Input Validation," "Business Rules," "Output Formatting," and "Error Handling," those are components. The headers exist because even you, the author, cannot hold the entire prompt in working memory. The model faces the same challenge.

Version control diffs reveal decomposition opportunities. If every requirement change touches the same 2,000-token prompt, you have high coupling. When one product manager adds a fraud rule and another adds a formatting requirement, and both edits conflict, you need separation. Components that change for different reasons should be different components.

Testing difficulty is a strong signal. If you cannot write a unit test for one aspect of your prompt's behavior without exercising the entire prompt, you lack modularity. Testability and decomposition are closely related. The ability to test extraction logic independently from generation logic requires separating them.

Performance profiling sometimes reveals decomposition needs. If 90 percent of your requests need only 40 percent of your prompt's functionality, you are wasting tokens. A loan application system might need fraud checking only for applications over 50,000 dollars, but the monolithic prompt includes fraud instructions for all applications. Decomposition with conditional execution saves costs.

The decision to decompose involves tradeoffs. Decomposition adds latency because each step requires a separate API call. It adds complexity because you must manage data flow between steps. It adds failure points because each step can fail independently. Only decompose when these costs are justified by gains in debuggability, testability, or optimization.

## Designing Information Flow

Each step in a decomposed chain consumes inputs and produces outputs. The information architecture determines whether the chain works smoothly or accumulates friction.

The simplest pattern passes complete outputs forward. Step one extracts fifteen fields from a document. Step two receives all fifteen fields as input. This maximizes information availability but wastes tokens. If step two needs only three fields, passing all fifteen is inefficient.

Selective forwarding includes only necessary information. Step one extracts fifteen fields and stores them in state. Step two receives only the three fields it needs. This requires explicit schemas defining each step's inputs. You must document what each step expects and ensure upstream steps provide it.

Enrichment patterns add information at each stage. Step one extracts entities from text. Step two queries a database for additional data about those entities. Step three receives both the original extractions and the enriched data. The information grows richer as it flows through the chain, with each step adding context.

Transformation patterns change data format between steps. Step one produces JSON. Step two converts that JSON to a domain object graph. Step three converts domain objects to a narrative summary. Each step works in its natural format rather than forcing all steps to share one representation.

The challenge is error propagation. If step one makes a small mistake and you pass its output to step two, the error might compound. Step two might make decisions based on incorrect data, producing wrong results confidently. Validation between steps catches errors early before they cascade.

Confidence metadata helps downstream steps handle uncertain inputs. If step one extracts a field but has low confidence, it marks that field as uncertain. Step two can be more conservative with uncertain inputs or route them to human review. This prevents low-quality extractions from polluting downstream processing.

## Managing Error Accumulation

Error propagation in chains follows probability multiplication. If step one has 95 percent accuracy and step two has 95 percent accuracy, the combined accuracy is not 95 percent. It is 90.25 percent, assuming independence. In practice, errors often correlate, making combined accuracy even lower.

Compounding occurs when step two's errors depend on step one's errors. If step one misclassifies a document type, step two applies wrong processing rules, producing garbage. If step two had received the correct classification, it would have worked perfectly. The error chain creates failures that no individual step would produce alone.

You mitigate propagation through validation steps. After extraction, validate that all required fields are present and values are within expected ranges. After calculation, check that results satisfy basic invariants. After generation, verify that outputs match required formats. These validation steps act as circuit breakers that prevent bad data from flowing further.

Retry mechanisms can recover from transient errors. If validation fails, regenerate the step with a different temperature or a rephrased prompt. If three attempts all fail, escalate to human review. This trades latency for quality in cases where automatic processing struggles.

Confidence thresholds provide another layer of defense. If any step produces output below a confidence threshold, halt the chain and route to manual processing. This prevents low-quality automatic decisions from reaching users. The threshold balances automation rate against error rate based on your domain's tolerance.

Some domains require perfect accuracy from critical steps. A pharmaceutical dosage calculator must never make arithmetic errors. You might use symbolic computation for that step rather than language model processing, creating a hybrid chain where language models handle ambiguous inputs and rule-based systems handle deterministic calculations.

## Parallel Decomposition Patterns

Not all workflows are linear. Some tasks decompose into parallel branches that execute independently and then merge.

Fan-out patterns split one input to multiple processing paths. A contract might go to separate analyzers for legal review, financial analysis, compliance checking, and risk assessment. Each analyzer specializes in one dimension. Parallelism reduces latency because analyses happen simultaneously rather than sequentially.

The merge step is critical. Parallel branches produce independent analyses that might conflict or overlap. Financial analysis flags cost concerns. Legal analysis flags liability issues. Compliance notes regulatory gaps. The merge must synthesize these independent perspectives into coherent recommendations with clear priorities.

Merge complexity grows with branch count. Two branches are easy to reconcile. Seven branches require sophisticated synthesis that understands domain priorities and can resolve contradictions. The merge step often becomes the most complex prompt in the chain because it must understand all specialized outputs.

Conditional parallelism executes different branches based on input characteristics. A support ticket might go to technical analysis if it mentions error codes, billing analysis if it mentions charges, or account analysis if it mentions login issues. Only the relevant branch executes, saving cost compared to running all branches always.

The challenge is ensuring branches are truly independent. If the legal analyzer needs insights from the financial analyzer, they are not parallel. Dependencies between parallel branches create coordination problems. You must either serialize them (losing the parallelism benefit) or duplicate information (increasing costs).

Partial failure handling is more complex with parallelism. If one of five parallel branches fails, should the merge proceed with four inputs or should the entire operation fail? The right answer depends on whether the missing branch's information is critical. Your error handling must make these tradeoffs explicit.

## Granularity Tradeoffs

Decomposition granularity exists on a spectrum from coarse-grained (few large steps) to fine-grained (many small steps). The optimal granularity depends on your specific requirements.

Coarse-grained decomposition might have three steps: input processing, business logic, output formatting. Each step is substantial, perhaps 800 to 1,500 tokens. This minimizes coordination overhead but limits optimization opportunities. If business logic and fraud detection are both in the business logic step, you cannot tune them independently.

Fine-grained decomposition might have twelve steps, each 200 to 400 tokens, each performing one narrow function. This maximizes optimization and testing opportunities but increases coordination complexity. You must manage data passing across eleven boundaries, version twelve components, and debug failures across a longer chain.

The testing tradeoff is significant. Fine-grained steps are easier to unit test because each has a narrow responsibility and clear inputs. Coarse-grained steps require more complex test fixtures because they do more. If you value testability highly, lean toward finer granularity.

The latency tradeoff matters for interactive applications. If you need results in under two seconds and each model call takes 600 milliseconds, you can afford three steps but not twelve. Coarse-grained decomposition with parallelism might be your only option for latency-sensitive workflows.

Maintenance complexity correlates with granularity. Three components are easier to understand and modify than twelve. But if those three components are each doing four unrelated things, they become harder to reason about than twelve focused components. The right granularity makes components independently comprehensible.

You can iterate granularity. Start coarse and decompose further as you identify optimization opportunities. If one coarse step has high error rates, split it into finer steps and tune them independently. Let observed failure modes guide your granularity decisions rather than committing to fine-grained decomposition upfront.

## State Management Across Chains

Some workflows require maintaining state across multiple model calls. A multi-turn conversation needs memory of previous exchanges. A long document analysis processes sections incrementally while building cumulative understanding.

Explicit state management uses data structures that flow through the chain. Each step reads state, performs operations, and writes updated state. For a conversation, state might include message history, user preferences, and context flags. For document analysis, state might include extracted entities, running summaries, and identified patterns.

State scope prevents unbounded growth. A customer service conversation maintains state within a session but resets when switching to a new issue. A document analyzer maintains state within a section but not across the entire document. Scoping rules prevent state from accumulating until it exceeds context windows.

State pruning removes obsolete information. After processing ten conversation turns, summarize the history and discard turn-by-turn details. After analyzing twelve document sections, retain high-level patterns but discard section-specific details. Pruning balances context richness against context length.

Implicit state relies on context windows to maintain information. You include relevant history in each prompt without explicit state management. For short chains, this works well. For longer chains, context windows fill up and you must decide what to keep and what to discard.

Stateless chains avoid state management by making each step self-contained. Each step receives all the information it needs as input and produces complete output. No information persists between steps except what is explicitly passed forward. This simplicity comes at the cost of token efficiency because information must be repeated in each step's input.

The state management strategy affects error recovery. Stateful systems must decide whether to preserve or reset state after errors. If step five fails, should you retry with the same state or reset to step one? If state is corrupted, how do you detect and recover? These questions have no universal answers; they depend on your domain's error tolerance and recovery requirements.

## Optimizing Individual Steps

Decomposition's key advantage is that you can optimize each step independently using techniques appropriate to that step's function.

Extraction steps benefit from abundant examples showing input variations and correct parses. You can include examples of malformed inputs, ambiguous cases, and edge cases without overwhelming a monolithic prompt. An extraction prompt might have twenty examples where a monolithic prompt could only afford five.

Classification steps benefit from clear decision criteria and boundary case handling. You can provide detailed rubrics that would clutter a multi-function prompt. A classification prompt might include a decision tree walking through ambiguous cases.

Generation steps benefit from style guides, tone examples, and audience profiles. You can include multiple example outputs showing excellent communication at different complexity levels. A generation prompt might demonstrate five variations of the same information tailored to different audiences.

Calculation steps benefit from explicit formulas, worked examples, and edge case handling. You can show step-by-step arithmetic that would be too verbose in a general prompt. A calculation prompt might walk through three example calculations with different conditions.

Model selection can vary by step. Fast, cheap models like Claude Opus 4.5 Haiku work well for simple extraction. Complex reasoning might need Claude Opus 4. By choosing models per step rather than using one model for everything, you optimize the cost-performance tradeoff across the entire chain.

Temperature can vary by step. Extraction and classification benefit from low temperature (0.2 to 0.3) for consistency. Generation might use moderate temperature (0.6 to 0.7) for natural variation. Calculation should use temperature near zero for deterministic results. Step-specific temperature optimization produces better results than one-size-fits-all settings.

## Testing Decomposed Systems

Decomposition creates natural testing boundaries that make comprehensive testing practical.

Unit tests validate individual steps with synthetic inputs covering edge cases, malformed inputs, and boundary conditions. You can generate hundreds of test cases per step because each test is focused and fast. Unit tests catch most errors during development before integration.

Integration tests verify that steps compose correctly. They ensure step one's output format matches step two's input expectations, that data types are preserved correctly, and that edge cases flow through boundaries properly. Integration tests catch interface mismatches and data transformation errors.

End-to-end tests validate complete workflows with realistic inputs. These tests are slower and more brittle but necessary to catch emergent behaviors that only appear when all steps interact. End-to-end tests use production-like data and verify business outcomes rather than intermediate artifacts.

Snapshot testing captures the output of each step for representative inputs. When you modify a step, snapshot tests show exactly how outputs changed. This makes code review meaningful because reviewers can see concrete impact rather than inferring it from code changes.

Property-based testing generates random inputs within valid ranges and verifies that outputs satisfy invariants. For a claims processor, invariants might include: patient responsibility never exceeds total charges, denial reasons always reference valid policy sections, calculated amounts always have exactly two decimal places. Property tests catch corner cases that example-based tests miss.

The decomposed architecture makes testing comprehensive rather than superficial. You cannot effectively test a 6,000-token monolithic prompt, so you write a few smoke tests and hope for the best. You can effectively test twelve 500-token steps, so you write thorough test suites that give you confidence in production behavior.

## When Decomposition is Wrong

Decomposition is not always the answer. Some tasks genuinely require holistic processing that resists decomposition.

Creative writing tasks often need unified generation. Breaking a blog post into separate prompts for introduction, body, and conclusion produces disjointed text. The natural flow of ideas, callbacks to earlier points, and thematic coherence require seeing the whole piece. Decomposition fragments voice and structure.

Highly iterative tasks with tight feedback loops between components work better as monoliths. Code generation that must maintain consistency across function definitions, type signatures, implementations, and tests might produce more coherent results in one pass. Decomposing into separate prompts for each element risks inconsistencies that are hard to repair.

Very small tasks do not benefit from decomposition overhead. If your entire workflow fits comfortably in 500 tokens with clear logic and few edge cases, keep it monolithic. Decomposition adds latency, complexity, and failure points. Apply it when benefits exceed costs.

Tasks where context sharing across functions is critical resist decomposition. If business rule application requires understanding nuances from data extraction, and generation requires understanding nuances from rule application, separating them loses critical context. The token cost of repassing context might exceed any optimization benefit.

Some domains have regulatory requirements for auditability that favor monolithic processing. If regulators require reviewing the complete reasoning chain for each decision, a single prompt producing one output is easier to audit than a twelve-step chain requiring analysis of intermediate states. The governance burden shapes architectural choices.

## Iteration Strategy for Decomposition

Most successful teams start monolithic and decompose based on observed pain points rather than architectural idealism.

The first version is a straightforward monolithic prompt that implements core functionality. You learn what the task actually requires, what edge cases exist, and what errors occur. This exploration phase builds understanding without premature commitment to a complex architecture.

The second version identifies natural boundaries based on failure patterns. If errors cluster in data extraction, that becomes a candidate for separation. If formatting problems are independent from content problems, that suggests a boundary. Let actual failures guide decomposition decisions.

The third version implements targeted decomposition for the highest-value boundaries. Split off the most problematic or most frequently changing components. Validate that decomposition improves outcomes before continuing. This incremental approach reduces risk compared to redesigning everything at once.

Later versions gradually decompose additional components as benefits become clear. You might decompose further when adding new features that fit naturally into existing boundaries. Or when optimization opportunities appear that require focused prompts. Or when testing coverage demands better isolation.

This progressive refinement is healthier than big upfront design because it grounds architectural decisions in production reality. You decompose when you have evidence that decomposition solves real problems, not when it seems theoretically appealing.

Understanding when and how to decompose prompts transforms your ability to manage complex AI workflows. The final topic in foundational architecture examines how these decomposition decisions interact with risk tier requirements and deployment context constraints.

