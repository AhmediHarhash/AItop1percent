# 1.12 — Prompt Architecture Patterns Across Risk Tiers

A legal tech company deployed document review assistants across three different practice areas in November 2024. Their marketing content review used a streamlined 300-token prompt with no guardrails. Their contract analysis used an 800-token prompt with output validation. Their medical malpractice discovery review used a 1,400-token prompt with redundant processing, confidence scoring, explanation requirements, and mandatory human review for any flagged items.

All three systems used the same base model, Claude Opus 4, but had radically different architectures driven by risk tier requirements. The marketing system needed speed and cost efficiency. The contract system balanced accuracy and throughput. The medical malpractice system prioritized correctness above all else because errors could affect litigation outcomes worth millions of dollars.

The company discovered that risk tier determines architectural patterns more than task type or domain does. Low-risk systems can use aggressive simplification. High-risk systems require defensive depth. Trying to use the same architecture across risk tiers either over-engineers simple problems or under-protects critical ones. Architecture must match consequence profile.

## Risk Tier Definitions in Production Systems

**Risk tiers** classify AI system outputs by their potential impact if incorrect. Low-risk outputs affect user experience but have minimal business or user consequences. Medium-risk outputs affect business metrics or user outcomes but have recovery mechanisms. High-risk outputs affect compliance, safety, or financial outcomes with significant consequences. Critical-risk outputs affect life safety, major financial exposure, or regulatory violations.

Low-risk examples include marketing copy suggestions, internal knowledge base search, and draft email generation. Errors are annoying but not harmful. Users can easily spot problems and discard bad outputs. The cost of an error is seconds of wasted time.

Medium-risk examples include customer support ticket routing, content moderation decisions, and basic financial calculations. Errors affect user experience or operational efficiency. Recovery requires human intervention but is straightforward. The cost of an error is degraded service quality or operational inefficiency.

High-risk examples include loan approval recommendations, legal document analysis, and medical diagnosis support. Errors can violate regulations, create liability, or cause financial harm. Recovery may be impossible after the fact. The cost of an error ranges from thousands to millions of dollars.

Critical-risk examples include autonomous vehicle decisions, medical treatment recommendations, and financial trading execution. Errors can cause injury, death, or catastrophic financial loss. Recovery is often impossible. The cost of an error is measured in lives or systemic failure.

## Low-Risk Architecture: Speed and Simplicity

Low-risk systems optimize for speed, cost, and iteration velocity. Prompt architecture is deliberately minimal because the cost of over-engineering exceeds the cost of occasional errors. Users serve as the final quality filter.

A typical low-risk prompt is 200-500 tokens with clear instructions and 2-3 examples. No guardrails beyond basic output format validation. No redundancy or confidence scoring. No audit logging beyond standard application monitoring. The model runs once and returns results directly to users.

Model selection favors faster, cheaper options. Claude Opus 4.5 Haiku or GPT-4 often suffice even when Opus-class models would produce marginally better results. The quality delta does not justify the cost and latency delta for low-consequence tasks.

Testing focuses on typical cases rather than exhaustive edge case coverage. You need confidence that the system works for 90% of inputs. The remaining 10% can fail gracefully because users will recognize failures and work around them. A/B testing and user feedback drive iterations more than pre-deployment validation.

## Medium-Risk Architecture: Validation and Monitoring

Medium-risk systems add validation layers and monitoring without the full defensive depth of high-risk systems. The architecture acknowledges that errors have real consequences but accepts that some errors are inevitable and manageable.

Prompts grow to 600-1,200 tokens with more detailed instructions, 5-8 examples, and explicit handling of ambiguous cases. Output validation checks structural correctness and basic semantic sanity. If the model should return a date, validate it is a real date. If it should classify into five categories, ensure the output matches one of them.

Confidence scoring becomes important. The model provides a confidence level with outputs. Low-confidence outputs trigger different handling: additional review, more conservative defaults, or escalation to human operators. This separates clear cases that can be automated from borderline cases that need oversight.

Monitoring tracks error rates, confidence distributions, and downstream impacts. If customer support ticket routing errors increase, you see it in metrics before it significantly degrades user experience. You can respond within hours or days rather than waiting for major incidents.

Model selection balances quality and cost. You might use Claude Opus 4 for complex cases and Claude Opus 4.5 for straightforward ones, routing based on input characteristics. This hybrid approach delivers strong performance on hard cases while controlling costs on easy ones.

## High-Risk Architecture: Redundancy and Audit

High-risk systems implement defense in depth. No single component is trusted completely. Multiple layers of checking catch errors before they reach production consequences. Every decision is auditable.

Prompts are carefully engineered with 1,000-2,000 tokens, explicit reasoning requirements, and comprehensive examples. The prompt instructs the model to show its reasoning, not just provide answers. This makes errors more detectable and outputs more auditable.

Redundant processing runs the same input through multiple prompts or models and compares results. If two independent processing paths agree, confidence is high. If they disagree, the case escalates to human review. This catches errors that individual prompts would miss.

Human-in-the-loop integration is mandatory, not optional. High-risk systems do not make final decisions autonomously. They prepare recommendations with supporting evidence. Humans review and approve. The system must make the human reviewer's job easier and safer, not simply defer all work to humans.

Audit logging captures complete context: input data, model version, prompt version, intermediate reasoning, final output, confidence scores, and human decisions. If an error causes harm, you can reconstruct exactly what happened and why. This is essential for regulatory compliance and liability management.

## Critical-Risk Architecture: Multi-Layer Safety

Critical-risk systems treat AI outputs as one signal among many in a broader safety system. The architecture assumes the AI will eventually fail and designs for graceful degradation rather than prevention of all failures.

Prompts emphasize uncertainty acknowledgment and safety margins. The system is trained to express doubt and defer to safer options when confidence is not extremely high. Instructions explicitly state that errors can have catastrophic consequences and missing information should trigger conservative behavior.

Multiple independent verification systems check outputs. These are not just redundant AI processing but fundamentally different approaches: rule-based systems, traditional algorithms, and human experts. Agreement across multiple verification methods is required before any high-consequence action.

Circuit breakers halt operation when error rates increase or unusual patterns emerge. If the system detects that its recent decisions have higher-than-normal downstream corrections or rejections, it automatically escalates all new cases to human review until the issue is diagnosed and resolved.

Regulatory compliance is embedded in architecture from the start. Systems in healthcare, financial services, or autonomous vehicles must comply with domain-specific regulations. Architecture decisions must be documentable and defensible to regulators. This often requires capabilities that seem over-engineered from a pure performance perspective.

## Guardrail Scaling Across Tiers

**Guardrails** are constraints that prevent the model from producing unacceptable outputs. Low-risk systems need minimal guardrails because unacceptable outputs are rare and non-catastrophic. High-risk systems need extensive guardrails because unacceptable outputs must be prevented at almost any cost.

Low-risk guardrails are typically format validators. Check that JSON is valid. Check that required fields are present. Check that outputs do not contain injection attacks or malicious code. These guardrails run in microseconds and catch only the most obvious failures.

Medium-risk guardrails add semantic validation. Check that extracted dates are within reasonable ranges. Check that financial calculations satisfy basic consistency constraints. Check that classifications align with provided evidence. These guardrails may call additional models or rules engines.

High-risk guardrails implement multi-stage verification. First-stage guardrails catch obvious violations. Second-stage guardrails apply domain-specific rules. Third-stage guardrails use separate models to critique outputs. Each stage has increasing sophistication and computational cost.

Critical-risk guardrails include formal verification where possible. If certain properties must hold mathematically, verify them with traditional algorithms rather than trusting model outputs. If outputs must satisfy legal or physical constraints, check those constraints explicitly before releasing outputs.

## Human-in-the-Loop Integration Patterns

Low-risk systems treat humans as end users who consume outputs and provide feedback. No approval workflow. No mandatory review. Users vote on output quality or report problems, generating data for continuous improvement.

Medium-risk systems implement sampling-based review. A random 5-10% of outputs go to human reviewers. High-risk cases detected by confidence scoring get reviewed even if not randomly selected. Reviewers can override decisions, which triggers retraining or prompt updates.

High-risk systems implement full human review before any consequential action. The AI does the analytical work and presents findings with evidence. Humans make final decisions. The interface is designed to make review efficient while ensuring humans actually engage rather than rubber-stamping.

Critical-risk systems implement double-verification. Two independent human reviewers examine AI outputs separately. Disagreements trigger escalation to senior reviewers or subject matter experts. AI outputs inform human decision-making but cannot override human judgment.

The human-in-the-loop pattern must account for automation bias—humans trusting AI outputs too much. High-risk interfaces explicitly highlight uncertainty, show alternative interpretations, and require humans to document their independent reasoning, not just approve or reject AI recommendations.

## Audit Requirements by Tier

Low-risk systems maintain standard application logs. You can see what inputs generated what outputs and when. Logs retain for days or weeks, sufficient for debugging recent issues but not for historical analysis.

Medium-risk systems add decision logging. Why did the model make this classification? What features were most important? What was the confidence level? Logs retain for months, allowing analysis of trends and systematic issues.

High-risk systems implement comprehensive audit trails. Every input, every intermediate reasoning step, every model call, every validation check, and every human decision is logged with timestamps and versions. Logs retain for years. You can reconstruct any historical decision completely.

Critical-risk systems add cryptographic integrity. Audit logs are immutable and cryptographically signed. This prevents tampering and enables legal defensibility. If a decision is challenged in court or by regulators, you can prove exactly what your system did and when.

Audit infrastructure affects architecture beyond just logging. If you need to explain every decision, your prompts must generate explanations. If you need to track intermediate reasoning, your decomposed chains must expose intermediate outputs. Audit requirements flow backward into design decisions.

## Testing Depth Across Risk Tiers

Low-risk systems use lightweight testing focused on typical cases. A few dozen test cases covering common inputs and basic edge cases. Testing primarily catches regressions during development. User reports drive discovery of new edge cases.

Medium-risk systems implement structured test suites with hundreds of cases covering known edge cases, boundary conditions, and historical failure modes. Testing includes adversarial examples designed to trigger known model weaknesses. Coverage targets 95% of expected production scenarios.

High-risk systems require exhaustive testing with thousands of cases including rare combinations and corner cases. Adversarial testing is extensive. Testing includes failure mode analysis where you deliberately try to make the system fail and document how it responds. Coverage targets 99%+ of plausible scenarios.

Critical-risk systems add formal methods where possible. For components with formal specifications, prove mathematically that they satisfy requirements. For components that cannot be formally verified, use extensive simulation and stress testing. Test under conditions more extreme than expected in production.

The testing pyramid inverts as risk increases. Low-risk systems have minimal unit tests, moderate integration tests, and rely heavily on production monitoring. Critical-risk systems have exhaustive unit tests, comprehensive integration tests, extensive end-to-end tests, and very cautious production monitoring because problems in production are unacceptable.

## Cost and Performance Tradeoffs

Low-risk systems optimize for cost per output. Use the cheapest model that produces acceptable quality. Minimize token usage. Accept lower accuracy to reduce costs. The cost of errors is lower than the cost of perfect accuracy.

Medium-risk systems balance cost and accuracy. Use better models when input complexity justifies it. Invest in confidence scoring to avoid wasting expensive human review on clear cases. The marginal cost of higher accuracy justifies itself by reducing downstream costs.

High-risk systems optimize for accuracy and auditability. Cost is secondary. Use the best available models. Implement redundant processing even though it doubles inference costs. The cost of a single error exceeds the incremental cost of comprehensive validation.

Critical-risk systems treat inference cost as negligible compared to consequence cost. If running ten different models and three human reviews prevents one catastrophic error, it is worth it. Performance bottlenecks are addressed through infrastructure scaling, not quality reduction.

This tradeoff spectrum is not about willingness to spend. It reflects that different risk tiers have fundamentally different cost structures. For low-risk tasks, inference cost dominates total cost. For critical-risk tasks, error consequences dominate total cost by orders of magnitude.

## Migrating Systems Across Risk Tiers

Systems often start in one risk tier and migrate to another as they mature or as business requirements change. A prototype customer service bot starts as low-risk because it handles simple queries with human escalation. As it handles more complex queries with less human oversight, it moves to medium-risk.

Migrating up in risk tier requires architectural additions. You cannot just add guardrails to a low-risk system and call it high-risk. You need confidence scoring, audit logging, human review workflows, and comprehensive testing that were not part of the original design.

Planned migration is cheaper than reactive migration. If you know a low-risk prototype will become a medium-risk production system, design extension points from the start. Use logging libraries that support audit requirements even if you do not enable them initially. Separate concerns so adding validation layers does not require rewriting core logic.

Migrating down in risk tier allows architectural simplification. If a high-risk system's decisions become less consequential due to business changes, you can remove expensive redundancy and validation. This is rare because systems typically move toward more responsibility over time, not less.

## Pattern Selection Framework

Choosing the right architecture starts with honest risk assessment. What is the worst realistic outcome if this output is completely wrong? How often can you tolerate that outcome? What are the regulatory requirements? What is your liability exposure?

Low-risk pattern: outputs affect only user convenience, failures are obvious to users, no regulatory requirements, no liability exposure, cost optimization is primary concern.

Medium-risk pattern: outputs affect business metrics or user outcomes, failures require effort to detect, minimal regulatory requirements, limited liability exposure, balance cost and quality.

High-risk pattern: outputs affect compliance or financial outcomes, failures create significant liability, meaningful regulatory requirements, substantial financial exposure, quality trumps cost.

Critical-risk pattern: outputs affect safety or systemic stability, failures can cause catastrophic harm, strict regulatory requirements, unlimited liability exposure, quality is non-negotiable.

The framework prevents both over-engineering and under-engineering. Low-risk tasks do not need high-risk architecture. High-risk tasks cannot use low-risk architecture regardless of how much you want to optimize costs. Architecture must match consequences.

Understanding how risk tier shapes architecture allows you to build systems that are appropriately robust for their deployment context, neither wastefully over-engineered nor dangerously under-protected. This concludes the foundation of prompt architecture patterns that enable reliable AI product development.