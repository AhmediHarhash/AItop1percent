# 8.2 — Structured Output Validation and Error Recovery

In August 2025, an insurance claims processing company discovered that 12 percent of their AI-extracted claim data failed downstream validation checks, causing claims to be routed to manual review queues. The AI was supposed to extract structured information from claim forms—dates, amounts, policy numbers, injury descriptions—and feed that data into their automated adjudication system. But when the adjudication system received claim amounts like "approximately 5000" instead of numeric values, or policy numbers with embedded spaces, or dates in inconsistent formats, it rejected the records. The company had built sophisticated extraction models but had no validation or error recovery layer. Every validation failure meant human intervention, destroying the automation value. After implementing a validation and recovery pipeline that caught and fixed common errors, their straight-through processing rate increased from 88 percent to 96 percent, and manual review costs dropped by 60,000 dollars per month.

The lesson is that generating structured output is only half the challenge. The other half is validating that output and recovering from errors when validation fails. Language models are probabilistic systems. Even with constrained generation and careful prompting, they produce outputs that don't match schemas, violate business rules, or contain semantic errors. Production systems need validation layers that catch these errors and recovery mechanisms that fix them automatically when possible or escalate intelligently when automation fails.

## The Validation Hierarchy

Validation happens at multiple levels, each catching different categories of errors.

**Syntactic validation** confirms that the output is well-formed according to its format. For JSON, this means the output can be parsed without syntax errors: balanced brackets, properly escaped strings, valid number formats, no trailing commas. For XML, this means matching opening and closing tags, proper attribute syntax, and valid entity encoding. For CSV, this means consistent column counts and properly escaped delimiters.

Syntactic validation is binary: the output either parses or it doesn't. A single misplaced character can cause total failure. This is why constrained generation (like JSON mode) is valuable—it eliminates syntactic errors at generation time. But even with constrained generation, you should still validate syntax because API failures, truncation errors, or token limit violations can produce incomplete outputs that violate syntax.

**Schema validation** confirms that parsed output matches your expected structure. You specified a schema defining required fields, types, and constraints. Schema validation verifies compliance: all required fields are present, all fields have correct types, enum values are from allowed sets, nested structures match expected shapes.

Schema validation is more granular than syntax validation. The output might be valid JSON but still fail schema validation because a required field is missing, a string appears where a number was expected, or an array is empty when it should contain at least one element. Schema validation produces detailed error reports: which fields failed, what was expected versus what was received, which constraints were violated.

**Semantic validation** confirms that values make sense in your domain. The output might be syntactically valid JSON that passes schema validation, but the values might still be wrong. A date field might contain "2025-13-45"—syntactically a string, schema-valid as a date format field, but semantically invalid because there is no 13th month or 45th day. An amount field might contain -5000—syntactically a number, schema-valid as a number type, but semantically invalid because insurance claims don't have negative amounts.

Semantic validation encodes business rules and domain constraints that schemas cannot express. It checks for impossible dates, out-of-range amounts, inconsistent field relationships, invalid reference IDs, and logically contradictory values. Semantic validation is custom code specific to your domain and use case.

**Relational validation** confirms that extracted data is consistent with external reference data. A policy number field might contain a syntactically valid ID that passes schema validation, but when you look it up in your policy database, it doesn't exist. A customer name might be spelled slightly differently than the canonical name in your CRM. Relational validation cross-references AI outputs with databases, APIs, and master data sources to catch referential inconsistencies.

You need all four levels. Syntactic validation catches corruption. Schema validation catches structure errors. Semantic validation catches nonsensical values. Relational validation catches referential mistakes. Each level provides progressively deeper quality assurance.

## Building Validation Rules

Effective validation rules are specific, actionable, and maintainable.

**Required field checks** are the simplest rules. If your schema specifies required fields, validation confirms they're present and non-empty. This catches models that skip fields when information is missing from inputs. The error message should be specific: "Missing required field: policy_number" rather than generic "Validation failed."

**Type validation** confirms field values match expected types. Numbers should be numeric, booleans should be true or false, dates should be parseable as dates, arrays should be arrays. Type validation catches models that generate "N/A" for missing numeric fields or "unknown" for missing date fields instead of using null values.

**Range constraints** check that values fall within acceptable bounds. Claim amounts should be positive and below some maximum (say, 10 million dollars). Dates should be within reasonable ranges (not in the distant past or future). Quantities should be positive integers. Range constraints catch hallucinated values that are technically valid types but logically impossible.

**Format validation** uses regex or parsing functions to verify string formats. Email addresses should match email patterns. Phone numbers should match regional formatting rules. IDs should match expected patterns (alphanumeric, specific length, checksum validation). Format validation catches models that generate plausible-looking but invalid values.

**Enum validation** confirms that fields with limited value sets only contain allowed values. If claim type must be one of "property", "casualty", "medical", "auto", validation rejects other values like "damage" or "accident" that the model might hallucinate.

**Relationship constraints** verify that fields are consistent with each other. If a claim has injury_type "none", the medical_expenses field should be zero. If coverage_type is "liability", certain fields should be present that aren't required for "collision" coverage. Relationship constraints catch internally inconsistent extractions.

Validation rules should produce structured error objects, not just boolean pass-fail. Each validation failure should report which rule failed, which field was involved, what was expected, and what was found. This error detail enables targeted error recovery.

## Error Recovery Strategies

When validation fails, you have several options beyond rejecting the output entirely.

**Automatic correction** fixes errors that follow predictable patterns. If a date field contains "08-15-2025" but your system expects ISO 8601 format, you can parse and reformat it to "2025-08-15". If a numeric field contains commas as thousands separators ("5,000"), you can strip them and parse the number. If a string field has leading or trailing whitespace, you can trim it. Automatic correction handles common format mismatches without model interaction.

The risk is over-correcting: applying transformations that change semantic meaning. If the model output "5,000" but meant "five thousand units" not "5000 dollars", automatic comma stripping could be wrong. Automatic correction should be conservative, only applying transformations that preserve meaning with high confidence.

**Field-level retry** re-requests specific fields that failed validation while keeping fields that passed. If an extraction has 10 fields and 9 are valid but the policy_number field is malformed, you don't need to regenerate the entire output. You can prompt the model specifically for the policy number: "Extract the policy number from this document. It should be a string matching the pattern XXX-YYYYYYY." This is more efficient than full retry and reduces the chance of introducing new errors in previously valid fields.

Field-level retry requires tracking which fields failed and constructing focused prompts. It works well for independent fields but is problematic for fields with interdependencies. If claim_type and coverage_details are related, retrying just one might create inconsistencies.

**Prompt clarification** retries the full extraction with an enriched prompt that addresses the specific validation failure. If the model generated a date in the wrong format, retry with "Extract dates in ISO 8601 format (YYYY-MM-DD)." If the model generated a non-enum value for claim_type, retry with "The claim_type must be exactly one of: property, casualty, medical, auto." Prompt clarification steers the model toward valid outputs by explicitly stating constraints that the original prompt left implicit.

This approach works well for novel error patterns that your initial prompt didn't anticipate. But it increases latency and cost because you're running a second inference. Use prompt clarification for high-value extractions where accuracy matters more than speed.

**Partial acceptance** uses valid fields and marks invalid fields as missing or unknown. If a claim extraction fails validation on 2 of 10 fields, you might accept the 8 valid fields and route the claim to a fast human review workflow where an operator only fills in the 2 missing fields. This is faster than full manual review and still provides automation value.

Partial acceptance requires prioritizing fields by criticality. Some fields (like claim amount) might be essential and their absence requires full manual review. Other fields (like secondary contact phone number) might be optional and their absence is acceptable. Your validation layer should distinguish critical failures from acceptable gaps.

**Fallback to heuristics** uses rule-based extraction when AI extraction fails validation. If the model can't reliably extract a claim number, you might have a regex-based fallback that searches the document for patterns matching your claim number format. Heuristics are less flexible than AI but more predictable. For highly structured fields in consistent document formats, heuristic fallback is reliable.

This requires implementing and maintaining parallel extraction logic: AI-based and rule-based. But for high-volume use cases, the reliability gain justifies the engineering cost.

**Human escalation** routes validation failures to human operators when automatic recovery fails. The escalation should include full context: the original input document, the model's attempted output, the specific validation errors, and any automatic correction attempts. This gives the human reviewer everything needed to make a quick decision.

Effective escalation systems prioritize by failure severity, estimate effort required, and route to appropriately skilled reviewers. A missing optional field might go to a low-priority queue. A completely malformed extraction might go to a specialist queue. A borderline case might get rapid review by an experienced operator.

## Validation Failure Analysis

When validation fails, the failure itself is data you can learn from.

**Error pattern tracking** aggregates validation failures by type, field, and input characteristics. If 80 percent of date field failures happen on handwritten forms, you know where to focus improvement efforts. If policy_number extraction fails more often for a specific policy series, you might need targeted examples in your prompt. Error pattern tracking turns failures into actionable improvement signals.

You should track not just what failed but why it failed: was it missing, wrong type, wrong format, semantically invalid, or relationally inconsistent. Different failure modes point to different solutions. Missing fields might need stronger prompts about handling absent information. Wrong formats might need explicit format specifications in prompts or automatic format conversion in validation.

**Model comparison** runs the same inputs through different models and compares validation failure rates. Maybe GPT-4 has a 5 percent failure rate on your task but Claude has 2 percent. Maybe GPT-3.5 is cheaper but has 15 percent failures, making it more expensive overall when you account for retry costs. Model comparison data informs model selection decisions.

Different models fail on different inputs. GPT-4 might struggle with handwritten forms but excel at typed documents. Claude might handle complex nested structures better but struggle with inconsistent formatting. Model comparison helps you route different input types to different models based on their strengths.

**Prompt iteration based on failures** uses validation errors to improve prompts. If validation reveals the model frequently generates dates in MM-DD-YYYY format when you need YYYY-MM-DD, add explicit format instructions. If the model often hallucinates claim types not in your enum, add the enum list to the prompt with explicit instructions to only use those values. Each validation failure is feedback that helps you write better prompts.

This creates a continuous improvement loop: deploy, collect validation failures, analyze patterns, update prompts, redeploy, measure improvement. Over weeks and months, your validation failure rate drops as your prompts become more precise and your error recovery becomes more robust.

**Schema refinement** adjusts schemas based on validation failures. If a field that you marked required is frequently missing from input documents, maybe it should be optional with a default value. If a field that you thought was a simple string actually needs to be a complex object to capture the information correctly, update the schema. Validation failures reveal mismatches between your schema design and real-world data.

Schema evolution should be versioned and backward-compatible. If you have existing data extracted with schema v1, you need migration logic when moving to schema v2. If you have downstream systems expecting schema v1, you need compatibility layers or coordinated updates.

## Validation Performance Optimization

Validation adds latency and cost. Optimizing validation reduces both without sacrificing quality.

**Early exit validation** checks critical constraints first and fails fast. If policy_number is required and missing, you don't need to validate 20 other fields—fail immediately and retry. If claim_amount is out-of-range by orders of magnitude, the entire extraction is suspect—fail immediately and escalate. Early exit reduces wasted validation effort on outputs that are clearly wrong.

Ordering validation rules by cost and selectivity improves efficiency. Cheap checks (field presence, type checking) run first. Expensive checks (database lookups, complex semantic validation) run only if cheap checks pass. This minimizes unnecessary work.

**Caching validation results** for deterministic rules avoids redundant checks. If you're validating 1000 claims from the same policy and the policy_number validation involves a database lookup, cache the lookup result. All claims from that policy can reuse the cached validation. This is especially valuable for relational validation where external lookups are slow.

**Batching validations** amortizes costs across multiple outputs. If you're validating 100 extracted claims, batch database lookups for all policy numbers in one query rather than 100 individual queries. Batch API calls to external validation services. Batching reduces overhead and improves throughput.

**Async validation** for non-critical checks allows you to return results faster while continuing validation in the background. If most fields pass critical validation but you want to do expensive relational validation, return the output immediately with a "validation in progress" flag and update its status asynchronously. This improves user-perceived latency while maintaining quality.

Async validation requires handling eventual consistency: systems that use the output need to tolerate asynchronous validation failures that are discovered later. This works for some use cases (analytics, background processing) but not others (real-time decisions, transactions).

## Building Validation Test Suites

Production validation logic needs comprehensive testing to avoid false positives (blocking valid outputs) and false negatives (accepting invalid outputs).

**Positive cases** test that valid outputs pass validation. Collect or generate examples of correctly structured, semantically valid outputs and verify they pass all validation rules. This catches overly strict validation that rejects legitimate data.

**Negative cases** test that invalid outputs fail validation with appropriate error messages. For each validation rule, create outputs that violate that rule and verify the rule catches the violation with a clear error message. This confirms your validation actually works.

**Boundary cases** test values at the edge of acceptable ranges. If claim amounts must be between 0 and 10 million, test 0, 10 million, -1, 10000001. If dates must be between 2020 and 2030, test those exact years and just outside. Boundary testing catches off-by-one errors and clarifies whether boundaries are inclusive or exclusive.

**Real-world failure cases** from production incidents become permanent test cases. Every validation failure that reached production should be added to your test suite to prevent regression. This builds a test set that reflects actual failure modes rather than theoretical ones.

**Mutation testing** verifies validation rules aren't too permissive. Take valid outputs and introduce small errors (change types, remove fields, violate constraints). If your validation doesn't catch the mutations, your rules are too weak. This helps you identify gaps in validation coverage.

## Validation as a Feedback Signal

Validation failures aren't just errors to fix—they're signals about model performance, prompt quality, and input data characteristics.

**Validation metrics** should be tracked alongside other model metrics. What's the validation failure rate overall. What's the failure rate by field, by validation rule type, by input source. How does the failure rate change over time. How does it vary across models. These metrics tell you where your system is fragile and where it's robust.

**Validation failures as training data** can improve models through fine-tuning or few-shot learning. If you have validation failures that human operators corrected, those corrections are labeled examples of correct extraction from inputs where the model failed. This data is especially valuable because it represents exactly the cases where the model struggles.

**Validation as quality gates** determines when outputs are reliable enough for full automation versus requiring human review. High-confidence outputs that pass all validation can go straight to production. Low-confidence outputs or outputs with validation warnings go to human review. This creates a two-tier processing system that optimizes for both speed and accuracy.

The threshold between automatic and human review should be tuned based on the cost of errors versus the cost of human review. High-stakes decisions (medical, financial, legal) need stricter thresholds. Low-stakes applications (search ranking, content recommendations) can tolerate higher validation failure rates.

## The Complete Validation Stack

A production-grade validation system combines multiple components into a unified pipeline.

**Validation engine** that applies syntactic, schema, semantic, and relational rules in sequence, collecting all failures rather than stopping at the first one. This gives you complete visibility into everything wrong with an output.

**Error recovery orchestrator** that attempts automatic fixes, field-level retries, and prompt clarification in order of cost and likelihood of success. Simple fixes happen instantly, expensive retries happen only when necessary.

**Failure tracking** that logs every validation failure with full context: the input, the output, the validation errors, the recovery attempts, the final outcome. This data feeds into analytics and continuous improvement.

**Human review interface** for escalated failures that presents operators with context, suggests corrections based on similar past cases, and captures operator decisions as training data for future improvement.

**Monitoring and alerting** that tracks validation failure rates in real-time and alerts when rates spike, when new error patterns emerge, or when specific validation rules consistently fail.

Each component is essential. Validation without recovery wastes the partial value in failed outputs. Recovery without tracking prevents learning from failures. Tracking without alerting means problems fester until they become crises. Building structured output systems means building the complete validation and recovery stack, not just the generation layer.

The next section explores streaming structured output: how to progressively render partial structures as they're generated, handle incomplete data mid-stream, and provide responsive user experiences while maintaining structured output guarantees.
