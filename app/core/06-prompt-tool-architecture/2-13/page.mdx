# 2.13 — Prompt Patterns for Classification, Extraction, and Generation

An e-commerce company built three AI features in September 2025: product categorization, attribute extraction from descriptions, and marketing copy generation. They used the same basic prompt structure for all three tasks, figuring that clear instructions and good examples would work universally. The classification feature worked well. Extraction was acceptable. Generation was a disaster, producing bland, generic copy that their marketing team rejected 68% of the time.

The engineering lead couldn't understand the inconsistency. They had followed the same prompt engineering principles across all three features. Clear task description, relevant examples, output format specifications. Why did the same approach produce such different outcomes. The team spent two weeks tweaking the generation prompt, adding more examples, refining instructions, trying different temperature settings. Improvements were marginal.

The problem was fundamental. Different task types require different prompt architectures. **Classification prompts** need clear category boundaries and disambiguation logic. **Extraction prompts** need pattern recognition and structured output handling. **Generation prompts** need creative constraints and quality criteria. The team was trying to use a classification prompt pattern for a generation task. No amount of tweaking would bridge that architectural mismatch.

## Classification Prompt Architecture

Classification tasks require prompts that make category boundaries explicit and provide disambiguation strategies for edge cases. Your prompt needs to define not just what each category is but how to distinguish between similar categories. For a support ticket classifier, defining "Billing" as "questions about charges" is insufficient. You need to specify how to distinguish billing questions from refund requests, payment method changes, and pricing inquiries if those are separate categories.

Start classification prompts with a clear category enumeration. List every valid category with a brief but distinctive description. Use parallel structure: each category gets the same type of description. If you describe one category with example inputs and another with abstract criteria, the model has inconsistent guidance. Consistency in category definitions produces consistency in classification decisions.

Include boundary examples that show how to handle ambiguous cases. A customer message saying "I was charged twice, I want my money back" straddles the billing-refund boundary. Your prompt should include examples of exactly this kind of ambiguity with clear classifications and brief justifications. These examples teach the model your disambiguation logic, not just your categories.

Specify a default category or rejection mechanism for inputs that don't clearly fit anywhere. In multi-class classification, you might have an "Other" or "General" category. In binary classification, you need to specify whether edge cases should lean positive or negative. Don't leave ambiguous cases undefined—the model will make inconsistent choices without guidance.

Order your category definitions strategically. If you have a catch-all category like "Other," define it last after specific categories. This ordering encourages the model to check specific categories first before defaulting. If certain categories are frequently confused, define them adjacent to each other and explicitly contrast them.

Category descriptions should emphasize distinguishing features, not comprehensive definitions. You don't need to describe everything a technical support inquiry might contain. You need to describe what makes it technical support rather than billing or general questions. Focus on the boundaries between categories, not the centers.

## Extraction Prompt Patterns

Extraction prompts need to specify what to extract, where to find it, and how to handle missing or ambiguous information. Unlike classification, where every input gets exactly one label, extraction produces variable-length output. Your prompt architecture must account for zero matches, multiple matches, and partial matches.

Define extraction targets with both abstract criteria and concrete examples. If you're extracting company names from news articles, specify "proper nouns referring to business entities, including corporations, partnerships, and LLCs." Then show examples: "Apple Inc.", "Acme Corp", "Smith & Associates". The abstract definition provides the rule, the examples anchor what the rule means in practice.

Specify the extraction granularity you need. Should the model extract "Google" or "Google Cloud Platform" or "Google Cloud Platform's BigQuery data warehouse"? All three are technically correct for different granularities. If you need entity-level extraction, say so explicitly. If you need fine-grained span extraction, provide examples at that level. Mismatched granularity is a common source of extraction errors.

Handle multi-instance extraction carefully. If a document contains five company names, your prompt needs to specify how to return all five. Structured output formats like JSON arrays work well here. Show an example input with multiple extraction targets and the complete structured output. Don't assume the model will infer the pattern from single-instance examples.

Specify extraction boundaries precisely. When extracting addresses, do you want just the street address, or street plus city and state, or the complete address including zip code? When extracting dates, do you want just the date or date plus time? Boundary ambiguity causes extraction inconsistency. Make boundaries explicit in your prompt and examples.

Address overlapping extractions explicitly. If you're extracting both organizations and people, and the text says "Apple CEO Tim Cook," should you extract "Apple" as organization and "Tim Cook" as person, or should you extract the full span "Apple CEO Tim Cook" as a relationship? Your prompt must specify how to handle these compositional structures.

## Structured Output for Extraction

Extraction tasks benefit enormously from structured output formats. Instead of asking for free-form text responses, specify JSON, XML, or another parseable format. This makes downstream processing reliable and catches malformed responses early. Claude Opus 4.5 and GPT-5 both handle structured output well when you specify the schema clearly.

Provide a complete output schema before any examples. Show the JSON structure with field names, types, and optional vs required fields. Then show examples that populate this schema. This ordering—schema first, examples second—helps the model understand the structure as a constraint that examples demonstrate rather than a pattern to infer from examples.

Include an example of empty results. When no extraction targets are found in the input, what should the output look like? An empty JSON array, a null value, or an explicit "none found" marker? Specify this clearly and show an example. Models often stumble on empty cases because they're rare in training data and ambiguous in prompts.

Specify how to handle extraction uncertainty. If the model is 90% confident a span is a company name but not certain, should it include the extraction with a confidence score, omit it, or flag it specially? Your prompt needs to make this decision explicit. Confidence thresholds and uncertainty handling are architectural choices, not implementation details the model should guess.

Structured output formats also enable validation. You can specify constraints like "dates must be in ISO 8601 format" or "amounts must be positive numbers" directly in the schema. The model can validate its own output against these constraints before returning results. This reduces downstream parsing errors.

For complex extractions, nested structures beat flat lists. If you're extracting invoices with line items, a nested structure where each invoice contains an array of line items is more maintainable than a flat list of invoice ID and line item pairs. Design your schema to match the natural structure of what you're extracting.

## Generation Prompt Foundations

Generation tasks need prompts that specify not just what to create but what good looks like. Unlike classification and extraction, where correctness is often objective, generation quality is multidimensional and subjective. Your prompt must make your quality criteria explicit and measurable.

Start with audience and purpose. "Generate marketing copy" is underspecified. "Generate product description for budget-conscious shoppers emphasizing value and practical benefits" gives the model a target. Specify who will read the output and what outcome you want. This context shapes tone, vocabulary, detail level, and emphasis.

Provide quality criteria as specific constraints. Instead of "make it engaging," specify "include a question in the first sentence, use concrete numbers rather than vague claims, and end with a clear call to action." Instead of "keep it concise," specify "maximum 150 words, three paragraphs." Vague quality terms are interpreted inconsistently. Specific constraints produce consistent output.

Show multiple examples that demonstrate variety within your quality criteria. If all your examples use the same structure or tone, the model will imitate that structure too closely. Show different valid approaches—different openings, different emphasis, different structural patterns—all meeting your quality criteria. This teaches the model the boundaries of acceptable variation.

Specify what to avoid as explicitly as what to include. "Do not make claims we cannot verify" is actionable. "Do not use superlatives without supporting evidence" prevents a specific quality failure. "Avoid cliches like 'game-changer' or 'revolutionary'" gives the model concrete negative examples. Negative constraints are as valuable as positive ones for generation.

Generation prompts benefit from persona framing when appropriate. "Write as a knowledgeable but approachable expert explaining to a curious non-specialist" sets tone and style expectations. But avoid vague persona instructions like "be professional"—specify what professional means in your context with concrete examples.

## Task-Specific Example Selection

Classification examples should emphasize category boundaries and edge cases. Don't just show clear, prototypical examples. Show ambiguous cases that could reasonably belong to multiple categories, and demonstrate your disambiguation logic. Show near-misses where something almost fits a category but belongs elsewhere due to a subtle distinction.

Extraction examples should show structural variety in the input and completeness in the output. If you're extracting addresses from forms, show addresses in different formats: single line, multiple lines, with or without apartment numbers, with country codes or without. For each input variation, show complete extraction. This teaches the model to handle input diversity.

Generation examples should span your quality spectrum. Don't show only perfect examples—show acceptable examples at different points in your quality range. If you accept output from terse to elaborate, show both extremes and the middle ground. This calibrates the model's understanding of acceptable variation and prevents it from overfitting to one style.

Keep classification examples brief and numerous. You want coverage of categories and boundaries, not deep illustration of each case. Five to eight examples usually suffice for classification tasks with 3-7 categories. Extraction benefits from fewer, more detailed examples. Three to five examples that show different extraction scenarios work better than ten examples that show the same pattern. Generation needs the fewest but most substantial examples. Two to three high-quality examples that demonstrate different approaches are often sufficient.

Example placement matters. For classification, examples work well after category definitions. For extraction, examples work well after schema specification. For generation, examples work well after quality criteria. This ordering lets each section build on previous context.

Annotate examples when disambiguation logic is complex. For a classification example at a category boundary, include a brief note: "Classified as BILLING despite mentioning refund because primary intent is understanding charges." This annotation makes your reasoning transparent and teachable.

## Output Format Specifications

Classification output format is usually simple: return the category label or ID. Specify the exact format: "Return only the category name in uppercase" or "Return the category ID as an integer." Specify what to return if no category fits. Make the output format machine-parseable and unambiguous.

Extraction output format should separate extracted content from metadata. Don't just return extracted text—return it in a structure that includes position information, extraction type, and optionally confidence scores. JSON works well: each extraction gets an object with text, start_position, end_position, and entity_type fields. This format supports downstream processing and validation.

Generation output format varies by use case. For structured generation like product descriptions with specific sections, use headings or JSON keys to delimit sections. For free-form generation like creative writing, simple text output is fine. For generation that fills templates, specify the template structure and how the model's output should populate it.

Always specify trailing matter: should the model include explanations or confidence notes after the primary output, or return only the requested content? By default, models often add commentary. If you want clean output only, state: "Return only the classification label with no explanation or commentary." If you want explanations, specify their format and placement.

Format specifications should account for downstream parsing. If your output feeds into a pipeline that expects strict JSON, specify exact JSON formatting including quote types and whitespace handling. If humans will read the output directly, optimize for readability over machine parseability.

For multi-field outputs, specify field ordering and required vs optional fields. An extraction that returns person name, title, and company should specify whether all three fields are required or whether any can be null/empty if not found. Ambiguity in field requirements causes parsing failures downstream.

## Multi-Task Prompts and Task Switching

Some applications need a single prompt that handles multiple task types based on context. A customer service AI might classify the inquiry, extract key details, and generate a response all in one interaction. These multi-task prompts are complex to structure but powerful when done well.

Sequence tasks explicitly rather than leaving the model to infer order. "First, classify this inquiry into one of these categories: [categories]. Second, extract any relevant account numbers, order IDs, or dates. Third, generate a response using the classification and extracted information." Clear sequencing prevents the model from skipping tasks or conflating them.

Use structured output that separates task results. JSON with top-level keys for each task works well: classification, extracted_entities, and generated_response as separate fields. This structure makes it clear what outputs belong to which task and makes parsing reliable. Avoid free-form output where task results blur together.

Specify dependencies between tasks. If the response generation should use the extracted entities, say so explicitly: "Generate the response using the account number and order ID you extracted." If classification should inform extraction focus, specify that too: "For billing inquiries, extract payment amounts and dates. For technical inquiries, extract error messages and product names."

Multi-task prompts are longer than single-task prompts. Budget accordingly. The token overhead of combining tasks is usually less than the overhead of three separate API calls, but you still need to justify the complexity. Use multi-task prompts when tasks are tightly coupled and need to share context. Use separate prompts when tasks are independent.

Error handling in multi-task prompts requires careful specification. If classification fails, should the system still attempt extraction and generation, or should it halt? Your prompt must specify failure modes and fallback logic for each task stage.

## Task-Specific Temperature and Sampling

Classification tasks work best at low temperatures, typically 0.0 to 0.3. You want deterministic, consistent category assignment. High temperature introduces randomness that makes identical inputs produce different classifications, which is almost never desired for classification systems.

Extraction also favors low temperatures in the 0.0 to 0.4 range. Extraction should be consistent and deterministic. The same document should yield the same extractions on repeated runs. Higher temperatures can cause the model to hallucinate entities or extract different spans on successive attempts.

Generation benefits from moderate to high temperatures depending on the application. For creative content like marketing copy or storytelling, temperatures from 0.7 to 1.0 produce variety and creativity. For factual generation like report summaries or technical documentation, use lower temperatures (0.3 to 0.6) to maintain accuracy while allowing natural variation in phrasing.

Test temperature sensitivity for your specific task and domain. Some generation tasks produce acceptable variety even at temperature 0.3. Others feel wooden and repetitive below 0.8. Your validation set should include temperature as a parameter you sweep during development. Find the temperature that balances consistency and quality for your use case.

Temperature interacts with prompt structure. More constrained prompts (with specific format requirements and detailed examples) work well at higher temperatures because constraints counterbalance randomness. Less constrained prompts need lower temperatures to prevent output from wandering off-specification.

For multi-task prompts, you may need to compromise on temperature. If you're combining classification (wants low temp) with generation (wants higher temp), test different values and measure which matters more for overall quality. Often you'll find a middle ground like 0.4-0.5 works acceptably for both.

## Hybrid Tasks and Prompt Composition

Some applications blur task boundaries. Sentiment classification with explanation is part classification, part generation. Entity extraction with normalization is part extraction, part transformation. These hybrid tasks need prompt patterns that combine approaches from multiple task types.

For classification with explanation, structure the prompt as a classification task first, then add a generation component: "Classify the sentiment, then explain your reasoning in one sentence." This two-stage structure gives the model clear subtasks. Specify the output format to separate classification from explanation: use JSON with sentiment and explanation fields.

For extraction with transformation, specify the extraction task, then the transformation rules: "Extract all dates mentioned in the text. Convert each date to ISO 8601 format (YYYY-MM-DD)." Show examples of both the extraction and the transformation. The model needs to understand both components and how they connect.

Watch for task interference in hybrid prompts. Sometimes generation components cause the model to be less precise in classification or extraction. If adding explanation degrades classification accuracy, you may need to separate the tasks into sequential API calls rather than combining them in one prompt. Measure task accuracy for hybrid prompts carefully.

Hybrid prompts often benefit from staged processing within a single prompt. "First extract all dollar amounts. Then classify each amount as revenue, expense, or investment. Finally, sum amounts by category." Each stage has clear inputs and outputs. Staging reduces interference between task types.

Validation becomes complex with hybrid tasks. You need to validate each task component separately and validate that components compose correctly. Your test suite should check classification accuracy, extraction completeness, transformation correctness, and overall output quality. Don't rely on end-to-end testing alone.

## Domain Adaptation for Task Types

Classification in specialized domains requires category definitions that use domain vocabulary. A medical symptom classifier needs to define categories using clinical terminology, not lay descriptions. Don't dumb down your categories—use precise domain language and provide examples that reflect how domain experts think about the categories.

Extraction in technical domains needs to specify technical patterns. Extracting software version numbers requires different patterns than extracting person names. Extracting legal citations requires different patterns than extracting URLs. Your extraction prompt should acknowledge domain-specific patterns and show examples that reflect them.

Generation in specialized domains needs domain-appropriate quality criteria. Marketing copy has different quality dimensions than legal disclaimers or technical documentation. Don't use generic generation prompts across domains. Build domain-specific generation prompts that reflect what quality means in that domain, with examples from domain experts.

Test your domain-adapted prompts with domain practitioners, not just engineers. A prompt that makes sense to you might miss critical domain nuances that experts would catch immediately. Domain review of prompts is as important as code review, especially for regulated industries or high-stakes applications.

Domain-specific prompts often require domain-specific validation metrics. Medical classification might track error rates by severity of misclassification. Legal extraction might measure completeness of citation elements. Financial generation might validate adherence to disclosure requirements. Build evaluation metrics that reflect domain priorities.

## Error Handling for Each Task Type

Classification errors fall into clear categories: wrong category selected, no category selected, or malformed output. Your prompt should specify fallback behavior: if the model can't determine a category, should it choose a default, return an error, or defer to human review? Make this decision in the prompt, not in post-processing code.

Extraction errors include missed entities, hallucinated entities, and incorrect span boundaries. Specify how the model should handle uncertainty: skip uncertain extractions or flag them with confidence scores? Specify the confidence threshold if using scores. For span boundaries, show examples that make clear whether to extract minimal spans or maximal spans when boundaries are ambiguous.

Generation errors are harder to define because output quality is multidimensional. Specify unacceptable output explicitly: "Never generate responses that promise specific timelines or make commitments outside company policy." This negative specification helps the model avoid failure modes even if it doesn't guarantee high-quality output.

Build validation into your prompts when possible. For classification, you can ask the model to return both the category and a confidence score, then filter low-confidence classifications for human review. For extraction, you can ask the model to validate that extracted spans appear verbatim in the input. For generation, validation is harder, but you can ask the model to check its output against specific criteria as a final step.

Error recovery strategies differ by task. For classification, you might have a secondary classifier that handles uncertain cases. For extraction, you might have a verification step that validates extracted entities against a knowledge base. For generation, you might have a quality filter that rejects output below thresholds and retries. Build these recovery strategies into your system architecture, informed by prompt-level error handling.

## Testing Task-Specific Prompts

Your test suite should reflect task-specific success criteria. For classification, measure accuracy, precision, recall, and confusion between categories. For extraction, measure both entity-level accuracy (did you find the right entities) and span-level accuracy (did you extract the right boundaries). For generation, measure against task-specific quality rubrics.

Build task-specific test fixtures. Classification tests need balanced category representation and edge cases at category boundaries. Extraction tests need inputs with varying entity density, from zero entities to high-density documents. Generation tests need quality rubrics and ideally human evaluation for a sample of outputs.

Automate what you can, but recognize when human judgment is necessary. Classification and extraction are largely automatable—you can calculate metrics from ground truth. Generation quality often requires human assessment, at least for a sample. Build tooling that makes human evaluation efficient, like side-by-side comparisons or quality scoring interfaces.

Track task-specific failure modes over time. For classification, which category pairs get confused most often? For extraction, what entity types have the lowest recall? For generation, what quality criteria fail most frequently? These patterns inform prompt refinement priorities and help you focus optimization efforts where they'll have the most impact.

Regression testing is critical for all task types. When you modify a prompt, test that you haven't broken existing functionality. For classification, verify category-level accuracy holds for all categories. For extraction, verify entity type coverage is maintained. For generation, verify quality dimensions don't regress. Automated regression tests catch unintended consequences of prompt changes.

## Cross-Task Patterns and Reusable Components

While classification, extraction, and generation require different prompt architectures, some components are reusable across task types. Output format specifications often follow similar patterns whether you're formatting classification results, extraction results, or generated content. Instruction phrasing conventions can be consistent across prompts.

Build a prompt component library for your organization. Reusable components might include standard output format templates, common instruction patterns, domain-specific terminology definitions, and error handling specifications. When building a new prompt, start with relevant components and customize rather than writing from scratch.

Document why specific patterns work for specific task types. Your team should understand that classification prompts emphasize category boundaries because that's where classification fails. Extraction prompts emphasize structured output because that's what makes extraction results usable. Generation prompts emphasize quality criteria because that's what separates good generation from bad. This understanding prevents copy-paste errors across task types.

Version control your prompt component library alongside your prompts. When you improve a component, you can propagate that improvement across all prompts using that component. When a component proves problematic, you can identify all affected prompts. Treat components as reusable code with the same discipline as application code.

Balance reuse with customization. Components provide starting points, not final solutions. Every prompt needs task-specific adaptation. Don't force a component to fit where it doesn't belong. Recognize when a task is different enough to need custom prompt architecture rather than component assembly.

Now that you understand task-specific prompt patterns, you need to recognize common prompt anti-patterns and know how to refactor them.
