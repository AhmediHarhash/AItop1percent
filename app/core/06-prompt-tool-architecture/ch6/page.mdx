# Chapter 6 — Prompt Security and Safety

Prompt injection is real, evolving, and underestimated. Jailbreaks leak system prompts, indirect injection poisons retrieval, and adversarial inputs exfiltrate credentials. Security is not a post-launch audit — it's architectural, testable, and layered.

---

## What This Chapter Covers

- **6.1** — Prompt Injection: Attack Taxonomy and Defense Layers
- **6.2** — Indirect Prompt Injection via External Content
- **6.3** — System Prompt Exfiltration and Leakage Prevention
- **6.4** — Jailbreak Patterns and Evolving Attack Surfaces
- **6.5** — Input Sanitization and Pre-Processing Defenses
- **6.6** — Output Filtering and Post-Processing Safety Layers
- **6.7** — Defensive Prompt Design: Writing Injection-Resistant Prompts
- **6.8** — Guardrails Architecture: Constitutional AI, Classifiers, and Rule Engines
- **6.9** — Red-Teaming Your Prompts: Process and Tooling
- **6.10** — PII and Sensitive Data in Prompts: Detection and Redaction
- **6.11** — Prompt Audit Trails and Forensics
- **6.12** — Security Testing Automation for Prompt Pipelines
- **6.13** — Credential Exfiltration Scenarios and Defenses

---

*Assume every input is adversarial until proven otherwise.*
