# 5.12 â€” Answerability Detection: Deciding When the System Does Not Know

A healthcare AI company settled a class-action lawsuit for 4.7 million dollars because their clinical decision support system generated treatment recommendations when critical patient information was missing. Eleven percent of cases had incomplete allergy records, pending test results, or missing medication histories. The system detected none of these gaps. It generated plausible recommendations from partial context anyway. Three recommendations conflicted with undocumented allergies. Physicians caught the errors before harm occurred, but the legal claim was negligent design for lack of answerability detection. Answerability is the measurable mechanism that implements abstention: given retrieved context and a query, can the system reliably answer, or should it refuse? Detection is not philosophical; it is a binary classification problem with thresholds, logic, and audit trails.

You are building a RAG system right now, and you need to solve the answerability problem. Answerability is the binary question: given the retrieved context and the user query, can the system reliably answer the question, or should it abstain? This is not a subjective philosophical question about the nature of knowledge. This is a measurable classification problem. You need a mechanism that evaluates the match between question requirements and available context, assigns a confidence score, and makes a yes-or-no decision about whether to generate an answer or to abstain. Answerability detection is the measurable implementation of the abstention concept. It is the difference between a system that fails safely by refusing to answer when information is insufficient and a system that fails dangerously by generating plausible-sounding hallucinations.

## The Measurable Method for Abstention

Abstention, as discussed in an earlier chapter, is the high-level design principle that your system should refuse to answer when confidence is low or context is insufficient. Answerability detection is the technical mechanism that implements abstention. It is the code, the model, the thresholds, and the logic that decide whether a specific query with specific retrieved context is answerable.

You implement answerability detection as a classification layer that sits between retrieval and generation. After retrieval completes and before generation starts, you evaluate answerability. You take the user query and the retrieved context as input. You compute a set of signals: retrieval scores, context relevance scores, query-context matching scores, coverage estimates, and model uncertainty estimates. You combine these signals into an answerability score, a single number between 0 and 1 that represents the system's confidence that it can answer the question reliably. You compare this score against a threshold. If the score is above the threshold, you proceed to generation. If the score is below the threshold, you abstain and return a non-answer response.

This is a measurable process. You can evaluate the answerability classifier on a labeled dataset where each query-context pair is labeled as answerable or not answerable based on whether a correct answer is possible given the context. You can measure precision: what percentage of queries classified as answerable actually produce correct answers? You can measure recall: what percentage of queries that should be answerable are classified as answerable? You can measure the false-answer rate: what percentage of generated answers are incorrect? You can measure the correct-abstention rate: what percentage of abstentions correctly avoided generating an incorrect answer?

The goal is to maximize correct answers while minimizing false answers. Abstention is the mechanism for managing this tradeoff. By refusing to answer when answerability is low, you reduce false answers at the cost of reducing coverage. The answerability threshold determines where on this tradeoff curve you operate. A high threshold means you only answer when very confident, which gives high precision but low coverage. A low threshold means you answer more often, giving high coverage but lower precision. You tune the threshold based on your risk tolerance.

## Binary Classification: Answerable Versus Not Answerable

At its core, answerability detection is a binary classification problem. For each query-context pair, you classify it as answerable or not answerable. Answerable means that a correct, grounded answer can be generated from the retrieved context. Not answerable means that the context is insufficient, irrelevant, ambiguous, or contradictory, such that generating an answer would likely result in hallucination or error.

The ground truth label for this classification is defined by whether a human, given the same context, could answer the question correctly. If a human expert reads the retrieved context and can confidently answer the user's question with information directly stated or clearly implied in the context, the query-context pair is answerable. If the human cannot answer the question because the information is not in the context, is ambiguous, is contradictory, or requires external knowledge not present in the context, the pair is not answerable.

You build a training set by having human annotators label query-context pairs. You sample queries from your production logs or create synthetic queries. For each query, you run retrieval and collect the top-K retrieved chunks. You show the query and the chunks to annotators and ask: "Can this question be answered correctly based on these retrieved documents?" The annotator labels it yes or no. If yes, they ideally also provide the correct answer or indicate which parts of the context support the answer. This gives you ground truth labels for training and evaluating an answerability classifier.

The classifier itself can take several forms. The simplest is a rule-based classifier using thresholds on retrieval scores. If the top retrieval score is below a threshold, classify as not answerable. This is easy to implement but crude. It does not account for context relevance or query-context semantic matching beyond the retrieval score. A more sophisticated approach is to train a machine learning classifier that takes features like retrieval scores, context relevance scores, query length, context length, lexical overlap, semantic similarity, and outputs a probability that the pair is answerable. You can use logistic regression, gradient boosting, or a neural model.

The most sophisticated approach is to use a language model as the answerability classifier. You prompt the model with the query and the retrieved context and ask: "Can this question be answered based on the provided context? Answer yes or no." The model outputs yes or no, possibly with a confidence score. You can fine-tune a small model specifically for this task, which gives good performance with low latency. Or you can use few-shot prompting with a larger general-purpose model, which is easier to implement but slower.

## Confidence Scoring Approaches

Answerability is not truly binary. There is a spectrum from "definitely answerable with high-quality context" to "definitely not answerable due to missing information." Between these extremes are many cases where answerability is uncertain. A good answerability detection system outputs a confidence score, not just a binary label. The confidence score allows you to set thresholds flexibly and to handle uncertain cases with appropriate strategies like hedging or requesting clarification.

Confidence scoring can be based on retrieval scores. The simplest approach is to use the top retrieval score as the answerability confidence. If your retrieval system outputs cosine similarity scores between 0 and 1, you can interpret the top score directly as confidence. If the top score is 0.9, you are highly confident the retrieved chunk is relevant and the query is answerable. If the top score is 0.5, confidence is low. You can refine this by using multiple retrieval scores: the mean of the top-K scores, the minimum of the top-K scores, or the gap between the top score and the second score. A large gap indicates a single highly relevant document, which is often a strong signal of answerability.

You can also base confidence on context relevance scores. If you have a relevance classifier that scores how well the retrieved context matches the query intent, you use that score as answerability confidence. A relevance score of 0.85 means high confidence that the context is relevant and the query is answerable. A score of 0.4 means low confidence. The advantage of relevance scores over raw retrieval scores is that relevance models are trained specifically to assess whether context can answer a query, which is exactly the answerability question.

Another source of confidence is semantic coverage. You analyze the query to identify the key information requirements: entities, attributes, relationships, facts that need to be present in the answer. You then analyze the retrieved context to determine what percentage of these requirements are covered. If all requirements are covered, confidence is high. If only 40% are covered, confidence is low. This coverage-based approach is more interpretable than black-box similarity scores, because you can explain exactly what is missing when you abstain.

You can also derive confidence from the language model itself. If your model outputs token probabilities, you can measure the model's uncertainty in its own generation. Low average token probability indicates the model is uncertain, which correlates with low answerability. High perplexity in the generated text is another signal of uncertainty. Some models support explicit uncertainty estimation, outputting a confidence score along with the generated answer. You can use this as an answerability signal.

The most robust approach is to combine multiple signals. You compute retrieval scores, relevance scores, coverage scores, and model uncertainty scores, and you train a classifier or use a weighted combination to produce a final answerability confidence score. This multi-signal approach is more robust because different signals capture different aspects of answerability, and their combination reduces the risk of false positives and false negatives.

## Using Retrieval Scores, Context Relevance, and Model Uncertainty

Let us make this concrete. You have a query: "What is the filing deadline for Form 10-K for fiscal year 2024?" You perform retrieval and get back five chunks with similarity scores 0.78, 0.72, 0.69, 0.54, 0.51. The top score of 0.78 is decent but not outstanding. You pass the query and the top three chunks to your context relevance classifier, which outputs a relevance score of 0.81, indicating high confidence that these chunks are relevant to the query. You analyze the chunks and find that all three mention Form 10-K filing deadlines, and two of them specifically mention 2024. Coverage is high: the key information requirements are met. You generate an answer using the top chunks. The language model outputs an answer with an average token probability of 0.92, indicating high confidence in the generation. You combine these signals: retrieval score 0.78, relevance 0.81, coverage 1.0, generation confidence 0.92. The weighted average is 0.85. This exceeds your answerability threshold of 0.75, so you classify the query as answerable and return the generated answer.

Now consider a different query: "How have ESG reporting requirements evolved since 2020 and what impact have they had on small cap valuations?" This is a complex multi-hop question. You retrieve five chunks with scores 0.61, 0.58, 0.56, 0.54, 0.52. The top score is marginal. You pass the query and chunks to your relevance classifier, which outputs 0.62, indicating uncertainty about whether the chunks address the full query. You analyze coverage and find that the chunks discuss ESG reporting requirements but do not discuss small cap valuations or the impact of ESG requirements on valuations. Coverage is 50%: one part of the question is addressed, the other is not. You attempt generation anyway to see what the model does. The generated answer has an average token probability of 0.68, indicating uncertainty. The model is hedging and generating vague statements. You combine the signals: retrieval 0.61, relevance 0.62, coverage 0.5, generation confidence 0.68. The weighted average is 0.60. This is below your threshold of 0.75, so you classify the query as not answerable. You abstain and return a message: "I found information about ESG reporting requirements since 2020, but I could not find information about their impact on small cap valuations. You might try searching our market analysis database or contacting the equity research team."

This process is systematic, measurable, and tunable. You are not making subjective judgments. You are computing signals, combining them, and applying thresholds. You can evaluate this process on labeled data and optimize the signal weights and thresholds to achieve your desired precision-recall tradeoff.

## The False-Answer Versus Correct-Abstention Tradeoff

Every answerability detection system faces a fundamental tradeoff. You can set a low threshold and generate answers frequently, which maximizes coverage but increases the risk of false answers. Or you can set a high threshold and abstain frequently, which minimizes false answers but reduces coverage. The optimal threshold depends on the relative costs of false answers and abstentions in your application.

A false answer is when the system generates an answer but the answer is incorrect. This happens when the answerability classifier predicts answerable but the context is actually insufficient or misleading. The cost of a false answer varies by domain. In healthcare, a false answer can lead to patient harm. In finance, it can lead to financial loss. In legal, it can lead to liability. In customer support, it might just be mildly annoying. You need to estimate the expected cost of a false answer in your application.

A correct abstention is when the system refuses to answer and this refusal is correct because the query was not answerable. This is a good outcome. You avoided a false answer. But abstention has a cost too. The user did not get the information they needed. They might be frustrated. They might have to spend time looking elsewhere. In applications where the RAG system is the only source of information, abstention cost is high. In applications where the RAG system is one of several tools, abstention cost is lower.

You also have false abstentions, where the system refuses to answer but the query was actually answerable. This happens when the answerability classifier is too conservative. The cost is lost value: you could have provided a correct answer but did not. And you have correct answers, where the system generates an answer and the answer is correct. This is the desired outcome, but there is still a small cost in terms of latency and compute.

You model these costs explicitly. Assign a cost to each outcome: false answer, correct answer, correct abstention, false abstention. Measure the rate of each outcome on a validation set for different threshold values. Calculate the expected total cost for each threshold. Choose the threshold that minimizes expected cost. This is a principled approach to threshold tuning that accounts for your specific application's risk profile.

In the healthcare AI case, the cost of a false answer was extremely high: potential patient harm, legal liability, reputational damage. The cost of abstention was much lower: the physician would need to look up information manually, which takes a few extra minutes. The optimal threshold should have been very high, erring strongly on the side of abstention. The company had set a low threshold, optimizing for coverage and user satisfaction in the short term. This was a catastrophic miscalibration. After the lawsuit, they recalibrated with a threshold that produced a 35% abstention rate, up from 8%. The false answer rate dropped from 11% to less than 1%. The system was far less risky, even though it answered fewer queries.

## Building Answerability Classifiers

Building a robust answerability classifier requires training data, feature engineering, model selection, and iterative evaluation. You start by collecting a dataset of query-context pairs labeled as answerable or not answerable. You can collect this from production logs: sample queries, retrieve context for each, have annotators label answerability, and ideally also label whether the generated answer was correct. This gives you ground truth for both answerability and answer correctness, which allows you to measure how well answerability classification predicts answer correctness.

You engineer features from the query and context. Retrieval scores are obvious features: top score, mean of top-K scores, score variance. Text statistics are also useful: query length, context length, ratio of query length to context length. Lexical features capture surface-level matching: what percentage of query terms appear in the context? How many exact matches are there? Semantic features capture meaning: what is the cosine similarity between the query embedding and the context embedding? What is the output of a cross-encoder model that scores query-context relevance? Coverage features capture whether the query's information requirements are met: you might use NER to extract entities from the query, then check what percentage of those entities appear in the context.

You train a binary classifier using these features. Logistic regression is a simple and interpretable baseline. Gradient boosting models like XGBoost or LightGBM often perform well and provide feature importance analysis, which helps you understand what signals matter most. Neural models like BERT-based classifiers can achieve high performance if you have enough training data. You train the model to predict answerability, and you evaluate it on precision, recall, F1, and AUC.

You can also use a language model as the classifier. You prompt a model with the query and context and ask it to judge answerability. "Given this question and these documents, can the question be answered? Respond with 'yes' or 'no' and provide a confidence score." You can use few-shot prompting with a general-purpose model, or you can fine-tune a smaller model specifically for answerability classification. Fine-tuning on your domain data often outperforms few-shot prompting because the model learns domain-specific patterns of answerability.

Whichever approach you use, you need to iterate. You evaluate the classifier on a test set. You analyze false positives: cases where the classifier said answerable but the answer was wrong. You analyze false negatives: cases where the classifier said not answerable but a correct answer was possible. You use these analyses to improve features, adjust thresholds, or collect more training data for underrepresented cases. This is not a one-time training job. This is an ongoing process of refinement as you learn from production data.

## Threshold Tuning for Your Risk Tolerance

The answerability threshold is the single most important tuning parameter in your abstention system. It determines the balance between coverage and precision. You need to set it based on careful analysis of your application's risk tolerance, user expectations, and cost model.

You start by measuring performance across a range of thresholds. You take a validation set of queries with labeled answerability and known-correct answers. For each threshold value from 0.1 to 0.9 in increments of 0.05, you classify queries as answerable or not based on whether the answerability score exceeds the threshold. For queries classified as answerable, you check whether the generated answer is correct. For queries classified as not answerable, you check whether abstention was correct or whether the query was actually answerable. You compute precision, recall, false answer rate, correct abstention rate, and false abstention rate for each threshold.

You plot these metrics against the threshold. As the threshold increases, precision increases and false answer rate decreases, because you are only answering when very confident. But recall decreases and false abstention rate increases, because you are abstaining on more queries, including some that were answerable. You visualize this tradeoff curve and choose the threshold that aligns with your risk tolerance.

If you are building a system for a high-stakes domain like healthcare, finance, or legal, you choose a high threshold that minimizes false answers even at the cost of high abstention rate. You might accept a 30% or 40% abstention rate to keep the false answer rate below 1%. If you are building a system for a low-stakes domain like general customer support or entertainment recommendations, you choose a lower threshold that maximizes coverage, accepting a higher false answer rate because the cost of errors is low.

You can also set different thresholds for different query types or user contexts. If you can classify queries by risk level, you can use a high threshold for high-risk queries and a low threshold for low-risk queries. If a user is asking about medical dosages, you use a high threshold. If a user is asking for restaurant recommendations, you use a low threshold. This adaptive thresholding allows you to optimize for both safety and coverage.

You should also consider dynamic thresholding based on retrieval quality. If retrieval returns very high scores, you can lower the threshold because high retrieval quality is a strong signal of answerability. If retrieval returns marginal scores, you raise the threshold to compensate. This dynamic approach is more sophisticated but can improve performance by adapting to the evidence available for each query.

## Monitoring and Iteration

Answerability detection is not set-and-forget. You need to monitor it continuously in production and iterate based on real-world performance. You log every query with its answerability score, the decision to answer or abstain, and if you generated an answer, whether it was correct based on user feedback or manual review. This gives you ongoing data about how well your answerability classifier is performing.

You monitor false answer rate in production. If you see it creeping up, your threshold might be too low or your classifier might be degrading due to distribution shift. You investigate and retune. You monitor false abstention rate by sampling abstained queries and manually evaluating whether they were answerable. If you find many false abstentions, your threshold might be too high or your features might be missing important signals.

You also collect user feedback on abstentions. When you abstain, you can ask the user: "I could not find enough information to answer this question. Was this the right decision?" If users frequently say no, your abstention is too aggressive. If users say yes, your abstention is appropriately calibrated. This user feedback is valuable ground truth for improving your classifier.

You periodically retrain your answerability classifier on new data from production. As your knowledge base evolves, as user query patterns change, as you fix bugs in retrieval or relevance models, the distribution of query-context pairs changes. Your classifier needs to adapt. You collect new labeled data, retrain, reevaluate, and redeploy. This is part of the ongoing maintenance of a production RAG system.

## The Difference Between Answerability and Confidence

It is important to distinguish answerability from confidence. Answerability is about whether the retrieved context is sufficient to answer the question. Confidence is about how certain the model is in its generated answer. These are related but not identical. A query can be answerable with high-quality context but the model might still generate a low-confidence answer if the question is complex or the synthesis is difficult. Conversely, a query might be not answerable due to missing context, but the model might generate a high-confidence answer by hallucinating based on its parametric knowledge.

Answerability detection happens before generation. You evaluate the query and context and decide whether to proceed to generation. Confidence estimation happens during or after generation. You evaluate the generated answer and assess how confident the model is. Ideally, you use both. You check answerability before generation to decide whether to generate. If you proceed to generate, you check confidence after generation to decide whether to present the answer to the user or to flag it for review.

The combination of answerability detection and confidence estimation provides defense in depth. Some failures are caught by answerability detection: the context is obviously insufficient, so you abstain before wasting time on generation. Other failures are caught by confidence estimation: the context seemed sufficient, but the model struggled to synthesize an answer, so you abstain after seeing the low-confidence output. Using both layers reduces the risk of false answers more effectively than using either alone.

## Lessons from the Healthcare AI Lawsuit

The healthcare AI company that faced a 4.7 million dollar settlement learned painful lessons about answerability detection. Their post-incident analysis revealed that their system had no explicit answerability detection. They had relied on the language model to implicitly refuse to answer when information was insufficient. This reliance was misplaced. The model was trained to be helpful and to generate answers. It would generate plausible-sounding recommendations even when critical patient information was missing.

After the lawsuit, the company implemented a multi-layer answerability detection system. The first layer checked for critical missing data fields in patient records. If allergy information, current medications, or recent lab results were missing, the query was automatically flagged as not answerable for safety-critical recommendations. The second layer used a trained answerability classifier that scored query-context pairs based on retrieval quality, context relevance, and coverage of clinical information requirements. If the score was below a high threshold, the system abstained. The third layer was a confidence check on generated recommendations. If the model expressed uncertainty or hedging language, the recommendation was flagged for physician review rather than being presented as a confident suggestion.

These layers reduced the false answer rate from 11% to less than 0.5%. The abstention rate increased to 38%, which was acceptable because physicians understood that abstention was a safety feature. The system became defensible from a liability perspective. The company resumed growth. The investment in answerability detection transformed the system from a legal risk into a valuable clinical tool.

You are building a RAG system, and you need to implement answerability detection as a core component, not as an afterthought. Define what answerability means in your domain. Collect labeled data on answerable versus not answerable query-context pairs. Build a classifier using retrieval scores, relevance scores, coverage estimates, and model uncertainty. Set thresholds based on your risk tolerance and cost model. Monitor performance in production and iterate. Treat answerability detection as a measurable, tunable, improvable system, not as a vague hope that the model will do the right thing. The difference between systems with and without answerability detection is the difference between systems that fail safely and systems that fail dangerously. In high-stakes domains, that difference is measured in lawsuits, settlements, and lives. In all domains, it is measured in user trust, system reliability, and long-term product viability. Build answerability detection from the start, and you build a system that earns and keeps user trust.
