# 3.2 — Dense vs Sparse vs Learned Sparse Embeddings

**Dense embeddings smooth away the precise terms that users are searching for.** Semantic similarity excels at conceptual queries—"what are the requirements for reporting suspicious transactions"—but fails catastrophically when lawyers search for "Section 1557 of the Affordable Care Act" and need exact statutory citations. The embedding model maps "Section 1557" and "health insurance coverage requirements" to nearby points in vector space, which is semantically correct and utterly useless for exact-match retrieval. Production RAG systems in 2026 run both dense and sparse retrieval in parallel because real users issue both query types in the same session.

By 2026, production RAG systems have learned that there is no single "best" retrieval approach. Dense embeddings excel at semantic similarity. Sparse embeddings excel at keyword precision. Learned sparse embeddings attempt to bridge the gap with neural models that produce sparse representations. The right architecture uses all three strategies, routing queries to the appropriate retrieval method or blending results from multiple retrievers. This chapter teaches you when to use dense, sparse, and learned sparse embeddings, how they work, where each fails, and how to combine them into hybrid retrieval systems that handle the full spectrum of real-world queries.

## Dense Embeddings: The Semantic Powerhouse

Dense embeddings are continuous, high-dimensional vectors where every dimension is a real number, typically a 32-bit float. A 768-dimensional dense embedding is a list of 768 floating-point values, each contributing to the semantic representation of the text. These embeddings are produced by neural models—transformers, sentence encoders, or language models—trained to map semantically similar text into nearby points in vector space. The core idea is that cosine similarity or dot product between embeddings correlates with semantic similarity between the original texts.

Dense embeddings revolutionized information retrieval starting in 2018 with models like BERT and sentence-transformers. By 2026, they are the default choice for semantic search tasks where users ask questions in natural language, describe concepts, or search for ideas rather than specific words. Dense retrieval works because transformers learn contextualized representations that capture synonymy, paraphrase, and conceptual relatedness. A query for "how to prevent fraud in credit card transactions" will retrieve documents about fraud detection, anti-fraud systems, and transaction monitoring even if those exact words do not appear.

The strength of dense embeddings is generalization. They handle vocabulary mismatch, where the query uses different words than the document but expresses the same idea. They handle abstraction, where the query describes a high-level concept and the document provides concrete details. They handle multilingual retrieval, where the query and document are in different languages but the embedding space aligns them. These capabilities make dense retrieval indispensable for open-ended, exploratory search.

The weakness of dense embeddings is precision on specific terms. If your query is "find documents mentioning GDPR Article 17," a dense model might retrieve documents about data privacy, user rights, and deletion policies, but it might rank a document that explicitly mentions "Article 17" lower than a document that discusses "right to be forgotten" in general terms. Dense embeddings smooth semantic meaning across contexts, which is powerful for conceptual search but dangerous for exact-match requirements.

Dense embeddings also struggle with rare or out-of-vocabulary terms. If your query includes a technical identifier, a product code, a legal citation, or a newly coined term that did not appear in the model's training data, the embedding might fail to encode it meaningfully. The model has never seen "Regulation S-K Item 105," so it encodes it based on subword tokens and surrounding context, which might not distinguish it from "Regulation S-K Item 106."

In 2026, dense embeddings are table stakes for production RAG, but they are not sufficient on their own. You need them for semantic queries, but you also need complementary strategies for keyword-precise queries.

## Sparse Embeddings: The Keyword Precision Tool

Sparse embeddings are high-dimensional vectors where most dimensions are zero. Instead of encoding semantic meaning in continuous values, sparse embeddings encode term presence or importance in discrete, interpretable dimensions. The classic sparse retrieval method is TF-IDF, where each dimension corresponds to a term in the vocabulary, and the value represents the term's frequency weighted by its inverse document frequency. BM25 is a refined version that adds saturation and length normalization.

Sparse retrieval has been the backbone of search engines for decades. It works because it matches terms directly—if the query contains the word "GDPR" and the document contains "GDPR," the retrieval score reflects that match. Sparse methods are exact, interpretable, and immune to vocabulary mismatch problems where the model has never seen the term. If you search for a newly released product code, a legal statute number, or a technical acronym, BM25 will find it as long as it appears in the document.

In 2024, many ML practitioners dismissed sparse retrieval as outdated, a relic from the pre-transformer era that dense embeddings had obsoleted. By 2026, production teams rediscovered that sparse retrieval still solves problems dense embeddings cannot. Keyword-heavy queries, exact-match requirements, rare term retrieval, and domains with precise controlled vocabularies all benefit from sparse methods. The failure mode of abandoning sparse retrieval entirely is predictable: users complain that they cannot find documents containing specific terms they know exist.

The weakness of sparse embeddings is vocabulary mismatch. If the query says "automobile" and the document says "car," traditional sparse methods return no match. If the query is a natural-language question and the document is a technical description using different phrasing, sparse methods fail. Sparse retrieval also struggles with semantic nuance—it does not understand that "fraud prevention" and "anti-fraud measures" mean the same thing unless you manually build synonym dictionaries or query expansion rules.

Another limitation is dimensionality. Sparse embeddings have one dimension per vocabulary term. For a large corpus, the vocabulary might contain hundreds of thousands or millions of unique terms, resulting in extremely high-dimensional vectors. Most of those dimensions are zero for any given document, which is why they are called sparse, but storing and indexing these vectors still requires specialized data structures.

The correct use of sparse embeddings in 2026 is as a complementary retrieval method, not a replacement for dense embeddings. You deploy BM25 or TF-IDF alongside dense retrieval and route keyword-heavy queries to sparse retrieval or blend results from both.

## Learned Sparse Embeddings: The Neural Middle Ground

Learned sparse embeddings are a hybrid approach that uses neural models to produce sparse, interpretable vectors. Instead of relying on handcrafted term weighting like TF-IDF, learned sparse models train transformers to output high-dimensional sparse vectors where each dimension corresponds to a vocabulary term, and the value represents the term's importance. The most prominent learned sparse method in 2026 is SPLADE, which stands for Sparse Lexical and Expansion model.

SPLADE works by passing the input text through a transformer encoder, then applying a learned projection that maps the contextualized token representations to a vocabulary-sized sparse vector. The model is trained with a loss function that encourages both retrieval accuracy and sparsity—non-zero values should appear only for terms that are truly relevant. The result is a sparse vector that looks like TF-IDF but is informed by the transformer's contextualized understanding of the text.

The key innovation is term expansion. SPLADE can activate dimensions for terms that do not literally appear in the text but are semantically related. If the document says "car," the model might activate dimensions for "automobile," "vehicle," and "transportation." This gives you the precision of sparse retrieval with some of the generalization power of dense embeddings. You get exact matches on important terms plus implicit expansion to related terms, all in a single sparse vector.

Learned sparse embeddings deliver measurably better retrieval quality than classical BM25 on many benchmarks, often approaching dense embedding performance while maintaining the interpretability and exact-match properties of sparse methods. They also compress better than dense embeddings because most dimensions are zero, which allows efficient storage and indexing with inverted index data structures.

The downside is computational cost. Learned sparse models require transformer inference to produce embeddings, which is slower than classical TF-IDF. They also require careful tuning of the sparsity regularization term during training—if you do not penalize non-zero dimensions enough, the model produces dense-like vectors that lose the efficiency benefits. If you penalize too much, the model collapses to a few dimensions and loses expressiveness.

In 2026, learned sparse embeddings are a production-ready option for teams that want the benefits of both sparse and dense retrieval without maintaining two separate systems. SPLADE and its variants are deployed in real-world search engines, and open-source implementations are widely available. The main barrier is that fewer embedding-as-a-service APIs offer learned sparse embeddings compared to dense embeddings, so you are more likely to self-host.

## When Keyword Matching Beats Semantics

There are query patterns where keyword matching outperforms semantic similarity, and you need to recognize them to route retrieval appropriately. These include:

**Queries with rare or unique identifiers.** Product codes, legal citations, regulation numbers, ICD codes, patent numbers, case numbers, or any other controlled identifiers that carry precise meaning. A query for "ICD-10 code E11.9" should retrieve documents containing exactly that code, not documents about diabetes in general. Dense embeddings might fail because the model has never seen that specific code. BM25 succeeds because it matches the literal string.

**Queries with technical jargon or acronyms.** Domain-specific terms that might not have appeared in the embedding model's training data. A query for "WCAG 2.1 Level AA compliance" should match documents that mention those exact terms. A dense model trained on general web text might not have strong representations for WCAG, leading to poor retrieval.

**Queries with Boolean or structured logic.** Searches like "find documents mentioning both HIPAA and breach notification but not safe harbor" require precise term presence and absence logic. Sparse retrieval naturally supports Boolean operators. Dense embeddings do not.

**Queries where the user knows the exact phrasing.** If the user is searching for a specific quote, a phrase from a document they read before, or a substring they remember, they want exact matching, not semantic approximation. Dense retrieval might return conceptually similar but textually different results.

**Queries in low-resource languages or newly emerging terminology.** If your embedding model was trained in 2024 and your corpus includes documents from 2025 and 2026 with new terminology, product names, or events, the model has no representation for those terms. Sparse retrieval will find them as long as they appear literally.

The pattern is clear: when precision on specific terms matters more than semantic generalization, sparse retrieval wins. The mistake is to assume that because dense embeddings deliver better average retrieval quality on benchmarks, they deliver better quality on every query. They do not.

## Why Sparse Still Matters in 2026

Despite the dominance of transformer-based dense embeddings, sparse retrieval remains critical in production RAG systems for several reasons. First, it provides a fallback for queries that dense embeddings mishandle. If your dense retriever fails to surface a document that the user knows exists, a sparse retriever can catch it. Second, it enables interpretability. You can inspect a BM25 score and see exactly which terms contributed to the match. You cannot easily inspect a 768-dimensional dense embedding and understand why two documents are similar.

Third, sparse retrieval is computationally cheap. BM25 operates on term counts and inverted indexes, which are fast to compute and store. You do not need GPU inference to calculate a BM25 score. This makes sparse retrieval viable for latency-sensitive applications where dense embedding inference would be too slow.

Fourth, sparse retrieval integrates easily with existing search infrastructure. Most search engines—Elasticsearch, Solr, OpenSearch—have built-in BM25 support. Adding sparse retrieval to an existing system is a configuration change, not a ground-up rebuild. Adding dense retrieval requires vector databases, embedding inference, and new indexing pipelines.

Fifth, sparse retrieval is stable over time. BM25 scores are deterministic and reproducible. If you index a document today and query it tomorrow, the score is the same. Dense embeddings depend on the model version, and if the model updates, embeddings change. This stability matters for compliance, auditing, and debugging.

By 2026, the industry consensus is that production RAG systems should deploy hybrid retrieval that combines dense and sparse methods. The exact architecture varies—some systems run both retrievers in parallel and merge results, others route queries to one retriever or the other based on heuristics, and others train a reranker to blend scores from both. The common thread is that neither dense nor sparse is sufficient alone.

## Hybrid Retrieval: Combining Dense and Sparse

Hybrid retrieval systems run multiple retrieval strategies and combine their results. The simplest approach is parallel retrieval with score fusion. You embed the query with a dense model, retrieve the top k results from your vector database, compute a BM25 score for the query against your corpus, retrieve the top k results from an inverted index, and then merge the two result sets using a fusion algorithm like reciprocal rank fusion or weighted score combination.

Reciprocal rank fusion is a parameter-free method that assigns each result a score based on its rank in each retrieval method. If a document ranks third in dense retrieval and fifth in sparse retrieval, its RRF score is 1 divided by three-plus-k plus 1 divided by five-plus-k, where k is a small constant like 60. You sum RRF scores across all retrieval methods and rank documents by the total. This method is robust, does not require tuning weights, and tends to promote documents that rank well in multiple retrievers.

Weighted score combination is more flexible but requires tuning. You assign a weight to dense retrieval scores and a weight to sparse retrieval scores, normalize both to a common range, and compute a weighted sum. If dense retrieval gets a weight of 0.7 and sparse gets 0.3, you are saying that semantic similarity is more important than keyword matching for your use case. You can tune these weights based on held-out evaluation data or learn them with a small ranking model.

The more sophisticated approach is query routing. You classify each query into one of several categories—semantic, keyword, hybrid—and route it to the appropriate retrieval method. A query like "explain the benefits of serverless architecture" is semantic and goes to dense retrieval. A query like "find references to section 409A" is keyword and goes to sparse retrieval. A query like "what are the tax implications of stock options" might go to both. You can implement routing with simple heuristics—count the number of rare terms, proper nouns, or identifiers—or train a classifier on labeled query data.

The most sophisticated approach is learned reranking. You retrieve candidates from both dense and sparse retrievers, then pass the query and each candidate document to a cross-encoder model that scores relevance. Cross-encoders are slower than bi-encoders because they must process each query-document pair jointly, but they deliver higher accuracy. You use dense and sparse retrieval to narrow the candidate set to, say, 100 documents, then rerank those 100 with a cross-encoder to produce the final top ten. This architecture is common in production systems that need the highest possible retrieval quality and can tolerate the reranking latency.

## Strengths and Weaknesses of Each Approach

Dense embeddings excel at:
- Semantic similarity and conceptual queries
- Vocabulary mismatch and paraphrase handling
- Multilingual and cross-lingual retrieval
- Open-ended exploratory search
- Generalization to unseen phrasings

Dense embeddings struggle with:
- Exact keyword matching on rare terms
- Identifiers, codes, and controlled vocabularies
- Boolean logic and structured queries
- Newly coined terms and out-of-vocabulary words

Sparse embeddings excel at:
- Exact keyword matching
- Rare and unique term retrieval
- Identifiers, product codes, and citations
- Interpretability and debugging
- Low-latency, low-compute retrieval

Sparse embeddings struggle with:
- Vocabulary mismatch and synonymy
- Semantic similarity without shared terms
- Natural-language question answering
- Multilingual retrieval

Learned sparse embeddings excel at:
- Balancing keyword precision with term expansion
- Interpretability with neural quality
- Efficient storage and indexing
- Generalization better than classical sparse, precision better than dense

Learned sparse embeddings struggle with:
- Computational cost of transformer inference
- Tuning sparsity regularization
- Fewer API providers and less tooling maturity

The right choice depends on your query distribution, your corpus characteristics, and your quality requirements. If 90 percent of your queries are semantic and 10 percent are keyword, hybrid retrieval with a dense-first strategy makes sense. If 50 percent of your queries include technical identifiers, you need strong sparse retrieval. If you cannot tolerate the latency or infrastructure cost of dense embeddings, learned sparse embeddings might be a middle ground.

## Implementation Patterns in 2026

By 2026, the production patterns for hybrid retrieval have stabilized. Most teams follow one of these architectures:

**Parallel retrieval with RRF.** Run dense and sparse retrieval in parallel, retrieve top 50 from each, merge with reciprocal rank fusion, return top 10. Simple, robust, no parameter tuning required. Works well when both retrievers contribute useful signals.

**Query routing with fallback.** Classify query as semantic or keyword, route to the appropriate retriever, return results. If the retriever returns fewer than a threshold number of results, fall back to the other retriever. This works when query types are clearly separable.

**Dense-first with sparse augmentation.** Retrieve top 100 with dense embeddings, rerank using a combination of dense similarity score and BM25 score, return top 10. This prioritizes semantic retrieval but boosts results that also have strong keyword matches.

**Two-stage retrieval with reranking.** Retrieve top 100 from dense, top 100 from sparse, merge with RRF to get top 50 candidates, rerank with a cross-encoder, return top 10. This is the highest-quality approach but also the highest-latency.

The infrastructure for hybrid retrieval requires both a vector database and an inverted index. Some modern search platforms—Elasticsearch with vector search, Weaviate, Vespa—support both dense and sparse retrieval in a single system. Others require you to maintain separate systems and merge results at query time. The operational complexity depends on whether your platform supports hybrid retrieval natively or requires custom integration.

## The Latency and Cost Tradeoffs

Dense retrieval requires embedding the query with a neural model, which adds latency. A small embedding model might take 10 to 20 milliseconds on a GPU, a large model might take 50 to 100 milliseconds. Then you perform a nearest-neighbor search in your vector database, which might take another 10 to 50 milliseconds depending on index size and algorithm. Total latency for dense retrieval is typically 50 to 150 milliseconds.

Sparse retrieval requires tokenizing the query and scoring against an inverted index, which is extremely fast—typically under 10 milliseconds for BM25 on a well-tuned search engine. If you need sub-50-millisecond query latency, sparse retrieval might be your only option unless you cache embeddings aggressively or use a very small dense model.

Learned sparse retrieval requires transformer inference to produce the sparse embedding, so it has similar latency to dense retrieval—50 to 100 milliseconds depending on model size and hardware. The retrieval itself is fast because it uses an inverted index, but the encoding step is the bottleneck.

Cost follows latency. Dense retrieval requires GPU or TPU instances for embedding inference, plus vector database hosting. Sparse retrieval requires CPU compute and inverted index storage, which is cheaper. Learned sparse retrieval requires GPU inference but cheaper indexing than dense embeddings. If you are optimizing for cost, sparse or learned sparse retrieval might deliver better quality per dollar than dense embeddings.

The decision tree is: if you need the absolute best retrieval quality and can tolerate 100-millisecond query latency, use dense embeddings plus reranking. If you need sub-50-millisecond latency, use sparse retrieval or learned sparse retrieval. If your queries are mixed, use hybrid retrieval with query routing to minimize latency on keyword queries.

## Evaluating Dense, Sparse, and Hybrid on Your Data

The only way to know which retrieval strategy works best for your use case is to evaluate all three on your own query and document distribution. Build a test set of at least 100 queries with relevance labels indicating which documents should be retrieved. Run dense retrieval, sparse retrieval, and hybrid retrieval, and measure recall at k, precision, and mean reciprocal rank. Compare the results.

You will likely find that dense retrieval wins on average, sparse retrieval wins on specific query types, and hybrid retrieval wins overall. The exact numbers depend on your domain, your query patterns, and your corpus. Do not trust benchmarks from other domains, and do not assume that the method with the best average score is the best method for every query.

The discipline of production RAG is to measure, compare, and choose based on data, not assumptions. Dense embeddings are powerful, but they are not magic. Sparse retrieval is old, but it is not obsolete. Learned sparse embeddings are promising, but they are not a silver bullet. The correct answer is almost always a hybrid approach informed by careful evaluation on your specific data.
