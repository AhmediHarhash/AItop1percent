# 2.14 — Source Trust and Document Authenticity: Provenance, Signing, and Publisher Controls

Embedding models treat all documents identically, which creates a catastrophic problem when your knowledge base mixes authoritative policy with editable wikis. A well-meaning but misinformed engineer posts incorrect security advice on an internal wiki. The advice is verbose and matches common query patterns, so it ranks above official CISO documentation. Three thousand four hundred employees receive incorrect recommendations. Two security incidents result from following the bad advice. Remediation costs 2.1 million dollars. The root cause: retrieval ranked by semantic similarity with complete blindness to source authority, document authenticity, or publisher trust.

You inherit a fundamental problem the moment you build RAG over enterprise document collections: not all documents are equally trustworthy, but embedding models treat them identically. Official policy documents carry institutional authority. User forums contain speculation and opinion. Third-party tutorials might be outdated or incorrect. Internal wikis mix authoritative knowledge with personal opinions. Drafts and works-in-progress sit alongside approved publications. Your retrieval system ranks documents by semantic similarity to queries, completely blind to trustworthiness, authenticity, or authority.

This chapter teaches you how to implement source trust and document authenticity controls for RAG systems. You'll learn why provenance tracking matters for knowledge base integrity, how to implement trust tiers that weight sources appropriately, how document signing ensures authenticity, and how publisher controls prevent untrusted content from poisoning your index. By the end, you'll understand that source trust isn't optional security theater—it's essential infrastructure that determines whether your RAG system provides reliable information or confidently cites garbage.

## Why Source Trust Matters for RAG Reliability

Traditional information retrieval assumes users evaluate source credibility themselves. Search engines return results with source URLs visible, allowing users to judge whether Wikipedia is appropriate for their needs versus peer-reviewed journals. RAG systems hide this evaluation behind generated responses that synthesize multiple sources. Users see confident answers with citations but rarely evaluate whether cited sources are trustworthy. This abstraction increases user trust while simultaneously hiding the trust evaluation that users would normally perform.

A healthcare company built a clinical decision support RAG that indexed medical journals, clinical guidelines, and an internal discussion forum where doctors shared experiences. When asked about treatment protocols, the system retrieved evidence-based guidelines but also unvetted forum discussions of off-label uses and personal opinions. The LLM synthesized these sources into recommendations that blended evidence-based medicine with anecdotal speculation, presenting both with equal confidence. Physicians trusted the system's authoritative tone, unaware that some recommendations came from forum speculation rather than clinical guidelines.

Untrusted sources poison RAG through several mechanisms. Incorrect information appears in retrieved contexts and propagates into generated responses. Outdated information from unofficial sources contradicts current official information. Speculative or opinion content is treated as factual. Malicious content deliberately crafted to manipulate RAG systems injects false information into organizational knowledge. Each mechanism undermines system reliability and user trust.

An enterprise software company indexed their documentation site alongside Stack Overflow discussions and third-party tutorials. When developers asked about API usage, retrieval mixed official documentation with community workarounds that exploited undocumented behavior. The RAG system recommended approaches that worked historically but were never officially supported and broke in new releases. The company's support team fielded hundreds of tickets from developers who followed RAG recommendations based on outdated Stack Overflow answers indexed alongside current documentation.

Authority inference from embeddings alone is impossible. Semantic similarity doesn't correlate with trustworthiness. Well-written misinformation looks identical to well-written fact in embedding space. Confident assertions by unqualified sources produce embeddings similar to expert analysis. Outdated authoritative sources might match queries better than current authoritative sources if query language hasn't evolved. Retrieval ranking based purely on similarity inevitably surfaces untrustworthy content when both trustworthy and untrustworthy sources exist.

A financial services company discovered this when their RAG system retrieved investment advice from anonymous blog posts indexed alongside official research reports. The blog posts used engaging language and specific examples that matched user queries well, causing them to rank highly despite being unvetted opinion. Official research reports used formal, technical language that matched queries less precisely, ranking lower despite being authoritative. Semantic similarity rewarded engaging writing over institutional authority.

Compliance and regulatory requirements in many domains mandate using only approved, authenticated information sources. Healthcare regulations require clinical decisions be based on current evidence-based guidelines, not unverified sources. Financial regulations require advice be based on approved disclosures and research. Legal practice requires citing verified case law and statutes, not unofficial summaries. Pharmaceutical companies must base manufacturing on validated procedures, not informal notes. RAG systems in these domains must enforce source restrictions as compliance requirements, not optional features.

## Document Provenance: Tracking Source, Publisher, and Authority

Provenance tracking records where documents originated, who published them, when, through what process, and with what authority. This metadata enables trust evaluation, publisher-based filtering, and authority-weighted retrieval that treats sources appropriately for their reliability and authority level.

Source system tracking records which system provided each document. Internal document management systems, public websites, vendor APIs, internal wikis, email systems, version control repositories all represent different source systems with different trust characteristics. Store source system identifiers as metadata. A pharmaceutical company tagged documents with source systems—regulatory submissions came from the validated document control system, procedures from the quality management system, informal guidance from SharePoint wikis—enabling trust tier assignment based on source system.

Publisher identity tracks who created or approved each document. User identity for user-generated content, department identity for departmental documents, role identity for official publications. Store publisher metadata enabling filtering by authorized publishers and weighting by publisher authority. A legal research company tracked whether case summaries came from official court reporters, editorial staff, or community contributors, weighting official court reporters highest in retrieval ranking.

Publication workflow tracking records what approval process documents went through. Drafts, submitted documents, reviewed documents, and officially approved documents represent different authority levels. Store workflow status and approval metadata. A manufacturing company tracked document workflow status—draft, engineering review, quality review, final approval—indexing only documents that reached final approval status for production use while indexing all statuses for engineering development.

Approval authority records who approved documents and what authority they hold. CISO-approved security policies carry more authority than engineer-written security guides. CEO-signed policies carry more authority than department-level procedures. Regulatory-approved protocols carry more authority than internal procedures. Store approver identity and role. A healthcare company weighted clinical protocols by approver credentials—protocols approved by medical directors received highest weight, protocols approved by department heads medium weight, protocols with no formal approval lowest weight.

Version and revision tracking identifies whether documents are current, superseded, or draft. Current official versions carry more authority than outdated versions or draft revisions. A financial services firm implemented version status metadata—current, superseded, draft, archived—filtering retrieval to current versions by default while allowing historical queries to access superseded versions, preventing outdated information from contaminating current advice.

Temporal validity explicitly marks documents with effective dates and expiration dates. Time-bound content like promotions, temporary policies, or seasonal guidance should only appear in retrieval during validity periods. An insurance company tagged policy documents with policy year, ensuring queries about current policies retrieved only current-year documents while historical queries could access past policies.

External source verification tracks third-party content provenance. When indexing external content, record the original URL, retrieval date, and verification status. A consulting firm indexed industry research from various sources, recording source URLs and last verification dates, enabling review of external content credibility and freshness during retrieval.

## Trust Tiers: Weighting Sources by Reliability

Trust tiers classify sources into reliability categories and apply different retrieval weights, filtering policies, or presentation rules based on tier. This enables RAG systems to prefer authoritative sources while still accessing lower-authority sources when authoritative sources don't answer queries.

Defining tier structure requires analyzing your source landscape and identifying natural authority gradations. A typical enterprise might use three tiers: tier 1 for official approved documents, tier 2 for vetted but less formal content, tier 3 for unvetted user-generated content. A government agency might use four tiers: legislation and regulation, official agency guidance, approved training materials, archived historical documents. Define tiers that match your organization's information authority structure.

A healthcare company implemented a four-tier system: tier 1 for regulatory-approved clinical protocols, tier 2 for evidence-based clinical guidelines from recognized medical organizations, tier 3 for internal clinical best practices, tier 4 for clinical discussion forums. Retrieval weighted tiers progressively—tier 1 documents received 2x relevance boost, tier 2 1.5x boost, tier 3 no adjustment, tier 4 0.5x downweight, ensuring authoritative sources appeared preferentially.

Automatic tier assignment uses source metadata to classify documents. Map source systems to tiers—official document management maps to tier 1, departmental SharePoint maps to tier 2, internal wikis map to tier 3. Map publisher roles to tiers—executive approvals map to tier 1, department manager approvals map to tier 2, individual contributors map to tier 3. A manufacturing company automatically assigned tiers based on source system and approval workflow, requiring manual review only for edge cases.

Manual tier assignment allows human curation of trust levels for documents where automatic rules are insufficient. Operators review documents and explicitly assign tier levels based on content evaluation. A legal research company manually tiered external content—peer-reviewed legal journals to tier 1, respected legal blogs to tier 2, general legal information sites to tier 3—because source URL alone didn't reliably indicate quality.

Retrieval weighting by tier adjusts relevance scores based on trust level. Multiply relevance scores by tier weight factors before ranking. High-tier documents rank higher than equal-relevance low-tier documents. A customer support company implemented tier weights of 2.0, 1.3, 1.0, 0.6 for their four tiers, ensuring official documentation appeared above user-contributed troubleshooting guides when both matched queries similarly.

Filtering by tier enables restricting retrieval to minimum trust levels. For high-stakes queries or sensitive domains, retrieve only from tier 1 and tier 2 sources. For general queries, retrieve from all tiers but weight by tier. A pharmaceutical company restricted manufacturing guidance queries to tier 1 validated procedures only, preventing unvalidated informal guidance from affecting production, while allowing general informational queries to use all tiers.

Presentation differentiation shows users which tier sources came from. Cite tier level alongside source references, use visual indicators like badges or colors for different tiers, or segregate responses by tier. A financial services firm presented tier 1 sources prominently with official source badges, tier 2 sources normally, and tier 3 sources with "community contribution" warnings, enabling users to evaluate source authority themselves.

Tier evolution policies define how documents move between tiers. New content might start in tier 3, move to tier 2 after editorial review, move to tier 1 after official approval. Documents might downgrade tiers over time if not refreshed. An insurance company implemented tier lifecycle policies where all user-contributed content started tier 3, moved to tier 2 after subject matter expert review, moved to tier 1 after compliance approval, and downgraded to tier 3 after two years without review.

## Document Signing and Verification: Ensuring Authenticity

Digital signatures provide cryptographic proof that documents haven't been tampered with since publication and come from claimed publishers. Signing is critical for domains where document authenticity has compliance or security implications—regulatory submissions, policies, contracts, financial disclosures, clinical protocols.

Cryptographic signing uses public-key cryptography to create signatures that verify document integrity and publisher identity. Publishers sign documents with private keys. Verification uses public keys to confirm signatures. Signature verification proves the document hasn't changed since signing and that the claimed publisher actually signed it. A pharmaceutical company implemented document signing for clinical protocols, requiring all protocols be signed by authorized medical directors before indexing, verifying signatures during ingestion to ensure authenticity.

Certificate authorities establish trust chains for publisher identity. Instead of managing public keys directly, use certificates from trusted CAs that vouch for publisher identities. This provides organizational trust infrastructure where document signatures can be verified against organizational certificate authority. A financial services firm implemented PKI infrastructure for document signing, issuing signing certificates to authorized publishers, verifying document signatures against the corporate CA during ingestion.

Signature verification during ingestion ensures only authentic documents enter indexes. Check signatures when documents are received, reject documents with invalid signatures or signatures from unauthorized publishers. Log verification failures for security analysis. A legal firm required all contract templates be signed by legal department staff, verifying signatures during template ingestion, rejecting unsigned or incorrectly signed templates that might contain unauthorized modifications.

Signature validation during retrieval provides defense-in-depth verification. Even if documents passed ingestion checks, validate signatures again before presenting to users. This catches tampering that occurred in storage or transport. A healthcare company validated clinical protocol signatures both at ingestion and at retrieval, ensuring that even if storage was compromised, only authentic protocols reached clinicians.

Timestamp authorities provide trusted timestamps proving when documents were signed. This prevents backdating attacks where documents are signed with old keys after key compromise. Trusted timestamps ensure documents were signed during the signing key's validity period. A pharmaceutical company used timestamp authority services for protocol signatures, proving to regulators that protocols were signed before certain dates, establishing timeline evidence for audit compliance.

Revocation checking verifies that signing certificates haven't been revoked since signing. Certificate revocation lists or OCSP protocols provide real-time revocation status. Check revocation during signature verification to catch compromised signing keys. A financial services firm implemented certificate revocation checking, automatically removing documents from indexes when signing certificates were revoked due to key compromise or employee termination.

Content addressable storage using cryptographic hashes provides alternative integrity verification. Store documents by their content hash, making tampering detectable through hash mismatch. This doesn't verify publisher identity but ensures content integrity. A media company used content-addressed storage for articles, storing content by SHA-256 hash, detecting any tampering attempts when retrieved hashes didn't match stored content.

## Publisher Controls: Allowlists, Denylists, and Approval Workflows

Publisher controls restrict who can contribute content to knowledge bases, preventing unauthorized or unqualified sources from injecting content. This is critical for enterprise environments where information quality has operational or compliance consequences.

Publisher allowlists explicitly enumerate authorized content publishers. Only documents from allowlisted publishers are indexed. This provides strong control but requires maintaining current allowlists. A pharmaceutical company maintained allowlists of approved medical directors and quality managers authorized to publish clinical and manufacturing content, rejecting documents from unauthorized publishers during ingestion.

Publisher denylists explicitly block known problematic publishers. Documents from denylisted publishers are rejected regardless of content. This handles identified bad actors or compromised accounts. A customer support company denylisted user accounts that repeatedly submitted low-quality content, preventing those accounts from cluttering knowledge bases with unhelpful contributions.

Role-based publishing uses organizational role hierarchies to determine publishing authority. Map roles to allowed content types and trust tiers. Executives can publish tier 1 policies, managers tier 2 procedures, individual contributors tier 3 informal guides. A manufacturing company implemented role-based publishing where engineers could publish informal work instructions to tier 3, engineering managers could approve tier 2 procedures, and quality directors could approve tier 1 validated procedures, matching organizational approval authority.

Approval workflows require review before publication. Submitters propose content, approvers review and approve. Only approved content enters indexes. This provides quality gates and authority verification. A healthcare company required all clinical content go through medical director approval before indexing, ensuring clinical accuracy and authority before content reached clinicians through RAG.

Automated quality gates supplement human approval with automated checks. Validate document format, completeness, required metadata, content characteristics before allowing publication. Reject documents failing quality standards regardless of publisher authority. An insurance company implemented quality gates requiring policy documents include effective dates, state jurisdiction, and policy numbers, rejecting incomplete submissions even from authorized publishers.

Publisher reputation systems track publishing quality over time. Monitor user feedback, error rates, and quality metrics for each publisher. Downweight publishers with poor quality histories. A customer support company tracked satisfaction ratings for help articles by author, downweighting articles from authors with low average ratings, incentivizing quality contributions.

Temporary publishing permissions grant time-limited or context-limited publishing authority. Allow specific publishers to contribute specific content during specific periods. A consulting firm granted project team members temporary publishing authority for project documentation during project execution, automatically revoking authority after project completion, preventing stale project documentation from being updated by former team members.

External publisher verification validates third-party content providers. When indexing external content, verify publisher identity and authority. Check domain ownership, SSL certificates, author credentials. A legal research company verified external legal commentary came from licensed attorneys at recognized law firms, checking bar membership and firm credentials before indexing external legal analysis.

## Production Reality: Trust Infrastructure for Reliable RAG

That Fortune 500 company rebuilt their RAG system with comprehensive trust controls after their security incident. They implemented three-tier trust classification: tier 1 for officially approved policies signed by department heads, tier 2 for vetted documentation from authorized technical writers, tier 3 for wiki content marked as community contributions. They implemented publisher allowlists requiring formal authorization before anyone could publish tier 1 or tier 2 content. They required digital signatures on tier 1 policies. They filtered security and compliance queries to tier 1 sources only. The rebuilt system prevented untrusted content from contaminating official guidance while still allowing community knowledge sharing in appropriate contexts.

You build trust infrastructure by understanding which sources in your environment are authoritative, implementing provenance tracking that records source metadata, defining trust tiers appropriate to your organizational structure, optionally implementing document signing for high-stakes content, and enforcing publisher controls that prevent unauthorized content injection. This isn't security theater—it's operational necessity that directly determines whether your RAG system provides reliable information.

Start by auditing your document sources and classifying them by authority. Identify which systems contain official approved content versus informal community content. Identify which roles have authority to approve different content types. Map this structure into trust tiers and publisher controls. A consulting firm audited their 23 different document sources, classifying 4 as tier 1 official content, 9 as tier 2 vetted content, 10 as tier 3 community content, and built tier assignment rules encoding these classifications.

Implement progressively stronger controls as stakes increase. Low-stakes general information needs minimal controls. High-stakes operational guidance needs comprehensive controls. Compliance-critical content needs signing and strict publisher controls. A healthcare company used minimal controls for general administrative content, moderate controls for clinical educational content, and comprehensive controls including signing and approval workflows for clinical decision support content where errors could harm patients.

Make trust visible to users so they understand what authority backs retrieved information. Show source tier in citations. Badge official content. Warn about community-contributed content. Enable users to filter by trust tier. A financial services firm displayed trust tier badges on all citations—gold badges for tier 1 official research, silver for tier 2 vetted content, bronze for tier 3 community contributions—enabling users to evaluate source authority and request higher-tier sources if desired.

Monitor trust violations and quality incidents to refine controls. Track cases where low-tier sources caused problems. Track unauthorized publishing attempts. Track documents that should have been higher tier but weren't. Use incidents to improve classification, tighten controls, or expand publisher allowlists. A pharmaceutical company held monthly reviews of quality incidents, analyzing whether trust controls prevented problems or whether incidents revealed control gaps, iteratively improving trust infrastructure based on operational experience.

The companies that succeed with production RAG implement trust controls appropriate to their domain stakes and compliance requirements. They understand that semantic similarity alone doesn't distinguish authoritative from speculative content. They build provenance tracking, trust tiers, and publisher controls that encode organizational knowledge authority. The companies that fail treat all sources equally, discover through incidents that untrusted content poisoned their knowledge bases, and retrofit trust controls after damage occurs.

Your RAG system's reliability depends entirely on the trustworthiness of indexed sources. Retrieval finds semantically relevant content—it doesn't evaluate trustworthiness. Generation synthesizes retrieved content—it doesn't verify authority. Trust evaluation must happen during indexing through provenance tracking, trust tier classification, signature verification, and publisher controls. Get this right and your system provides reliable, authoritative information. Get it wrong and your system confidently cites speculation, opinion, and misinformation alongside authoritative sources, eroding user trust with every response. Source trust isn't optional—it's the foundation of RAG reliability.
