# 2.6 — Handling Tables, Images, and Non-Text Content

What happens when your RAG system extracts text from PDFs but skips every table, chart, and diagram? Researchers ask about efficacy outcomes, adverse events, and dosage comparisons—information presented exclusively in tables and figures—and the system returns nothing. The answer exists in Table 4 of the report, but the ingestion pipeline treated it as non-indexable content and discarded it. After six weeks of complaints, the team rebuilds to extract tables and caption images, but researchers have already learned not to trust the system for quantitative questions. That trust never fully returns.

You are handling non-text content in 2026, and this is not an edge case. Real-world documents contain tables, images, charts, diagrams, equations, and other non-text elements that carry critical information. Ignoring this content creates blind spots in retrieval where users ask questions that the documents answer but the RAG system cannot see. Extracting and representing non-text content as text or embeddings requires format-specific techniques, quality validation, and tradeoffs between accuracy and cost. This chapter walks through table extraction and representation, image captioning for RAG, chart and graph description, handling mixed content documents, and deciding when to embed images versus text descriptions.

## Table Extraction and Representation as Text

Tables are structured data embedded in documents, typically representing relationships, comparisons, or measurements that are difficult to express in prose. A table with rows representing drugs and columns representing efficacy, side effects, and dosage contains information that is dense, precise, and queryable. Extracting this information and making it retrievable is essential for RAG systems in domains like healthcare, finance, and research.

The first challenge is extracting tables from documents. PDFs, Word documents, and HTML pages each require different table extraction approaches. In HTML, tables are explicitly marked with table tags, and extraction is straightforward using libraries like BeautifulSoup or lxml. In Word documents, tables are accessible through libraries like python-docx that expose table objects with rows and cells. In PDFs, tables are notoriously difficult to extract because PDFs do not have a table structure: they have text positioned on a page. A table is just text arranged in columns and rows visually, with no metadata indicating that it is a table.

PDF table extraction tools like Tabula, Camelot, and pdfplumber use heuristics to detect table structures: they identify gridlines, analyze whitespace patterns, cluster text by alignment, and infer rows and columns. These tools work well for clean, well-formatted tables but struggle with tables that lack explicit gridlines, tables with merged cells, or tables where text wraps within cells. Extraction accuracy varies widely based on document quality and table complexity. You must validate extracted tables and handle failures gracefully.

Once a table is extracted, you must represent it as text for embedding and retrieval. There are several strategies, each with tradeoffs. The simplest is to serialize the table as plain text with delimiters: convert each row into a line with cells separated by pipes or tabs. For example, "Drug A | 85% efficacy | 12% side effects | 50mg dosage". This representation is human-readable and can be embedded like any text chunk. The problem is that it loses table structure: there is no indication of which column headers correspond to which values, and the embedding model may not learn the relationships between columns.

A better representation is to serialize the table with column headers included in each row. For example, "Drug: Drug A, Efficacy: 85%, Side Effects: 12%, Dosage: 50mg". Each row is a self-contained record with labeled fields. This representation is more verbose but provides explicit context about what each value represents. Embeddings of these rows capture both the values and their meanings, improving retrieval for queries that reference column names.

For complex tables with hierarchical headers or multi-level indexing, you may need to flatten the hierarchy into text. A table with row groups or column groups should be expanded so that group labels are repeated for each cell. This increases verbosity but ensures that each serialized row contains all necessary context to be understood in isolation. The tradeoff is that large tables become very long text chunks, potentially exceeding chunk size limits or introducing excessive noise.

Another strategy is to convert tables into natural language descriptions. Use an LLM to read the table and generate a paragraph summarizing its content: "Table 4 compares the efficacy and side effects of three drugs. Drug A showed 85% efficacy with 12% side effects. Drug B showed 78% efficacy with 8% side effects. Drug C showed 92% efficacy with 15% side effects." This representation is highly readable and embeds well, but it loses the precise structure of the table and may introduce errors if the LLM misinterprets the data.

The production pattern is to use labeled row serialization for most tables and reserve LLM-based summarization for complex tables that are difficult to serialize. Validate extracted tables by checking row and column counts, verifying that headers are detected correctly, and sampling serialized output to ensure it is coherent. Store the original table structure in metadata so that downstream systems can reconstruct it if needed.

## Image Captioning for RAG

Images in documents serve various purposes: diagrams illustrating concepts, photographs of products or people, screenshots of user interfaces, flowcharts showing processes, and plots or charts visualizing data. For a RAG system to retrieve information from images, the images must be converted into text descriptions or embedded using multimodal models.

Image captioning is the process of generating a text description of an image using a vision model. Models like CLIP, BLIP, or GPT-4V can analyze an image and produce captions like "A flowchart showing the steps for user authentication, starting with login and ending with session creation." These captions can be embedded and indexed alongside text chunks, enabling retrieval based on image content.

The quality of image captions depends on the model and the image complexity. Simple images like product photos produce accurate captions: "A red bicycle with white tires." Complex images like dense diagrams or charts produce captions that may miss details: "A chart with multiple lines and a legend." The caption may describe the general structure but omit specific data points or labels. For charts and graphs, dedicated chart-to-text models or OCR-based text extraction may be necessary to capture quantitative information.

The production pattern is to use image captioning for photographs, diagrams, and illustrations, but use specialized techniques for charts, graphs, and tables as images. For charts, extract the chart type (line, bar, pie), axis labels, data points, and legend using a combination of OCR and chart parsing tools. Convert this into a structured text description: "A line chart showing revenue over time. X-axis: months from January to December. Y-axis: revenue in millions of dollars. Three lines representing products A, B, and C. Product A revenue increased from 5M in January to 12M in December."

For screenshots of user interfaces, OCR can extract visible text, and layout analysis can describe the structure: "A screenshot of a login page with a username field, password field, and a login button. The page header reads 'Welcome to the Application.'" This representation enables retrieval for queries about UI elements or user flows.

Image captions should be stored as metadata associated with the image or as separate text chunks with references to the source image. When a query matches an image caption, the retrieval system can return both the caption text and a reference to the original image, allowing users to view the image directly. This is critical for visual information that cannot be fully captured in text: a user asking about a system architecture diagram needs to see the diagram, not just a text description.

## Chart and Graph Description: Extracting Quantitative Information

Charts and graphs are a special category of images because they encode quantitative data visually. A bar chart comparing sales across regions, a line graph showing trend over time, or a scatter plot showing correlation between variables contains precise numerical information that is essential for answering quantitative queries. Generic image captioning models may describe the chart type but fail to extract the specific data points.

To make charts retrievable, you need to extract the underlying data or generate detailed text descriptions. For simple charts, OCR can extract axis labels, legends, and data point labels. Chart parsing libraries like ChartOCR or commercial services can detect chart type, extract axes, and read data values from the visual representation. This structured data can then be serialized as text: "Sales by region in 2024. North: 45M, South: 38M, East: 52M, West: 41M."

For more complex charts with many data points or overlapping elements, the extracted data may be incomplete or inaccurate. In these cases, LLM-based chart description is an alternative. Pass the chart image to a multimodal LLM and prompt it to describe the chart in detail, including key trends, outliers, and comparisons. The LLM generates a narrative description: "This line chart shows quarterly revenue from Q1 2023 to Q4 2024. Revenue increased steadily from 10M in Q1 2023 to 18M in Q4 2024, with a notable spike in Q3 2024 reaching 20M before declining slightly."

The tradeoff is between precision and generality. OCR and parsing produce precise data extraction but only for well-formed, simple charts. LLM-based description is more general and works for complex or unusual visualizations but may miss specific data points or introduce errors. The production pattern is to attempt automated extraction first and fall back to LLM description if extraction fails or produces low-confidence results.

Chart descriptions should include metadata about the chart type, data source, and date to enable filtering. A chart showing 2023 data should not be retrieved for queries about 2025 trends unless explicitly requested. Metadata allows retrieval logic to prioritize recent charts and filter by relevance.

## Mixed Content Documents: Text, Tables, and Images Together

Most real-world documents contain a mix of text, tables, and images. A research paper has text paragraphs, data tables, and result plots. A technical specification has text descriptions, configuration tables, and architecture diagrams. A financial report has narrative sections, financial tables, and performance charts. Ingesting mixed content documents requires extracting each type of content, representing it appropriately, and maintaining the relationships between elements.

The challenge is that tables and images are often tightly coupled to surrounding text. A paragraph may reference "as shown in Figure 3" or "see Table 2 for details." If you extract the figure or table in isolation, it loses the context provided by the referencing text. If you extract the text without the figure or table, the text is incomplete. The solution is to preserve cross-references and include contextual text with extracted tables and images.

When you extract a table, include the preceding and following paragraphs as context. If a paragraph says "Table 2 summarizes the results of the experiment," and the next element is Table 2, chunk them together or add the paragraph text to the table's metadata. This ensures that when the table is retrieved, the user receives the contextual explanation. Similarly, when you extract a chart, include the caption and any text that references the chart.

Chunking mixed content documents requires a different strategy than chunking pure text. Instead of splitting by token count, split by content elements: a chunk may contain a section of text, the next chunk may contain a table with its caption, and the next chunk may contain a chart with its description. This preserves the document structure and ensures that each chunk is a coherent unit that can be understood in isolation.

Some teams use a parent-child chunking strategy for mixed content. The parent chunk is the text section, and child chunks are the tables and images within that section. When a query matches a child chunk, the parent chunk is also retrieved to provide context. This hierarchical approach maintains the relationship between text and non-text elements.

## When to Embed Images vs Text Descriptions

The decision of whether to embed images directly using multimodal embeddings or to embed text descriptions of images depends on your use case, available models, and infrastructure constraints. Multimodal embeddings capture visual features that text descriptions cannot fully represent, but they require specialized models and increase storage and compute costs.

Multimodal embedding models like CLIP or ImageBind produce embeddings that can match both text queries and image content. A query like "system architecture diagram" can retrieve an image of an architecture diagram even if the image has no text caption. This is powerful for visual search and discovery. However, multimodal embeddings are larger than text embeddings, retrieval requires supporting both text and image embeddings, and not all vector databases support multimodal embeddings natively.

Text descriptions of images are simpler to implement and integrate with existing text-based retrieval pipelines. You generate a caption, embed the caption with your text embedding model, and index it like any other text chunk. The disadvantage is that text descriptions may miss visual features that are hard to describe: the layout of a diagram, the style of an illustration, or the specific visual appearance of a product. Queries that rely on visual similarity will not work well with text descriptions.

The production pattern in 2026 is to use text descriptions for most images, especially in domains where retrieval is based on semantic content rather than visual appearance. For images where visual features are critical, such as product photos, medical images, or design mockups, consider multimodal embeddings. For mixed use cases, index both text descriptions and multimodal embeddings, and use hybrid retrieval that combines both signals.

Another consideration is the downstream task. If retrieved images are presented to users directly for visual inspection, multimodal embeddings may improve discovery. If retrieved images are used as context for LLM-generated text answers, text descriptions are sufficient because the LLM only receives text. If your LLM is multimodal and can process images directly, you can pass the retrieved image to the LLM instead of a text description, preserving all visual information.

## Multimodal Embedding Approaches and Future Directions

Multimodal embeddings are an active research area in 2026, and production systems are beginning to adopt them for specific use cases. The core idea is to embed text, images, and other modalities into a shared embedding space where semantic similarity can be computed across modalities. A text query about "authentication flow diagram" can retrieve a diagram image, and an image of a chart can retrieve related text describing the data.

CLIP is the most widely used multimodal model, trained to align image and text embeddings. You can embed images and text with CLIP and compute cosine similarity between them. This enables cross-modal retrieval: text queries retrieving images, or image queries retrieving text. However, CLIP is optimized for general image-text pairs like photographs and captions, not for document images like charts or diagrams. Its performance on document images is mixed.

Specialized models for document understanding, like LayoutLM or Donut, are designed for documents with text, images, and layout structure. These models can process scanned documents, extract text, understand layout, and generate embeddings that capture both textual and visual information. They are particularly effective for forms, invoices, receipts, and other structured documents.

The challenge with multimodal embeddings is infrastructure complexity. You need a model that supports your content types, a vector database that can store and query multimodal embeddings, and retrieval logic that handles cross-modal queries. You also need to decide how to weight text versus image signals when both are available: should text descriptions be preferred over visual embeddings, or vice versa?

The future direction is toward end-to-end multimodal RAG systems where documents are ingested with all content types preserved, embeddings capture both text and visual information, and LLMs can process retrieved text and images together to generate answers. As multimodal LLMs like GPT-4V become more capable and accessible, this workflow will become standard. For now, most production systems rely on converting non-text content to text descriptions and using text-only embeddings.

## Non-Text Content as a Retrieval Quality Bottleneck

The pharmaceutical company that ignored tables and charts in clinical trial reports failed not because their embedding model was weak or their retrieval algorithm was flawed, but because their ingestion pipeline discarded critical information. When researchers asked quantitative questions, the answers were in tables that were never indexed. No amount of tuning retrieval parameters or prompts could recover information that was never captured.

Non-text content is a common retrieval quality bottleneck that teams overlook until users complain. Prototypes focus on text because it is easy to extract and embed. Production systems encounter real documents with tables, charts, and images, and users expect the system to retrieve information from all content types. The gap between prototype and production is often the gap between text-only and mixed-content ingestion.

When you build your ingestion pipeline, plan for non-text content from the start. Invest in table extraction, image captioning, and chart parsing. Test that your retrieval system can answer questions whose answers are in tables or charts. Measure the fraction of your corpus that is non-text content and validate that this content is represented in your index. Do not defer non-text content handling to a later phase. By the time you discover that twenty percent of user queries require information from tables or images, you have already created a blind spot that undermines user trust.

Your ingestion pipeline must handle the full complexity of real documents, not just the text. Tables, images, and charts carry information that is often more precise and valuable than prose. Extracting and representing this content is challenging, but it is non-negotiable for production RAG systems in domains where quantitative data, visual information, or structured relationships are essential. Get it right, and your retrieval system can answer the full range of user queries. Get it wrong, and users learn that your system only works for certain types of questions, and they stop trusting it for anything.
