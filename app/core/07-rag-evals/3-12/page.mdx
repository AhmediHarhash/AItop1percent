# 3.12 — Vector Search at Scale: Sharding, Replication, and Performance

Black Friday traffic spiked from 8,000 to 47,000 queries per second. The vector database, running on a single large instance with 512GB of RAM, saturated CPU, memory bandwidth, and network IO simultaneously. Query latency ballooned from 40 milliseconds to 3.2 seconds. The recommendation engine timed out. Users saw generic trending content instead of personalized recommendations. Revenue per user dropped 34 percent over six hours. The estimated loss was 2.8 million dollars. The root cause: a system designed to scale vertically with bigger machines, not horizontally with sharding and replication. Vector search at scale requires distributing data across machines, and teams that ignore this lose millions during traffic spikes.

You're building vector search systems in 2026, and you face a scaling challenge that traditional databases solved decades ago but that vector databases are still learning to handle. Storing a billion embeddings requires hundreds of gigabytes to terabytes of memory. Serving hundreds of thousands of queries per second requires massive computational throughput. Tolerating hardware failures and data center outages requires replication and failover. Scaling from thousands to millions to billions of vectors is not a matter of buying a bigger machine—it's a matter of distributing your index across multiple machines, routing queries intelligently, and replicating data for availability without destroying performance or exploding costs.

The infrastructure patterns you need are sharding to distribute data across machines, replication to ensure availability, query routing to distribute load, and performance optimization to minimize latency at scale. The tradeoffs involve consistency versus availability, query latency versus indexing throughput, and cost versus resilience. Getting this right means your system handles traffic spikes gracefully and survives partial failures. Getting this wrong means your system becomes the bottleneck that limits your application's scale and availability.

## Sharding: Distributing Vectors Across Machines

When your vector index exceeds the memory capacity of a single machine—typically 200-500 million vectors depending on dimensionality and instance size—you must shard the index across multiple machines. Sharding means partitioning your vectors into disjoint subsets, storing each subset on a separate database instance, and querying multiple shards in parallel to retrieve results across the full dataset.

The simplest sharding strategy is hash-based partitioning. You compute a hash of each document ID, modulo the number of shards, and assign that document to the corresponding shard. Document IDs hash uniformly across shards, ensuring roughly equal distribution. When a query arrives, you send it to all shards in parallel, retrieve the top K results from each shard, merge the results, and return the overall top K. This works and scales linearly—doubling the number of shards doubles your capacity and throughput—but it forces every query to touch every shard, multiplying network overhead and limiting query efficiency.

The optimized sharding strategy is semantic partitioning. You cluster your documents by topic, domain, or semantic similarity, assigning each cluster to a shard. Medical documents go to shard 1, legal documents to shard 2, technical documentation to shard 3. At query time, you route each query to the most relevant shard based on query classification or query-to-shard similarity. A query about medical procedures hits only shard 1, avoiding unnecessary work on shards 2 and 3. This reduces query fanout and latency when queries target specific domains, but it requires upfront clustering, risks load imbalance if query distribution is skewed, and complicates multi-domain queries.

One media company sharded 400 million article embeddings across 8 database instances using semantic partitioning by topic—news, sports, entertainment, business, technology, health, lifestyle, opinion. Query routing classified each user query and directed it to the top 2 most relevant shards. Average queries hit 2 of 8 shards instead of all 8, reducing query latency from 95 milliseconds to 42 milliseconds. But queries like "technology companies in the entertainment industry" required hitting multiple shards and merging results, complicating the implementation. The tradeoff was worthwhile because 78% of their queries were single-topic and benefited from reduced fanout.

The hybrid approach combines hash sharding within topic-based partitions. First-level partitioning is semantic—medical, legal, technical. Second-level partitioning within each topic is hash-based. This enables topic-aware query routing for most queries while maintaining load balance within topics. The complexity is managing a two-tier sharding scheme and ensuring that adding or removing shards does not require full reindexing.

## Replication: Availability and Read Scaling

Sharding increases capacity but does not increase availability. If any shard fails, queries touching that shard fail or return incomplete results. Replication solves this by maintaining multiple copies of each shard on different machines, typically in different availability zones or data centers. A standard configuration is 3x replication: each shard exists on three separate instances. If one instance fails, queries automatically route to the other two replicas.

Replication also enables read scaling. With three replicas per shard, you can distribute query load across replicas, tripling your query throughput for read-heavy workloads. A single shard replica handles 1000 queries per second. With three replicas and load balancing, the shard handles 3000 queries per second. This is critical for production systems where query load vastly exceeds indexing load—most vector databases are read-dominated, making replication a cost-effective way to scale query throughput without adding shards.

The cost of replication is storage—3x replication triples your storage cost—and write complexity. Every document insertion or update must propagate to all replicas. Synchronous replication waits for all replicas to acknowledge writes before completing, ensuring consistency but adding latency. Asynchronous replication acknowledges writes immediately and propagates to replicas in the background, minimizing write latency but allowing temporary inconsistency where replicas serve stale data.

For vector search, asynchronous replication is typically acceptable. Users tolerate slightly stale retrieval results—seeing yesterday's indexed documents instead of today's—in exchange for faster indexing and query performance. The exception is real-time use cases where freshness matters: fraud detection, breaking news, live social media feeds. These scenarios require synchronous replication or accept the complexity of eventually consistent reads with explicit freshness guarantees.

One financial services company replicated their fraud detection vector index 3x across availability zones with synchronous replication. Write latency for new transactions increased from 8 milliseconds to 22 milliseconds, but query availability improved from 99.5% to 99.95%, and they could tolerate any single availability zone failure without service disruption. The storage cost tripled from 1,200 dollars per month to 3,600 dollars per month, but the business cost of fraud detection downtime was orders of magnitude higher, making the investment obvious.

## Query Routing and Load Balancing

With multiple shards and multiple replicas per shard, query routing becomes critical. The naive approach sends every query to a randomly selected replica of every shard, merges results, and returns the top K. This works but wastes resources—queries often hit hot shards with high load and cold shards with idle capacity simultaneously, leading to uneven latency.

The sophisticated approach uses load-aware query routing. A routing layer tracks the current query load and latency for each shard replica, sending new queries to the least-loaded replica. If replica A is processing 800 queries per second with 50-millisecond latency and replica B is processing 600 queries per second with 35-millisecond latency, the router sends the next query to replica B. This dynamically balances load and minimizes tail latencies caused by hot spots.

Query routing also enables speculative execution for latency-critical use cases. You send the same query to two replicas of the same shard simultaneously and use whichever responds first, discarding the slower response. This doubles your query throughput consumption but cuts tail latencies dramatically—the 99th percentile latency drops because you're no longer waiting for the slowest replica. This is expensive but effective for systems where single-digit millisecond latency differences matter, such as real-time bidding or high-frequency trading.

Adaptive query routing adjusts fanout based on query complexity and latency budgets. Simple queries with high confidence scores after hitting the first shard may skip querying remaining shards. Complex queries with low confidence scores after initial retrieval may query additional shards to improve coverage. This requires a routing layer that can evaluate partial results and decide dynamically whether to continue querying or return early, adding complexity but reducing average query cost.

## Horizontal Versus Vertical Scaling

When your vector database becomes the bottleneck, you face a choice: scale vertically by moving to larger instances with more CPU, memory, and network bandwidth, or scale horizontally by adding more shards and replicas. Vertical scaling is simpler—no sharding logic, no replication complexity, no distributed query merging—but it hits hard limits. The largest AWS instance offers 768GB of RAM and 96 vCPUs, enough for roughly 400-600 million vectors depending on dimensionality. Beyond that, vertical scaling is impossible.

Horizontal scaling is more complex but unlimited. You can shard across 10, 50, or 500 instances, storing billions or trillions of vectors. The cost scales linearly with data size, and query throughput scales linearly with the number of replicas. The tradeoff is operational complexity—managing distributed systems, ensuring replicas stay synchronized, debugging performance issues that span multiple machines, and handling partial failures gracefully.

The production guidance: start with the largest instance you can afford and vertically scale until you hit memory or throughput limits. Vertical scaling avoids sharding complexity for as long as possible. When you exhaust vertical scaling, transition to horizontal scaling with sharding and replication. Plan this transition early—migrating from a monolithic instance to a sharded architecture under production load is high-risk and error-prone. One retail company waited until their single-instance vector database saturated before implementing sharding, and the migration caused three outages over two weeks as they debugged query routing bugs and replication lag.

## Cloud Cost at Scale

Vector database costs at scale are dominated by memory and compute, not storage. A billion 768-dimensional float32 vectors occupy roughly 3TB of raw data. With 3x replication, you're at 9TB. Stored on block storage at 0.10 dollars per GB per month, that's 900 dollars monthly. But vector indexes require memory, not just storage, for fast retrieval. Loading 3TB into RAM requires instances with 3TB of memory capacity, costing 1,500-2,500 dollars per month per instance depending on cloud provider. With 3x replication, you're at 4,500-7,500 dollars monthly just for memory.

Compute costs add another layer. High query throughput requires high CPU utilization across all replicas. At 10,000 queries per second, you need 20-40 large instances to maintain acceptable latency depending on query complexity and index size. At 200-400 dollars per instance per month, compute costs range from 4,000 to 16,000 dollars monthly. Combined memory and compute costs for a billion-vector system with high query throughput easily exceed 15,000-25,000 dollars per month before optimizations.

Quantization dramatically reduces these costs. Int8 quantization cuts memory requirements by 4x, reducing a 3TB float32 index to 750GB. That fits in 5-8 large instances instead of 15-20, cutting memory costs from 7,500 dollars to 2,000 dollars monthly. Binary quantization cuts memory another 4x, but recall degradation may be unacceptable. The production strategy uses int8 quantization as the default, binary quantization for first-stage retrieval in multi-stage pipelines, and float32 only for reranking small candidate sets.

Managed vector databases like Pinecone, Weaviate Cloud, and Qdrant Cloud abstract infrastructure management but charge premium pricing—0.15-0.50 dollars per GB of indexed vectors per month. For a billion-vector system, expect monthly costs between 7,500 and 25,000 dollars depending on provider and query throughput tier. The tradeoff is simplicity versus cost: managed services handle sharding, replication, scaling, and operations, but self-hosted infrastructure costs 40-60% less if you have the expertise to operate it.

## Performance Benchmarking and Tuning

Scaling vector search requires continuous performance monitoring and tuning. The key metrics are query latency at different percentiles—p50, p95, p99—and queries per second per shard. As you add data, query latency increases because approximate nearest neighbor algorithms traverse larger graphs or scan more clusters. As you add query load, latency increases due to CPU and memory contention. Understanding how these metrics degrade under load determines when you need to shard, replicate, or optimize.

Benchmarking means generating realistic query loads—representative query distributions, realistic concurrency levels, production-like data sizes—and measuring latency under those conditions. Synthetic benchmarks with uniform random queries are useless because real queries have skewed distributions—some documents are queried far more than others, some query types are more expensive than others. You need production-like benchmark workloads to identify bottlenecks.

One e-commerce company benchmarked their product search vector database using replay of production query logs. They discovered that 12% of queries—complex multi-concept searches—accounted for 58% of total query latency because those queries required deeper traversal of the HNSW graph. They optimized by caching embeddings for frequently retrieved products and pre-computing approximate results for common complex queries, reducing p95 latency from 180 milliseconds to 95 milliseconds without adding hardware.

Tuning approximate nearest neighbor algorithms involves adjusting tradeoffs between recall and latency. HNSW graphs have parameters for layer connectivity and search depth. Increasing connectivity improves recall but increases memory usage and indexing time. Increasing search depth improves recall but increases query latency. IVF indexes have parameters for number of clusters and number of clusters to probe at query time. More clusters improve precision but increase indexing cost. Probing more clusters improves recall but increases query latency. These parameters must be tuned per deployment based on your quality requirements and latency budgets.

## Handling Traffic Spikes and Autoscaling

Production vector databases face unpredictable traffic spikes—product launches, news events, marketing campaigns, seasonal peaks. Autoscaling adjusts capacity dynamically to handle spikes without overprovisioning for steady-state load. The challenge is that scaling vector databases is slower than scaling stateless web servers. Adding a shard requires rebalancing data, reindexing vectors, and updating query routing, taking minutes to hours. This latency makes reactive autoscaling ineffective—by the time new capacity comes online, the spike has passed.

The solution is proactive scaling based on leading indicators. If query volume consistently spikes on weekends, scale up on Friday evening and scale down on Monday morning. If latency starts degrading—p95 crosses a threshold—scale up immediately rather than waiting for total service degradation. If upcoming marketing campaigns are expected to drive traffic, scale up in advance and scale down after the campaign ends. This requires integrating your vector database autoscaling with business calendars and monitoring systems.

Read replicas enable fast temporary scaling for read-heavy spikes. Adding a read replica of existing shards takes 5-15 minutes and immediately increases query throughput without reindexing. After the spike, you can remove replicas to reduce cost. This is the standard pattern for handling Black Friday traffic, product launches, or viral events—pre-scale replicas before the event, monitor during the event, and scale down after.

Write-heavy spikes are harder. Adding shards requires reindexing, which takes hours for large indexes. The alternative is over-provisioning write capacity for expected peak loads or using asynchronous indexing where writes queue and process as capacity allows. One news aggregation platform over-provisioned their indexing capacity to handle 5x average load, accepting higher steady-state costs to ensure they could ingest breaking news spikes without delays. The cost was an extra 3,000 dollars monthly, but the business value of timely indexing during major news events justified it.

## The Scaling Maturity Path

Production systems scale through predictable stages. Stage one is a single monolithic instance serving thousands to tens of thousands of queries per second and storing up to a few hundred million vectors. This works for many production systems and avoids distributed complexity. Stage two is vertical scaling to the largest available instance, buying time before sharding becomes necessary. Stage three is horizontal scaling with hash-based sharding across 5-10 instances and 2-3x replication for availability, serving hundreds of thousands of queries per second and billions of vectors.

Stage four is semantic sharding with intelligent query routing, reducing query fanout and latency for domain-specific workloads. Stage five is multi-region replication for geo-distributed users and disaster recovery, accepting the complexity of cross-region consistency and latency. Most production systems never need stage five. Many systems never need stage three. The mistake is prematurely optimizing for scale you haven't reached, adding complexity that provides no immediate value and creates operational burden.

The video streaming platform that opened this chapter learned this the hard way. After their Black Friday incident, they sharded their vector database across 12 instances with 3x replication, implemented load-aware query routing, deployed autoscaling based on latency thresholds, and over-provisioned capacity to handle 2x peak load. The infrastructure cost increased from 8,500 dollars per month to 31,000 dollars per month, but they handled the next Black Friday with zero incidents and supported query volumes 4x higher than their previous peak. The following year, they optimized using int8 quantization and semantic partitioning, reducing their costs to 22,000 dollars monthly while maintaining the same capacity and performance. Scaling is expensive, but failing to scale when you need it is more expensive. Sometimes the right answer is to over-engineer for availability and scale, because downtime and degraded performance cost more than infrastructure.
