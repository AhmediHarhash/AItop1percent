# 8.6 â€” Index Maintenance: Rebuilds, Compaction, and Cleanup

In August 2025, a content recommendation platform's RAG system began experiencing mysterious performance degradation. Query latency increased gradually from 800 milliseconds to 2.4 seconds over two months. The engineering team investigated: servers had spare capacity, network latency was stable, and LLM API calls were fast. The bottleneck was the vector database. Retrieval queries that once took 120 milliseconds now took 1.8 seconds. The team inspected the database and discovered the cause: the vector index had degraded due to fragmentation and tombstoned vectors. Over six months of operation, they had updated 200,000 documents and deleted 80,000, but the database retained tombstones, markers indicating deleted vectors. The index contained 1.2 million vectors, of which 300,000 were tombstones. Every query scanned tombstones, wasting compute and memory. Additionally, the index structure had fragmented due to incremental updates, degrading search efficiency. The team initiated an index rebuild, reprocessing the corpus from scratch. The rebuild took 14 hours and temporarily halted query processing, causing a service outage. After the rebuild, query latency dropped to 600 milliseconds, faster than the original baseline. The CTO asked why they let it degrade for two months. The engineer admitted they had no index maintenance plan.

You need to understand that vector indexes degrade over time. Vector databases are not static data structures. They are dynamic, evolving as you add, update, and delete vectors. Each operation leaves traces: fragmentation, tombstones, suboptimal graph structures, bloated metadata. These traces accumulate, and performance degrades. If you do not maintain your indexes, retrieval latency increases, query throughput decreases, and memory usage grows. Maintenance is not optional. It is a scheduled operational task, like database backups or log rotation.

The content recommendation team's mistake was treating the vector index like immutable infrastructure. They assumed that once built, the index would perform consistently indefinitely. They ignored warning signs: gradual latency increases, growing memory usage, occasional timeout errors. By the time they investigated, degradation had compounded for months, requiring drastic intervention and user-facing downtime. Regular maintenance would have prevented the crisis entirely.

Tombstoned vectors are the primary source of degradation. When you delete a vector from the database, the database does not immediately remove it from disk or memory. Instead, it marks the vector as deleted, creating a tombstone. Tombstones allow the database to handle concurrent reads and writes safely: ongoing queries see a consistent snapshot, unaffected by deletes. But tombstones consume space and degrade search performance. If your index contains 10 percent tombstones, every query scans 10 percent more vectors than necessary, increasing latency and memory usage.

The team analyzed their index and found 25 percent tombstones. Their system deleted outdated content daily: news articles older than 90 days, deprecated product listings, user-generated content flagged as spam. Deletes accumulated faster than they anticipated. Each query scanned 1.5 million vectors, of which 375,000 were tombstones, wasting 25 percent of compute on irrelevant data. This overhead alone accounted for 400 milliseconds of latency.

Compaction removes tombstones and reclaims space. During compaction, the database scans the index, removes tombstoned vectors, and rewrites the index structure without them. Compaction can be incremental, running continuously in the background and cleaning up tombstones as they accumulate, or batch, running periodically and cleaning up all tombstones at once. Incremental compaction is less disruptive but may not fully eliminate fragmentation. Batch compaction is more effective but requires locking the index or running on a replica to avoid impacting live queries.

The team implemented weekly batch compaction, scheduled during low-traffic windows at 3 AM on Sundays. Compaction took 45 minutes, during which they routed traffic to a read-only replica. After each compaction, tombstone percentage dropped from 12 percent back to zero, and query latency improved by 300 milliseconds. Over time, they observed that degradation followed a predictable weekly pattern: fast queries on Sunday, gradual slowdown through the week, fast again after compaction.

Fragmented storage occurs when vectors are added, updated, or deleted in non-sequential order. Vector databases store embeddings in contiguous memory regions or disk pages for efficient access. When you update a vector, the database may allocate new storage and mark the old location as free, creating gaps. Over time, these gaps fragment storage, reducing cache efficiency and increasing I/O overhead. Fragmentation is especially severe for databases using memory-mapped files or SSDs, where fragmented access patterns degrade performance.

The team measured fragmentation using database metrics: storage utilization dropped from 92 percent to 68 percent over six months, indicating that 32 percent of allocated space was wasted on gaps and unused pages. Fragmented storage increased random I/O operations, saturating disk bandwidth and adding 200 milliseconds to query latency. Compaction alone was insufficient; they needed full index rebuilds to eliminate fragmentation.

Periodic rebuilds eliminate fragmentation and optimize index structure. A rebuild processes the current corpus from scratch, re-embedding if necessary, and constructing a new index. The new index is optimized: vectors are stored sequentially, graph structures are balanced, and metadata is compact. Rebuilds are expensive: they require re-processing the entire corpus, which can take hours or days for large corpora. But rebuilds restore index performance to baseline, often improving latency by 20 to 50 percent compared to the degraded index.

The content recommendation team scheduled monthly rebuilds, running them on a dedicated build cluster separate from production. Each rebuild took 12 hours, processing 1.5 million documents, generating embeddings, and constructing a new HNSW index. After validating the new index in staging, they promoted it to production using blue-green deployment, eliminating downtime. Post-rebuild latency was consistently 600 to 700 milliseconds, compared to 1.8 seconds before the first rebuild.

Online versus offline rebuild strategies trade availability for simplicity. An offline rebuild takes the database offline, rebuilds the index, and brings it back online. During the rebuild, queries cannot be served, causing downtime. Offline rebuilds are simple: you stop the service, rebuild the index, swap the old index for the new, and restart. Downtime can be minimized by rebuilding on a replica, then promoting the replica to primary once the rebuild completes. An online rebuild processes queries using the old index while building the new index in parallel, then atomically swaps the old index for the new. Online rebuilds avoid downtime but are complex to implement and require double the storage and compute during the rebuild.

The team chose online rebuilds using blue-green deployment. They maintained two identical production environments, blue and green. Blue served live traffic while green underwent index rebuilds. Once the rebuild completed and passed validation, they switched traffic from blue to green using load balancer configuration. The old blue environment became the standby for the next rebuild cycle. This approach required double the infrastructure but eliminated user-facing downtime.

Blue-green deployment patterns enable zero-downtime rebuilds. You maintain two environments, blue and green. Blue serves production traffic while you rebuild the index in green. Once the rebuild completes and passes validation, you switch traffic from blue to green. Blue becomes the standby environment for the next rebuild. Blue-green deployments require maintaining two full replicas of your infrastructure, doubling storage and compute costs, but they eliminate downtime and provide instant rollback if the new index has issues.

Compaction schedules balance performance and disruption. If you compact too frequently, you waste resources on redundant work. If you compact too infrequently, tombstones accumulate and performance degrades. A reasonable schedule depends on your update and delete rate. If you delete 1 percent of your corpus per week, run compaction weekly. If you delete 10 percent per month, run compaction monthly. Monitor tombstone percentage: if it exceeds 5 percent, schedule compaction. If it exceeds 15 percent, compaction is overdue.

The team monitored tombstone percentage daily using database metrics exported to their observability platform. They set alerts: if tombstones exceeded 8 percent, an automated job triggered compaction. If tombstones exceeded 15 percent, the on-call engineer was paged to investigate why automated compaction had not run. This proactive monitoring prevented degradation from exceeding acceptable thresholds.

Cleanup of orphaned vectors prevents database bloat. Orphaned vectors are embeddings stored in the vector database but no longer referenced by any document in your document store. Orphans occur when document deletions are not propagated to the vector database, or when indexing errors create duplicate embeddings. Over time, orphans accumulate, consuming storage and degrading search performance. Cleanup requires cross-referencing the vector database with the document store: for each vector, check if the corresponding document exists. If not, mark the vector for deletion and compact.

The team discovered 50,000 orphaned vectors, 4 percent of their index, caused by a bug in their deletion pipeline. When documents were deleted from PostgreSQL, the deletion event occasionally failed to reach the vector database due to message queue failures. They implemented a weekly cleanup job that queried PostgreSQL for all active document IDs, compared them to vector database IDs, and deleted orphans. This cleanup reclaimed 8 GB of storage and eliminated irrelevant retrieval results.

Maintaining index health requires monitoring key metrics: tombstone percentage, fragmentation ratio, query latency P95, query throughput, and memory usage. Tombstone percentage is the ratio of deleted vectors to total vectors. Fragmentation ratio measures how much storage is wasted due to gaps and inefficiencies. Query latency P95 and throughput measure index performance. Memory usage tracks resource consumption. If any metric exceeds thresholds, schedule maintenance.

The team built dashboards visualizing index health metrics over time. Tombstone percentage, fragmentation ratio, and P95 latency were plotted on a single chart, making degradation trends visible at a glance. They observed that latency correlated strongly with tombstone percentage: each 5 percent increase in tombstones added 150 milliseconds to P95 latency. This correlation informed their compaction schedule and alert thresholds.

Automated maintenance pipelines reduce operational burden. Instead of manually triggering compaction or rebuilds, you configure automated schedules: compact weekly, rebuild monthly, or rebuild when tombstone percentage exceeds 10 percent. Automation ensures maintenance happens consistently, even when the team is busy or distracted. Automated pipelines should include health checks before and after maintenance: validate that the index is functional, query latency is within bounds, and result quality has not degraded.

The team implemented a maintenance orchestration pipeline using Airflow. Weekly compaction tasks ran automatically at 3 AM on Sundays. Monthly rebuild tasks ran on the first Sunday of each month. Each task included pre-checks validating that replicas were healthy and post-checks verifying that latency improved. If post-checks failed, the pipeline automatically rolled back to the previous index and paged the on-call engineer. This automation reduced manual intervention from weekly to quarterly.

Incremental updates versus bulk updates impact index health. If you update documents one at a time, each update creates fragmentation and may trigger compaction overhead. If you batch updates into bulk operations, you reduce fragmentation and amortize compaction costs. For example, instead of updating 10,000 documents individually over a week, batch them and apply the updates in one operation. Bulk updates also enable more efficient index rebuilds: you rebuild once per batch instead of incrementally after each update.

The team shifted from real-time incremental updates to hourly batch updates. Documents changed frequently throughout the day, but users tolerated 1-hour staleness. Batching reduced update operations from 10,000 per day to 24 per day, cutting fragmentation by 80 percent and improving index stability. They monitored user satisfaction scores before and after the change: no degradation was observed, validating that hourly updates met user needs.

Versioned indexes enable safe rollback and experimentation. Instead of overwriting the production index during a rebuild, you create a new index version, validate it, and promote it to production. If validation fails or the new index degrades performance, you roll back to the previous version. Versioning requires additional storage to maintain multiple index versions, but it provides safety and confidence during maintenance operations. Some vector databases support versioning natively, while others require manual versioning through naming conventions or separate instances.

The team implemented index versioning using naming conventions: production index was named "vectors-v42", new index was "vectors-v43". After building v43 and validating it in staging, they updated the application configuration to route queries to v43. If issues arose, rolling back was instantaneous: revert the config change to point back to v42. They retained the previous three index versions, allowing rollback up to three months back if needed.

Testing index changes in staging before production is essential. If you rebuild the index in production and discover that query latency increased or retrieval quality degraded, you have a production incident. If you rebuild in staging, validate performance and quality, and only promote to production after validation passes, you avoid incidents. Staging validation includes load testing, quality testing, and latency testing. Load testing simulates production query volume to ensure the new index can handle traffic. Quality testing runs eval queries and compares results to the old index. Latency testing measures P95 and P99 latency under realistic load.

The team's staging validation suite included 500 eval queries covering diverse intents and document categories. After each rebuild, they ran evals comparing the new index to the previous index. Metrics tracked: retrieval recall at k=10, answer quality scores from an LLM judge, and P95 latency under 1000 queries per minute load. If any metric regressed by more than 3 percent, the rebuild was rejected and the team investigated root causes before retrying.

Capacity planning for rebuilds ensures you have sufficient resources. Rebuilding a large index requires compute, memory, and storage. If your production database has 32 GB of RAM and your rebuild process requires 64 GB, the rebuild will fail or thrash. Plan for peak resource usage: ensure rebuild servers have 2x the memory and storage of production, and allocate sufficient compute for embedding regeneration if needed. For very large corpora, rebuilds may require distributed processing: partition the corpus, rebuild partitions in parallel, and merge the results.

The team allocated dedicated rebuild infrastructure: 8 high-memory instances with 128 GB RAM each, compared to production's 64 GB instances. Rebuilds processed documents in parallel across all 8 instances, reducing rebuild time from 20 hours to 12 hours. Storage capacity was triple production to accommodate intermediate build artifacts, embeddings, and multiple index versions.

Monitoring rebuild progress prevents surprises. A rebuild that you expect to take 4 hours but actually takes 12 hours disrupts schedules and may exceed maintenance windows. Monitor progress in real time: track how many vectors have been processed, estimated time to completion, resource utilization, and error rates. If a rebuild is progressing slower than expected, investigate: is the embedding service throttled, is disk I/O saturated, or is the corpus larger than expected? Early detection allows you to abort and reschedule, rather than discovering the issue hours later.

The team instrumented their rebuild pipeline with detailed progress metrics. Every 5 minutes, the pipeline emitted metrics: documents processed, embeddings generated, index nodes built, estimated completion time. They visualized these metrics in real-time dashboards. When a rebuild slowed unexpectedly, they discovered that the embedding API was rate-limiting requests. They increased request batching from 10 to 50 queries per request, reducing API calls and completing the rebuild within the expected window.

Handling schema changes during rebuilds requires careful planning. If you change the embedding model, dimensionality, or metadata schema, you must regenerate embeddings and rebuild the index. Schema changes are expensive: they require re-processing the entire corpus, which may take days for large datasets. Plan schema changes during scheduled maintenance windows, communicate downtime to users, and validate changes in staging before production. Some teams maintain multiple indexes with different schemas, routing queries to the appropriate index based on version or feature flags, enabling gradual migration.

The team migrated from a 768-dimensional embedding model to a 1024-dimensional model, requiring a full corpus re-embedding and index rebuild. They built the new index in parallel with production, validating quality improvements in staging. Once validated, they deployed both indexes to production and gradually shifted traffic from the old index to the new using feature flags: 10 percent of traffic for one week, then 50 percent, then 100 percent. This gradual migration allowed them to detect and fix issues before full rollout.

Index compression reduces storage and memory costs. High-dimensional embeddings consume significant space: 1 million vectors at 1024 dimensions and 4 bytes per float require 4 GB of storage. Compression techniques such as quantization, dimensionality reduction, or product quantization reduce storage by 50 to 90 percent with minimal quality loss. Compressed indexes fit in smaller memory footprints, reducing infrastructure costs and enabling larger corpora on the same hardware.

The team implemented scalar quantization, reducing embedding precision from 32-bit floats to 8-bit integers. This reduced storage from 4 GB to 1 GB and memory usage from 16 GB to 4 GB, allowing them to scale their index from 1 million to 4 million vectors on the same infrastructure. Retrieval quality degraded by 2 percent, measured by recall at k=10, which was acceptable given the cost savings.

Index sharding distributes vectors across multiple nodes, enabling horizontal scaling. When your index grows beyond a single node's memory or compute capacity, you shard it: partition vectors across nodes, each responsible for a subset. Queries are distributed to all shards, results are aggregated, and top-k results are returned. Sharding complicates maintenance: compaction and rebuilds must run on each shard, and shard rebalancing may be needed if data distribution changes.

The team sharded their index into 4 nodes when it exceeded 64 GB RAM on a single node. Maintenance jobs ran in parallel across all shards: compaction on Sunday, rebuilds on the first Sunday of the month. They monitored per-shard metrics to detect imbalances: if one shard had 30 percent more vectors than others, they rebalanced by redistributing documents across shards. Sharding increased operational complexity but enabled scaling to 10 million vectors.

The content recommendation platform built a maintenance plan after their incident. They scheduled weekly compaction, triggered automatically when tombstone percentage exceeded 8 percent. They scheduled monthly index rebuilds during low-traffic windows, using blue-green deployment to avoid downtime. They monitored tombstone percentage, query latency, and memory usage daily, alerting if thresholds were exceeded. They validated rebuilds in staging before production, running load tests and quality evals. Six months later, query latency remained stable at 600 to 700 milliseconds, and they had not experienced a performance degradation incident. The VP of Engineering asked how they maintained such consistent performance. The engineer replied: "We treat indexes like we treat databases: maintain them, or they rot."

You build maintenance discipline now. You schedule compaction based on delete rates and tombstone thresholds. You schedule rebuilds monthly, quarterly, or when fragmentation degrades performance. You automate maintenance pipelines to ensure consistency. You monitor index health metrics and alert on degradation. You test rebuilds in staging before production. You plan for resource capacity during rebuilds. You version indexes to enable safe rollback.

Indexes are not static artifacts. They are living data structures that degrade, fragment, and bloat. You maintain them, or you pay the price in latency, throughput, and incidents. That maintenance starts now.
