# 6.10 â€” Building RAG Evaluation Datasets: Ground Truth Collection

In September 2025, a Series B legal tech company launched an aggressive RAG evaluation program after six months of customer complaints about answer quality. They invested forty thousand dollars in an evaluation framework, hired two ML engineers to build automated pipelines, and instrumented every component with detailed metrics. Three months later, their precision scores looked exceptional at ninety-two percent, recall hovered at eighty-eight percent, and faithfulness metrics exceeded ninety-five percent. Management celebrated the turnaround. Then a major law firm client cancelled their contract, citing the same answer quality issues that had plagued the system from the beginning.

The evaluation team discovered their fundamental mistake within hours: their entire test dataset consisted of thirty-seven questions that three engineers had written in a conference room over two afternoons. The questions bore no resemblance to actual user queries, covered only the most straightforward legal topics, and had been written by people who already knew the system's strengths and weaknesses. They had built a sophisticated evaluation infrastructure on top of a worthless dataset. Every metric was meaningless. Every optimization had been guided by false signals. Six months of work had been theater because they had skipped the hardest and most important step: building a representative ground truth dataset.

You cannot evaluate what you cannot measure, and you cannot measure what you have not labeled. Every RAG evaluation framework, every automated pipeline, every sophisticated metric ultimately depends on ground truth data: a collection of questions paired with expected answers and the documents that should support those answers. This dataset serves as the foundation for all evaluation activities. You compare system outputs against ground truth answers to measure answer quality. You compare retrieved documents against ground truth documents to measure retrieval performance. You use the dataset to detect regressions when you change the system. The quality of your ground truth dataset determines the quality of every insight you extract from evaluation.

A small, biased, or unrepresentative dataset produces misleading metrics that guide you toward optimizations that harm real-world performance. A large, diverse, carefully curated dataset reveals the true strengths and weaknesses of your system and enables you to make improvements that actually matter to users. Your evaluation dataset is not just a testing artifact. It is the compass that guides every technical decision you make about your RAG system. If the compass is broken, every subsequent navigation decision leads you further from your destination.

## The Three-Part Challenge of RAG Ground Truth

Building ground truth datasets for RAG systems presents unique challenges compared to traditional machine learning datasets. In a classification task, you label examples with categories. In a generation task, you might have multiple acceptable outputs. In RAG, you need three coordinated pieces of information for each example: a question that represents realistic user queries, an expected answer that correctly responds to that question, and a set of relevant documents that contain the information needed to construct that answer.

These three elements must be consistent with each other and with your actual document corpus. The question must be answerable from your documents. The expected answer must be grounded in those documents. The relevant documents must actually exist in your index. If any of these conditions fail, the evaluation example becomes useless or actively misleading. You are not just labeling data; you are constructing coherent test scenarios that mirror the complexity of production usage.

The interdependencies between these three components create a coordination challenge that does not exist in simpler ML tasks. Change your document corpus and some questions become unanswerable. Update your domain understanding and some expected answers become incorrect. Add new documents and the set of relevant documents for existing questions might expand. Managing these interdependencies across hundreds or thousands of evaluation examples requires systematic processes and tooling that many teams lack when they start building ground truth.

## Question Sourcing Strategies

The first decision you face is where to source your evaluation questions. The most obvious approach is to write them yourself or have domain experts write them. This method offers complete control over question distribution and ensures coverage of important topics. You sit down with subject matter experts, identify key areas of the knowledge base, and systematically write questions that probe different aspects of system functionality. For a medical RAG system, you might write questions about common diagnoses, rare conditions, drug interactions, and procedural guidelines. For a customer support system, you might write questions about product features, troubleshooting steps, billing issues, and return policies.

This approach works well for establishing baseline coverage and testing specific scenarios you know are important. The danger lies in the disconnect between what experts think users will ask and what users actually ask. Engineers and domain experts tend to write well-formed, unambiguous questions with clear answers. Real users ask vague, ambiguous, poorly specified questions with embedded assumptions and contextual dependencies. If your entire dataset consists of expert-written questions, your evaluation metrics will look great while your real users continue to struggle.

Expert-written questions also suffer from unconscious bias toward system capabilities. When you know how your RAG system works, you naturally write questions that play to its strengths. You avoid phrasing that confuses the query understanding component. You ask about topics where you know the documents are comprehensive. You structure questions in ways that align with how information is organized in your corpus. This creates evaluation datasets that systematically underestimate real-world difficulty and overestimate system performance.

Production logs provide the most realistic source of evaluation questions because they capture exactly what users actually ask. You sample questions from recent user sessions, ensuring your dataset reflects current usage patterns and vocabulary. This approach eliminates the bias introduced by expert question writing and automatically adapts as user behavior evolves. A financial services RAG system might discover that thirty percent of production queries involve comparing multiple products, twenty percent ask about edge cases in eligibility requirements, and fifteen percent are follow-up questions that reference previous conversation context. None of these patterns would appear in an expert-written dataset, yet they dominate actual usage.

Production logs also reveal the long tail of rare but important questions that stress-test system capabilities. Users ask about obscure edge cases, combine requirements in unexpected ways, and phrase questions using terminology that diverges from official documentation. These queries are precisely the ones where your RAG system is most likely to fail, making them invaluable for evaluation. An expert would never think to write "what happens if I try to return a gift card I bought with rewards points after the promotion ended but before I activated it" but users absolutely ask questions with this level of complexity and specificity.

The challenge with production-sourced questions is that they require significantly more effort to annotate. Expert-written questions come with known answers because the expert writes both the question and the expected response. Production questions must be answered by someone with domain expertise, and those answers must then be verified for accuracy and groundedness. You are trading authenticity for annotation cost. The questions are more realistic, but each one costs more to turn into a valid ground truth example.

Synthetic generation represents a third approach that has gained traction in 2025 and 2026 as LLM capabilities improved. You use a language model to generate evaluation questions based on documents in your corpus. The simplest version involves showing the model a document and asking it to generate questions that could be answered using information from that document. More sophisticated approaches generate questions that require synthesizing information across multiple documents, handling temporal reasoning, or dealing with conflicting information.

Synthetic generation enables rapid dataset creation and ensures questions are answerable from your corpus by construction. You can quickly create hundreds of questions covering all documents, guaranteeing comprehensive topic coverage. The primary risk is that synthetically generated questions may not reflect real user information needs. A model generating questions from a technical manual might create grammatically perfect questions about obscure specifications while missing the practical troubleshooting questions that real users actually care about.

Synthetic questions also tend to be too well-aligned with document structure. If your documents are organized into sections about features, benefits, and pricing, synthetic questions will cleanly ask about features, benefits, and pricing. Real users ask hybrid questions that cut across these organizational boundaries: "which features help with my specific use case and what would they cost?" Synthetic generation struggles to produce these naturalistic cross-cutting queries.

## Balancing Multiple Sources

Most mature RAG evaluation practices combine all three sources in proportion. Start with production logs to capture the core distribution of real user needs. Use expert-written questions to ensure coverage of critical scenarios that might be underrepresented in production data, such as edge cases, adversarial inputs, or recently added content. Employ synthetic generation to fill gaps in topic coverage and create stress tests for specific system capabilities.

A balanced dataset for an enterprise HR system might consist of sixty percent production queries, twenty-five percent expert-written questions targeting policy edge cases and new hire scenarios, and fifteen percent synthetically generated questions ensuring coverage across all departments and benefit categories. This combination provides both realism and coverage while keeping annotation costs manageable. You get the authenticity of real user questions, the strategic coverage of expert questions, and the comprehensiveness of synthetic questions.

The proportions should shift based on your system maturity and usage patterns. Early-stage systems with limited production traffic rely more heavily on expert-written and synthetic questions because you lack sufficient production data. As usage grows, you shift toward production sampling. Systems in rapidly evolving domains need more synthetic questions to cover new content before users have asked about it. Systems in stable domains with heavy usage can rely primarily on production sampling.

## Dataset Size and Coverage Requirements

Dataset size requirements depend on the diversity of your domain and the statistical power you need for regression detection. For a narrow domain with limited topic variation, you might achieve adequate evaluation with two hundred to three hundred examples. A medical triage system focused on a specific set of conditions could evaluate effectively with this scale if examples are well-distributed across conditions, severity levels, and patient demographics.

For broad domains with high topic diversity, you need substantially larger datasets to ensure coverage. A general-purpose corporate knowledge base spanning finance, legal, engineering, HR, and operations might require a thousand to two thousand examples to adequately represent the variety of queries users will ask. The key metric is not absolute dataset size but coverage: can you segment your dataset by important dimensions and still have enough examples in each segment to draw meaningful conclusions.

If you have only five examples related to a critical business process, you cannot reliably detect whether changes to your system improve or harm performance on that process. You need at least thirty to fifty examples per important segment to have statistical power for meaningful evaluation. This minimum segment size multiplied by the number of important segments determines your total dataset size requirements.

Statistical considerations for regression detection provide a lower bound on dataset size. Suppose your RAG system currently achieves eighty percent accuracy and you want to detect a five percentage point regression with high confidence. Basic power analysis suggests you need at least three hundred to four hundred examples to reliably detect this effect size. If you want to detect smaller regressions or need higher confidence, you need proportionally more examples.

Many teams discover this requirement the hard way: they build a dataset with one hundred examples, make a change that slightly harms the system, see their accuracy drop from eighty percent to seventy-seven percent, and cannot determine whether this represents real regression or random variation. You waste time investigating false alarms or miss real regressions because your dataset lacks statistical power. Building adequate statistical power into your dataset from the beginning saves enormous debugging time later.

## Stratification and Topic Balance

Balancing coverage across topics requires explicit stratification and monitoring. You cannot simply sample questions uniformly from production logs because production usage typically follows a power law distribution: a small number of common queries account for the majority of traffic while a long tail of rare queries occurs infrequently. If you sample uniformly, your evaluation dataset will over-represent common queries and under-represent rare but important topics.

A customer support RAG system might see forty percent of queries about password resets and account access, but these queries might be trivial to answer correctly. The challenging and business-critical queries about billing disputes, feature limitations, and integration issues might each represent only one to two percent of traffic. An evaluation dataset that mirrors production distribution will spend forty percent of its examples testing trivial password reset questions while barely covering the scenarios that actually drive customer satisfaction.

The solution is stratified sampling and intentional oversampling of important low-frequency topics. First, develop a taxonomy of query types or topics relevant to your domain. For a technical documentation RAG system, this might include installation procedures, configuration options, troubleshooting guides, API references, and best practices. Analyze production data to understand the natural distribution across these categories.

Then deliberately oversample underrepresented categories to ensure adequate coverage. You might allocate fifteen percent of your evaluation dataset to troubleshooting queries even if they represent only five percent of production traffic, because troubleshooting represents high-value interactions where failures cause significant user frustration. This intentional bias ensures your evaluation metrics reflect performance across all important scenarios rather than being dominated by the easiest and most common queries.

Maintaining this stratification requires discipline and tooling. As you add new evaluation examples, check which categories they belong to and monitor category distribution. Set target distributions for each category and flag when categories fall below their targets. This prevents evaluation datasets from drifting toward over-representing easy categories while neglecting hard ones.

## Creating Expected Answers

Creating expected answers for ground truth examples requires careful consideration of answer variability and acceptable response ranges. For factual questions with clear answers, ground truth is straightforward. If the question asks for the capital of France, the expected answer is Paris. But most RAG queries do not have single correct answers. A question about the advantages of a particular database system might have multiple valid answers depending on which advantages the system chooses to emphasize, how much detail it provides, and how it organizes the information.

You cannot write a single expected answer and evaluate system outputs with exact string matching. Instead, you need flexible evaluation approaches that accommodate answer variability while still detecting meaningful quality differences. Some teams address this challenge by creating multiple acceptable answer variants for each question, annotating common valid responses that differ in emphasis or detail level.

Others use rubric-based evaluation where human annotators score answers on multiple dimensions rather than comparing against a single gold standard. The rubric might include dimensions like factual correctness, completeness, relevance to the specific question, appropriate level of detail, and clarity of presentation. An answer receives scores on each dimension rather than a binary correct or incorrect judgment.

Still others rely on LLM-based judges that evaluate whether a generated answer captures the essential information from the expected answer while allowing for variation in phrasing and structure. The judge receives the question, the expected answer, and the system-generated answer, then determines whether the generated answer is equivalent in meaning and quality to the expected answer despite potential differences in wording.

The key is recognizing that expected answers serve as reference points for evaluation rather than exact targets for matching. Your evaluation methodology must account for legitimate answer diversity while still catching genuine quality issues. A system that consistently produces correct information in awkward phrasing should score differently than a system that produces polished phrasing with incorrect information, and your evaluation approach must distinguish these cases.

## Document-Level Annotations

Identifying relevant documents for each evaluation example provides crucial data for retrieval evaluation but requires additional annotation effort. For expert-written questions, the experts can simultaneously identify which documents should be retrieved while writing the question and answer. The expert knows which documents contain the information they used to construct the expected answer and can annotate those documents as ground truth relevant sources.

For production questions, annotators must search the document corpus to find all documents that contain information relevant to answering the question. This process is time-consuming and requires domain expertise. A medical annotator reviewing a question about drug interactions must search the knowledge base, identify all relevant drug monographs and interaction databases, and mark which specific sections of those documents contain pertinent information.

Missing relevant documents introduces false negatives in retrieval evaluation. Your system might retrieve a document that actually is relevant, but because that document was not annotated as relevant, retrieval metrics penalize the system for finding it. Incorrectly marking irrelevant documents as relevant introduces false positives. Your system retrieves only the documents you marked as relevant, receives perfect retrieval scores, but those documents do not actually contain the information needed to answer the question.

Many teams initially skip document-level annotations to reduce costs, focusing only on question-answer pairs. This prevents detailed retrieval evaluation but still enables answer quality assessment. You can measure whether the system produces correct answers without measuring whether it retrieved the right documents to construct those answers. As the system matures and retrieval becomes a known bottleneck, teams retrospectively add document annotations to a subset of examples, enabling focused retrieval optimization.

This phased approach balances annotation costs against evaluation needs. Early in system development, you care most about whether answers are correct; retrieval details matter less. Later, when answer quality plateaus, detailed retrieval data becomes valuable for identifying the next round of optimizations. You do not need document annotations for every evaluation example, but you need them for enough examples to make retrieval metrics meaningful.

## Maintaining Datasets Through Corpus Changes

Maintaining evaluation datasets as knowledge bases change presents an ongoing challenge that many teams underestimate. Your ground truth dataset reflects your document corpus at a specific point in time. When you add documents, remove documents, or update content, some evaluation examples become invalid. A question about a product feature might have a clear answer based on current documentation, but after the next product release, the feature might work differently or be deprecated entirely. The expected answer is now wrong, and the relevant documents might no longer exist.

If you continue evaluating against outdated ground truth, your metrics become misleading. System changes that correctly reflect new information will be penalized for not matching the old expected answers. You appear to be regressing when you are actually improving. Conversely, a system that continues producing outdated answers will score well on outdated ground truth while failing to help users who need current information.

Systematic dataset maintenance requires version control and periodic review. Version your evaluation dataset alongside your document corpus, clearly marking which dataset version corresponds to which corpus version. When you make significant updates to the knowledge base, trigger a review process for affected evaluation examples. This does not necessarily mean re-annotating the entire dataset. Instead, you identify examples whose relevant documents have changed and review whether those examples remain valid.

An automated check might flag any example where a relevant document has been modified or deleted, queuing those examples for human review. The reviewer determines whether the expected answer remains correct, needs updating, or whether the example should be retired because it no longer reflects realistic queries against the current knowledge base. This selective review keeps maintenance costs manageable while ensuring evaluation remains aligned with current reality.

Some teams maintain multiple evaluation datasets with different update cadences. A stable core dataset captures fundamental capabilities that should remain constant across knowledge base updates. This dataset includes questions about basic product information, common procedures, and established policies that change rarely. A volatile extended dataset reflects current state, including questions about recent features, ongoing initiatives, and temporary policies.

The stable core dataset enables long-term trend analysis, showing whether fundamental system capabilities improve or degrade over time. You can compare performance across months or years because the evaluation criteria remain constant. The volatile dataset ensures you are testing against current reality rather than outdated scenarios. When you review metrics, you examine both: the core dataset reveals whether you are maintaining baseline quality while the extended dataset shows whether you are adapting to current content.

## Documentation and Institutional Knowledge

Dataset documentation proves essential as teams grow and knowledge about dataset construction fades. Document the source of each evaluation example: was this written by an expert, sampled from production, or synthetically generated. Record the annotation process: who created the expected answer, what guidelines they followed, what date the annotation occurred. Capture the rationale for including specific examples: this example tests multi-hop reasoning, that example probes temporal handling, this one covers an adversarial input pattern.

Six months after dataset creation, when a new team member reviews evaluation results, this documentation enables them to understand why certain examples exist and what aspects of system behavior they are intended to test. Without documentation, datasets become opaque artifacts that people trust but do not truly understand. Someone sees that example 247 failed and has no idea whether this represents a critical failure or an edge case that can be deprioritized.

Documentation also supports dataset evolution. When you identify gaps in coverage, documentation helps you understand what the existing dataset already tests so you can strategically add examples that fill specific gaps rather than duplicating existing coverage. When you consider retiring outdated examples, documentation reminds you what capability or scenario that example was meant to test, helping you decide whether to update it or replace it with a new example testing the same capability.

## Avoiding One-Time Dataset Syndrome

The most common mistake teams make with ground truth datasets is treating dataset creation as a one-time project rather than an ongoing practice. They allocate two weeks at the beginning of RAG system development to create an evaluation dataset, build their entire evaluation infrastructure around that dataset, and then never meaningfully update it. Six months later, their knowledge base has doubled in size, user query patterns have evolved, and the system has been optimized for scenarios that no longer reflect reality.

Their evaluation metrics continue to look good because the system is optimized for the static test set, but production quality degrades because real-world usage has diverged from the frozen evaluation scenarios. You must treat ground truth dataset maintenance as a regular operational task, reviewing and updating examples quarterly or after major knowledge base updates. Budget time for this work the same way you budget time for infrastructure maintenance and security updates.

Building evaluation datasets feels like overhead when you are eager to build and ship features. The temptation to write thirty questions in an afternoon and call it done is powerful. Resist it. Your evaluation dataset determines whether you can reliably improve your system or are flying blind, making changes based on anecdotes and intuition. A week invested in building a high-quality, representative, well-balanced ground truth dataset will pay dividends for the entire lifecycle of your RAG system.

Every regression you catch before production, every optimization you validate with confidence, every performance report you deliver to stakeholders depends on the quality of this foundation. Build it right. You start by determining your dataset sources, choosing the balance of production queries, expert-written questions, and synthetic generation that fits your domain and resources. You develop a topic taxonomy and stratify your sampling to ensure coverage across important scenarios rather than just common scenarios.

You annotate expected answers and relevant documents with domain expertise, accepting that this process is expensive but essential. You version your dataset and establish maintenance processes to keep it synchronized with your evolving knowledge base. You document everything so future team members can understand and extend your work. This is not glamorous work. It will not appear in your company blog post about your innovative RAG architecture. But it is the difference between an evaluation system that enables confident iteration and one that produces impressive metrics while your users suffer. Choose wisely.
