# 6.8 â€” LLM-as-Judge for RAG: Designing Evaluation Prompts

Why did ninety-four percent of summaries score above 0.9 for faithfulness when manual checks revealed significant issues? A content moderation platform deployed an LLM judge with a vague prompt: "Is this summary faithful?" The judge had no definition of faithfulness, no examples, no criteria. It defaulted to lenient scoring, approving plausible-sounding text. When the team redesigned the prompt with clear definitions, explicit criteria, and examples, average scores dropped to 0.68, matching human judgment. The initial month of evaluation data was useless. Thousands of summaries had to be re-evaluated, delaying launch by six weeks. LLM judges are powerful but not magical. The quality of LLM-based evaluation depends entirely on prompt design.

You need to evaluate RAG outputs at scale, and LLM judges offer a scalable solution. But LLMs do not automatically know what to evaluate or how to score it. The quality of LLM-based evaluation depends entirely on the prompt. A well-designed prompt produces reliable scores that align with human judgment. A poorly designed prompt produces noisy scores that mislead optimization efforts. Understanding how to design evaluation prompts for faithfulness, relevance, correctness, and other quality dimensions is essential for using LLMs as judges effectively.

## Why LLMs Work as Judges

LLMs are effective judges because they excel at reading comprehension, reasoning, and comparison tasks. Evaluating RAG outputs requires reading the query, the retrieved context, and the generated answer, then reasoning about relationships between them. LLMs are trained on vast text corpora and learn to recognize patterns like contradiction, entailment, relevance, and correctness. They can handle nuanced cases that rule-based methods miss, such as paraphrasing, indirect support, and complex multi-sentence claims.

LLM judges scale better than human evaluation. Humans are the gold standard for quality assessment, but they are slow, expensive, and limited in availability. LLM judges operate at API speed, processing thousands of evaluations per hour. This scalability enables continuous monitoring, rapid iteration, and comprehensive coverage of production traffic. You can evaluate every query or sample broadly without the cost and time constraints of human annotation.

LLM judges are also more consistent than humans in some ways. Human annotators suffer from fatigue, bias, and drift. Scores vary across annotators and even across time for the same annotator. LLMs produce deterministic or near-deterministic scores when given the same input and prompt, though randomness in sampling can introduce variation. This consistency makes LLM judges useful for tracking trends and detecting regressions. If scores change, you know the inputs or system changed, not the evaluator.

However, LLM judges are not perfect. They inherit biases from training data and can be influenced by superficial features like length, formality, or fluency. They sometimes favor answers that sound confident over answers that are accurate. They struggle with tasks requiring specialized knowledge or deep reasoning. They can be fooled by plausible but incorrect content. Understanding these limitations is essential for designing prompts that mitigate weaknesses and leverage strengths.

## Judge Prompt Design for Faithfulness

Faithfulness evaluation asks whether the generated answer is supported by the retrieved context. Designing a faithfulness prompt requires defining what support means, specifying what to check, and providing examples that anchor the evaluation.

A minimal faithfulness prompt might be: "Is this answer faithful to the context?" This prompt is too vague. It does not define faithfulness, does not specify what to check, and does not clarify edge cases. The judge will interpret faithfulness inconsistently, leading to noisy scores. A better approach is to define faithfulness explicitly: "Faithfulness means every factual claim in the answer is supported by the context. A claim is supported if the context explicitly states it or if it can be directly inferred from the context without adding new information."

Next, specify what the judge should do. Instruct it to read the context and the answer, identify all factual claims in the answer, and check whether each claim is supported. For example: "Read the context and the answer. Identify all factual claims in the answer. For each claim, determine whether the context supports it. If all claims are supported, the answer is faithful. If any claim is not supported or contradicts the context, the answer is unfaithful." This step-by-step instruction guides the judge through the evaluation process, reducing ambiguity.

Provide examples of faithful and unfaithful answers. Examples serve as few-shot demonstrations that calibrate the judge's understanding of the task. For instance, show a context about return policies, a faithful answer that sticks to the stated policy, and an unfaithful answer that adds details not in the context. Include explanations: "This answer is faithful because it only includes information from the context. This answer is unfaithful because it claims a 30-day return window, but the context does not specify a timeframe." Examples anchor the evaluation and reduce the chance of misinterpretation.

Finally, specify the output format. Ask the judge to output a binary label, a score, or both a score and an explanation. For example: "Output a faithfulness score from 0 to 1, where 0 means completely unfaithful and 1 means completely faithful. Also provide a brief explanation of your score, noting which claims are unsupported if any." Explanations improve interpretability and help you validate that the judge is reasoning correctly.

A complete faithfulness prompt might look like this: "You are evaluating whether a generated answer is faithful to the provided context. Faithfulness means every factual claim in the answer is supported by the context. Read the context and answer below. Identify all factual claims in the answer. For each claim, check if the context supports it. If all claims are supported, score 1. If some claims are unsupported, score proportionally. If the answer contradicts the context, score 0. Provide a score from 0 to 1 and a brief explanation."

## Judge Prompt Design for Relevance

Relevance evaluation asks whether the retrieved context or generated answer addresses the query. Designing a relevance prompt requires defining what addressing the query means and distinguishing relevance from related concepts like topical similarity.

For context relevance, the prompt evaluates whether retrieved documents contain information useful for answering the query. A basic prompt might be: "Does this document help answer the query?" This is too vague. Help is underspecified. A better prompt defines usefulness: "A document is relevant if it contains specific information that directly answers the query or provides necessary background. A document is not relevant if it only discusses related topics without addressing the query."

Instruct the judge to compare the query and the document carefully. For example: "Read the query and the document. Determine whether the document contains information that would help answer the query. Consider whether the document addresses the question directly or only discusses related topics. Score 1 if the document is highly relevant, 0.5 if it is somewhat relevant, and 0 if it is not relevant." This instruction provides a graded scale and clarifies what each score means.

For answer relevance, the prompt evaluates whether the generated answer addresses the query. A good prompt might be: "Read the query and the answer. Determine whether the answer directly addresses the question asked. An answer is relevant if it provides the information requested. An answer is not relevant if it discusses related topics, provides background without answering, or ignores the question. Score 1 if the answer fully addresses the query, 0.5 if it partially addresses it, and 0 if it does not address it."

Examples are especially important for relevance because the boundary between relevant and not relevant can be subtle. Show a query asking for a specific date, a relevant answer providing the date, and an irrelevant answer discussing the event without mentioning the date. Include explanations: "This answer is relevant because it directly provides the requested date. This answer is not relevant because it discusses the event but does not answer the question about the date." Examples help the judge distinguish between topical similarity and actual relevance.

## Judge Prompt Design for Correctness

Correctness evaluation asks whether the generated answer matches the ground truth answer. Designing a correctness prompt requires defining what matching means and handling cases where answers are semantically equivalent but phrased differently.

A basic correctness prompt might be: "Is this answer correct?" This is too vague without providing ground truth. A better prompt includes ground truth and defines correctness: "You are given a query, a generated answer, and a ground truth answer. Determine whether the generated answer is correct by comparing it to the ground truth. Answers are correct if they convey the same information, even if phrased differently."

Instruct the judge to compare semantic meaning, not exact wording. For example: "Read the generated answer and the ground truth answer. Determine whether they convey the same information. Focus on meaning, not wording. If the answers are semantically equivalent, score 1. If the generated answer is partially correct, score proportionally. If the generated answer is incorrect, score 0. Provide a score from 0 to 1 and a brief explanation."

Handle edge cases explicitly. For instance, what if the generated answer is more detailed than ground truth? Is it still correct, or is extra detail a problem? Define this in the prompt: "If the generated answer includes additional correct information not in the ground truth, consider it correct as long as the core information matches. If the generated answer omits key information, score it as partially correct." Clarifying edge cases reduces ambiguity and improves scoring consistency.

Provide examples showing correct, partially correct, and incorrect answers. For a query about product availability, show a ground truth answer listing three colors, a correct generated answer listing the same three colors in different order, a partially correct answer listing only two colors, and an incorrect answer listing wrong colors. Include explanations for each score. Examples calibrate the judge and ensure it applies the correctness definition consistently.

## Calibrating Judge Scores

Calibration ensures that LLM judge scores align with human judgment. Without calibration, judge scores might be systematically too high, too low, or noisy. Calibrating involves comparing judge scores to human scores on a sample of outputs, measuring agreement, and adjusting prompts or thresholds to improve alignment.

Start by collecting human scores on a representative sample. Have human annotators evaluate a few hundred query-context-answer triples using the same rubric you want the LLM judge to follow. Collect both scores and explanations. This human-labeled data serves as ground truth for calibration. Ensure the sample covers a range of quality levels, from excellent to poor, and includes edge cases that are difficult to score.

Run your LLM judge on the same sample and compare its scores to human scores. Compute agreement metrics like Pearson correlation, Spearman correlation, or mean absolute error. High correlation indicates the judge is aligned with humans. Low correlation indicates misalignment. Examine cases where judge and human scores disagree significantly. Are there patterns? Does the judge consistently overestimate or underestimate certain types of outputs? Understanding disagreement patterns guides prompt refinement.

Adjust the prompt based on disagreement analysis. If the judge is too lenient, add stricter criteria or examples of low-quality outputs that should receive low scores. If the judge is too harsh, clarify that certain variations are acceptable or provide examples of acceptable outputs. If the judge is inconsistent, simplify the task, provide more examples, or break the evaluation into smaller sub-tasks. Iterate on the prompt, re-run the judge, and re-measure agreement until alignment is satisfactory.

You can also calibrate by adjusting score thresholds. Suppose the judge produces scores on a 0 to 1 scale, but you want binary classifications. You need to choose a threshold, such as 0.7, above which outputs are considered good. Calibrate this threshold by finding the value that maximizes agreement with human binary labels. Plot precision and recall at different thresholds and choose the threshold that balances both based on your application requirements.

## Multi-Judge Approaches

Using multiple LLM judges can improve evaluation robustness. Different judges might have different strengths and weaknesses. Aggregating their scores reduces the impact of individual judge errors and provides more reliable signals. Multi-judge approaches are especially valuable when evaluation is high-stakes and errors are costly.

One approach is to use multiple prompts for the same task. Design several faithfulness prompts with different phrasings, levels of detail, or examples. Run all prompts on the same outputs and aggregate their scores by averaging or voting. If most judges agree on a score, confidence is high. If judges disagree, the case is ambiguous and might require human review. Multi-prompt approaches catch cases where a single prompt is misleading or biased.

Another approach is to use multiple models as judges. Run the same evaluation prompt on different LLMs, such as GPT-4, Claude, and Llama. Aggregate scores across models. Different models have different training data, architectures, and biases. Aggregating across models reduces the impact of model-specific weaknesses. However, using multiple models is expensive, so this approach is best reserved for high-stakes evaluations or for validating that evaluation is robust across judges.

A hybrid approach combines LLM judges with rule-based or heuristic methods. For example, use an LLM judge to score faithfulness and a separate NLI model to detect contradictions. If the LLM judge scores high but the NLI model detects contradictions, flag the output for review. Hybrid approaches leverage complementary strengths and provide layered defenses against evaluation errors.

## Agreement Rates and When Judges Disagree with Humans

LLM judges do not always agree with humans. Understanding agreement rates and the sources of disagreement helps you interpret judge scores correctly and decide when to rely on judges versus humans.

Agreement rates between LLM judges and humans vary depending on the task, prompt quality, and judge model. For straightforward tasks like binary faithfulness on simple answers, agreement rates can exceed 85%. For complex tasks like graded correctness on nuanced answers, agreement rates might be 60% to 70%. Measuring agreement rates on your specific task and data is essential for understanding judge reliability.

Disagreement occurs for several reasons. One reason is ambiguity. Some cases are genuinely hard to score, and even humans disagree. LLM judges reflect this ambiguity. If human annotators agree only 70% of the time, expecting LLM judges to agree with humans more than 70% is unrealistic. Another reason is different interpretations of the rubric. Judges might interpret faithfulness or correctness differently than humans, even with explicit definitions. Calibration through examples reduces this gap but does not eliminate it.

LLM judges also disagree with humans due to superficial biases. Judges might favor longer answers, more formal language, or answers that sound confident. Humans might penalize these same features if they obscure meaning or introduce verbosity. Understanding these biases helps you interpret judge scores and adjust prompts to mitigate them. For example, if judges favor length, instruct them to ignore answer length and focus only on content quality.

When judges disagree with humans, investigate why. Sample disagreement cases and review them manually. Are judges making systematic errors, or are disagreements random? Systematic errors suggest prompt issues that can be fixed. Random disagreements suggest inherent task difficulty or ambiguity. For systematic errors, refine prompts and re-calibrate. For random disagreements, accept the noise and use judges for trends and large-scale patterns rather than individual case adjudication.

## Cost of LLM Evaluation at Scale

LLM-based evaluation is not free. Every evaluation call costs money and adds latency. Understanding the cost structure and optimizing for efficiency is essential for deploying LLM judges at scale.

Cost depends on the model you use, the length of inputs, and the number of evaluations. Larger, more capable models like GPT-4 or Claude Opus cost more per token than smaller models. Evaluating long contexts and answers costs more than evaluating short ones. Running judges on every production query costs more than sampling. For a system serving one million queries per month, evaluating all queries with an LLM judge might cost thousands to tens of thousands of dollars monthly, depending on model and input length.

To reduce costs, sample rather than evaluating everything. Evaluate 1% or 10% of production queries, or evaluate only queries flagged by cheaper heuristics as potentially problematic. Sampling provides sufficient signal for monitoring trends and detecting regressions without the cost of exhaustive evaluation. Stratified sampling ensures you cover different query types and user segments, providing representative coverage even with reduced volume.

Use cheaper models when possible. For straightforward evaluation tasks, smaller models or instruction-tuned models might suffice. Test whether a cheaper model produces scores that correlate well with a more expensive model or with human judgment. If correlation is high, switch to the cheaper model. If not, use the expensive model only for difficult cases and the cheap model for routine cases, creating a tiered evaluation strategy.

Cache evaluation results when possible. If the same query-context-answer triple appears multiple times, cache the evaluation score and reuse it. Caching reduces redundant API calls and cuts costs significantly for systems with repeated patterns. However, caching assumes evaluation is deterministic, which is not always true for LLMs with sampling-based generation. Use caching carefully and validate that cached scores remain valid over time.

Finally, consider running LLM judges asynchronously. Real-time evaluation adds latency to user-facing responses. If evaluation is for monitoring rather than filtering, run it asynchronously after serving the response to the user. This decouples evaluation cost and latency from user experience, allowing you to evaluate comprehensively without degrading performance.

## Moving Forward

LLM-as-judge is a powerful tool for RAG evaluation, providing scalable, flexible scoring for faithfulness, relevance, correctness, and other quality dimensions. However, judge quality depends entirely on prompt design. Well-designed prompts define the task clearly, provide step-by-step instructions, include examples, and specify output formats. Poorly designed prompts produce noisy, unreliable scores.

Calibrating judge scores against human judgment ensures alignment and reliability. Multi-judge approaches improve robustness by aggregating across prompts or models. Understanding agreement rates and disagreement sources helps you interpret judge scores correctly and decide when to rely on judges versus humans. Managing cost requires sampling, using cheaper models, caching, and asynchronous evaluation.

LLM judges are not a replacement for human evaluation, but they are a powerful complement. Use judges for continuous monitoring, rapid iteration, and broad coverage. Use humans for ground truth creation, calibration, and adjudication of difficult cases. Together, LLM judges and human evaluation provide a comprehensive, scalable approach to measuring and improving RAG quality. The next section covers human evaluation protocols, completing the evaluation stack.
