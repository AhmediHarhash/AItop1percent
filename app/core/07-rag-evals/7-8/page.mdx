# 7.8 — Cross-Language and Multilingual RAG Testing

In January 2026, a global HR platform deployed a RAG-powered employee handbook assistant supporting English, Spanish, French, German, and Mandarin. The system worked well for English users. Spanish users reported that answers were generic and missed policy details. French users reported that some answers mixed French and English mid-sentence. German users reported that compound words were tokenized incorrectly, breaking retrieval. Mandarin users reported that the system could not retrieve documents at all and defaulted to English responses.

The engineering team had tested the system in English extensively. They assumed multilingual support would "just work" because they used a multilingual embedding model and a multilingual LLM. They did not test cross-language retrieval: queries in one language, documents in another. They did not test language-specific chunking: French legal text requires different splitting logic than English. They did not test translation quality: whether generated answers in Spanish accurately reflected the source documents. They learned that multilingual RAG is not English RAG plus a multilingual model. It is a different system with different failure modes requiring different tests.

Testing multilingual RAG means verifying that every language you support actually works: retrieval finds relevant documents, chunking preserves meaning, generation produces coherent answers, citations are accurate. It means testing cross-language scenarios where query and documents do not match. It means testing language-specific edge cases that do not exist in English. Multilingual testing is not optional if you serve global users. It is the difference between "supports Spanish" on your feature list and Spanish users actually getting correct answers.

## Testing Cross-Language Retrieval: Query in English, Docs in French

Cross-language retrieval is the scenario where users query in one language but relevant documents are in another. A French employee asks "Quelle est la politique de télétravail?" but the remote work policy document is in English. Does retrieval work?

The naive approach is to require query and documents to match language. English queries retrieve English documents. French queries retrieve French documents. This fails when your corpus is multilingual and users do not know which language contains the answer. It also fails when users are multilingual and query in their preferred language regardless of document language.

The robust approach is cross-language embeddings: embed queries and documents in a shared multilingual semantic space where "remote work policy" and "politique de télétravail" are close vectors. Modern multilingual models support this. But they do not support it equally well for all language pairs.

Testing cross-language retrieval means creating a test corpus with documents in multiple languages and queries in multiple languages. You write a query in English that should retrieve a French document. You verify retrieval succeeds and the French document ranks high. You test all language pairs you support: English to French, French to English, English to Spanish, Spanish to French, etc.

You measure cross-language retrieval quality compared to same-language retrieval. If English-to-English retrieval has 90 percent precision and English-to-French retrieval has 60 percent precision, you have a significant gap. You either accept the gap, tune embeddings to improve cross-language alignment, or implement translation strategies.

You test query translation as an alternative to cross-language embeddings. Before retrieval, translate the query to all supported languages. Retrieve in each language. Merge results. This increases recall at the cost of latency and complexity. You test whether translation improves retrieval quality enough to justify the cost.

You test edge cases: queries in language A retrieving documents that code-switch between languages A and B. Queries in regional variants: European Spanish versus Latin American Spanish. Queries in languages with multiple scripts: Serbian Cyrillic versus Serbian Latin.

You test cross-language retrieval failure modes. What happens when a query is in a language you do not support? Does retrieval fail gracefully, or does it return random documents? You issue queries in unsupported languages and verify the system acknowledges it cannot help rather than hallucinating answers.

## Multilingual Embedding Quality: Testing Semantic Similarity Across Languages

Multilingual embeddings map text from different languages into a shared semantic space. The quality of this mapping determines whether cross-language retrieval works.

Testing embedding quality starts with parallel corpora: documents with the same meaning in multiple languages. You embed "remote work policy" in English and "politique de télétravail" in French. You measure cosine similarity. High similarity means the embedding model understands they are semantically equivalent. Low similarity means the model does not align the languages well.

You test embedding quality across all language pairs you support. Some models align English-French well but English-Mandarin poorly. You test each pair and measure alignment quality. You identify weak pairs and decide whether to accept lower quality, use different embeddings, or implement translation.

You test domain-specific embedding quality. General-purpose multilingual models are trained on web text, Wikipedia, news. They might not align domain-specific terminology well. You test whether your HR jargon, legal terminology, or technical vocabulary is well-aligned across languages. You create test pairs of domain-specific terms and measure similarity.

You test embedding quality for rare languages or low-resource languages. If you support Swahili or Telugu, you test whether the embedding model handles them well. Low-resource languages often have poor embedding quality because training data is scarce.

You test compositionality: whether multi-word phrases embed well. "Maternity leave policy" in English should be close to "política de licencia de maternidad" in Spanish. You test phrase embeddings and verify similarity is high.

You test negation and modifiers. "Allowed" versus "not allowed" should have different embeddings. You test whether multilingual models correctly handle negation and modifiers in each language. Some languages express negation differently, and models do not always capture this.

You benchmark against multilingual retrieval datasets. MIRACL, Mr. TyDi, and XOR-TyDi are standard benchmarks for cross-language retrieval. You evaluate your embedding model on these benchmarks and measure performance. This gives you an objective comparison to state-of-the-art models.

## Language-Specific Chunking: Sentence Boundaries, Compound Words, Scripts

Chunking logic that works for English often fails for other languages. Testing language-specific chunking means verifying that chunk boundaries make sense in each language.

English chunking often splits on sentence boundaries using periods. This fails for languages where periods are rare or used differently. German uses periods in abbreviations more than English. Chinese uses different punctuation. Testing means verifying that your chunker correctly identifies sentence boundaries in each language.

Some languages have compound words that should not be split. German creates long compounds like "Arbeitnehmerüberlassungsgesetz." If you chunk by word count and split the compound, you lose meaning. Testing means verifying that compounds are preserved or split semantically.

Some languages use non-Latin scripts. Arabic is right-to-left. Mandarin does not use spaces between words. Thai and Khmer require special tokenization. Testing means verifying your chunker handles these scripts correctly, preserving reading order and semantic units.

Language-specific chunking also means handling code-switching: documents that mix languages. A Spanish document might include English product names or acronyms. Testing means verifying that code-switching does not break chunking or retrieval.

You test chunk length consistency across languages. The same semantic content might be 100 tokens in English and 150 tokens in German due to compound words. If you enforce a fixed token limit, German chunks contain less semantic information. You test whether fixed token limits cause quality issues and whether you need language-specific limits.

You test chunking for documents that mix scripts. A document might be primarily English but include Chinese names or Arabic numerals. Testing means verifying these mixed-script documents chunk correctly without corruption.

## Translation Quality in Generated Answers

When the query and documents are in different languages, the generator must translate or generate in the query language. Testing translation quality means verifying answers are accurate, fluent, and semantically equivalent to the source.

You test answer translation by comparing against human reference translations. You have a French query, an English document, and a French answer generated by the system. You also have a human-translated French answer. You measure similarity: BLEU score, semantic similarity, or human judgment. High similarity means translation quality is good.

You test whether translation preserves factual accuracy. The English document says "30 days." The French answer should say "30 jours," not "3 jours" or "trente jours ouvrables." You test that numbers, dates, and named entities are translated correctly.

You test whether translation preserves tone and formality. Some languages have formal and informal registers. French uses "tu" versus "vous." Japanese uses different levels of politeness. You test whether generated answers match the expected formality for your domain.

You test whether generated answers are fluent and natural. Machine translation often produces grammatically correct but awkward text. You test whether answers sound like they were written by a native speaker or like they were translated by a machine. You use human evaluators or fluency metrics.

You test translation of technical terminology. Domain-specific terms often do not translate directly. "Vesting" in English might not have a one-word Spanish equivalent. You test whether the system translates technical terms correctly or provides explanations when direct translation is not possible.

You test translation failure modes. What happens when a term is untranslatable? Does the system use the English term, provide a definition, or skip it? You test edge cases and verify the system degrades gracefully.

## Testing Cross-Language Citation and Source Attribution

Citations are challenging in multilingual RAG. The query is in French, the document is in English, the answer is in French. How do you cite the source clearly?

You test that citations correctly reference source documents regardless of language mismatch. A French answer should cite the English document title, page number, and section. The citation should be unambiguous: users should be able to find the source even if they do not speak English.

You test citation format consistency. Do you translate document titles in citations, or use original language? Both are defensible. You choose a policy and test that it is applied consistently. You verify users can interpret citations.

You test citation accuracy when translation is involved. If the answer says "30 jours," the citation should point to the section that says "30 days." You verify the cited section actually supports the claim in the answer, even when languages differ.

You test what happens when cited documents are not available in the user's language. The answer cites an English document. The user clicks the citation. Do you provide the English document, a machine translation, or a message that translation is not available? You test that the experience is clear and helpful.

You test multilingual snippet extraction. Some systems show snippets of cited documents alongside citations. If the query is in French and the document is in English, do you show the English snippet, translate it, or skip snippets? You test each approach and verify it provides value to users.

## Building Multilingual Test Suites: Coverage Across Languages

A multilingual test suite covers all languages you support and all cross-language scenarios.

You start with a core test set in English. You translate it to all supported languages. You now have parallel test sets: the same queries and expected answers in multiple languages. You verify that each language performs as well as English.

You add language-specific test cases for edge cases unique to each language. German compound words. French gendered nouns. Mandarin characters versus pinyin. Spanish regional variants. You test that your system handles these correctly.

You add cross-language test cases: queries in language A expecting documents in language B. You cover all important language pairs. You do not test all NxN pairs if you support N languages, but you test the pairs your users actually use.

You automate multilingual tests and run them in CI. When you change chunking logic or update embeddings, you run tests for all languages. You catch regressions in any language before shipping.

You track per-language quality metrics. You measure precision, recall, fluency, citation accuracy separately for each language. You identify which languages perform well and which need improvement. You prioritize optimization for the languages your users care about most.

You expand test coverage as you add languages. When you add support for a new language, you add test cases for it. You do not ship a language until it passes quality thresholds.

## Practical Challenges: Labeling, Evaluation, and Linguistic Diversity

Multilingual testing is harder than English-only testing because it requires linguistic expertise you might not have in-house.

Labeling multilingual test data requires native speakers. You cannot write high-quality French test cases if you do not speak French fluently. You hire native speakers or use translation services to create test cases. This is expensive but necessary.

Evaluating multilingual answers requires human judgment in each language. Automated metrics like BLEU score are imperfect proxies for quality. You need human evaluators who speak each language to assess fluency, accuracy, and naturalness. You run periodic human eval campaigns for each language.

Linguistic diversity means some languages are structurally different from English. Translating English test cases might not cover language-specific phenomena. You work with linguists or native speakers to identify language-specific test cases that do not have English equivalents.

Some teams use professional translation services for test data creation. You write English test cases and pay for professional translation to all supported languages. This is expensive but produces high-quality parallel test data.

Other teams use back-translation to validate translation quality. You translate a test case from English to French, then back to English. If the back-translated English matches the original, translation quality is likely good. If it differs significantly, translation might be lossy.

You test incrementally. You do not launch 10 languages simultaneously. You launch one non-English language, test it thoroughly, learn from the experience, then launch the next. Each language teaches you something about what to test and how to test it.

The HR platform rebuilt their system with multilingual testing. They created a 200-query test set in English and professionally translated it to Spanish, French, German, and Mandarin. They added language-specific test cases: 30 for Spanish, 25 for French, 40 for German handling compound words, 50 for Mandarin handling character-based retrieval.

They tested cross-language retrieval for all pairs. They discovered English-to-Spanish worked well, English-to-Mandarin worked poorly. They implemented query translation for Mandarin queries to improve retrieval. They tested chunking for each language and discovered their German chunking was splitting compounds incorrectly. They fixed it and retested.

They hired native speakers to evaluate answer quality. French answers scored 7.8 out of 10 for fluency. They iterated on prompt engineering and improved to 8.6. Spanish citation accuracy was 82 percent. They fixed a bug in citation extraction and improved to 94 percent.

They ran multilingual tests on every deployment. They caught a regression where a dependency update broke Mandarin tokenization. They caught a bug where French answers were sometimes generated in English. They verified each fix with the test suite before redeploying.

Six months later, multilingual quality metrics matched English: precision 88 percent across languages, fluency scores above 8.5, citation accuracy above 92 percent. Spanish, French, German, and Mandarin users reported the same level of satisfaction as English users. The system truly supported multiple languages, not just English with aspirational flags for other languages.

Cross-language and multilingual RAG testing is the discipline of verifying that your system works in every language you claim to support. It is testing cross-language retrieval, embedding alignment, chunking logic, translation quality, and citation accuracy for each language and each language pair. Teams that test multilingually ship systems that global users trust. Teams that only test English ship systems that work for English users and fail for everyone else. The difference is deliberate, comprehensive testing that respects linguistic diversity.
