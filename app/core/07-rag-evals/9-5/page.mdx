# 9.5 â€” RAG Over Code: Searching and Understanding Codebases

Code is not prose. It has structure, dependencies, call graphs, and data flow that arbitrary line-count chunking destroys completely. Treating code like documentation and applying standard RAG chunking strategies produces a system that retrieves random snippets instead of understanding logic flow, function relationships, and dependency chains. RAG over code requires structure-aware chunking and specialized retrieval that respects how code actually works.

The system would retrieve random snippets of code that mentioned authentication or databases, but it couldn't understand the actual logic flow, function call relationships, or data dependencies. An engineer asked "What functions call the sendEmail utility?" expecting a comprehensive list of every call site in the codebase. The system retrieved code files that imported sendEmail, but it missed most of the actual call sites because they weren't in the same chunk as the import statement. The chunks were arbitrary line-count windows that split functions, broke logical units, and destroyed the semantic structure that makes code comprehensible.

Another engineer asked "How does user registration work end-to-end?" The system retrieved fragments from different parts of the registration flow, completely out of order, with no context about how they connected. The engineer got the HTTP endpoint handler, a database schema definition, a validation function, and an email template, but no explanation of how these pieces fit together. It was like getting random pages from different chapters of a book and being asked to understand the plot. The semantic similarity search had found relevant code, but relevance alone wasn't enough to understand code architecture and flow.

After two months of frustrated engineers complaining that the system was worse than just using grep, the team held a postmortem. The consensus was damning: the AI assistant didn't understand code. It understood text that happened to be code, treating it like prose, searching for semantic similarity in what was fundamentally a structured, syntactic artifact. They had built a document search engine over code, and it failed because code isn't documents. Code has explicit structure defined by abstract syntax trees. It has semantic relationships through imports, function calls, class inheritance, and variable references. It has execution flow and data dependencies. You can't understand code through semantic similarity alone. You need to understand its structure.

The engineers realized their fundamental mistake: they had treated code like prose. They needed to rebuild from first principles, respecting what makes code different from natural language.

## Code-Specific Chunking

The first challenge is chunking. Naively chunking code by line count or character count breaks semantic units. A function might be split across two chunks, losing the context needed to understand what it does. Local variables defined at the top of a function become separated from their usage. Class methods get divorced from their class context. The software company switched to AST-aware chunking, where chunks correspond to meaningful syntactic units: individual functions, class definitions, or logical blocks.

They used tree-sitter, a parsing library that works across multiple programming languages, to parse their codebase into abstract syntax trees and extract these semantic units. Each function became a chunk, complete from signature to closing brace. Each class definition became a chunk, including all its methods and properties. Top-level module code became chunks based on logical boundaries: imports together, configuration together, initialization code together. This meant chunks varied dramatically in size, from ten-line utility functions to three-hundred-line complex methods, but each chunk was a semantically coherent unit that could be understood in isolation.

AST-aware chunking immediately improved retrieval quality. When someone asked about JWT validation, the system retrieved the entire validateJWT function, not random snippets that happened to mention JWT. When someone asked about database modifications, the system retrieved complete database access functions with their full logic, not fragments that referenced tables. The chunks now represented actual code constructs that developers reason about: functions, classes, modules. This alignment between chunk boundaries and semantic boundaries made retrieved code actually useful.

But they discovered that chunking alone wasn't enough. A function retrieved in isolation often depended on other functions, imported types, or configuration values. Understanding what the function does required understanding its dependencies. They needed to capture not just individual code units but the relationships between them.

## Building a Code Knowledge Graph

The next enhancement was building a code knowledge graph. They extracted relationships from the codebase: function calls, class inheritance, module imports, variable definitions and usages. They created nodes for every function, class, and module, and edges for every relationship. Now when someone asked "What functions call sendEmail?" they could query the graph for all function nodes with a calls edge pointing to the sendEmail node. This was a graph query, not a semantic search. It returned precise results: every single call site across the entire codebase, with context about which functions made the calls.

They combined this with RAG: the graph query identified relevant functions, then those functions' code was retrieved and provided as context for the LLM to explain how sendEmail was being used. For the question "What functions call sendEmail?" the system would find all call sites via graph query, retrieve the calling functions' code, analyze the call contexts to understand the parameters being passed and the purposes of the calls, then generate a comprehensive explanation: "sendEmail is called in seventeen places across the codebase. In the registration service, it sends welcome emails. In the order service, it sends order confirmations. In the notification service, it sends various alerts. Here are the key usage patterns..." This combined graph queries with LLM synthesis.

The knowledge graph transformed the system from a search engine into an understanding engine. They could answer precise structural questions that semantic search couldn't handle: "What classes inherit from BaseModel?" "What modules import the logging utility?" "What are all the callers of this database query function?" These questions have definitive answers derivable from code structure, and the graph provided those answers reliably.

## Symbol-Level Search

Symbol-level search became another crucial capability. Code isn't just functions and classes; it's specific variable names, constants, type definitions, API endpoints, configuration keys. Engineers often search for specific symbols: "Where is DATABASE_CONNECTION_POOL defined?" "What endpoints use the UserProfile type?" "What files reference the MAX_RETRY_ATTEMPTS constant?" The company implemented symbol indexing, creating a separate index of every symbol in the codebase with metadata about its type, location, scope, and usage.

Symbol search was deterministic: you're looking for exact matches of identifier names. When a symbol was found, the surrounding code context was retrieved for the RAG system. This hybrid approach combined exact symbol lookup with semantic understanding. An engineer could ask "How is DATABASE_CONNECTION_POOL configured?" and the system would find the symbol definition, retrieve the surrounding configuration code, then explain how it was initialized, what values it used, and where it was referenced throughout the codebase.

They built a symbol taxonomy that classified symbols by kind: constants, variables, functions, classes, types, interfaces, configuration keys, API endpoints. This taxonomy enabled filtered search. "Show me all API endpoints that use authentication" would search specifically for endpoint symbols and filter for those whose code included authentication calls. This precision was impossible with pure semantic search but straightforward with structured symbol indexing.

## Multi-Strategy Retrieval

Repository-scale understanding requires multiple retrieval strategies working together. For the question "How does authentication work?" the system would use multiple approaches in parallel: retrieve documentation files mentioning authentication, find modules with "auth" in their names, identify the authentication service in the microservice architecture metadata, use the code graph to find all functions called by the auth service, and retrieve those functions' implementations. This multi-strategy retrieval provided comprehensive context: documentation for high-level explanation, architecture metadata for service boundaries, and actual code for implementation details.

The system weighted and ranked results from different strategies. Documentation was prioritized for conceptual questions. Code was prioritized for implementation questions. Graph-derived relationships were prioritized for structural questions. The ranking combined multiple signals: semantic similarity from embeddings, structural relevance from graph queries, exact symbol matches, and recency based on recent code changes. This hybrid ranking outperformed any single retrieval strategy.

For complex questions like "How does user registration work end-to-end?" the system would orchestrate multiple retrievals: find the registration API endpoint, walk the call graph from that endpoint to identify all functions in the execution path, retrieve those functions in call order, identify database operations and retrieve relevant schema definitions, find email templates or notification logic triggered during registration, retrieve configuration values that affect registration behavior. The assembled context provided a complete picture of the registration flow from HTTP request to database persistence to user notification.

## Call Graph Analysis

Call graph analysis enables answering complex dependency questions. "What happens when a user places an order?" is a question about execution flow. The system would find the order placement endpoint, walk the call graph to identify all functions invoked from that endpoint, retrieve those functions in execution order, and use that context to generate a narrative explanation of the order processing flow. This required the code graph to capture not just static calls but also the order and nesting of calls, essentially a call tree rooted at the entry point.

They implemented interprocedural analysis to build accurate call graphs. Simple function calls were straightforward, but handling dynamic dispatch, callbacks, event handlers, and dependency injection required sophisticated analysis. They used a combination of static analysis for direct calls and runtime tracing data for dynamic behavior. The hybrid approach provided reasonably complete call graphs even for codebases with significant runtime polymorphism.

The call graph enabled impact analysis. An engineer could ask "What would break if I change this function signature?" The system would find all callers of the function via the call graph, analyze how they use the function, and report potential breakages. This was invaluable for refactoring large, unfamiliar codebases where understanding dependencies was critical to safe changes.

## IDE Integration

IDE integration patterns make code RAG practical for developers. The software company built IDE plugins that integrated their RAG system into the developer workflow. Engineers could highlight a function and ask "What does this do?" or "Where is this called?" or "How do I use this?" The plugin would use the current code file as context, perform RAG retrieval augmented with code graph queries, and display results inline. This was more powerful than traditional IDE features like "find usages" because it could answer semantic questions, not just syntactic lookups.

"How do I authenticate a user?" would retrieve example code showing authentication patterns, retrieve the authentication utility functions with their documentation, find recent code that implemented authentication to show current best practices, and synthesize all of this into a practical guide with example usage. This went far beyond code search; it was code understanding powered by retrieval and generation.

They implemented contextual retrieval that used the developer's current location in the code to inform search. If an engineer was editing a file in the payment service and asked about "transactions," the system would prioritize payment-related transaction code over database transaction utilities or other unrelated uses of the term. This context-awareness made the system feel intelligent and reduced the noise of irrelevant results.

## Code-Specific Embeddings

Embedding code requires different considerations than embedding prose. Code semantics are tied to syntax and structure, not just token meanings. Two code snippets might be semantically similar if they perform the same operation even if they use different variable names or slightly different logic. The company experimented with code-specific embedding models like CodeBERT and GraphCodeBERT, which were trained on code and better captured code semantics than general-purpose text embeddings.

CodeBERT and similar models understand programming language syntax and common coding patterns. They can recognize that two functions are similar even if they use different variable names or different but equivalent implementations. They can identify code that performs the same logical operation regardless of superficial textual differences. The company found that code-specific embeddings improved retrieval quality for semantic queries like "Find code that parses JSON" or "Show examples of error handling" where exact textual matches weren't available but semantically equivalent code existed throughout the codebase.

They also experimented with hybrid embeddings that combined semantic code embeddings with structural features extracted from ASTs: function signature types, cyclomatic complexity, dependency patterns, API usage. These hybrid representations captured both what the code does semantically and how it's structured syntactically, providing richer retrieval signals.

## Handling Code Evolution

Handling code evolution is crucial. Codebases change constantly. Functions are refactored, moved, renamed, deleted. The RAG index needs to stay synchronized. The company implemented continuous indexing: every commit triggered incremental index updates. Changed files were re-parsed, their AST chunks re-embedded, and their code graph relationships updated. Deleted code was removed from the index. This kept the RAG system current with the active codebase.

They also maintained historical indexing for each release tag, allowing engineers to ask questions about specific versions: "How did authentication work in version 2.3?" or "What changed in the order processing logic between version 3.1 and 3.2?" Historical indices enabled temporal queries that compared code across versions, explaining how implementations evolved. This was invaluable for debugging regressions or understanding why certain design decisions were made.

Incremental indexing was a significant engineering challenge. They implemented change detection that identified modified functions and classes, updated only the affected nodes and edges in the knowledge graph, and re-embedded only changed code chunks. This reduced indexing overhead from reprocessing the entire codebase to processing just the changed portions, making continuous indexing practical even for their six-million-line monorepo.

## Code Documentation Context

Code documentation and comments provide valuable context beyond the code itself. The company's RAG system indexed code documentation, inline comments, README files, architecture decision records, and design documents alongside the code. When answering a question, the system would retrieve both code and documentation, using documentation for high-level explanation and code for implementation details. This mixed retrieval was more useful than code alone because it provided the "why" behind the "what."

They implemented documentation proximity ranking where code and its associated documentation were strongly linked. A function's docstring, nearby comments, and references in design docs were retrieved together with the function code. This ensured that explanations included both implementation and intent. When an engineer asked "How does the caching layer work?" they would get the cache implementation code, the architecture doc explaining the caching strategy, comments describing edge cases and performance considerations, and recent commit messages explaining recent cache optimizations.

They discovered that commit messages were surprisingly valuable context. Recent commits explained why code changed, what problems were solved, and what trade-offs were made. They indexed commit messages linked to the code they modified, allowing retrieval to include change rationale. "Why does the authentication service use this particular JWT library?" could be answered by retrieving the commit that introduced it and the commit message explaining the decision.

## Testing and Validation

Testing and validation for code RAG is different from document RAG. Ground truth is often deterministic. "What functions call X?" has an objectively correct answer that can be verified by static analysis. The company built a test suite of code queries with ground truth answers derived from static analysis tools. They measured whether their RAG system retrieved the correct code elements, not just semantically similar ones. For semantic questions like "How does feature X work?" evaluation was more subjective, using engineer ratings of answer quality and correctness.

They implemented regression testing where they maintained a set of canonical questions and their expected answers. Every index update or model change was tested against this suite to ensure quality didn't degrade. They also implemented A/B testing where engineers would use different versions of the system and rate answer quality, allowing data-driven optimization of retrieval strategies and ranking functions.

Human evaluation involved having senior engineers review system responses to complex architectural questions. These reviews identified failure modes: when the system missed critical code paths, when it retrieved irrelevant code that confused the explanation, when it failed to follow dependency chains far enough. The insights from these reviews drove continuous improvement in retrieval strategies and graph construction.

## Scale and Selective Indexing

One challenge specific to code RAG is scale. Large codebases have millions of functions, classes, and symbols. Indexing everything is expensive in both compute cost and storage. The company implemented selective indexing: public APIs and interfaces were fully indexed with high-quality embeddings and comprehensive graph relationships. Internal implementation details were indexed with cheaper methods or on-demand. Generated code and vendored dependencies were excluded from indexing or indexed with minimal detail. This reduced index size and cost while maintaining coverage of the code engineers actually cared about.

They built a prioritization system that identified important code: frequently modified files, core APIs, entry points, and shared utilities were prioritized for detailed indexing. Rarely modified legacy code, test fixtures, and build scripts were deprioritized. This pragmatic approach focused resources on the code that mattered most to daily development while still maintaining basic coverage of the entire codebase.

On-demand indexing handled edge cases. If an engineer asked about a rarely-used module that wasn't fully indexed, the system would trigger detailed indexing of that module on the fly, then cache the results for future queries. This lazy indexing balanced coverage and cost, indexing deeply only what was actually being used.

## Developer Tool Integration

Code RAG also enables powerful developer tools beyond question answering. Automatic code documentation generation: retrieve a function, its call graph, its usage examples from across the codebase, and generate comprehensive documentation explaining what it does, how to use it, what it depends on, and where it's used. Code migration assistance: retrieve old API usages, understand the context, generate updated code using new APIs with explanation of the changes. Bug explanation: retrieve the buggy code, its dependencies, recent changes that might have introduced the bug, and explain what might be wrong.

The software company built all of these tools on top of their code RAG infrastructure, treating code RAG as a platform for AI-assisted development. The same retrieval and graph infrastructure powered multiple tools: the Q&A assistant, the documentation generator, the migration tool, the bug explainer, the code review assistant. This platform approach amortized the infrastructure cost across multiple high-value applications.

They implemented a code review assistant that used RAG to analyze pull requests. When a PR was opened, the system retrieved the changed files, tests covering those files, documentation that needed updating, and similar past changes. It generated a PR description, suggested reviewers based on code ownership derived from git history and code graph analysis, and flagged potential issues like missing tests, undocumented API changes, or patterns that violated established conventions. This automated PR analysis saved hours of manual review time and caught issues earlier.

## Privacy and Security

Privacy and security considerations are heightened with code RAG. Your codebase contains sensitive business logic, proprietary algorithms, API keys, database credentials, and other secrets if developers made mistakes. If you're using external LLM APIs, you're sending code snippets to third parties. The company implemented strict data governance: only non-sensitive repositories were indexed by the cloud-based RAG system. Sensitive codebases used an on-premise deployment with local LLMs that never sent data outside the company network.

They also implemented automatic secret detection in the retrieval pipeline. Before sending retrieved code to the LLM, they scanned it for patterns matching API keys, credentials, tokens, and other secrets. If secrets were detected, they were redacted or the retrieval was blocked entirely. This prevented accidentally leaking sensitive information even if secrets had been committed to the codebase.

Access control integration ensured that engineers could only retrieve code they had permission to see. The RAG system respected the same repository access controls as their source control system. An engineer working on the frontend team couldn't retrieve code from sensitive backend repositories they didn't have access to. This permission-aware retrieval prevented information leakage across team boundaries.

## Multimodal Understanding

Multimodal code understanding is an emerging area. Code often includes architecture diagrams, data flow diagrams, sequence diagrams, UI mockups, and other visual artifacts. The company started indexing these artifacts alongside code, using vision models to extract information from diagrams and linking them to relevant code modules. When an engineer asked about system architecture, the RAG system could retrieve both the architecture diagram and the code that implemented each component shown in the diagram.

They implemented diagram parsing that extracted entities and relationships from architecture diagrams, linking diagram elements to code modules. This created a connection between high-level design artifacts and low-level implementation, enabling queries that bridged both levels: "Show me the code that implements the authentication flow depicted in this architecture diagram." The system would parse the diagram, identify the authentication components, map those to code modules, and retrieve the relevant implementations.

Database schema diagrams were linked to ORM models and database access code. UI mockups were linked to component implementations. Sequence diagrams were linked to the actual code paths they documented. This multimodal integration created a comprehensive understanding that connected design artifacts with implementation reality.

## Personalized Retrieval

Personalized code retrieval based on the developer's recent work improved relevance. If an engineer had been working on the payment service for the past week, questions about "the current service" would retrieve payment service code, not search across the entire repo. The system maintained per-developer context windows that tracked recent files viewed and edited, using that as an implicit filter on retrieval. This personalization made the system feel smarter and more contextually aware.

They implemented a relevance scoring boost for recently modified code, code in the same service or module the developer was currently working in, and code written by the same developer or their team. This personalization was subtle but effective, ensuring that the most relevant code for a developer's current work appeared first rather than requiring explicit filtering.

They also tracked per-developer query history and used it to refine understanding. If a developer frequently asked about authentication, the system would prioritize auth-related code in subsequent queries even for ambiguous terms. This personalized ranking adapted to each developer's focus areas and interests.

## Hybrid Ranking Signals

The company found that code RAG quality improved when they combined multiple signals: vector similarity for semantic search, graph queries for relationship questions, symbol search for exact matches, and recency signals for recently changed code. Their ranking function combined these signals: boost code that's semantically similar to the query, boost code that's graph-connected to already-retrieved code, boost symbols that exactly match query terms, boost code changed in the last week. This hybrid ranking outperformed any single signal.

They trained a learning-to-rank model that combined all these signals with weights optimized based on engineer feedback. When engineers marked retrieved code as relevant or irrelevant, that feedback trained the ranking model. Over time, the model learned which signals mattered most for different query types and optimized the ranking accordingly.

The ranking also considered code quality signals: well-documented code, code with high test coverage, code frequently referenced by other code, and code modified by senior engineers received ranking boosts. This ensured that retrieval prioritized high-quality examples rather than arbitrary matches.

## Impact and Adoption

After a year of development, their code RAG system became an essential developer tool. Seventy-two percent of engineers used it daily. It answered over fifteen thousand code questions per week. The average time to understand unfamiliar code dropped by forty percent, measured by time from first viewing a file to making a confident modification. New hire onboarding time decreased by three weeks because new engineers could query the codebase instead of bothering senior developers with basic questions about how systems worked.

The system wasn't perfect. It still struggled with very abstract questions like "How should I architect a new feature?" It occasionally retrieved irrelevant code when query phrasing was ambiguous. It sometimes missed subtle dependencies in dynamically typed code. But it was enormously valuable for the majority of daily developer questions: understanding what code does, finding usage examples, tracking down dependencies, explaining system behavior.

Engineers reported that the system changed how they explored code. Instead of reading through files trying to piece together understanding, they would ask directed questions and get targeted explanations. Instead of asking colleagues for help and waiting for responses, they would query the RAG system and get immediate answers. Instead of writing documentation manually, they would generate it with the documentation tool and review it for accuracy. The system became a force multiplier for developer productivity.

## Principles for Code RAG

Building code RAG requires accepting that code is fundamentally different from documents. You need AST-aware chunking that respects syntactic boundaries. You need code knowledge graphs that capture structural relationships: calls, inheritance, imports, definitions, usages. You need symbol indexing for exact identifier lookup. You need multiple retrieval strategies working together: semantic search, graph queries, symbol search, documentation retrieval. You need code-specific embeddings that understand programming language semantics. You need to handle code evolution with continuous incremental indexing. You need to integrate with developer tools and workflows. You need to respect security boundaries and prevent information leakage.

It's more complex than document RAG. Chunking is harder because code structure is rigid and meaningful. Retrieval is harder because relationships matter as much as content. Understanding is harder because code semantics derive from structure and syntax, not just natural language. But the value for engineering organizations is substantial. Code RAG transforms the codebase from a static artifact that you search with grep into an interactive knowledge base that you can query, reason about, and learn from.

For any organization with a large codebase and engineers who spend hours navigating unfamiliar code, code RAG is an investment that pays for itself in developer productivity and reduced onboarding time. The key is respecting the unique nature of code and building retrieval and understanding systems that work with code's structure rather than treating it as unstructured text. When you do that, you unlock a qualitatively different developer experience where understanding code becomes conversational and comprehensive rather than fragmentary and frustrating.

Start with the foundations: parse your code into AST-based chunks, build a knowledge graph of code relationships, implement symbol indexing, and combine these with semantic embeddings. Then layer on the developer-facing features: the Q&A assistant, the documentation generator, the code review tools. Integrate with IDEs and existing workflows. Measure impact on developer productivity and onboarding time. Iterate based on engineer feedback. Code RAG is a platform capability that enables multiple high-value applications, and treating it as such ensures you extract maximum value from the investment in building the underlying infrastructure.

The software company's journey from their initial failed attempt with naive chunking to a comprehensive code understanding platform demonstrates the importance of domain-specific design. Code isn't prose, and systems that treat it as such will fail. Systems that embrace code's structured nature, leverage its explicit relationships, and combine multiple retrieval modalities succeed in making large codebases navigable and understandable. That capability is increasingly essential as codebases grow beyond what any individual can hold in their head, and code RAG provides the solution.
