# 1.12 — The Cost of RAG: Token Economics and Infrastructure

**RAG costs six times more than teams budget for, and LLM output tokens are the smallest expense.** Teams build financial models around generation costs because that is what they read about in pricing pages. Then production traffic hits and they discover that embedding API calls consume 45 percent of the budget, vector database compute takes 30 percent, and input tokens from retrieved context cost another 20 percent. The bill is 180,000 dollars instead of 30,000 dollars, and the CFO demands explanations. Teams that model full-stack costs before launch build systems that stay within budget. Teams that optimize only LLM output get budget overruns that kill their projects.

You inherit a complex cost structure when building RAG systems. LLM output tokens are the visible cost everyone optimizes for. But RAG adds three massive cost layers: embedding API calls to vectorize documents and queries, vector database storage and compute for retrieval, and LLM input tokens from retrieved context. Each layer can dominate total cost depending on your usage patterns. Teams that don't model full cost upfront build systems that work beautifully in development and become financially unsustainable in production. The top 1 percent build cost models before writing code, instrument cost tracking from day one, and continuously optimize across all cost dimensions.

## Embedding Costs: The Hidden Per-Document and Per-Query Tax

Embedding models convert text into vectors. Every document chunk you index requires an embedding API call. Every user query requires another call. For large corpora and high query volumes, embedding costs dwarf LLM inference costs.

Document embedding happens at index time. You chunk a 100,000-document corpus into 500,000 chunks averaging 400 tokens each. That's 200 million tokens to embed. OpenAI text-embedding-3-large costs 0.13 dollars per million input tokens. Your initial indexing costs 26 dollars. This seems cheap until you realize you need to re-embed whenever documents change.

Re-indexing frequency determines ongoing embedding costs. If you re-embed your entire corpus monthly, you pay that 26 dollars every month. If you update 10 percent of documents weekly, you pay roughly 10 dollars per week or 40 dollars monthly. For systems that require real-time freshness, you're embedding documents continuously, and the cost scales with change frequency.

Query embedding costs scale with traffic. If users make 2 million queries per month and each query is 50 tokens, that's 100 million tokens monthly. At 0.13 dollars per million tokens, that's 13 dollars monthly for query embeddings. This seems negligible until you hit 100 million queries—suddenly you're spending 6500 dollars monthly just on query embeddings.

Multi-turn conversations multiply query embedding costs. If you reformulate queries based on conversation history, you're embedding longer context. A reformulated query might be 200 tokens instead of 50. That quadruples query embedding cost. Running hybrid retrieval with multiple embedding models doubles or triples it again.

Optimization starts with model selection. Smaller embedding models are cheaper. OpenAI text-embedding-3-small costs 0.02 dollars per million tokens versus 0.13 dollars for large. If quality degradation is acceptable, you save 85 percent on embedding costs. Some teams use small models for query embedding and large models for document embedding, balancing cost and quality.

Caching eliminates redundant embedding calls. If 30 percent of queries are duplicates or near-duplicates, cache their embeddings. When you see a repeated query, retrieve from cache instead of calling the embedding API. This requires a key-value store and cache invalidation logic, but the cost savings are immediate.

Self-hosting embeddings is the ultimate optimization. You run open-source embedding models on your own infrastructure. Upfront cost is higher—GPU instances, model serving infrastructure—but per-query cost drops to near zero. The break-even point depends on volume. For millions of queries monthly, self-hosting often wins. For thousands, API calls are cheaper.

## Vector Database Costs: Storage, Compute, and Index Maintenance

Vector databases store embeddings and handle approximate nearest neighbor search. Pricing models vary wildly across providers. Some charge per vector stored, some per query, some per compute hour. Understanding your usage pattern is critical for cost modeling.

Storage costs scale with corpus size and dimensionality. OpenAI text-embedding-3-large produces 3072-dimensional vectors. Storing 500,000 vectors at 3072 dimensions requires roughly 6GB of raw vector data, plus index overhead. Pinecone charges approximately 0.096 dollars per GB per month for storage. Your 500,000 vectors cost roughly 0.58 dollars monthly for storage alone.

This seems cheap until you scale. A 50 million chunk corpus is 600GB of vectors, costing 58 dollars monthly just for storage. If you're running multiple indexes—separate indexes for different languages, tenants, or data types—multiply accordingly. Dimensionality reduction can help. Using 1536-dimensional models instead of 3072-dimensional cuts storage costs in half.

Query costs vary by provider. Some charge per query, some per compute hour, some bundle queries into tiers. Pinecone's pricing includes queries in storage tier pricing. Weaviate and Qdrant run on your infrastructure, so query cost is your compute cost. Hosted services like Elastic or Postgres with pgvector bill compute time, not queries.

Index rebuilds are expensive. When you add or update large batches of documents, the vector database must rebuild its approximate nearest neighbor index. This is CPU and memory intensive. Some providers charge extra for index builds. Self-hosted databases experience latency spikes or downtime during rebuilds.

Replication and high availability multiply costs. Production systems need replicas for fault tolerance and geographic distribution. Running three replicas triples your storage and compute costs. Some teams run a single primary for writes and read replicas for queries, balancing cost and reliability.

The optimization is matching database choice to usage pattern. Low query volume with large corpus? Use a cheap storage-optimized service. High query volume with small corpus? Self-host on compute-optimized instances. Multi-tenant with strict isolation? Use separate indexes per tenant and scale granularly. Mismatching database to usage pattern wastes money.

## LLM Input Tokens: The RAG Context Cost Multiplier

RAG systems feed retrieved documents to the LLM as context. This increases input tokens dramatically compared to non-RAG systems. Input tokens cost less than output tokens, but at RAG scale, input cost often exceeds output cost.

Context size scales with retrieval strategy. Retrieving top-5 documents at 500 tokens each adds 2500 input tokens per query. Retrieving top-10 doubles that to 5000 tokens. Multi-document retrieval for complex queries can hit 10,000 to 20,000 input tokens.

Token pricing varies by model. Claude Opus 4.5 charges 15 dollars per million input tokens. GPT-4 Turbo charges 10 dollars. If you're processing 2 million queries monthly at 5000 context tokens each, that's 10 billion input tokens. At 10 dollars per million, your monthly input token cost is 100,000 dollars. Output tokens might add another 20,000 dollars. Context dominates.

Multi-turn conversations accumulate context. If you maintain conversation history and feed it to the LLM on each turn, context grows linearly with conversation length. A 20-turn conversation might have 50,000 tokens of history plus 5000 tokens of retrieved documents—55,000 input tokens for a single query. At 15 dollars per million, that's 0.83 dollars for one query's input tokens.

The optimization is aggressive context management. Retrieve fewer documents—test whether top-5 performs nearly as well as top-10. Use smaller chunks—400 tokens instead of 800. Summarize retrieved documents before feeding to the LLM—use a small model to compress 5000 tokens of context into 1000 tokens of summary.

Prompt caching helps for repeated context. If the same documents appear in many queries, prompt caching lets you pay once to process them and reuse across queries. Anthropic's prompt caching reduces cost for cached input tokens by 90 percent. This is transformative for RAG with stable document sets.

Re-ranking reduces context without reducing retrieval. Retrieve top-20 documents with cheap first-stage retrieval, re-rank them with a cross-encoder, send only top-5 to the LLM. You get better precision with less context. The re-ranking cost is tiny compared to LLM input token savings.

## Infrastructure Costs: Compute, Networking, and Orchestration

Beyond API costs, RAG requires infrastructure: application servers, databases, caching layers, job queues for async indexing. These costs are often overlooked in early projections and become significant at scale.

Application server costs scale with query concurrency. If you have 100 queries per second peak and each query takes 2 seconds end-to-end, you need capacity for 200 concurrent requests. At 20 requests per instance, that's 10 instances. AWS EC2 instances appropriate for this workload cost roughly 100 dollars per instance monthly, so 1000 dollars for compute.

Caching infrastructure reduces API costs but adds infrastructure cost. Running Redis for embedding caching and result caching might cost 200 to 500 dollars monthly depending on data size and throughput. The savings from reduced API calls must exceed caching infrastructure cost for it to be worthwhile.

Networking costs appear in high-traffic systems. Transferring data between services, regions, or availability zones incurs charges. Retrieving 5KB of documents per query at 2 million queries monthly is 10TB of transfer. Inter-region transfer costs roughly 0.09 dollars per GB, so 900 dollars monthly. Same-region transfer is free on most clouds, which incentivizes co-locating services.

Async job processing for indexing requires queues and workers. You might use SQS or RabbitMQ for job queues and background workers to process documents, chunk them, embed them, and update indexes. This infrastructure adds cost but enables scalable ingestion.

Monitoring and logging infrastructure tracks system health and cost. Storing logs, metrics, and traces costs money. Comprehensive observability might cost 5 to 10 percent of total infrastructure spend. Skimping on observability saves money short-term but makes debugging and optimization nearly impossible.

The hidden cost is engineering time. Building and maintaining RAG infrastructure requires engineers. If three engineers spend 50 percent of their time on RAG at 150,000 dollar annual cost each, that's 225,000 dollars in engineering cost annually. For low-traffic systems, engineer cost exceeds API cost. For high-traffic systems, both are significant.

## The Total Cost Breakdown: Real-World Examples

A customer support RAG system with 500,000 document chunks, 1 million queries monthly, and average 5 retrieved documents per query has the following cost structure: Document embeddings: 10 dollars monthly for incremental updates. Query embeddings: 5 dollars monthly. Vector database: 20 dollars for storage and queries. LLM input tokens at 5000 tokens per query: 50 dollars. LLM output tokens at 500 tokens per query: 15 dollars. Infrastructure: 300 dollars. Total: 400 dollars monthly, with infrastructure dominating.

An enterprise knowledge RAG system with 10 million chunks, 5 million queries monthly, multi-turn conversations averaging 5 turns, looks very different: Document embeddings: 200 dollars monthly for updates. Query embeddings with reformulation: 250 dollars monthly. Vector database at scale: 2000 dollars. LLM input tokens at 15,000 average per turn: 112,500 dollars. LLM output tokens: 22,500 dollars. Infrastructure: 8000 dollars. Total: 145,450 dollars monthly, with LLM input tokens dominating.

An e-commerce product search RAG with 2 million products, 50 million queries monthly, real-time inventory updates: Document embeddings: 500 dollars monthly for continuous updates. Query embeddings: 500 dollars. Vector database with high query throughput: 5000 dollars. LLM input tokens: 25,000 dollars. LLM output tokens: 10,000 dollars. Infrastructure and caching: 15,000 dollars. Total: 56,000 dollars monthly, with infrastructure and database dominating.

The cost structure varies wildly by use case. Low query volume means indexing costs dominate. High query volume means LLM costs dominate. Multi-turn conversations explode input token costs. Real-time updates increase embedding costs. Understanding your specific usage pattern is essential for accurate cost modeling.

## Cost Optimization Levers: Where to Squeeze

The highest-ROI optimization is usually context size reduction. Cutting retrieved documents from 10 to 5 halves input token cost with minimal quality loss in many cases. This is easy to test and deploy. Measure retrieval precision at different K values and pick the smallest K that maintains quality.

Model selection is the next lever. Using smaller, cheaper models where possible saves significantly. Query reformulation can use Claude Haiku instead of Opus. Re-ranking can use a fine-tuned open model instead of GPT-4. Generation might use GPT-4 only for complex queries and GPT-3.5 for simple ones, with query routing.

Prompt caching is transformative when applicable. If your RAG system retrieves from a stable knowledge base where documents don't change frequently, caching the document context saves 90 percent of input token cost for cached content. For systems with stable corpus and high query volume, this is the single highest-impact optimization.

Embedding model choice trades quality for cost. Test whether smaller embedding models maintain acceptable retrieval quality. The cost difference between large and small models is often 5 to 10x. If quality degradation is minimal, the savings compound across all documents and queries.

Self-hosting becomes economical at scale. If you're spending 50,000 dollars monthly on embedding API calls, investing 100,000 dollars in GPU infrastructure and engineering to self-host breaks even in two months. If you're spending 2000 dollars monthly, self-hosting is overkill.

Aggressive caching at multiple layers reduces redundant work. Cache query embeddings, cache retrieval results for common queries, cache generated answers for FAQs. Each cache layer has a hit rate and cost-saving multiplier. Achieving 30 percent cache hit rate on embeddings saves 30 percent of embedding cost for minimal infrastructure cost.

## Why RAG Is More Expensive Than Teams Expect

The fundamental mismatch is between LLM-only pricing intuition and RAG reality. Teams budget for output tokens because that's the visible cost in LLM pricing. They're used to thinking "1000 output tokens per query times query volume equals cost."

RAG adds three invisible cost layers. Embedding every document and query. Storing and querying vectors. Feeding large contexts as input tokens. Each layer can dominate depending on usage. Teams discover this in production when bills arrive at 3x to 10x projections.

The second surprise is scaling behavior. Doubling query volume doubles most costs, which is expected. But doubling corpus size only marginally increases query-time costs while doubling indexing costs. And adding multi-turn conversations quadruples input token costs due to history accumulation. Scaling behavior is non-linear and varies by dimension.

The third surprise is operational cost. Running production RAG requires monitoring, debugging, re-indexing, freshness management, and incident response. This requires engineers and tooling. For small systems, operational cost exceeds API cost. For large systems, both are significant.

Most teams also underestimate the cost of quality. Achieving high accuracy requires larger embedding models, more retrieved documents, stronger LLMs, and re-ranking—all expensive. The cheapest RAG configuration is rarely acceptable quality. The optimization challenge is finding the minimum cost that achieves acceptable quality, not finding the minimum absolute cost.

## Building Cost-Aware RAG From Day One

Cost modeling starts before coding. Estimate corpus size, query volume, average query complexity, and required freshness. Calculate embedding costs, vector database costs, and LLM costs based on these estimates. Build a spreadsheet with cost per component and sensitivity analysis for key variables.

Instrument cost tracking immediately. Tag every API call with cost metadata. Log embedding token counts, retrieval latency, and LLM token usage per query. Aggregate cost by user, query type, and time period. Without instrumentation, you can't identify what's expensive or measure optimization impact.

Set cost budgets and alerts. If your model predicts 5000 dollars monthly and you're on track for 15,000, you need to know immediately, not at month-end. Cost alerts let you investigate spikes and rollback changes that unexpectedly increase cost.

Build cost into your product roadmap. Features that require larger context, more frequent indexing, or stronger models have cost implications. Evaluate features based on value delivered per dollar spent, not just user value in isolation.

Test cost optimization strategies in staging before production. Measure quality degradation from reducing retrieved documents, using smaller models, or compressing context. If quality drop is acceptable, deploy the optimization. If not, seek other optimizations.

The top 1 percent treat cost as a first-class metric alongside latency and quality. They build dashboards showing cost per query, cost by component, and cost trends over time. They run regular cost optimization sprints where they experiment with cheaper models, caching strategies, and context reduction. They educate stakeholders about RAG cost structure so business expectations align with economic reality.

RAG is expensive because it does more work than pure LLM inference. You're embedding, retrieving, and processing large contexts. But expensive doesn't mean unaffordable. With careful architecture, aggressive optimization, and continuous cost management, you can build RAG systems that deliver value at sustainable cost. The teams that fail are those that ignore cost until production bills arrive, then scramble to retrofit optimizations into systems designed without cost constraints. Build cost awareness into your architecture from the start, and you'll avoid the painful surprises that derail so many RAG projects.
