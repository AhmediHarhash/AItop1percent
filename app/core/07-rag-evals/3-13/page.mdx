# 3.13 — Learning-to-Rank and Reranker Training Loops

In December 2025, a job search platform deployed cross-encoder reranking and immediately saw a 9% improvement in click-through rate on search results. But their data scientists noticed something puzzling: users with 10+ years of experience clicked jobs ranked 3-5 more often than jobs ranked 1-2, while users with less than 2 years of experience exhibited the opposite pattern. The cross-encoder optimized for generic semantic similarity between query and job description, but it ignored signals like years of experience, salary range, location preferences, and company size that users cared about deeply. Semantic similarity was one relevance signal among many, and the system that treated it as the only signal left performance on the table. They needed a way to combine embedding similarity with structured features, user context, and historical engagement signals into a single learned ranking model that optimized for actual user satisfaction, not just semantic closeness.

You're building retrieval systems in 2026, and you've learned that relevance is not reducible to a single similarity score. A document can be semantically related to a query but irrelevant because it's outdated, too technical for the user's expertise level, written in the wrong language, or outside the user's geographic region. A document can be less semantically similar than another but more relevant because it's from a trusted source, recently updated, or frequently clicked by similar users. The production solution is learning-to-rank: training a machine learning model to predict relevance by combining embedding similarity with dozens of other features—document metadata, user context, engagement history, query characteristics—into a single optimized ranking.

Learning-to-rank transforms retrieval from a static similarity computation into a dynamic prediction problem. You collect data about which documents users engage with for which queries, extract features from queries, documents, and users, and train a model to predict engagement likelihood. This model becomes your reranker, scoring candidates retrieved by your bi-encoder or hybrid search system and ordering them to maximize predicted user satisfaction. When you get this right, retrieval quality improves by 10-25% over pure semantic similarity, and the improvements compound as you collect more data and retrain continuously.

## Beyond Embedding Similarity: Multi-Signal Relevance

Embedding similarity captures semantic relatedness: whether the words and concepts in a query match the words and concepts in a document. This is a powerful signal but incomplete. Consider a user searching for "entry-level software engineering jobs in Seattle." A semantically similar job posting for a senior engineering role in Seattle will score high on embedding similarity but is irrelevant due to seniority mismatch. A less semantically similar posting for a junior developer role in Seattle suburbs is more relevant despite lower embedding score.

Learning-to-rank models combine embedding similarity with structured features that embeddings cannot easily encode. The job search example requires features like years of experience required, years of experience the user has, salary range, user's salary expectations, distance from user's location, company size, company industry, posting recency, and application-to-interview conversion rate for similar jobs. Each feature captures a dimension of relevance that pure semantic similarity misses.

The feature engineering process identifies signals that correlate with user satisfaction. For document search systems, relevant features include document recency, author reputation, citation count, reading level match between document and user expertise, domain authority, and multimedia content presence. For e-commerce product search, features include price, user's historical price range, product rating, number of reviews, seller reputation, in-stock status, shipping speed, and category match. For content recommendation, features include content age, creator popularity, topic diversity from recently consumed content, and predicted watch time.

One media platform built a learning-to-rank reranker for article recommendations using 42 features: 6 from embeddings—query-to-article similarity, query-to-title similarity, user history embedding similarity—plus 36 metadata and engagement features—article age, author follower count, topic match, predicted read time, historical click-through rate for similar articles, and 30 others. The multi-signal reranker improved click-through rate by 14% and time-on-site by 18% compared to pure embedding-based ranking, because it learned that recency, author reputation, and predicted read time mattered as much as semantic similarity for user satisfaction.

## Training Data from User Behavior

Learning-to-rank requires training data: examples of queries paired with documents and relevance labels indicating how relevant each document is to each query. The gold standard is explicit relevance judgments from human annotators who read queries and documents and rate relevance on a scale—highly relevant, somewhat relevant, not relevant. This produces high-quality labels but is expensive and slow, costing 2-5 dollars per query-document-label triple.

The scalable alternative is implicit feedback from user behavior. When a user searches for "machine learning tutorials" and clicks the third result, spends 8 minutes reading, and does not return to search, the third result is implicitly labeled as relevant. When the user clicks the first result, bounces after 10 seconds, and continues searching, the first result is implicitly labeled as not relevant. Click-through signals, dwell time, bounce rate, conversion events—purchases, applications, downloads—all provide implicit relevance labels at scale with zero marginal cost.

The challenge is label noise. Not every click indicates relevance—users click misleading titles and attractive thumbnails. Not every skip indicates irrelevance—users might skip highly relevant results if they appear below the fold or have unappealing titles. Long dwell time usually indicates relevance, but users might leave pages open in background tabs. The solution is aggregating signals across many users and many queries to extract robust relevance patterns despite individual noise.

One e-commerce platform collected training data from 90 days of search logs, extracting query-product pairs with engagement signals. They labeled products as relevant if users clicked, added to cart, or purchased within 5 minutes of search. They labeled products as highly relevant if users purchased. They labeled products as not relevant if they appeared in the top 20 results but received no clicks across 50+ impressions. This produced 4.2 million labeled query-product pairs with reasonably high-quality labels derived entirely from user behavior, enabling training of a production learning-to-rank model without manual annotation.

## Pointwise, Pairwise, and Listwise Learning-to-Rank

Learning-to-rank algorithms fall into three categories based on their training objectives. Pointwise approaches treat ranking as a regression or classification problem: predict a relevance score for each query-document pair independently, then sort by predicted score. Pairwise approaches optimize for correct relative ordering: given two documents for a query, predict which is more relevant. Listwise approaches optimize directly for ranking metrics like NDCG: given all documents for a query, learn to rank them in the order that maximizes evaluation metrics.

Pointwise methods are simplest to implement—train a gradient boosted tree or neural network to predict relevance scores, optimize mean squared error or cross-entropy loss. The weakness is that pointwise methods do not directly optimize for ranking quality—a model that predicts scores 0.9, 0.8, 0.7 for documents A, B, C is treated the same as a model that predicts 0.91, 0.89, 0.1, even though the second ranking is worse because B and C are nearly tied. Pointwise methods work well when you have many labeled examples and relevance score calibration matters, but they underperform pairwise and listwise methods when ranking quality is critical.

Pairwise methods like RankNet and LambdaMART optimize pairwise preference loss: for every pair of documents where document A is more relevant than document B, the model should rank A above B. Training minimizes the number of incorrectly ordered pairs. This directly optimizes for ranking correctness and empirically outperforms pointwise methods on most benchmarks. The downside is training complexity—pairwise methods require generating all pairs of documents for each query, quadratically increasing training data size.

Listwise methods like LambdaLoss and ListNet optimize directly for ranking metrics like NDCG by using differentiable approximations of those metrics as loss functions. These methods achieve the best ranking quality in academic benchmarks but are harder to implement and tune than pointwise or pairwise methods. Production systems typically use gradient boosted trees with pairwise losses—LightGBM with LambdaMART or XGBoost with pairwise objectives—because they balance quality, training speed, and implementation simplicity.

## Feature Engineering for Production Rerankers

The quality of a learning-to-rank model is determined mostly by feature quality, not algorithm choice. A gradient boosted tree with 50 well-designed features will outperform a neural network with 10 poorly designed features. Feature engineering for reranking starts with three categories: query features, document features, and query-document interaction features.

Query features capture characteristics of the query itself: query length, presence of rare terms, query category classification—navigational, informational, transactional—query popularity, and query reformulation history. These features help the model learn that different query types require different ranking strategies. A navigational query like "facebook login" should prioritize exact domain matches. An informational query like "how does photosynthesis work" should prioritize high-quality explanatory content.

Document features capture characteristics of the document independent of the query: document length, readability score, publication date, update frequency, author reputation, domain authority, historical click-through rate, conversion rate, and average dwell time. These features help the model learn document quality signals that apply across queries. A well-written recent article from a reputable source should rank higher than an outdated poorly written article, even if both have similar semantic similarity to the query.

Query-document interaction features capture the relationship between query and document: embedding cosine similarity, BM25 score, exact term match count, term coverage—what fraction of query terms appear in document—positional features indicating where query terms appear in document—title, first paragraph, body. These features directly measure relevance and are often the strongest signals.

One financial services company built a learning-to-rank reranker for regulatory document search using 67 features: 8 query features, 23 document features, 36 query-document features. The most important features according to feature importance analysis were: embedding similarity rank 1, BM25 score rank 2, document recency rank 3, exact term match in title rank 4, and average dwell time for document rank 5. Combining these 5 features alone achieved 82% of the full model's performance, while the remaining 62 features contributed the final 18%. This analysis guided their feature development—investing in improving the top 10 features mattered more than adding marginal features.

## Continuous Reranker Improvement Loops

Deploying a learning-to-rank reranker is not the endpoint—it's the beginning of a continuous improvement loop. User behavior generates new training data daily. Query distributions drift as users discover what works. Document collections evolve as new content is added. A reranker trained in January performs worse by June if not retrained. Production systems require automated pipelines that continuously collect training data, retrain models, evaluate performance, and deploy updated rerankers.

The continuous training pipeline extracts query-document pairs with engagement signals from production logs, filters low-quality signals—single-click examples, bot traffic, outlier sessions—computes features for each pair, trains a new reranker model, evaluates on a held-out test set, and deploys the new model if it outperforms the current production model. This pipeline runs on a schedule—daily, weekly, or monthly depending on data velocity and domain stability.

One news aggregation platform retrained their reranker daily using the previous day's click and dwell time data. They extracted 100,000-200,000 new labeled examples daily, combined them with a 30-day rolling window of historical data to maintain 3-4 million training examples, trained a LightGBM model overnight, evaluated on a held-out test set, and deployed automatically if NDCG at 10 improved by at least 0.5%. This continuous loop kept the reranker adapted to current news trends and user preferences, maintaining NDCG at 10 above 0.81 despite rapid content turnover.

The monitoring component tracks reranker performance in production using online metrics—click-through rate, dwell time, conversion rate, bounce rate—segmented by query type, user segment, and time of day. Degradation in any segment triggers investigation: has the data distribution shifted, has a feature pipeline broken, has label quality degraded? Automated alerts catch issues before they impact users significantly.

## A/B Testing Reranker Changes

Before deploying a new reranker to all users, you A/B test it on a fraction of traffic to measure impact on real user behavior. A/B testing compares the current production reranker against the candidate reranker by randomly assigning users to treatment groups and measuring engagement metrics. If the candidate improves click-through rate by 3% and increases dwell time by 5% with statistical significance, you roll it out. If it degrades metrics or shows no significant change, you reject it.

The A/B test design requires choosing a split ratio—typically 90-10 or 95-5, sending most traffic to the stable production model and a small fraction to the candidate—and a duration—typically one to four weeks depending on traffic volume and effect size. You need sufficient sample size to detect meaningful differences with statistical power above 0.8. For a system serving 100,000 queries daily, a one-week 5% test exposes 35,000 queries to the candidate, sufficient to detect a 2-3% improvement in click-through rate with high confidence.

The risk is canary deployment failures. If the candidate reranker contains a bug or performs catastrophically worse than expected, 5% of users see degraded results. The mitigation is automated rollback based on real-time metrics. If click-through rate or conversion rate drops more than 5% in the first hour of the A/B test, automatically revert to the production model and alert engineers. This limits exposure to bad deployments while allowing safe experimentation.

One SaaS documentation search platform A/B tested 22 reranker candidates over six months. Twelve candidates showed no significant improvement and were rejected. Seven showed small improvements—1-3% click-through rate gains—and were deployed. Three showed regressions and were rejected after early rollback. The cumulative effect of the seven successful deployments improved click-through rate from 0.54 to 0.67—a 24% relative improvement—over six months through continuous experimentation and iteration.

## Balancing Exploitation and Exploration

Learning-to-rank models trained on historical engagement data suffer from feedback loops. If the current ranking algorithm shows certain documents at the top, those documents receive more clicks simply because they're more visible, generating more training data that reinforces their top ranking. Less visible documents receive fewer clicks, generating less training data, even if they might be highly relevant. This exploitation bias degrades ranking quality over time because the model never explores potentially relevant documents ranked low by the current system.

The solution is randomization during data collection. For a small fraction of queries—typically 1-5%—randomize the ranking of results rather than using the production ranker. Users see randomly ordered results, and their engagement signals reveal true relevance independent of current ranking bias. This exploration data is mixed with exploitation data during training, ensuring the model learns from unbiased examples and can discover hidden relevant documents.

One academic search engine randomized 2% of queries to break feedback loops. They observed that 8% of papers surfaced through randomization received higher engagement than papers ranked at the same position by the production ranker, indicating the ranker was systematically underranking those papers. Incorporating randomization data into retraining improved NDCG at 10 by 4% and increased diversity of clicked papers by 12%, showing that exploration data improved both ranking quality and result diversity.

The cost is degraded user experience for the randomized fraction of queries. Users exposed to randomized rankings see worse results than they would under the production ranker, potentially reducing satisfaction. The tradeoff is short-term degradation for a small fraction of users versus long-term improvement for all users as exploration data improves the model. Most systems find that 1-3% randomization strikes an acceptable balance.

## Multi-Objective Optimization in Ranking

Real production systems optimize for multiple objectives simultaneously: relevance, diversity, freshness, fairness, and business metrics. A pure relevance model might rank the top 10 results as 10 highly similar documents on the same narrow topic, providing no diversity. A pure recency model might rank breaking news that is not relevant. A pure business-metric model might rank paid content above organic results to maximize revenue, degrading user trust.

Multi-objective learning-to-rank balances these objectives using weighted loss functions or constraint optimization. You define multiple loss terms—one for relevance, one for diversity, one for recency—and combine them with weights that represent their relative importance. Alternatively, you optimize relevance subject to constraints: maximize NDCG while ensuring at least 3 of the top 10 results are less than 7 days old, or maximize click-through rate while ensuring no more than 2 of the top 10 results are paid placements.

One e-commerce platform optimized for both relevance and revenue, training a reranker to maximize a weighted objective: 0.7 times click-through rate plus 0.3 times revenue per search. This balanced user satisfaction—users finding products they want—with business goals—promoting higher-margin products. The model learned to rank high-margin products slightly higher when relevance was comparable but not to rank irrelevant expensive products above relevant cheaper products. The result was 3% higher revenue per search without significant degradation in user engagement metrics.

The implementation requires careful tuning of objective weights and continuous monitoring to ensure one objective does not dominate at the expense of others. Automated alerts trigger when diversity drops below thresholds, when paid placements exceed limits, or when user engagement degrades. Multi-objective optimization is complex but necessary for production systems where business viability depends on balancing multiple competing goals.

## Production Deployment of Trained Rerankers

Deploying a trained reranker to production requires serving infrastructure that evaluates features and computes model predictions in real-time with low latency. Gradient boosted trees like LightGBM and XGBoost serialize to compact model files—typically 10-100 MB—that load into memory and evaluate in single-digit milliseconds for datasets with dozens of features. Neural rerankers are slower and larger but are improving rapidly with quantization and optimized inference engines like ONNX Runtime and TensorRT.

The serving pattern collocates feature computation and model inference. When a query arrives, your retrieval system fetches candidate documents, your feature pipeline computes query features, document features, and query-document interaction features for each candidate, and your reranker model scores each candidate. Results are sorted by predicted score and returned to the user. The bottleneck is often feature computation—fetching document metadata, computing embeddings, extracting text features—rather than model inference.

Caching optimizes repeated computations. Document features like publication date, author reputation, and historical click-through rate are static or slowly changing and can be precomputed and cached. Query features like query category and query popularity can be cached for common queries. Only query-document interaction features like embedding similarity need to be computed per request. One job search platform reduced reranker latency from 120 milliseconds to 45 milliseconds by caching 80% of feature computations.

The deployment pipeline includes model versioning, A/B testing, gradual rollout, and rollback capability. New models are deployed alongside the current production model, tested on a small fraction of traffic, gradually rolled out to larger fractions if metrics improve, and instantly rolled back if metrics degrade. This ensures safe experimentation and limits blast radius of broken deployments.

## The Compound Value of Learning-to-Rank

The job search platform that opened this chapter deployed a learning-to-rank reranker using 38 features—embedding similarity, BM25 score, years of experience match, salary range match, location distance, company size preference, posting recency, application conversion rate for similar jobs, and 30 others. They trained on 2.1 million query-job pairs extracted from 60 days of user behavior logs using LightGBM with a pairwise ranking loss. The reranker improved click-through rate by 18% and application submission rate by 12% compared to pure cross-encoder reranking.

More importantly, they built a continuous improvement loop. Every week, they retrained the model using the most recent 60 days of data, A/B tested the new model, and deployed if it outperformed the current production model. Over six months, the reranker improved incrementally through 18 weekly deployments, with cumulative click-through rate improvement reaching 26% and application submission rate improvement reaching 19% compared to their pre-learning-to-rank baseline. The system got better continuously as it learned from more user behavior data, and the compound improvement far exceeded the initial deployment gains. Sometimes the right answer is not a static ranking algorithm—it's a learning system that continuously adapts to user preferences and feedback.
