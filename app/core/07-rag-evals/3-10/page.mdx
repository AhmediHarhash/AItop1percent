# 3.10 — Cross-Encoder Reranking: When and How

Why do customer support agents scan the third and fourth search results instead of clicking the first one? Because bi-encoder retrieval gets relevant documents into the top ten but ranks them imperfectly. The most relevant article appears in position three or four, forcing agents to read multiple results before finding the answer. Time-to-resolution increases 23 seconds per ticket. At 12,000 tickets daily, that is 76 hours of wasted agent time. Cross-encoder reranking fixes this by processing the top candidates with a model that sees query and document together, reordering results so the best answer appears first. The cost is 10 to 30 milliseconds of added latency.

You're building retrieval systems in 2026, and you've learned that getting relevant documents into the top 10 or top 20 results is a different problem than ranking those top results in perfect order. Bi-encoder models—the sentence transformers you use for vector search—embed queries and documents independently, then compute similarity as a dot product or cosine between pre-computed vectors. This is fast and scales to millions of documents, but it's fundamentally limited because the model never sees the query and document together. Cross-encoder models take a different approach: they concatenate the query and document, pass the combined text through a transformer model, and output a relevance score. This allows the model to perform deep interaction between query and document tokens, capturing nuances that bi-encoders miss.

The tradeoff is computational cost. Bi-encoders compute document embeddings once at indexing time, making query-time retrieval cheap. Cross-encoders must process every query-document pair at query time, making them 10x to 100x slower than bi-encoders for the same corpus size. The production solution is two-stage retrieval: use bi-encoders for fast initial retrieval of 50-100 candidates, then use cross-encoders to rerank those candidates into perfect order. When you get this right, the cross-encoder fixes the ordering errors from the bi-encoder while adding only 10-30 milliseconds of latency.

## Why Cross-Encoders See What Bi-Encoders Miss

Bi-encoder models encode the query "how do I reset my password" into a 384-dimensional vector and encode the document "Password reset instructions: click forgot password link" into another 384-dimensional vector. The similarity score is the cosine of the angle between these vectors—a single number between -1 and 1 that summarizes how related these texts are. This architecture requires the model to compress all semantic information about the query into a single vector and all semantic information about the document into another single vector, discarding any query-specific or document-specific nuances.

Cross-encoder models concatenate the query and document with a separator token: "how do I reset my password [SEP] Password reset instructions: click forgot password link." This combined sequence flows through the transformer, allowing attention mechanisms to directly compare query tokens to document tokens. The model can recognize that "reset" in the query aligns with "reset" in the document, that "password" appears in both, and that "instructions" in the document relates to the question word "how" in the query. These token-level interactions produce richer relevance signals than the fixed-dimensional bottleneck of bi-encoder vectors.

The performance gap is measurable and consistent across domains. On BEIR benchmark datasets, cross-encoder reranking improves NDCG at 10 by 5-12% compared to bi-encoder retrieval alone. On domain-specific evaluations—legal search, medical QA, technical documentation—the improvements range from 8-18%. The effect is largest when queries are complex, documents are long, and subtle differences in phrasing determine relevance. The effect is smallest when queries are simple keyword lookups and documents are short snippets.

One insurance company measured cross-encoder reranking on their policy document search. Agents search for clauses related to specific coverage scenarios: "does this policy cover flood damage to basement if caused by sewer backup" or "exclusions for pre-existing conditions in supplemental plans." Bi-encoder retrieval achieved 0.68 NDCG at 10. Cross-encoder reranking improved this to 0.81 NDCG at 10. The 19% relative improvement translated to agents finding the correct policy clause in the top 3 results 78% of the time instead of 61% of the time. That difference saves 30-45 seconds per search, and agents perform dozens of searches per shift.

## The Two-Stage Retrieval Pattern

You cannot run cross-encoder scoring across your entire corpus at query time unless your corpus is tiny—a few thousand documents at most. A cross-encoder processes roughly 50-100 query-document pairs per second on a single CPU core, or 200-500 pairs per second on a GPU. Scoring a million documents would take hours on CPU, minutes on GPU. This is unacceptable for production retrieval systems where users expect sub-second responses.

The solution is two-stage retrieval. Stage one uses bi-encoder similarity search to retrieve the top K candidates—typically 50 to 200 documents—from your full corpus in 20-50 milliseconds. Stage two passes each of those K candidates through the cross-encoder along with the query, computes a precise relevance score for each, and returns the top N after reranking—typically 10 to 20 documents. If K is 100 and your cross-encoder processes 200 pairs per second, reranking takes 500 milliseconds on CPU or 200 milliseconds on GPU. That's slow but acceptable for many use cases, especially when the quality improvement is substantial.

The critical parameter is K—how many candidates to retrieve before reranking. Set K too low, and you exclude relevant documents before the cross-encoder ever sees them. Reranking 20 candidates cannot fix a bi-encoder that fails to surface the best document in its top 20. Set K too high, and reranking latency becomes prohibitive. Reranking 500 candidates might take multiple seconds even on GPU. The optimal K balances coverage and latency, and it depends on your bi-encoder quality and cross-encoder speed.

The empirical guidance from production systems: if your bi-encoder achieves recall at 50 above 0.85—meaning 85% of relevant documents appear in the top 50 results—then reranking the top 50 captures most of the available quality improvement. If your recall at 50 is only 0.70, you need to retrieve and rerank the top 100 or even 200 to give the cross-encoder enough candidates to work with. The tradeoff is that low bi-encoder quality forces you into higher K, which increases reranking cost and latency. Improving your bi-encoder to achieve higher recall at smaller K is often more cost-effective than compensating for poor bi-encoder quality with expensive reranking of many candidates.

## Reranker Model Selection

Not all cross-encoders are created equal. Small reranker models like MiniLM-L6-rerank process query-document pairs quickly—300-500 pairs per second on CPU—but provide modest quality improvements over bi-encoder baselines, typically 3-6% NDCG gains. Large reranker models like DeBERTa-v3-large or cross-encoder variants of RoBERTa-large provide substantial quality improvements—8-15% NDCG gains—but process only 50-100 pairs per second on CPU, requiring GPU acceleration to meet production latency budgets.

The decision tree starts with latency tolerance. If your system can tolerate 200-500 milliseconds of reranking latency, you can use large reranker models on GPU and achieve maximum quality. If you need sub-100-millisecond end-to-end retrieval, you must either use small fast reranker models on CPU or limit K to 20-30 candidates and accept reduced coverage. There is no free lunch—quality, latency, and cost are locked in tension, and you must choose which constraints to violate.

Domain-specific rerankers fine-tuned for your use case outperform general-purpose rerankers by significant margins. A reranker trained on medical literature relevance judgments will outperform a general-purpose reranker for medical search by 5-12%. A reranker trained on legal contracts will outperform a general-purpose reranker for contract search by 8-18%. The cost is training data—you need thousands of labeled query-document relevance judgments—and training infrastructure—fine-tuning cross-encoders requires GPU hours. But if retrieval quality is a core business differentiator, training a domain-specific reranker delivers measurable competitive advantage.

One academic search engine fine-tuned a cross-encoder reranker on 15,000 labeled query-paper pairs curated from user click data and explicit relevance feedback. Their domain-specific reranker improved NDCG at 10 by 14% compared to a general-purpose reranker and by 22% compared to bi-encoder retrieval alone. The training cost was roughly $800 in GPU time plus two weeks of engineering effort to curate training data and run experiments. The quality improvement increased user engagement by 11%, which translated to higher subscription renewal rates and justified the investment within two months.

## Latency Impact and Mitigation

The naive implementation runs reranking serially: retrieve K candidates, then score each with the cross-encoder one by one, then sort by score and return top N. If K is 100 and your cross-encoder processes 100 pairs per second, you wait 1 second for reranking. This is often unacceptable, but it's also avoidable. Batch processing allows you to score multiple query-document pairs in parallel, exploiting both CPU vectorization and GPU parallelism.

On a modern GPU, batching 16 or 32 query-document pairs into a single forward pass increases throughput from 200 pairs per second to 800-1200 pairs per second with minimal increase in per-batch latency. Scoring 100 candidates in batches of 32 takes roughly 100 milliseconds instead of 500 milliseconds. The implementation requires careful tensor batching and padding to ensure all sequences in a batch fit within the model's maximum input length, but the latency reduction is worth the complexity.

Caching is another mitigation strategy. If certain documents appear frequently in retrieval results—popular FAQ entries, commonly accessed policies, trending news articles—you can precompute cross-encoder scores for those documents paired with recent queries and cache the scores for 5-10 minutes. Cache hit rates of 20-40% are achievable in production systems with skewed access patterns, and each cache hit eliminates one reranking operation. One customer support system cached cross-encoder scores for the top 50 most-accessed articles paired with the 200 most common query patterns, achieving a 38% cache hit rate and reducing average reranking latency from 180 milliseconds to 115 milliseconds.

Approximate reranking trades perfect ranking for speed by scoring only a subset of the K candidates. Instead of reranking all 100 candidates, you sample 50 randomly or select 50 with highest bi-encoder scores, rerank those 50, and return the top N from the reranked set. This halves reranking latency at the cost of occasionally missing the true best document if it falls outside the sampled subset. The quality degradation is small—typically 2-4% NDCG loss—if your sampling strategy is reasonable. This is a viable option when latency constraints are severe and perfect ranking is less important than fast approximate ranking.

## When Cross-Encoder Reranking Is Essential

Certain retrieval scenarios expose the limitations of bi-encoders so clearly that cross-encoder reranking is not optional—it's the difference between acceptable and unacceptable retrieval quality. Multi-hop reasoning queries are one example. A user asks, "which policies cover both dental and vision for dependents under age 26," a query that requires matching two distinct concepts—dental and vision coverage—plus a constraint on dependent age. Bi-encoders struggle because they must encode this multi-faceted query into a single vector, losing the nuance of the conjunction and constraint. Cross-encoders process the full query and can explicitly verify that candidate documents address both coverage types and the age constraint.

Long-form document retrieval is another scenario where cross-encoders excel. When documents are 500-2000 words—blog posts, research papers, policy documents, technical guides—bi-encoders compress the entire document into a single 384 or 768-dimensional vector, inevitably losing details. Cross-encoders process the full document text alongside the query, allowing token-level attention to focus on the specific passages most relevant to the query. One legal research platform measured retrieval quality for case law search, where documents average 1200 words. Bi-encoder retrieval achieved 0.54 MRR—the first truly relevant case appeared around position 2 on average. Cross-encoder reranking improved MRR to 0.72—the first relevant case appeared in position 1 or 2 consistently.

Adversarial or SEO-optimized content is the third major scenario. When your corpus includes documents deliberately crafted to match common queries despite lacking real relevance—think spam, keyword-stuffed pages, or low-quality content farms—bi-encoders can be fooled by keyword overlap and superficial similarity. Cross-encoders perform deeper semantic analysis and are more robust to keyword stuffing because they evaluate actual token-level coherence between query and document. One job search platform used cross-encoder reranking to filter out low-quality job postings that keyword-stuffed titles and descriptions to rank for popular searches. Their user satisfaction scores improved 16% after deploying reranking, primarily because misleading listings stopped appearing in top positions.

## Implementation Patterns in Production

The standard implementation uses a vector database for bi-encoder retrieval and a separate inference service for cross-encoder reranking. Your application sends a query to the vector database, receives the top K candidate document IDs, fetches the full text for those candidates from your document store, sends query and candidate texts to the reranker service, and receives relevance scores back. This architecture separates concerns but introduces multiple network round-trips and data marshaling overhead.

The optimized implementation colocates reranking with retrieval by embedding a lightweight reranker model directly into your vector database query pipeline or into a proxy layer between your application and the database. Weaviate, Qdrant, and Vespa support pluggable reranker modules that run as part of the query execution path, eliminating external network calls. This reduces end-to-end latency by 10-30 milliseconds compared to separate services, but it requires your vector database to support GPU acceleration if you're using large reranker models.

Adaptive reranking adjusts K dynamically based on query characteristics. Simple keyword queries with high bi-encoder confidence scores—top result has cosine similarity above 0.85—retrieve and rerank only the top 20 candidates. Complex multi-concept queries with low bi-encoder confidence scores—top result has cosine similarity below 0.65—retrieve and rerank the top 100 candidates. This requires building a query complexity classifier and confidence threshold logic, but it optimizes the latency-quality tradeoff across diverse query types. One enterprise search platform reduced average reranking latency by 42% using adaptive K while maintaining the same NDCG as fixed K equals 100 reranking, because most queries were simple and benefited from smaller K.

## Cost Dynamics of Reranking at Scale

Reranking costs scale with query volume and K, not corpus size, which distinguishes cross-encoders from bi-encoders. Bi-encoder indexing costs scale with corpus size—embedding ten million documents costs ten times more than embedding one million documents. But bi-encoder query costs are roughly constant regardless of corpus size because you're always doing approximate nearest neighbor search. Cross-encoder costs scale with queries per second times K candidates per query. At 100 queries per second with K equals 50, you're scoring 5000 query-document pairs per second, requiring multiple GPU instances to maintain acceptable latency.

The breakeven analysis is straightforward. A single GPU instance with a large cross-encoder model can process roughly 1000-1500 pairs per second. At K equals 50 and 100 queries per second, you need 4-5 GPU instances to handle load with headroom for spikes. At 100 dollars per GPU instance per month, you're paying 400-500 dollars monthly for reranking. Compare this to the value of the quality improvement: if better ranking increases user engagement by 10% and your user base generates 5000 dollars per month in revenue, the 500-dollar reranking cost is trivial. If your user base generates 200 dollars per month in revenue, reranking may not be economically justified.

The alternative for cost-sensitive use cases is CPU-based reranking with small fast models. MiniLM-L6-rerank runs on CPU at 300-400 pairs per second per core. With 16 cores, you process 5000-6000 pairs per second on a single instance costing 50-100 dollars per month. The quality improvement is smaller—3-6% NDCG instead of 8-15%—but the cost is an order of magnitude lower. This is the pragmatic choice for systems where retrieval quality is important but not mission-critical, and latency budgets allow 100-200 milliseconds for reranking.

## The Future of Reranking

The trend in 2025-2026 is toward learned rerankers that combine cross-encoder scores with other signals—bi-encoder scores, BM25 scores, document popularity, recency, user context—using a gradient-boosted tree or neural network trained to predict relevance. This multi-signal reranking outperforms pure cross-encoder reranking by 4-8% NDCG because it exploits both semantic understanding from the cross-encoder and non-semantic signals like freshness and popularity that matter for real user satisfaction.

Training multi-signal rerankers requires labeled data—thousands of queries with relevance judgments—and a feature engineering pipeline to extract signals from your corpus and user behavior logs. The infrastructure complexity is higher than deploying a pre-trained cross-encoder, but the quality ceiling is also higher. One media recommendation system trained a LightGBM reranker using cross-encoder scores plus 18 other features—article age, author popularity, topic match, user reading history—and improved their NDCG at 10 from 0.76 with pure cross-encoder reranking to 0.84 with multi-signal reranking. The training data came from six months of click logs, and the model retrained weekly to adapt to changing content and user preferences.

The customer support platform that opened this chapter deployed cross-encoder reranking with K equals 40 and a small fast reranker model running on CPU. Their reranking latency was 85 milliseconds, bringing total retrieval time to 120 milliseconds—well within their budget. Their NDCG at 10 improved from 0.68 to 0.76, and agents reported finding the right article in the top 2 results 72% of the time instead of 54% of the time. Time-to-resolution improved by 19 seconds per ticket compared to the bi-encoder-only system, saving 63 hours of agent time daily. The infrastructure cost was 180 dollars per month for a larger CPU instance. The business value—faster resolutions, happier customers, reduced agent frustration—justified that cost within the first week. Sometimes the right answer is not faster retrieval—it's better ranking of the results you already retrieved.
