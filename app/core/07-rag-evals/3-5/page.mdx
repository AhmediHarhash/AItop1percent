# 3.5 — Similarity Metrics: Cosine, Dot Product, and Euclidean Distance

In January 2025, a SaaS company launched a knowledge base search feature using a vector database configured with Euclidean distance as the similarity metric. They embedded their documentation with OpenAI's text-embedding-3-large model, which produces normalized embeddings, and built their index. The system worked, but users complained that search results were "random" and "didn't make sense." The engineering team investigated and discovered that they were using the wrong distance metric for their embedding model. OpenAI embeddings are L2-normalized, meaning they all have magnitude one and lie on the surface of a unit hypersphere. For normalized vectors, Euclidean distance is mathematically related to cosine similarity, but the database implementation applied Euclidean distance incorrectly, leading to nonsensical rankings. They switched to cosine similarity, and retrieval quality immediately improved. The team had blindly copied a configuration from a tutorial without understanding what the metric meant.

By 2026, similarity metrics are a fundamental but often misunderstood component of vector search. The metric determines how you measure the distance between a query vector and document vectors, which in turn determines the ranking of search results. Choose the wrong metric, and your retrieval quality collapses. Choose the right metric but apply it incorrectly, and you waste compute on redundant calculations. This chapter teaches you what cosine similarity, dot product, and Euclidean distance mean, when to use each, how they relate to each other, how metric choice affects retrieval quality, and the common mistakes that break production systems.

## Distance Metrics: What You Are Actually Measuring

A distance metric defines how you measure similarity or dissimilarity between two vectors. Vector databases support several metrics, but the three most common in 2026 are cosine similarity, dot product, and Euclidean distance. Each metric computes a scalar score from two vectors, and the database uses that score to rank documents.

**Cosine similarity** measures the cosine of the angle between two vectors. It ranges from minus one to one, where one means the vectors point in the same direction, zero means they are orthogonal, and minus one means they point in opposite directions. Cosine similarity is normalized—it does not care about vector magnitude, only direction. Two vectors with the same direction but different magnitudes have cosine similarity of one.

**Dot product** is the sum of element-wise products of two vectors. It is not bounded—it can range from negative infinity to positive infinity depending on the vectors' magnitudes and alignment. Dot product measures both direction and magnitude. Two vectors with the same direction but one twice as long will have a dot product twice as large as two vectors with the same direction and equal length.

**Euclidean distance** measures the straight-line distance between two points in high-dimensional space. It is the L2 norm of the difference between two vectors. Euclidean distance is zero when vectors are identical and increases as they diverge. Unlike cosine similarity, Euclidean distance is sensitive to magnitude—two vectors with the same direction but different magnitudes have non-zero Euclidean distance.

The choice of metric depends on the properties of your embeddings and the semantics of your retrieval task.

## Cosine Similarity: The Default for Normalized Embeddings

Cosine similarity is the most common distance metric in production RAG systems as of 2026. It is the default choice for embeddings produced by neural models that normalize their outputs. Most modern embedding models—OpenAI, Cohere, sentence-transformers, and open-source models—produce L2-normalized embeddings, meaning each vector has magnitude one.

For normalized vectors, cosine similarity is equivalent to dot product because the magnitudes are constant. If vectors A and B both have magnitude one, then cosine similarity equals the dot product of A and B. This makes cosine similarity and dot product interchangeable for normalized embeddings, and many vector databases optimize this case by using dot product internally when they know embeddings are normalized.

Cosine similarity is appropriate when you care about the direction of the vector but not its magnitude. In semantic embeddings, direction encodes meaning, and magnitude is irrelevant or normalized away during training. A document about "machine learning" and a document about "deep learning" should be close in direction because they are conceptually similar, regardless of how long the text is or how many times those terms appear.

Cosine similarity is also robust to scale differences. If one document is ten times longer than another but discusses the same topic, their embeddings should have high cosine similarity even if the longer document's raw embedding has higher magnitude. Normalization ensures that similarity is based on content, not length.

The formula for cosine similarity between vectors A and B is the dot product of A and B divided by the product of their magnitudes. For normalized vectors, the magnitudes are one, so the formula simplifies to the dot product. Most vector databases that support cosine similarity implement it as dot product when embeddings are normalized, which is faster because it skips the magnitude computation.

The failure mode is to use cosine similarity for embeddings that are not normalized. If your embedding model does not normalize its outputs, cosine similarity might still work, but the results depend on the magnitude distribution of your embeddings. If different documents produce embeddings with wildly different magnitudes, cosine similarity might rank them incorrectly.

## Dot Product: When Magnitude Matters

Dot product is appropriate when both direction and magnitude encode meaningful information. In some embedding models, magnitude represents confidence, importance, or relevance. A document that strongly matches a topic might have a higher-magnitude embedding than a document that weakly matches the topic. In this case, you want to rank documents by both how well they align with the query direction and how strong their signal is.

Dot product is also faster to compute than cosine similarity for non-normalized vectors. You do not need to compute magnitudes or perform division—you just sum element-wise products. For high-throughput systems, this performance difference can matter.

The tradeoff is that dot product is sensitive to magnitude, which can be a blessing or a curse. If magnitude encodes meaningful information, dot product leverages it. If magnitude is noise—for example, if longer documents produce higher-magnitude embeddings purely because they have more text—dot product can produce poor rankings.

In 2026, most embedding models normalize their outputs, so dot product and cosine similarity are functionally equivalent for most use cases. The exception is when you use embeddings that explicitly encode magnitude, such as some learned sparse embeddings or older TF-IDF-based embeddings. In those cases, you need to decide whether magnitude should influence ranking.

One common use of dot product is in hybrid scoring systems where you combine dense embeddings with other signals. For example, you might compute a dot product between the query embedding and document embedding, then add a BM25 score weighted by some factor. Dot product allows you to blend multiple signals without normalizing each one separately.

## Euclidean Distance: Rarely the Right Choice

Euclidean distance measures the straight-line distance between two vectors in high-dimensional space. It is the metric you learned in geometry class, generalized to many dimensions. For two-dimensional vectors, Euclidean distance is the length of the line connecting two points. For 768-dimensional vectors, it is the same idea.

Euclidean distance is rarely the right choice for semantic embeddings. The problem is that Euclidean distance is sensitive to both direction and magnitude in a way that does not align with how embeddings encode meaning. Two vectors with the same direction but different magnitudes have non-zero Euclidean distance, even though semantically they might represent the same concept. Euclidean distance penalizes magnitude differences, which is usually not what you want for semantic similarity.

There are cases where Euclidean distance is appropriate. If your vectors represent points in a geometric space where straight-line distance has physical meaning—for example, geospatial coordinates or image feature vectors from certain computer vision models—Euclidean distance makes sense. But for text embeddings, Euclidean distance is almost never the right metric.

The failure mode is to use Euclidean distance with normalized embeddings. For normalized vectors, Euclidean distance is mathematically related to cosine similarity: Euclidean distance equals the square root of two times one minus cosine similarity. This means Euclidean distance and cosine similarity rank documents in the same order for normalized vectors, but the scores are on different scales. If you use Euclidean distance, you are doing redundant computation compared to cosine similarity or dot product.

Some vector databases default to Euclidean distance because it is a general-purpose metric. If you blindly accept the default without understanding your embeddings, you might end up using Euclidean distance when cosine similarity or dot product would be more appropriate. Always check your database configuration and choose the metric that matches your embedding model.

## The Mathematical Relationship Between Metrics

For normalized vectors, cosine similarity, dot product, and Euclidean distance are related by simple mathematical transformations. If vectors A and B both have magnitude one, then:

- Cosine similarity equals dot product.
- Euclidean distance squared equals two times one minus cosine similarity.

This means all three metrics produce the same ranking for normalized vectors—they just use different scales. A vector database that supports any one of these metrics can efficiently implement the others for normalized embeddings.

For non-normalized vectors, the metrics diverge. Cosine similarity depends only on direction. Dot product depends on direction and magnitude. Euclidean distance depends on both but in a different way. The rankings produced by these metrics can differ significantly for non-normalized vectors.

The implication is that you must know whether your embedding model normalizes its outputs. Most modern models do, but you should verify by inspecting a sample of embeddings and checking their magnitudes. If magnitudes are all close to one, the model normalizes. If magnitudes vary widely, the model does not normalize, and you need to choose your metric carefully.

## How Metric Choice Affects Retrieval Quality

The choice of metric affects which documents rank highest for a given query. If you use the wrong metric, documents that should rank first might rank tenth, and retrieval quality suffers.

Consider a query embedding Q and two document embeddings D1 and D2. Suppose Q and D1 point in almost the same direction with magnitude one, and Q and D2 point in a slightly different direction but D2 has magnitude two.

With cosine similarity, D1 ranks higher because it aligns better with Q's direction. Magnitude is ignored.

With dot product, D2 might rank higher if the magnitude boost outweighs the direction misalignment. You are rewarding D2 for having a stronger signal.

With Euclidean distance, the ranking depends on the exact magnitudes and directions. Euclidean distance penalizes both magnitude differences and direction differences in a non-linear way.

The correct metric depends on what the magnitude encodes. If magnitude is meaningless and only direction matters, use cosine similarity. If magnitude encodes relevance or confidence, use dot product. If neither metric fits your use case, you probably have the wrong embedding model.

In practice, the vast majority of RAG systems in 2026 use cosine similarity because the vast majority of embedding models normalize their outputs. The failure mode is to assume that all metrics are equivalent and pick one arbitrarily. They are not equivalent for non-normalized vectors, and the wrong choice degrades retrieval quality.

## Common Mistakes and How to Avoid Them

**Mistake one: using Euclidean distance for normalized embeddings.** This is inefficient because Euclidean distance and cosine similarity produce the same rankings for normalized vectors, but Euclidean distance requires computing a square root. Use cosine similarity or dot product instead.

**Mistake two: using cosine similarity for non-normalized embeddings without understanding the magnitude distribution.** If your embeddings have variable magnitudes and magnitude encodes meaningful information, cosine similarity discards that information. Use dot product or investigate why magnitudes vary.

**Mistake three: not verifying that your embedding model's output matches your metric.** Many teams assume their embeddings are normalized because "everyone uses normalized embeddings," but they never check. Verify by computing the magnitude of a sample of embeddings. If magnitudes are not close to one, your embeddings are not normalized.

**Mistake four: using different metrics for embedding and querying.** If you index your database with cosine similarity but accidentally query with Euclidean distance, rankings break. Ensure your database uses the same metric consistently for indexing and querying.

**Mistake five: not tuning the metric when you change embedding models.** If you switch from a model that normalizes to a model that does not, or vice versa, you need to reconsider your metric choice. Do not assume the same metric works for all models.

The mitigation is simple: understand your embedding model, check whether it normalizes outputs, and choose the metric that matches. For most teams in 2026, the answer is cosine similarity or dot product for normalized embeddings.

## Performance Implications of Different Metrics

Different metrics have different computational costs. Dot product is the cheapest—you sum element-wise products and you are done. Cosine similarity requires computing dot product and dividing by the product of magnitudes, which adds two magnitude computations and one division. For normalized vectors, the magnitudes are one, so the division is trivial and cosine similarity reduces to dot product.

Euclidean distance requires computing the difference between vectors, squaring each element, summing, and taking a square root. This is more expensive than dot product or cosine similarity, especially if you need exact Euclidean distance rather than squared Euclidean distance.

For high-throughput systems, these performance differences matter. If you query thousands of times per second, shaving a few microseconds off each distance computation saves CPU cycles and reduces latency. Most vector databases optimize for dot product because it is the fastest metric for normalized embeddings.

The performance difference is small enough that you should not choose a metric solely based on speed. Choose based on correctness first, then optimize if profiling shows that distance computation is a bottleneck.

## Negative Dot Products and Asymmetric Queries

Some embedding models produce vectors that can have negative dot products with the query. This happens when the query and document vectors point in opposite directions. For semantic embeddings, this usually means the document is anti-relevant—it discusses the opposite of what the query is asking.

If your use case has true negative examples—documents that should rank low because they are anti-relevant—negative dot products are meaningful, and you should rank documents by dot product from highest to lowest. If your use case does not have negative examples, you might want to filter out negative dot products or use a metric that maps all distances to a positive range.

In practice, most embedding models trained for semantic search produce vectors that are rarely anti-aligned. Negative dot products are uncommon, and you can usually ignore them.

Some retrieval systems use asymmetric metrics where the query embedding and document embeddings are computed differently. For example, Cohere's embed-v3 model has separate query and document embeddings, and you compute dot product between a query embedding and document embeddings but not between two document embeddings. This asymmetry is baked into the model design, and you must respect it in your retrieval pipeline.

## When to Revisit Your Metric Choice

You should revisit your metric choice when you change embedding models, when you observe unexpected retrieval behavior, or when you upgrade your vector database and the default metric changes.

If you switch from OpenAI embeddings to a self-hosted model, check whether the new model normalizes outputs. If it does not, you might need to switch from cosine similarity to dot product or manually normalize embeddings before indexing.

If users report that search results are poor or that obviously relevant documents rank low, investigate whether your metric is correct. Compute a few example similarities by hand and verify that the rankings make sense.

If you upgrade your vector database and retrieval quality changes, check the release notes for changes to how metrics are computed. Some databases have fixed bugs or changed default behaviors in ways that affect rankings.

The discipline is to treat metric choice as a configuration parameter that you validate, test, and monitor, not as a one-time decision you make and forget.

## Practical Guidance for 2026

For the vast majority of production RAG systems in 2026, the correct metric is cosine similarity or dot product, depending on how your vector database optimizes for normalized embeddings. If your database automatically detects normalized embeddings and uses dot product internally for cosine similarity, use cosine similarity in your configuration and let the database optimize.

If you are unsure which metric to use, start with cosine similarity. It is the safest default for semantic embeddings. If retrieval quality is poor, investigate whether your embeddings are normalized and whether a different metric is more appropriate.

If you are building a custom retrieval system and implementing similarity computation yourself, use dot product for normalized embeddings because it is the fastest. Normalize your embeddings during the embedding step if the model does not do it for you.

Never use Euclidean distance for text embeddings unless you have a specific reason and you understand the implications. It is almost never the right choice.

The teams that build reliable RAG systems are the teams that understand their embedding models, choose metrics deliberately, and validate that their configuration matches their data. The teams that break are the ones that copy configurations from tutorials without understanding what the settings mean.
