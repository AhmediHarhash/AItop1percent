# 8.1 â€” RAG Observability: Tracing Queries Through the Pipeline

In February 2025, a healthcare tech startup's RAG-powered patient documentation assistant began returning incomplete medical histories to clinicians. The symptom was subtle and insidious: roughly 12 percent of queries returned answers that omitted critical medication interactions, but only for patients with records exceeding 50 pages. Not every large record. Not consistently. Just enough to be dangerous, but not enough to trigger immediate alarms. The engineering team spent four days debugging, manually testing queries one at a time, inspecting vector databases with raw SQL queries, and reviewing LLM responses in isolation. They checked the embedding service: operational. They checked the vector database: query times normal. They checked the LLM API: responses looked reasonable when tested independently.

The discovery came on day four, when a senior engineer, exhausted and desperate, manually reconstructed a single failed query's complete journey through the system. She logged into the embedding service, grabbed the query vector, passed it manually to the vector database, inspected the raw results, fed those results to the reranker service, and watched the reranker service time out. Not error out. Time out. Silently. The reranker, when presented with large result sets from patients with extensive medical histories, exceeded its 500 millisecond timeout threshold and fell back to a hardcoded subset of results without logging the timeout. The failure happened in a microservice three hops away from the user-facing API. The timeout was not surfaced to the application layer. No error appeared in logs. No metric spiked. The system silently degraded, returned partial results, and continued processing as if nothing was wrong.

The incident cost them an enterprise customer who had been piloting the system. It cost them six weeks of trust rebuilding with existing customers who questioned whether the system could be relied upon for clinical decision support. It cost them a product roadmap delay as they rebuilt their observability infrastructure. Most painfully, it cost them the confidence that they understood their own system. They had metrics, logs, and monitoring dashboards. But they had no way to follow a single query's path through the pipeline, to see what happened at each stage, to understand where failures hid in the gaps between components.

## Why Distributed Tracing Is Not Optional

You need distributed tracing for RAG systems. Not because tracing is trendy, not because observability vendors tell you to, but because RAG pipelines are inherently multi-stage, multi-service architectures where failures hide in the spaces between components. A user submits a question. Your system transforms it into an embedding vector, queries a vector database, retrieves candidate document chunks, possibly reranks them using a cross-encoder model, assembles context from the top results, generates a prompt, calls an LLM API, and streams a response back to the user. That is six to eight distinct operations, often crossing service boundaries, network calls, and external APIs operated by third parties you do not control.

When something goes wrong, you cannot rely on aggregate metrics or scattered log lines. Aggregate metrics tell you that average latency increased, but not why. They tell you that error rates spiked, but not where. Log lines tell you that a particular service logged a warning, but not what query triggered it or what happened before and after. You need a trace: a single thread that stitches together every operation that contributed to a response, showing you the complete picture of what your system did, how long each step took, what data passed between stages, and where things went wrong.

Distributed tracing assigns a unique trace ID to every incoming request. That ID propagates through every service, every database call, every external API invocation. Each operation records a span: a timestamped record of what happened, how long it took, what inputs it received, what outputs it produced, and whether it succeeded or failed. The spans assemble into a trace tree, a complete hierarchical picture of the query's journey from user input to final response.

You can visualize it as a waterfall diagram: embedding generation took 45 milliseconds, vector search took 120 milliseconds, reranking took 200 milliseconds, LLM generation took 1.8 seconds, and context assembly took 15 milliseconds. You can inspect individual spans to see the exact embedding vector's metadata, the retrieved document IDs and their similarity scores, the reranker scores that determined final ordering, the prompt sent to the LLM with all its context, and the tokens returned. You can see errors, retries, timeouts, and fallback logic. You can see what actually happened, not what you thought happened.

## Implementation: OpenTelemetry and Modern Tracing Backends

Implementing distributed tracing for RAG starts with choosing a tracing backend. OpenTelemetry is the industry standard in 2026, supported by every major observability platform: Datadog, Honeycomb, Grafana Tempo, AWS X-Ray, Google Cloud Trace, and dozens of others. You instrument your code with OpenTelemetry SDKs, which handle trace context propagation, span creation, and data export to your chosen backend. The SDKs are available for every major language and framework: Python, JavaScript, Java, Go, Rust. The instrumentation overhead is minimal, typically adding single-digit milliseconds to request latency, and you can sample traces aggressively to reduce costs in high-traffic environments without losing observability.

Start tracing at the entry point: the API endpoint or web handler that receives the user query. Generate a trace ID and attach it to the request context. Every downstream operation that participates in handling this query must receive and propagate that trace ID. If your embedding service is a separate microservice called over HTTP or gRPC, the HTTP client must include the trace ID in outgoing request headers using standardized propagation formats. If your vector database client supports OpenTelemetry tracing, configure it to create spans for search operations. If your LLM provider supports tracing or correlation IDs, pass the trace ID in API calls so you can correlate their logs with your traces.

The goal is unbroken propagation: every operation that contributes to the response must be part of the same trace. When you view the trace in your observability tool, you should see the complete pipeline from start to finish, with no gaps, no missing stages, and no orphaned operations. Gaps in traces are blind spots. Blind spots hide failures.

## Instrumenting Each Pipeline Stage

For each stage of the RAG pipeline, create a span that captures the operation's semantics and critical metadata. The embedding generation span should record the input text or a hash of it for privacy, the embedding model name and version, the dimensionality of the output vector, and the latency. You do not need to log the full embedding vector in production, especially for high-dimensional vectors, but you should log metadata like its L2 norm or a fingerprint hash, so you can detect anomalies like zero vectors, vectors with unexpectedly low norms, or vectors that differ dramatically from similar queries.

The vector search span should record the query vector's metadata, the search parameters such as top-k and similarity threshold, the database shard or partition queried, the number of candidates returned, and the similarity scores of those candidates. This data is critical for debugging retrieval issues: if users report irrelevant results, you can inspect the search span to see whether the vector database returned low-scoring results, whether it returned the expected number of candidates, or whether the problem lies elsewhere in the pipeline.

The reranking span is where many production issues hide, because rerankers are often the slowest and most fragile component. Rerankers are typically neural models that perform expensive cross-encoder inference, computing relevance scores for query-document pairs. They are often slower than vector search by an order of magnitude, and they introduce additional failure modes: model inference errors, timeout configurations that are too aggressive or too lenient, and fallback logic that silently degrades quality.

Your reranking span should record the input: how many candidates were passed to the reranker, their document IDs, and their vector search scores. It should record the output: the reranked order, the reranker scores assigned to each candidate, and any candidates that were filtered out due to low scores or other criteria. It should record latency broken down by inference time and overhead. It should record any errors or warnings, such as timeouts, model loading failures, or degraded performance due to resource constraints. If your reranker falls back to vector search scores on timeout, that fallback must be logged as a span event, visible in the trace, so you can correlate reranker timeouts with answer quality degradation and user complaints.

Context assembly is the stage where retrieved chunks are combined, deduplicated, and formatted into the prompt that will be sent to the LLM. This operation is often implemented in application code, not in a dedicated service, and it is easy to under-instrument because it feels trivial. It is not trivial. Context assembly makes critical decisions about what information reaches the LLM, and mistakes here directly cause answer quality issues.

Your context assembly span should record how many chunks were selected for inclusion, their total token count, the deduplication strategy if any, the truncation behavior if the context exceeds token limits, and the final prompt length. If your system applies retrieval filters or business logic, such as filtering out outdated documents, prioritizing certain sources, or enforcing access control, those decisions should be logged as span attributes. When users report that critical information was omitted from answers, the context assembly span tells you whether that information was retrieved but excluded during assembly, or whether it was never retrieved in the first place. This distinction determines whether you need to fix retrieval or fix context assembly.

## LLM Generation and Streaming Complexity

LLM generation is the most expensive and variable stage. Your LLM span should record the model name and version, the prompt length in tokens, the generation parameters such as temperature and max tokens, the response length, the latency broken down by time to first token and total generation time, and the token usage for cost attribution. If your LLM provider returns metadata such as finish reason, content filter flags, or quality scores, log those as span attributes. If the LLM request fails or times out, the span should record the error message, stack trace, and any retry attempts.

Streaming responses introduce additional complexity to tracing. When you stream LLM output, generation happens incrementally: the first tokens arrive quickly, and subsequent tokens arrive over seconds. You may want to record multiple events within the span, such as time to first token, time to completion, and any interruptions or backpressure events where the client could not consume tokens fast enough. Time to first token is critical for user-perceived latency: users see the response starting to appear and perceive the system as responsive, even if total generation takes several seconds.

## Structured Span Attributes and Semantic Conventions

Structured span attributes are far more valuable than freeform logs. Use semantic conventions: standardized attribute names that observability tools recognize and index efficiently. OpenTelemetry defines semantic conventions for HTTP requests, database queries, and RPC calls. Extend these conventions for RAG-specific operations. For example, use attributes like rag.query.text for the user query, rag.embedding.model for the embedding model name, rag.retrieval.top_k for the retrieval parameter, rag.reranker.model for the reranker model, rag.context.token_count for the assembled prompt size, and llm.prompt.tokens for LLM input tokens.

Consistent naming enables you to query traces programmatically, aggregate metrics from span attributes, and build dashboards that work across different RAG implementations. If every team uses different attribute names, your observability tools cannot aggregate data, and cross-team debugging becomes painful. Standardization is not bureaucracy. It is leverage.

## Trace Sampling and Cost Management

Trace sampling reduces cost and storage in high-traffic systems. You do not need to trace every query in production. A well-designed sampling strategy balances observability needs with cost constraints: 1 percent sampling for baseline monitoring, 10 percent for active debugging, 100 percent for incident response or high-value customers. Implement intelligent sampling: always trace errors, always trace slow queries above a latency threshold, always trace queries from high-value customers or critical workflows, and randomly sample successful queries at a low rate.

Head-based sampling decisions are made at the start of the trace, before you know whether the trace will be interesting. You decide to sample based on trace ID, user ID, or random chance. Tail-based sampling decisions are made after the trace completes, allowing you to retain interesting traces such as errors, slow requests, or unusual patterns, and discard routine successful traces. Tail-based sampling requires a tracing backend that supports it, such as Honeycomb or Grafana Tempo, and it requires buffering traces until they complete, which increases memory usage.

## Asynchronous Operations and Message Queues

Trace context propagation across asynchronous operations requires careful design. If your RAG pipeline uses message queues or background workers for indexing, batch processing, or scheduled tasks, the trace context must be serialized and included in the message payload. When the worker picks up the message, it deserializes the trace context and continues the trace. Without this, you lose visibility into asynchronous stages, and the trace appears incomplete. Users see a trace that ends at the message queue, with no indication of what happened afterward.

OpenTelemetry SDKs provide utilities for injecting and extracting trace context from message headers using standard formats like W3C Trace Context or B3. But you must explicitly wire them into your message producers and consumers. Test asynchronous propagation carefully: publish a message, consume it in a worker, and verify that the trace spans appear connected in your tracing backend.

## Tracing External APIs and Third-Party Services

Tracing external API calls to embedding providers, vector databases, and LLM services is essential but not always straightforward. If the provider supports OpenTelemetry or includes trace IDs in their response headers, propagation is automatic or requires minimal configuration. If not, you must create client-side spans that represent the external call. These spans will not contain the internal details of the provider's processing, which you cannot observe, but they will record the request parameters, response metadata, latency, and errors.

For debugging provider-side issues, include request IDs or correlation IDs from the provider's API in your span attributes, so you can cross-reference with the provider's support team when issues arise. When you open a support ticket with OpenAI or Anthropic about a slow or failed request, the first thing they will ask for is the request ID. If that ID is in your trace, you can instantly provide it.

## Error Propagation and Incident Response

Error propagation in traces is critical for incident response. When a span fails, it should be marked with an error status, and the error message, stack trace, and any relevant context should be attached as span events or attributes. Errors should propagate up the trace tree: if a reranker span fails, the parent RAG pipeline span should also be marked as failed. This allows you to query for all failed traces and drill into the specific span that caused the failure.

Without proper error marking, failed requests appear as slow requests, and the root cause is obscured. You see high latency, but you do not see that the latency was caused by retries after failures. You see successful responses, but you do not see that the success came from a fallback path that returned degraded quality.

## Correlation with Logs and Metrics

Correlation between traces and logs is the final piece of the observability puzzle. Your application logs should include the trace ID in every log line, allowing you to jump from a trace span to the full application logs for that request. Conversely, when reviewing logs for an error or warning, you should be able to click on a trace ID and view the complete trace. This bidirectional linkage is invaluable for debugging: traces show you the structure and timing of operations, logs show you the detailed application state and business logic decisions.

Configure your logging library to extract the trace ID from the OpenTelemetry context and include it in the log formatter. Most logging libraries support structured logging with custom fields. Add a trace_id field to every log entry, and your log aggregation tool will automatically correlate logs with traces.

## Debugging Production Issues with Traces

Debugging production issues with traces transforms reactive firefighting into systematic problem-solving. A user reports that a query returned an irrelevant answer. You search for traces matching that user's session ID or query text, pull up the trace, and see the entire pipeline in one view. The embedding generation span shows the query was embedded correctly with a reasonable vector norm. The vector search span shows 50 candidates were retrieved, but all had similarity scores below 0.6, indicating a retrieval failure. You inspect the search parameters and discover that the similarity threshold was recently raised from 0.5 to 0.7 as part of a quality improvement initiative, but the retrieval set did not contain sufficiently similar documents for this particular query.

The root cause is not a bug in the traditional sense. It is a mismatch between the threshold configuration and the corpus content. You revert the threshold change, re-evaluate it with more comprehensive testing, and retrieval quality recovers. Total debugging time: ten minutes, most of it spent deciding on the right fix. Without tracing, this would have required reproducing the query in a development environment, manually inspecting database results, guessing at recent parameter changes, and testing hypotheses one at a time over hours or days.

## Performance Optimization and Capacity Planning

Performance optimization relies on trace data to identify bottlenecks and quantify improvements. You notice that P95 latency for RAG queries has increased from 2 seconds to 3.5 seconds over the past week. Aggregate metrics tell you the problem exists but not where. You query traces for high-latency requests and group by span name. The data shows that reranking latency increased from 200 milliseconds to 1.2 seconds on average. You drill into reranking spans and discover that a recent model update increased the model size from 200MB to 800MB, and the inference server is now CPU-bound.

You scale up the inference server with more CPU cores, and latency drops back to baseline. Tracing turned a vague performance regression, which could have resulted from dozens of potential causes, into a specific, actionable diagnosis that you can fix in minutes.

Capacity planning uses trace data to model scaling needs and predict future resource requirements. You export span duration data for the past month, calculate the latency contribution of each pipeline stage, and project how latency will change as query volume increases. If embedding generation takes 50 milliseconds per query and you are processing 100 queries per second, you need embedding throughput of 5,000 embeddings per second. If your current embedding service can handle 10,000 embeddings per second, you have 2x headroom before you need to scale.

If retrieval latency is 150 milliseconds and your vector database scales linearly with shard count, you can calculate how many shards you need to maintain target latency at 10x query volume. Tracing data replaces guesswork with measurement, turning capacity planning from an art into a science.

## Advanced Use Cases: Trace Data Joins and Predictive Models

Custom trace analysis enables advanced use cases that go beyond debugging. You can export trace data to a data warehouse, join it with business metrics such as user satisfaction scores, thumbs-up and thumbs-down feedback, or conversion rates, and analyze which pipeline characteristics correlate with positive outcomes. For example, you might discover that queries with reranker scores above 0.8 have 90 percent user satisfaction, while queries with scores below 0.6 have 40 percent satisfaction. This insight informs your quality thresholds and guardrails: you can automatically flag or escalate low-confidence queries for human review.

You can build machine learning models that predict query success based on trace features, enabling proactive intervention or fallback strategies. If your model predicts that a query will produce a low-quality answer based on low retrieval scores and high embedding uncertainty, you can route it to a human agent, prompt the LLM with additional instructions to be cautious, or ask the user for clarification before answering.

## Tracing as a Living Discipline

Tracing is not a one-time implementation. As your RAG system evolves, you must evolve your instrumentation. When you add a new pipeline stage, such as a query rewriting step or a retrieval filter, instrument it with spans and attributes. When you change a critical parameter, such as similarity thresholds or top-k values, add it as a span attribute so you can correlate changes with outcomes. When you refactor code, ensure trace context propagation remains intact and that spans are created in the right places.

Treat tracing as a first-class concern, reviewed in code reviews and tested in integration tests. Write tests that verify trace propagation across service boundaries, that check for required span attributes, and that validate that errors are marked correctly. A well-instrumented RAG system is debuggable, optimizable, and understandable. A poorly instrumented system is a black box, where every incident is a multi-day investigation and every optimization is a guess.

## The Aftermath: Visibility as a Competitive Advantage

The healthcare tech startup eventually rebuilt their RAG pipeline with comprehensive tracing. Every query, every embedding, every retrieval, every reranking operation, every LLM call, traced end-to-end with structured attributes and error propagation. When the next production issue occurred, a query returning stale data for a specific patient, the team pulled up the trace, saw that the vector database was querying an outdated index shard due to a recent rebalancing operation, identified the misconfigured shard routing logic, and deployed a fix in 45 minutes. Total user impact: fewer than 100 queries. The CEO, who had been burned by the previous incident, asked how they debugged it so fast. The engineer pulled up the trace on a screen and replied: "We could see everything."

You build that visibility now. You do not wait for an incident, for a customer complaint, or for a competitor to outpace you. You instrument every stage of your RAG pipeline with distributed tracing, you propagate trace context across all service boundaries, you record structured span attributes for every operation, and you integrate tracing with your logs and metrics. You build dashboards that show trace latency distributions, error rates by span name, and slow trace exemplars. You train your team to use traces as the primary debugging tool, not as an afterthought.

You make tracing cheap enough to run in production through intelligent sampling, comprehensive enough to answer any question through detailed span attributes, and reliable enough to trust in an incident through rigorous testing.

Your RAG system is no longer a black box. It is a glass pipeline, where you can watch every query flow from input to output, inspect every decision, measure every delay, and diagnose every failure. That visibility is not optional. It is the foundation of production RAG operations, the difference between firefighting and systematic problem-solving, and the reason you sleep at night.
