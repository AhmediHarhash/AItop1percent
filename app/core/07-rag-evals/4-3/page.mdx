# 4.3 — Hypothetical Document Embeddings (HyDE) and Query Transformation

A legal research platform launched in September 2025 with a retrieval system trained on three million case law documents and statutes. Lawyers using the system kept reporting frustrating search results. When an attorney searched "can an employer terminate an employee for off-duty marijuana use in states where it is legal," the system retrieved documents about employment law, marijuana legalization, and termination procedures, but rarely the specific case law addressing this exact intersection. When someone searched "what is the statute of limitations for breach of contract in California," the system returned documents defining statute of limitations and documents about California contract law, but struggled to surface the specific statutes and cases with the actual time limits. By December 2025, the company's renewal rate was 34 percent. The embedding model was excellent. The problem was that user queries and legal documents speak fundamentally different languages. Queries ask questions. Documents provide answers. Embedding space does not magically bridge this structural gap.

You are running into the query-document mismatch problem, and it is more fundamental than vocabulary differences. Even with perfect terminology alignment, queries and documents have different structures, different purposes, and different information densities. A query is a question: "What are the requirements for GDPR compliance?" A document is a statement: "GDPR compliance requires appointment of a Data Protection Officer, implementation of technical safeguards, and regular compliance audits." The query uses question words and modal verbs. The document uses declarative sentences and specific requirements. These structural differences push queries and documents apart in embedding space, even when they are topically identical.

The linguistic patterns are different at every level. Queries tend to be short, informal, and exploratory. They use question words like what, how, why, when, and where. They include modal verbs like should, can, must, and might. They express uncertainty and information needs. Documents, especially professional and technical documents, use declarative statements, formal terminology, and structured information presentation. They state facts, provide procedures, define concepts, and establish requirements. The discourse structure is completely different.

This structural mismatch affects retrieval quality in ways that are invisible when you measure embedding quality on standard benchmarks. Benchmarks typically measure how well embeddings cluster semantically similar documents or how well they retrieve documents given document queries. But real users issue question-form queries against statement-form documents. The embedding model might excel at document-document similarity and still fail at query-document matching because it was never optimized for bridging this structural gap.

Hypothetical Document Embeddings, or HyDE, solves this problem by transforming the query into the form of a document. Instead of embedding the user's question and searching for documents similar to that question, you use an LLM to generate a hypothetical answer to the question. You then embed that hypothetical answer and search for documents similar to it. The intuition is that document-to-document similarity is more reliable than query-to-document similarity. Your hypothetical answer and the real answer in your corpus are both documents with similar structure, terminology, and information density. They will be close in embedding space even if the original question and the real answer would have been far apart.

## How HyDE Works in Practice

The HyDE pipeline has three steps. First, you receive a user query. Second, you prompt an LLM to generate a hypothetical answer to that query as if it were a knowledgeable document. Third, you embed the hypothetical answer and use it as your retrieval query. The retrieved documents are responses that are semantically similar to your hypothetical answer, which means they are likely to contain the information needed to answer the user's original question.

For example, a user asks "What are the main benefits of using serverless architecture." You send this to an LLM with a prompt like "Generate a detailed answer to this question as it would appear in a technical documentation: What are the main benefits of using serverless architecture." The LLM returns something like "Serverless architecture offers several key benefits including reduced operational overhead since infrastructure management is handled by the cloud provider, automatic scaling to handle variable workloads, cost efficiency through pay-per-use pricing models, and faster time to market due to reduced infrastructure complexity." You embed this generated answer and search your vector database for similar documents.

Why does this work better than embedding the original query? Because the hypothetical answer uses the language of technical documentation: declarative sentences, specific benefit categories, technical terminology. Your actual documentation about serverless architecture uses the same language. The embedding similarity between your hypothetical answer and your real documentation is high. The embedding similarity between the question "What are the main benefits" and your documentation is lower because questions and documentation have different linguistic patterns.

The transformation happens at the linguistic level. Questions use interrogative constructions and information-seeking language. Answers use informative constructions and information-providing language. Your corpus contains answers, not questions. By transforming the query into an answer before embedding, you make the retrieval problem document-to-document instead of query-to-document. The embedding model performs better because it is matching similar document types rather than dissimilar discourse forms.

HyDE is particularly effective for questions that require synthesis or comparison. A user asks "How does Kubernetes differ from Docker Swarm for container orchestration." You generate a hypothetical answer: "Kubernetes differs from Docker Swarm in several ways. Kubernetes offers more complex and powerful orchestration capabilities with extensive configuration options, while Docker Swarm provides simpler setup and tighter Docker integration. Kubernetes has a steeper learning curve but better scalability for large deployments. Docker Swarm is easier for small teams but has fewer advanced features." This hypothetical answer embeds the comparison structure and the specific dimensions being compared: complexity, setup, scalability, learning curve. Documents comparing these tools will use similar comparison structures and dimensions, making retrieval more effective.

The quality of the hypothetical answer determines retrieval quality. If your LLM generates a vague, generic answer, you will retrieve vague, generic documents. If your LLM generates a specific, detailed answer, you will retrieve specific, detailed documents. You control this through prompt engineering. Your prompt should instruct the LLM to generate answers that match the style, detail level, and structure of your corpus documents. If your corpus is technical and detailed, prompt for technical and detailed answers. If your corpus is concise and structured, prompt for concise and structured answers.

## When HyDE Helps and When It Hurts

HyDE is not universally beneficial. It helps most when there is a large structural gap between queries and documents. Academic literature search benefits enormously from HyDE because research papers are dense, technical, and structurally different from search queries. Technical documentation search benefits from HyDE when users ask exploratory questions rather than looking up specific facts. Conversational QA benefits from HyDE when users ask complex questions that require synthesis.

You can identify structural gaps by examining your query logs and comparing them to your corpus. If user queries are predominantly questions and your corpus is predominantly statements, you have a structural gap. If user queries use conversational language and your corpus uses formal language, you have a structural gap. If user queries are short and your corpus documents are long, you have a structural gap. These gaps are where HyDE provides the most value.

HyDE helps less when queries are already document-like. If a user submits "serverless architecture benefits reduced operational overhead automatic scaling cost efficiency," this query is already in document language. Generating a hypothetical answer does not add much value because the query is already similar to how documents express this information. HyDE also helps less for simple factual lookups. If a user asks "what is the capital of France," generating a hypothetical answer "The capital of France is Paris" does not improve retrieval because the query and answer are both short and simple.

The pattern recognition is important. You need to classify queries into types where HyDE helps and types where it does not. Exploratory queries, complex analytical queries, and synthesis queries benefit from HyDE. Factual lookup queries, keyword searches, and already-document-like queries do not. You can use query complexity scoring and query classification from earlier subchapters to route queries to HyDE selectively. Complex queries get HyDE. Simple queries skip HyDE and use direct embedding.

HyDE can hurt retrieval when the hypothetical answer is wrong or misleading. LLMs hallucinate. If your LLM generates a confidently wrong hypothetical answer, you will embed that wrong answer and retrieve documents similar to wrong information. For example, if a user asks "what is the recommended dosage of aspirin for heart attack prevention," and your LLM hallucinates "the recommended dosage is 500mg daily," you will retrieve documents discussing high-dose aspirin rather than the correct low-dose aspirin preventive regimens. The retrieval is semantically successful but factually misaligned with what the user needs.

Hallucination risk is domain-dependent. For general knowledge queries, hallucination is manageable because the LLM has seen similar information during training. For specialized domains, rare facts, or recent information, hallucination risk is higher. Medical dosages, legal precedents, technical specifications, and regulatory requirements are all high-risk areas where hallucinated hypothetical answers can lead to dangerous misretrieval.

You mitigate hallucination risk by using HyDE only in domains where retrieval is based on semantic similarity rather than factual correctness. If you are retrieving technical documentation to answer how-to questions, hallucinated details in your hypothetical answer are less harmful because the retrieval is based on topic and structure, not specific facts. If you are retrieving medical or legal information where precision is critical, hallucination risk is higher and you need additional safeguards.

Additional safeguards include using multiple hypothetical answers with different LLMs or different prompts, then retrieving using all of them and merging results. If one LLM hallucinates, the others might not, and the merged results will be more robust. You can also use confidence scoring on the hypothetical answer. If the LLM expresses low confidence in its generation, you skip HyDE and use direct query embedding instead. Some advanced implementations use fact-checking modules that verify key facts in the hypothetical answer before using it for retrieval.

Another scenario where HyDE hurts is when the user's query is very specific and your hypothetical answer generalizes it. A user asks "what is the GDPR Article 17 right to erasure implementation deadline for our company given that we are a data processor not a controller." The LLM generates a generic hypothetical answer about GDPR Article 17 without preserving the critical distinction between processor and controller. You retrieve general information about erasure rights instead of the specific guidance for data processors. The hypothetical answer lost important details from the original query.

This detail-loss problem is common when queries contain many specific constraints, entities, or qualifiers. The LLM's generation process tends to simplify and generalize, discarding nuances that seem minor but are actually critical for retrieval. A query about "Python 3.11 async generator performance optimization techniques" might produce a hypothetical answer about "Python async generators" that loses the version specificity and the performance focus. You retrieve generic async generator documentation instead of version-specific performance guidance.

You handle this by prompting the LLM to preserve all specific constraints and details from the original query in the hypothetical answer. Your prompt might be "Generate a detailed answer to this question, preserving all specific constraints, entities, and details mentioned in the question." You also use query complexity scoring from earlier subchapters. For highly specific queries with many constraints, you might skip HyDE and use direct query embedding instead. For broad exploratory queries, you use HyDE.

Another technique is to use constraint extraction before HyDE. You identify critical constraints in the query—version numbers, entity names, date ranges, specific parameters—and ensure they appear verbatim in the hypothetical answer. You can template the LLM response to enforce inclusion of these constraints. "When generating the answer, ensure that these specific terms appear: GDPR Article 17, data processor, implementation deadline." This reduces the risk of losing critical details during hypothesis generation.

## Latency Cost and When to Pay It

HyDE requires an additional LLM call to generate the hypothetical answer. This adds latency. Depending on your LLM choice and prompt length, you are adding 300 to 1200 milliseconds per query. For latency-sensitive applications, this cost is prohibitive. A user expecting sub-second search results will not tolerate an extra second for HyDE processing. For latency-tolerant applications like research, due diligence, or deep analysis, the quality improvement is worth the latency cost.

The latency impact is worse when you use large, powerful LLMs for hypothesis generation. GPT-4 or Claude Opus might produce excellent hypothetical answers but take 800 to 1200 milliseconds per generation. Smaller models like GPT-3.5 or Claude Haiku produce decent hypothetical answers in 200 to 400 milliseconds. The quality difference between large and small models for hypothesis generation is often smaller than the quality difference for final answer generation, because hypothesis generation is a simpler task. You do not need perfect answers; you just need good-enough document-like text.

You can reduce HyDE latency by using smaller, faster models for hypothesis generation. You do not need your most powerful LLM to generate hypothetical answers. A smaller model can produce good-enough hypothetical documents for retrieval purposes. You can also use aggressive prompt optimization to reduce generation length. A 50-token hypothetical answer is often sufficient for effective retrieval. You do not need a 300-token detailed response. Shorter generation reduces both latency and cost.

Token length tuning is underappreciated. Many teams use default LLM generation settings that produce verbose outputs. For HyDE, you want concise outputs that capture the key concepts without unnecessary elaboration. You can set max tokens to 50 or 100 and use prompts that emphasize brevity. "Generate a concise answer to this question in 2-3 sentences." The retrieval quality difference between a 50-token and 200-token hypothetical answer is usually negligible, but the latency and cost savings are substantial.

Another latency optimization is to use HyDE selectively based on query characteristics. If initial retrieval using the raw query returns high-confidence results, you skip HyDE and proceed with answer generation. If initial retrieval returns low-confidence results or high diversity in retrieved documents, you fall back to HyDE and retry retrieval. This adaptive approach uses HyDE only when needed, reducing average latency while maintaining quality for difficult queries.

Confidence scoring for retrieval results requires calibration. You measure the similarity scores from your initial retrieval and set thresholds. If the top result has similarity above 0.85 and the gap between the top result and the second result is above 0.1, you have high confidence and skip HyDE. If the top result has similarity below 0.7 or the gap is small, you have low confidence and trigger HyDE. You tune these thresholds on your validation set by measuring how often skipping HyDE leads to poor answer quality.

You can also parallelize HyDE processing with other query transformations. While your LLM generates the hypothetical answer, you simultaneously perform traditional query rewriting, acronym expansion, and synonym enrichment. You then retrieve using both the HyDE-generated document and the traditionally rewritten query. You merge the two result sets using reciprocal rank fusion, which we cover in a later subchapter. This parallel approach increases retrieval quality without doubling latency because both paths run simultaneously.

Parallel processing requires infrastructure that can handle concurrent operations. You need asynchronous programming or multi-threading to run hypothesis generation and traditional query processing at the same time. The total latency becomes the maximum of the two paths plus merging overhead, rather than the sum. If hypothesis generation takes 400 milliseconds and traditional rewriting takes 50 milliseconds, total latency is around 410 milliseconds plus 20 milliseconds for merging, rather than 470 milliseconds if run sequentially.

## Variants and Improvements Over Basic HyDE

Basic HyDE generates one hypothetical answer and retrieves using that. Advanced variants generate multiple hypothetical answers with different perspectives or phrasings and retrieve using all of them. For a query like "what are the security risks of using third-party APIs," you generate three hypothetical answers: one focused on data privacy risks, one focused on availability and reliability risks, and one focused on authentication and authorization risks. You retrieve using all three hypothetical documents and merge the results. This multi-perspective approach increases coverage of different aspects of the topic.

Multi-hypothesis HyDE addresses the single-perspective limitation of basic HyDE. A single hypothetical answer represents one interpretation of the query. Different interpretations might lead to different sets of relevant documents. By generating multiple hypotheses with different emphases, you cover more ground. The downside is multiplied latency and cost. If you generate three hypotheses, you are making three LLM calls and three retrieval calls. You can mitigate this by parallelizing hypothesis generation and retrieval, but the cost still scales linearly with the number of hypotheses.

You can also vary the hypotheses systematically based on query analysis. If the query is ambiguous, generate hypotheses corresponding to different interpretations. If the query is multi-faceted, generate hypotheses corresponding to different facets. If the query is comparative, generate hypotheses from the perspective of each entity being compared. This structured variation ensures that your multiple hypotheses are genuinely diverse rather than redundant paraphrases.

Another variant is chain-of-thought HyDE. Instead of directly generating a hypothetical answer, you prompt the LLM to think through how it would answer the question, then generate the answer. The chain-of-thought reasoning helps the LLM produce more accurate and structured hypothetical answers, reducing hallucination risk. The prompt might be "First, break down what this question is asking. Then generate a detailed answer." The LLM's reasoning process often leads to better hypothetical documents that align more closely with real documents in your corpus.

Chain-of-thought prompting works by making the reasoning explicit. When the LLM writes out its thinking before generating the answer, it catches errors, considers multiple angles, and produces more coherent responses. For HyDE, this means fewer hallucinations and better alignment with how real documents would answer the question. The cost is additional tokens in the prompt and response, increasing both latency and cost. You use chain-of-thought HyDE selectively for complex or ambiguous queries where answer quality is critical.

A third variant is corpus-aware HyDE. You include metadata about your corpus in the hypothesis generation prompt. "Generate an answer to this question in the style and terminology of enterprise software documentation" or "Generate an answer in the style of academic medical literature." This style guidance helps the hypothetical answer match the linguistic patterns of your specific corpus, improving embedding similarity. You can even include example documents from your corpus in the prompt to further align the generated hypothesis with your corpus style.

Corpus-aware HyDE is especially valuable when your corpus has a distinctive style or terminology. Legal documents use specific citation formats and formal language. Medical literature uses standardized terminology and structured abstracts. Technical specifications use precise language and standard formats. By prompting the LLM to generate hypotheses in the same style, you improve the likelihood that the hypothetical answer will be similar to real corpus documents in both content and form.

You implement corpus-aware prompting by analyzing your corpus to identify stylistic patterns, then encoding those patterns in your prompt. "Generate a hypothetical research paper abstract answering this question. Use formal academic language, include background, methods, results, and conclusions sections, and use medical terminology." The more specific your style guidance, the better the alignment between hypothesis and corpus.

Reverse HyDE flips the approach. Instead of generating a hypothetical answer to the query, you generate a hypothetical question that would elicit the answer provided in each document. During indexing, you generate hypothetical questions for each document chunk and store those questions alongside the chunks. At retrieval time, you embed the user's query and search for similar hypothetical questions. The documents associated with the most similar hypothetical questions are your retrieval results. This approach pre-computes the query-document bridge at indexing time rather than retrieval time, eliminating the latency penalty. The tradeoff is increased storage cost and indexing complexity.

Reverse HyDE is appealing for latency-critical applications where you cannot afford the retrieval-time LLM call. You pay the cost upfront during indexing, generating hypothetical questions for every chunk. The storage overhead is manageable—you add one additional text field per chunk. The indexing time increases because you need to generate questions for millions of chunks, but indexing is offline and can be parallelized. At retrieval time, you have zero additional latency because you are just searching the hypothetical question index.

The challenge with reverse HyDE is generating good hypothetical questions. The LLM must predict what questions users might ask that this document chunk would answer. For some chunks, this is straightforward. For others, especially dense technical or reference material, generating good questions is difficult. You might generate 2-5 hypothetical questions per chunk to increase coverage. You can also combine reverse HyDE with regular retrieval, using reverse HyDE as one retrieval method and merging its results with traditional semantic search.

## Measuring HyDE Impact on Retrieval Quality

You measure HyDE effectiveness by comparing retrieval quality with and without it. Take a test set of 200 queries with known relevant documents. For each query, retrieve using three methods: direct query embedding, traditional query rewriting, and HyDE. Measure recall at different k values for each method. If HyDE is effective, it should show higher recall at k equals 5 and k equals 10 compared to the other methods.

The evaluation protocol is critical. You need queries that represent real user information needs and labeled relevant documents for each query. Synthetic or artificial evaluation sets often do not capture the query-document mismatch that HyDE is designed to solve. Use actual user queries from logs and actual documents from your corpus. Have domain experts label which documents are relevant for each query. This labeling is expensive but necessary for meaningful evaluation.

In production systems where HyDE is well-suited, you typically see 15 to 35 percent improvement in recall at k equals 10. This means HyDE retrieves significantly more relevant documents in the top 10 results. Precision often stays similar or increases slightly because the hypothetical answer is semantically closer to relevant documents than the raw query, reducing false positive retrievals. The combination of higher recall and stable precision is powerful for improving end-to-end answer quality.

The magnitude of improvement depends on the baseline quality and the structural gap. If your baseline retrieval already has 80 percent recall, HyDE might improve it to 88 or 90 percent—a meaningful but not dramatic improvement. If your baseline recall is 60 percent, HyDE might improve it to 75 or 80 percent—a large improvement. Systems with larger query-document structural gaps see larger HyDE benefits.

You also need to measure failure modes. For what percentage of queries does HyDE produce worse results than direct retrieval? If this percentage is above 10 to 15 percent, your HyDE implementation needs improvement. Common causes of HyDE failure include hallucination, loss of specific constraints, and style mismatch between hypothetical answers and corpus documents. You address these by improving your hypothesis generation prompts, adding corpus-awareness, and using selective HyDE based on query characteristics.

Failure analysis requires examining individual queries where HyDE underperformed. You read the query, the hypothetical answer, and the retrieved documents. You identify what went wrong. Did the hypothetical answer hallucinate? Did it lose critical details? Did it use the wrong style or terminology? You collect these failure patterns and iterate on your prompts and routing logic to reduce failures.

Cost measurement is also critical. Each HyDE query requires an LLM inference call. If you are using a model like GPT-4 or Claude, each call costs between 0.01 and 0.05 dollars depending on prompt and completion length. At 10,000 queries per day, you are spending 100 to 500 dollars per day on HyDE inference. Over a year, this is 35,000 to 180,000 dollars. You need to determine whether the retrieval quality improvement justifies this cost for your use case. High-value applications like legal research, medical diagnosis support, or strategic business intelligence can easily justify the cost. Lower-value applications like casual FAQ lookup cannot.

Cost-benefit analysis should include both direct costs and opportunity costs. The direct cost is the LLM inference spend. The opportunity cost is the value of improved retrieval. If better retrieval leads to faster time-to-answer for high-value users, the benefit might be worth thousands of dollars per user per year. If better retrieval reduces incorrect decisions or missed information, the benefit might be even higher. You quantify these benefits through user studies, business metrics, and productivity measurements.

## Combining HyDE With Other Query Transformations

HyDE is not mutually exclusive with other query transformation techniques. You can use HyDE in combination with query rewriting, expansion, and decomposition. One effective pattern is to use query rewriting first to clean up and clarify the query, then use HyDE to generate the hypothetical document. The rewritten query is clearer and more specific, so the hypothetical answer generated from it is also more accurate and aligned with user intent.

Sequential combination involves running transformations in a pipeline. Query rewriting corrects spelling, expands acronyms, and resolves ambiguities. The cleaned query goes to HyDE for hypothesis generation. The hypothetical answer goes to retrieval. Each transformation builds on the previous one. This pipeline approach is simple to implement and easy to debug, but it accumulates latency from each stage.

Another pattern is to use HyDE for the main query and traditional rewriting for sub-queries. If you decompose a complex query into three sub-queries, you might use HyDE for the primary sub-query that requires synthesis and use traditional embedding for the secondary sub-queries that are factual lookups. This hybrid approach optimizes cost and latency by using HyDE only where it provides the most value.

Selective application based on sub-query type requires classification and routing logic. You classify each sub-query as synthesis, factual, procedural, or comparative. Synthesis queries get HyDE. Factual queries get direct embedding. Procedural queries might get query rewriting with step-oriented prompting. Comparative queries might get multi-hypothesis HyDE with each hypothesis representing one side of the comparison. This fine-grained control maximizes quality while minimizing cost.

You can also use HyDE as a fallback strategy. First, retrieve using traditional methods. If the retrieval confidence is low or the top results have poor diversity, trigger HyDE retrieval. Generate a hypothetical answer and re-retrieve using that. Merge the original results and the HyDE results. This adaptive approach gives you the speed of traditional retrieval for easy queries and the quality of HyDE for difficult queries. Your average latency is lower than always-on HyDE, and your quality is higher than never using HyDE.

Confidence-based fallback requires careful threshold tuning. You measure retrieval confidence using similarity scores, score gaps, or diversity metrics. You set thresholds that trigger HyDE when confidence is low. You validate these thresholds on your test set by measuring how often fallback to HyDE improves results and how often it wastes resources on queries that would have been fine without it. Over time, you refine thresholds based on production performance.

## When to Use HyDE in Production

HyDE is most valuable in four scenarios. First, when your users ask complex, exploratory questions that require synthesis. These questions benefit most from the document-to-document similarity that HyDE enables. Second, when there is a large structural gap between how users ask questions and how your documents express information. Academic papers, legal documents, and technical specifications have very different language patterns from conversational queries. HyDE bridges this gap.

Third, when retrieval recall is more important than latency. If your users are doing research, due diligence, or deep analysis, they will tolerate an extra second of latency for significantly better retrieval coverage. Fourth, when you have budget for the additional LLM inference costs. HyDE is not free. If your cost constraints are tight, you need to evaluate whether the quality improvement justifies the expense.

Use case alignment is critical. You should not deploy HyDE just because it is a sophisticated technique. You should deploy it because your specific use case has characteristics that HyDE addresses. If your users ask questions and your corpus provides answers, you have the structural gap that HyDE solves. If your users already use document-like queries, HyDE adds little value.

HyDE is less valuable when your users ask simple factual questions, when latency requirements are strict, when your corpus and query language are already well-aligned, or when cost constraints are prohibitive. You should measure retrieval quality with and without HyDE on your actual queries and corpus before committing to it in production. The theoretical benefits are clear, but the practical benefits depend heavily on your specific use case, user behavior, and corpus characteristics.

User behavior analysis helps you decide. Review query logs to understand how users phrase their information needs. Are they asking questions or stating keywords? Are they using natural language or technical jargon? Are their queries similar in style to your corpus or dissimilar? The larger the style gap, the more valuable HyDE becomes. The smaller the gap, the less valuable.

The companies that get the most value from HyDE are those that customize the hypothesis generation process to their domain. They craft prompts that produce hypothetical answers in the style and terminology of their corpus. They use corpus-aware generation that understands their document structures. They measure retrieval quality improvements and iterate on their HyDE implementation until it consistently outperforms traditional retrieval. They do not treat HyDE as a plug-and-play solution. They treat it as a technique that requires domain adaptation and careful tuning. You should approach it the same way.

Domain customization starts with corpus analysis. What is the typical structure of documents in your corpus? What terminology do they use? What level of formality? What discourse patterns? You encode these patterns into your hypothesis generation prompts. You provide examples of corpus documents to guide the LLM. You test generated hypotheses against corpus documents to measure style alignment. This iterative refinement process produces a HyDE implementation that is tailored to your specific retrieval challenge, not a generic solution that might or might not work.
