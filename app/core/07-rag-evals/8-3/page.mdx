# 8.3 â€” Cost Management: Embedding, Storage, and Inference Costs

**RAG costs scale with usage, and unmonitored scaling leads to budget catastrophes.** Every query triggers embedding calls, vector searches, reranking inference, and LLM generation, each with per-request costs that multiply across millions of queries. Treating cost as a future optimization problem instead of a current operational metric means discovering runaway expenses months into growth, when cutting costs requires emergency architecture changes under financial pressure.

They had focused obsessively on building features and scaling infrastructure to handle growth, treating costs as an afterthought, something to optimize later when they had more revenue. Feature velocity was the priority. Cost was a future problem. Now the future had arrived, and costs were growing faster than revenue. At their current burn rate, they would exhaust their runway three months earlier than projected. The CEO demanded a plan to cut costs by 50 percent without degrading quality. Not a suggestion. A mandate.

The team spent four desperate weeks re-architecting their RAG pipeline under immense pressure. They analyzed every API call, every database query, every token processed. They renegotiated vendor contracts, begging for volume discounts. They implemented aggressive caching, semantic deduplication, and model downgrading. They cut retrieval top-k from 50 to 20. They switched 60 percent of queries to cheaper LLM models. They self-hosted embeddings on GPU instances. They reduced costs to 22,000 dollars per month, meeting the target, but the delay consumed their feature roadmap for an entire quarter. Two engineers burned out and left. A fundraising round was nearly derailed when investors questioned why unit economics were so poor. They learned a painful lesson: RAG cost management is not optional, and optimization after the fact is far more expensive than building discipline from the start.

## Understanding RAG Cost Structure Before You Scale

You need to understand RAG cost structure before you scale, not after. RAG systems incur costs at five stages: embedding generation, vector storage, embedding queries, reranker inference, and LLM generation. Each stage has different cost drivers, scaling characteristics, and optimization levers. If you do not model these costs early, you will build a system that works beautifully in development, processes 10,000 queries per month affordably, and becomes unsustainably expensive at 1 million queries per month.

By then, refactoring is risky. Customers depend on current performance levels. Product teams resist changes that might degrade user experience. Cost optimization competes with feature development for engineering resources. You are trapped between burning cash and breaking things. You build cost-awareness into your architecture from day one, not as an afterthought when the CFO panics.

Embedding generation costs scale with the volume of text you embed. If you use an external embedding API such as OpenAI, Cohere, or Voyage AI, you pay per token embedded. Prices in 2026 range from 0.02 dollars per million tokens for older models to 0.10 dollars per million tokens for state-of-the-art models. A typical document corpus might average 500 tokens per chunk. Embedding 1 million chunks costs 25 dollars at 0.05 dollars per million tokens. That is the one-time cost of indexing your corpus. The cost is manageable for initial indexing, but it recurs whenever you update documents, expand your corpus, or switch embedding models.

Ongoing costs arise when you embed user queries. Each query is typically 10 to 50 tokens. At 1 million queries per month and 30 tokens per query, you embed 30 million tokens per month, costing 1.50 dollars. Embedding costs appear trivially cheap at low volume. But they scale linearly with query count, and linear growth becomes expensive at scale. At 10 million queries per month, embedding costs 15 dollars. At 100 million queries, 150 dollars. At 1 billion queries, 1,500 dollars. Embedding is the smallest line item in your RAG budget, but it is not free, and it scales relentlessly.

## Self-Hosting Embeddings: When Does It Pay Off?

Self-hosting embeddings eliminates per-token API costs but introduces infrastructure costs. You run an embedding model on your own servers, AWS EC2 instances, or serverless functions, paying for compute instead of API calls. Small embedding models such as all-MiniLM-L6-v2 can run on CPU instances, costing a few hundred dollars per month for moderate query volumes. Larger models such as OpenAI's text-embedding-3-large or Cohere's embed-v3 require GPU instances for acceptable latency, costing 500 to 2,000 dollars per month depending on instance type and utilization.

Self-hosting becomes cost-effective at high query volumes. If you process 50 million queries per month, API costs are 75 dollars per month at 0.05 dollars per million tokens. A single GPU instance costs 800 dollars per month. Self-hosting is more expensive. But if you process 500 million queries per month, API costs are 750 dollars, and self-hosting breaks even. At 5 billion queries per month, API costs are 7,500 dollars, while a GPU cluster costs 3,000 dollars, saving 4,500 dollars monthly.

The breakeven depends on your query volume, embedding model size, inference throughput, and operational overhead. Self-hosting requires expertise: model deployment, performance tuning, scaling, and monitoring. Small teams may lack this expertise, making APIs more cost-effective even at high volumes. Large teams with ML infrastructure can self-host efficiently, turning fixed GPU costs into significant savings.

## Batching Embedding Requests for Efficiency

Batching embedding requests reduces both API costs and infrastructure costs. If you embed queries one at a time, you pay full latency and overhead for each request. If you batch 10 queries together, you amortize overhead across the batch, increasing throughput and reducing cost per query. Embedding APIs often support batch requests with discounted pricing or simply higher efficiency. Self-hosted models achieve higher GPU utilization with batching, reducing the number of instances needed to handle target throughput.

Batching introduces latency trade-offs. You must wait for a batch to fill before processing, adding milliseconds or seconds to response time. For real-time user queries, batch sizes are small, typically 4 to 16, to keep latency acceptable. For offline indexing or batch processing, batch sizes can be hundreds or thousands, maximizing throughput and minimizing cost.

## Vector Storage: Costs That Grow With Your Corpus

Vector storage costs scale with corpus size and dimensionality. Vector databases charge for storage volume, typically 0.10 to 0.50 dollars per GB per month, and for IOPS or query throughput, typically 0.01 to 0.10 dollars per thousand queries. A vector with 1,536 dimensions stored as 32-bit floats occupies 6 KB. A corpus of 1 million vectors occupies 6 GB, costing 0.60 to 3.00 dollars per month for storage alone. Query costs at 1 million queries per month range from 10 to 100 dollars depending on the database, query complexity, and indexing strategy.

Storage costs grow linearly with corpus size. A 10 million vector corpus costs 10x the storage. A 100 million vector corpus costs 100x. Query costs can grow sub-linearly if the database is sharded efficiently or if approximate nearest neighbor indexes reduce the number of comparisons required. But query costs still scale with volume, and at high query rates, query costs can exceed storage costs by an order of magnitude.

Dimensionality reduction is a powerful cost optimization. If you reduce embedding dimensions from 1,536 to 768, you halve storage costs and reduce query latency because fewer dimensions must be compared. Many embedding models support dimension truncation: you discard the least significant dimensions without re-embedding, trading a small quality loss for significant cost savings. Empirical studies show that truncating dimensions by 50 percent typically degrades retrieval quality by less than 5 percent, an acceptable trade-off for many use cases.

Principal component analysis or other compression techniques can reduce dimensions further, extracting the most informative components and discarding noise. Quantization reduces precision: storing embeddings as 16-bit or 8-bit integers instead of 32-bit floats cuts storage costs by 50 to 75 percent. Quantization degrades similarity score precision, but empirical studies show minimal impact on retrieval quality for well-calibrated quantization schemes. Some vector databases support quantization natively, automatically compressing vectors on write and decompressing on read.

## Vector Database Selection: Managed vs Self-Hosted

Vector database selection impacts cost significantly. Managed services such as Pinecone, Weaviate Cloud, and Qdrant Cloud offer predictable pricing, operational simplicity, and automatic scaling, but they charge premiums for convenience. Pinecone charges based on pod count and query volume, typically 70 to 300 dollars per month per pod depending on pod size and region. A production deployment with high availability might require 3 pods, costing 210 to 900 dollars per month before query charges.

Self-hosted open-source databases such as Milvus, Qdrant, or FAISS require infrastructure management, monitoring, and scaling, but they offer lower costs at scale. You pay only for the underlying compute and storage, typically 100 to 500 dollars per month for a modest deployment on AWS or GCP. For high-traffic systems with millions of queries per day, self-hosting can save thousands of dollars per month compared to managed services.

Serverless vector databases such as Pinecone's serverless tier charge only for queries and storage, eliminating idle costs for low-traffic applications. You pay per query and per GB stored, with no minimum spend. Serverless is cost-effective for spiky or unpredictable traffic, where traditional pod-based pricing would require over-provisioning. For steady high-volume traffic, pod-based or self-hosted options are cheaper.

Hybrid approaches balance cost and complexity: use managed databases for production to ensure reliability and simplicity, and use self-hosted for development, staging, or batch processing to reduce costs. Some teams start with managed services for speed and simplicity, then migrate to self-hosted as query volume and engineering maturity increase.

## Embedding Query Costs and Index Tuning

Embedding query costs are typically lower than storage costs at low volumes but become significant at high query volumes. Each query to the vector database incurs compute costs: the database computes similarity scores between the query vector and millions of stored vectors. Approximate nearest neighbor algorithms such as HNSW, Hierarchical Navigable Small World graphs, or IVF, Inverted File Index, reduce query costs by trading exact results for fast approximations.

Tuning index parameters, such as the number of clusters in IVF or the number of graph layers in HNSW, affects both query latency and cost. More aggressive approximations reduce cost but may degrade retrieval quality, returning results that are close to the true top-k but not exact. You tune these parameters empirically, measuring the trade-off between cost, latency, and relevance. For most use cases, approximate results are indistinguishable from exact results, and aggressive tuning yields significant savings with no perceptible quality loss.

## Reranker Inference: The Expensive Bottleneck

Reranker inference costs arise when you use a cross-encoder model to reorder retrieved candidates. Rerankers process query-document pairs, computing relevance scores through full transformer inference. This is computationally expensive. Reranker APIs charge per pair: 0.0001 to 0.001 dollars per pair, depending on model size and provider. If you retrieve 50 candidates and rerank all of them, each query costs 0.005 to 0.05 dollars. At 1 million queries per month, reranker costs range from 5,000 to 50,000 dollars. Rerankers are often more expensive than embeddings, vector databases, and LLMs combined.

Reranker cost optimization starts with reranking fewer candidates. If retrieval returns 50 documents, rerank only the top 20 by vector similarity, reducing reranker inference by 60 percent. Empirical studies show that reranking the top 20 to 30 captures most of the quality gain, and reranking beyond that offers diminishing returns. Alternatively, use a two-stage reranker: apply a cheap, fast model to filter the top 50 down to 20, then apply an expensive, accurate model to rerank those 20. The cheap model might be a lightweight cross-encoder or a pointwise relevance classifier. The expensive model is your state-of-the-art reranker.

Self-hosting rerankers reduces per-query costs but requires GPU infrastructure. Cross-encoder models such as ms-marco-MiniLM or bge-reranker-large require significant compute for inference. A single GPU instance can process hundreds of query-document pairs per second, supporting thousands of queries per hour. At high query volumes, self-hosting is cost-effective: 50,000 dollars per month in API costs justifies a 5,000 dollar per month GPU cluster, saving 45,000 dollars monthly. At low volumes, API costs are cheaper and simpler.

## LLM Generation Costs: The Dominant Expense

LLM generation costs dominate RAG system expenses at scale. In 2026, OpenAI charges 0.005 dollars per 1,000 input tokens and 0.015 dollars per 1,000 output tokens for GPT-5. Anthropic charges similar rates for Claude Sonnet. A typical RAG query sends 2,000 tokens of context plus a 50-token query, totaling 2,050 input tokens, and receives a 300-token response. The cost per query is 0.01 dollars for input and 0.0045 dollars for output, totaling 0.0145 dollars. At 1 million queries per month, LLM costs are 14,500 dollars. At 10 million queries, costs are 145,000 dollars. LLM costs are the single largest line item for high-traffic RAG systems.

LLM cost optimization focuses on reducing token usage without sacrificing quality. Shorter context reduces input token costs. If you retrieve 10 chunks of 200 tokens each, you send 2,000 tokens. If you retrieve 5 chunks, you send 1,000 tokens, halving input costs. Retrieval tuning, such as lower top-k or higher similarity thresholds, reduces context size. Chunk overlap reduction, such as deduplicating repeated sentences, compresses context. Summarization techniques, such as using a cheap LLM to condense retrieved chunks before passing them to the expensive LLM, can reduce token usage by 50 percent but introduce additional latency and complexity.

Output token reduction is harder but impactful. If you prompt the LLM to be concise, you may reduce output tokens from 300 to 150, halving output costs. Structured output formats, such as JSON or bullet points, are often shorter than prose. Streaming responses allow you to interrupt generation if the user stops reading, saving tokens on unused output. Early stopping, such as terminating generation after a confidence threshold is reached or after a complete answer is formed, reduces tokens for queries that can be answered briefly.

## Model Selection: Balancing Cost and Quality

Model selection trades cost for quality. GPT-5 costs 0.015 dollars per 1,000 output tokens. GPT-5-mini costs 0.002 dollars per 1,000 output tokens, a 7.5x reduction. Claude Opus costs more than Sonnet. Smaller open-source models such as Llama 4 Scout 8B can run on inexpensive hardware, reducing costs by 10x or more compared to API-based models. The challenge is quality: smaller models produce less accurate answers, struggle with complex reasoning, and hallucinate more frequently.

You must evaluate whether the quality trade-off is acceptable for your use case. Some teams use a two-tier approach: route simple queries to cheap models based on a complexity classifier, and route complex queries to expensive models. If 70 percent of queries are simple, such as FAQs or straightforward lookups, routing them to GPT-5-mini saves 70 percent of LLM costs with minimal quality degradation. Complex queries, such as multi-step reasoning or nuanced questions, still use GPT-5 for maximum quality.

Self-hosting LLMs eliminates per-token costs but introduces infrastructure and operational overhead. A Llama 4 Scout 8B model can run on a single GPU costing 500 dollars per month, handling thousands of queries per hour. A Llama 4 Scout 70B model requires multiple GPUs, costing 3,000 to 10,000 dollars per month. Self-hosting becomes cost-effective at very high query volumes: 10 million queries per month at 0.0145 dollars per query costs 145,000 dollars in API fees, justifying significant infrastructure investment. Self-hosting also requires expertise in model deployment, fine-tuning, serving infrastructure, and scaling, which many teams lack.

## Caching: The Highest-Leverage Optimization

Caching is the most powerful cost optimization lever across all stages of the RAG pipeline. If you cache embedding vectors for frequently asked queries, you eliminate redundant embedding API calls. If you cache retrieval results for popular queries, you eliminate vector database queries. If you cache LLM responses for common questions, you eliminate reranker and LLM costs entirely. Caching is cheap: storing cached responses costs pennies per GB per month. Cache hit rates of 20 to 40 percent are realistic for user-facing applications with repeating query patterns. A 30 percent cache hit rate reduces total RAG costs by 30 percent.

Semantic caching extends exact-match caching to similar queries. If a user asks "What is the capital of France?" and you cache the response, an exact match on that string hits the cache. Semantic caching recognizes that "What is France's capital?" is semantically identical and returns the cached response. Semantic caching uses embedding similarity: you embed the query, search a cache of embedded queries, and return the cached response if similarity exceeds a threshold such as 0.95. Semantic caching increases cache hit rates from 20 percent to 40 percent or higher, but it introduces latency for cache lookups and complexity for cache invalidation.

## Cost Per Query: The Ultimate Metric

Cost per query is the ultimate metric for RAG systems. Calculate the total cost of processing one query, including embedding, vector search, reranking, and LLM generation. Track cost per query over time as your system scales and evolves. If cost per query increases, investigate why: did you add more retrieval candidates, switch to a larger LLM, or disable caching? If cost per query decreases, identify which optimizations worked and apply them more broadly.

Cost per query informs pricing decisions: if your cost per query is 0.02 dollars, you cannot profitably charge customers 0.01 dollars per query unless you subsidize with other revenue streams or accept negative margins temporarily. If your cost per query is 0.005 dollars and you charge 0.02 dollars, you have healthy margins and room to invest in quality improvements.

## Budget Planning and Forecasting at Scale

Budget planning for RAG at scale requires forecasting query volume, corpus growth, and cost per query. If you currently process 1 million queries per month at 0.015 dollars per query, your monthly cost is 15,000 dollars. If you expect to grow to 10 million queries in six months due to customer acquisition or expanded use cases, your cost will grow to 150,000 dollars unless you optimize. Budget for optimization work: dedicating engineering time to reduce cost per query by 50 percent can save 75,000 dollars per month at 10 million queries, far exceeding the cost of the engineering effort.

Forecast conservatively. Traffic spikes, seasonal patterns, and viral growth can cause query volumes to exceed projections. Build cost controls: set spending alerts at 80 percent and 100 percent of budget, throttle low-priority queries when approaching budget limits, and implement tiered pricing for enterprise customers to pass costs through.

## The Aftermath: Cost as a Feature

The customer support startup built a comprehensive cost model after their crisis. They calculated cost per query for each component: 0.001 dollars for embedding, 0.002 dollars for vector search, 0.004 dollars for reranking, and 0.012 dollars for LLM generation, totaling 0.019 dollars. They monitored it daily on dashboards alongside latency and quality metrics. They set a target of 0.012 dollars per query, requiring 37 percent cost reduction.

They implemented semantic caching, achieving a 35 percent cache hit rate that eliminated costs for cached queries. They reduced retrieval top-k from 50 to 30, cutting reranker costs by 40 percent. They switched from GPT-5 to GPT-5-mini for 60 percent of queries, routed by a complexity classifier, reducing average LLM costs by 55 percent. They negotiated volume discounts with their embedding API provider, reducing embedding costs by 20 percent. Within three months, they reduced cost per query from 0.023 dollars to 0.011 dollars, beating their target and making the unit economics profitable. The CFO asked how they turned it around so quickly. The engineer replied: "We finally treated cost as a feature, not an afterthought."

You build cost discipline now. You model the cost structure of your RAG pipeline before you deploy to production, calculating cost per query for each component. You set cost targets aligned with your pricing model and business goals. You track cost metrics in dashboards alongside performance and quality metrics, making cost visible and actionable. You implement caching aggressively, optimize retrieval parameters empirically, and evaluate smaller models where quality allows.

You negotiate volume discounts with vendors as you scale. You consider self-hosting for high-cost components when query volume justifies infrastructure investment. You treat cost as a constraint, not an afterthought, and you optimize continuously as you scale. You build cost awareness into engineering culture: every feature, every parameter change, every model upgrade is evaluated for cost impact.

RAG systems do not scale cheaply by default. They scale cheaply when you design them to. That design starts now, with measurement, with trade-offs, with discipline, and with the recognition that cost is not just a finance problem. It is an engineering problem, a product problem, and a competitive advantage.
