# 6.5 â€” Answer Correctness: Does the Response Actually Answer the Question

In August 2024, a customer support RAG system achieved 96% faithfulness scores, meaning nearly all generated answers were grounded in retrieved knowledge base articles. Yet customer satisfaction scores were dropping, and support ticket resolution times were increasing. The engineering team investigated and discovered that many answers, while faithful to context, did not actually answer the questions asked. A customer asking "how do I cancel my subscription?" would receive a faithful summary of the subscription terms of service, which mentioned cancellation in passing but did not provide the step-by-step procedure. The system retrieved relevant documents and generated faithful summaries, but it failed to extract and present the specific information needed. The company had measured faithfulness but not correctness. They built answer correctness evaluation, comparing generated responses to ground truth answers and measuring semantic match. After retraining and redeploying with correctness as a primary metric, resolution times dropped by 30% and satisfaction scores recovered.

You built a RAG system that generates faithful answers, but faithfulness is not enough. An answer can be perfectly grounded in context and still fail to answer the question. Correctness evaluation measures whether the generated response actually provides the information the user needs. It compares the generated answer to a ground truth answer and assesses whether they convey the same information. Correctness requires both faithfulness and relevance. The answer must be grounded in context and must address the query. Measuring correctness is essential for ensuring your RAG system delivers value, not just generates safe text.

## Why Faithfulness and Correctness Diverge

Faithfulness measures adherence to context. Correctness measures alignment with the right answer. These are different properties, and they diverge in several common scenarios. Understanding when and why they diverge helps you design evaluation that captures both dimensions.

One divergence occurs when the context is incomplete. Suppose a user asks for the return policy, and the retrieved context mentions that returns are accepted but does not specify the timeframe or procedure. A faithful answer might say "returns are accepted." This answer is grounded in the context, so faithfulness is high. But it does not fully answer the question, so correctness is low. The ground truth answer is "returns are accepted within 30 days with a receipt by contacting customer support." The generated answer is missing critical details. Faithfulness does not penalize this omission because the model only reports what the context contains. Correctness does penalize it because the answer is incomplete.

Another divergence occurs when the context is correct but the model extracts the wrong information. Suppose the context contains both the return policy and the shipping policy. The user asks about returns, but the model generates an answer about shipping. The answer is faithful because it is grounded in the context, but it is incorrect because it does not answer the query. Faithfulness checks whether the answer matches the context, not whether it matches the question. Correctness checks both.

A third divergence occurs when the context is wrong. Suppose the retrieved document contains outdated information. The model faithfully summarizes this outdated information, earning high faithfulness scores. But the answer is incorrect compared to ground truth, which reflects current information. Faithfulness assumes the context is correct and only measures grounding. Correctness measures truth, regardless of what the context says. This divergence is critical in dynamic domains where documents may be stale or incorrect.

## Ground Truth Answers and Their Construction

Correctness evaluation requires ground truth answers: the correct responses to queries in your test set. Ground truth provides the reference point for measuring whether generated answers are right. Constructing high-quality ground truth is one of the most important and challenging parts of building a correctness evaluation pipeline.

Ground truth answers can come from several sources. One source is human-written answers. You have annotators read queries and retrieved context, then write the correct answer. Human-written answers are flexible and high-quality but expensive and slow to produce. Another source is existing answer data. If your system has historical user interactions, support tickets, or FAQ entries, these can serve as ground truth. Existing data is cheaper but may be noisy or inconsistent. A third source is extracted answers from authoritative documents. If your corpus includes structured data like product specifications or policy documents, you can extract answers programmatically. Extracted answers are scalable but only work for structured data.

Regardless of source, ground truth must be validated for quality. Answers should be accurate, complete, and clearly written. They should address the query directly and provide the information a user needs. If ground truth is ambiguous, incomplete, or incorrect, correctness metrics will be meaningless. Invest in quality control. Have multiple annotators review each answer, measure agreement, and resolve disagreements through discussion or escalation. High-quality ground truth is the foundation of reliable correctness evaluation.

Ground truth should also be representative. Your test set must cover the diversity of queries your system handles in production. Include easy queries, hard queries, common queries, and rare queries. Include queries with straightforward answers and queries with nuanced or conditional answers. Include queries where retrieval succeeds and queries where it fails. A representative test set ensures correctness metrics reflect real-world performance, not just performance on cherry-picked examples.

Finally, ground truth must be maintained. As your document corpus evolves, answers change. A ground truth answer that was correct six months ago might be outdated now. Periodically review and update ground truth to reflect current information. Stale ground truth leads to misleading metrics, where your system is penalized for generating answers that are actually correct but differ from outdated references. Maintaining ground truth is an ongoing investment, but it is necessary for keeping correctness evaluation accurate.

## Exact Match Versus Semantic Match

Once you have ground truth answers, you need to measure how closely generated answers match them. There are two broad approaches: exact match and semantic match. Exact match checks whether the generated answer is identical or nearly identical to ground truth. Semantic match checks whether the generated answer conveys the same meaning, even if the wording differs. Each approach has strengths and weaknesses.

Exact match is simple and deterministic. You compare the generated answer to ground truth as strings, checking for equality or high overlap. For short, factual answers like dates, numbers, or names, exact match works well. If the ground truth is "March 15, 2025" and the generated answer is "March 15, 2025," exact match succeeds. If the generated answer is "15th of March, 2025," exact match might fail even though the meaning is identical. Exact match is strict and does not tolerate paraphrasing, which makes it less suitable for longer or more expressive answers.

To handle minor variations, you can use fuzzy matching. Fuzzy matching allows small differences in wording, punctuation, or formatting. Techniques include edit distance, token overlap, and normalized string comparison. For example, you might compute the fraction of tokens in the ground truth that appear in the generated answer. If this overlap exceeds a threshold like 0.8, you consider the match successful. Fuzzy matching is more flexible than strict exact match but still struggles with paraphrasing and semantic equivalence.

Semantic match evaluates meaning rather than wording. Two answers are semantically equivalent if they convey the same information, even if phrased differently. Semantic match typically uses embeddings or language models. One approach is to embed both the generated answer and the ground truth using a sentence embedding model, then compute cosine similarity. High similarity indicates semantic match. Another approach is to use an LLM to judge whether the answers are equivalent. You prompt the LLM with both answers and ask if they convey the same information. Semantic match is more robust to paraphrasing and captures meaning, but it is less deterministic and more expensive.

For most RAG applications, semantic match is the better choice. Users care whether the answer is correct, not whether it uses the exact wording of ground truth. Semantic match aligns with this user-centric definition of correctness. However, for applications where precise wording matters, such as legal or regulatory compliance, exact or fuzzy match may be appropriate. Choose the matching method based on your application requirements and the nature of your answers.

## Partial Correctness Scoring

Correctness is not always binary. An answer can be partially correct, providing some but not all of the required information. Partial correctness scoring captures this nuance by measuring the degree of match between generated and ground truth answers. This approach provides finer-grained signal than binary correct or incorrect labels and enables more informative evaluation.

One method for partial correctness is to decompose answers into components and measure component-level match. For example, a ground truth answer to "what are the return policy terms?" might include three components: the return window, the required documentation, and the contact method. A generated answer that includes the return window and required documentation but omits the contact method is two-thirds correct. You compute partial correctness as the fraction of ground truth components present in the generated answer.

Another method is to use a graded similarity score. Instead of thresholding semantic similarity at a binary cutoff, you use the similarity score directly as a measure of partial correctness. If ground truth and generated answers have a cosine similarity of 0.9, the correctness score is 0.9. If similarity is 0.5, correctness is 0.5. This method is continuous and captures the full spectrum from completely correct to completely incorrect. It is well-suited for applications where partial credit is meaningful.

LLM-based judges can also provide partial correctness scores. You prompt the LLM to compare the generated answer and ground truth, then rate the generated answer on a scale from zero to one or zero to five based on how much of the required information it includes. The LLM can reason about missing, extra, or incorrect information and produce a nuanced score. For example, the judge might output 0.7 and explain "the answer correctly states the return window and documentation requirements but does not mention the contact method." This explanation provides interpretability and helps you understand where the model is falling short.

Partial correctness scoring is especially valuable when answers are complex or multi-faceted. For simple, atomic answers like "yes" or "March 15," binary correctness suffices. For longer answers that synthesize information from multiple sources or address multi-part queries, partial scoring provides richer feedback. It helps you distinguish between answers that are completely wrong, mostly right, and almost perfect, guiding optimization efforts more precisely than binary labels.

## When Correctness and Faithfulness Diverge

Correctness and faithfulness diverge most often when retrieval fails or when the context does not contain the information needed to answer the query. Understanding these divergence scenarios helps you diagnose system failures and decide which metric to prioritize in different situations.

One common scenario is incomplete retrieval. The query asks for three pieces of information, but retrieval only provides two. A faithful model generates an answer with the two pieces it has, omitting the third. Faithfulness is high because the model is grounded in context. Correctness is lower because the answer is incomplete. This divergence signals a retrieval problem. Improving retrieval to surface the missing information will improve correctness without requiring changes to generation.

Another scenario is irrelevant retrieval. The query asks about topic A, but retrieval returns documents about topic B. A faithful model either refuses to answer or generates a vague response acknowledging the lack of relevant context. Faithfulness is high because the model does not fabricate information. Correctness is low because the answer does not address the query. This divergence also signals a retrieval problem. The generation model is behaving correctly by not hallucinating, but the system as a whole is failing because retrieval provided the wrong context.

A third scenario is incorrect context. Retrieval provides documents that are wrong, outdated, or misleading. A faithful model summarizes these documents, earning high faithfulness scores. Correctness is low because the answer does not match ground truth. This divergence signals a data quality problem. The issue is not retrieval or generation but the underlying document corpus. Fixing this requires updating or removing incorrect documents, which is a content management task, not a model optimization task.

When correctness and faithfulness diverge, both metrics provide valuable diagnostic information. If faithfulness is high but correctness is low, the problem is usually retrieval or data quality. If correctness is high but faithfulness is low, the model is generating right answers for the wrong reasons, likely by using pretrained knowledge instead of context. This is risky because the model might hallucinate when its pretrained knowledge is wrong. Both high correctness and high faithfulness indicate a well-functioning system. Both low indicates multiple problems that need investigation.

## Practical Implementation Strategies

Implementing correctness evaluation requires ground truth construction, matching method selection, and integration into your evaluation pipeline. You need to decide whether to compute correctness offline during development, online in production, or both. You need to choose metrics and thresholds that align with your application goals.

Offline correctness evaluation during development uses labeled test sets. You have queries, retrieved contexts, ground truth answers, and generated answers. You compute semantic similarity or use an LLM judge to score correctness. Aggregate scores across the test set reveal whether your system generates correct answers consistently. Low scores indicate problems with retrieval, generation, or both. You iterate on prompts, models, and retrieval strategies until correctness meets your targets.

Online correctness evaluation in production is more challenging because you do not have ground truth for every query. One approach is to sample queries and collect ground truth answers asynchronously. Annotators review sampled queries and write correct answers, then you compute correctness scores and track them over time. Another approach is to use user feedback as a proxy for correctness. If users mark answers as helpful or unhelpful, you can treat helpful as a signal of correctness. User feedback is noisy but provides a scalable signal for continuous monitoring.

Choosing a correctness threshold depends on your application and the cost of incorrect answers. In high-stakes applications like healthcare or finance, incorrect answers can cause harm, so you need high correctness thresholds, such as 90% or 95%. In lower-stakes applications like general knowledge search, moderate correctness thresholds, such as 70% or 80%, may suffice. Calibrate thresholds based on user tolerance for errors and the availability of fallback mechanisms. If users can easily verify answers or request clarification, lower thresholds are acceptable. If answers are used without verification, higher thresholds are necessary.

Debugging incorrect answers requires examining specific failure cases. When correctness scores are low, sample incorrect answers and review them manually. Identify patterns. Are answers incomplete because retrieval missed documents? Are answers off-topic because retrieval was irrelevant? Are answers wrong because context was outdated? Each pattern suggests a different fix. Incomplete answers signal retrieval recall issues. Off-topic answers signal retrieval precision or query understanding issues. Wrong answers signal data quality issues. Correctness evaluation provides the signal, and human analysis provides the understanding needed to improve the system.

## Balancing Correctness with Other Metrics

Correctness is a critical metric, but it is not the only one. You also need to balance correctness with faithfulness, latency, cost, and user experience. Optimizing for correctness alone can lead to unintended trade-offs that degrade overall system performance.

One trade-off is between correctness and faithfulness. You can improve correctness by allowing the model to use pretrained knowledge when context is incomplete. This increases the chance of generating the right answer but reduces faithfulness. The model might fabricate information that happens to be correct most of the time but is not grounded in context. This is risky because pretrained knowledge can be wrong, outdated, or biased. A better approach is to improve retrieval so the model has the context it needs to be both faithful and correct.

Another trade-off is between correctness and latency. Achieving high correctness might require retrieving more documents, using larger models, or running multiple generation attempts and selecting the best. These strategies increase latency and cost. In latency-sensitive applications, users prefer fast, mostly correct answers over slow, perfectly correct answers. Balance correctness with response time based on user expectations and application requirements.

Correctness also trades off with coverage. A system can achieve high correctness by only answering queries it is confident about and refusing to answer uncertain queries. This improves average correctness but reduces utility because many queries go unanswered. Users prefer systems that attempt to help, even if answers are not always perfect. Balance correctness with coverage by setting appropriate confidence thresholds and providing graceful degradation when the system is uncertain.

Finally, correctness interacts with user experience. A correct but terse answer might score well on correctness metrics but poorly on user satisfaction. Users often prefer answers that provide context, explanation, or additional relevant information. Conversely, a verbose answer that includes correct information buried in irrelevant text might score well on partial correctness but frustrate users. Optimize for user experience alongside correctness, using qualitative feedback and satisfaction metrics to guide design decisions.

## Moving Forward

Answer correctness measures whether your RAG system delivers the information users need. It complements faithfulness by ensuring answers are not just grounded but also right. Correctness requires ground truth answers, which are expensive to construct but essential for meaningful evaluation. Semantic matching is the most robust way to compare generated and ground truth answers, capturing meaning rather than wording. Partial correctness scoring provides finer-grained signal and helps you understand the spectrum from completely correct to completely incorrect.

Correctness and faithfulness diverge when retrieval fails, context is incomplete, or data is incorrect. Both metrics provide diagnostic value. Measuring both helps you distinguish between retrieval problems, generation problems, and data quality problems. Balancing correctness with faithfulness, latency, coverage, and user experience ensures you optimize for real-world value, not just individual metrics.

The next sections continue building the evaluation stack, covering hallucination detection, citation accuracy, and evaluation protocols. Correctness ensures answers are right, but hallucinations introduce fabricated information that can be dangerous. Citation accuracy ensures references are trustworthy. Together with faithfulness and correctness, these metrics provide a complete picture of generation quality and enable you to build RAG systems that users can rely on.
