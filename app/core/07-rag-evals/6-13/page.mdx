# 6.13 â€” Evaluation Coverage: What Your Metrics Miss

An e-commerce company achieved ninety-four percent answer correctness, ninety percent faithfulness, and eighty-five percent retrieval precision. Every metric glowed green. Management presented these numbers to the board as proof of technical excellence. Then customers complained the system recommended discontinued products with detailed specs and pricing for items out of stock for eight months. Investigation found three dozen similar cases involving discontinued products, depleted inventory, and unavailable seasonal items. The evaluation suite had zero examples testing product availability. Faithfulness checked if information came from documents, but documents were outdated. Correctness compared against ground truth, but ground truth was never updated. Perfect scores on measured dimensions coexisted with systematic failures on unmeasured ones. Metrics have blind spots. You optimize what you measure while unmeasured quality silently degrades.

Metrics have blind spots. Every evaluation approach measures specific aspects of system behavior while ignoring others. Faithfulness metrics catch hallucinations but miss cherry-picking or biased selection of true facts. Retrieval metrics measure whether relevant documents are found but do not measure whether documents contain current information. End-to-end correctness metrics detect wrong answers but may not detect answers that are right but inappropriate for the user's context.

These blind spots are not failures of the metrics; they are inherent limitations of trying to reduce complex system behavior to numerical scores. The danger lies in treating metrics as complete measures of quality rather than partial indicators. You optimize what you measure, so unmeasured aspects of quality degrade over time as you focus engineering effort on improving measured metrics while neglecting everything else.

## Faithfulness Blind Spots

Faithfulness metrics have well-documented blind spots that become apparent when you examine failure cases. Standard faithfulness checks verify whether claims in generated answers are supported by retrieved context. An answer stating "The product costs two hundred dollars and includes free shipping" receives high faithfulness if both facts appear in retrieved documents. The metric confirms that every claim in the answer can be traced back to source material.

But faithfulness metrics typically do not check completeness of information. If the product costs two hundred dollars but requires a fifty dollar activation fee, and the activation fee appears in retrieved documents but not in the generated answer, faithfulness scores remain high. The answer is faithful to the information it includes; it simply omits critical information that users need to make informed decisions.

Users make purchasing decisions based on incomplete information and feel misled when they discover hidden fees. The faithfulness metric reported perfect scores while the system systematically withheld information that changed the value proposition. This omission problem affects legal domains where disclosure requirements mandate mentioning certain caveats, medical domains where complete risk information is essential, and financial domains where full fee structures determine product suitability.

Faithfulness metrics also struggle with synthesis and reasoning. Suppose the retrieved documents state "Product A costs one hundred dollars" and "Product B costs one hundred fifty dollars." The generated answer states "Product A is cheaper than Product B." This claim is not explicitly stated in any retrieved document, yet it is a valid inference from information that does appear.

Strict faithfulness metrics may flag this as unsupported, penalizing valuable synthesis that users expect. Loose faithfulness metrics may accept it, but then also accept invalid inferences. The metric must encode some model of valid reasoning, which varies by domain and context. Legal reasoning follows different rules than mathematical reasoning. The metric cannot easily distinguish valid domain-appropriate inference from hallucinated claims.

Cherry-picking true facts to present misleading impressions represents another faithfulness blind spot. A financial RAG system answering questions about investment risks might cite only the positive historical returns while ignoring documented risks and downsides, all of which appear in the retrieved documents. Every claim is grounded in source material, so faithfulness scores are perfect, but the answer is deeply misleading.

Detecting this requires measuring not just whether included information is supported but whether important available information is inappropriately omitted. This is a much harder problem requiring models of information importance and balance. You need to know which facts are material to the query, which omissions change the meaning significantly, and which information must be presented together to avoid misrepresentation.

## Retrieval Metric Limitations

Retrieval metrics measure whether the right documents are found and ranked appropriately, but they have limited visibility into several critical concerns. The most obvious blind spot is document content quality. Retrieval metrics check whether you retrieved document D for query Q, assuming document D is labeled as relevant in your ground truth dataset.

But what if document D contains outdated information, factual errors, or contradictory statements. The retrieval system performed correctly by finding the designated document, but the answer will still be wrong because the document itself is flawed. Retrieval evaluation cannot detect this problem because it operates purely at the document selection level without assessing document content quality.

Document granularity mismatches create another retrieval blind spot. Many RAG systems retrieve document chunks rather than full documents. Your retrieval metric might measure precision and recall at the document level: did you retrieve the right documents. But the generation model receives specific chunks from those documents, not the entire document.

You might retrieve the correct document but extract the wrong section, providing context about installation procedures when the user asked about troubleshooting. Document-level retrieval metrics show success while chunk-level retrieval failed. Measuring at the appropriate granularity requires instrumentation that tracks exactly what text the generation model receives, not just which documents were retrieved.

Retrieval metrics also miss interaction effects between retrieved documents. You might retrieve five relevant documents individually, each providing accurate information, but those documents might present contradictory information or different versions of policies. Document A says feature X is supported, document B says feature X is deprecated, and document C says feature X is available only in enterprise plans.

Each document is individually relevant to a query about feature X, so retrieval metrics look good, but the contradiction makes answer generation difficult or error-prone. The system must reconcile conflicting sources, and most RAG systems handle this poorly. Detecting this requires analyzing relationships between retrieved documents rather than evaluating each independently.

Temporal relevance represents another gap. Your retrieval system might find documents that were relevant when they were written but are outdated now. A query about current product pricing retrieves documents with last year's pricing. The documents are topically relevant but temporally stale. Standard retrieval metrics based on topic relevance do not catch temporal mismatches.

## End-to-End Metric Opacity

End-to-end metrics that measure final answer quality provide valuable signal but hide which components caused failures. Suppose your answer correctness metric drops from eighty-five percent to seventy-eight percent after a system update. You know something got worse, but you do not know whether retrieval started failing, generation quality degraded, query understanding regressed, or some combination of factors changed.

Without component-level metrics, debugging requires manual investigation of failure cases to hypothesize root causes. This is time-consuming and error-prone. You might optimize the wrong component because you misdiagnosed the failure mode. A drop in answer quality might be attributed to generation when it actually stems from retrieval finding lower-quality documents.

End-to-end metrics also average over diverse failure types, hiding important patterns. An answer correctness metric might aggregate performance on factual lookups, multi-hop reasoning questions, questions requiring temporal understanding, and questions needing synthesis across multiple sources.

If your system excels at factual lookups but fails at temporal reasoning, overall accuracy might look acceptable at eighty percent while specific critical capabilities are broken. The aggregated metric hides the concentrated failure in one capability category. Without segmentation by query type, question complexity, or required reasoning patterns, you cannot identify these localized failures.

## Missing Temporal Consistency

Metrics measuring answer quality at a single point in time miss temporal consistency. A RAG system might provide different answers to the same question on consecutive queries due to nondeterminism in retrieval or generation. Users find this inconsistency confusing and trust-destroying.

They ask the same question on Monday and Wednesday, receive different answers, and conclude the system is unreliable. Standard evaluation runs each query once, measures the quality of that single response, and moves on. Consistency problems remain invisible unless you specifically test by running queries multiple times and measuring answer variation.

Even with deterministic settings, systems can become inconsistent over time as the knowledge base evolves. The answer to "what is the return policy" might change when policies update, which is appropriate. But if the answer changes multiple times within a single day as different engineers deploy conflicting documentation updates, consistency is broken.

Measuring consistency requires evaluating the same queries across multiple runs separated by time, tracking whether answers remain stable when they should and change appropriately when underlying information changes. This temporal dimension of quality is rarely measured but frequently complained about by users.

## Intent Beyond Literal Questions

Metrics rarely capture user intent beyond the literal question text. A user asking "What is the return policy?" might actually want to know whether they can return their specific purchase given their specific situation. The system might provide a perfectly accurate general return policy statement that does not help the user.

Answer correctness scores are high because the answer is factually right, but user satisfaction is low because the answer is not useful for their actual need. Measuring this requires understanding context beyond the query text, which standard RAG evaluation datasets typically do not capture.

Users ask questions with implicit context, assumptions, and goals that are not stated explicitly. "Is product X good for my use case" assumes the system knows or can infer the use case. "What happens if I do Y" assumes the system understands the user's current state and constraints. Evaluation that treats each query as an isolated text string misses these contextual requirements.

## Format and Presentation Quality

Presentation and formatting quality often goes unmeasured. Your faithfulness and correctness metrics might be excellent, but if answers are formatted as dense paragraphs without structure, users struggle to extract information. Answers that could be formatted as bulleted lists, tables, or step-by-step instructions instead appear as paragraph prose that is technically correct but hard to use.

Evaluation frameworks focused on semantic content quality do not typically measure whether information is presented in the most usable format for the query type. A comparison query asking "what are the differences between X and Y" ideally receives a table or side-by-side comparison, not a paragraph discussing each product separately. The information might be complete and correct but poorly structured for the task.

Tone and style appropriateness similarly goes unmeasured. A customer support system might provide correct answers in overly technical language that frustrates non-expert users. A medical information system might use casual language when formal professional tone would inspire more confidence. These presentation issues affect user satisfaction significantly but rarely appear in standard evaluation metrics.

## Identifying Coverage Gaps

Identifying coverage gaps in your evaluation suite requires systematic analysis of what you measure versus what matters to users. Start by listing the quality dimensions that affect user experience: factual correctness, completeness, currentness, consistency, relevance, appropriateness, clarity, formatting, tone, safety, and any domain-specific dimensions.

For each dimension, identify whether you have metrics that measure it, how well those metrics capture the dimension, and what blind spots exist. A financial services RAG system might realize they measure correctness and faithfulness but not whether answers appropriately communicate uncertainty, whether they meet regulatory disclosure requirements, or whether they avoid language that could be construed as investment advice without proper disclaimers.

Production data analysis reveals gaps between evaluation metrics and real-world performance. Analyze support tickets, user feedback, and manual review of low-rated answers. Categorize the problems users report and compare against your evaluation metrics.

If twenty percent of complaints involve outdated information but you have no metric measuring currentness, you have found a gap. If users frequently report confusing answers but you only measure correctness not clarity, you have found another gap. This feedback loop between production experience and evaluation coverage ensures your metrics evolve to measure what actually matters.

## Complementary Metrics Strategy

Complementary metrics address blind spots by measuring different aspects of the same underlying quality. Instead of relying on a single faithfulness metric, you might combine claim-level verification with metrics measuring information completeness, detecting omitted critical facts, and checking for balanced presentation of positive and negative information.

No single metric captures faithfulness perfectly, but the combination provides broader coverage. When all complementary metrics agree, you have high confidence in the assessment. When they disagree, you investigate to understand which aspect of faithfulness is problematic. A system might score well on claim verification but poorly on information completeness, revealing selective omission issues.

The danger of optimizing a single metric while ignoring others is well-documented in ML more broadly but particularly acute for RAG systems. Suppose you optimize aggressively for faithfulness by making your system extremely conservative, only making claims that are nearly verbatim from source documents. Faithfulness scores soar to ninety-eight percent, but answer relevance drops because the system fails to synthesize information appropriately.

You achieved perfect scores on one metric at the cost of degrading another important quality dimension. Multi-objective optimization requires monitoring multiple metrics simultaneously and making tradeoffs explicit rather than inadvertently sacrificing unmeasured qualities for measured ones.

## Goodhart's Law and Metric Gaming

Goodhart's Law states "When a measure becomes a target, it ceases to be a good measure." This applies fully to RAG evaluation. Once you start optimizing for a specific metric, system behavior shifts to maximize that metric, including through means that do not actually improve user experience.

You might improve retrieval precision by retrieving fewer documents, which helps precision but harms recall and potentially answer completeness. You might improve faithfulness by shortening answers to include fewer claims, which helps faithfulness but harms answer usefulness. Being aware of this dynamic helps you resist over-optimization and maintain focus on actual user value rather than metric scores.

The phenomenon is particularly insidious because it happens gradually. You make a change that improves your target metric by two points. The change feels like progress, the metric validates it, and you ship it. Over many iterations, you have drifted toward optimizing the measurement rather than the underlying quality the measurement was meant to proxy.

## Building Comprehensive Coverage

Building evaluation coverage requires deliberate effort to identify and address blind spots. You cannot measure everything, so prioritize dimensions that matter most to your use case and have the largest gaps in current coverage.

A medical triage system must measure safety and appropriate escalation to human providers, even if these metrics are expensive to implement, because failures have serious consequences. A product recommendation system might prioritize measuring currentness and appropriateness for user context over absolute factual precision.

Your evaluation suite should reflect your system's priorities and risk profile. High-stakes domains require extensive evaluation coverage across many quality dimensions. Lower-stakes applications can focus on a smaller set of critical metrics. The key is making these prioritization decisions explicitly rather than implicitly through metric selection.

Regularly audit your evaluation metrics against production issues. Quarterly reviews should ask: what problems occurred in production over the last quarter, and would our evaluation metrics have detected these problems if they had occurred in test data.

If the answer is no, you need additional metrics. If production issues consistently fall outside evaluation coverage, your metrics are not measuring what matters. This continuous calibration between evaluation and reality keeps your measurement infrastructure relevant as your system and usage patterns evolve.

## Common Mistakes

The most common mistake teams make with evaluation coverage is assuming comprehensive metrics exist or will emerge from standard frameworks. You adopt RAGAS or DeepEval, run the provided metrics, and believe you have comprehensive evaluation. Standard frameworks provide valuable metrics but are intentionally general-purpose.

They cannot encode your domain-specific quality requirements, your user population's specific needs, or the particular failure modes your system exhibits. Comprehensive evaluation requires augmenting standard metrics with custom metrics targeting known blind spots and domain requirements.

The second common mistake is treating evaluation as static rather than evolving. You build an evaluation suite that covers known concerns at system launch, then run the same evaluations for the next year without reconsidering coverage.

As your system evolves, failure modes change. As usage patterns shift, user expectations change. As your document corpus grows, new quality challenges emerge. Evaluation coverage must evolve alongside these changes or it becomes less relevant over time, measuring historical concerns while missing current problems.

You build evaluation coverage by systematically inventorying quality dimensions, identifying which are measured by existing metrics and which are not. You analyze production issues to discover gaps between metrics and reality. You implement complementary metrics that measure different aspects of core quality dimensions.

You balance metric investment against user impact, prioritizing coverage of high-consequence failure modes. You audit regularly to ensure coverage remains aligned with system evolution and production experience. You resist the temptation to optimize individual metrics at the cost of unmeasured dimensions.

Your metrics guide your optimization. If they have blind spots, you are flying blind in those dimensions. Identify the gaps, measure what matters, and recognize that perfect coverage is impossible but thoughtful coverage is achievable. The goal is not to measure everything but to measure the things that have the largest impact on user outcomes in your specific domain. Build that coverage deliberately, maintain it continuously, and let it guide you toward real improvements rather than metric optimization.
