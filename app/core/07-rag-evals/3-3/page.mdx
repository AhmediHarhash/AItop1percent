# 3.3 — Vector Databases: Architecture, Selection, and Operations

In September 2024, a healthcare analytics startup built a RAG system on top of a vector database they chose because it had the most GitHub stars and the cleanest documentation. They embedded three million patient discharge summaries, loaded them into the database, and launched their clinical insights product. For the first two months, everything worked. In November, they needed to update a subset of embeddings because they had retrained their embedding model on domain-specific medical text. They discovered their vector database had no efficient update mechanism—you could insert new vectors or delete old ones, but updating an existing vector required deleting and reinserting, which invalidated the index and triggered a full rebuild. The rebuild took fourteen hours and brought their production system offline. They had chosen a database optimized for read-heavy workloads without understanding their write patterns. By December, they had migrated to a different database, re-ingested all three million vectors, and lost a month of engineering time.

By 2026, the vector database landscape is mature but crowded. You have managed services, self-hosted open-source systems, and database extensions that bolt vector search onto traditional databases. Each option has different performance characteristics, operational tradeoffs, and cost structures. Choosing the wrong vector database costs you money, latency, reliability, and engineering time. This chapter teaches you how vector databases work, how to evaluate and select one for your production workload, and how to operate it reliably over the long term.

## Vector Database Fundamentals: What They Do and Why

A vector database stores high-dimensional vectors and supports efficient nearest-neighbor search. Given a query vector, the database returns the k most similar vectors according to a distance metric like cosine similarity or Euclidean distance. This is the core operation that powers RAG retrieval: you embed a user query into a vector, search the database for the most similar document vectors, and return the corresponding documents.

The naive approach to vector search is to compute the distance between the query vector and every vector in the database, sort by distance, and return the top k. This brute-force approach is exact—it always returns the true nearest neighbors—but it scales linearly with database size. For a database with ten million vectors, you perform ten million distance calculations per query. At 768 dimensions, each distance calculation requires 768 multiplications and additions, plus a normalization step for cosine similarity. This is computationally expensive and far too slow for production queries.

Vector databases solve this problem with approximate nearest neighbor algorithms that trade exactness for speed. Instead of checking every vector, these algorithms use index structures—graphs, trees, inverted lists, or quantized representations—to prune the search space and check only a subset of vectors. The result is approximate: you might not get the true top k nearest neighbors, but you get a set of vectors that are very close, and you get them in milliseconds instead of seconds.

The quality of a vector database is measured by recall, latency, and memory usage. Recall is the fraction of true nearest neighbors that the approximate search returns. If the true top ten neighbors are A, B, C, D, E, F, G, H, I, J, and the database returns A, B, C, D, E, F, G, H, X, Y, the recall is 80 percent. Latency is how long the search takes. Memory usage is how much RAM the index consumes. The tradeoff is that higher recall requires checking more vectors, which increases latency and memory usage.

Vector databases also store metadata alongside vectors. Each vector corresponds to a document, chunk, or text snippet, and you need to store the original text, a document ID, timestamps, tags, or other fields. Retrieval systems often filter by metadata before or after the vector search—for example, "find the most similar vectors in documents published after 2023" or "retrieve from the legal corpus, not the marketing corpus." Vector databases support metadata filtering with varying efficiency.

## The 2026 Vector Database Landscape

By 2026, the vector database ecosystem has consolidated into several tiers. At the top, you have managed cloud services that handle infrastructure, scaling, backups, and monitoring for you. Pinecone is the most mature managed service, offering serverless vector search with automatic scaling, high availability, and usage-based pricing. Weaviate Cloud Services and Qdrant Cloud offer managed hosting for their respective open-source databases. These services cost more per query or per GB than self-hosting, but they save you operational overhead.

In the middle tier, you have self-hosted open-source vector databases. Weaviate, Qdrant, and Milvus are the most widely deployed in production. These systems offer feature parity with managed services—fast ANN search, metadata filtering, horizontal scaling—but you run them on your own infrastructure. You are responsible for deployment, monitoring, backups, upgrades, and incident response. The software is free, but the operational cost is real.

At the lower tier, you have vector search extensions for existing databases. PostgreSQL with the pgvector extension, Elasticsearch with vector search, Redis with vector similarity search, and MongoDB with vector indexes. These options are attractive if you already run the underlying database and want to add vector search without introducing a new system. They tend to have lower performance and fewer vector-specific features than purpose-built vector databases, but they reduce operational complexity by consolidating systems.

There is also a tier of embedded or in-memory vector databases like Chroma, LanceDB, and FAISS. These are libraries, not servers—you embed them directly into your application process. They are fast, simple, and appropriate for small-scale or single-machine workloads. They do not scale horizontally, do not support distributed queries, and lose all data if the process crashes unless you persist to disk. They are prototyping tools, not production infrastructure for large-scale RAG.

The selection question is which tier matches your scale, team capabilities, and budget. A startup with 100,000 vectors and two engineers should use a managed service. An enterprise with 100 million vectors and a platform engineering team should self-host. A team that already runs PostgreSQL and has ten million vectors might start with pgvector and migrate later if performance becomes a bottleneck.

## Pinecone: Managed Simplicity at a Price

Pinecone is the most mature managed vector database in 2026. It is a fully managed service—you create an index via API, insert vectors, and query them. Pinecone handles scaling, replication, failover, and backups. You pay based on the number of vectors stored and the number of queries per second. Pricing is usage-based, which means you can start small and scale without upfront infrastructure costs.

Pinecone's strengths are operational simplicity and reliability. You do not run servers, configure indexes, or tune parameters beyond basic settings like dimension count and distance metric. Pinecone automatically shards your index across pods, replicates for availability, and scales up or down based on traffic. For teams that want to ship a RAG product without building vector database expertise, Pinecone is the default choice.

The weaknesses are cost and vendor lock-in. Pinecone is expensive at scale. For a production workload with 50 million vectors and 10,000 queries per second, you might pay thousands of dollars per month. Self-hosting the same workload on AWS with Qdrant or Milvus might cost half as much or less. You also depend on Pinecone's pricing, uptime, and API stability. If Pinecone raises prices, you pay more or migrate. If Pinecone has an outage, your RAG system goes down.

Pinecone is the right choice when operational simplicity and time-to-market outweigh cost concerns. It is the wrong choice when you need to optimize spend or have complex requirements that the managed service does not support.

## Weaviate: Open-Source Versatility with Built-In ML

Weaviate is an open-source vector database that you can self-host or use via Weaviate Cloud Services. It supports dense and sparse vectors, hybrid search, metadata filtering, and built-in integrations with embedding models. Weaviate can embed text for you using modules that call OpenAI, Cohere, or Hugging Face APIs, or you can bring your own embeddings.

Weaviate's architecture is built around GraphQL for querying and a modular plugin system for extending functionality. It supports multi-tenancy, CRUD operations on objects with vectors, and vector search combined with BM25 keyword search. The hybrid search feature is particularly strong—you can query with both a vector and a keyword search, and Weaviate merges results automatically.

Weaviate's strengths are flexibility and feature richness. It supports a wide range of use cases, integrates easily with ML workflows, and has an active open-source community. The documentation is excellent, and the Cloud Services option gives you managed hosting without sacrificing control.

The weaknesses are complexity and performance at extreme scale. Weaviate is a feature-rich system, which means there are many knobs to tune and many ways to configure it incorrectly. Performance is good for most workloads, but at 100 million-plus vectors, purpose-built systems like Milvus or Qdrant might deliver better throughput and lower latency.

Weaviate is the right choice when you need hybrid search, rich metadata filtering, and tight integration with ML pipelines. It is a strong general-purpose option for teams that want open-source flexibility with an optional managed service.

## Qdrant: High-Performance Open Source with Production Focus

Qdrant is an open-source vector database written in Rust, optimized for performance, scalability, and operational simplicity. It is designed to be fast, memory-efficient, and easy to deploy. Qdrant supports HNSW and other ANN algorithms, metadata filtering, payload storage, and distributed deployment with horizontal scaling.

Qdrant's strengths are raw performance and resource efficiency. Benchmarks consistently show Qdrant delivering high query throughput with low latency and modest memory usage. The Rust implementation means low overhead, predictable performance, and good multi-core utilization. Qdrant also offers a clean REST and gRPC API, making it easy to integrate into existing systems.

Qdrant supports advanced filtering—you can combine vector search with complex metadata filters, and the database applies filters efficiently without scanning irrelevant vectors. This is critical for multi-tenant RAG systems where each user should only see their own documents, or for systems that partition by time, geography, or category.

Qdrant Cloud offers managed hosting, and the self-hosted version is straightforward to deploy with Docker or Kubernetes. The operational maturity is high—Qdrant supports snapshots, backups, rolling upgrades, and monitoring integrations.

The weaknesses are a smaller ecosystem compared to Pinecone or Weaviate and fewer built-in ML integrations. You are responsible for embedding your own data—Qdrant does not call OpenAI for you. This is not a real limitation if you already have an embedding pipeline, but it means more code to write.

Qdrant is the right choice when you need high performance, efficient resource usage, and production-grade reliability in an open-source package. It is popular with teams that self-host and optimize for cost and latency.

## Milvus: Massively Scalable Open Source for Enterprises

Milvus is an open-source vector database designed for massive scale. It is built for workloads with hundreds of millions or billions of vectors, distributed across clusters of machines. Milvus supports multiple ANN algorithms, GPU acceleration, and separation of storage and compute, allowing you to scale independently.

Milvus's architecture separates data nodes, query nodes, and index nodes, which allows horizontal scaling at each layer. You can add query nodes to handle more concurrent queries or add data nodes to store more vectors. Milvus integrates with object storage like S3 for vector persistence, which reduces costs compared to storing everything in memory or on local disks.

Milvus's strengths are scalability and flexibility. If you need to index a billion vectors and serve 100,000 queries per second, Milvus is built for that workload. It supports multiple index types, including HNSW, IVF, and DiskANN, and allows you to choose the right algorithm for your latency and memory constraints.

The weaknesses are operational complexity and resource requirements. Milvus is a distributed system with many moving parts. You need to run multiple services—etcd for coordination, MinIO or S3 for storage, Pulsar or Kafka for message queues—and configure them correctly. The deployment footprint is large, and the learning curve is steep. Milvus is not a system you spin up in five minutes; it is infrastructure you plan, deploy, and operate.

Milvus is the right choice when you operate at massive scale, have a platform engineering team, and need the flexibility to tune every layer of the stack. It is overkill for small-scale RAG and inappropriate for teams without Kubernetes and distributed systems expertise.

## pgvector: PostgreSQL Extension for Familiar Infrastructure

pgvector is a PostgreSQL extension that adds vector storage and search to PostgreSQL. If you already run PostgreSQL, pgvector allows you to store vectors in the same database as your relational data, query them with SQL, and use familiar PostgreSQL tooling for backups, replication, and monitoring.

pgvector supports exact and approximate nearest-neighbor search using IVFFlat or HNSW indexes. You can combine vector search with SQL filters, joins, and transactions. This makes pgvector attractive for applications where vector search is part of a larger data model—for example, a RAG system where you also store user accounts, documents, and access control rules in PostgreSQL.

pgvector's strengths are simplicity and integration. You add an extension to your existing database, create a vector column, build an index, and query with SQL. No new infrastructure, no new operational processes. If your team already understands PostgreSQL, pgvector has zero learning curve.

The weaknesses are performance and scalability. pgvector is not as fast as purpose-built vector databases. At ten million vectors, pgvector performance starts to degrade unless you tune indexes aggressively. At 100 million vectors, pgvector is not viable. The HNSW implementation in pgvector is less optimized than Qdrant or Milvus, and query latency suffers.

Another limitation is that pgvector runs on a single PostgreSQL instance unless you shard manually. Purpose-built vector databases handle sharding and distribution automatically. If you need horizontal scaling, pgvector requires you to build it yourself.

pgvector is the right choice when you have a small to medium-sized vector workload—under ten million vectors—and you already run PostgreSQL. It is the wrong choice when you need the absolute best performance or plan to scale to hundreds of millions of vectors.

## Chroma: Embedded Database for Prototyping

Chroma is an embedded vector database designed to be easy to use and fast to prototype with. It is a Python library that you import into your application, and it stores vectors in memory or on local disk. Chroma supports inserting, updating, deleting, and querying vectors with a simple API.

Chroma's strength is simplicity. You can install it with pip, write ten lines of code, and have a working vector search system. It integrates seamlessly with LangChain, LlamaIndex, and other LLM frameworks. For research, prototyping, or single-machine applications, Chroma is fast and convenient.

The weaknesses are scalability and durability. Chroma does not scale horizontally. It runs in a single process on a single machine. If your vector count exceeds what fits in RAM, performance collapses. If the process crashes and you did not persist to disk, you lose data. Chroma is not production infrastructure for large-scale RAG systems.

Chroma is the right choice for prototyping, local development, or applications that run on a single machine with a small vector count. It is the wrong choice for production systems with high availability or horizontal scaling requirements.

## Selection Criteria: Matching Database to Workload

When you select a vector database, evaluate it on these dimensions:

**Scale.** How many vectors do you need to store? How many queries per second do you need to serve? Databases designed for millions of vectors do not necessarily scale to billions.

**Latency requirements.** What is your acceptable query latency? Purpose-built databases deliver sub-50-millisecond latency for millions of vectors. Database extensions like pgvector might take 100 to 200 milliseconds.

**Write patterns.** Are you inserting vectors once and querying many times, or do you frequently update, delete, or reindex? Some databases optimize for read-heavy workloads and penalize writes.

**Metadata filtering.** Do you need to filter by tags, timestamps, user IDs, or other fields? How complex are the filters? Databases differ in how efficiently they combine vector search with metadata filtering.

**Operational maturity.** Does your team have the expertise to run distributed systems, Kubernetes, monitoring, and incident response? Managed services reduce operational burden. Self-hosted systems require expertise.

**Cost.** What is your budget for database hosting? Managed services cost more per query but save engineering time. Self-hosted systems cost less per query but require infrastructure and staffing.

**Integration with existing infrastructure.** Do you already run PostgreSQL, Elasticsearch, or Redis? Adding vector search to an existing database might be simpler than introducing a new system.

**Ecosystem and community.** How mature is the tooling, documentation, and community? Can you find answers to operational questions, or are you on your own?

No single database wins on all dimensions. The correct choice depends on your specific constraints and priorities.

## Operational Concerns: Backups, Upgrades, and Monitoring

Vector databases are infrastructure, and infrastructure requires operational discipline. You need backups, monitoring, alerting, and upgrade procedures. Many teams underestimate the operational cost of running a vector database and discover problems in production.

**Backups.** Vector indexes are large and expensive to rebuild. If you lose your vector database, you must re-embed your entire corpus and rebuild all indexes, which might take hours or days. You need regular snapshots or backups of your vector data. Managed services handle this automatically. Self-hosted systems require you to configure snapshot schedules, store backups in durable storage like S3, and test restore procedures.

**Upgrades.** Vector databases release new versions with performance improvements, bug fixes, and new features. Upgrading a distributed vector database without downtime requires rolling restarts, version compatibility checks, and index migration procedures. Some databases support zero-downtime upgrades. Others require taking the system offline. You need to test upgrades in staging before applying them to production.

**Monitoring.** You need to monitor query latency, query throughput, error rates, memory usage, disk usage, and index health. If query latency spikes, you need to know why—did traffic increase, did an index degrade, did a node fail? If memory usage grows unbounded, you need alerts before the system runs out of RAM and crashes. Managed services provide built-in monitoring dashboards. Self-hosted systems require you to integrate with Prometheus, Grafana, Datadog, or similar tools.

**Scaling.** As your corpus grows, you need to scale your vector database. Managed services scale automatically. Self-hosted systems require you to add nodes, reshard indexes, and rebalance data. Horizontal scaling is complex, and not all databases handle it gracefully.

**Index tuning.** ANN indexes have parameters that affect recall, latency, and memory usage. HNSW indexes have efConstruction and M parameters. IVF indexes have nlist and nprobe parameters. The optimal values depend on your data distribution, query patterns, and hardware. You need to benchmark and tune these parameters for production workloads.

The operational maturity required to run a vector database is similar to running a relational database or a search engine. If your team is comfortable operating PostgreSQL or Elasticsearch, you can operate a self-hosted vector database. If not, use a managed service.

## The Migration Problem: Changing Databases is Hard

Once you commit to a vector database, migrating to a different one is expensive. Your embeddings are stored in a specific format, your queries are written against a specific API, and your infrastructure is built around a specific deployment model. Changing databases requires re-ingesting all vectors, rewriting query code, and redeploying infrastructure.

The cost of migration creates lock-in. Even if a new database offers better performance or lower cost, the migration effort might not justify the benefit. This makes the initial selection decision high-stakes—you are likely to live with your choice for years.

The mitigation strategy is to abstract your vector database behind an interface layer. Write your application code against an abstraction that hides the database implementation, so you can swap databases by changing the implementation without rewriting application logic. This is good engineering discipline, but it does not eliminate the cost of re-ingesting vectors and tuning a new index.

The lesson is to choose carefully, evaluate thoroughly, and plan for the long term. Do not pick a database because it has the flashiest marketing or the most GitHub stars. Pick it because it matches your workload, fits your operational capabilities, and solves your actual problems.

## The Right Database for Your Stage

Early-stage startups with small corpora and limited engineering resources should use managed services like Pinecone or Weaviate Cloud. The cost is higher per vector, but the time-to-market is faster, and you avoid operational complexity.

Growth-stage companies with predictable traffic and infrastructure teams should consider self-hosting Qdrant, Weaviate, or Milvus. The cost per vector is lower, and you gain control over performance tuning, upgrades, and scaling.

Enterprises with massive scale and platform engineering teams should deploy Milvus or a heavily tuned self-hosted system. The operational complexity is justified by the cost savings and performance requirements.

Teams that already run PostgreSQL and have small to medium vector workloads should start with pgvector and migrate to a purpose-built database only if performance becomes a bottleneck.

The common pattern is to start with a managed service, scale until the cost becomes prohibitive, then migrate to self-hosted infrastructure. This is a rational strategy, but it requires planning for eventual migration from the start.
