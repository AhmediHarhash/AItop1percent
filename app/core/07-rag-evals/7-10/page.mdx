# 7.10 â€” Continuous Evaluation: Monitoring RAG Quality Over Time

Industry data consistently shows that the majority of production RAG systems experience quality degradation within six months, yet fewer than a third of organizations actively monitor answer quality over time. The degradation is not caused by code changes or deployments. It is caused by model drift, corpus staleness, and embedding space shifts that happen silently while your dashboards show green and your team assumes stability means quality.

Nothing in the code had changed. The degradation was not caused by a deployment. It was caused by model drift: the LLM provider updated their model without announcement, changing response patterns. It was caused by corpus staleness: new regulations were not being indexed, making answers based on old rules. It was caused by embedding drift: the semantic space shifted as the index grew, changing which documents were retrieved for common queries. The team discovered all this through an audit, not through proactive monitoring. By the time they caught it, users had been receiving degraded answers for months.

Continuous evaluation is the practice of measuring RAG quality constantly, not just at launch. It treats quality as a time-series metric that can drift, degrade, or improve based on factors outside your control. It samples production queries, evaluates them, tracks trends, and alerts when quality drops. It is the difference between "quality was good at launch" and "quality is good right now."

## Why RAG Quality Changes Without Code Changes

Traditional software quality is deterministic. If the code does not change, behavior does not change. RAG quality is non-deterministic and depends on external factors that change without warning.

The first factor is model drift. Your LLM provider updates the model behind your API endpoint. The new version might be better on average but worse on your specific use case. Response formatting might change subtly. Citation patterns might shift. Instruction-following might improve or degrade. You do not control model updates and often are not notified. Quality changes overnight.

The second factor is corpus evolution. Your document corpus grows as you add new content. New documents change the semantic space: queries that previously returned five relevant results now return three relevant and two irrelevant because the new content diluted relevance scores. Old documents become outdated but remain indexed, creating retrieval collisions where old and new information compete.

The third factor is query distribution shift. User queries change over time. New products launch, new terminology emerges, new regulations take effect. Queries that were common at launch become rare. Queries that did not exist at launch become common. Your system was optimized for the launch query distribution. It degrades on the new distribution.

The fourth factor is embedding drift. Some embedding models are continuously trained or fine-tuned by their providers. The embedding space shifts slightly with each update. Documents that were close to a query embedding become farther. Documents that were far become closer. Retrieval quality changes even though you did not change anything.

The fifth factor is infrastructure changes. Your vector database provider updates their ANN algorithm, changing recall characteristics. Your hosting provider changes network routing, affecting latency. Your LLM provider changes rate limits or batching behavior, affecting throughput. Each change impacts user experience.

The sixth factor is adversarial content. Users discover they can game your system by submitting documents designed to be retrieved for high-value queries. Competitors pollute your corpus with misleading content. SEO-style spam optimized for embeddings gets indexed. Quality degrades as adversarial content crowds out legitimate content.

These factors are not bugs. They are the operating environment of production RAG. Quality is not a constant. It is a variable that requires continuous measurement and correction.

## Continuous Eval Pipelines: Automated Quality Measurement

A continuous evaluation pipeline measures quality automatically at regular intervals. The pipeline samples production queries, evaluates answers, computes metrics, logs results, and alerts on degradation.

The simplest pipeline runs daily. Every day at midnight, you sample 100 random production queries from the past 24 hours. You re-run them through your current system. You evaluate the answers using automated metrics: citation rate, answer length, keyword overlap, toxicity scores. You log the results. You plot them alongside historical results. You check if today's metrics are significantly worse than the 30-day average.

More sophisticated pipelines evaluate in real-time. You sample 1 percent of production queries as they arrive. You evaluate them asynchronously. You aggregate metrics every hour. You detect degradation within hours, not days.

Automated evaluation uses metrics that do not require human judgment. Citation rate: what percentage of answers include citations? You can compute this by parsing answer text. Grounding rate: what percentage of claims in the answer appear in retrieved documents? You can approximate this with keyword overlap or semantic similarity. Factuality: does the answer contradict known facts? You can check against a fact database.

Automated metrics are imperfect proxies for quality. High citation rate does not guarantee correct citations. High keyword overlap does not guarantee coherent answers. But automated metrics are cheap and scalable. You run them on every query. They catch gross degradations even if they miss subtle ones.

You combine automated metrics with periodic human evaluation. Once a week, a human evaluator reviews 50 sampled queries and rates answer quality on a 1-to-5 scale. Human eval catches issues that automated metrics miss: hallucination, incoherence, inappropriate tone, citation inaccuracy. You track human eval scores over time and compare to automated metrics. If automated metrics are stable but human eval drops, your automated metrics are insufficient.

## Sampling Strategies for Production Queries

You cannot evaluate every production query. You sample. The sampling strategy determines what quality you measure.

Random sampling gives you an overall quality estimate. You sample queries uniformly at random. This measures average quality across all users and all query types. Random sampling is simple but might miss degradation in specific query categories.

Stratified sampling ensures you measure quality across important dimensions. You stratify by query type, user segment, or document category. You sample 20 queries from category A, 20 from category B, etc. This ensures you measure quality for each category even if some categories are rare.

Importance sampling weights queries by value. Some queries matter more than others. Queries from enterprise customers matter more than free-tier users. Queries about high-stakes topics matter more than casual questions. You sample in proportion to importance, measuring quality where it matters most.

Adversarial sampling targets likely failure modes. You sample queries that are long, ambiguous, or use rare terminology. You sample queries that retrieve few results or contradictory results. You measure quality on hard cases, where degradation is most likely.

Temporal sampling ensures you capture daily and weekly patterns. You sample queries from different times of day and different days of the week. If quality degrades during peak hours due to load, temporal sampling catches it. If quality degrades on weekends when your corpus update jobs run, temporal sampling catches it.

You combine strategies. You do 60 percent random sampling for overall quality, 20 percent stratified sampling for category coverage, 10 percent importance sampling for high-value queries, 10 percent adversarial sampling for edge cases. This gives you broad coverage with emphasis on what matters.

## Trend Detection and Alerting: Catching Quality Drift Early

Quality degrades gradually. A single day's drop might be noise. A week-long trend is signal. Trend detection distinguishes noise from drift.

The simplest trend detection is moving averages. You compute the 7-day moving average of your quality metric. If today's value is more than two standard deviations below the moving average, you alert. This catches sudden drops. It also catches slow degradation: as quality drops day by day, the moving average drops, but if the drop continues, eventually today's value falls below the moving average minus two sigma.

More sophisticated detection uses regression analysis. You fit a linear trend to the past 30 days of data. If the slope is significantly negative, quality is degrading. You alert. This catches gradual drift earlier than moving averages.

Change-point detection identifies the moment quality shifted. You use statistical tests to find points in time where the distribution of your metric changed. If quality was stable at 87 percent for weeks, then shifted to 81 percent and stayed there, change-point detection identifies the shift day. You investigate what changed that day: a deployment, a corpus update, a model version change.

Anomaly detection flags individual days that are outliers. If quality is normally 87 percent with 2 percent standard deviation, and one day it drops to 78 percent, that is an anomaly. You investigate even if the next day it recovers. Anomalies sometimes indicate transient issues, other times they are the first signal of sustained degradation.

You set alert thresholds based on impact. If quality drops below 80 percent, you page the on-call engineer immediately. If quality drops below 85 percent, you create a ticket for investigation. If the 7-day trend is negative, you discuss in the weekly team meeting. You tier alerts by severity and urgency.

You route alerts to the right people. Quality degradation is not always an engineering issue. If quality drops because the corpus is stale, you alert the content team. If quality drops because queries shifted to topics you do not cover, you alert the product team. If quality drops because the model changed, you alert the platform team.

## Weekly Quality Reports: Making Quality Visible to the Team

Quality metrics are only valuable if the team sees them and acts on them. Weekly quality reports make quality a first-class concern.

A quality report summarizes the past week's metrics. Overall quality: 86 percent, down from 87 percent last week. Breakdown by category: legal queries 91 percent, technical queries 84 percent, general queries 85 percent. Trend: quality has dropped 2 percentage points over the past month. Incidents: one day with 79 percent quality due to database outage, since resolved.

The report highlights changes. What improved? Retrieval latency dropped by 100 milliseconds due to index optimization. What degraded? Citation accuracy dropped 3 percentage points; cause unknown, under investigation. What stayed stable? User satisfaction scores remained at 4.2 out of 5.

The report includes user feedback. Sampled user comments from the past week. Common complaints: answers too generic, missing recent product information. Common praise: fast responses, good citations. Qualitative feedback complements quantitative metrics.

The report identifies action items. Quality degraded in legal queries; investigate whether recent regulatory changes are not indexed. Citation accuracy dropped; audit citation extraction logic for bugs. User complaints about generic answers; consider tuning prompt for more specific responses.

You review the report in a standing weekly meeting. The team discusses trends, investigates anomalies, and prioritizes fixes. Quality becomes a regular agenda item, not a crisis response. The team develops intuition for normal variation versus concerning trends.

You share the report with stakeholders. Product managers see which query categories perform well and which need improvement. Executives see quality trends and understand the system's health. Customer success teams see common user complaints and can set expectations.

## Longitudinal Analysis: Understanding Quality Evolution

Continuous evaluation generates time-series data. Longitudinal analysis uses that data to understand how quality evolved and why.

You plot quality over months or years. You annotate the plot with significant events: model updates, corpus changes, feature launches, infrastructure changes. You look for correlations. Did quality drop after a model update? Did it improve after you added reranking? Visual analysis reveals patterns.

You segment by user cohort. Do users who joined in month 1 have different quality experience than users who joined in month 6? If early users see degrading quality as the corpus grows, you might need better retrieval scaling. If new users see low quality, your onboarding might be poor.

You analyze seasonal patterns. Does quality drop during holidays when your content team is not updating documents? Does it improve after quarterly corpus refreshes? Seasonal analysis helps you plan: if quality always drops in December, you schedule a corpus update for November.

You compare to external events. If you are building financial RAG, does quality correlate with market volatility? If you are building legal RAG, does it correlate with regulatory changes? External events drive query distribution shifts. Understanding the correlation helps you anticipate and prepare.

You run counterfactual analysis. What would quality have been if you had not made change X? You identify a change you made, like switching embedding models. You look at quality before and after. You account for confounding factors like corpus growth. You estimate the causal impact of the change. This validates whether past changes actually helped.

You build predictive models. Can you predict next week's quality based on corpus size, query volume, and recent changes? Predictive models let you catch degradation before it happens. If the model predicts quality will drop next week, you investigate proactively.

## Dealing With Non-Determinism: Measuring Variance in Quality

RAG systems are non-deterministic. Running the same query twice might produce different answers. This makes quality measurement harder.

You measure variance by running each test query multiple times. You run query Q five times. You get five answers. You evaluate each. You compute mean quality and standard deviation. If mean is 8 out of 10 with standard deviation 0.5, the query has consistent quality. If mean is 7 with standard deviation 2, quality is unstable.

High variance is a quality issue. Users want consistent answers. If asking the same question twice gives wildly different answers, users lose trust. You track variance as a metric alongside mean quality. You investigate queries with high variance and determine if you can reduce it by lowering temperature, using constrained decoding, or providing more specific prompts.

You distinguish between acceptable and problematic variance. Phrasing variance is acceptable: two answers with identical facts but different wording are fine. Factual variance is problematic: one answer says 30 days, another says 14 days. You design evaluation to tolerate acceptable variance and flag problematic variance.

You use majority voting or ensembling to reduce variance in evaluation. You generate three answers for a test query. You evaluate all three. If two out of three meet quality criteria, you count the query as passing. This is more robust than single-sample evaluation.

You set temperature and sampling parameters to balance quality and variance. Lower temperature reduces variance but might reduce answer diversity. You test different settings and measure the trade-off. You choose settings that keep variance low while maintaining answer quality.

## Integrating Continuous Eval into Operations

Continuous evaluation is not a standalone project. It integrates into your operational workflows.

You add continuous eval metrics to your monitoring dashboard. Alongside latency, error rate, and throughput, you show answer quality, citation accuracy, and user satisfaction. Quality is a first-class operational metric.

You page on quality degradation like you page on downtime. If quality drops below threshold, the on-call engineer is paged. They investigate: is it a code bug, a model change, a corpus issue, an infrastructure failure? They mitigate: rollback a deployment, refresh the corpus, switch embedding models, scale resources. Quality incidents are real incidents.

You include quality metrics in post-mortems. When quality degrades, you write a post-mortem. What happened? Why did monitoring not catch it sooner? How do we prevent recurrence? You treat quality incidents with the same rigor as availability incidents.

You tie continuous eval to deployment gates. Before deploying to production, you run the golden dataset through staging. If staging quality is significantly worse than production, the deployment is blocked. Continuous eval in staging catches regressions before they reach users.

You use continuous eval to measure the impact of infrastructure changes. You are migrating to a new vector database. You run continuous eval before and after migration. If quality stays stable, migration succeeded. If quality degrades, you rollback and debug.

The financial services company rebuilt their system with continuous evaluation. They implemented a daily eval pipeline sampling 200 production queries. They tracked answer accuracy, citation accuracy, grounding rate, and user satisfaction. They plotted trends and set alerts.

Two months later, daily quality dropped from 87 to 83 percent over five days. The alert fired. The team investigated. They discovered their LLM provider had updated the model. The new model formatted citations differently, breaking their citation extraction logic. They updated the extraction logic and quality recovered to 86 percent.

Four months later, grounding rate trended downward from 92 to 85 percent over three weeks. No single day was an outlier, but the trend was clear. They investigated. They found the corpus had grown by 40 percent, diluting retrieval scores. They re-tuned retrieval thresholds and added a reranker. Grounding rate recovered to 91 percent.

Six months later, user satisfaction dropped from 4.3 to 4.0 stars over a month. Automated metrics looked fine. They sampled user feedback. Users complained answers were "outdated." They investigated and found their corpus refresh job had been failing silently for six weeks. They fixed the job, refreshed the corpus, and satisfaction recovered to 4.2 stars.

Each incident was caught by continuous evaluation, not by user complaints. The team fixed issues within days, not months. Quality became a managed metric, not a hope. The system that launched at 87 percent quality was still at 86 percent quality a year later, despite model changes, corpus growth, and query distribution shifts. Continuous evaluation made quality sustainable.

Continuous evaluation is the discipline of treating quality as a dynamic metric that requires constant measurement and management. It acknowledges that RAG systems operate in a changing environment where models drift, corpora evolve, and user needs shift. Teams that implement continuous eval catch degradations early and fix them before users churn. Teams that skip it discover quality issues through audits, user complaints, or lost revenue. The difference is proactive measurement versus reactive firefighting.
