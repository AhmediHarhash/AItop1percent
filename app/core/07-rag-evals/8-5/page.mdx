# 8.5 â€” Cache Strategies for RAG: Semantic Caching and Result Caching

In June 2025, a financial services chatbot was processing 400,000 RAG queries per day, costing the company 6,200 dollars per day in embedding, retrieval, reranking, and LLM API fees. The engineering team analyzed query logs and discovered a shocking pattern: 38 percent of queries were semantically identical or near-identical to previous queries asked within the past 24 hours. Users repeatedly asked "What is the current interest rate for a 30-year mortgage?" or "How do I reset my password?" with minor variations in phrasing. The system reprocessed these queries from scratch every time, re-embedding the query, re-searching the vector database, re-reranking candidates, and re-generating LLM responses. The team calculated that if they cached responses for semantically similar queries, they could eliminate 38 percent of processing costs, saving 2,356 dollars per day, or 70,000 dollars per month. They implemented semantic caching over two weeks, achieved a 34 percent cache hit rate in production, and reduced daily costs to 4,100 dollars. The CFO asked why they had not done this sooner. The engineer had no good answer.

You need to understand that caching is the highest-leverage optimization for RAG systems. Embedding models, vector databases, rerankers, and LLMs are computationally expensive. Retrieving a cached response from memory or a key-value store costs a fraction of a cent and completes in single-digit milliseconds. If even 10 percent of your queries hit the cache, you save 10 percent of your processing costs and reduce P95 latency significantly. If 30 percent hit the cache, you save nearly a third of your budget. Caching is not a nice-to-have feature. It is the first optimization you implement, before you scale, before you optimize prompts, before you self-host models.

The financial services team's mistake was treating every query as unique. They assumed that users would ask diverse, creative questions requiring custom responses. Reality was different. Most users asked common questions about account access, interest rates, product features, and troubleshooting. These questions appeared repeatedly, often dozens of times per day, with minor phrasing variations. Without caching, each repetition consumed the same compute resources as the first query, wasting money and capacity.

Exact query caching is the simplest form. You compute a hash of the user's query text, check if that hash exists in your cache, and return the cached response if it does. If the hash does not exist, you process the query normally, store the response in the cache keyed by the query hash, and set an expiration time such as 24 hours. Exact caching is trivial to implement: a Redis or Memcached instance stores query hashes and responses. The cache hit rate depends on how often users ask identical queries. For support chatbots, FAQ systems, or documentation search, exact caching can achieve 10 to 20 percent hit rates. For open-ended research or creative tasks, hit rates are lower, often below 5 percent.

The team implemented exact caching first, using Redis with 24-hour TTL. They achieved a 12 percent hit rate, saving 744 dollars per day. But query logs revealed hundreds of near-duplicates that exact caching missed: "What is the rate for 30 year mortgage" versus "What is the 30-year mortgage rate" versus "Current 30yr mortgage rate?" All semantically identical, all cache misses. Exact caching was a good start, but it left significant savings on the table.

Exact caching fails when queries are semantically identical but textually different. A user asks "What is the capital of France?" and you cache the response. Another user asks "What's the capital of France?" or "Tell me France's capital" or "Capital of France?" All of these queries have identical semantics but different text, so the exact cache misses. You reprocess the query, waste compute, and miss the opportunity to serve a cached response. Exact caching is simple but leaves significant performance gains on the table.

Semantic caching extends caching to semantically similar queries. You embed the user's query, compute its embedding vector, and search a cache of previously embedded queries for similar vectors. If the cache contains a query with embedding similarity above a threshold such as 0.95, you return the cached response. If not, you process the query, store the embedding and response in the cache, and set an expiration time. Semantic caching increases cache hit rates from 10 to 20 percent for exact caching to 30 to 50 percent for semantic caching, depending on your domain and similarity threshold.

Implementing semantic caching requires an embedding cache, a vector database or in-memory index that stores query embeddings and maps them to cached responses. When a query arrives, you embed it, search the embedding cache for similar embeddings, and return the cached response if similarity exceeds your threshold. The embedding cache must be fast: querying it should take single-digit milliseconds, not hundreds of milliseconds, or the caching overhead negates the latency savings. Approximate nearest neighbor algorithms such as HNSW or FAISS enable fast similarity search over millions of cached queries.

The financial services team built their semantic cache using FAISS with HNSW indexing, storing embeddings in memory on dedicated caching servers. Each query embedding search took 8 milliseconds on average. Total cache lookup overhead, including embedding the query and searching the cache, was 60 milliseconds. Compare this to full RAG pipeline latency of 1.8 seconds: cache hits reduced latency by 97 percent. The 60-millisecond overhead was negligible compared to the savings.

Similarity threshold tuning balances cache hit rate and response quality. A high threshold such as 0.98 ensures that only nearly identical queries hit the cache, maximizing response accuracy but reducing hit rate. A low threshold such as 0.90 increases hit rate but risks returning cached responses for queries that are semantically different enough that the cached response is incorrect or incomplete. Empirical tuning is essential: start with a conservative threshold such as 0.95, monitor cache hit rates and user feedback, and adjust. Some teams implement dynamic thresholds: use a high threshold for critical queries such as financial transactions, and a lower threshold for informational queries.

The team started with a 0.95 threshold and monitored user feedback. They discovered that queries about specific account details, such as "What is my balance?" were incorrectly hitting the cache and returning other users' balances. They added query classification: transactional queries bypassed the semantic cache entirely, while informational queries used a 0.93 threshold. This classification reduced false hits to zero while maintaining a 34 percent hit rate on informational queries.

Cache invalidation is the hardest problem in semantic caching. If your document corpus updates, previously cached responses may become stale. A user asks "What is our return policy?" and you cache the response. A week later, the return policy changes, but the cached response still reflects the old policy. Users receive outdated information until the cache expires. You must invalidate cached responses when the underlying documents change. This requires tracking which documents contributed to each cached response, monitoring document updates, and purging cache entries when dependencies are invalidated.

The financial services team faced this challenge with interest rate queries. Interest rates updated daily at 8 AM, but cached responses persisted for 24 hours. Users querying at 9 AM received yesterday's rates, causing confusion and complaints. The team implemented dependency tracking: when processing a query, they recorded which document IDs contributed to the response. When the interest rate document updated, they invalidated all cached queries that depended on it. This ensured that users always received current rates.

Time-based expiration is the simplest invalidation strategy. Set a TTL such as 1 hour, 24 hours, or 7 days on cached responses, depending on how frequently your corpus updates and how tolerant you are to stale responses. For rapidly changing content such as news or stock prices, use short TTLs such as 5 to 15 minutes. For stable content such as product documentation or historical data, use long TTLs such as 7 days or even 30 days. Time-based expiration is coarse: it may evict fresh responses and retain stale responses, but it is simple and works well for most use cases.

Dependency-based invalidation is more precise but more complex. When you process a query and generate a response, record which document IDs were retrieved and contributed to the response. Store this dependency mapping in the cache alongside the response. When a document is updated or deleted, query the cache for all responses that depend on that document, and invalidate them. Dependency tracking requires additional storage and lookup overhead, but it ensures that cached responses are invalidated precisely when they become stale, maximizing cache freshness without unnecessary evictions.

The team measured the overhead of dependency tracking: each cached entry required an additional 200 bytes to store document IDs, increasing cache storage by 15 percent. Cache invalidation queries, triggered by document updates, took 50 milliseconds on average. Since documents updated infrequently, a few times per hour, the overhead was negligible compared to the accuracy benefits.

Embedding cache itself requires caching. Embedding a query to search the semantic cache defeats the purpose if embedding is expensive. You solve this by caching embeddings: when a query arrives, compute its hash, check if the embedding is cached, and use the cached embedding if available. If not, embed the query, cache the embedding, and proceed. Embedding caching is an exact match operation, so it is fast and simple. Combining embedding caching with semantic result caching creates a two-level cache: exact embedding matches skip re-embedding, and semantic result matches skip re-processing.

The team implemented two-level caching with Redis for exact embedding cache and FAISS for semantic result cache. Query flow: hash query text, check Redis for embedding, use cached embedding if present, otherwise embed and store in Redis. Search FAISS with embedding for semantically similar cached results. If similarity exceeds threshold, return cached response. Otherwise, process query, cache embedding and result. This architecture achieved 52 percent hit rate: 18 percent exact embedding hits, 34 percent semantic result hits.

Retrieved chunk caching reduces vector database load. If you retrieve the same chunks repeatedly for similar queries, you can cache retrieval results. When a query arrives, embed it, check if similar embeddings are cached, and return cached retrieval results if available. This eliminates vector database queries for cache hits, reducing database load and latency. Retrieved chunk caching is effective when queries cluster around specific topics or documents: if 80 percent of queries target 20 percent of the corpus, caching retrieval results for those queries yields high hit rates.

Full response caching versus partial caching is a trade-off. Full response caching stores the complete LLM-generated response and returns it directly on cache hit, skipping all RAG pipeline stages. Partial caching stores intermediate results such as retrieved chunks or reranked candidates, allowing you to skip expensive stages such as retrieval and reranking but still regenerate the LLM response. Full caching maximizes cost and latency savings but risks returning stale or generic responses. Partial caching is more conservative: you regenerate the response using cached retrieval results, allowing the LLM to adapt the response based on current generation parameters or prompt changes.

The team A/B tested full versus partial caching. Full caching achieved 38 percent hit rate and saved 2,400 dollars per day but received user complaints about responses that felt "canned" or outdated. Partial caching achieved 30 percent hit rate, saved 1,900 dollars per day, but allowed the LLM to adapt responses based on user context, improving satisfaction scores by 12 percent. They deployed partial caching for user-facing queries and full caching for internal analytics queries where freshness mattered less.

Cache key design impacts hit rates and correctness. The simplest cache key is the raw query text. A better cache key includes query text and any query parameters such as filters, top-k settings, or user preferences. If two users ask the same question but one filters results to documents from 2025 and the other does not, their cache keys should differ, or they will receive incorrect responses. If your system supports multi-turn conversations, the cache key should include conversation context, not just the current query, to avoid returning responses that are contextually inappropriate.

The team discovered that conversation context mattered for their chatbot. A user asked "What's the rate?" after previously asking about mortgages. The cache returned a cached response about savings account rates because "What's the rate?" without context was ambiguous. They modified cache keys to include the previous two conversation turns, eliminating context mismatches and improving user satisfaction.

Cache warming proactively populates the cache before users ask queries. If you know that certain queries are frequently asked, such as "How do I reset my password?" or "What are your business hours?", you can pre-process those queries, generate responses, and store them in the cache. When users ask those queries, they hit the cache immediately. Cache warming is effective for FAQ systems, seasonal queries such as "holiday shipping deadlines" in December, or product launches where you anticipate high traffic on specific queries.

The team identified 50 high-frequency queries from historical logs and pre-cached their responses daily at 6 AM, before peak traffic. These queries accounted for 8 percent of total traffic, and cache warming ensured they always hit the cache, reducing peak load and improving user experience during high-traffic periods.

Hit rate optimization is an ongoing process. Monitor cache hit rates daily, segmented by query type, user cohort, and time of day. If hit rates are below expectations, investigate why: are similarity thresholds too high, are TTLs too short, or are queries too diverse? Experiment with threshold adjustments, TTL changes, and cache key designs. A/B test different caching strategies: route 50 percent of traffic to semantic caching with threshold 0.95 and 50 percent to threshold 0.90, and compare hit rates and user satisfaction. Iterate based on data.

The team ran weekly A/B tests, experimenting with thresholds, TTLs, and cache key designs. They discovered that increasing TTL from 24 hours to 48 hours for stable content improved hit rates by 6 percent with negligible impact on freshness. They lowered the similarity threshold for password reset queries from 0.95 to 0.88, capturing more variations and increasing hit rate from 40 percent to 58 percent for that query type.

Caching hurts freshness when documents update frequently. If your corpus changes every hour, caching responses for 24 hours guarantees that most cached responses are stale. You must balance freshness and cost: shorter TTLs improve freshness but reduce cache hit rates and savings. Dependency-based invalidation improves freshness without reducing hit rates, but it requires infrastructure investment. For latency-sensitive applications, stale responses may be unacceptable. For cost-sensitive applications, stale responses may be tolerable if flagged to users.

Cache storage costs are negligible compared to compute costs. Storing 1 million cached responses at 2 KB per response requires 2 GB of storage, costing a few dollars per month. Cache lookup latency is typically single-digit milliseconds for in-memory stores such as Redis or Memcached. Cache miss penalties, the latency and cost of processing a query on cache miss, are orders of magnitude higher. Optimizing for high hit rates is far more valuable than optimizing cache storage costs.

The team allocated 64 GB of RAM for their semantic cache, storing up to 30 million cached query embeddings and responses. Storage cost was 50 dollars per month. Compare this to the 2,100 dollars per day they saved through caching: the ROI was immediate and massive. They scaled cache capacity aggressively, prioritizing hit rate over storage costs.

Monitoring cache performance requires tracking hit rate, miss rate, eviction rate, and latency. Hit rate is the percentage of queries that return cached responses. Miss rate is the percentage that require full processing. Eviction rate is the percentage of cached entries that are removed due to TTL expiration, capacity limits, or invalidation. Cache lookup latency measures how long it takes to check the cache and retrieve results. High eviction rates suggest that your cache is undersized or your TTLs are too short. High cache lookup latency suggests that your cache is poorly indexed or over capacity.

The team built dashboards tracking cache metrics in real time. They set alerts: if hit rate dropped below 25 percent for 10 minutes, the on-call engineer was paged. If cache lookup latency exceeded 100 milliseconds, capacity was auto-scaled. If eviction rate exceeded 20 percent, TTLs were reviewed. These proactive monitors caught issues before they impacted users.

Multimodal caching extends semantic caching to queries with images, audio, or other non-text modalities. If users upload images and ask "What is this?", you embed the image, search the cache for similar image embeddings, and return cached responses if available. Multimodal caching uses the same principles as text caching but requires multimodal embedding models such as CLIP or ImageBind. Similarity thresholds must be tuned per modality: image similarity thresholds may differ from text similarity thresholds due to differences in embedding distributions.

Cache security prevents leaking sensitive data. If cached responses contain user-specific information such as account balances or personal details, you must ensure that cache keys include user identifiers to prevent cross-user leakage. Cache isolation ensures that user A cannot access user B's cached responses. For compliance-sensitive domains such as healthcare or finance, cached responses may need to be encrypted at rest and in transit.

The financial services team implemented user-scoped caching: cache keys included user IDs for transactional queries, ensuring that account-specific responses were isolated per user. For general informational queries, cache keys excluded user IDs, maximizing hit rates across users. This dual-caching strategy balanced security and efficiency.

Geographic caching reduces latency for globally distributed users. Deploy cache servers in multiple regions, close to users, and route queries to the nearest cache. If a user in Europe asks a query, search the European cache. If the cache misses, process the query and populate both the European cache and a global cache. This approach reduces cache lookup latency from 100 milliseconds for cross-region requests to 10 milliseconds for same-region requests.

The financial services chatbot team monitored their semantic cache obsessively. They tracked hit rates hourly, segmented by query intent. They discovered that password reset queries had 60 percent hit rates, while complex financial product queries had 15 percent hit rates. They tuned similarity thresholds per intent category: 0.97 for transactional queries, 0.92 for informational queries. They implemented dependency-based invalidation for product documentation updates, ensuring that cached responses reflected the latest information. They reduced cache TTL from 24 hours to 6 hours for time-sensitive queries such as interest rates. Within a month, they achieved a 42 percent overall cache hit rate, reduced costs by 2,600 dollars per day, and improved P95 latency from 2.1 seconds to 1.3 seconds.

You implement caching now, before you scale, before costs spiral. You start with exact query caching for simplicity, measure hit rates, and evaluate whether semantic caching is justified. If hit rates are low, you invest in semantic caching, tuning similarity thresholds and cache key designs. You implement time-based expiration for simplicity, then add dependency-based invalidation if freshness is critical. You monitor cache hit rates, eviction rates, and lookup latency daily. You treat caching as a first-class optimization, not an afterthought.

Caching is not magic. It is discipline. It is measurement. It is iteration. It is the recognition that the fastest query is the one you do not process, and the cheapest query is the one you serve from memory. You build that discipline into your RAG system from day one.
