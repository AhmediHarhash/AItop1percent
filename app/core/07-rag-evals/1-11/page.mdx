# 1.11 — RAG Architecture Patterns by Use Case

Ninety-three percent of startups building horizontal RAG platforms fail within two years. They design one elegant architecture—512-token chunks, top-10 retrieval, generic generation—and market it to every vertical. Law firms need exact citations and jurisdiction filtering. E-commerce needs real-time inventory. Developer tools need code-aware chunking. The generic architecture satisfies nobody, and all three early customers churn within 90 days. The companies that survive pivot to vertical-specific architectures, rebuild everything three times, and learn that domain requirements are not cosmetic variations—they are fundamental architectural constraints.

You inherit a critical reality when building RAG systems: there is no universal architecture. What works for customer support breaks for legal research. What works for e-commerce product search fails for enterprise knowledge bases. Each domain has different requirements for chunking strategy, retrieval ranking, citation formats, and freshness guarantees. The patterns aren't arbitrary variations—they're domain-specific optimizations that directly impact user value. Building RAG that serves your actual use case requires understanding these patterns and making deliberate architectural choices aligned with domain requirements.

## Customer Support RAG: Fast Answers with Conversation Context

Customer support RAG prioritizes speed and conversation continuity. Users have problems they want solved immediately. They don't care about comprehensive answers—they care about the fastest path to resolution. This shapes every architectural decision.

Chunking for support focuses on solution-sized passages. Support articles typically have a problem statement followed by step-by-step solutions. You chunk at the solution level, not paragraph level. A single chunk might be "Password reset solution: 1. Click forgot password 2. Enter email 3. Check spam folder 4. Contact support if no email received." This chunk is self-contained and actionable.

Retrieval ranking prioritizes recency and success rates. Newer articles often supersede older ones as products evolve. Articles that historically resolved similar issues rank higher. Some teams track which articles led to case closures and boost those in retrieval. This requires integrating RAG with your ticketing system to gather feedback signals.

Multi-turn conversation is essential. Support interactions are inherently dialogues. "I can't log in" followed by "I tried resetting my password" followed by "The email never arrived." Your RAG must maintain conversation context, reformulate queries, and progressively narrow toward resolution. Single-turn support RAG frustrates users by forgetting context.

Citation requirements are minimal. Users don't need to know which knowledge base article you retrieved. They need solutions. Some systems don't show citations at all, just generated answers. Others include a "Read more" link to the source article for users who want additional context.

Freshness demands are high. When a product changes, support articles update, and RAG must reflect updates within minutes. Batch nightly re-indexing is insufficient. You need near-real-time ingestion from your knowledge base, with incremental index updates rather than full rebuilds.

The evaluation metric is resolution rate, not answer quality. Did the RAG-assisted conversation resolve the user's issue without escalation to human agents? This requires tracking outcomes, not just measuring retrieval precision. A perfectly relevant article that doesn't solve the problem is a failure.

## Legal Document RAG: Citation Accuracy and Jurisdiction Filtering

Legal RAG has opposite priorities from customer support. Speed matters less than precision. Comprehensiveness matters more than brevity. Citations are mandatory—lawyers need to verify every claim against source documents. Jurisdiction and date ranges are critical filters.

Chunking preserves legal structure. Court opinions have sections: facts, procedural history, analysis, holding, dissent. You chunk by section, preserving structure that indicates how to use each part. A chunk from the holding section is more authoritative than a chunk from a dissent. Metadata tags section type, letting retrieval and generation handle them differently.

Metadata filtering is mandatory. Lawyers searching for precedents need jurisdiction filtering: federal versus state, circuit, district. They need date filtering: cases decided after a specific statute passed, before a ruling was overturned. They need court level filtering: Supreme Court precedents have different weight than district court opinions.

Retrieval must handle legal citation formats. Lawyers search by case name, docket number, or citation format like "550 US 544." Your retrieval must recognize these as exact-match queries, not semantic queries. Hybrid retrieval with keyword matching is essential. Pure semantic search will miss exact citation queries.

Citation generation is the most critical component. Generated answers must cite specific cases, statutes, or regulations with correct legal citation formats. "As established in Chevron USA Inc v Natural Resources Defense Council, Inc, 467 US 837, 1984..." The LLM must extract citations from retrieved documents and format them correctly.

Hallucination is catastrophic. Inventing a case citation or misrepresenting a holding is professional malpractice. Legal RAG needs verification layers. Some systems use a second LLM to verify that generated citations match retrieved documents. Others require lawyers to review citations before using them. Pure trust in LLM output is unacceptable.

Freshness is complex. New decisions supersede old ones. A case might be overturned or distinguished. Your index needs to mark cases as good law versus overturned. Legal citation databases like Shepard's or KeyCite track this. Integrating with these services ensures you're not citing invalid precedent.

The evaluation metric is citation precision. Does every cited case actually exist? Does the cited holding match the actual holding? Does the retrieved case law apply to the jurisdiction and time period relevant to the query? Building ground truth for this requires legal expertise, making eval datasets expensive to create.

## Code Search RAG: Syntax-Aware Chunking and Repository Context

Code search RAG helps developers find relevant code, understand unfamiliar codebases, and generate implementation examples. The architectural requirements differ fundamentally from document search because code is structured text with syntax, semantics, and dependencies.

Chunking respects code structure. You don't chunk by character count or paragraph. You chunk by syntactic units: functions, classes, modules. A single chunk is a complete function with its docstring and signature. This preserves context needed to understand what the code does. Chunking mid-function destroys semantic meaning.

Language-aware parsing is necessary. You use Abstract Syntax Trees to identify function boundaries, class definitions, and import statements. Generic text chunking breaks code into nonsensical fragments. Tree-sitter and similar parsers handle multiple languages and provide structured chunking.

Retrieval combines semantic and lexical strategies. Developers search for concepts: "authentication middleware." They also search for exact identifiers: "UserAuthManager class." Dense embeddings handle semantic search. Keyword search handles exact identifier matching. Hybrid retrieval is essential.

Repository context matters. Retrieving a function without its dependencies is often useless. If a function calls three internal utilities, the answer needs those utilities too. Some code RAG systems retrieve not just the matched function but also its imports and called functions, building a context graph.

Metadata includes file path, language, commit date, and author. Developers often filter by module or directory: "authentication code in the API layer." They might prioritize recent code over legacy code. File path becomes a critical filter dimension.

Documentation integration is valuable. Code search benefits from retrieving both code and documentation together. If a user asks "How do I use the caching API?" retrieve both the cache implementation and its docstrings or README. Hybrid code-plus-docs retrieval provides better answers than code alone.

The evaluation metric is task success. Did the developer find the code they needed? Did the generated example compile and run? Did it solve their problem? This is hard to measure without user studies, so many teams use proxy metrics like retrieval precision on labeled query-code pairs.

## Enterprise Knowledge RAG: Siloed Data and Permission-Aware Retrieval

Enterprise knowledge RAG aggregates information across internal systems: wikis, Slack, Google Docs, Jira, Confluence, email. The architectural challenge is dealing with siloed sources, inconsistent formats, and complex permissions.

Multi-source ingestion is mandatory. You can't just embed one data source. You need connectors for every system your company uses. Each connector handles different authentication, API rate limits, and data formats. Building and maintaining these connectors is significant engineering overhead.

Permission-aware retrieval is critical. Not every employee can see every document. Your RAG must respect source system permissions. If a user can't access a document in Confluence, they shouldn't see it in RAG results. This requires storing permissions metadata and filtering retrieval by user identity.

Implementing permission filtering is complex. You either filter at retrieval time or filter after retrieval. Retrieval-time filtering is efficient but requires encoding permissions into your vector database schema. Post-retrieval filtering is simpler but wasteful—you retrieve 10 documents and then filter out 7 due to permissions, leaving only 3 results.

Schema heterogeneity requires normalization. Slack messages have timestamps, channels, reactions. Jira tickets have statuses, assignees, priorities. Google Docs have editors, comment threads, version history. You need a unified schema that captures relevant metadata from each source while allowing source-specific filtering.

Source attribution is essential. Users need to know whether information came from Slack, email, or official documentation. Official policy documents have different authority than casual Slack discussions. Your citations must indicate source type and link back to the original.

Staleness is a constant problem. Employees update documents, create new Slack channels, close Jira tickets. Your index must stay synchronized with source systems. Real-time synchronization is expensive. Most systems poll periodically, accepting some staleness. The polling frequency becomes a cost-quality trade-off.

The evaluation challenge is lack of ground truth. Unlike public benchmarks, enterprise knowledge is proprietary. You can't use pre-built eval datasets. You must create internal datasets by sampling employee queries, manually labeling correct answers, and continuously measuring quality on real usage.

## E-Commerce Product RAG: Structured Attributes and Real-Time Inventory

E-commerce product search combines RAG with structured filtering. Users search semantically: "red winter jacket under 200 dollars." But they also filter by attributes: size, color, price range, brand. The architecture must handle both.

Product data is semi-structured. Each product has a title, description, specifications, reviews, images. Specifications are structured: dimensions, materials, compatibility. Reviews are unstructured text. You need hybrid retrieval that searches both.

Attribute filtering is mandatory. Users expect to filter by price, rating, availability. This requires maintaining structured metadata alongside embeddings. Some teams store products in a traditional database and use vector search only for semantic matching, then join results.

Real-time inventory affects retrieval. Out-of-stock products should rank lower or be excluded. Inventory changes constantly. Your retrieval must incorporate near-real-time inventory status, either by filtering results or adjusting scores.

Personalization improves relevance. Past purchase history, browsing behavior, and user preferences should influence retrieval. A user who previously bought running shoes sees running-related products ranked higher for ambiguous queries like "athletic gear."

Reviews influence ranking. Products with many positive reviews rank higher than products with few or negative reviews. Some systems embed reviews separately and use review sentiment as a ranking signal.

Image search is increasingly common. Users upload a photo of a product and ask "find similar products." This requires multimodal embeddings that encode both text and images. Retrieval matches image embeddings to find visually similar products.

The evaluation metric is conversion rate. Did users find products they purchased? Click-through rate and add-to-cart rate are proxy metrics. A-B testing different retrieval strategies on real traffic is the gold standard for measuring impact on business outcomes.

## Medical and Scientific Literature RAG: Evidence Quality and Citation Standards

Medical RAG serves researchers, clinicians, and patients. Retrieval must distinguish between peer-reviewed research, clinical guidelines, case reports, and general health information. Evidence quality determines how much weight to give each source.

Source quality metadata is critical. Peer-reviewed journal articles rank higher than blog posts. Systematic reviews rank higher than single case studies. Your chunking and retrieval must preserve publication type, journal impact factor, and study methodology.

Citation linking enriches context. Scientific papers cite other papers. When you retrieve a paper, also retrieving highly-cited papers it references provides richer context. Some systems build citation graphs and use graph-based retrieval to find related work.

Date filtering handles evolving science. Medical knowledge changes. Guidelines from 2015 may be superseded by 2024 research. Users need to filter by publication date and prioritize recent evidence while still accessing historical context.

Acronym and terminology handling is essential. Medical text is dense with abbreviations. "MI" could mean myocardial infarction or mitral insufficiency. Disambiguation requires domain-specific entity resolution. Some teams use medical ontologies like UMLS to normalize terminology.

Patient safety requires confidence thresholds. Medical RAG for patients should refuse to answer when evidence is insufficient or contradictory. Overconfident wrong answers about medical treatment are dangerous. Some systems include disclaimers or require human verification before showing medical advice.

Clinical guideline RAG for doctors needs fast access to treatment protocols. When a doctor asks "What's the recommended antibiotic for community-acquired pneumonia?" they need evidence-based guidelines from sources like UpToDate or specialty societies. Retrieval must prioritize authoritative clinical guidelines over general research.

The evaluation metric is answer accuracy against expert judgment. You need medical professionals to review generated answers and assess correctness and completeness. Automated metrics are insufficient—hallucinated medical information is too dangerous to detect with just BLEU scores or retrieval precision.

## Building Domain-Specific RAG: The Pattern Selection Process

Choosing the right pattern starts with understanding your domain's unique requirements. Interview users, analyze query patterns, identify what makes your domain different from generic document search.

Map requirements to architectural decisions. If users need exact citations, your chunking must preserve citation-worthy units. If freshness matters, you need real-time ingestion. If permissions matter, you need permission-aware retrieval. Each requirement constraints architecture.

Start with the simplest architecture that could work. Don't build multi-source permission-aware real-time RAG on day one if you only have one data source and no permission requirements. Add complexity only when requirements demand it.

Measure domain-specific metrics. Customer support needs resolution rate. E-commerce needs conversion rate. Legal needs citation accuracy. Code search needs task success. Generic retrieval metrics like precision and recall are insufficient for evaluating whether RAG serves your domain's actual goals.

Iterate based on failure analysis. Deploy, gather user feedback, identify where RAG fails, and trace failures to architectural decisions. If users complain about stale results, improve freshness. If they complain about irrelevant results, improve retrieval ranking. Let real failures guide evolution.

The pattern catalog in this chapter isn't exhaustive—it's illustrative. Your domain might combine elements from multiple patterns. Healthcare customer support needs both fast resolution and citation accuracy. Legal e-discovery needs both legal citation and large-scale document processing. The top 1 percent don't follow patterns dogmatically. They understand the principles behind each pattern and compose architectures that fit their specific requirements, constraints, and users.

The mistake is building generic RAG and expecting it to work for specialized domains. The second mistake is over-engineering for requirements you don't have. The third mistake is copying architectures from different domains without understanding why they're designed that way. Build the RAG architecture your use case actually needs, measure whether it delivers value in domain-specific terms, and evolve it based on real user feedback and measurable outcomes.
