# Chapter 2 â€” Indexing, Chunking, and Data Preparation

Your RAG system is only as good as the data you feed it. This is not a platitude. It is a hard constraint. If your documents are poorly chunked, your embeddings will be incoherent. If your metadata is incomplete, your filters will fail. If your ingestion pipeline drops tables or images, your answers will be incomplete. Data preparation is the foundation of retrieval quality, and in 2026 it remains the most underestimated part of the RAG stack.

This chapter teaches you how to ingest, chunk, enrich, and index data for retrieval. You will learn how to design chunking strategies that balance semantic coherence with embedding model constraints, how to preserve structure across tables and images, how to attach metadata that improves filtering and ranking, and how to maintain data quality as your corpus grows. The decisions you make here propagate through every query your system serves.

Ingestion is the process of extracting text and structure from source documents. In 2026, production systems ingest from PDFs, HTML, Markdown, Word documents, spreadsheets, images, audio transcripts, and APIs. Each format requires different parsers, and each parser introduces edge cases. PDFs with embedded tables require layout-aware extraction. HTML pages require cleaning of navigation and ads. Scanned documents require OCR, which introduces errors. You will learn how to build robust ingestion pipelines that handle format variance without losing fidelity.

Chunking is the process of splitting documents into retrievable units. The core challenge is that embeddings represent fixed-length vectors, but documents have arbitrary length and internal structure. Naive chunking by character count or sentence breaks destroys semantic coherence. Chunking too small produces fragments that lack context. Chunking too large produces embeddings that represent multiple topics poorly. You will learn how to choose chunk size based on your embedding model, your query patterns, and your document structure.

Chunk size is a function of embedding model capacity and the granularity of facts in your corpus. Sentence transformers perform best with one to three sentences per chunk. Long-context embedding models can handle paragraphs or pages. But larger chunks reduce retrieval precision because embeddings average over more content. In 2026, the default is 256 to 512 tokens per chunk, but you will learn when to deviate based on domain and query length.

Overlap between chunks ensures that facts split across boundaries remain retrievable. Without overlap, a sentence that spans two chunks may be invisible to both embeddings. Typical overlap is 10 to 20 percent of chunk size, but overlap increases index size and slows retrieval. You will learn how to measure whether overlap improves recall and when to skip it.

Metadata is the structured information attached to each chunk: source document, author, timestamp, section heading, page number, confidence score, access control tags. Metadata enables filtering before or after retrieval, which improves precision and reduces cost. For example, filtering by date range before embedding search avoids retrieving outdated policies. Filtering by user permissions ensures secure retrieval in multi-tenant systems. You will learn what metadata to extract, how to store it, and how to query it efficiently.

Tables and images are first-class citizens in modern RAG. Tables contain dense structured information that chunking destroys. Images contain diagrams, charts, and visual context that text embeddings miss. In 2026, production systems either convert tables to text with row and column headers preserved, or use multimodal embeddings that encode tables and images directly. You will learn when to use each strategy and how to evaluate quality.

Hierarchical chunking creates nested levels of granularity: documents, sections, paragraphs, sentences. Retrieval can happen at any level, and context assembly can pull parent chunks when a child chunk is retrieved. This improves coherence when answers require broader context than a single chunk provides. Hierarchical chunking is standard in long-form document retrieval such as legal contracts and research papers.

Multi-format corpora mix unstructured text, structured tables, code, and metadata. Unified retrieval over multi-format data requires schema-aware indexing and hybrid search strategies. You cannot embed SQL tables the same way you embed prose. You will learn how to design ingestion pipelines that handle heterogeneous data without losing structure or precision.

Incremental indexing is the process of updating the index as new documents arrive or existing documents change. Batch reindexing is expensive and introduces downtime. Incremental indexing requires tracking document versions, updating embeddings for changed chunks, and invalidating cached retrievals. In 2026, production systems use incremental indexing with eventual consistency guarantees. You will learn how to design update pipelines that maintain quality without full reindexing.

Data quality is measured by completeness, accuracy, and consistency. Incomplete documents produce incomplete answers. Incorrect metadata produces incorrect filters. Inconsistent chunking across document versions produces retrieval drift. You will learn how to audit data quality before indexing, how to detect quality regressions in production, and how to design feedback loops that improve data over time.

Deduplication is essential when the same content appears in multiple documents or formats. Duplicate chunks waste storage, slow retrieval, and bias ranking toward repeated content. Deduplication can happen at the document level before chunking, or at the chunk level after embedding. Exact deduplication is cheap. Near-duplicate detection requires embedding similarity thresholds. You will learn when each strategy is necessary and how to measure impact on retrieval.

Versioning ensures that answers reflect the correct version of source documents. When a policy changes, old chunks must be deprecated and new chunks indexed. When a bug fix updates documentation, stale chunks must be purged. Versioning requires timestamping chunks, maintaining version history, and supporting queries scoped to specific versions or date ranges. In 2026, versioning is standard in compliance-sensitive domains like legal and medical.

Preprocessing includes cleaning, normalization, and enrichment before chunking. Cleaning removes boilerplate, ads, and navigation. Normalization standardizes dates, numbers, and abbreviations. Enrichment adds synthetic metadata like topic tags or named entities. Preprocessing improves retrieval precision but adds latency and cost to ingestion. You will learn what preprocessing is worth the complexity and what is premature optimization.

Source trust assigns confidence scores to documents based on origin, author, or freshness. Trusted sources such as official documentation or peer-reviewed papers receive higher ranking than user-generated content. Stale documents receive lower ranking than recently updated ones. Source trust can be encoded as metadata and used in reranking or filtering. You will learn how to model source trust and how to balance it against relevance.

This chapter is dense because data preparation has more edge cases than any other part of the RAG pipeline. But the payoff is direct. Better chunking, metadata, and ingestion produce better retrieval, which produces better answers. In 2026, the teams with the best RAG systems are the teams who treat data preparation as a first-class engineering discipline.
