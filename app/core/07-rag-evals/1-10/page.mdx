# 1.10 — Hybrid RAG: Combining Multiple Retrieval Strategies

In October 2024, a legal research platform launched semantic search over case law. Their vector embeddings captured conceptual similarity beautifully. Queries like "cases about privacy violations in workplace surveillance" retrieved relevant precedents even when exact terminology varied. But they lost precision on exact-match queries. Lawyers searching for "Section 230 of the Communications Decency Act" got back tangentially related cases instead of cases specifically citing that statute. Users revolted. By January 2025, the company had rebuilt their system to combine dense vector search with traditional keyword search, using a fusion algorithm to merge results. Precision increased by 34 percent. The lesson: single retrieval strategies excel in specific scenarios but leave gaps that hybrid approaches can fill.

You inherit a fundamental tension when building RAG systems: no single retrieval strategy optimally handles all query types. Dense embeddings capture semantic similarity but struggle with exact matches. Keyword search handles precise terms perfectly but misses paraphrases and conceptual queries. Lexical matching finds structural patterns but ignores meaning. The solution isn't choosing the best strategy. It's running multiple strategies and intelligently combining results. Hybrid RAG uses dense plus sparse, vector plus keyword, semantic plus lexical retrieval simultaneously, then fuses results into a unified ranking. This is more complex and more expensive than single-strategy retrieval, but it eliminates entire classes of failure modes.

## The Single-Strategy Ceiling

Dense vector embeddings became the default RAG retrieval strategy because they're conceptually elegant and work remarkably well for semantic queries. You embed documents and queries into continuous vector spaces where semantic similarity corresponds to geometric proximity. Similar meanings cluster together regardless of exact wording.

This magic breaks on specific query patterns. Exact entity matches like product codes, legal citations, or technical identifiers get lost in semantic space. Searching for "RFC 9110" might retrieve documents about HTTP specifications generally rather than documents specifically citing that RFC. The embedding space smooths over the distinction between "mentions HTTP" and "cites RFC 9110."

Rare terms and proper nouns also challenge dense retrieval. If your corpus has one document mentioning "Anthropic," a query for that company might not retrieve it if the embedding model hasn't seen that term frequently during training. Common words and concepts dominate the embedding space. Rare but precisely relevant terms get diluted.

Query length affects dense retrieval quality. Very short queries like "API docs" embed into vague regions of vector space. Very long queries average many concepts into a single vector that might not prioritize the most important concepts. The sweet spot is medium-length queries with clear semantic intent, but users don't always provide that.

Keyword search has opposite strengths and weaknesses. It excels at exact matching. Searching for "RFC 9110" retrieves every document containing that exact string. It handles rare terms perfectly—if the term appears, it matches. It works regardless of query length. But it misses semantic similarity entirely. "How do I authenticate users?" won't match documents about "user login" or "credential verification" unless they use the exact word "authenticate."

## Dense and Sparse: Complementary Strategies

The most common hybrid approach combines dense embeddings with sparse keyword matching. Dense retrieval handles semantic queries. Sparse retrieval handles exact matches and rare terms. Together they cover more query types than either alone.

Sparse retrieval usually means BM25, an algorithm that ranks documents by keyword frequency with length normalization and inverse document frequency weighting. If a query term appears frequently in a short document but rarely across the corpus, that document ranks high. BM25 is fast, deterministic, and explainable—you can see exactly which keywords matched.

Running both strategies requires executing two retrievals per query. Your dense retrieval queries the vector database for approximate nearest neighbors. Your sparse retrieval queries an inverted index with BM25 scoring. Each returns a ranked list of documents. Then you need fusion—combining two ranked lists into a single ranking.

The simplest fusion is reciprocal rank fusion. For each document, you sum its reciprocal ranks from both retrievers. If a document ranks third in dense retrieval and fifth in sparse, its fusion score is one-third plus one-fifth. Documents appearing high in both lists get high fusion scores. Documents appearing in only one list get lower scores. This requires no parameter tuning and works surprisingly well.

Weighted fusion lets you prioritize one retriever over another. You might weight dense retrieval at 0.7 and sparse at 0.3 if most queries are semantic. Or weight sparse higher if most queries are exact-match searches. The weights become hyperparameters you tune on validation data, adding complexity but potentially improving quality.

## When Hybrid Beats Single-Strategy

Hybrid retrieval shines on diverse query workloads. If 100 percent of your queries are semantic questions, pure dense retrieval might be optimal. If 100 percent are exact-match searches, pure keyword search works fine. But real-world query distributions mix both types.

Customer support queries range from vague problem descriptions to specific error codes. "My upload is failing" versus "Error code E4307." The first needs semantic search to find documents about upload failures regardless of exact wording. The second needs keyword search to find documents that mention that specific error code. Hybrid retrieval handles both.

Technical documentation search benefits enormously from hybrid. Developers search for concepts: "how to handle authentication." They also search for exact API names: "UserAuthenticationManager class." Dense retrieval handles the first, sparse the second. Without hybrid, you force users to learn which search style works for which query type.

Legal and regulatory search absolutely requires hybrid. Lawyers need semantic search for conceptual queries: "cases about employer liability for contractor actions." They also need exact citation search: "42 USC 1983." A pure semantic system frustrates users half the time. A pure keyword system frustrates them the other half.

The cost is running two retrievers and fusion logic. You're doubling retrieval infrastructure—maintaining both a vector database and an inverted index. You're increasing per-query latency if retrievals run sequentially, or increasing backend load if they run in parallel. You need eval datasets that cover both semantic and exact-match queries to validate that hybrid actually outperforms single-strategy.

## Semantic and Lexical: Capturing Different Similarity Dimensions

Another hybrid dimension is semantic versus lexical similarity. Semantic retrieval uses embeddings that understand meaning. Lexical retrieval uses surface-form matching—word overlap, character n-grams, edit distance.

Lexical matching catches queries that semantic embeddings miss. Abbreviations, acronyms, and misspellings often don't embed well. Searching for "k8s" might not retrieve "Kubernetes" if the embedding model doesn't know that abbreviation. Lexical matching can use fuzzy matching to catch spelling variations and common abbreviations.

Code search benefits from lexical patterns. Searching for "def calculate_total" should match function names exactly. Semantic embeddings might retrieve functions with similar purposes but different names. Lexical matching ensures you find the exact function you're looking for.

Product search uses lexical matching for model numbers and SKUs. "iPhone 14 Pro Max 256GB" should match exactly, not retrieve semantically similar products like "iPhone 15 Pro." Lexical matching handles these precise specifications better than semantic embeddings that blur distinctions between similar products.

The challenge is that pure lexical matching is brittle. Typos break it unless you implement fuzzy matching, which has its own precision problems. Synonyms don't match: "purchase" versus "buy," "terminate" versus "end." This is why hybrid semantic plus lexical works—lexical catches exact matches and spelling variations, semantic catches paraphrases and conceptual similarity.

## Multiple Embedding Models: Ensemble Dense Retrieval

Some teams run hybrid retrieval using multiple embedding models, not just dense plus sparse. Different embedding models have different strengths. OpenAI embeddings might excel at general semantic similarity. Domain-specific embeddings trained on legal text might be better for legal search. Multilingual embeddings handle cross-language queries.

Ensemble retrieval runs queries through multiple embedding models and fuses results. You embed documents with model A and model B at index time. At query time, you embed the query with both models and retrieve from both indexes. Fusion combines the two result sets.

The cost is significant. You're storing multiple vector indexes, potentially doubling or tripling storage. You're running multiple embedding API calls per query, multiplying cost and latency. The benefit must justify this expense. If a single embedding model handles 95 percent of queries well, ensemble retrieval is overkill.

The use case for ensemble embeddings is when your query distribution is genuinely multimodal. You serve both technical queries and general knowledge queries. You handle multiple languages. You need both recency-sensitive embeddings and timeless semantic embeddings. Each dimension benefits from a specialized model, and fusion lets you leverage all of them.

Very few production systems use ensemble embeddings because the cost usually exceeds the benefit. The exception is when different user personas have different query patterns. Technical users search with jargon, non-technical users with plain language. Training or selecting embeddings for each persona, then fusing results, can improve satisfaction across both groups.

## Re-Ranking as a Hybrid Layer

Re-ranking is a hybrid strategy that uses fast retrieval to get candidates, then uses a slower, more accurate model to re-rank them. Your first-stage retrieval might use dense embeddings to get the top 100 documents. Then a cross-encoder re-ranks those 100 to pick the best 10.

Cross-encoders are powerful but slow. They encode the query and document together, allowing attention across both. This captures relevance better than independent embeddings but requires encoding every query-document pair. Running a cross-encoder over a million documents is infeasible. Running it over 100 candidates is practical.

Re-ranking improves precision at the cost of latency. Your first-stage retrieval is fast but might include false positives. Re-ranking filters them out, ensuring the final top-K results are highly relevant. This two-stage architecture is common in search engines and increasingly common in RAG systems.

The hybrid aspect is that first-stage retrieval and re-ranking can use different strategies. First-stage might use hybrid dense-sparse retrieval. Re-ranking might use a cross-encoder, an LLM-based relevance scorer, or even a combination. You're stacking multiple retrieval strategies at different stages.

The cost model changes. First-stage retrieval must be cheap because it runs on every query. Re-ranking can be expensive because it only runs on a small candidate set. You might use a small embedding model for first-stage and a large LLM for re-ranking, balancing cost and quality.

## Fusion Algorithms: How to Combine Rankings

Reciprocal rank fusion is simple and works well, but it's not the only fusion option. Score-based fusion normalizes scores from each retriever and combines them with weights. If dense retrieval returns scores from 0.6 to 0.9 and sparse returns scores from 0 to 100, you normalize both to a common range, then average them.

The challenge is that scores from different retrievers aren't comparable. A dense similarity score of 0.8 doesn't mean the same thing as a BM25 score of 80. Normalization helps but doesn't eliminate the semantic mismatch. Some teams use learned fusion, training a model to predict relevance from multiple retriever scores.

Learned fusion requires training data—queries with ground truth relevant documents. You run multiple retrievers, collect their scores for each document, and train a model to predict relevance from the score vector. This can outperform simple fusion rules but requires labeled data and introduces another model to maintain.

Distribution-based fusion looks at the score distributions from each retriever and uses percentile-based combination. If a document is in the 95th percentile of dense scores and 80th percentile of sparse scores, it gets a high fusion score even if the raw scores aren't directly comparable. This is more robust to score scale differences than raw score averaging.

The evaluation gap is that most teams test fusion methods on small validation sets and pick the one with the highest metrics. They don't test robustness to query distribution shift or retriever failures. What happens if one retriever starts returning degraded results? Does your fusion fail gracefully or degrade catastrophically? Building eval for fusion robustness is rare but valuable.

## The Cost of Running Multiple Retrievers

Hybrid retrieval doubles or triples infrastructure cost compared to single-strategy. You maintain a vector database for dense retrieval and an inverted index for sparse retrieval. Each requires storage, compute for indexing, and compute for queries.

Storage cost compounds because you're indexing the same documents multiple times. A 10GB document corpus might become 5GB of vectors and 3GB of inverted index. If you're using multiple embedding models, add another 5GB per model. Cloud storage is cheap, but vector database storage is expensive—some services charge per vector stored.

Query cost increases because you're running multiple retrievals. If dense retrieval costs 10ms and sparse costs 5ms, sequential execution takes 15ms. Parallel execution takes 10ms but uses more backend resources. Fusion adds another 1-2ms. The latency increase might be acceptable, but it's noticeable.

Embedding API costs can become prohibitive with multiple models. If you're using two embedding models and processing 1 million queries per month, you're paying for 2 million embedding calls instead of 1 million. API costs scale linearly with model count.

The operational burden is maintaining multiple systems. Your vector database might fail while your keyword index is healthy. Your inverted index might need reindexing while your vectors are fine. Each component has different scaling characteristics, different failure modes, and different operational requirements.

Cost optimization focuses on selective hybrid retrieval. Not every query needs all retrievers. If a query looks like a semantic question, skip sparse retrieval. If it looks like an exact-match search, skip dense retrieval. Query classification adds latency but can significantly reduce infrastructure cost by avoiding unnecessary retrievals.

## When Hybrid Isn't Worth It

Hybrid retrieval is overkill for many use cases. If your query distribution is homogeneous and a single strategy works well, adding complexity doesn't help. Simple FAQ systems with straightforward questions often work fine with dense retrieval alone.

Internal documentation search with technical users might work fine with sparse retrieval alone if users are comfortable with Boolean search and exact matching. Adding semantic search helps, but if users aren't asking semantic queries, the investment is wasted.

The litmus test is query analysis on real user data. Sample 1000 queries and categorize them. How many are semantic? How many are exact-match? How many are ambiguous? If 95 percent are one type, optimize for that type. If queries are 50-50, hybrid makes sense.

Another consideration is whether better single-strategy optimization beats hybrid. Maybe your dense retrieval underperforms because your chunking strategy is wrong or your embedding model is weak. Improving those might close the gap enough that hybrid isn't needed. Hybrid is a patch for gaps between strategies, but improving the primary strategy might eliminate gaps entirely.

Very small corpora often don't benefit from hybrid. If you have 500 documents, both dense and sparse retrieval will return similar results. The cost and complexity of hybrid isn't justified. Hybrid shines on large, diverse corpora where different retrieval strategies produce genuinely different result sets that fusion can combine productively.

## The Future: Learned Hybrid Retrieval

Current hybrid systems use hand-designed fusion rules. Reciprocal rank fusion, weighted averaging, score normalization—all are heuristics. The future is learned hybrid retrieval where a model learns to combine retrievers based on query characteristics.

Query-aware routing uses a classifier to predict which retrieval strategy will work best for each query. Semantic queries route to dense retrieval, exact-match queries route to sparse, ambiguous queries route to hybrid. This optimizes cost by avoiding unnecessary retrievals while maintaining quality.

Learned fusion uses a model to predict relevance from multiple retriever signals. Instead of reciprocal rank fusion, you train a learning-to-rank model on query-document pairs with features from all retrievers. This can capture subtle interactions between retrieval strategies that fixed rules miss.

The challenge is gathering training data. You need queries with ground truth relevance labels for documents. In many domains, this data doesn't exist. You might bootstrap with user clicks or LLM-generated relevance scores, but both are noisy signals. High-quality learned hybrid retrieval requires high-quality relevance judgments.

Some research systems use the LLM itself as a fusion mechanism. You retrieve with multiple strategies, send all results to the LLM, and let it decide which to use in generation. This is conceptually appealing but expensive—you're feeding more documents to the LLM and relying on its judgment about relevance. For domains where LLM inference cost is acceptable, this might be simpler than building explicit fusion logic.

## Building Hybrid RAG: A Practical Roadmap

Start with single-strategy retrieval and measure its failure modes. Deploy dense retrieval, log queries, and review failures. What query patterns underperform? Exact-match searches? Rare terms? Multilingual queries? Let data drive the decision to add hybrid.

If you decide hybrid is justified, start with dense plus sparse. This is well-understood and widely supported. Many vector databases now support hybrid search natively, handling fusion internally. This is simpler than building separate indexes and fusion logic yourself.

Use reciprocal rank fusion initially. It's simple, parameter-free, and works well. Only move to weighted fusion or learned fusion if you have validation data showing significant improvement. Most teams overestimate the benefit of sophisticated fusion and underestimate the operational cost.

Monitor cost carefully. Track retrieval latency, infrastructure cost, and embedding API spend. Hybrid retrieval that improves quality by 10 percent but triples cost is often a bad trade-off. The cost must be justified by user value or business outcomes, not abstract quality metrics.

Build eval datasets that specifically test hybrid benefit. Include semantic queries, exact-match queries, and edge cases. Measure single-strategy performance and hybrid performance on each category. If hybrid doesn't significantly outperform single-strategy on any category, it's not adding value.

The goal isn't to build the most sophisticated retrieval system. It's to build the simplest system that meets quality requirements at acceptable cost. For many teams, that's single-strategy dense retrieval. For teams with diverse query patterns, exact-match requirements, or high precision needs, hybrid retrieval is worth the complexity. The top 1 percent let use case requirements and measured failure modes drive the decision, not architectural fashion. They build hybrid when it's needed, avoid it when it's not, and always measure whether the added complexity delivers enough value to justify the cost.
