# 1.9 — RAG for Structured vs Unstructured Data

Why do analytics teams build RAG systems that hallucinate numbers instead of querying databases? Because every tutorial teaches document retrieval, and teams assume the pattern generalizes to structured data. It does not. Retrieving documentation about sales tables is not the same as executing SQL against live data. The analyst asks "What were total sales in Q4 2024?" and gets a confident answer synthesized from schema documentation—a number that sounds plausible but is completely fabricated. The rebuild takes three months and requires text-to-SQL, schema-aware retrieval, and hybrid structured-unstructured fusion.

You inherit a critical architectural distinction when building RAG systems: structured versus unstructured data sources. Most RAG tutorials focus exclusively on unstructured data—documents, PDFs, web pages, support tickets. This is conceptually simpler. You chunk text, embed it, retrieve it, generate answers. But real enterprise systems have structured data—databases, spreadsheets, APIs, knowledge graphs. Retrieving from structured sources requires fundamentally different strategies. Text-to-SQL, schema-aware retrieval, hybrid structured-unstructured fusion—each adds complexity but unlocks data that document-only RAG can't access.

## The Document-Centric RAG Default

Most RAG implementations start with unstructured documents because the pattern is well-established. You have a corpus of text—documentation, articles, support tickets, email threads. You chunk it into passage-sized pieces, embed each chunk with a vector model, store embeddings in a vector database. When users query, you embed the query, retrieve similar chunks, feed them to an LLM, and generate an answer.

This pattern works beautifully for inherently textual domains. Customer support knowledge bases are collections of articles explaining how to solve problems. Legal case databases are collections of court opinions and legal briefs. Medical literature databases are collections of research papers and clinical guidelines. The source material is text, the queries are text-seeking, and the answers are synthesized from text.

The chunking strategy dominates retrieval quality. How do you split documents into retrievable units? Sentence-level chunking gives fine-grained retrieval but loses broader context. Document-level chunking preserves context but reduces precision—you retrieve entire documents when only one section is relevant. Paragraph-level or section-level chunking is often the sweet spot, balancing context and precision.

Metadata enrichment adds structure to unstructured sources. You tag documents with author, date, category, topic, or domain-specific metadata. Then you can filter retrieval: "Find documents about API authentication written after January 2024." This hybrid approach uses semantic search for content matching and metadata filtering for categorical constraints.

The limitation is that document-centric RAG can only answer questions whose answers exist in the text corpus. "What was our revenue last quarter?" can only be answered if someone wrote a document stating that number. If the number exists only in a database, document RAG fails unless you maintain documents that mirror database content—an expensive and error-prone synchronization problem.

## The Structured Data Challenge: Databases and APIs

Structured data sources have explicit schemas. Relational databases have tables, columns, and relationships. APIs have endpoints, parameters, and response formats. Spreadsheets have rows, columns, and formulas. The data is organized for programmatic access, not narrative text retrieval.

When users ask questions about structured data, they're implicitly requesting computations. "What were total sales in Q4 2024?" requires summing a sales column filtered by date. "Which products had the highest return rate?" requires joining sales and returns tables, grouping by product, and calculating percentages. "What's the average customer lifetime value for enterprise clients?" requires complex multi-table joins and aggregations.

Naive document-RAG approaches fail here. You could embed database documentation or schema descriptions. When someone asks about sales, you retrieve documentation about the sales table. But the LLM generates an answer based on documentation, not actual data. It might hallucinate plausible numbers or refuse to answer because the documentation doesn't contain the specific data point.

The correct approach is text-to-SQL: translate natural language queries into SQL queries, execute them against the database, and use results in answer generation. This is RAG in the sense that you're retrieving information to augment generation, but the retrieval mechanism is SQL execution, not semantic search over embeddings.

Text-to-SQL introduces entirely new failure modes. The LLM must understand your database schema well enough to write correct SQL. It must handle table joins, aggregations, filtering, and complex logic. It must avoid SQL injection vulnerabilities. It must generate efficient queries that won't time out on large tables. Each of these is a hard problem that document-RAG doesn't face.

## Text-to-SQL as a RAG Variant

Text-to-SQL systems take natural language questions and generate SQL queries to answer them. This has been an AI research problem for decades, but modern LLMs made it dramatically more capable. GPT-4 can write remarkably sophisticated SQL when given schema information and well-crafted prompts.

The architecture has three stages: schema retrieval, SQL generation, and result synthesis. Schema retrieval identifies which tables and columns are relevant to the user's question. If your database has 200 tables, you can't fit the entire schema in an LLM prompt. You need semantic search over table and column descriptions to find relevant schema elements.

SQL generation takes the question and relevant schema and asks the LLM to write SQL. The prompt includes schema definitions, example queries, and constraints like "only use SELECT queries, no mutations" or "limit results to 1000 rows." The quality of your prompt determines the quality of generated SQL.

Result synthesis executes the SQL, gets back data, and uses that data in answer generation. If the query returns 1500 rows, you don't feed all of them to the LLM. You might aggregate them first, take a sample, or ask the LLM to generate a summary query that reduces data volume. Then you generate a natural language answer incorporating the query results.

Each stage can fail. Schema retrieval might miss relevant tables, causing SQL generation to fail or use wrong tables. SQL generation might produce syntactically invalid SQL, semantically wrong SQL, or dangerously inefficient SQL. Result synthesis might misinterpret query results or hallucinate conclusions not supported by the data.

## Schema-Aware Chunking and Retrieval

Large databases require schema-aware retrieval. You can't send a 5000-table schema to an LLM. You need to identify the 5-10 tables relevant to each query. This is a retrieval problem, but over schema metadata rather than documents.

The naive approach embeds table descriptions and column names, then uses semantic search to find relevant tables. If a user asks "What were sales in Q4?" you retrieve tables with "sales" in the name or description. This works for simple cases but breaks when table names are opaque or when relevant tables don't have obvious semantic similarity to the query.

Schema graphs encode relationships between tables. Foreign keys, join paths, and semantic relationships help retrieval. If the user asks about "customer lifetime value," you might need to join customers, orders, and products tables. A schema-aware retrieval system understands these relationships and retrieves connected tables together.

Some teams maintain a separate vector database of schema metadata. Each table and column has a detailed description optimized for retrieval. "The orders table contains all customer purchase transactions including order date, total amount, and payment status. Used for revenue analysis and customer behavior queries." This description is embedded and retrieved when users ask about orders, revenue, or purchase history.

Schema evolution creates a maintenance burden. When developers add tables, rename columns, or change relationships, your schema metadata must stay synchronized. Stale schema metadata causes SQL generation to fail or reference non-existent tables. Most teams underestimate this operational overhead, discovering it painfully when generated SQL starts breaking after database migrations.

## Hybrid Structured and Unstructured Retrieval

Real enterprise questions often require both structured and unstructured data. "What were Q4 sales and what were the main drivers?" The sales number comes from a database query. The drivers come from analyst reports, executive summaries, or meeting notes—unstructured documents.

Hybrid RAG systems retrieve from multiple sources and synthesize results. You run text-to-SQL against the database to get quantitative data. You run semantic search against document embeddings to get qualitative context. You feed both to the LLM for synthesis.

The challenge is coordination. Do you retrieve in parallel or sequentially? Parallel retrieval is faster but might retrieve irrelevant documents if you don't know what the database query will return. Sequential retrieval lets you use database results to inform document retrieval—"sales were down 15 percent, now find documents explaining why"—but adds latency.

Query decomposition helps. You use an LLM to analyze the user's question and identify sub-questions. "What were Q4 sales?" goes to text-to-SQL. "What were the main drivers?" goes to document retrieval. You execute both, then synthesize results. This is more complex but provides better control over retrieval strategy for each sub-question.

The evaluation complexity multiplies. You need ground truth for database queries, ground truth for document retrieval, and ground truth for synthesis quality. Most teams struggle to build comprehensive hybrid eval datasets, instead testing database RAG and document RAG separately and hoping they compose well. This is insufficient for production systems where hybrid queries are common.

## Spreadsheets and CSV Files: The Middle Ground

Spreadsheets are structured data in unstructured formats. They have rows, columns, and formulas like databases, but they're files, not queryable systems. Users share them, email them, and version them independently. Enterprise teams often have hundreds of spreadsheets with overlapping, inconsistent, or outdated data.

RAG over spreadsheets requires ingesting them into a queryable format. Some teams load spreadsheets into databases on ingestion, then treat them like database tables. Others parse spreadsheets into structured metadata and use text-to-SQL-like approaches to query them. The challenge is that spreadsheet schemas are often implicit—column headers might be informal, data types are inconsistent, and business logic is embedded in formulas.

Another approach is embedding spreadsheet content as documents. You convert each row into a text passage like "Product: Widget A, Category: Electronics, Q4 Sales: 15000, Growth: 12 percent" and embed it. This lets you use standard semantic search but loses the ability to do aggregations or complex filtering. You can answer "Which products are in the Electronics category?" but not "What was the average sales growth for Electronics products?"

Hybrid spreadsheet RAG combines both approaches. You maintain structured metadata for aggregations and filtering—product categories, date ranges, numerical columns. You also maintain text embeddings for semantic search over product descriptions, notes, or categorical values. Queries that need computation use structured access. Queries that need semantic matching use embeddings.

The operational burden is spreadsheet management. When users upload new versions, you need to detect schema changes, validate data quality, and update both structured and unstructured indexes. Most teams underinvest in this ETL infrastructure, leading to RAG systems that answer questions based on stale or inconsistent spreadsheet data.

## Knowledge Graphs: Structured Semantic Data

Knowledge graphs are structured data optimized for relationships. Instead of tables and rows, you have entities and edges. "Alice works for Acme Corp" becomes nodes for Alice and Acme Corp with a "works for" edge. "Acme Corp acquired Beta Inc in 2024" adds nodes and edges representing the acquisition relationship.

RAG over knowledge graphs uses graph traversal instead of SQL queries. A user asks "Who are the key executives at companies we acquired in the last two years?" You identify entities matching "acquired companies," traverse edges to find people with executive roles, and filter by acquisition date. This is conceptually similar to SQL but uses graph query languages like SPARQL or Cypher.

The advantage is explicit relationship modeling. Databases use foreign keys and joins. Knowledge graphs have typed relationships as first-class entities. "Reports to," "subsidiary of," "competes with," "partner of"—each relationship type is explicit and queryable. This makes certain kinds of questions easier to answer, especially multi-hop reasoning like "Find people who worked at companies that competed with our portfolio companies."

The disadvantage is construction and maintenance cost. Building a knowledge graph requires entity extraction, relationship extraction, and entity resolution. If two documents mention "Apple," is that the same entity? If one says "Tim Cook is CEO of Apple" and another says "Apple acquired Beats in 2014," you need to link both statements to the same Apple entity.

Hybrid knowledge graph RAG combines graph traversal with document retrieval. The graph provides structured facts and relationships. Documents provide detailed context and explanations. A user asks about a company's acquisition strategy. You use the graph to identify all acquired companies and key people. You use document search to retrieve press releases, analyst reports, and strategy memos explaining the acquisitions.

## API Data as Dynamic Structured Sources

Some structured data lives behind APIs, not databases. Product catalogs, inventory systems, customer profiles, real-time metrics—all accessed via API calls rather than SQL queries. RAG systems that need this data must orchestrate API calls as part of retrieval.

Text-to-API-call is analogous to text-to-SQL. The LLM receives API documentation—endpoints, parameters, response schemas. The user asks a question. The LLM generates an API call specification. Your system makes the API call, receives data, and feeds it to answer generation.

API-based retrieval introduces latency and reliability challenges. API calls are slower than database queries. They might have rate limits, requiring queuing or backoff strategies. They might fail intermittently, requiring retries and error handling. They might return huge responses that exceed your LLM context limits.

Some teams pre-fetch common API responses and cache them. If users frequently ask about current product inventory, you refresh inventory data every few minutes and serve from cache. This reduces latency and API load but introduces staleness. Your answers might reflect inventory from five minutes ago, not real-time state.

The evaluation gap is that most RAG benchmarks assume static data sources. Eval datasets with fixed ground truth don't capture dynamic API scenarios where correct answers change over time. Building eval for API-based RAG requires either synthetic APIs with controlled responses or timestamp-aware ground truth that specifies correct answers at specific times.

## Combining Everything: The Enterprise RAG Architecture

Production enterprise RAG systems almost never use a single data source type. You have documentation in Confluence, customer data in Salesforce, sales data in a data warehouse, product specs in Notion, engineering discussions in Slack, analytics in Looker, and monitoring data from APIs. Comprehensive question-answering requires retrieving from all of these.

Multi-source RAG requires orchestration layers that route queries to appropriate sources. Query analysis determines whether a question needs documents, databases, APIs, or multiple sources. Retrieval orchestration executes the right retrieval strategies in parallel or sequentially. Result fusion combines results from different sources into coherent context for the LLM.

The complexity is non-linear. Supporting three source types isn't three times harder than supporting one; it's ten times harder. Each source has different authentication, rate limits, schemas, and failure modes. Keeping retrieval logic synchronized across sources is an ongoing maintenance burden. Debugging failures requires understanding which source failed and why.

Most teams start with one source type—usually documents—and expand incrementally. You build document RAG, deploy it, measure usage, and discover that 40 percent of questions require database access. You add text-to-SQL. Later you discover another 20 percent need API calls. You add API orchestration. Each expansion is a major architecture change, not a minor feature addition.

The alternative is designing multi-source architecture from the start. You build abstraction layers that treat all sources uniformly: a retrieval interface that can dispatch to document embeddings, SQL execution, or API calls based on query analysis. This is more upfront engineering but provides flexibility for expansion. The trade-off is that you're building infrastructure for sources you might not need, which risks over-engineering.

## The Structured Data Evaluation Gap

Most public RAG benchmarks use document-based question-answering. HotpotQA, Natural Questions, MS MARCO—all assume unstructured text sources. Very few benchmarks evaluate text-to-SQL quality, API orchestration, or hybrid retrieval across structured and unstructured sources.

This evaluation gap means teams building structured RAG often have no external baselines. You can't compare your text-to-SQL accuracy to published results because there aren't standardized benchmarks. You must build internal eval datasets specific to your database schema and API contracts.

Building structured RAG eval requires database snapshots and ground truth query-answer pairs. You freeze a database state, write questions that require querying that state, and record correct answers. As your database evolves, your eval dataset becomes stale. Maintaining eval datasets that track schema changes is expensive but necessary for confidence in system quality.

The top 1 percent build eval infrastructure that mirrors production data sources. You maintain a staging database with the same schema as production but synthetic data. You build eval datasets against staging, run evaluations continuously, and detect regressions before they reach production. This is significantly more infrastructure investment than document-RAG eval, but it's essential for structured data systems where wrong answers can drive bad business decisions.

Text-to-SQL errors are particularly dangerous. Wrong document retrieval generates a mediocre answer. Wrong SQL generates confidently wrong numbers that look authoritative. "Q4 revenue was 8.5 million dollars" when the actual number was 5.2 million because your SQL joined the wrong tables. Decision-makers trust numbers from databases, so errors here have outsized impact. This is why structured RAG demands higher quality bars and more rigorous evaluation than document RAG.

## When To Use Structured vs Unstructured RAG

The decision comes down to where your data lives and what questions users ask. If users ask questions whose answers exist in documents, use document RAG. If users ask questions that require computation over structured data, use text-to-SQL or API-based RAG. If both are common, build hybrid systems.

Customer support often needs only document RAG. Support articles explain solutions to common problems. Code search is mostly document RAG over code files and documentation. Product information sites can use document RAG over product descriptions and specifications.

Business analytics requires structured RAG. Sales dashboards, financial reporting, operational metrics—all need database queries, not document retrieval. E-commerce recommendation might need hybrid RAG: product attributes from databases, reviews from documents.

The mistake is treating structured data as if it were documents. Embedding database documentation and generating answers from descriptions instead of querying actual data wastes the structure and invites hallucination. The other mistake is forcing structure onto inherently unstructured data. Trying to load free-text support articles into database schemas loses the narrative context that makes them useful.

Match your RAG architecture to your data sources. Unstructured text gets embedding-based retrieval. Structured databases get text-to-SQL. APIs get text-to-API-call. Hybrid needs query decomposition and multi-source orchestration. Building the wrong architecture for your data is the fastest way to deliver a RAG system that looks impressive in demos but fails in production when users ask real questions that your retrieval strategy can't handle.
