# 2.7 â€” Document Hierarchy: Preserving Structure Through Chunking

In December 2024, a knowledge management platform serving enterprise clients launched a RAG system to search internal wikis and documentation repositories. Users immediately reported that search results were confusing and lacked context. A query about "how to configure VPN access" returned a chunk containing step 4 of a 10-step procedure, with no indication that there were steps before or after. Another query about security policies returned a chunk from a subsection titled "Exceptions" without any reference to the parent section "Remote Access Security Policy" or the broader policy document it came from. Users could not tell whether the retrieved chunk was a complete answer or a fragment, whether it was the only relevant section or one of many, or how it related to the rest of the document. The team realized their chunking logic treated all chunks as independent, discarding the document structure that gave chunks meaning. When they added hierarchical metadata and breadcrumb context to chunks, retrieval quality improved dramatically because users could understand where each chunk fit in the document structure.

You are chunking documents in 2026, and most real documents have hierarchical structure: books have chapters and sections, manuals have parts and procedures, research papers have introduction, methods, and results, legal contracts have articles and clauses, and code repositories have directories and files. Naive chunking that splits documents mechanically destroys this structure, creating chunks that lose their position in the hierarchy and their relationship to other chunks. Preserving structure through chunking is essential for retrieval systems where context, provenance, and coherence matter. This chapter walks through the problem of structure loss, parent-child chunk relationships, breadcrumb context, hierarchical retrieval strategies, and the production patterns that maintain document structure through the chunking process.

## The Problem: Naive Chunking Destroys Structure

When you split a document into fixed-size chunks without regard for structure, you sever the relationships that make the document coherent. A section titled "Security Considerations" may contain subsections on authentication, authorization, encryption, and auditing. Naive chunking splits this section into multiple chunks based on token count, with no indication of which subsection each chunk belongs to or how the subsections relate to the parent section.

A user retrieving a chunk from the encryption subsection receives information about encryption but has no way to know that this is part of a broader security section or that there are related subsections on authentication and authorization. If the user's query is about overall security, they may need information from multiple subsections, but retrieval treats each chunk independently and may rank chunks from unrelated sections higher based on keyword overlap.

Structure loss is especially problematic for procedural documents like manuals, tutorials, or troubleshooting guides. A procedure with numbered steps loses its sequential nature when chunked. Step 5 of a configuration procedure may reference "the API key generated in step 2," but if step 2 is in a different chunk that is not retrieved, the reference is meaningless. Users receive instructions that assume prior context they do not have.

The consequences compound in enterprise environments where documents evolve over time. A policy document may have multiple versions, each with different section structures. Without hierarchical metadata tracking version and section path, users cannot tell which version a retrieved chunk came from or how it relates to current policy. A financial services company discovered this when compliance officers retrieved outdated policy chunks that contradicted current requirements, leading to audit findings and regulatory questions.

Hierarchical structure also provides important metadata for filtering and ranking. A chunk from a high-level overview section should be ranked differently from a chunk from a detailed technical appendix. A chunk from a "Getting Started" section should be prioritized for novice queries, while a chunk from an "Advanced Configuration" section should be prioritized for expert queries. Without structure metadata, retrieval has no signal to make these distinctions.

The problem extends beyond missing context. Structure loss creates semantic ambiguity. A heading like "Installation" could refer to software installation, hardware installation, or installation prerequisites, depending on its position in the document hierarchy. Without knowing whether this heading appears under "Software Setup," "Hardware Requirements," or "Before You Begin," the chunk is ambiguous. Retrieval systems that ignore hierarchy will match this chunk to any installation query, regardless of whether it is the right type of installation.

Code documentation presents unique structural challenges. API references have hierarchical relationships between modules, classes, methods, and parameters. A method description chunk makes sense only in the context of its parent class and module. Without this context, users retrieving a method description cannot tell which class it belongs to, what module provides it, or how to import it. A developer documentation platform discovered this when users complained that code examples returned by search were missing import statements and class context, making them impossible to use without consulting the full documentation.

## Parent-Child Chunk Relationships

One solution to structure loss is to maintain parent-child relationships between chunks. Instead of treating all chunks as independent, model the document hierarchy explicitly: a chapter is a parent chunk, sections within the chapter are child chunks, and subsections are grandchild chunks. Each chunk stores metadata about its parent and children, enabling hierarchical retrieval.

When you ingest a document, parse its structure to identify headings, section numbers, and hierarchical levels. Create chunks at multiple levels of granularity: a high-level chunk for the entire chapter, mid-level chunks for sections, and low-level chunks for subsections or paragraphs. Store the hierarchy in metadata: each chunk has a parent ID, a list of child IDs, and its depth in the hierarchy.

This hierarchical chunking enables several retrieval strategies. The simplest is to retrieve a child chunk and also provide its parent as additional context. If a query matches a subsection chunk, retrieve both the subsection and the parent section. The LLM receives the focused answer from the subsection plus the broader context from the parent, improving answer completeness and coherence.

The implementation requires careful design of chunk granularity. Too many hierarchy levels create storage overhead and retrieval complexity. Too few levels fail to capture meaningful structure. The production pattern is to create three levels: document-level chunks for overviews and tables of contents, section-level chunks for main topics, and subsection-level chunks for detailed content. This balances granularity with manageability.

Another strategy is to retrieve at multiple levels based on query specificity. For a broad query like "what is the security policy," retrieve high-level chunks like the entire security section. For a specific query like "how to configure SSL encryption," retrieve low-level chunks like the SSL subsection. Query analysis or metadata filtering can determine which level of granularity to target.

Query classification can route different query types to different hierarchy levels. Definitional queries like "what is X" benefit from high-level chunks that provide overviews. How-to queries like "how do I configure X" benefit from low-level chunks with step-by-step instructions. Comparison queries like "X versus Y" benefit from mid-level chunks that discuss both topics within their broader context.

Hierarchical relationships also enable expansion and refinement. If a high-level chunk is retrieved but the LLM determines it lacks sufficient detail, the retrieval system can fetch child chunks for more specifics. Conversely, if a low-level chunk is retrieved but lacks context, the system can fetch the parent chunk to provide background. This progressive retrieval pattern is useful for interactive systems where users iteratively refine their queries.

Progressive retrieval works particularly well for exploratory use cases. A user starts with a broad query about "network security," retrieves the high-level security section chunk, sees it mentions encryption as a subsection, and follows up with a more specific query about encryption. The system retrieves the encryption subsection chunk with the parent security section as context, giving the user both focused detail and awareness of how encryption fits into the broader security framework.

Implementing parent-child relationships requires additional metadata storage and retrieval logic. Each chunk must store its position in the hierarchy, and the retrieval system must support queries that fetch related chunks. Vector databases vary in their support for graph-like relationships, so you may need to implement this logic in the application layer rather than in the database.

The metadata schema for hierarchical chunks typically includes parent chunk ID, child chunk IDs, sibling chunk IDs for navigation, depth level, and hierarchical path from root to current node. This path enables breadcrumb generation and allows filtering by document structure. For example, you can retrieve all chunks under "Chapter 3: Security" by filtering on chunks whose hierarchical path starts with that prefix.

## Breadcrumb Context: Adding Section Headers to Chunks

An alternative to explicit parent-child relationships is to enrich each chunk with breadcrumb context: prepend the chunk text with its position in the document hierarchy, including section titles, chapter numbers, and heading paths. For example, a chunk from a technical manual might start with "Chapter 5: Network Configuration / Section 5.3: Firewall Rules / Subsection 5.3.2: Configuring SSL Inspection" followed by the chunk content.

Breadcrumbs provide context in a way that is transparent to the retrieval system. The breadcrumb text is embedded along with the chunk content, so queries that match the section titles or hierarchy will rank the chunk higher. More importantly, when the chunk is retrieved and passed to the LLM, the breadcrumb tells the LLM where the chunk came from and how it relates to the rest of the document.

The advantage of breadcrumbs is simplicity. You do not need to maintain explicit parent-child relationships or modify retrieval logic. You just prepend metadata to each chunk during ingestion, and the rest of the pipeline is unchanged. The disadvantage is that breadcrumbs increase chunk size, consuming tokens that could be used for content. For deeply nested documents with long section titles, breadcrumbs may take up a significant fraction of the chunk.

Token budget considerations become critical with breadcrumbs. If your target chunk size is 512 tokens and breadcrumbs consume 100 tokens, you are left with 412 tokens for actual content. This reduces the amount of information each chunk can carry and may require splitting content into more chunks than would otherwise be necessary. You need to balance the value of context against the cost of reduced content density.

The production pattern is to include breadcrumbs for documents with clear hierarchical structure like manuals, specifications, or knowledge bases, and to omit them for flat documents like articles or blog posts. You can also make breadcrumbs optional and include them only when the chunk is ambiguous without context. For example, a chunk titled "Installation" might refer to installation of software, hardware, or a service. Prepending "User Guide / Chapter 3: Software Setup / Installation" disambiguates it.

Adaptive breadcrumbing adjusts the detail level based on chunk depth. High-level chunks receive minimal breadcrumbs since they represent major sections that are self-explanatory. Deep subsections receive full breadcrumbs because they are more likely to be ambiguous without context. This approach optimizes token usage while preserving essential context.

Breadcrumbs should be formatted consistently and concisely. Use separators like " / " or " greater than " to indicate hierarchy levels. Avoid redundant information: if the section number is "5.3.2" and the title is "Configuring SSL Inspection," you do not need to repeat the number in the breadcrumb. Use the most meaningful titles, abbreviating if necessary to save tokens.

Abbreviation strategies include removing common words like "the" and "a," truncating long section titles after the first meaningful noun phrase, and using standard abbreviations for common terms. A breadcrumb "Chapter 5: Advanced Network Configuration and Security Hardening Procedures" might become "Ch5: Adv Network Config and Security," saving tokens while preserving essential meaning.

Some systems use symbolic breadcrumbs instead of text. Instead of "Chapter 5 / Section 5.3 / Subsection 5.3.2," use "5.3.2" as a compact representation. This saves tokens but requires additional metadata to decode the numeric path into meaningful section titles, which can be provided separately to the LLM or displayed in the user interface.

## Preserving Document Outline for Retrieval

Some teams go further and preserve the entire document outline as metadata or as a separate retrievable entity. The outline is a high-level summary of the document structure: chapter titles, section titles, and subsection titles, organized hierarchically. When a query matches the document, the outline is retrieved along with specific chunks, providing the user or the LLM with a roadmap of the document structure.

The outline serves multiple purposes. It helps users understand what the document covers and where specific information is located. It helps the LLM reason about document structure: if the outline shows that the document has a "Troubleshooting" section, the LLM knows to look for chunks from that section for debugging queries. It also enables navigation: users can browse the outline and select sections to dive into, rather than relying solely on search.

Outlines are particularly valuable for long, complex documents where users need orientation. Technical specifications, regulatory documents, and research papers often exceed 100 pages with dozens of sections and subsections. Without an outline, users retrieving a specific chunk have no sense of how much content the document contains or where else they might find relevant information. The outline provides that orientation.

Preserving the outline requires extracting it during ingestion. For structured formats like Markdown, reStructuredText, or HTML with proper heading tags, outline extraction is straightforward: parse the headings and their hierarchy. For PDFs or Word documents, outline extraction is more challenging because headings may not be tagged semantically. You need heuristics based on font size, font weight, or formatting to identify headings, which is error-prone.

PDF outline extraction can leverage document metadata when available. Many PDFs include bookmark structures or outline dictionaries that explicitly define the document hierarchy. Extracting these structures is reliable but not all PDFs have them, particularly older documents or those converted from other formats. Fallback heuristics based on visual formatting become necessary for documents without embedded outlines.

Machine learning models trained on document layout analysis can improve outline extraction from unstructured formats. Models like LayoutLM or Donut can identify headings based on visual features, font characteristics, and position on the page. These models work well for documents with consistent formatting but struggle with documents that mix multiple layouts or use non-standard heading styles.

Once extracted, the outline can be stored as metadata on the document record or as a separate chunk that is retrieved alongside content chunks. Some systems store the outline in a structured format like JSON or a nested list, and render it in the UI as a table of contents. Other systems generate a text representation of the outline and embed it so that queries matching section titles retrieve the outline.

The text representation strategy enables semantic retrieval of outlines. A query about "network security configuration" can retrieve not just chunks containing that content but also the document outline that shows the document has a "Network Security" chapter with a "Configuration" section. This helps users identify which document to explore further, even before reading specific chunks.

The production pattern is to extract outlines for long, structured documents where users benefit from seeing the big picture. Technical manuals, research papers, legal contracts, and policy documents are good candidates. Short articles, emails, or chat logs do not benefit from outlines because their structure is minimal.

Outline storage decisions depend on retrieval patterns. If users frequently ask "what does this document cover," store outlines as separate retrievable chunks with high-level query matching. If users primarily drill down to specific sections, store outlines as metadata and display them in the UI when showing retrieved chunks. Hybrid approaches retrieve outlines for broad queries and individual chunks for specific queries.

## Hierarchical Retrieval Strategies

Hierarchical retrieval is the practice of leveraging document structure to improve retrieval quality. Instead of retrieving chunks purely based on embedding similarity, you use structure metadata to filter, rank, or expand retrieval results. There are several strategies, each with different use cases.

Section-filtered retrieval allows users or the system to specify which sections to search. A user querying "how to install" can filter to the "Installation" section, excluding chunks from "Configuration" or "Troubleshooting" sections. This reduces noise and improves precision. Section filtering requires extracting section metadata during chunking and exposing it as a filter parameter in the retrieval API.

Section filtering becomes particularly powerful when combined with faceted search interfaces. Users can see which sections contain results for their query and refine their search by selecting specific sections. This browsing paradigm complements semantic search by letting users navigate the document structure while searching content. An enterprise knowledge base implemented this pattern, improving user satisfaction scores by 23 percent.

Hierarchical boosting adjusts retrieval scores based on structure. Chunks from high-level sections like introductions or overviews are boosted for broad queries, while chunks from detailed subsections are boosted for specific queries. The intuition is that high-level chunks provide better answers for general questions, while low-level chunks provide better answers for specific questions. Implementing hierarchical boosting requires defining boosting rules based on section depth or section type.

The boosting algorithm can use query analysis to determine specificity. Broad queries with general terms like "overview" or "introduction" boost high-level chunks. Specific queries with technical terms or concrete nouns boost low-level chunks. Query length serves as a proxy for specificity: longer queries with multiple conditions tend to be specific and benefit from detailed chunks.

Contextual expansion retrieves additional chunks based on hierarchy. When a chunk is retrieved, the system automatically retrieves its parent, children, or siblings to provide additional context. This ensures that the LLM receives not just the direct answer but also related information from nearby sections. The tradeoff is that contextual expansion increases the number of chunks passed to the LLM, consuming more context window budget.

Expansion strategies can be configured based on chunk type. When retrieving a detailed subsection chunk, expand to include the parent section for context. When retrieving a high-level section chunk, expand to include child subsections for detail. When retrieving a procedural step, expand to include preceding and following steps to maintain sequence. This adaptive expansion provides the right level of context for each situation.

Multi-level retrieval performs retrieval at multiple granularity levels and combines the results. For example, retrieve the top 3 high-level section chunks and the top 5 low-level paragraph chunks, then merge them. This provides both broad context and specific details. The challenge is combining retrieval scores from different granularities: a high-level chunk may have a lower similarity score than a low-level chunk, but it may still be more valuable for certain queries.

Score normalization becomes essential when combining multi-level results. High-level chunks tend to have broader semantic coverage and may score lower on specific query terms. Low-level chunks are more focused and may score higher on exact matches. Normalizing scores by chunk level or using separate scoring models for each level prevents low-level chunks from always dominating the results.

The production pattern is to start with simple breadcrumb context and section metadata, and add hierarchical retrieval strategies incrementally as you identify use cases where they improve quality. Hierarchical retrieval is most valuable for complex documents with deep structure and for use cases where users need to understand context and relationships, not just find isolated facts.

## Implementation Patterns for Structure Preservation

Preserving document structure requires changes to both the ingestion pipeline and the retrieval logic. During ingestion, you must parse document structure, extract headings and hierarchy, and store structure metadata with each chunk. During retrieval, you must use structure metadata to filter, rank, or expand results.

For ingestion, use format-specific parsers that understand document structure. For Markdown, parse headings based on hash symbols. For HTML, parse heading tags like H1, H2, H3. For Word documents, use python-docx to access styles and identify headings. For PDFs, use heuristics based on font size or extract outline information from the PDF structure if available. Store the hierarchy as metadata on each chunk: section title, section number, depth level, parent chunk ID, and child chunk IDs.

Format-specific parsers need careful validation. Markdown parsers should handle edge cases like headings within code blocks that should not be treated as structural headings. HTML parsers should distinguish between heading tags used for navigation versus content structure. PDF heuristics should validate detected headings by checking that they form a coherent hierarchy without missing levels or duplicate numbers.

For retrieval, expose structure metadata as filters in your query API. Allow users or application logic to filter by section, chapter, or document type. Use metadata to adjust retrieval ranking: boost chunks from relevant sections, penalize chunks from irrelevant sections. Implement post-retrieval expansion logic that fetches parent or child chunks based on the initially retrieved chunks.

The API design should support both explicit structure queries and implicit structure awareness. Explicit queries include filter parameters like "section equals Installation" or "depth less than 3 for high-level chunks only." Implicit structure awareness uses query understanding to automatically apply appropriate filters: a query mentioning "overview" automatically boosts high-level chunks without requiring explicit user action.

The challenge is that structure preservation adds complexity to the ingestion pipeline and increases the amount of metadata stored per chunk. For large corpora, this metadata storage can be substantial. The benefit is that retrieval quality improves significantly for structured documents, and users receive answers with context rather than isolated fragments.

Storage optimization strategies include using integer IDs for parent and child references instead of full chunk content, compressing hierarchical paths using prefix encoding, and storing structure metadata separately from chunk content with foreign key relationships. These optimizations reduce overhead while preserving full hierarchical functionality.

Teams that succeed with structure preservation treat it as a first-class feature, not an afterthought. They invest in robust document parsing, validate that structure metadata is extracted correctly, and design retrieval UIs that surface structure to users. They do not bolt structure on after building a flat chunking pipeline. They design the pipeline with structure in mind from the start.

Testing structure preservation requires both unit tests for parsing accuracy and integration tests for retrieval effectiveness. Unit tests verify that extracted hierarchies match document structure: all headings are captured, nesting levels are correct, sibling relationships are accurate. Integration tests measure whether hierarchical retrieval improves answer quality on representative queries compared to flat retrieval.

## Structure Preservation as a Retrieval Quality Multiplier

The knowledge management platform that launched without structure metadata failed not because their embedding model was weak or their chunks were poorly sized, but because users received chunks without context. A chunk containing step 4 of a procedure is useless without steps 1 through 3. A chunk from an exceptions clause is misleading without the main rule it modifies. When they added breadcrumb context and hierarchical metadata, retrieval quality improved not because the retrieval algorithm changed, but because the chunks became interpretable.

Structure is the scaffolding that makes chunks meaningful. Without structure, chunks are isolated fragments that may or may not make sense on their own. With structure, chunks are pieces of a coherent document that can be located, understood, and contextualized. Preserving structure transforms retrieval from keyword matching to contextual search where users understand what they are retrieving and how it relates to the whole.

The measurable impact of structure preservation extends beyond retrieval quality. User engagement metrics improve when structure is preserved: users spend more time with retrieved results because they understand context, they rate answers as more helpful because answers include necessary background, and they return to the system more often because it consistently provides usable information rather than confusing fragments.

When you design your chunking strategy, do not treat structure as optional metadata to be added later. Treat it as essential context that determines whether retrieved chunks are useful. Parse document structure during ingestion, store hierarchical metadata with each chunk, and design retrieval logic that uses structure to improve ranking and context. Validate that users receive enough context to understand and trust retrieved information. Measure how often users need to consult the source document to understand a retrieved chunk, and use that as a signal that structure preservation is insufficient.

Structure preservation requires upfront investment but pays dividends throughout the system lifecycle. Each improvement to structure parsing benefits every query that retrieves structured documents. Each enhancement to hierarchical retrieval improves answer quality across your entire corpus. Structure becomes a competitive advantage that distinguishes professional RAG systems from naive keyword search.

Your chunking strategy is not just about splitting documents into pieces. It is about preserving the relationships and context that make those pieces useful. Structure is the difference between a bag of sentences and a coherent knowledge base. Get structure preservation right, and your retrieval system provides answers with context, provenance, and clarity. Get it wrong, and your users receive fragments that confuse more than they inform.
