# 6.15 â€” Eval Leakage and Contamination: When Test Questions Appear in Documents

In October 2025, a Series B legal research company presented impressive metrics to potential investors during their Series C fundraising roadshow. Their RAG system achieved ninety-six percent accuracy on a comprehensive evaluation dataset of five hundred legal questions, with faithfulness scores above ninety-eight percent and retrieval precision at ninety-one percent. These numbers significantly exceeded published benchmarks from competitors and academic research papers. The evaluation dataset had been carefully curated by experienced legal professionals, covered diverse areas of law, and represented months of annotation work.

Investors were impressed with the technical achievements. The system appeared to be years ahead of competitors based on the metrics. Then a technical due diligence consultant asked to review the evaluation methodology as part of the investment process. Within two hours, she discovered that approximately forty percent of the evaluation questions had been inadvertently included in the indexed document corpus.

The company had created their evaluation dataset by having lawyers write questions based on recent case summaries and legal precedents, then had indexed those same case summaries as part of their expanding legal database. When asked "What was the key holding in Smith v. Jones regarding contract interpretation," the system was not demonstrating legal reasoning; it was retrieving and paraphrasing a document that literally contained that exact question and its answer.

The metrics were meaningless. The deal stalled while the company rebuilt their evaluation infrastructure with properly isolated test data. Six months of carefully cultivated performance data became worthless because test questions had leaked into the knowledge base. The VP of Engineering was forced to admit they had never implemented basic checks to prevent evaluation contamination, and that nobody on the team had considered it a risk worth addressing.

Evaluation leakage occurs when your test questions, expected answers, or close variants appear in the documents your RAG system can retrieve. When this happens, you are not evaluating the system's ability to understand queries and synthesize information from source material; you are evaluating its ability to find and copy text that happens to match your test data.

Metrics become inflated because test cases become trivial lookup tasks rather than representative challenges. You optimize based on these inflated metrics, making changes that improve test performance but do not improve real-world performance. Worse, you lose the ability to detect genuine regressions because your evaluation suite has lost sensitivity to actual capability differences.

## Direct Indexing of Test Data

The most obvious source of leakage is directly indexing test data as documents. You create an evaluation dataset with questions and answers, then accidentally add those questions and answers to your document corpus. This happens more often than teams expect, particularly when evaluation data is stored in shared drives, wikis, or documentation systems that are later bulk-indexed.

A technical writing team creates an FAQ document to test the system, forgets to exclude it from indexing, and suddenly all FAQ questions become trivial to answer because the system retrieves the FAQ document. The evaluation appears to show excellent performance, but it measures nothing about capability on real user questions.

Teams using collaborative documentation systems like Confluence, Notion, or Google Docs for evaluation dataset management are particularly vulnerable. The evaluation dataset lives in the same system as product documentation. Someone bulk imports all Confluence pages into the RAG system without checking which pages contain evaluation data. Every test question now has its answer verbatim in the indexed corpus.

Subtler leakage occurs when test questions are derived from documents that later get indexed. You create test questions by reading internal documentation, then that documentation gets added to the production knowledge base. The system can now find documents that closely match the test questions because that is where the test questions came from.

A customer support team writes test questions based on draft help articles, those articles go through editing and get published, and the published versions get indexed. The test questions might not be verbatim in the indexed documents, but they are close enough that retrieval becomes trivial and answer generation reduces to light paraphrasing. The evaluation no longer tests whether the system can handle real user queries with different phrasing and context.

## Training Data Contamination

Training data contamination represents a related but distinct problem. If you fine-tune your retrieval or generation models using your evaluation dataset as training data, you are explicitly teaching the models to perform well on your tests rather than on the underlying task. The models memorize test examples rather than learning general capabilities.

This is standard train-test split violation familiar from traditional ML, but it often gets missed in RAG contexts because training happens at multiple stages. You might fine-tune the embedding model to improve retrieval, fine-tune the generation model to improve answer quality, or fine-tune ranking models to improve document prioritization.

Each fine-tuning step risks contamination if you do not maintain clean data separation. An engineer looking for diverse training examples might include evaluation data without realizing the implications. The fine-tuning improves test performance dramatically, everyone celebrates, and nobody realizes the improvement comes from memorization rather than capability gain.

Synthetic evaluation data generation can inadvertently create leakage. You use an LLM to generate questions from documents in your corpus, those questions become your evaluation set, and those same documents are searchable by your system. By construction, every test question is answerable from specific documents that generated it.

This seems fine at first: you have ensured all questions are answerable from your corpus. The problem is that the generated questions often closely mirror phrasing and structure in the source documents, making retrieval and answering artificially easy. The evaluation measures whether your system can find the document that generated each question and extract information that is phrased similarly to the question, not whether it can handle natural user queries that do not share vocabulary and structure with source documents.

## Detection Methods

Detection of evaluation leakage requires systematic checks that many teams do not implement until after discovering problems. The most direct approach is text search: for each evaluation question and expected answer, search your document corpus to see if the text appears.

Exact matches are obvious leakage that must be addressed immediately. Near matches are suspicious and require manual review to determine whether the similarity is coincidental or represents contamination. This catches blatant cases but misses subtler leakage where questions are paraphrased or where answers appear in documents but questions do not.

Embedding similarity provides a more sophisticated detection approach. Embed all evaluation questions using your retrieval model and compute similarity against all document chunks. Questions with very high similarity to specific chunks are likely leaked or derived from those chunks.

Set a threshold, perhaps ninety-five percent similarity, and flag any evaluation examples that exceed it for manual review. This catches cases where questions are not verbatim in documents but are close paraphrases. The challenge is calibrating thresholds: legitimate answerable questions should have moderate similarity to relevant documents. You are looking for suspiciously high similarity that suggests the question came from the document rather than representing an independent information need.

Statistical anomaly detection looks for evaluation examples that the system finds implausibly easy. If ninety-five percent of your evaluation examples require retrieving from rank three to fifteen in the retrieval results, but five percent are answered from rank one with perfect confidence scores, those five percent are suspicious.

Manual review might reveal they correspond to leaked examples where the system can retrieve exact or near-exact text matches. This approach does not identify which examples are contaminated but flags outliers for investigation. You still need manual review to confirm contamination, but the statistical signal tells you where to look.

## Prevention Strategies

A priori prevention strategies are more effective than post-hoc detection because they prevent leakage from occurring rather than discovering it later. The foundational principle is strict separation between evaluation data and indexed documents from the moment evaluation datasets are created.

Evaluation data lives in a completely separate storage system from document corpora, with access controls preventing accidental indexing. You treat evaluation data with the same isolation rigor that traditional ML pipelines apply to test sets. Different storage systems, different access permissions, different deployment pipelines.

Some teams implement content hashing and automated checks. Every time you index new documents, compute content hashes and check whether any match hashes of evaluation examples. If matches are found, halt indexing and trigger alerts for manual review.

This provides a safety net against accidental indexing of evaluation data even when humans make mistakes in data management. The automated check catches problems before they corrupt metrics. The system refuses to index documents that match evaluation data, forcing explicit review and override before proceeding.

Temporal separation provides natural protection when feasible. Create evaluation datasets using information from before a specific cutoff date, and only index documents created after that cutoff. A RAG system for news summarization might use evaluation questions from January 2025 and only index articles from February 2025 onward.

By construction, the evaluation questions cannot have been derived from indexed documents because they predate those documents. This approach works when your domain has natural temporal flow and when old evaluation data remains relevant for measuring current system capabilities. It does not work for domains where information is timeless or where test data becomes stale quickly.

Withholding subsets of your document corpus from indexing while using them to generate evaluation data creates clean separation. Identify ten percent of documents, generate evaluation questions from them, and permanently exclude those documents from indexing.

Your evaluation measures whether the system can answer questions when the exact source documents are unavailable, forcing it to find information in other documents or correctly abstain. This approach sacrifices some corpus coverage but ensures evaluation integrity. You lose ten percent of your content from production indexes in exchange for guaranteed clean evaluation.

## Why Leakage is So Common

Why evaluation leakage is the most common RAG evaluation mistake comes down to complexity and distributed ownership. Traditional ML systems have clear train-test-validation splits with data provenance carefully tracked. Someone owns the data pipeline and enforces separation rigorously.

RAG systems have documents that get indexed through various pipelines, often managed by different teams than those building evaluation datasets. Product teams add documents, engineering teams create tests, content teams publish FAQs, and no one maintains a global view of what might contaminate what.

The infrastructure grows organically without centralized data governance, creating many paths for leakage to occur. An engineer adds a new document source, a PM updates FAQs, a content writer publishes help articles, and evaluation contamination happens at the intersection of these independent activities.

Another factor is the iterative nature of RAG development. You build an initial system, create a small evaluation set, index some documents, and start iterating. As the system matures, you expand the document corpus and expand the evaluation set. Documents and evaluation data are added at different times by different people, and tracking which came from which becomes difficult.

Six months into development, no one remembers whether the customer support evaluation questions were written before or after the help center articles were indexed or whether they derived from an early draft of those articles. Institutional knowledge about data provenance fades as team members change and time passes.

The temptation to use the same data for multiple purposes contributes to leakage. You have a valuable set of curated question-answer pairs that your domain experts spent two weeks creating. Using them only for evaluation feels wasteful. Maybe you include them in the indexed corpus to ensure users can find this valuable content. Or you use them to fine-tune your models because they represent high-quality examples.

Each repurposing creates contamination risk. The discipline to maintain separate datasets for separate purposes requires organizational commitment that many early-stage teams lack. The perceived efficiency of reusing data overcomes the discipline of keeping evaluation pure.

## Decontamination Procedures

Decontamination procedures are necessary when leakage is discovered after the fact. First, identify all contaminated evaluation examples through the detection methods described earlier: text search, embedding similarity, statistical anomaly detection. Mark every example that shows evidence of contamination for review.

Second, remove those examples from your evaluation dataset or mark them as invalid and exclude them from metric calculations. You cannot fix contaminated examples by removing documents from the index because the system has already been optimized based on contaminated metrics.

Third, analyze historical evaluation results to understand how contamination affected your conclusions. Which system changes appeared to improve metrics but may have only improved performance on contaminated examples. A change that improved accuracy by five points might have improved clean examples by one point and contaminated examples by twenty points.

Fourth, review any optimization decisions made based on contaminated metrics and consider whether those decisions remain valid with clean evaluation data. You might have prioritized work on retrieval ranking because metrics showed it as a major bottleneck, but contaminated examples might have inflated the apparent impact of ranking changes.

## Rebuilding Trust

Rebuilding trust after discovering evaluation leakage requires transparency and systematic remediation. If you have reported metrics to stakeholders, investors, or customers, you must disclose that previous numbers were inflated due to contamination and provide corrected numbers based on clean evaluation.

This is painful but essential for maintaining credibility. The alternative is having someone else discover the contamination later, which destroys trust more thoroughly than proactive disclosure. The due diligence consultant finding forty percent contamination could have happened during acquisition instead of fundraising, with far worse consequences.

Organizations that handle this well treat it as a learning opportunity, documenting what went wrong and what processes they implemented to prevent recurrence. They publish internal post-mortems explaining how contamination occurred, what the real performance numbers are, and what safeguards now exist. This demonstrates maturity and earns back trust more effectively than trying to hide the problem.

Creating clean evaluation datasets from scratch after contamination is discovered costs significant time and resources. You must generate new questions that have not been derived from indexed documents, annotate expected answers, and identify relevant documents. If you had five hundred evaluation examples and forty percent are contaminated, you need to create two hundred new examples to maintain dataset size.

At fifteen to thirty minutes per example for quality annotation, this represents fifty to one hundred hours of expert time. The cost of rebuilding is real, making prevention far more economical than cleanup. Investing in proper separation infrastructure at the beginning costs a few days. Rebuilding contaminated datasets costs weeks.

## Continuous Monitoring

The most common mistake teams make beyond not checking for leakage at all is assuming that checking once is sufficient. You check during initial evaluation setup, find no leakage, and never check again. Six months later, after multiple document corpus expansions and evaluation dataset updates, leakage has crept in through various paths.

Continuous monitoring is necessary. Every time you add documents to your corpus, check for potential contamination of evaluation data. Every time you add evaluation examples, check whether they match or derive from indexed documents. Automated checks make this continuous monitoring feasible without significant manual effort.

The second common mistake is treating leakage as a binary condition when it exists on a spectrum. Verbatim question text in documents is obvious leakage, but what about questions that share fifty percent word overlap with documents or questions whose expected answers appear verbatim in documents even though questions do not.

These gray areas require judgment calls about how much similarity constitutes contamination for your purposes. Different teams draw lines in different places based on their evaluation goals and metric sensitivity requirements. Document your thresholds and rationale so others understand what level of similarity you consider acceptable versus contaminating.

You prevent evaluation leakage through strict data separation from the beginning, treating evaluation data with the same isolation rigor as test sets in traditional ML. You implement automated checks that detect when documents match or closely resemble evaluation examples. You use temporal or random subset separation to ensure evaluation data derives from different sources than indexed documents.

You monitor continuously as both document corpora and evaluation datasets evolve. You avoid reusing the same data for multiple purposes, maintaining separate datasets for evaluation, training, and content. When leakage is discovered, you decontaminate thoroughly, recompute historical metrics, and implement processes to prevent recurrence.

Your evaluation metrics are only as trustworthy as your evaluation data is clean. Contaminated evaluations are worse than no evaluations because they provide false confidence while hiding real problems. Protect your evaluation integrity with the same rigor you protect production data security. The health of your entire development process depends on it.
