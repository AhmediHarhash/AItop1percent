# 9.7 â€” Multimodal RAG: Images, Tables, and Mixed Content

In February 2025, an engineering firm built a RAG system over their technical documentation to help engineers answer questions about equipment specifications and maintenance procedures. They carefully chunked all their PDF manuals, embedded the text, and deployed a standard text-based RAG system. Within a week, engineers were complaining that the system was useless for their most common questions. An engineer asked, "What's the correct wiring configuration for the industrial pump controller?" and the system retrieved a text chunk that said "See Figure 3.7 for wiring diagram." But Figure 3.7 wasn't in the retrieved context. It was an image on the next page, and the RAG system had completely ignored it. Another engineer asked about pressure specifications and received text saying "Pressure limits vary by model as shown in Table 2.1," but again, the table itself was missing. The engineers pointed out what should have been obvious: technical documentation is not pure text. It's filled with diagrams, schematics, charts, tables, graphs, and images that contain critical information. The text often references these visual elements rather than duplicating their information. A text-only RAG system is fundamentally incomplete for any domain where visual content matters.

You encounter this problem the moment your knowledge base contains anything other than plain text. Technical manuals have diagrams. Scientific papers have plots and figures. Financial reports have charts and tables. Medical literature has anatomical illustrations and imaging scans. Product documentation has screenshots and interface mockups. E-commerce catalogs have product photos. Educational materials have infographics and illustrations. In all these domains, the visual content isn't supplementary decoration; it's essential information. A wiring diagram conveys connection patterns that would take paragraphs to describe in text and would still be ambiguous. A table presents structured data that's immediately scannable visually but becomes dense prose when serialized. A chart shows trends and patterns at a glance that would require statistical analysis to extract from raw numbers.

Multimodal RAG addresses this by retrieving and reasoning over non-text content: images, tables, charts, graphs, diagrams, and mixed-content documents where text and visuals are intertwined. This requires new embedding strategies, new retrieval techniques, and models capable of understanding visual content. The complexity increases significantly, but for domains where visual information is essential, multimodal RAG is not optional. It's the difference between a system that can answer questions and a system that actually has access to the knowledge it needs.

## Extracting Visual Content

The first challenge is extracting and processing non-text content from documents. PDFs and other document formats contain embedded images, tables rendered as graphics, charts, and diagrams. The engineering firm implemented a multi-stage extraction pipeline. They used PDF parsing libraries to identify image regions, table regions, and text regions. Images were extracted as separate files. Tables were detected and converted to structured data using table extraction models. Charts and diagrams were isolated. Each content type was then processed differently: text was chunked and embedded using text embeddings, images were embedded using vision models, tables were converted to both text descriptions and structured representations.

You build this pipeline by selecting tools for each extraction task. PyMuPDF or pdfplumber for PDF parsing and layout analysis. Table detection models like TableTransformer or LayoutLM to identify table regions. Image extraction routines to save images as separate files. OCR tools like Tesseract or cloud OCR APIs to extract text from images. The pipeline processes each document page by page, classifies regions by type, extracts each region, and routes it to the appropriate processing path. Maintaining the association between extracted elements and their source documents is critical for citation and reference.

The engineering firm discovered that extraction quality varied wildly by document. Clean, modern PDFs with embedded text and well-defined image boundaries were easy to process. Scanned PDFs required OCR for all text, which introduced errors. Documents with complex layouts, multi-column formats, or overlapping elements challenged the layout analysis. They implemented manual review and correction for critical documents, ensuring that key diagrams and tables were correctly extracted even if automated processing failed.

## Embedding Images

Image embedding requires vision models. Text embedding models don't work on images because they operate on token sequences, not pixels. The engineering firm used CLIP-based embeddings, which create vector representations of images that are semantically aligned with text. CLIP is trained to embed images and their text descriptions into the same vector space, so you can perform cross-modal retrieval: search for images using text queries, or find text descriptions using image queries. When an engineer asked about wiring configurations, the system could search the image index for embeddings similar to the query "wiring diagram pump controller" and retrieve the relevant schematic images.

You implement image embedding by running images through a vision encoder, typically the image encoder from a CLIP model or similar multimodal architecture. Each image becomes a vector, just like text chunks. These vectors go into an index, often a separate index from text or a multi-index system. At query time, you embed the query using the text encoder, search the image index, and retrieve images whose embeddings are nearest to the query embedding. This cross-modal retrieval is powerful: you can find visual content using text descriptions, even when the image itself contains no text or when its embedded text doesn't match the query terms.

The firm experimented with different vision models. CLIP worked well for general images and photos. For technical diagrams and schematics with specific domain vocabulary, they fine-tuned a CLIP model on their internal documentation, teaching it to associate domain-specific terms like "pressure valve" and "wiring harness" with the corresponding visual elements. This domain-adapted vision embedding significantly improved retrieval quality for technical queries.

## Understanding Tables

Table understanding is a distinct problem. Tables contain structured information: rows, columns, headers, data cells. The firm used table parsing models to extract table structure, converting visual tables into structured data. For each table, they generated multiple representations: a markdown text version that could be embedded as text, a structured JSON representation preserving row-column relationships, and a natural language description generated by an LLM summarizing what the table contains. When a question referenced table data, the system could retrieve the table in the most useful format: structured data for precise lookups, text description for semantic search.

You handle tables by first detecting them in documents, then extracting their structure. Table detection identifies table boundaries. Structure extraction identifies rows, columns, headers, and cells. Tools like Camelot, Tabula, or transformer-based table extraction models accomplish this. Once you have structured table data, you create multiple representations. The markdown or CSV version can be chunked and embedded as text. The JSON version preserves structure for programmatic access. The natural language description makes the table searchable: "This table shows maximum pressure ratings for five industrial pump models, ranging from 150 PSI to 300 PSI."

The engineering firm found that large tables created problems. A fifty-row specification table couldn't fit in a chunk. They implemented table segmentation: split large tables by rows or logical sections, creating multiple chunks each covering part of the table. Each chunk included the table headers so it was self-contained. When queries were specific enough to match a subset of the table, retrieval would return just the relevant rows rather than the entire table, keeping context manageable.

## Interpreting Charts and Graphs

Chart and graph interpretation required vision-language models. Charts contain information encoded visually: bar heights, line trends, pie slice proportions. Extracting this information requires understanding the visual encoding. The firm used multimodal LLMs to generate text descriptions of charts: "This bar chart shows pressure ratings for five pump models, ranging from 150 PSI for Model A to 300 PSI for Model E." These descriptions were then embedded and indexed as text, allowing retrieval based on chart content. Additionally, the original chart images were stored and could be presented to multimodal LLMs at generation time for direct visual interpretation.

You process charts by first extracting them as images, then using a vision-language model to describe them. You can use specialized chart-parsing tools that extract data from charts into structured formats, but these are brittle and work only for standard chart types. General vision-language models are more flexible: you show them the chart image and prompt them to describe what it shows, what variables it plots, what trends are visible, and what conclusions can be drawn. This description becomes searchable text associated with the chart image.

The firm maintained both the chart descriptions and the original chart images. When a query matched a chart description, the system would retrieve both the text description and the image. At generation time, the multimodal LLM could read the description and also visually inspect the chart image to extract precise values or details that the description might have missed. This dual representation balanced searchability with fidelity.

## Mixed-Modality Documents

Mixed-modality documents are common in technical domains: a maintenance procedure might include text instructions, a diagram showing component locations, a table of torque specifications, and photos of correct assembly. The firm chunked these documents preserving multimodal context. A chunk might include the text description, references to associated images and tables, and the actual visual content. During retrieval, they would fetch these multimodal chunks and provide both text and images to the LLM for generation.

You implement mixed-modality chunking by defining chunks that can contain multiple content types. A chunk is no longer just a text string; it's a bundle of text, image references, table data, and metadata. Your chunking strategy needs to keep related elements together: the text instructions, the diagram they reference, and the table with specifications should all be in the same chunk or adjacent chunks that get retrieved together. This requires understanding document structure and logical groupings of content.

The engineering firm built a document understanding pipeline that identified semantic sections in documents: a section about a particular component would include all the text, diagrams, tables, and photos related to that component. These sections became multimodal chunks. Retrieval would fetch entire sections, ensuring that engineers received complete, contextualized information including all relevant visual aids.

## Unified Multimodal Search

Multimodal embedding models like ImageBind and BLIP enable unified retrieval across modalities. These models embed text, images, audio, and other modalities into a shared vector space. You can search for "pump wiring diagram" and retrieve both text chunks describing wiring and image chunks showing diagrams, ranked by relevance across modalities. The engineering firm experimented with multimodal embeddings and found they improved recall: queries would retrieve both textual explanations and visual diagrams, giving the LLM richer context.

You build unified multimodal search by using a model that embeds all your content types into a single vector space. Index all your chunks, whether text, image, or mixed, in one index. At query time, embed the query and search across all content types simultaneously. The retrieved results will be a mix of text chunks, image chunks, and multimodal chunks, ranked by semantic similarity to the query. This unified retrieval is more powerful than separate text and image indexes because it naturally balances and integrates different content types.

The firm found that unified search sometimes retrieved images when text would be more appropriate, or vice versa. They implemented content-type boosting based on query classification. If a query was about procedures or explanations, text chunks were boosted. If a query was about diagrams, configurations, or visual identification, image chunks were boosted. This heuristic improved relevance by aligning content types with query intent.

## Vision-Language Models for Generation

Vision-language models at generation time are crucial. Even with perfect retrieval of images and tables, the LLM needs to be able to interpret that visual content when generating answers. This requires multimodal models like GPT-4 Vision, Claude with vision capabilities, or Gemini that can accept both text and images as input. The engineering firm's workflow became: retrieve relevant text chunks and images, pass both to a vision-capable LLM, let the model read the text and visually interpret the images to generate the answer. When asked about the pump wiring configuration, the system retrieved the text chunk referencing Figure 3.7 and the actual Figure 3.7 image, then passed both to the model. The model could read the diagram and describe the wiring configuration accurately.

You implement vision-language generation by selecting a model with vision capabilities and formatting your prompt to include both text and images. The prompt structure typically includes text context chunks followed by images, with instructions for the model to use both sources to answer the query. The model processes the text and visually analyzes the images, integrating information from both modalities to generate a comprehensive answer. This is far more powerful than text-only generation because the model can directly see diagrams, photos, charts, and tables rather than relying on text descriptions of them.

The firm discovered that prompt engineering for multimodal generation required new patterns. They had to instruct the model explicitly to examine the images: "Refer to the provided wiring diagram and describe the configuration." Without this prompt guidance, the model would sometimes ignore images or rely solely on text. They also found that image quality and resolution mattered: low-resolution or blurry diagrams were hard for models to interpret. They implemented image preprocessing to enhance contrast, increase resolution, and clean up scanned diagrams.

## Embedding Images vs Describing Them

When to embed images versus text descriptions is a practical trade-off. Embedding actual images using vision models is expensive and produces high-dimensional embeddings. Generating text descriptions of images using a vision-language model and embedding those descriptions is cheaper and works with standard text embeddings. The engineering firm used a hybrid approach: for critical diagrams and schematics, they embedded the images directly using CLIP. For photos and less critical visuals, they generated text descriptions and embedded those. They also stored the original images regardless, so even if retrieval was based on text descriptions, the actual images could be provided to the model at generation time.

You decide this trade-off based on your content, budget, and performance requirements. Image embedding enables true visual similarity search: you can find diagrams that look similar even if their text descriptions differ. Text description embedding is cheaper and works with your existing text pipeline but relies on the quality of the descriptions. A hybrid approach uses image embedding for high-value visual content where visual similarity matters, and text descriptions for images where conceptual similarity is sufficient. Always preserve the original images for generation time use, regardless of how you indexed them.

The firm implemented a classification system that tagged images by importance and content type. Wiring diagrams, schematics, and safety-critical visuals were tagged for image embedding. Generic photos, decorative images, and redundant visuals were tagged for description-only embedding. This selective approach balanced cost and quality, investing in image embedding where it mattered most.

## OCR for Text in Images

OCR for text within images adds another layer. Many technical diagrams contain labels, callouts, and annotations. The firm ran OCR on all images to extract embedded text, which was then included in the image's text description. A wiring diagram might have labels like "L1: Line 1," "L2: Line 2," "GND: Ground." OCR would extract these labels, and the image description would include them: "Wiring diagram showing three terminals labeled L1, L2, and GND." This improved retrieval because queries mentioning specific labels could now match images containing those labels.

You implement OCR by running an OCR tool over each image, extracting any text present, and associating that text with the image. OCR text can be included in the image's description, added as metadata for search, or stored separately but linked to the image. OCR quality varies with image quality, font size, and text orientation. Diagrams with clear, horizontal text produce good OCR results. Handwritten annotations, small fonts, or rotated text produce poor results. You can improve OCR quality with image preprocessing: binarization, deskewing, contrast enhancement.

The engineering firm found OCR particularly valuable for part numbers and model identifiers in diagrams. Engineers would search for specific part numbers, and OCR-extracted text would allow retrieval of diagrams containing those part numbers even if the surrounding text didn't mention them. They implemented specialized OCR pipelines for high-value documents, using commercial OCR APIs with better accuracy than open-source tools for critical diagrams.

## Table Segmentation and Indexing

Table retrieval has unique patterns. Sometimes you want the whole table, sometimes you want specific rows or columns. The firm implemented table indexing where large tables were split into sub-tables or individual rows, each indexed separately. A query about a specific pump model would retrieve just the rows for that model from larger specification tables, rather than retrieving massive tables in their entirety. This targeted retrieval kept context manageable while maintaining precision.

You segment tables by identifying logical divisions: individual rows, groups of rows by category, or specific columns. Each segment becomes a chunk with enough context to be self-contained. For row-based segmentation, include the table headers with each row so the row can be interpreted independently. For column-based segmentation, include row identifiers. Metadata can help: tag each table chunk with the full table name, source document, and what subset it represents. This enables precise retrieval while preserving the user's ability to request the full table if needed.

The firm also implemented table querying where natural language questions were translated to structured queries over table data. "What's the max pressure for Model C?" would be parsed to identify that the user wants a specific cell from a table: the intersection of the Model C row and the Max Pressure column. The system would retrieve the relevant table segment and extract the specific value. This structured approach was more reliable than hoping semantic search would retrieve the exact right table chunk.

## Cross-Modal Ranking

Cross-modal ranking is a challenge. When you retrieve both text and images, how do you rank them? Is a highly relevant image more valuable than moderately relevant text? The firm experimented with ranking strategies and settled on a content-type aware approach: if the query was about visual information, diagrams, or configurations, images were boosted in ranking. If the query was about procedures or explanations, text was boosted. This heuristic improved user satisfaction because engineers got the content type they actually needed.

You implement cross-modal ranking by classifying queries by information need. Does the user need visual information or textual information? For visual needs, prioritize images and diagrams. For textual needs, prioritize text chunks. You can also use learned ranking where you train a model to score text and image pairs for relevance, learning the appropriate balance from labeled data. The simplest approach is separate relevance thresholds: only include images above a certain relevance score, only include text above a certain score, allowing both types when both are highly relevant.

The engineering firm found that engineers often needed both text and images, so they implemented a balanced retrieval strategy: always retrieve the top text chunks and the top image chunks, even if one modality scored lower. This ensured answers included both explanatory text and visual references. Users appreciated having both instruction and illustration rather than one or the other.

## Infrastructure and Costs

Multimodal RAG infrastructure is heavier than text-only. You need storage for images and other media, embedding models for multiple modalities, potentially separate indexes for different content types, and multimodal LLMs for generation. The engineering firm's infrastructure costs increased by a factor of four when they added multimodal capabilities. Image embeddings required GPU inference for the vision models, image storage consumed significant space, and vision-capable LLM calls were more expensive than text-only calls. But the value was undeniable: engineers could finally get complete answers including the diagrams and tables they needed.

You build multimodal infrastructure by scaling up storage, compute, and model serving. Object storage for images and media. GPU-enabled embedding services for vision models. Vector databases that can handle multiple embedding types or multiple indexes. Multimodal LLM API access with sufficient rate limits for your query volume. Image serving infrastructure to display retrieved images in your UI. Monitoring and logging that tracks costs by modality so you can understand where your spending goes.

The firm implemented cost controls: caching vision model outputs to avoid re-embedding the same images, compressing images to reduce storage, using efficient vision models with lower inference costs, and rate limiting expensive multimodal generation calls. They also implemented tiered access: basic users got text-only RAG, premium users got full multimodal RAG. This aligned costs with value.

## Maintaining Document Structure

Document structure preservation is important for reference. When you retrieve an image, you need to know where it came from: which document, which page, what section. The firm maintained metadata linking every image and table to its source document and page number. Generated answers would include citations like "Based on Figure 3.7 from Pump Controller Manual, page 47" along with displaying the figure itself. This allowed engineers to verify the information and find related content in the source documents.

You implement structure preservation through metadata. Every chunk, whether text, image, or table, has metadata including source document, page number, section heading, and figure or table number. When you retrieve a chunk, this metadata comes with it. Your generation system uses this metadata to create citations. Your UI uses it to provide links back to source documents. This traceability is critical in domains where users need to verify information or dive deeper into source materials.

The engineering firm built a document viewer integration where engineers could click on a citation and see the full source document page, with the retrieved content highlighted. This seamless navigation from generated answer to source material built trust in the system and supported workflows where users needed to consult the original documentation.

## Updating Multimodal Content

Handling updates to multimodal content is complex. When a PDF manual was updated, the firm had to re-extract text, images, and tables, re-embed everything, and update all indexes. They implemented change detection: hash each page of each PDF, and only re-process pages that changed. This incremental update strategy reduced the cost of keeping the multimodal index current.

You handle updates by tracking document versions and content hashes. When a document changes, identify which pages changed, re-extract content from those pages, re-embed, and update indexes. Delete chunks associated with removed pages. Update metadata for renumbered pages or sections. This incremental approach is much cheaper than full re-indexing on every update, especially for large documentation sets where most content remains stable across versions.

The firm also implemented versioning where multiple versions of a document could coexist in the system. Users could specify which version to search, or search across all versions. This was important for engineers working with equipment of different generations, where maintenance procedures varied by equipment version.

## Visual Question Answering

One interesting pattern is using images as queries. An engineer might take a photo of a piece of equipment and ask, "What is this component?" The system would embed the photo, search the image index for similar diagrams or photos, retrieve matches, and use a vision model to compare the query photo with retrieved images to identify the component. This reverse image search combined with multimodal reasoning enabled visual question answering where the input itself was an image.

You implement visual queries by allowing users to upload images as part of their query. Embed the query image using the same vision model used for document images. Search the image index for similar images. Retrieve matches and use a vision-language model to compare the query image with retrieved images, identifying similarities, differences, and potential matches. This enables use cases like equipment identification, defect comparison, and visual troubleshooting.

The engineering firm built a mobile app where engineers in the field could photograph equipment and get instant identification and documentation. The app would upload the photo, the multimodal RAG system would identify the equipment from visual similarity, and return the relevant manual sections and diagrams. This visual search dramatically reduced time spent identifying unfamiliar equipment.

## Extracting Data from Charts

Chart data extraction enables quantitative queries. Instead of just describing a chart as "shows pressure ratings," the firm extracted the actual data points from charts using vision models or specialized chart parsing tools. This data was stored as structured numbers, enabling queries like "What's the maximum pressure rating shown in the specs?" that required numerical comparison rather than just semantic retrieval.

You extract chart data using specialized tools like PlotDigitizer or vision-language models prompted to read charts and output structured data. For a bar chart, extract the bar labels and heights. For a line chart, extract the data points. Store this extracted data as structured table data alongside the chart image. This enables both visual and quantitative access to chart information: users can see the chart and the system can reason numerically over the extracted data.

The firm found chart data extraction unreliable for complex or low-quality charts. They used it selectively for clean charts with clear labels and scales. For complex charts, they relied on vision-language models to interpret the chart at generation time rather than trying to extract data upfront.

## Domain-Specific Multimodal RAG

Multimodal RAG is particularly valuable in several domains. Technical documentation, as the engineering firm discovered, is full of diagrams, schematics, and tables. Medical literature includes anatomical diagrams, medical imaging, and result tables. Financial reports contain charts, graphs, and financial tables. Scientific papers include figures, plots, and data visualizations. E-commerce needs product images alongside text descriptions. Any domain where visual content carries information that text alone cannot convey needs multimodal RAG.

Each domain has specific requirements. Medical imaging requires specialized models trained on medical image types. E-commerce requires product image search with visual similarity. Scientific literature requires understanding plots, graphs, and mathematical notation in images. You adapt your multimodal RAG to your domain by selecting appropriate vision models, tuning retrieval strategies to prioritize relevant content types, and building domain-specific extraction pipelines for specialized visual content.

The engineering firm's system became a template for other technical documentation RAG systems. The patterns they developed, extracting diagrams, embedding them with vision models, retrieving mixed text and image content, generating with vision-language models, transferred to other domains with visual knowledge bases. The core insight was universal: if your knowledge is multimodal, your RAG must be too.

## User Interface Considerations

User interface considerations matter. When your RAG system retrieves images and tables, you need to display them. The engineering firm built a UI that showed text answers with inline images and tables. Engineers could click images to zoom, download tables as spreadsheets, and navigate to source documents. The presentation layer is crucial for multimodal RAG because the value comes from showing users the actual visual content, not just telling them about it.

You design multimodal UIs by integrating visual content into answer presentation. Show retrieved images inline with generated text. Render tables in readable formats, not as raw text dumps. Provide image zoom, download, and sharing. Include citations linking visuals to source documents. The UI should make visual content a natural part of the answer, not an afterthought or attachment.

The firm's UI included side-by-side view where generated text appeared on one side and relevant diagrams on the other, allowing engineers to read instructions while viewing referenced diagrams. This spatial layout mimicked physical manuals and felt natural to users.

## Evaluating Multimodal RAG

Testing and evaluation of multimodal RAG is harder than text-only. How do you measure whether the right image was retrieved? The firm built an evaluation set where engineers labeled which images and tables were relevant for specific questions. They measured retrieval recall for images and tables separately from text, and they had engineers rate whether the visual content in the final answers was helpful. They found that including relevant diagrams increased answer satisfaction scores by fifty-three percent even when the text content was identical.

You evaluate multimodal RAG with labeled test sets covering different content types. For each test query, label which text chunks, images, and tables should be retrieved. Measure recall and precision for each modality. Evaluate whether the generated answers correctly interpret and reference visual content. Measure user satisfaction comparing multimodal answers versus text-only answers for the same questions. These multimodal evaluations validate that your visual retrieval and reasoning are adding value, not just complexity.

The firm also ran user studies where engineers compared the multimodal RAG system to the old text-only system. Engineers overwhelmingly preferred the multimodal system, citing time savings, reduced errors, and better understanding from having access to diagrams and tables. This user validation justified the additional infrastructure investment.

## The Multimodal Future

The engineering firm's multimodal RAG system became essential for their technical support team. Response times for equipment questions dropped by forty percent because engineers no longer had to manually hunt through PDFs for specific diagrams. Accuracy improved because answers included the actual schematics and specification tables. Training new engineers became faster because they could ask questions and receive both explanations and visual references. The system went from "useless" to "indispensable" once it could handle the visual content that technical documentation depends on.

Building multimodal RAG requires accepting significant added complexity. You need vision models, multimodal embeddings, content extraction pipelines for different media types, vision-capable LLMs, and infrastructure to store and serve images and other media. You need to handle different content types differently while maintaining a unified retrieval and generation experience. The payoff is a RAG system that can answer questions requiring visual information, not just text. For domains where diagrams, tables, charts, and images are integral to the knowledge base, multimodal RAG is the only architecture that delivers complete, useful answers. The engineering firm's lesson was clear: if your documents are multimodal, your RAG system must be too. Text-only RAG in a visual domain isn't just limited; it's fundamentally broken. Your knowledge lives in images and tables as much as text. Your retrieval needs to access all of it.
