# 8.7 â€” Document Update Propagation: Keeping Answers Current

What happens when you update a document in your CMS but forget to update the vector index? Your RAG system confidently serves outdated information while users assume they are getting current answers. The system appears operational, latency is normal, and nothing breaks, but the truth does not flow through it. Document update propagation is not automatic. It requires deliberate architecture, testing, and monitoring.

The failure was not dramatic. No servers crashed. No error rates spiked. No alerts fired. The system ran perfectly, from an infrastructure perspective. But the information it served was wrong, and that wrongness was silent, invisible, and catastrophic. The researcher who submitted the protocol trusted the system. Why would they not? It had answered hundreds of questions correctly before. The system appeared authoritative, confident, and fast. There was no indication that the answer was weeks out of date. No warning label, no staleness timestamp, no caveat. Just a clean, confident answer based on obsolete information. That is the insidious nature of propagation failures: the system works, but the truth does not flow through it.

You need to understand that document updates create consistency problems in RAG systems. A RAG system has at least two data stores: a document store, which holds the raw documents or chunks, and a vector store, which holds the embeddings. When a document changes, both stores must be updated. If only one store is updated, the system is inconsistent: queries may retrieve outdated chunks, or embeddings may point to documents that no longer exist. Consistency is hard to achieve in distributed systems, and RAG systems are inherently distributed. You must design update propagation carefully, or you will serve stale, incorrect, or dangerous information to users.

The challenge is deeper than it appears. In a traditional database, updates are atomic: you issue an UPDATE statement, the row changes, and all future reads see the new value. In a RAG system, updates are multi-stage: update the source document, detect the change, re-chunk the document, re-embed the chunks, update the vector index, and verify consistency. Each stage can fail independently. Each stage introduces latency. Each stage requires orchestration. The distributed nature of RAG systems means that consistency is not guaranteed by the storage layer. You must build consistency into your propagation pipeline, explicitly and deliberately.

## The Hidden Complexity of Change Detection

Update detection is the first challenge. How do you know when a document has changed? If your documents are stored in a content management system, database, or file system with versioning, the storage layer can notify you of changes through webhooks, change feeds, or polling. If documents are uploaded manually, you rely on timestamps or version numbers to detect changes. If documents are fetched from external sources, you must periodically refetch and compare to detect updates. Without reliable update detection, you cannot propagate changes.

The failure modes are subtle. If you poll for changes every hour, documents updated between polls are invisible for up to an hour. If you rely on webhooks and the webhook service is down, changes are silently missed. If you use file modification timestamps and the file system clock skews, you may miss updates or double-process unchanged files. If you compare document hashes to detect changes and the hash function is weak, you may miss minor edits or falsely detect changes in binary-identical files. Each detection mechanism has trade-offs: latency, reliability, cost, and complexity.

Event-driven update detection is the most robust approach. When a document is created, updated, or deleted in the source system, the system emits an event to a message queue or event stream. Your RAG indexing pipeline subscribes to these events and processes them in real time. Event-driven propagation minimizes latency: changes are reflected in the RAG system within seconds or minutes of the source update. Event-driven systems require infrastructure: message queues such as Kafka, RabbitMQ, or AWS SQS, and event consumers that process messages reliably.

But event-driven systems introduce their own failure modes. What happens if the event consumer crashes while processing an event? If you do not acknowledge the event, it is reprocessed, potentially leading to duplicate updates. If you acknowledge too early, before completing the update, a crash loses the event and the update is never applied. You need idempotent processing: ensure that processing the same event twice produces the same result as processing it once. You need retry logic: if embedding or indexing fails, retry with exponential backoff. You need dead-letter queues: if an event fails repeatedly, move it to a separate queue for manual investigation.

Polling-based update detection is simpler but introduces latency. Your indexing pipeline periodically scans the document store for changes, comparing timestamps or version numbers to identify new or updated documents. Polling intervals range from minutes to hours, depending on how current your answers need to be. Polling is easier to implement than event-driven systems, but it introduces propagation latency equal to the polling interval. If you poll every hour and a document updates at 10:01 AM, the update is not reflected until 11:01 AM or later.

Polling also scales poorly. If you have 100,000 documents and poll every 10 minutes, you scan 100,000 document metadata records every 10 minutes. Most scans find zero changes. The cost of polling scales with corpus size, not update frequency. For large corpora with infrequent updates, polling is wasteful. For small corpora with frequent updates, polling is acceptable. The crossover point depends on your infrastructure costs and latency requirements.

## The Computational Burden of Re-Chunking and Re-Embedding

Re-chunking is required when a document is updated. If you originally chunked a document into 20 chunks of 500 tokens each, and the document is updated, you must re-chunk it. The updated document may have different content, structure, or length, producing a different number of chunks with different boundaries. Re-chunking is not as simple as replacing the old chunks with new ones: you must handle cases where chunks are added, removed, or reordered. If the update adds a new section to the document, you add new chunks. If it removes a section, you delete chunks. If it reorders sections, you reorder chunks.

Re-chunking introduces complexity around chunk identity. How do you know which new chunks correspond to which old chunks? If a document originally had 20 chunks and now has 22, which 2 chunks are new? If a chunk's text changes slightly, is it the same chunk with updated content, or a different chunk? Chunk identity matters for tracking provenance, maintaining references, and minimizing unnecessary re-embedding. Some systems use content-based chunk IDs: hash the chunk text and use the hash as the ID. If the text changes, the ID changes, and the chunk is treated as new. Other systems use position-based chunk IDs: the third chunk is always chunk-3, regardless of content. Each approach has trade-offs.

Re-embedding is the most expensive part of update propagation. After re-chunking, you must generate new embeddings for each chunk. If a document produces 20 chunks and each chunk requires an embedding API call, updating the document requires 20 API calls. If you update 1,000 documents per day, you make 20,000 embedding API calls per day. Embedding costs and throughput limits become a bottleneck. You must batch embedding requests, parallelize processing, and budget for ongoing embedding costs. Some teams optimize by only re-embedding chunks that changed, comparing the old and new chunk text and skipping re-embedding if they are identical.

The cost calculation is brutal. If each embedding call costs 0.0001 dollars and you update 1,000 documents per day with an average of 20 chunks per document, you spend 2 dollars per day on embeddings for updates alone. That is 730 dollars per year. Scale to 10,000 documents per day and you spend 7,300 dollars per year. Add initial indexing costs, query-time embedding costs, and re-indexing costs, and embedding becomes a significant line item in your infrastructure budget. You must forecast embedding costs based on update frequency, corpus size, and chunk size.

## Latency, Consistency, and the Propagation Gap

Propagation latency is the time between a document update and the updated answer appearing in query results. Propagation latency has four components: update detection latency, re-chunking latency, re-embedding latency, and index update latency. If you poll for updates every 30 minutes, re-chunk and re-embed in 5 minutes, and update the index in 2 minutes, total propagation latency is 37 minutes. For event-driven systems, detection latency is seconds, but re-chunking and re-embedding still take minutes. Real-time propagation, where updates are reflected in seconds, requires aggressive optimization: streaming re-chunking, pre-computed embeddings, and in-memory index updates.

Propagation latency defines the window of vulnerability. For 37 minutes after a document is updated, users may receive outdated answers. In high-stakes domains such as healthcare, legal, or finance, 37 minutes of stale data is unacceptable. In documentation or FAQ systems, 37 minutes may be tolerable. You must define your latency requirements based on domain risk and user expectations. If your SLA guarantees propagation within 10 minutes, you must architect your system to meet that guarantee, not hope that it usually does.

Consistency between document store and vector index is critical. If a document is updated in the document store but the vector index still contains embeddings of the old version, queries retrieve old chunks. The RAG system generates answers based on outdated information, misleading users. Ensuring consistency requires transactional semantics: either the document store and vector index are both updated, or neither is. In practice, distributed transactions are complex and slow. Most teams use eventual consistency: the document store is updated first, then the vector index is updated asynchronously. For a brief period, the system is inconsistent, but it converges to consistency.

Eventual consistency is a euphemism for temporary wrongness. During the propagation window, the system is serving incorrect answers. Users do not know this. The system does not warn them. If a user queries during the window, they receive stale data and make decisions based on it. The pharmaceutical researcher queried during the window and submitted a protocol based on outdated dosage information. The system was eventually consistent, but the damage was done during the inconsistency window.

You can reduce the inconsistency window by optimizing propagation latency, but you cannot eliminate it entirely without synchronous updates, which introduce unacceptable latency for queries. The trade-off is fundamental: fast queries with eventual consistency, or slow queries with strong consistency. Most production systems choose fast queries and eventual consistency, accepting the risk of serving stale data for brief periods.

## Handling Deletions, Versions, and Partial Updates

Handling deleted documents is a special case of update propagation. When a document is deleted, you must remove its chunks and embeddings from the RAG system. If you fail to remove embeddings, users may retrieve deleted chunks, and the RAG system may cite documents that no longer exist. Deletion propagation requires identifying all chunks and embeddings associated with the document, marking them for deletion, and running compaction to reclaim storage. Some systems use soft deletes: instead of removing embeddings, they mark them as deleted and filter them from search results. Soft deletes simplify rollback but consume storage.

Deletion failures are insidious. If a document is deleted from the source but its embeddings remain in the vector index, users retrieve the deleted document. The system cites a document that no longer exists, providing a reference that cannot be verified. In legal or compliance contexts, citing deleted documents is a serious error. Deletion propagation must be as reliable as update propagation, but it is often treated as an afterthought.

Versioned documents introduce additional complexity. If your document store maintains multiple versions of a document, your RAG system must decide which version to index. Most systems index only the latest version, but some use cases require indexing multiple versions: legal archives, historical research, or regulatory compliance. Multi-version indexing increases storage costs and complicates retrieval: queries must specify which version to search, or the system must merge results across versions. Multi-version indexing is rare but essential for certain domains.

Version management requires careful design. If you index multiple versions of the same document, how do you distinguish them in retrieval results? Do you show the user that multiple versions exist? Do you rank versions by recency or relevance? If a user queries for information that appears in both old and new versions with conflicting content, which version does the system cite? Version conflicts are common in evolving documentation, regulatory filings, and policy documents. Your system must handle them gracefully, not silently choose one version and ignore the other.

Partial updates are more efficient than full re-indexing. If a document changes minimally, such as fixing a typo or updating a single sentence, re-chunking and re-embedding the entire document is wasteful. Partial updates identify which chunks changed, re-embed only those chunks, and update only the affected embeddings. Partial updates require chunk-level change detection: compare old and new chunks, compute a diff, and determine which chunks are new, modified, or deleted. Partial updates reduce embedding costs and propagation latency but increase implementation complexity.

The efficiency gain can be substantial. If a 50-chunk document has 1 chunk updated, partial updates require re-embedding 1 chunk instead of 50, reducing cost by 98 percent and latency by a similar factor. But partial updates introduce complexity: you must store old chunks, compare old and new chunks, and maintain a mapping between chunk IDs and document positions. For frequently updated documents, partial updates are essential. For infrequently updated documents, the complexity may outweigh the savings.

## Monitoring, Testing, and Guaranteeing Freshness

Freshness guarantees define how current your answers are. If your SLA guarantees that answers reflect document updates within 10 minutes, you must design propagation to meet that guarantee. Freshness guarantees depend on propagation latency: you must measure end-to-end latency from document update to index update and ensure it stays within bounds. Freshness is a trade-off: faster propagation requires more resources, more complexity, and higher costs. You must balance freshness requirements with budget and operational constraints.

Freshness is a product requirement, not an engineering preference. Talk to your users. Ask them how current the information needs to be. For a customer support knowledge base, 10-minute propagation may be acceptable. For a real-time news aggregation system, 10-minute propagation is too slow. For a legal research tool, even 1-minute propagation may be risky if users make time-sensitive decisions. Freshness requirements should be specified in your SLA, measured in production, and reported to stakeholders.

Monitoring update propagation is essential for detecting lag and failures. Track the age of indexed content: for each chunk in the vector database, store the timestamp of the source document's last update. When a query retrieves chunks, log the maximum age of retrieved chunks. If users frequently receive chunks that are hours or days old, propagation is lagging. Alert on propagation lag: if the vector index is more than 30 minutes behind the document store, fire an alert. Investigate the bottleneck: is re-embedding slow, is the index update queue backed up, or is update detection failing?

Propagation lag is often invisible until it causes an incident. The pharmaceutical company had no monitoring for lag. They assumed updates were propagating correctly because no errors were logged. In reality, the vector index was weeks behind the document store, but the system ran silently, serving stale data without complaint. Monitoring would have detected the lag within hours and prevented the incident.

Rollback and recovery mechanisms handle propagation failures. If a document update is propagated incorrectly, corrupting the index or introducing errors, you must roll back to the previous state. Rollback requires versioning: maintain snapshots of the document store and vector index at regular intervals, such as hourly or daily. If a propagation error is detected, roll back both stores to the last consistent snapshot. Rollback is disruptive: it discards recent updates and may cause temporary inconsistencies. But it is preferable to serving incorrect information indefinitely.

Rollback scenarios are rare but critical. If a bug in your chunking logic produces malformed chunks, and those chunks are embedded and indexed, your index is corrupted. Queries return nonsensical results. Users lose trust. Rolling back to the last known-good snapshot restores service, but you lose updates processed after the snapshot. The trade-off is acceptable: temporary data loss is better than permanent incorrectness.

Testing update propagation end-to-end prevents production surprises. In staging, simulate document updates: add a new document, update an existing document, and delete a document. Measure propagation latency at each stage. Query the RAG system after propagation and verify that answers reflect the updated content. Automated tests should run daily, validating that propagation works correctly and meets latency SLAs. Propagation failures are often silent: updates are not applied, but the system does not error. Automated tests detect these silent failures.

The pharmaceutical company rebuilt their update propagation pipeline after the incident. They implemented event-driven update detection, subscribing to change feeds from the document store. They built a re-chunking and re-embedding service that processed updates within 5 minutes. They added consistency checks: after updating the vector index, they queried for updated chunks and verified they matched the latest document version. They monitored propagation lag, alerting if it exceeded 10 minutes. They tested propagation end-to-end daily, simulating document updates and verifying query results. Six months later, a critical regulatory filing was updated, and the RAG system reflected the changes within 6 minutes. Scientists queried the system the next day, received current information, and the trial proceeded on schedule. The new VP of Engineering asked how they ensured such consistency. The engineer replied: "We stopped treating updates as an afterthought."

## Building Propagation Discipline Into Your System

You design update propagation now, before you deploy to production, before users depend on your answers. You implement event-driven update detection or polling with acceptable latency. You build re-chunking and re-embedding pipelines that process updates efficiently. You ensure consistency between document store and vector index. You handle deletions cleanly. You monitor propagation lag and alert on delays. You test propagation end-to-end and validate that answers reflect the latest content.

You define freshness requirements with your stakeholders. You measure propagation latency in production and report it in dashboards. You budget for embedding costs based on update frequency and corpus size. You implement rollback mechanisms for propagation failures. You run automated tests daily to catch silent propagation failures.

You recognize that update propagation is not a feature you add later. It is a core architectural decision that affects reliability, cost, and user trust. If you treat it as an afterthought, you will serve stale data. If you design it carefully, you will serve current, trustworthy information.

Documents change. Answers must change too. If your RAG system serves stale information, it is worse than useless: it is dangerous. You build propagation discipline now, before the incident, before the audit, before the trust is lost.
