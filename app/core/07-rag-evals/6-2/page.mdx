# 6.2 â€” Retrieval Metrics: Precision, Recall, MRR, and NDCG

**Precision measures whether retrieved documents are relevant. Recall measures whether all relevant documents are retrieved.** A legal research platform optimized for precision and speed, doubling query throughput. Three months later, lawyers reported missing critical precedents. Investigation revealed recall had dropped from eighty-five percent to sixty-one percent. The system was fast and precise but incomplete. By the time the team discovered the problem, four major clients were evaluating competitors. You cannot optimize what you do not measure, and measuring only precision while ignoring recall is optimization malpractice. Every retrieval metric captures a different dimension of quality, and every dimension matters.

You need to measure whether your retrieval system finds the right documents. This sounds simple, but "right" has multiple dimensions. Precision measures whether the documents you return are relevant. Recall measures whether you find all the relevant documents that exist. Mean reciprocal rank measures how quickly users find what they need. Normalized discounted cumulative gain measures whether you rank relevant documents appropriately. Each metric captures a different aspect of retrieval quality, and each matters for different use cases. Optimizing for one metric often degrades others. Understanding when each metric matters and how to compute them correctly is essential for building effective RAG systems.

## Precision at K: Are Retrieved Documents Relevant

Precision at k measures the fraction of retrieved documents that are actually relevant. If your system returns ten documents and seven are relevant, precision at ten is 0.7. This metric answers a simple question: when users look at your results, how much noise do they have to filter through? High precision means most results are useful. Low precision means users waste time sorting through junk. Precision is the most intuitive retrieval metric and often the first one teams implement.

Computing precision requires ground truth relevance labels. For each query in your test set, you need to know which documents are relevant and which are not. These labels usually come from human annotators who read the query, examine candidate documents, and mark each as relevant or irrelevant. Some datasets use graded relevance, where documents are rated on a scale from highly relevant to not relevant at all. For binary precision, you typically threshold these grades, treating highly and moderately relevant as positive and slightly relevant or irrelevant as negative.

Once you have labels, computing precision is straightforward. For a query, you run your retrieval system and get back the top k documents. You check how many of those k documents are marked as relevant in your ground truth. Divide that count by k, and you have precision at k. Suppose you retrieve ten documents and your ground truth labels indicate that six are relevant. Precision at ten is six divided by ten, which equals 0.6. The metric is clean, easy to explain, and directly interpretable.

Precision trades off with recall and ranking metrics. You can achieve perfect precision by being extremely conservative, returning only documents you are absolutely certain are relevant. But this strategy hurts recall because you will miss many relevant documents. A system that returns one document per query might have 100% precision if it always picks correctly, but it provides minimal value if dozens of other relevant documents exist. Precision tells you about the quality of what you return but says nothing about what you miss. This is why precision alone is insufficient for evaluating retrieval systems.

Different applications prioritize precision differently. In applications where users examine every retrieved document, such as legal research or medical literature review, low precision is costly. Users spend time reading irrelevant papers or cases, which wastes expert hours and increases frustration. In applications where users skim results or where a generation model consumes the context automatically, moderate precision is acceptable as long as some relevant documents appear. The generation model or the user can filter out noise. Know your use case and set precision targets accordingly.

## Recall at K: Did You Find All Relevant Documents

Recall at k measures the fraction of all relevant documents that appear in your top k results. If twenty relevant documents exist for a query and your system retrieves fifteen of them in the top fifty results, recall at fifty is 0.75. This metric answers: are you missing important information? High recall means you capture most of what matters. Low recall means critical documents slip through the cracks. Recall is essential when completeness matters more than convenience.

Computing recall requires knowing the total number of relevant documents for each query. This is harder than it sounds. In a large document collection, enumerating every relevant document for every query is expensive. You would need annotators to review the entire corpus for each query, which is impractical for collections with millions of documents. Instead, most datasets use pooling methods. You run multiple retrieval systems, pool their top results, and have annotators label those pooled documents. This approach captures most relevant documents but may miss some, making recall estimates conservative.

With ground truth relevance labels, computing recall is straightforward. For a query, retrieve the top k documents. Count how many are marked as relevant. Divide by the total number of relevant documents in your ground truth. Suppose your ground truth indicates twelve relevant documents exist for a query. Your system retrieves ten documents, and eight are relevant. Recall at ten is eight divided by twelve, which equals 0.67. You found two-thirds of the relevant documents, and one-third is missing.

Recall trades off with precision and computational cost. You can achieve high recall by returning many documents. If you return a thousand documents per query, you will probably capture most relevant ones, but precision will be terrible. Users or downstream models must wade through enormous amounts of noise. Additionally, retrieving and processing many documents increases latency and cost. A practical retrieval system must balance recall and precision, returning enough documents to capture key information without overwhelming users or downstream components.

Different applications prioritize recall differently. In healthcare, missing a relevant clinical guideline could lead to misdiagnosis or incorrect treatment. High recall is critical. In customer support, missing one relevant knowledge base article might be acceptable if other retrieved articles answer the question. Moderate recall suffices. In exploratory search, where users are browsing and discovering, high recall helps uncover unexpected connections. In focused search, where users need a specific answer, moderate recall with high precision is better. Understand your application's tolerance for missing information and set recall targets accordingly.

## Mean Reciprocal Rank: How Quickly Do Users Find Relevance

Mean reciprocal rank, or MRR, measures how highly your system ranks the first relevant document. If the first relevant document appears at position three, the reciprocal rank is one divided by three, which equals 0.33. MRR averages reciprocal ranks across all queries. This metric answers: do users find what they need quickly, or do they have to scroll? MRR is especially important for applications where users scan results sequentially and stop once they find something useful.

Computing MRR requires identifying the rank of the first relevant document for each query. Retrieve your top k documents, check them in order from first to last, and note the position of the first relevant one. If the first relevant document is at position one, the reciprocal rank is one. If it is at position five, the reciprocal rank is 0.2. If no relevant documents appear in your top k results, the reciprocal rank is zero. Average these reciprocal ranks across all queries in your test set to get MRR.

Suppose you evaluate on three queries. For query one, the first relevant document is at position two, giving a reciprocal rank of 0.5. For query two, it is at position one, giving a reciprocal rank of one. For query three, no relevant documents appear in the top ten, giving a reciprocal rank of zero. The mean reciprocal rank is the average of 0.5, one, and zero, which equals 0.5. This score indicates that on average, the first relevant document appears around position two, but performance varies significantly across queries.

MRR is sensitive to top-ranked results and ignores everything else. If the first relevant document is at position one but positions two through ten are all irrelevant, MRR is perfect even though precision is only 10%. If ten relevant documents exist but all are ranked after the first irrelevant document, MRR treats this the same as having only one relevant document at that position. MRR is useful when you care primarily about the first hit, but it does not capture overall result quality or the ranking of other relevant documents.

MRR is particularly relevant for question-answering systems where users expect a single answer. If your RAG system retrieves context for generating one answer, MRR tells you whether the best context is ranked highly enough to influence generation. If the top-ranked document is irrelevant and the relevant document appears at position eight, the generation model might focus on the wrong context. MRR drops serve as an early warning that ranking quality is degrading, even if precision and recall remain stable.

## Normalized Discounted Cumulative Gain: Are Relevant Documents Ranked Well

Normalized discounted cumulative gain, or NDCG, measures whether relevant documents are ranked appropriately across all positions. Unlike MRR, which only considers the first relevant document, NDCG evaluates the entire ranking. It rewards systems that place highly relevant documents at the top and less relevant documents further down. NDCG is the most sophisticated retrieval metric and the best choice when ranking quality matters across multiple positions.

NDCG starts with the concept of cumulative gain, which sums the relevance scores of retrieved documents. If you retrieve three documents with relevance scores of three, two, and one, cumulative gain is six. Discounted cumulative gain applies a discount factor based on position, reflecting the reality that users pay more attention to top results. The discount is typically logarithmic, meaning position two is not twice as bad as position one but much less bad than position ten. Discounted cumulative gain is the sum of relevance divided by the log of position plus one.

To compute NDCG, you first compute discounted cumulative gain for your retrieval results, then normalize by the ideal DCG. Ideal DCG is what you would get if you ranked all documents perfectly, placing the most relevant at the top. NDCG is the ratio of your DCG to ideal DCG, scaled to a value between zero and one. An NDCG of one means perfect ranking. An NDCG of 0.7 means your ranking is 70% as good as the ideal ranking. NDCG is position-sensitive, relevance-graded, and normalized, making it ideal for comparing systems across different queries and datasets.

Computing NDCG requires graded relevance labels, not just binary relevant or irrelevant. Documents are typically scored on a scale such as zero for irrelevant, one for marginally relevant, two for relevant, and three for highly relevant. These grades provide the signal NDCG needs to distinguish between good rankings and great rankings. With binary labels, NDCG reduces to a simpler metric that only distinguishes whether relevant documents are ranked higher than irrelevant ones, losing much of its discriminative power.

Suppose you retrieve five documents for a query with relevance grades of three, zero, two, one, and zero. DCG is computed as three divided by log two of two, plus zero divided by log two of three, plus two divided by log two of four, plus one divided by log two of five, plus zero divided by log two of six. This equals three plus zero plus one plus 0.43 plus zero, which equals 4.43. If the ideal ranking were three, two, one, zero, zero, ideal DCG would be three plus 1.26 plus 0.5 plus zero plus zero, which equals 4.76. NDCG is 4.43 divided by 4.76, which equals 0.93. This indicates very good but not perfect ranking.

NDCG is more complex to compute and explain than precision, recall, or MRR, but it captures ranking quality more completely. A system with high precision and recall might still have poor NDCG if relevant documents are ranked randomly among irrelevant ones. NDCG rewards systems that surface the best documents first, which directly impacts user experience and downstream generation quality. If your RAG pipeline passes the top three documents to the generation model, NDCG tells you whether those three are the best available or merely adequate.

## When Each Metric Matters

Choosing the right retrieval metrics depends on your application, user behavior, and system architecture. Precision matters when users or models examine most retrieved documents. Recall matters when completeness is critical. MRR matters when the first result dominates user attention. NDCG matters when ranking quality across multiple positions affects outcomes. Many teams track all four metrics because they provide complementary views of retrieval quality.

For customer support chatbots, precision and MRR are often most important. Users ask a question and expect a quick, relevant answer. The bot retrieves a few knowledge base articles, generates a response, and presents it. If the top-ranked article is relevant, the bot probably generates a good answer. If the top article is irrelevant, the answer suffers. Recall is less critical because a single relevant article often suffices. NDCG adds value if the bot uses multiple articles to synthesize answers, but for simple retrieval-based bots, MRR and precision dominate.

For legal research platforms, recall is paramount. Lawyers need to find all relevant precedents to build strong cases. Missing a critical case can lose a trial or expose liability. Precision also matters because lawyers do not want to read dozens of irrelevant cases, but they would rather review extra cases than miss important ones. MRR is less important because legal research is thorough, not fast. Lawyers expect to examine many results. NDCG is valuable because ranking precedents by relevance helps prioritize reading, but recall is the primary constraint.

For RAG systems that pass multiple documents to a generation model, NDCG becomes very important. The model attends to context based on relevance and position. If the most relevant documents are ranked first, the model focuses on strong evidence and generates better answers. If relevant documents are scattered among irrelevant ones, the model must filter noise, which reduces generation quality. Precision and recall still matter, but NDCG captures the ranking quality that directly impacts generation. Track NDCG alongside precision and recall to understand not just what you retrieve but how well you rank it.

For exploratory search and recommendation systems, recall and NDCG drive user satisfaction. Users want to discover a variety of relevant content, not just the single best result. High recall ensures diverse coverage. High NDCG ensures the best items appear early, encouraging engagement. Precision is less critical because users are browsing and expect to filter. These applications prioritize diversity and coverage over focused precision, which shifts the metric emphasis toward recall and NDCG.

## Practical Computation Challenges

Computing retrieval metrics in practice involves several challenges. You need ground truth relevance labels, which are expensive to create. You need to decide how many documents to retrieve and evaluate. You need to handle queries with no relevant documents and documents with varying relevance. You need to compute metrics efficiently for large test sets and track them over time. Addressing these challenges requires careful design and tooling.

Creating ground truth relevance labels is the most expensive step. You need annotators to examine queries and documents and judge relevance. For each query in your test set, annotators review a pool of candidate documents and label them. Binary labels, relevant or irrelevant, are easier and cheaper to collect than graded labels. Graded labels, which distinguish between highly relevant, relevant, and marginally relevant, provide more signal for metrics like NDCG but require more annotation time and clear guidelines. Invest in annotator training and inter-annotator agreement checks to ensure label quality.

Choosing the value of k, the number of documents to retrieve and evaluate, affects all metrics. Precision at five is different from precision at fifty. Recall at ten is different from recall at one hundred. You should choose k based on your system design. If your RAG pipeline passes ten documents to the generation model, compute metrics at k equals ten. If users typically examine the top five results, compute metrics at k equals five. Computing metrics at multiple k values provides a more complete picture. You might track precision at one, five, and ten to see how precision degrades as you retrieve more documents.

Handling queries with no relevant documents requires care. If a query has no relevant documents in your ground truth, recall is undefined or trivially perfect, and precision is zero if you return anything. These queries are edge cases but they occur in practice, especially if your test set includes off-topic or adversarial queries. Decide whether to include them in metric averages or filter them out. Including them deflates metrics but reflects real-world performance. Filtering them provides a cleaner measure of retrieval quality for in-distribution queries.

Efficient computation matters for large test sets. Computing metrics for thousands of queries requires optimized code and potentially distributed processing. Libraries like rank-eval, pytrec-eval, and ir-measures provide efficient implementations of standard retrieval metrics. Use these tools rather than writing metrics from scratch. They handle edge cases, vectorize operations, and produce standardized output formats. Integrating these libraries into your evaluation pipeline enables fast, reproducible metric computation.

## Common Misinterpretations

Retrieval metrics are often misinterpreted, leading to poor decisions. Precision and recall are presented as a trade-off, but many teams do not realize that this trade-off is tunable and context-dependent. MRR is sometimes treated as a substitute for precision, but it measures something different. NDCG is dismissed as too complex when it actually provides the most complete view of ranking quality. Understanding what each metric measures and what it does not measure is essential for interpreting results correctly.

A common mistake is treating precision and recall as inversely related. While there is often a trade-off, it is not automatic or fixed. Improving retrieval quality can increase both precision and recall simultaneously. A better embedding model, improved query processing, or expanded indexing might surface more relevant documents and fewer irrelevant ones. Conversely, a bad change can hurt both metrics. Precision and recall trade off when you adjust ranking thresholds or the number of documents returned, but they move together when you improve the underlying retrieval model. Do not assume improving one requires sacrificing the other.

Another mistake is using MRR as a proxy for precision. MRR measures the rank of the first relevant document, not the fraction of relevant documents in your results. A system with MRR of one might have terrible precision if only the first document is relevant. A system with MRR of 0.5 might have excellent precision if half the results are relevant and the first relevant result is at position two. MRR and precision are correlated but distinct. Use MRR when you care about the first hit and precision when you care about overall result quality.

Teams sometimes avoid NDCG because it seems complex or because they lack graded relevance labels. This is a mistake. NDCG provides the most complete picture of ranking quality, and it works with binary labels, though it is less powerful. If you can collect graded labels, do so. If not, compute NDCG with binary labels as a starting point and upgrade to graded labels later. The complexity of NDCG computation is handled by libraries, so implementation is not a barrier. Avoiding NDCG means missing critical information about how well you rank results.

Finally, teams often compute metrics on a small or unrepresentative test set and overfit to those results. Your test set must reflect the diversity of queries your system will encounter in production. If you optimize for a test set of fifty queries and deploy to millions of users, your test set metrics will not predict production performance. Invest in building representative test sets, updating them as user behavior shifts, and validating that test performance correlates with production outcomes.

## Moving Forward

Retrieval metrics are the foundation of RAG evaluation. Precision tells you whether returned documents are relevant. Recall tells you whether you found all relevant documents. MRR tells you whether the first relevant document is ranked highly. NDCG tells you whether your entire ranking is well-ordered. Each metric captures a different dimension of retrieval quality, and each matters for different use cases. Effective evaluation requires tracking multiple metrics, understanding their trade-offs, and choosing targets based on your application.

Building a retrieval evaluation pipeline requires ground truth labels, efficient metric computation, and continuous monitoring. Labels are expensive to collect but essential for meaningful metrics. Libraries provide efficient implementations that scale to large test sets. Dashboards and alerts keep you informed of retrieval quality in production. This infrastructure is not optional. Without it, you are guessing whether retrieval works, and guesses are usually wrong.

The next sections build on retrieval metrics to evaluate whether retrieved context is actually useful, whether generated answers are faithful and correct, and whether the end-to-end system delivers value. Retrieval metrics tell you whether you found the right documents. The remaining metrics tell you whether those documents lead to good outcomes. Together, they form the complete evaluation stack that enables you to build, monitor, and improve production RAG systems.
