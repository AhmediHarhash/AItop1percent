# Chapter 6 â€” RAG Evaluation Metrics and Methods

You cannot improve what you cannot measure. This is the first principle of RAG evaluation, and in 2026 it separates production systems from prototypes. RAG evaluation is not prompt evaluation. It is not model evaluation. It is pipeline evaluation across retrieval, assembly, and generation, with metrics that measure grounding, faithfulness, citation accuracy, and coverage. Without rigorous evaluation, RAG systems drift into hallucination, miss critical facts, and lose user trust.

This chapter teaches you how to design evaluation systems for RAG pipelines. You will learn what metrics matter at each stage, how to measure retrieval quality independently from generation quality, how to detect hallucination and citation errors, how to use LLM judges for automated evaluation, and how to integrate human evaluation into continuous improvement loops. RAG evaluation is multi-layered, and each layer requires different tools and thresholds.

The eval stack refers to the hierarchy of evaluation checkpoints in a RAG pipeline. Evaluations happen at retrieval, ranking, context assembly, generation, and post-processing. Each stage has its own success criteria and failure modes. Retrieval eval measures whether the right documents were fetched. Generation eval measures whether the model used those documents correctly. End-to-end eval measures whether the final answer is correct and grounded. You will learn how to structure evals across the stack without redundant measurement.

Retrieval metrics measure whether the retrieval step returned relevant documents. Standard metrics include precision at k, recall at k, mean reciprocal rank, and normalized discounted cumulative gain. These metrics require ground truth labels: which documents are relevant for a given query. In production, labels come from manual annotation, user feedback, or synthetic generation. You will learn how to collect retrieval labels and how to compute retrieval metrics at scale.

Context relevance measures whether the retrieved chunks contain information necessary to answer the query. High retrieval recall does not guarantee context relevance if chunks are poorly formed or metadata is missing. Context relevance is measured by human judgment or LLM judges that assess whether each chunk is useful for answering. You will learn how to measure context relevance and how to tune chunking and retrieval to improve it.

Faithfulness measures whether the generated answer is fully supported by the retrieved context. A faithful answer contains no claims that cannot be traced to the provided chunks. Faithfulness is the most important RAG metric because it directly measures grounding. Automated faithfulness measurement uses LLM judges to decompose answers into claims and verify each claim against context. You will learn how to implement faithfulness checks and how to calibrate them against human judgment.

Correctness measures whether the answer is factually accurate according to ground truth. Correctness is distinct from faithfulness: a faithful answer can be incorrect if the retrieved context is wrong. Correctness requires external ground truth labels or expert judgment. You will learn when correctness matters more than faithfulness and how to collect correctness labels.

Hallucination detection identifies claims in the answer that are not supported by retrieved context. Hallucinations are the inverse of faithfulness. Hallucination rates are measured as the fraction of responses containing at least one unsupported claim. High hallucination rates indicate prompt design failures, weak grounding instructions, or retrieval gaps. You will learn how to detect hallucinations automatically and how to reduce hallucination rates through prompt tuning.

Citation accuracy measures whether citations point to the correct sources and whether those sources actually support the cited claims. Citation errors include missing citations, incorrect chunk IDs, and citations that do not support the claim. Citation accuracy is critical in legal, medical, and enterprise applications where provenance is required. You will learn how to validate citations programmatically and how to measure citation error rates.

LLM judges are models used to automate evaluation tasks like faithfulness checking, relevance scoring, and hallucination detection. Judges can be instruction-tuned models like GPT-4 or specialized evaluator models. LLM judges scale better than human eval but introduce their own errors and biases. Calibration against human judgment is essential. You will learn how to design judge prompts, how to calibrate judge outputs, and when to trust automated eval.

Human eval remains the gold standard for RAG quality. Humans label retrieval relevance, assess answer quality, identify hallucinations, and validate citations. Human eval is expensive and slow but provides ground truth for calibrating automated systems. You will learn how to design human eval workflows, how to sample queries for annotation, and how to measure inter-rater agreement.

Datasets for RAG evaluation include queries, retrieved documents, generated answers, and ground truth labels. Public benchmarks like MS MARCO, Natural Questions, and HotpotQA provide starting points, but domain-specific datasets are necessary for production systems. You will learn how to build custom eval datasets from production logs, user feedback, and synthetic generation.

Pipelines for RAG evaluation run evals continuously as part of CI/CD. Eval pipelines catch regressions before deployment, validate improvements from experiments, and monitor production quality over time. Pipelines require versioned datasets, reproducible metrics, and automated reporting. You will learn how to build eval pipelines that integrate with deployment workflows.

Frameworks like RAGAS, DeepEval, and LangSmith provide pre-built metrics and eval harnesses for RAG. Frameworks reduce boilerplate but require customization for domain-specific needs. You will learn how to choose an eval framework and how to extend it with custom metrics.

Coverage metrics measure whether the system retrieved all necessary information to answer a query. Low coverage means critical facts were missed during retrieval. Coverage is measured by comparing retrieved documents to a known set of relevant documents or by checking whether all sub-questions in a query are answerable. You will learn how to measure coverage and how to improve it through better retrieval strategies.

Answerability metrics measure whether a query can be answered given the retrieved context. Answerability is distinct from coverage: a query may be unanswerable even with full coverage if the context is ambiguous or contradictory. Answerability is measured by human judgment or classifiers trained to detect unanswerable queries. You will learn how to measure answerability and how to use it to filter bad queries.

Leakage detection identifies when the model uses external knowledge instead of retrieved context. Leakage is problematic in RAG because it violates grounding guarantees. Leakage is detected by comparing answers to context and identifying claims that could not have been derived from provided chunks. You will learn how to detect leakage and how to reduce it through prompt design.

Judge reliability measures how well automated judges agree with human judgment. Judge reliability is measured using correlation, precision, recall, or F1 against human labels. Low reliability means judge outputs are noisy and should not be trusted for decisions. You will learn how to measure judge reliability and how to improve it through prompt tuning or model selection.

Claim-level grounding decomposes answers into atomic claims and verifies each claim against retrieved context. Claim-level eval is more precise than sentence-level or answer-level eval because it isolates specific failures. Claim decomposition can be done by humans or by LLM judges. You will learn how to implement claim-level grounding checks and when the added precision is worth the cost.

This chapter is the foundation of trust. Without measurement, you cannot know if your RAG system works. Without metrics, you cannot improve it. In 2026, the best teams measure retrieval, faithfulness, citation, and coverage at every stage. And they do it continuously.
