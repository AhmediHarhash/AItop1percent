# 6.2 — Step Depth and Trajectory Monitoring

The median agent trajectory in your system uses 6.4 steps. One percent of trajectories use more than twenty steps. If you are not tracking which requests fall into that one percent and why, you are not monitoring your agent system. You are watching it.

Step depth is the number of discrete actions an agent takes from receiving a user request to producing a final response. Each tool call, each search, each reasoning invocation, each self-correction — every action counts as a step. Step depth directly predicts cost, latency, and failure risk. An agent that takes three steps has three opportunities to make a mistake. An agent that takes seventeen steps has seventeen.

## The Cost-Per-Step Reality

A SaaS company deployed a research agent in late 2025 to help sales teams compile prospect information. The agent could search LinkedIn, query company databases, scan news articles, and synthesize findings into a briefing. Initial testing showed strong results with an average of 5.1 steps per request and total costs around four cents per briefing.

In production, average step depth climbed to 8.9 within three weeks. Costs hit seven cents per briefing. The culprit was trajectory drift — the agent would start with a clear plan, encounter ambiguous data, replan, execute new searches, and spiral into redundant information gathering. A request that should take four steps was taking eleven. The engineering team had no visibility into the problem because they were only tracking total cost per day, not cost per step or step distribution per request type.

When they instrumented step depth tracking, the pattern became clear. Simple requests — looking up a company's employee count or revenue — stayed under four steps. Complex requests — assessing market positioning or competitive threats — occasionally hit twenty-five steps, with the agent searching the same sources multiple times using slightly rephrased queries. Step depth variance, not average depth, was the cost driver.

## Tracking Step Depth in Practice

Step depth monitoring requires tagging every agent action with a trace ID and a step counter. When an agent receives a request, you initialize a trace. Each time the agent invokes a tool, calls the model for planning, or performs a self-correction, you increment the step counter. When the agent produces a final response or times out, you log the total step count.

The baseline distribution tells you what normal looks like. If ninety percent of requests use between four and eight steps, requests that hit fifteen steps are outliers worth examining. You define thresholds: typical range, elevated range, and critical range. A request at twelve steps might still succeed, but it is burning three times the expected cost. A request at thirty steps is almost certainly stuck in a loop and should be terminated.

**Typical range**: The interquartile range of step depth across successful requests. For most agent systems, this falls between four and ten steps. This is your expected operating zone.

**Elevated range**: Requests using step counts above the 90th percentile but below the 99th percentile. These requests are expensive but might be legitimately complex. You monitor them for patterns but do not necessarily intervene.

**Critical range**: Requests above the 99th percentile. These are either exceptionally complex tasks or, more commonly, agents stuck in unproductive loops. Critical-range requests get flagged for real-time intervention or automatic termination.

## Trajectory Shape Analysis

Step count alone does not reveal whether an agent is reasoning coherently. An agent that takes ten steps in a straight line toward a goal is behaving differently from an agent that takes ten steps while bouncing between three strategies. Trajectory shape describes the structure of an agent's action sequence — linear, branching, cyclic, or chaotic.

A healthcare documentation agent in early 2026 processed clinical notes into structured reports. Most requests followed a linear trajectory: extract patient demographics, extract diagnosis codes, extract procedure codes, format output. Clean, predictable, four to six steps. But five percent of requests showed cyclic trajectories — the agent would extract demographics, attempt to extract diagnoses, fail to find them, re-extract demographics, attempt diagnoses again, fail again, and loop three or four times before giving up.

The cycle revealed a data quality issue. When clinical notes were poorly formatted, the agent could not distinguish between demographic information and clinical findings. It would extract everything as demographics, realize it had no diagnoses, and retry. The extraction tool succeeded every time. The reasoning was stuck. Step depth monitoring showed the problem. Trajectory shape analysis showed the mechanism.

## Step Depth by Request Type

Not all requests should use the same number of steps. A request to check an account balance should take two or three steps. A request to investigate a billing discrepancy and determine resolution options should take eight to twelve. Monitoring average step depth across all requests hides these differences.

You segment step depth by request type. For each category of task your agent handles, you establish expected depth ranges. Account lookups should use three to five steps. Policy questions should use five to nine steps. Complex troubleshooting should use ten to fifteen steps. When a request type starts exceeding its expected range, you investigate.

An insurance agent system in mid-2025 handled claims inquiries. Expected step depths: claims status checks used four steps, coverage questions used seven steps, appeals processing used twelve steps. The team monitored deviations — any request type where median step depth increased more than twenty percent from baseline triggered an alert.

In February 2026, coverage questions suddenly jumped from seven steps to eleven steps. Investigation revealed that a recent update to the policy database had introduced a new schema, and the agent was now making four extra queries to reconstruct information that used to come from a single query. The database change was correct, but the agent's retrieval logic needed adjustment to match. Without step depth segmentation, the four-step increase would have been invisible in the overall average.

## Real-Time Step Depth Alerts

You do not wait until end-of-day to review step depth distributions. Real-time monitoring tracks in-flight requests and terminates agents that exceed critical step thresholds. An agent that hits twenty-five steps is not going to suddenly solve the problem at step twenty-six. It is lost. You kill it, log the failure, and return an error or escalate to a human.

Defining the termination threshold requires balancing false positives and runaway costs. Set the threshold too low and you kill agents working on legitimately complex tasks. Set it too high and you burn dollars on agents stuck in loops. The typical approach: set the threshold at three times the 95th percentile step depth for that request type. If ninety-five percent of coverage questions complete in under nine steps, you terminate at twenty-seven steps.

A legal research agent system set its termination threshold at thirty steps in late 2025. Within two weeks, they observed 0.3 percent of requests hitting the threshold. Manual review showed that eighty percent of those terminated requests were indeed stuck in loops, repeatedly querying the same legal database with slightly rephrased queries. Twenty percent were complex research tasks that needed more than thirty steps. The team raised the threshold to forty steps for research requests and kept it at thirty for everything else. Termination rate dropped to 0.1 percent, and nearly all terminated requests were confirmed loops.

## Depth Attribution to Specific Tools

Some tools consume more steps than others. A tool that returns ambiguous results forces the agent to make follow-up queries. A tool that provides comprehensive answers lets the agent move to the next task. Tracking which tools contribute most to step depth reveals which tools need improvement.

You log not just step count, but step count by tool. An agent that uses a search tool five times, a database query tool twice, and a synthesis tool once has a step depth of eight, with five steps attributed to search, two to database queries, and one to synthesis. If search consistently dominates step depth, either the search tool is returning poor results or the agent is over-relying on search when other tools would be more efficient.

A customer support agent in early 2026 used a knowledge base search tool, a ticket history tool, and a product database tool. Median step depth was 7.2 steps. Step depth attribution showed that knowledge base search accounted for 4.8 steps on average, while ticket history and product database together accounted for 2.4 steps. The agent was spending two-thirds of its steps searching the knowledge base, often running the same search multiple times with minor query variations.

The team instrumented the search tool to return a relevance score for each result. They discovered the agent was issuing follow-up searches whenever the top result had a relevance score below 0.7, even if the result was actually correct. The agent had learned to distrust low-scoring results and retry. Tuning the relevance threshold reduced average knowledge base steps from 4.8 to 2.9, and overall step depth dropped to 5.3 without degrading answer quality.

## Trajectory Visualization in Debugging

When an agent produces a wrong answer or times out, step depth logs tell you how much work the agent did. Trajectory visualization tells you what work the agent did. You reconstruct the sequence: step one was a database query, step two was parsing the result, step three was a follow-up query, step four was a search, step five was another search, step six was a third search, step seven was synthesis.

Visualization reveals patterns that summary statistics hide. An agent might use ten steps with five of them clustered at the beginning and five at the end, indicating the agent gathered all information first and then processed it. Or an agent might alternate between information gathering and processing throughout the trajectory, indicating incremental reasoning. Neither pattern is inherently wrong, but they have different performance characteristics and different failure modes.

A fraud detection agent in mid-2025 used an average of nine steps per case review. Trajectory visualization showed two distinct patterns. Pattern A: gather all transaction data in steps one through four, analyze in steps five through nine. Pattern B: gather one data point, analyze, gather another, analyze, repeating nine times. Pattern A had a 94 percent accuracy rate. Pattern B had an 81 percent accuracy rate and took thirty percent longer. The agent had learned two different strategies, and one was dramatically worse.

The team retrained the agent with demonstrations that followed Pattern A exclusively. Median step depth stayed around nine, but the proportion of requests using Pattern B dropped from forty percent to seven percent, and overall accuracy climbed to 92 percent. The step count did not change. The trajectory structure did, and that mattered more.

## Step Efficiency Metrics

Step efficiency measures whether the agent is making progress or wasting effort. The simplest efficiency metric is unique tools per step. An agent that calls ten different tools in ten steps is exploring the problem space. An agent that calls the same tool ten times in ten steps is probably stuck. More sophisticated efficiency metrics compare the agent's trajectory to a reference trajectory for similar requests.

You build reference trajectories by logging the paths taken by successful, high-quality agent responses. For a given request type, what sequence of tools did the best-performing agents use? The reference trajectory becomes the baseline. New requests are compared to the reference in real time. An agent that diverges significantly from the reference is either handling an unusual case or making poor decisions.

An e-commerce agent in early 2026 handled order modifications. The reference trajectory for cancellation requests was: verify order status, check cancellation eligibility, execute cancellation, confirm to user. Four steps. Some requests took seven or eight steps, with the agent repeatedly verifying order status or checking eligibility multiple times. Efficiency scoring flagged these as low-efficiency trajectories. Investigation showed that orders with partial shipments confused the agent — it would check status, see some items shipped, recheck to confirm, check eligibility, see partial eligibility, recheck to confirm. Adding a single tool that returned "partial shipment" status explicitly eliminated the confusion and brought all requests back to four or five steps.

## When Deep Trajectories Are Correct

Not every long trajectory indicates a problem. Some tasks are legitimately complex and require many steps. An agent that investigates a security incident might need to query logs, cross-reference timestamps, check user activity, examine network traffic, correlate events, and synthesize findings — easily fifteen to twenty steps for a thorough investigation. The question is whether those twenty steps are productive or repetitive.

You distinguish productive depth from wasteful depth by tracking unique actions per trajectory. An agent that takes twenty steps using eighteen different tool calls with minimal repetition is working through a complex task. An agent that takes twenty steps using six tool calls repeated multiple times is spinning. The ratio of unique actions to total steps is your productivity signal.

A cybersecurity agent system in late 2025 investigated suspicious login attempts. Expected step depth ranged from eight steps for simple cases to thirty steps for sophisticated attack patterns. The team tracked unique action ratios. Successful investigations had ratios above 0.75 — meaning seventy-five percent of steps were unique actions. Unsuccessful investigations had ratios below 0.5 — half the steps were redundant. When a real-time investigation dropped below 0.5 unique action ratio at step twelve, the system flagged it for human review rather than letting the agent continue to step thirty burning tokens on repetitive queries.

Step depth and trajectory monitoring turn agent reasoning from a black box into a measured, observable process. You see not just that an agent succeeded or failed, but how it got there, how much work it did, and whether that work was coherent or chaotic.

The next subchapter covers tool call success rates and latency attribution — tracking whether the tools your agent depends on are actually working and how much each tool contributes to overall response time.

