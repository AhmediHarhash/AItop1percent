# 3.3 — Inline Evaluation: Running Lightweight Judges on Every Request

You can measure quality on every production request if the measurement is fast enough and cheap enough. The teams that catch quality regressions within hours use inline evaluation — lightweight AI judges that score every response in real time, logging quality metrics alongside latency and error rate. The trade-off is precision versus scale. Inline judges are less accurate than careful human review, less thorough than batch evaluation with expensive models, but they run on one hundred percent of traffic and surface problems immediately.

Inline evaluation works when the judge is orders of magnitude faster and cheaper than the primary model, when the correlation between judge scores and human quality ratings is high enough to be useful, and when you design the system to tolerate false positives and false negatives. A judge that takes three seconds to evaluate a response that took one second to generate is not inline. A judge that costs more than the response itself is not sustainable. A judge that agrees with human raters sixty-two percent of the time is not useful. The goal is a judge that runs in under 200 milliseconds, costs less than ten percent of the primary request, and correlates with human judgment at 0.75 or higher.

## Choosing a Judge Model

The judge model is not the same as the production model. The production model is optimized for the task. The judge model is optimized for evaluation.

**Use a smaller, faster model than your production model** for inline judging. If your production model is Claude Opus 4.5, your inline judge might be Claude Haiku 4.5 or GPT-5-mini. If your production model is GPT-5.1, your inline judge might be GPT-5-nano or Llama 4 Scout. The judge does not need to be capable of generating the response. It only needs to be capable of evaluating it. Evaluation is often easier than generation.

A customer support system uses GPT-5.1 for response generation at an average cost of four cents per response. The inline judge uses GPT-5-nano at an average cost of 0.2 cents per evaluation. The judge adds five percent to the total per-request cost. That is sustainable at millions of requests per day. A judge that cost four cents per evaluation would double the total system cost. That is not sustainable.

**Fine-tune a small model specifically for judging your task**. Generic judge models give generic scores. A judge fine-tuned on your task, with your quality criteria, trained on examples of good and bad responses from your domain, gives much better signal. Collect human quality ratings on several thousand production responses. Fine-tune a small model to predict those ratings. The fine-tuned judge correlates with human judgment far better than a zero-shot prompt to a large model.

A legal document summarization system fine-tuned Llama 4 Scout on five thousand summaries with human quality ratings. The fine-tuned judge correlates with human judgment at 0.81. A zero-shot GPT-5-mini judge, given a detailed rubric, correlates at 0.64. The fine-tuned judge is faster, cheaper, and more accurate. The cost of fine-tuning is amortized over millions of evaluations.

**Run multiple specialized judges rather than one general judge**. Instead of asking one judge to score overall quality, run separate judges for factual correctness, tone appropriateness, format compliance, groundedness, and safety. Each judge focuses on one dimension, making it easier to train and more reliable. You combine the scores into an overall quality metric downstream.

A healthcare advice chatbot runs four inline judges. A factual correctness judge checks whether medical claims are supported by authoritative sources. A safety judge checks whether advice could cause harm if followed. A tone judge checks whether language is empathetic and non-alarmist. A completeness judge checks whether the response answered the user's question. Each judge runs in parallel, each returns a score in under 150 milliseconds, and the scores are combined into a single quality metric logged with every request.

## Designing Judge Prompts

The quality of inline evaluation depends on the quality of judge prompts. A vague prompt produces vague scores. A detailed rubric produces consistent, interpretable scores.

**Define explicit criteria for each quality dimension**. Do not ask "Is this response good?" Ask "Does this response directly answer the user's question without adding irrelevant information?" Ask "Does this response cite sources for all factual claims?" Ask "Is the tone of this response professional and neutral?" Specific criteria produce specific scores. Specific scores are debuggable and actionable.

A code generation tool's inline judge evaluates four criteria. Correctness: Does the generated code solve the problem described in the prompt? Safety: Does the code avoid dangerous patterns like SQL injection or hardcoded credentials? Readability: Is the code formatted consistently and clearly? Efficiency: Does the code avoid obvious performance problems like nested loops or redundant API calls? Each criterion gets a score from one to five. Each score is logged separately. When quality degrades, you can see which dimension is failing.

**Use reference-based evaluation when you have known-good examples**. Instead of asking the judge to evaluate a response in isolation, show the judge a reference response and ask how the production response compares. Reference-based evaluation is more consistent and more interpretable than absolute evaluation. A score of 4 out of 5 is ambiguous. A score that says "this response is slightly worse than the reference because it omits an important safety warning" is specific.

A customer support copilot maintains a library of high-quality reference responses for common issues. When a production response is generated, the inline judge compares it against the reference response for that issue type. The judge scores relevance, completeness, and tone relative to the reference. Responses that deviate significantly from reference quality trigger alerts.

**Include examples of failure modes in the judge prompt**. Tell the judge what bad responses look like. A judge that only sees examples of good responses will score everything as good. A judge that sees examples of hallucinations, tone problems, incorrect advice, and format errors learns to recognize those patterns in production outputs.

A medical information chatbot's inline judge prompt includes examples of hallucinated medical facts, examples of overly alarming language, examples of advice that contradicts medical guidelines, and examples of responses that ignore parts of the user's question. The judge is not just scoring quality. It is screening for specific, known failure modes that have caused problems in the past.

## Logging and Aggregating Judge Scores

Inline judges produce a score for every request. That is millions of scores per day. Logging and aggregating those scores is as important as generating them.

**Log judge scores alongside request metadata**. For every request, log the primary model's response, the judge's score, the user's identifier, the timestamp, the prompt category, the response length, the latency, and any other contextual data you collect. Treat judge scores as first-class telemetry. Store them in the same observability pipeline as latency metrics and error logs.

Correlate judge scores with user behavior. Do responses with low judge scores have higher abandonment rates? Do responses with high judge scores have higher acceptance rates? If judge scores do not correlate with user behavior, the judge is not measuring what users care about. Tune the judge until the correlation improves.

**Aggregate scores into time-series metrics**. Compute mean judge score over rolling windows — last hour, last day, last week. Compute percentiles — p50, p25, p10. Track the percentage of responses below critical thresholds. Alert when mean score drops, when the percentage of low-scoring responses increases, or when specific quality dimensions degrade.

A document generation tool tracks inline judge scores as time series in Datadog. Mean judge score for factual correctness is plotted on a dashboard. Alerts fire when mean score drops below 4.2 or when more than eight percent of responses score below 3.0. The team sees quality regressions in real time, before user complaints arrive.

**Segment scores by context**. Average quality across all prompts hides problems. A system that scores 4.5 on simple prompts and 2.8 on complex prompts has an average score of 3.7, which looks acceptable. But the complex prompts are broken. Segment scores by prompt type, user segment, intent category, response length, and any other dimension that might reveal heterogeneous quality.

A legal research assistant segments inline judge scores by practice area. Scores for contract law stay stable at 4.6. Scores for intellectual property drop from 4.4 to 3.1 over three days. The overall average only drops from 4.5 to 4.1, which looks like noise. Segmented metrics reveal a specific, severe regression in one domain.

## Handling Judge Latency and Cost at Scale

Inline judges add latency and cost to every request. At scale, those costs compound. The teams that run inline evaluation successfully build infrastructure that minimizes the overhead.

**Run judges asynchronously after returning the response to the user**. The user does not need to wait for the judge's score. Return the primary model's response immediately. Send the response to the judge in the background. Log the score when it arrives. The user sees no latency increase. The judge score arrives within a few hundred milliseconds, fast enough for real-time alerting but not blocking the user experience.

Async evaluation introduces a consistency problem. The judge score arrives after the response is sent. If the judge identifies a low-quality or unsafe response, the user has already seen it. Async inline evaluation is for monitoring and alerting, not for blocking bad responses. If you need to block responses, you need synchronous evaluation. If you need to monitor quality at scale, async evaluation is the only sustainable option.

**Batch judge requests when possible**. Instead of sending one response to the judge at a time, buffer responses for ten to fifty milliseconds and send them in a batch. Judge models often have batch APIs that amortize overhead across multiple evaluations. Batching reduces per-evaluation cost and latency, especially for models with high fixed overhead per request.

A summarization service buffers responses for thirty milliseconds and sends them to the inline judge in batches of twenty to fifty. Per-evaluation latency drops from 180 milliseconds to seventy milliseconds. Per-evaluation cost drops by forty percent. The thirty millisecond buffering delay is acceptable because judge scores are not user-facing.

**Sample judge evaluation on a subset of traffic if full coverage is too expensive**. Run the inline judge on ten percent of requests, or fifty percent, or ninety percent — whatever your budget and latency constraints allow. Sampled inline evaluation is better than no inline evaluation. It catches most quality regressions while keeping costs manageable.

Stratified sampling ensures you evaluate a representative cross-section of traffic. Sample proportionally across user segments, prompt types, and traffic sources. A system that only evaluates requests from power users will miss quality problems affecting new users. A system that only evaluates short responses will miss problems in long responses.

## Correlating Judge Scores With Human Review

Inline judges are useful only if they agree with human judgment. Continuously validate judge accuracy against human review to ensure the judge is measuring what you think it is measuring.

**Run daily human review on a sample of responses with low judge scores**. If the judge says a response is bad, check whether a human agrees. Low-scoring responses that humans rate as good are false positives. The judge is too strict or is measuring the wrong thing. High-scoring responses that humans rate as bad are false negatives. The judge is too lenient or is missing failure modes.

Track false positive rate and false negative rate separately. A judge with a ten percent false positive rate and a five percent false negative rate is probably acceptable. A judge with a thirty percent false positive rate creates alert fatigue. A judge with a twenty percent false negative rate misses too many problems. Tune the judge's prompt, retrain the model, or adjust scoring thresholds to improve accuracy.

**Run periodic side-by-side comparisons**. Show human reviewers pairs of responses — one with a high judge score, one with a low judge score. Ask the human which response is better. The judge should correctly rank responses more than eighty percent of the time. If ranking accuracy is below eighty percent, the judge is not reliably distinguishing good from bad.

A customer service copilot runs weekly side-by-side comparisons on one hundred response pairs. The inline judge correctly ranks responses eighty-six percent of the time. That is good enough to trust the judge for alerting. The judge is not perfect, but it is reliable enough to surface real problems and ignore noise.

**Retrain or retune judges when correlation degrades**. User expectations change. Product features change. The distribution of prompts changes. A judge trained six months ago might no longer correlate well with current human quality judgments. Collect fresh human ratings every quarter. Retrain the judge on recent data. Update judge prompts to reflect current quality criteria.

A financial advice chatbot retrains its inline judge every three months. In early 2025, users valued conciseness. The judge was trained to penalize verbose responses. By late 2025, users started asking more complex questions and expected detailed explanations. The judge retrained on recent human ratings learns that longer responses are now good, not bad. Correlation with human judgment stays above 0.78 across the transition.

## Inline Evaluation as Early Warning

The primary value of inline evaluation is early detection. A quality problem that takes two weeks to detect through user complaints or sampled human review is detected in two hours through inline evaluation.

**Set thresholds that trigger automatic alerts**. Define what "bad" means for each quality dimension. If the judge scores factual correctness below 3.5, alert the on-call engineer. If more than twelve percent of responses score below 3.0 on safety, page the team. If mean tone score drops below 4.0, open a ticket. Thresholds convert continuous judge scores into discrete signals that drive action.

Tune thresholds to balance false positives and false negatives. A very strict threshold catches every problem but fires alerts on noise. A very lenient threshold never fires false alarms but misses real problems. The right threshold depends on the cost of missing a problem versus the cost of alert fatigue. For safety-critical applications, tolerate false positives. For less critical applications, tolerate false negatives.

**Correlate inline judge scores with other signals**. A single low judge score on one request is noise. A ten percent drop in mean judge score sustained over two hours is a signal. Combine judge scores with proxy behavioral metrics, error rates, and latency shifts. Correlated signals confirm a real problem. Uncorrelated signals might be noise.

A medical advice chatbot alerts when inline judge scores drop below 4.0 AND session abandonment rate increases by more than five percentage points. Either signal alone is noisy. Both signals together indicate a real quality regression. The system alerts on true problems and ignores transient fluctuations.

Inline evaluation makes continuous quality monitoring scalable. It is not a replacement for human review. It is a filter that directs human attention to the requests most likely to have problems. The combination of inline judges for breadth and human review for depth creates a quality monitoring system that catches regressions early without overwhelming the team.

Next, we examine sampled deep evaluation — running expensive, thorough quality checks on a subset of traffic to catch problems that lightweight judges miss.

