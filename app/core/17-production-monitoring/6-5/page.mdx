# 6.5 — Planning Failure Detection: When Reasoning Goes Wrong

Every tool call succeeded. Every step executed cleanly. The agent produced a confident, well-formatted answer. The answer was completely wrong. This is planning failure — when an agent's high-level reasoning strategy is flawed even though its execution is flawless.

Planning failures are harder to detect than execution failures because the system logs show success at every step. The agent queried a database, got results, performed calculations, checked constraints, and returned an answer. Nothing crashed. Nothing timed out. But the agent asked the wrong question of the database, or performed the wrong calculation, or checked irrelevant constraints, and the final answer bore no relation to what the user needed.

## The Correct Execution of the Wrong Plan

An insurance claims agent in early 2026 processed claims for medical procedures. A customer asked whether a specific surgery was covered under their plan. The agent's trajectory: retrieve customer policy details, query coverage database for surgery code, check annual deductible status, calculate out-of-pocket costs, return answer. All five steps succeeded. The agent returned a confident response: "Your surgery is covered. You will owe 1,240 dollars after deductible."

The customer proceeded with the surgery. The hospital billed them 8,900 dollars. The surgery was not covered. The agent had queried the coverage database correctly but used the wrong surgery code — it had searched for a related but different procedure code that was covered, while the actual surgery the customer needed was not. Each step succeeded. The plan was wrong from step two.

The company discovered the error only after the customer filed a complaint. Review of the agent's logs showed technically correct execution of a fundamentally flawed strategy. The agent had selected the wrong surgery code based on a keyword match in the user's query, never validated that the match was semantically correct, and proceeded with confidence. Planning failure detection would have caught this — the agent's chosen surgery code did not match the procedure name the user provided, a mismatch that should have triggered a validation check.

## Detecting Misaligned Reasoning Chains

Planning failures manifest as breaks in logical flow. The agent performs action A, learns fact X, then performs action B that makes sense only if fact X were actually fact Y. The reasoning chain is broken — the agent did not incorporate what it learned into its next decision.

You detect this by validating that each step logically follows from prior steps. After the agent executes a step, you check: does the next planned action make sense given the result of the current step? This requires a reasoning validator — either a rule-based system that checks common patterns, or a separate model call that evaluates whether the agent's plan is coherent.

A customer service agent in mid-2025 handled account inquiries. A user asked to cancel a subscription. The agent's trajectory: retrieve account details, check subscription status, verify cancellation eligibility, inform user that cancellation is complete. Step three returned "not eligible for cancellation — account already inactive." Step four proceeded anyway and told the user the cancellation was complete. The agent learned the account was inactive but ignored that fact when formulating its response.

The team implemented a reasoning validator that checked for contradictions. If any step returned a status of "inactive" or "not found" or "ineligible," the validator flagged the request. If the agent's final response claimed success after receiving one of those statuses, the validator escalated the response for human review before sending it to the user. This pattern caught twelve percent of planning failures — cases where the agent's actions were individually correct but sequentially incoherent.

## Plan-vs-Goal Divergence Monitoring

Agents are given goals. They generate plans to achieve those goals. Sometimes the plan stops aligning with the goal partway through execution. The agent pivots to a new subgoal that is tangential or irrelevant, executes that subgoal successfully, and returns results that do not satisfy the original request.

You track goal-plan alignment by periodically checking whether the agent's current actions are still oriented toward the original goal. This requires maintaining a representation of the goal and evaluating each action against it. Some teams use a separate model call at steps five, ten, and fifteen to ask: "Given the original user request and the actions taken so far, is the agent still working toward an answer?"

A research agent in late 2025 was asked: "What were the financial results of Company X in Q3 2025?" The agent's trajectory: search for Company X financial reports, find a press release about a new product launch, extract details about the product launch, search for reviews of the product, summarize the reviews, return the summary. The agent drifted from financial results to product reviews. Every step succeeded. The final answer was irrelevant.

Goal-plan divergence monitoring flagged the request at step four when the agent began extracting product details instead of financial data. A mid-trajectory validation check asked: "Does the current action plan address the user's question about financial results?" The answer was no. The system terminated the agent, logged the divergence, and retried with additional constraints in the agent's instructions emphasizing financial data retrieval. The retry succeeded.

## Confidence-Output Mismatch Detection

Agents often output confidence scores or implicit signals of confidence in their final responses. A planning failure is indicated when the agent expresses high confidence in an answer despite encountering failures or ambiguous data during execution. An agent that says "I am confident this is correct" after hitting two API errors and receiving one empty result is exhibiting a confidence-output mismatch.

You track this by logging both the agent's trajectory quality and its expressed confidence. If the agent encountered multiple tool failures, retries, or ambiguous results, but still produces a high-confidence final response, flag the response for review. A simple heuristic: if more than twenty percent of the agent's tool calls failed or returned empty results, the agent should not claim high confidence.

A financial advisory agent in early 2026 provided investment recommendations. The agent queried market data APIs, portfolio analysis tools, and risk assessment tools. In one case, two of the three data sources timed out. The agent proceeded with the one successful data source and returned a recommendation with the phrasing "Based on comprehensive analysis, I recommend..." The language implied complete information. The reality was partial information.

Confidence-output mismatch detection flagged this. The agent's trajectory showed a thirty-three percent tool failure rate, but its final output used definitive language. The system downgraded the response to "preliminary recommendation based on partial data" and flagged it for human review before delivery. This pattern prevented overconfident recommendations based on incomplete information from reaching users directly.

## Detecting Strategy Selection Errors

Agents typically have multiple strategies available for a given task. When strategy A fails, the agent might switch to strategy B. Planning failures occur when the agent selects the wrong strategy initially, or fails to switch strategies when the chosen strategy is clearly not working.

You detect strategy selection errors by tagging each trajectory with the strategy the agent used and tracking success rates per strategy. If strategy A succeeds ninety percent of the time for request type X, and strategy B succeeds only sixty percent of the time, but the agent is choosing strategy B for forty percent of request type X, the agent's strategy selection is suboptimal.

A legal contract review agent in mid-2025 had two strategies for identifying risky clauses: keyword-based search and semantic similarity search. Keyword search was fast and accurate for standard contracts. Semantic search was slower but better for non-standard language. The agent was supposed to choose keyword search first and fall back to semantic search if keyword search returned fewer than three matches.

In production, the agent started using semantic search as the first strategy for thirty percent of requests, even standard contracts. Investigation showed that the agent had learned a preference for semantic search because it returned more results, which the agent interpreted as "better." The results were not better — they were noisier. But the agent's planning logic optimized for result count, not result relevance.

The team retrained the agent with demonstrations that explicitly showed choosing keyword search first for standard contracts and semantic search only for edge cases. Strategy selection errors dropped from thirty percent to eight percent, and median latency improved by 0.9 seconds because keyword search was faster.

## Intermediate Reasoning Checkpoints

Planning failure detection is most effective when integrated into the agent's execution loop, not performed only after the agent completes. You insert reasoning checkpoints at strategic points in the trajectory — after initial data retrieval, before committing to a high-cost action, after any step that returns unexpected results.

At each checkpoint, a validator evaluates: does the plan still make sense? Has the agent learned information that should change its strategy? Is the agent ignoring contradictions or ambiguous data? If the checkpoint fails, the agent either replans or escalates.

A procurement agent in early 2026 handled purchase requests. Checkpoints were inserted after vendor selection and before final purchase confirmation. At the first checkpoint, the validator checked: does the selected vendor meet the requirements specified in the request? At the second checkpoint: do the final terms match the authorized budget and timeline? These checkpoints caught cases where the agent selected vendors based on availability rather than suitability, or where the agent approved purchases that exceeded budget constraints.

Checkpoint failure rate was four percent — one in twenty-five requests triggered a checkpoint failure and required replanning or escalation. Without checkpoints, those four percent would have executed bad plans to completion, resulting in incorrect purchases, contract violations, or budget overruns.

## Plan Explanation and Auditability

Some teams instrument their agents to generate plan explanations: before executing a multi-step plan, the agent outputs a structured summary of what it intends to do and why. The explanation is logged and optionally reviewed by a validator. If the plan explanation reveals a flawed strategy, the system can intervene before the agent wastes cost executing it.

Plan explanations are particularly valuable for high-stakes agents — those handling financial transactions, legal decisions, or medical recommendations. The agent states its reasoning upfront, a human or automated system reviews the reasoning, and execution proceeds only if the plan is sound.

A loan approval agent in mid-2025 generated plan explanations before making credit decisions. The explanation included: applicant's credit score, income verification status, debt-to-income ratio, decision criteria, and planned outcome. A rule-based validator checked the explanation for logical consistency — if the credit score was below threshold but the planned outcome was approval, the validator flagged it. If income verification failed but the explanation proceeded as if income were verified, the validator flagged it.

Explanation validation caught six percent of planning failures before execution. These were cases where the agent had formulated a flawed decision based on misinterpreted data. Catching the error at the planning stage saved the cost of executing the flawed plan and prevented incorrect loan approvals from reaching underwriting.

## Cross-Validation with Expected Trajectories

For well-defined tasks, you can build reference trajectories — examples of correct plans for common request types. When an agent deviates significantly from the reference trajectory, it might be handling an edge case correctly, or it might be planning poorly. You flag deviations for review.

An IT support agent in early 2026 handled password reset requests. The reference trajectory: verify user identity, check account status, generate reset link, send link via authorized email, confirm completion. A request that followed a different trajectory — for example, generating the reset link before verifying identity — indicated a planning failure and a potential security risk.

Trajectory deviation detection flagged 1.2 percent of requests. Ninety percent of flagged requests were legitimate edge cases where the agent correctly adapted to unusual circumstances, such as users with multiple accounts or expired email addresses. Ten percent were planning failures where the agent skipped security checks. Reviewing flagged requests allowed the team to improve both the agent's planning logic and the reference trajectories to accommodate valid edge cases.

## Feedback Loop Integration

Planning failure detection improves over time when integrated with feedback loops. When the system detects a suspected planning failure, logs it, and a human later confirms the failure, that confirmation is used to retrain the detection model or update the validation rules. The system learns what planning failures look like.

A sales agent in late 2025 generated product recommendations. Planning failure detection flagged cases where the agent recommended products that did not match the customer's stated requirements. Initially, detection rules were simple: if the customer mentioned "budget under 500 dollars" and the agent recommended a product priced above 500 dollars, flag it. Over time, human reviewers confirmed that additional patterns also indicated planning failures: recommending products missing required features, recommending discontinued products, recommending products incompatible with the customer's existing setup.

Each confirmed planning failure was analyzed, and new detection rules were added. After six months, planning failure detection covered thirty distinct failure patterns and had a false positive rate below eight percent. The system had learned, from production data and human feedback, what incorrect reasoning looked like.

## The Limits of Automated Planning Validation

Planning failure detection is imperfect. Some planning failures are subtle and context-dependent. An agent might make a defensible but suboptimal choice that is hard to classify as correct or incorrect without deep domain knowledge. Automated validators catch obvious errors — contradictions, omitted steps, misaligned strategies — but they miss nuanced mistakes that require human judgment.

This is why planning failure detection reduces planning errors but does not eliminate them. You set realistic goals: catch fifty to seventy percent of planning failures automatically, escalate ambiguous cases to humans, and accept that some failures will reach users. The goal is not perfection. The goal is to catch the majority of egregious planning mistakes before they cause harm.

Planning failures are the silent killer of agent reliability. They do not show up in error logs. They do not trigger alerts. They quietly produce incorrect outputs that users trust because the system presented them confidently. Detecting planning failures requires reasoning-layer observability — monitoring not just what the agent did, but whether what it did made sense.

The next subchapter covers intermediate reasoning validation — techniques for checking the agent's thought process at each step, not just at the end.

