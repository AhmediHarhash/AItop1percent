# 5.4 — Retrieval Quality Metrics: Recall, Precision, and Context Relevance

Your retrieval system returns documents with high confidence scores. The model synthesizes them into articulate responses. Users report that the answers are not quite right. The failure is not in synthesis. The failure is in retrieval. The documents the model received did not contain the information needed to answer the query correctly. You retrieve fast, you retrieve at scale, but you do not retrieve the right documents. This is the retrieval quality problem, and it is invisible to traditional infrastructure metrics.

Retrieval quality is measured by three core metrics: **recall** — the fraction of relevant documents in your index that were actually retrieved, **precision** — the fraction of retrieved documents that are actually relevant, and **context relevance** — whether the specific passages within the retrieved documents contain the information the model needs. High recall with low precision means you retrieve all the right documents but also many wrong ones, drowning the signal in noise. High precision with low recall means every document you retrieve is relevant, but you miss important documents that should have been included. High precision and recall but low context relevance means you retrieve the right documents but the wrong sections, forcing the model to synthesize answers from tangentially related passages rather than directly relevant ones.

## The Measurement Problem

Measuring retrieval quality requires ground truth. You need queries paired with the documents that should have been retrieved. For a small evaluation set, you can manually label ground truth. For production traffic at scale, manual labeling is impractical. The measurement strategy depends on whether you are evaluating offline or monitoring online.

**Offline evaluation** uses a static query set with pre-labeled ground truth. You maintain 200 to 2,000 representative queries, each annotated with the document IDs that constitute correct answers. You run these queries through your retrieval system monthly or weekly and measure precision and recall against the labels. This tells you whether retrieval quality is stable over time. It does not tell you whether your evaluation set is still representative of real user queries, which drift as your product evolves and user needs change.

A software documentation RAG system maintained 600 labeled queries from 2024. Each query had between one and five ground truth documents. The team ran this eval suite weekly and tracked recall at k equals 5 — whether the retrieval system returned at least one ground truth document in the top five results. Recall held steady at 0.82 through mid-2025. In July 2025, the company released a major product update with new features. User queries shifted to ask about the new features. The eval suite did not include queries about the new features. Measured recall stayed at 0.82. Actual user-perceived recall plummeted because the eval set was stale. The team added 150 new queries covering the new features. Measured recall dropped to 0.68. The retrieval system had been failing on new-feature queries for weeks, but the stale eval set never caught it.

**Online evaluation** measures retrieval quality on live traffic. You cannot manually label every query, so you need proxies for ground truth. The most common proxy is **user engagement with citations**. If the model cites a document and the user clicks that citation, the document was likely relevant. If the model cites five documents and the user clicks none, the documents were likely irrelevant or the answer was sufficient without verification. This is noisy — users do not always click even when citations are relevant — but at scale it provides signal.

A legal research platform tracked citation click-through rates per query type. Queries about case law had a 47 percent CTR on citations. Queries about statutes had a 34 percent CTR. Queries about procedural rules had a 19 percent CTR. This did not mean procedural rule retrieval was worse. It meant users trusted procedural rule answers without verification more often, possibly because those answers were simpler. The team segmented CTR by query complexity and found that complex queries had higher CTR regardless of topic. The metric was useful as a trend indicator — if CTR for a query type dropped by 10 percentage points over a month, retrieval quality might be degrading — but not as an absolute quality measure.

## Recall: Are You Retrieving the Right Documents?

Recall answers the question: of all the documents in my index that could answer this query, what fraction did I actually retrieve? Perfect recall means you retrieved every relevant document. Zero recall means you retrieved none. Recall is expensive to measure precisely because it requires knowing the full set of relevant documents, which requires either exhaustive manual labeling or exhaustive search.

In practice, you measure recall at k — recall within the top k retrieved results. If your retrieval system returns the top 20 documents and the model uses the top 5, you measure recall at 5. Did at least one relevant document appear in the top 5? If yes, recall at 5 is 1.0 for that query. If no, recall at 5 is 0. Across hundreds or thousands of queries, recall at 5 is the fraction of queries where at least one relevant document appeared in the top 5.

A customer support RAG retrieved 50 documents per query but only passed the top 10 to the model due to context window constraints. The team measured recall at 10. They found that recall at 10 was 0.91, but recall at 5 was only 0.74. The retrieval system was finding relevant documents, but they were often ranked 6th through 10th. The model received them, but if the top 5 documents were irrelevant, the model had to wade through noise to find signal. The team tuned their ranking algorithm to prioritize precision in the top 5 results. Recall at 5 improved to 0.83. Recall at 10 dropped slightly to 0.88, but overall answer quality improved because the model encountered relevant context earlier.

## Precision: Are the Documents You Retrieved Actually Useful?

Precision answers the question: of the documents I retrieved, what fraction are actually relevant? High precision means most retrieved documents contain useful information. Low precision means most are noise. Precision is easier to measure than recall because you only evaluate the documents you retrieved, not the entire index. For each query, human evaluators or automated judges score each retrieved document as relevant or not. Precision is the fraction labeled relevant.

Automated precision measurement typically uses **LLM-as-judge**. For each retrieved document and query pair, you prompt a large model to judge whether the document is relevant to the query. The judge model returns a binary label or a relevance score. Averaged across all retrieved documents for all eval queries, this gives you precision. The challenge is that the judge model can be wrong. It might label a subtly relevant document as irrelevant or vice versa. Judge accuracy depends on the judge model's capabilities and the prompt quality.

A healthcare RAG system used Claude Opus 4.5 as a relevance judge. The prompt provided the query and a retrieved document, then asked: "Does this document contain information that would help answer the query? Respond with yes or no." The judge labeled each document. Precision was calculated as the fraction of yes labels. The team validated judge accuracy by having domain experts manually label a sample of 400 query-document pairs. The judge agreed with experts 89 percent of the time. Disagreements were usually cases where the document was tangentially relevant but not directly useful. The experts labeled those as not relevant. The judge labeled them as relevant. This inflated measured precision slightly, but the metric was still useful for tracking trends.

## Context Relevance: Is the Right Information in the Retrieved Passages?

Retrieval precision only measures whether a document is relevant. It does not measure whether the specific passages the model sees are relevant. Long documents often contain sections on multiple topics. A 50-page policy document might have one paragraph that answers the user's query and 49 pages of unrelated content. If your retrieval system returns the entire document and your chunking strategy does not isolate that paragraph, the model receives 49 pages of noise and one paragraph of signal. The document is relevant. The context is mostly irrelevant.

**Context relevance** measures whether the passages the model actually processes contain the information needed. This is distinct from document relevance. You can have high document precision and low context relevance if your retrieval returns the right documents but your chunking or passage extraction surfaces the wrong parts of those documents.

Measuring context relevance requires passage-level evaluation. For each query, retrieve documents. Extract the passages or chunks that would be sent to the model. Judge each passage for relevance to the query. Context relevance is the fraction of passages judged relevant. This is more granular and more expensive than document-level precision, but it measures what the model actually sees.

A legal contract analysis RAG retrieved full contracts as documents. Contracts were 20 to 80 pages. The system chunked contracts into 500-word passages and passed the top 10 passages to the model. Document-level precision was 0.91 — the right contracts were retrieved. Passage-level context relevance was 0.54 — only about half the passages sent to the model contained information relevant to the query. The other half were boilerplate, definitions, or unrelated clauses. The model spent half its context window processing irrelevant text. The team refined their chunking strategy to use semantic splitting based on contract structure — clauses, sections, schedules. Context relevance improved to 0.78. The model received less noise and synthesized better answers.

## Mean Reciprocal Rank: Is the Best Document in the Top Slot?

Recall and precision measure whether you retrieved relevant documents. They do not measure ranking quality. If you retrieve five documents and all five are relevant but the most relevant one is ranked fifth, precision is perfect but the user experience is poor. The model synthesizes context in the order retrieved. Burying the best document under less-relevant ones degrades answer quality.

**Mean Reciprocal Rank** measures ranking quality. For each query, identify the highest-ranked relevant document. If it is ranked 1st, the reciprocal rank is 1. If it is ranked 2nd, the reciprocal rank is 0.5. If it is ranked 5th, the reciprocal rank is 0.2. Average reciprocal ranks across all queries to get MRR. An MRR of 0.9 means that on average, the top relevant document appears in the top 1.1 positions. An MRR of 0.5 means it appears around position 2.

A product documentation RAG had high recall and precision but users complained that answers were verbose and unfocused. The team measured MRR and found it was 0.61. The retrieval system was finding relevant documents, but the most relevant one was often ranked 2nd or 3rd. The model synthesized all retrieved documents, so answers included information from the less-relevant documents ranked higher. The team retrained their ranking model using clickstream data to prioritize the documents users actually engaged with. MRR improved to 0.84. Answers became more focused because the most relevant context appeared first.

## Coverage Failures: When No Relevant Documents Exist

Sometimes retrieval fails not because the ranking is poor but because the knowledge base lacks the information needed to answer the query. The retrieval system searches the entire index and returns the highest-scoring documents, but none of them are actually relevant. The model synthesizes an answer from tangentially related context. The answer sounds confident and plausible but is wrong or incomplete.

**Coverage** measures how often queries have at least one relevant document in the index. If recall at 100 — measuring whether any of the top 100 results are relevant — is 0.95, coverage is 95 percent. Five percent of queries have no relevant documents retrievable. For those queries, retrieval will always fail. No amount of ranking tuning will fix a coverage problem.

Measuring coverage requires knowing whether relevant documents exist. For labeled eval sets, this is straightforward — queries without labeled ground truth documents have zero coverage. For live traffic, you need proxy signals. If a query retrieves documents but the model's answer includes an "I don't have information about that" disclaimer or if the user immediately rephrases the query, coverage might be insufficient. High rephrasing rates or high rates of "I don't know" responses for specific query types indicate coverage gaps.

A travel booking RAG tracked "no good answer" rates per destination. Queries about major cities had a 2 percent no-good-answer rate. Queries about small towns had a 23 percent rate. The knowledge base had rich information about major destinations and sparse information about small towns. Coverage was the bottleneck, not retrieval quality. The team prioritized content acquisition for high-demand, low-coverage destinations. No-good-answer rates for those destinations dropped as coverage improved. This was not a retrieval fix. This was a content fix that retrieval metrics revealed.

## Measuring at Scale with Sampling

You cannot manually evaluate every query in production. Sampling is necessary. The sampling strategy determines what you can measure. **Random sampling** gives you a general sense of retrieval quality across all traffic but might miss rare query types where quality is poor. **Stratified sampling** ensures coverage of different query types, user segments, or content categories. **Targeted sampling** focuses on high-stakes queries, recent queries, or queries with signals of user dissatisfaction like immediate rephrasing or low engagement.

A healthcare RAG system used stratified sampling across four query categories: diagnostic questions, treatment questions, medication questions, and administrative questions. They sampled 100 queries per category per week. Evaluators labeled ground truth documents for each sampled query. Retrieval quality was measured separately per category. Diagnostic question recall was 0.89. Treatment question recall was 0.81. Medication recall was 0.94. Administrative recall was 0.67. The aggregate metric would have been 0.83, hiding the fact that administrative queries were failing at nearly twice the rate of other categories. Stratified sampling exposed the category-specific problem. The team discovered that administrative content — appointment scheduling, insurance, billing — was poorly structured and embedded, making it hard to retrieve. They restructured that content and re-embedded it. Administrative recall improved to 0.79.

## The Retrieval Quality Dashboard

Retrieval quality deserves a dedicated dashboard. Track recall at 5, precision, MRR, and context relevance over time. Segment by query category, document type, user role, and time period. Overlay events — index updates, model upgrades, schema changes — to identify when quality shifts occur. If recall drops after an index rebuild, the rebuild broke something. If precision drops after a document ingestion, the new documents are low quality or poorly chunked. If MRR drops after a ranking model update, the update made ranking worse, not better.

A financial services RAG tracked retrieval quality weekly. In November 2025, recall at 5 dropped from 0.86 to 0.79 over two weeks. The team investigated. No code changes. No infrastructure changes. The trigger was a content update. The company had ingested new regulatory filings that used dense legal language. The embedding model represented that language poorly. Queries about the new regulations retrieved older, more naturally phrased documents instead of the new filings. Recall dropped because the right documents were not being ranked highly. The team fine-tuned their embedding model on the new regulatory language. Recall recovered to 0.84. Without the dashboard, the degradation would have been invisible until users complained.

## When High Metrics Do Not Mean Good Retrieval

Recall, precision, and MRR are necessary but not sufficient. You can have high scores on all three and still deliver poor answers if your ground truth labels are wrong, your eval set is unrepresentative, or your metrics measure the wrong thing. A team might optimize for recall at 10 when their model only uses the top 3 documents. Recall at 10 looks great. Recall at 3 is terrible. The model never sees relevant context.

Or a team might measure document-level precision when passage-level context relevance is the real problem. Precision is 0.92. Every retrieved document is relevant. But the passages extracted from those documents are mostly noise. The model cannot find the signal. Answer quality is poor despite high precision.

The metrics must match the failure mode you are trying to detect. If users complain that answers are incomplete, measure recall — are you retrieving all relevant documents? If users complain that answers are unfocused, measure precision and MRR — are you retrieving too many irrelevant documents or ranking the best one too low? If users complain that answers are close but not quite right, measure context relevance — are the passages the model sees actually useful? The symptom tells you which metric matters.

The next subchapter covers context window utilization — how to track whether your retrieval system is filling the model's context window efficiently, or wasting it on irrelevant content and truncated passages.

