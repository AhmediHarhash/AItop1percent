# 8.7 — Communication During AI Incidents: Internal and External

The incident had been running for thirty-seven minutes. The engineering team was deep in investigation. They had tried two rollbacks and implemented output filtering. User-facing errors had dropped from 18 percent to 4 percent. Progress. But no one had told the customer success team anything. At minute forty, a major customer called their account manager asking why the product was broken. The account manager had no idea there was an incident. They improvised an answer, promised to investigate, and then frantically messaged the engineering team asking what was happening. The engineering team, focused on mitigation, took nine minutes to respond. The customer called back, frustrated by the lack of communication. By the time the incident was resolved, the technical problem was fixed but the customer relationship was damaged. The failure was not technical — it was communication.

AI incidents require communication discipline that goes beyond traditional software incidents. The failures are often ambiguous. Users experience degraded quality, not clear errors. Stakeholders want to know not just that something is broken, but what the AI is doing wrong, why, and whether it is safe to keep using. You need internal communication to coordinate response and external communication to manage user expectations and trust. Both are harder for AI incidents than for traditional incidents because the nature of the failure is difficult to explain.

## The Internal Communication Cadence

Internal communication during an incident follows a rhythm: initial notification, regular status updates, escalation messages, and resolution confirmation. The cadence must be frequent enough that stakeholders have context but not so frequent that it distracts the response team.

A healthcare documentation platform had a policy: post a status update in the incident channel every five minutes during active incidents. The update could be as simple as "still investigating, no new information" or as detailed as "attempted rollback, issue persists, now testing hypothesis that training data is corrupted." The key was consistency. Stakeholders knew they would receive an update every five minutes. They did not need to interrupt the response team with questions. In March 2025, during a two-hour incident, the engineering team posted 24 status updates. Each was brief — one to three sentences. But the cumulative effect was that everyone following the incident had continuous visibility into progress.

Internal updates should include: current status, what has been tried, what is being tried now, and estimated time to next update. They should not include speculation about root causes unless those hypotheses are actively being tested. A customer service chatbot had an incident in July 2025 where the engineering team posted speculative updates: "might be a hallucination issue," "could be a prompt problem," "possibly a model regression." The speculation confused stakeholders. Product managers asked whether they should communicate a hallucination issue to users. Customer success asked whether the model had regressed. The speculation created more questions than it answered. A better approach: only post confirmed information. If you are testing a hypothesis, say "testing whether prompt change caused issue," not "the prompt probably caused the issue."

## The Stakeholder Notification Decision

Not every stakeholder needs to know about every incident immediately. The notification decision should be based on severity, user impact, and stakeholder role.

A financial advisory platform had tiered notification rules. Critical incidents: notify engineering leadership, product leadership, customer success leadership, and legal within five minutes. High incidents: notify engineering leadership and product leadership within fifteen minutes, customer success within thirty minutes. Medium incidents: notify engineering leadership within one hour, no immediate notification to other teams. Low incidents: no real-time notification, mention in weekly incident review. In October 2025, a high-severity incident occurred. Engineering leadership and product leadership were notified at minute twelve. Customer success was notified at minute twenty-eight. The staggered notification ensured that the most critical stakeholders had early awareness while avoiding alert fatigue for teams that did not need immediate involvement.

The notification should include: incident severity, affected functionality, current user impact, expected resolution timeline, and what the notified stakeholder is expected to do. A legal document assistant had an incident in December 2025. The notification to customer success said: "High severity incident, document analysis feature degraded, 8 percent of requests failing, expected resolution within two hours, please monitor for customer complaints and escalate immediately if any come in." The notification gave customer success a clear role. They were not expected to fix the technical issue. They were expected to be aware and ready to respond to customer inquiries.

## The Engineering to Product Communication

Engineering teams understand technical failures. Product teams understand user impact. The communication between these groups must translate technical symptoms into user experience language.

A travel booking assistant had an incident in May 2025 where eval pass rates dropped from 92 percent to 81 percent. The engineering team posted this information in the incident channel. The product manager asked: "What does that mean for users?" The engineers had to translate. The drop in eval pass rates meant that 11 percent of recommendations were incorrect — users were seeing hotel suggestions that did not match their stated preferences. The product manager understood the user impact and made a decision: disable the hotel recommendation feature until the issue was resolved. The translation from technical metric to user impact enabled a product decision.

Engineering to product communication should focus on observable user behavior, not system internals. Instead of "the model's confidence scores are dropping," say "users are receiving less accurate recommendations." Instead of "the retrieval pipeline is returning low-relevance documents," say "users are seeing search results that do not match their queries." The product team does not need to understand the technical mechanism. They need to understand what users are experiencing.

A document analysis platform had an incident in September 2025 where the engineering team described the issue as "catastrophic forgetting of domain-specific vocabulary." The product manager did not understand what that meant. The engineering team rephrased: "the model used to understand legal terminology but has lost that capability, so legal document summaries are now inaccurate." The rephrasing made the user impact clear. The product manager escalated to affected customers immediately.

## The Customer Success Enablement

Customer success teams are the front line of external communication. They need to know about incidents before customers do, and they need a script for what to say when customers ask questions.

A contract analysis platform had an incident in November 2025. The engineering team notified customer success at minute eight with this message: "Incident active, contract analysis feature producing incomplete outputs for some queries, fix in progress, ETA thirty minutes, customer-facing message: We are aware of an issue affecting contract analysis accuracy and are working on a fix. Expected resolution within one hour. We will notify you when resolved." The customer success team had a script. When customers reached out, they used the provided language verbatim. The script was concise, acknowledged the issue, and set expectations. Customers were not happy, but they were informed.

Customer success enablement requires providing not just information but guidance. A healthcare documentation platform had an incident in April 2025. They told customer success "eval scores are down." Customer success asked: "Should we tell hospitals to stop using the system?" The engineering team had not provided guidance. Customer success was forced to guess. After the incident, the platform implemented a decision framework: critical incidents require advising customers to stop using the system, high incidents require advising caution and manual review of outputs, medium incidents require acknowledgment but no usage restriction. The framework gave customer success clear guidance for every severity level.

## The External Communication Timing Decision

When do you tell customers about an incident? Immediately? After mitigation? After full resolution? The timing decision balances transparency with the risk of premature or incomplete information.

A customer service chatbot had a policy: communicate externally within thirty minutes for critical incidents, within two hours for high incidents, and only if customer impact exceeds four hours for medium incidents. In February 2026, a high-severity incident occurred. The engineering team resolved it in forty-two minutes. The external communication policy said two hours. But the incident was already resolved. The team decided not to communicate externally at all because there was no ongoing user impact and no user complaints. The decision avoided unnecessary alarm. Users never knew an incident had occurred because it was resolved before it significantly affected them.

Contrast this with a financial advisory platform that had an incident in August 2025. The incident affected 15 percent of users and lasted three hours. The team did not communicate externally until after full resolution. Customers had complained during the incident, been given vague responses by customer success, and felt ignored. When the post-incident communication finally went out, it was met with frustration: "Why did you wait three hours to tell us what was happening?" The delayed communication damaged trust more than the technical failure.

The timing decision should consider: Are users already experiencing the issue? Have they already contacted support? Is the issue likely to resolve quickly or will it persist? If users are actively experiencing the issue and have contacted support, communicate immediately. If the issue is resolved before users notice, consider whether communication adds value or just creates concern.

## The External Communication Content

What you say externally matters as much as when you say it. The message must be clear, honest, and appropriately detailed without being technical or speculative.

A legal document assistant had an incident in June 2025. Their external communication said: "We are experiencing technical difficulties with our document analysis feature. Some outputs may be incomplete. We are working on a fix." The message was accurate but vague. Customers did not know: How incomplete? Which outputs? Should they stop using the feature? Should they manually review everything? The vague message created more anxiety than clarity.

A better message: "We are experiencing an issue where document summaries are missing specific sections for approximately 10 percent of documents. We recommend manually reviewing all summaries generated in the past two hours. The issue is being resolved and we expect full functionality within one hour." This message is specific, quantifies impact, provides guidance, and sets expectations.

External messages should include: what is broken, how it is affecting users, what users should do, and when resolution is expected. They should not include: technical root causes, speculation, or blame. A customer service chatbot had an incident in January 2026 caused by a vendor API failure. Their external message said: "Our chatbot is experiencing degraded performance due to a third-party service outage." The message blamed the vendor. Customers did not care whose fault it was — they cared about when the chatbot would work again. A better message: "Our chatbot is experiencing degraded performance. We are actively working on a resolution and expect normal functionality within two hours."

## The Regulatory and Legal Communication

Some AI incidents require communication to regulators, legal teams, or compliance officers. If the incident involves data exposure, safety violations, or regulatory breaches, legal communication is mandatory and follows different rules than user communication.

A healthcare documentation platform had an incident in March 2025 where the model exposed patient data from one session to another session. The incident was both a technical failure and a HIPAA violation. The engineering team notified legal within eight minutes. Legal notified the compliance team. Compliance initiated mandatory breach notification procedures. The affected patients were notified within 24 hours as required by regulation. The incident report was filed with HHS within 60 days. The regulatory communication was non-negotiable — it followed legal requirements, not engineering preferences.

Legal communication should be factual, precise, and complete. You cannot minimize, speculate, or withhold information. A financial advisory platform had an incident in July 2025 where the model generated incorrect investment advice that led to a client losing money. Legal required a detailed incident report: what happened, when, how many users were affected, what data was involved, what the model generated, and what the correct output should have been. The report took four hours to compile and required collaboration between engineering, product, and legal. The delay in resolving the technical incident was acceptable because legal documentation was the priority.

## The Post-Mitigation Communication

After mitigation is applied and the incident is stabilized, communication shifts from "we are working on it" to "we have applied a fix, here is what changed."

A document summarization platform had an incident in October 2025. After applying mitigation, they sent this message: "The issue affecting document summaries has been resolved. We implemented a fix at 3:47 PM. All summaries generated after that time are accurate. If you generated summaries between 2:15 PM and 3:47 PM, we recommend reviewing them manually." The message was clear about what changed, when, and what users should do about outputs generated during the incident window.

Post-mitigation communication should also address whether the mitigation is temporary or permanent. A travel booking assistant applied a temporary fix during an incident in December 2025 — they disabled the hotel recommendation feature. Their post-mitigation message said: "We have temporarily disabled hotel recommendations to resolve an accuracy issue. Flight recommendations are fully functional. Hotel recommendations will return within 24 hours after we complete additional testing." The message set expectations that the fix was partial and temporary, not final.

## The Resolution Communication

Once the incident is fully resolved, the final communication confirms that everything is back to normal and, if appropriate, explains what happened and how you are preventing recurrence.

A customer service chatbot had an incident in May 2025. After resolution, they sent: "The issue affecting response quality has been fully resolved as of 5:20 PM. All functionality has been restored. The issue was caused by a model update that introduced unintended behavior. We have rolled back to the previous model and implemented additional testing procedures to prevent similar issues." The message confirmed resolution, explained the cause in non-technical terms, and described preventive measures.

Resolution communication should be proportional to incident severity and user impact. For a two-hour high-severity incident that affected many users, a detailed resolution message is appropriate. For a twenty-minute medium-severity incident that affected few users, a brief "everything is working normally now" message is sufficient. A healthcare documentation platform had a policy: critical and high incidents get detailed resolution messages, medium incidents get brief resolution confirmations, low incidents get no external communication unless customers asked.

## The Internal Post-Mortem Preview

Internal communication during the incident should set up the post-mortem. As the incident progresses, note what went well, what went poorly, and what should be discussed in the retrospective.

A contract analysis platform used a post-mortem tracker in their incident channel. Throughout the incident, team members added notes: "rollback took 12 minutes, should be faster," "initial severity classification was wrong, we under-classified," "scope assessment dashboard was helpful, saved time." These notes were captured in real time while context was fresh. After the incident, the post-mortem facilitator had a list of discussion topics ready. The retrospective was focused and productive because the observations were captured during the incident, not reconstructed from memory days later.

The post-mortem preview should include: what decisions were made, why, and what the outcomes were. A financial advisory platform had an incident in September 2025 where the team debated whether to roll back or implement output filtering. They chose output filtering. It took longer to implement than expected. The incident channel documented: "chose filtering over rollback because rollback would lose quality improvements, filtering took 18 minutes vs estimated 10 minutes." This documentation became a key discussion point in the post-mortem: should we have prioritized speed over preserving improvements? The answer informed future mitigation strategy decisions.

## The Communication Role Assignment

During complex incidents, assign a dedicated communication coordinator. This person is responsible for all internal and external communication, freeing the engineering team to focus on technical response.

A legal document assistant had an incident in November 2025 where the engineering manager served as communication coordinator. The engineering manager posted status updates every five minutes, coordinated with customer success, and drafted external messages. The engineers focused entirely on investigation and mitigation. The role separation was clear. No engineer had to stop debugging to answer "what is the status?" questions. The communication coordinator aggregated information from the engineering team and translated it for stakeholders.

The communication coordinator role works best for high and critical incidents. For medium incidents, the on-call engineer can handle communication. For low incidents, communication is minimal and does not require a dedicated role. A travel booking assistant had a severity-based policy: critical incidents get a dedicated coordinator immediately, high incidents get a coordinator if the incident lasts longer than thirty minutes, medium incidents do not get a coordinator. In January 2026, a high-severity incident was resolved in twenty-four minutes. No coordinator was assigned. The on-call engineer handled communication. When a similar high-severity incident lasted ninety minutes, a coordinator was assigned at the thirty-minute mark.

## The Over-Communication Versus Under-Communication Balance

Over-communication creates noise and distracts the response team. Under-communication creates confusion and duplicate effort. The balance is context-dependent.

A healthcare documentation platform had an incident in April 2025 where the engineering team posted updates every ninety seconds. The incident channel had 47 messages in thirty minutes. Most were minor: "checked logs, no errors," "restarting service," "service restarted, issue persists." The high-frequency updates overwhelmed stakeholders. People stopped reading the channel because there was too much noise. When a critical update was posted — "identified root cause, applying fix" — several stakeholders missed it because they had tuned out.

Contrast this with a customer service chatbot that posted three updates in sixty minutes: one at incident start, one at minute thirty, and one at resolution. The long gaps created anxiety. Stakeholders did not know what was happening. Some started their own investigations. Others pinged the engineering team with questions, interrupting the response. The under-communication caused coordination failures.

The right balance: post substantive updates every three to five minutes during active response. Substantive means new information, completed actions, or hypothesis test results. If nothing substantive has happened, post a status confirmation: "still testing hypothesis X, no results yet." The confirmation update prevents stakeholders from wondering whether the channel is being monitored.

## The Communication After False Alarms

Not every alert turns into an incident. Sometimes the alert fires, the on-call engineer investigates, and the issue resolves itself or turns out to be a false positive. Communicate about false alarms to maintain trust.

A document analysis platform had an alert fire in December 2025. The on-call engineer investigated. Metrics had briefly deviated but returned to normal within three minutes. No user impact. The engineer did not declare an incident and did not post anything. Two days later, a product manager asked: "I saw an alert fire on Friday, what happened?" The engineer had to reconstruct the situation from memory. The lack of communication about the false alarm created uncertainty.

A better approach: post a brief message when a significant alert fires, even if it turns out to be nothing. "Alert fired for eval degradation, investigated, metrics recovered within three minutes, no user impact, no further action needed." The message takes thirty seconds to write and provides closure. It confirms that the alert was seen, assessed, and resolved. Stakeholders do not wonder whether anyone is paying attention.

Once the incident is resolved and communication is complete, the team must conduct a post-incident review. AI incidents have specific questions that traditional incident reviews do not cover — questions about model behavior, training data, and eval coverage.

