# 11.5 — Token Waste Analysis: Finding and Fixing Inefficient Prompts

Most teams think about cost in aggregate: how much did we spend last month, how much will we spend next month. But aggregate cost hides the details that matter. Two features might cost the same total but have wildly different efficiency. One feature delivers value with every token. The other wastes half its tokens on redundant examples, verbose instructions, or unnecessary formatting. Token waste analysis identifies where your prompts burn tokens without improving quality, so you can optimize the waste away.

## What Counts as Token Waste

Token waste is any token usage that does not contribute to quality, user experience, or task success. Examples include redundant instructions repeated in every request, overly verbose system prompts, unnecessary formatting tokens, duplicated context, examples that do not improve accuracy, and retry attempts on requests that will never succeed.

In September 2025, a customer support assistant averaged 1,200 input tokens per request. The team reviewed a sample of prompts and found that 400 tokens were consumed by a system message explaining the assistant's role, tone, and constraints. The message was well-intentioned but mostly unnecessary: the model already understood its role from the few-shot examples, and users never saw the system message.

The team shortened the system message from 400 tokens to 80 tokens, preserving the essential constraints—never fabricate information, defer medical questions to professionals—and removing generic guidance like "be helpful and polite." Quality stayed the same. Cost per request dropped by 27 percent. The 320 tokens per request had been pure waste.

Token waste is not always obvious. Sometimes verbosity improves quality. Sometimes additional examples reduce errors. The waste is only waste if removing it does not harm outcomes. Token waste analysis measures both token usage and quality impact, so you optimize without degrading the experience.

## Measuring Token Distribution Across Prompt Components

Prompts are structured: system message, user input, few-shot examples, retrieved context, formatting. Each component consumes tokens. To find waste, you need to know how tokens are distributed.

A document summarization service logs token counts for each prompt component: system message, document text, user instructions, output formatting requirements. Over a week, the team analyzes 50,000 requests and calculates median token usage per component.

System message: 120 tokens. Document text: 2,400 tokens. User instructions: 30 tokens. Formatting requirements: 180 tokens. Total input: 2,730 tokens. Output: 320 tokens. The formatting requirements consume 6.6 percent of input tokens. The team reviews the formatting template and discovers it includes detailed JSON schema documentation for a response format the model already understands. They replace the schema documentation with a simple example, reducing formatting tokens to 40. Cost per request drops by 5 percent with no quality impact.

Token distribution analysis works at any granularity: per component, per request type, per feature, per user segment. The goal is to find components that consume a disproportionate share of tokens relative to their contribution to quality.

## Identifying Redundant Context and Duplication

Retrieval-augmented generation systems include context from document retrieval. Sometimes the retrieval step returns redundant or duplicated information. If three retrieved chunks contain the same fact, you pay to include that fact three times.

A legal research assistant retrieves five document chunks per query. In November 2025, the team analyzed retrieval results and found that 30 percent of queries retrieved multiple chunks from the same document with overlapping content. The overlap was not exact duplication—each chunk had slight variations—but the core information was repeated.

The team added a deduplication step: after retrieval, the system checks for high similarity between chunks and removes near-duplicates, keeping only the most relevant version. Median input tokens dropped from 3,200 to 2,600. Quality stayed flat. The redundant context had not been helping—it had just been costing money.

Duplication also happens in conversational systems. A chatbot that includes the full conversation history in every request pays to resend the same messages repeatedly. If a conversation has 10 turns and each turn includes the full history, the first message is sent 10 times. The cost grows quadratically with conversation length.

A customer success chatbot originally included the entire conversation history in every request. After five turns, input tokens exceeded 4,000. The team implemented a sliding window: only the last three user messages and three assistant messages are included in context. Older messages are summarized into a brief context statement. Cost per conversation dropped by 40 percent. Quality dropped slightly for conversations longer than eight turns but remained acceptable.

## Analyzing Few-Shot Example Efficiency

Few-shot examples improve model performance by showing the desired output format and reasoning style. But not all examples are equally valuable. Some examples are redundant. Some cover edge cases that almost never occur. Some are longer than necessary.

A content moderation system uses eight few-shot examples in its prompt. Each example includes a piece of content, a reasoning chain, and a decision. Total token cost for examples: 900 tokens per request. The team tested performance with different numbers of examples: two examples, four examples, six examples, eight examples.

With two examples, accuracy dropped by 3 percentage points. With four examples, accuracy was within 0.5 percentage points of the eight-example baseline. With six examples, accuracy matched the baseline. The team adopted four examples, reducing token cost by 450 tokens per request—a 12 percent cost reduction with negligible quality impact.

The optimal number of examples depends on task complexity and model capability. Simpler tasks need fewer examples. More capable models need fewer examples. The only way to know is to test. Run an eval suite with varying numbers of examples. Measure quality and cost. Choose the minimum number of examples that preserves quality.

## Optimizing System Messages and Instructions

System messages set the model's role, tone, and constraints. Early in development, teams often write long, detailed system messages with extensive instructions. Most of that detail is unnecessary.

A financial assistant started with a 600-token system message that included role definition, tone guidance, output format instructions, constraint explanations, example workflows, and edge case handling. The team tested shorter versions and found that a 150-token system message—role, key constraints, output format—delivered the same quality. The extra 450 tokens had been providing redundant or obvious guidance.

Verbose instructions often arise from defensive prompt engineering: adding more rules to handle every edge case. But models generalize well from examples. Instead of writing "never include medical advice, never predict stock prices, never provide legal opinions," you can write "defer questions outside your expertise" and include one example of appropriate deferral. The model infers the pattern.

System message optimization is iterative. Start with the full message. Remove one section. Test quality. If quality holds, keep the change. If quality drops, revert. Repeat until you reach a minimal message that preserves performance. Most teams find they can cut 40 to 60 percent of system message tokens with no measurable quality loss.

## Detecting Output Verbosity and Over-Generation

Token waste is not limited to input. Output tokens cost more than input tokens for most models, and verbose outputs drain budgets fast. Some verbosity is necessary—detailed explanations, formatted responses, thorough answers. Some is waste—repetitive phrasing, unnecessary elaboration, or overly formal language.

A search assistant generates answers to user queries. Median output length: 420 tokens. The team reviewed a sample of responses and found that many answers repeated the user's question, provided disclaimers that added no value, and included filler phrases like "It is important to note that" and "In conclusion."

They revised the prompt to emphasize conciseness: "Provide a direct answer without restating the question. Avoid filler phrases." They tested the change on an eval set. Median output length dropped to 280 tokens. User ratings stayed flat. The extra 140 tokens per response had been pure waste.

Output verbosity is harder to detect than input waste because users sometimes prefer longer, more detailed responses. You cannot optimize output tokens without checking user satisfaction. A/B testing is the gold standard: show some users the concise version, others the verbose version, and measure engagement, ratings, and task success.

## Token Waste in Retries and Error Handling

Retry logic is essential for handling transient API failures, but it also costs money. Every retry pays for input tokens, output tokens, and API overhead. When retries are applied indiscriminately, you pay for attempts that will never succeed.

A document processing pipeline retries failed requests up to three times. In October 2025, the team analyzed retry costs and found that 12 percent of total spend went to retries. Most retries were for transient errors and succeeded on the second attempt. But 30 percent of retries were for requests that failed consistently—malformed input, unsupported document types, oversized content.

The team added retry classification: transient errors (timeouts, rate limits) are retried. Non-transient errors (invalid input, unsupported format) are logged and rejected immediately. Retry-related spend dropped by 8 percentage points. Total cost dropped by 10 percent. The non-transient errors had been wasting tokens on attempts that could never succeed.

Retry budgets also help. Instead of retrying every request three times, set a per-request retry budget: two retries for high-priority users, one retry for standard users, zero retries for low-priority batch jobs. This reduces waste while preserving reliability for the requests that matter most.

## Token Waste from Over-Retrieval in RAG Systems

Retrieval-augmented generation systems retrieve relevant documents or chunks before generating a response. Retrieving too many chunks increases input tokens without improving quality.

A customer support bot retrieves ten document chunks per query. Each chunk is 300 tokens. Total retrieval cost: 3,000 input tokens. The team tested retrieval with different chunk counts: three chunks, five chunks, ten chunks. Quality with three chunks was 2 percentage points lower than ten chunks. Quality with five chunks was within 0.5 percentage points.

The team reduced retrieval to five chunks, cutting retrieval-related token usage by 50 percent. The slight quality loss was acceptable given the cost savings. They also implemented relevance filtering: if the retrieval step returns chunks with low similarity scores, the system excludes them even if fewer than five chunks remain. This prevents the model from processing irrelevant context.

Over-retrieval is common in RAG systems because teams default to conservative settings—better to include too much context than too little. But empirical testing usually reveals that half the retrieved context contributes little to quality. Measure quality at different retrieval depths. Find the minimum context that preserves performance.

## Prompt Compression Techniques

Prompt compression reduces token usage without changing prompt content. Techniques include removing unnecessary whitespace, shortening variable names, using abbreviations, and compressing repeated patterns.

A legal assistant uses JSON-formatted few-shot examples. The original formatting included indentation, newlines, and descriptive key names. Token cost per example: 180 tokens. The team compressed the JSON: removed indentation, removed unnecessary whitespace, shortened key names. Token cost per example: 110 tokens. The model still understood the format because the structure was intact.

Compression works for formatting and structured data, but it does not work for prose. Removing whitespace from natural language instructions can confuse the model and harm quality. Compression is a last resort after removing redundant content.

Some teams use external prompt compression tools that apply transformer-based summarization to reduce prompt length. These tools are experimental and can degrade quality unpredictably. Test thoroughly before deploying compression in production.

## Tracking Token Efficiency Over Time

Token waste analysis is not a one-time audit. Prompts evolve. New features add context. Retrieval logic changes. Token efficiency drifts. Continuous tracking catches waste before it compounds.

A fintech assistant tracks token efficiency metrics weekly: input tokens per request, output tokens per request, cost per request, quality score per request. When input tokens per request increase by more than 10 percent without a corresponding quality improvement, the team investigates.

In December 2025, input tokens per request increased from 1,800 to 2,100 over two weeks. Quality stayed flat. The team reviewed recent changes and found that a new feature had added a 300-token instruction block to every request, even for users who never used the feature. They moved the instruction block to a conditional path, triggered only when the feature is invoked. Token usage returned to baseline.

Tracking token efficiency alongside quality metrics ensures optimization does not sacrifice performance. If input tokens drop by 30 percent and quality drops by 5 percentage points, the optimization might not be worth it. If input tokens drop by 30 percent and quality stays flat, the optimization is a clear win.

## The ROI of Token Waste Elimination

Token waste elimination has high ROI because it reduces cost without requiring infrastructure changes, model retraining, or major rewrites. You edit prompts, test quality, and deploy. The savings are immediate and permanent.

A customer success platform spent $42,000/month on LLM API costs in September 2025. The team conducted a token waste audit over two weeks. They found redundant context in 20 percent of requests, verbose system messages in all requests, and unnecessary few-shot examples in three features. They implemented optimizations: deduplicated context, shortened system messages, reduced example counts.

By October, monthly spend was $31,000 with no measurable quality degradation. The changes saved $11,000/month, $132,000/year. The audit and implementation took 60 hours of engineering time. ROI: break-even in one week.

Token waste elimination scales with usage. A system processing 10 million requests per month with 2,000 tokens per request pays for 20 billion tokens. Reducing token usage by 10 percent saves 2 billion tokens per month, which at $3 per million input tokens is $6,000/month, $72,000/year. The savings grow with traffic.

## When Optimization Goes Too Far

Token waste elimination has limits. Not all token usage that seems unnecessary is actually waste. Some verbosity improves edge case handling. Some redundancy improves reliability. Some examples cover rare but important scenarios.

A content moderation system reduced few-shot examples from six to three, saving 450 tokens per request. Quality on the main eval set stayed flat. But after two weeks in production, the team noticed a 3 percentage point increase in false negatives for a rare category of policy violations. The rare category represented only 0.5 percent of traffic, so it had not appeared in the eval set in sufficient volume.

The team restored one of the removed examples—the one covering the rare category—and false negatives returned to baseline. The final prompt used four examples instead of six, still saving 300 tokens per request while preserving edge case handling.

The lesson: test optimizations on diverse eval sets that include rare cases, edge cases, and adversarial inputs. If your eval set only covers common cases, optimizations might harm rare-case performance without you noticing until production.

The next subchapter covers model cost comparison—measuring price-performance in production to decide whether switching models saves money without sacrificing quality.

