# 8.8 — Post-Incident Review: The AI-Specific Questions to Ask

The post-mortem meeting started forty-eight hours after the incident. Seven people in a conference room. The engineering manager opened with the standard question: "What happened?" The team recounted the timeline. Model deployment at 2:14 PM. Alert fired at 2:31 PM. Investigation started. Root cause identified at 3:18 PM. Rollback completed at 3:29 PM. Total duration: 75 minutes. The timeline was clear. Then the engineering manager asked: "Why did our eval not catch this before deployment?" Silence. No one had a good answer. The model had passed all pre-deployment eval. But it had failed in production within seventeen minutes. The gap between eval performance and production performance was the real lesson, but it took forty minutes of post-mortem discussion to articulate it. Traditional incident review questions — what went wrong, how did we fix it — were not enough for AI incidents. You need questions specific to model behavior, data quality, eval coverage, and the gap between testing and reality.

AI post-incident reviews must go deeper than traditional software post-mortems. Traditional reviews focus on code bugs, infrastructure failures, and deployment mistakes. AI reviews must examine why the model behaved differently than expected, why the eval did not predict the failure, what assumptions about model capabilities were wrong, and how the incident reveals gaps in monitoring or testing. The review is not just about fixing the immediate problem — it is about understanding what your eval, your monitoring, and your mental model of the system missed.

## The Model Behavior Question Block

The first set of questions examines what the model did and why. These questions are specific to AI and have no equivalent in traditional software.

Why did the model produce the incorrect outputs? Not just "because the model was wrong," but the mechanism. Did it hallucinate? Did it forget capabilities? Did it misinterpret instructions? Did it overfit to training data patterns? A healthcare documentation platform had an incident in March 2025 where the model started omitting medication sections from summaries. The post-incident review asked: why did the model omit medications? The investigation revealed that the latest training data had under-represented medication information because the data source had changed its formatting. The model learned that medication sections were less important. The behavior was not random — it was learned from biased training data. This understanding led to a fix: audit training data distribution before every fine-tuning run.

Did the model's behavior change suddenly or gradually? Sudden changes point to deployment or configuration issues. Gradual changes point to data drift, accumulating errors, or environmental shifts. A customer service chatbot had an incident in July 2025 where response quality degraded over three weeks before the incident was detected. The post-incident review asked: was this sudden or gradual? Logs showed a gradual decline. The model had been slowly losing capabilities as user queries shifted away from the distribution it was trained on. The gradual nature of the failure meant that detection thresholds were not sensitive enough. The fix: implement trend-based alerting that detects gradual degradation, not just sudden drops.

Were there early warning signs that the model was struggling? Lower confidence scores? Increased hedge language? Longer response times? A financial advisory platform had an incident in October 2025 where the model started generating incorrect investment recommendations. The post-incident review examined logs from the week before the incident. They found that confidence scores had been declining gradually for five days. The warning sign was there, but no one was monitoring confidence trends. The fix: add confidence score monitoring to the dashboard and alert when confidence drops over a rolling window.

## The Eval Gap Question Block

The second set of questions examines why the eval did not predict or catch the failure. Eval gaps are the most common root cause of AI incidents making it to production.

Did the model pass eval before deployment? If yes, why did eval not detect the issue? A legal document assistant had an incident in December 2025 where a model deployment passed eval with 94 percent accuracy but failed in production with 79 percent accuracy on real user queries. The post-incident review asked: why the gap? The investigation revealed that the eval dataset was not representative of recent user queries. Users had started asking more complex multi-clause questions. The eval dataset had only simple single-clause questions. The model passed eval because eval did not test the capabilities users needed. The fix: update eval dataset every two weeks to match evolving user query patterns.

Was the eval dataset representative of production traffic? Not just the overall distribution, but the long tail, the edge cases, the difficult examples. A travel booking assistant had an incident in May 2025 where the model failed on international hotel bookings but worked fine on domestic bookings. The eval dataset was 95 percent domestic bookings because that was the historical majority. The 5 percent international representation was not enough to detect the failure mode. The fix: oversample rare but critical query types in eval to ensure adequate coverage even when they are a small percentage of production traffic.

Did the eval measure the right thing? A document summarization platform had an incident in August 2025 where the model generated summaries that were fluent and grammatically correct but factually inaccurate. The eval measured fluency. It did not measure factual accuracy. The model passed eval because eval did not test the dimension on which it failed. The fix: add factual accuracy checks to eval, including comparing summary claims to source documents.

## The Detection Latency Question Block

The third set of questions examines how long it took to detect the incident and why.

How long between the first failure and detection? If the gap is more than a few minutes, why? A customer service chatbot had an incident in November 2025 where the model started producing inappropriate responses at 2:14 PM but was not detected until 2:47 PM. The post-incident review asked: why did detection take thirty-three minutes? The investigation found that eval ran every ten minutes, so there was up to ten minutes of eval latency. Then the eval results had to aggregate across five samples before alerting, adding another six minutes. Then the alert went to Slack, and no one was actively monitoring. The detection latency was the sum of structural delays. The fix: move to continuous eval on sampled traffic, reduce aggregation window to three samples, and send alerts to PagerDuty instead of Slack.

Were there leading indicators that were not monitored? Confidence scores dropping? User complaints increasing? Retry rates spiking? A healthcare documentation platform had an incident in April 2025. The post-incident review analyzed metrics from before the incident. They found that user edit rates — how often users manually edited AI-generated summaries — had increased by 40 percent in the two hours before the incident was detected. The leading indicator was there, but no one monitored edit rates. The fix: add user edit rate to the dashboard and alert when it spikes.

Did users notice before the monitoring did? If yes, your monitoring is insufficient. A contract analysis platform had an incident in September 2025 where users reported incorrect outputs twenty-two minutes before the monitoring detected the issue. The post-incident review asked: why did users detect it first? The answer: the failure mode was subtle domain-specific inaccuracy that the generic eval did not catch. Users had domain expertise. The eval did not. The fix: add domain expert review to the eval process and implement user feedback monitoring as a detection signal.

## The Response Efficiency Question Block

The fourth set of questions examines how well the team responded once the incident was detected.

What was the timeline from detection to mitigation? Break it down: How long for triage? How long for investigation? How long for mitigation? A financial advisory platform had an incident in February 2026 where total duration was 104 minutes: eight minutes for triage, forty-one minutes for investigation, eighteen minutes for mitigation, and thirty-seven minutes for validation and communication. The post-incident review asked: where can we improve? Investigation was the longest phase. Why? The team had tested hypotheses sequentially instead of in parallel. The fix: for future incidents, assign different engineers to test different hypotheses simultaneously.

Did the team have the tools and access needed to investigate quickly? A document analysis platform had an incident in June 2025 where the on-call engineer spent nineteen minutes waiting for database query results during investigation. The database was slow. The engineer did not have access to faster diagnostic tools. The post-incident review identified tooling as a bottleneck. The fix: build faster diagnostic queries and give all on-call engineers access to query optimization tools.

Were there delays caused by coordination or communication issues? A legal document assistant had an incident in January 2026 where the engineering team and the infrastructure team were investigating the same issue independently for twenty-three minutes before they realized it. The post-incident review identified the coordination failure. The fix: enforce strict incident channel discipline — all investigation happens in the incident channel, no side conversations, no independent investigation without posting in the channel.

## The Mitigation Effectiveness Question Block

The fifth set of questions examines whether the mitigation strategy was optimal and whether it worked as expected.

Was the first mitigation attempt successful? If not, why? A customer service chatbot had an incident in March 2025 where the first mitigation attempt was a rollback. The rollback did not fix the issue because the bug existed in multiple versions. The team had to roll back two more versions before finding a clean state. The post-incident review asked: why did the first rollback fail? The answer: version history was not well-documented, and the team did not know which versions were affected. The fix: maintain a detailed changelog for every model version with known issues tagged.

Could the mitigation have been faster? A travel booking assistant had an incident in July 2025 where mitigation took fourteen minutes because the team had to manually deploy a rollback. The post-incident review asked: could we have been faster? The answer: yes, if we had one-click rollback infrastructure. The fix: implement automated rollback with a single button in the deployment dashboard.

Did the mitigation introduce new problems? A healthcare documentation platform had an incident in October 2025 where they rolled back a model. The rollback fixed the immediate issue but reintroduced a bug that had been fixed in the newer version. The post-incident review identified the regression. The fix: before rolling back, check the changelog for what bugs the rollback will reintroduce, and decide whether the tradeoff is acceptable.

## The Root Cause Depth Question Block

The sixth set of questions examines whether the identified root cause is deep enough or just a surface-level explanation.

Why did the root cause occur? Not just what it was, but why it happened. A contract analysis platform had an incident in December 2025. The root cause was "training data contained duplicates." The post-incident review asked: why did training data contain duplicates? The answer: the data pipeline did not deduplicate. Why did the data pipeline not deduplicate? Because deduplication was considered optional and deprioritized during development. Why was it deprioritized? Because the team did not know that duplicates could cause model degradation. The deeper root cause was a knowledge gap about training data quality requirements. The fix: training for all engineers on the effects of data quality issues on model behavior.

Was this a one-time failure or a systemic issue? A financial advisory platform had an incident in May 2025 where a configuration error caused the model to use stale data. The post-incident review asked: was this a one-time mistake or a systemic issue? The investigation found that configuration management was ad hoc. Multiple engineers could change configurations with no review process. The systemic issue was lack of configuration governance. The fix: implement configuration-as-code with mandatory review for all configuration changes.

Have we had similar incidents before? If yes, why did the previous fix not prevent this recurrence? A customer service chatbot had an incident in August 2025 that was similar to an incident six months earlier. The post-incident review asked: why did this happen again? The previous incident had been fixed with a one-off patch. The patch addressed the specific symptoms but not the underlying pattern. The fix: when addressing repeat incidents, look for patterns and implement structural changes, not point fixes.

## The Human Factors Question Block

The seventh set of questions examines human decisions, assumptions, and knowledge gaps that contributed to the incident.

What assumptions did the team make that turned out to be wrong? A legal document assistant had an incident in November 2025 where the team assumed that the model would generalize from training data to slightly different query formats. The assumption was wrong. The model overfitted and performed poorly on format variations. The post-incident review identified the assumption and asked: why did we assume this? The answer: the team lacked experience with how fine-tuned models behave on out-of-distribution inputs. The fix: build institutional knowledge about model generalization limits through training and documentation.

Were there knowledge gaps in the team? A healthcare documentation platform had an incident in April 2025 where no one on the engineering team understood the medical domain well enough to recognize that the model's outputs were clinically wrong. The post-incident review identified the knowledge gap. The fix: add domain experts to the on-call rotation and ensure every incident response includes someone with domain expertise.

Did time pressure or external factors influence decisions? A travel booking assistant had an incident in February 2026 where the team deployed a model update without full eval coverage because of pressure to meet a product deadline. The deployment caused an incident. The post-incident review asked: why did we deploy without full eval? The answer: deadline pressure. The fix: establish a policy that no deployment happens without complete eval, regardless of deadlines. If the deadline is critical, extend it or reduce scope. Never skip eval.

## The Preventability Question Block

The eighth set of questions examines whether the incident could have been prevented and how.

Could this incident have been caught in pre-production testing? If yes, why was it not? A document summarization platform had an incident in September 2025 where the model failed on long documents. The issue could have been caught in testing if the team had tested long documents. The post-incident review asked: why did we not test long documents? The answer: the test suite focused on average cases, not edge cases. The fix: expand test suite to include edge cases for every known failure mode.

Were there procedural violations that contributed to the incident? A customer service chatbot had an incident in June 2025 where an engineer deployed a model update directly to production without going through the canary process. The direct deployment caused widespread failure. The post-incident review asked: why was the process bypassed? The engineer said the change seemed small and low-risk. The fix: enforce process compliance through technical controls, not just policy. Make it impossible to deploy to production without passing through canary first.

Is there a specific change we can make to prevent this class of incident in the future? A financial advisory platform had an incident in March 2026 where the model used outdated financial data. The post-incident review asked: how do we prevent this? The fix: implement data freshness checks in the model serving pipeline. If the most recent data is older than twenty-four hours, refuse to serve and alert. The specific change addressed the entire class of stale data incidents.

## The Organizational Learning Question Block

The ninth set of questions examines what the organization should learn from the incident and how to disseminate that learning.

What should we change about our processes, tooling, or architecture? A legal document assistant had an incident in January 2026 that revealed a gap in rollback capabilities. The post-incident review identified three changes: implement one-click rollback, maintain parallel model versions during deployments, and build automated canary testing. Each change was assigned an owner and a deadline.

Who else in the organization needs to know about this incident? A healthcare documentation platform had an incident in July 2025 that involved a failure mode no one had anticipated. The post-incident review decided to share the incident report with all product and engineering teams to ensure broad awareness of the failure mode. The dissemination prevented three other teams from making the same mistake.

Should this incident inform our hiring or training priorities? A contract analysis platform had an incident in October 2025 that revealed a gap in the team's understanding of prompt engineering best practices. The post-incident review led to a decision: invest in prompt engineering training for all engineers and hire a prompt engineering specialist. The incident informed talent strategy.

## The Blamelessness Principle

Post-incident reviews must be blameless. The goal is learning, not punishment. If the review becomes a blame session, people will hide mistakes and the organization will not learn.

A customer service chatbot had an incident in December 2025 caused by an engineer making a configuration error. The post-incident review focused on the question: why was it possible to make this error? Not: why did the engineer make this error? The review identified that configuration files were unvalidated and undocumented. The fix: implement configuration validation and improve documentation. The engineer who made the error was not blamed. The system that allowed the error was fixed.

Blamelessness does not mean no accountability. If an engineer consistently ignores process or makes reckless decisions, that is a performance issue handled separately. But the post-incident review is not the venue for that conversation. The review is about system improvement. A financial advisory platform had an incident in May 2025 where an engineer deployed without following the release checklist. The post-incident review asked: why is it possible to deploy without the checklist? Not: why did this engineer ignore the checklist? The fix: enforce checklist completion through automated gates in the deployment pipeline.

## The Action Items and Follow-Through

The post-incident review must produce specific, assigned, time-bound action items. Vague intentions like "we should be more careful" accomplish nothing.

A travel booking assistant had an incident in August 2025. The post-incident review identified eight action items: implement confidence score monitoring, update eval dataset with recent query patterns, add factual accuracy checks to eval, enable one-click rollback, document model version history, train engineers on data quality, add domain experts to on-call rotation, and improve alerting speed. Each action item was assigned to an owner with a deadline. Within six weeks, all eight were complete. The next incident had better detection, better tooling, and better response.

Action items should be tracked visibly. A healthcare documentation platform maintained a post-incident action item board visible to the entire team. Every action item from every incident was tracked until completion. The board created accountability. When an action item lingered for weeks, the delay was visible and prompted escalation.

## The Post-Incident Report

The post-incident review should produce a written report: timeline, root cause, user impact, mitigation, lessons learned, and action items. The report becomes organizational knowledge.

A legal document assistant maintained a library of incident reports. When new engineers joined, they read the past six months of incident reports as part of onboarding. The reports taught new engineers about failure modes, response procedures, and system weaknesses. The library turned incidents into education.

The report should be shared widely. A contract analysis platform shared every incident report with engineering, product, customer success, and executive leadership. The broad distribution ensured that everyone understood system risks and recent issues. A financial advisory platform even shared sanitized incident reports with customers as part of transparency efforts. The reports demonstrated that the company took reliability seriously and learned from failures.

## The Incident Taxonomy Update

Each incident should inform your incident taxonomy. If you encountered a new failure mode, add it to the taxonomy so future responders recognize it quickly.

A document summarization platform had an incident in November 2025 involving a new failure mode: context window truncation causing incomplete outputs. The post-incident review added "context window truncation" to the incident taxonomy with symptoms, diagnostic steps, and mitigation strategies. Two months later, a similar incident occurred. The on-call engineer recognized the symptoms immediately, followed the documented diagnostic steps, and resolved the incident in fourteen minutes. The taxonomy turned past incidents into future response efficiency.

Once you understand what happened and why, the next step is quantifying incident response performance. How fast did you detect the issue? How fast did you resolve it? What are your incident metrics showing about system reliability?

