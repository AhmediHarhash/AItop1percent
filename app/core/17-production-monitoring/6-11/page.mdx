# 6.11 — Agent Reliability Scoring: Composite Health Metrics

Your agent succeeded. But did it succeed well? It completed the task in seventeen steps when it should have taken seven. It made three self-corrections. Two tool calls failed and were retried. The final answer was correct but the trajectory was inefficient. Traditional success-or-failure metrics call this a success. Agent reliability scoring calls this marginal performance.

Agent reliability scoring combines multiple signals — step efficiency, tool success rates, reasoning coherence, cost, latency, and self-correction frequency — into a composite health metric. The score reflects not just whether the agent completed the task, but how well it completed the task. High scores indicate efficient, confident reasoning. Low scores indicate the agent struggled even if it eventually succeeded.

## Why Binary Success Metrics Fail for Agents

A single-call model either succeeds or fails. The prompt is processed, the response is returned, and you measure accuracy. An agent can succeed in dozens of ways — efficiently or wastefully, confidently or hesitantly, cheaply or expensively. Binary success metrics hide this variation.

A legal document agent in late 2025 had a ninety-four percent success rate. Management was satisfied. Engineering was not. Manual review of successful requests showed wide quality variation. Some requests were resolved in four steps with clean reasoning. Others took nineteen steps with redundant queries, contradictory intermediate conclusions, and excessive self-correction. Both counted as successes, but one was clearly better than the other.

The team implemented reliability scoring: each successful request received a score from 0 to 100 based on trajectory quality. Scores above 80 indicated excellent performance. Scores between 60 and 80 indicated acceptable performance. Scores below 60 indicated marginal performance — the agent succeeded, but barely. This revealed that while ninety-four percent of requests succeeded, only seventy-one percent scored above 80. The headline success rate masked underlying quality problems.

## Components of a Reliability Score

A reliability score aggregates multiple performance dimensions. No single metric captures agent health. You need a composite that reflects the full picture.

**Step efficiency** contributes to the score. Agents that complete tasks in fewer steps score higher. You calculate efficiency as the ratio of actual steps to expected steps for that request type. An agent that uses seven steps for a task that typically takes six steps scores well on efficiency. An agent that uses fifteen steps scores poorly.

**Tool success rate** contributes to the score. Agents that succeed at every tool call score higher than agents that fail and retry. You track per-request tool success rate — the percentage of tool calls that succeeded on the first attempt — and penalize agents for failures even if they eventually recovered.

**Reasoning coherence** contributes to the score. Agents that follow logical trajectories score higher than agents that oscillate or contradict themselves. Coherence is measured through self-correction frequency, contradiction detection, and consistency checks. Fewer self-corrections mean higher coherence.

**Cost efficiency** contributes to the score. Agents that complete tasks within expected cost score higher than agents that burn excessive tokens. You measure cost as a ratio of actual cost to expected cost for the request type.

**Latency efficiency** contributes to the score. Agents that complete tasks within expected latency score higher than agents that take unusually long. You measure latency as a ratio of actual time to expected time.

**Final answer quality** contributes to the score. Even if the trajectory was efficient, if the final answer is wrong or low-quality, the score suffers. You measure quality through user feedback, validation checks, or ground truth comparison when available.

## Calculating the Composite Score

You weight each component and compute a weighted average. The weights reflect your priorities. A cost-sensitive system might weight cost efficiency at thirty percent. A latency-sensitive system might weight latency efficiency at forty percent. You tune the weights based on what matters most to your use case.

A customer service agent in early 2026 used the following scoring formula: step efficiency weighted at twenty percent, tool success at twenty percent, reasoning coherence at fifteen percent, cost efficiency at twenty percent, latency efficiency at fifteen percent, and final answer quality at ten percent. Each component was scored from 0 to 100, and the weighted average produced the overall reliability score.

A request that used optimal steps, succeeded at all tool calls, showed no self-corrections, stayed within budget and latency targets, and produced a correct answer scored 95 or higher. A request that took twice the expected steps, failed two tool calls, self-corrected three times, and exceeded cost by fifty percent but ultimately produced a correct answer might score 62 — technically successful but unreliable.

## Score Distribution and Thresholds

You define score ranges that map to quality tiers. Scores above 85 are excellent. Scores between 70 and 85 are good. Scores between 50 and 70 are marginal. Scores below 50 are poor, and you investigate these even if they technically succeeded.

A fraud detection agent in mid-2025 tracked score distributions. Eighty-two percent of requests scored above 85. Fourteen percent scored between 70 and 85. Three percent scored between 50 and 70. One percent scored below 50. The team focused on the bottom four percent — requests that succeeded but with poor reliability — to identify failure patterns and improve agent logic.

## Using Scores to Prioritize Debugging

When something goes wrong, you start with the lowest-scoring requests. These are the cases where the agent struggled most. Debugging low-score successes often reveals latent bugs or inefficiencies that will eventually cause outright failures.

A hiring agent in early 2026 had a small number of requests that succeeded but scored below 45. Investigation revealed these requests involved resumes with unusual formatting. The agent succeeded by brute-forcing its way through multiple parsing attempts, retrying tools, and self-correcting repeatedly. The success masked a parsing problem. Fixing the parser improved scores for those requests from 45 to 82 and reduced median latency by 0.7 seconds across all requests.

## Real-Time Scoring for In-Flight Intervention

Some teams compute reliability scores in real time as the agent executes. If the score drops below a threshold mid-trajectory, the system intervenes — pausing the agent, escalating the request, or injecting corrective guidance.

Real-time scoring requires computing each component incrementally. After each step, recalculate step efficiency, tool success rate, and reasoning coherence. If the running score drops below 40, the agent is struggling. You can terminate it early and escalate rather than letting it burn more cost on a low-quality trajectory.

A legal research agent in mid-2025 implemented real-time scoring. If the score fell below 35 at any point, the agent paused and a human reviewer assessed the trajectory. Should the agent continue, replan, or escalate immediately? Thirty-eight percent of paused cases were escalated. Sixty-two percent were given corrective guidance and allowed to continue. Real-time scoring prevented low-quality completions and reduced wasted effort.

## Score Trends Over Time

Reliability scores should remain stable or improve over time. If median score declines, something is degrading — model performance, tool quality, or task complexity. You track score trends and investigate when trends move in the wrong direction.

A customer support agent in early 2026 started with a median reliability score of 84. Over six weeks, the score drifted down to 76. No single metric flagged the decline, but the composite score revealed a trend. Investigation showed that tool latencies had increased gradually, pushing latency efficiency down. Separately, the agent was self-correcting more frequently, reducing reasoning coherence scores. Both issues were subtle but compounding. The team addressed tool performance and retrained the agent to reduce self-correction, restoring the median score to 82.

## Per-Request-Type Score Benchmarks

Not all request types should score identically. Simple requests should score higher than complex requests. You establish baseline scores per request type and compare individual requests to their category baseline.

A financial advisory agent in mid-2025 handled three request types: account balance inquiries, transaction history reviews, and investment recommendations. Account balance inquiries scored a median of 92 — simple, fast, few steps. Transaction history scored a median of 81 — more complexity, more data retrieval. Investment recommendations scored a median of 73 — high complexity, multiple steps, reasoning-heavy. Each category had its own baseline. A balance inquiry that scored 68 was underperforming. An investment recommendation that scored 68 was within expected range.

## Identifying High-Score Patterns

High-scoring requests show you what good looks like. You analyze them to identify patterns: which tools were used, in what order, with what parameters. High-score trajectories become reference trajectories for training and evaluation.

A document analysis agent in early 2026 extracted high-scoring trajectories — those scoring above 90 — and used them to train the agent's planning logic. The agent learned: high-score trajectories use these tools in this order, with this level of redundancy, and this amount of self-correction. Over time, the agent's behavior shifted toward the high-score patterns, and median score climbed from 78 to 84.

## Penalizing Overconfidence

An agent that expresses high confidence but scores low on reasoning coherence or tool success is overconfident. You detect this by comparing expressed confidence to trajectory quality. If the agent claims ninety percent confidence but the reliability score is 58, penalize the confidence calibration.

A medical triage agent in mid-2025 output confidence scores with each recommendation. Reliability scoring tracked whether confidence aligned with trajectory quality. High-confidence, low-score cases were flagged as miscalibrated. The agent was retrained to temper confidence when trajectories were inefficient or involved multiple tool failures. Post-retraining, confidence scores better matched reliability scores, improving user trust.

## Score-Based Routing

Some systems use reliability scores to route requests. If an agent completes a request with a score below 60, the result is flagged for human review before delivery. If the score is above 85, the result is delivered immediately. This ensures that marginal completions receive additional validation.

A loan underwriting agent in early 2026 used score-based routing. Applications processed with scores above 80 proceeded to the next stage automatically. Applications with scores between 50 and 80 were reviewed by a human underwriter who validated the agent's work. Applications with scores below 50 were fully reprocessed by humans. This tiered approach balanced automation with oversight.

## Communicating Scores to Stakeholders

Reliability scores are useful for engineering teams, but they can also inform business stakeholders. Instead of reporting "94 percent success rate," you report "94 percent success rate with a median reliability score of 78, indicating that while most requests succeed, trajectory quality has room for improvement."

This gives stakeholders a more honest picture of system health. High success rates with low reliability scores indicate fragile success — the agent is succeeding, but inefficiently or inconsistently. Stakeholders can make better decisions about whether to invest in improving the agent or accept current performance.

## Score Normalization Across Agent Types

If you operate multiple agents, you normalize scores to enable comparison. An agent handling simple tasks might score higher than an agent handling complex tasks, but that does not mean the simple-task agent is better. You normalize by adjusting for task difficulty.

A company in mid-2025 operated five agents: account lookup, order status, returns processing, technical troubleshooting, and billing disputes. Account lookup scored a median of 91. Billing disputes scored a median of 68. Normalizing for complexity, both agents performed similarly relative to their task difficulty. Normalization prevented misleading comparisons where simple-task agents always looked better than complex-task agents.

## Score Feedback Loops

Reliability scores improve when they are used to guide improvements. You log scores, analyze low-score patterns, implement fixes, and measure whether scores improve. This feedback loop continuously refines agent performance.

A sales agent in early 2026 logged scores for every request. Weekly reviews identified the lowest-scoring ten percent. Engineering analyzed those requests, identified common failure modes, and deployed fixes. Over twelve weeks, median score climbed from 72 to 81. The feedback loop — measure, analyze, fix, remeasure — was the mechanism for improvement.

## Balancing Comprehensiveness with Simplicity

Reliability scoring can become arbitrarily complex. You can add dozens of metrics, intricate weighting schemes, and sub-scores for every conceivable dimension. The risk is over-complexity: a score that is too complicated to interpret or tune.

You balance comprehensiveness with simplicity by starting with five or six core components, assigning intuitive weights, and iterating based on whether the score aligns with manual quality assessments. If low-scoring requests are actually low-quality upon review, the score is working. If low-scoring requests look fine, your weights or components need adjustment.

## The Reliability Score as a North Star

A single composite score gives teams a shared target. Instead of optimizing for step depth, cost, latency, and quality separately — which often conflict — the team optimizes for the reliability score, which balances all dimensions. The score becomes the north star metric for agent performance.

A healthcare agent system in early 2026 adopted reliability score as its primary performance metric. Engineering, product, and operations aligned on improving the score. Cost-saving measures that degraded the score were rejected. Latency optimizations that improved the score were prioritized. The score unified decision-making across teams.

Agent reliability is not binary. It is a spectrum. Scoring that spectrum gives you the visibility to distinguish excellent performance from marginal performance, to identify improvement opportunities, and to communicate system health honestly to stakeholders. Agents that succeed are not all succeeding equally well. Reliability scoring measures the difference.

