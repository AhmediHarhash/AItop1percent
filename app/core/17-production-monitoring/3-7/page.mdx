# 3.7 — Groundedness and Hallucination Detection in Production

Why do RAG systems produce outputs that sound authoritative but cite documents that do not exist? The model received retrieved context, generated an answer, and then fabricated a source that seems plausible. This is a groundedness failure. The response is not anchored to the provided evidence. The model hallucinated content that appears true but is not supported by any document it had access to. In low-stakes applications, this is annoying. In legal, medical, or financial applications, this is professional negligence.

Groundedness measures whether a model's output is supported by the evidence it was given. A grounded response makes claims that are directly supported by retrieved documents, provided context, or verifiable knowledge. An ungrounded response makes claims that are plausible but unsupported, exaggerated, or invented. Hallucination is the extreme case of ungroundedness — the model generates information that is factually incorrect and not present in any provided source. You cannot prevent hallucinations entirely, but you can detect them in production, measure their frequency, and intervene before they reach users.

## Defining Groundedness for Your System

Groundedness is not one thing. It depends on what evidence the model is supposed to use and how strictly it must adhere to that evidence.

**Strict groundedness** requires every claim in the response to be explicitly supported by provided documents. If the model says "The policy allows X," there must be a sentence in a retrieved document that says the policy allows X. Strict groundedness is appropriate for compliance-sensitive tasks where users need to trust that every statement is backed by authoritative sources. The cost is that strictly grounded responses are often verbose, hedged, and incomplete when the documents do not fully answer the question.

A legal contract review tool enforces strict groundedness. Every clause recommendation must be supported by a cited precedent or regulation. If the model cannot find support for a recommendation, it does not make the recommendation. Users trust the tool because they know unsupported advice is not delivered. The trade-off is that the tool sometimes says "I cannot answer this based on available documents" when a human lawyer would make an informed judgment.

**Soft groundedness** allows the model to synthesize information, draw reasonable inferences, and fill gaps with general knowledge as long as core claims are supported. If the model says "The policy allows X, which typically means Y," X must be supported by documents, but Y can be a reasonable inference or general knowledge. Soft groundedness is appropriate for advisory tasks where users expect helpful context beyond strict document quotation.

A customer support chatbot uses soft groundedness. When answering a question about return policies, it must cite the actual policy from company documents. But it can also explain what "within thirty days" means in practice, even if the document does not spell it out. Soft groundedness balances trustworthiness with usability.

**Verifiable groundedness** applies when the model cites external sources like research papers, case law, or news articles. The response includes citations. Groundedness means those citations are real, accessible, and actually support the claims attributed to them. The model might say "According to Smith et al. 2023, dropout rates exceed forty percent." Verifiable groundedness requires that Smith et al. 2023 exists, is retrievable, and actually reports that finding.

A medical research assistant enforces verifiable groundedness. Every factual claim about clinical outcomes or treatment efficacy must include a citation to a peer-reviewed source. The system checks that the cited paper exists in PubMed or a similar database. It does not verify the claim itself — that would require reading and interpreting the full paper — but it verifies the citation is real and relevant.

## Measuring Groundedness with Automated Checks

Groundedness can be measured automatically using entailment models, citation verification, and claim extraction.

**Natural language inference models** measure whether a response is entailed by the provided context. An NLI model takes two inputs: a premise and a hypothesis. It predicts whether the hypothesis is entailed by, contradicts, or is neutral to the premise. In groundedness checking, the retrieved documents are the premise, and each claim in the response is the hypothesis. If the response is entailed by the documents, it is grounded. If it contradicts the documents, it is ungrounded. If it is neutral, the model made claims not supported by evidence.

A financial advice chatbot uses an NLI model to check groundedness. For every generated response, the system extracts individual claims and tests them against retrieved documents. Claims labeled as entailed are considered grounded. Claims labeled as contradictory or neutral are flagged as potentially ungrounded. The system logs the percentage of grounded claims per response. Responses where fewer than eighty percent of claims are grounded are flagged for human review.

NLI models are not perfect. They struggle with complex reasoning, implicit claims, and domain-specific language. A legal NLI model fine-tuned on case law performs far better than a generic NLI model trained on general text. Fine-tuning on your domain improves accuracy.

**Citation extraction and verification** measures whether the model's citations are real and relevant. Parse the response for citation patterns — case names, paper titles, URLs, regulation numbers. Check each citation against authoritative databases. Does the case exist in legal databases? Does the paper exist in PubMed or arXiv? Does the URL resolve? If a citation is fabricated, the response is ungrounded.

A legal research assistant extracts case citations from every response. It queries legal databases to verify that each cited case exists and is accessible. Fabricated case names are caught immediately. The system also checks whether the case is relevant to the query topic using keyword matching or embedding similarity. A case that exists but is irrelevant is flagged as a weak citation. Over two months, three percent of responses contain at least one fabricated case citation. Those responses are blocked from users and used as negative examples for model fine-tuning.

**Claim-level attribution scoring** measures whether specific claims in the response are supported by specific sentences in the retrieved documents. Extract claims from the response using a parser or an LLM. For each claim, compute similarity to sentences in the retrieved documents. If the most similar sentence has high similarity — above 0.85 in embedding space — the claim is considered attributed. If similarity is low, the claim is unsupported.

A medical Q and A system uses claim-level attribution scoring. For each response, an LLM extracts factual claims like "Metformin is a first-line treatment for type 2 diabetes." The system computes embedding similarity between each claim and sentences in retrieved clinical guidelines. Claims with similarity above 0.9 are marked as strongly supported. Claims with similarity between 0.7 and 0.9 are marked as weakly supported. Claims below 0.7 are marked as unsupported. The system logs the distribution of support levels. A response with more than two unsupported claims is flagged.

## Real-Time Groundedness Scoring in Production

Groundedness checks can run inline or in batch. Inline checks block ungrounded responses before they reach users. Batch checks measure groundedness trends without adding latency.

**Inline groundedness filters** evaluate groundedness before returning the response. If groundedness score is below a threshold, the response is not sent. The user receives a fallback response like "I could not find sufficient evidence to answer this question" or the request is escalated to a human. Inline filtering prevents hallucinations from reaching users but adds latency and requires fast groundedness models.

A compliance assistant runs inline NLI checks on every response. Responses where fewer than ninety percent of claims are entailed by retrieved documents are blocked. The user is told "I cannot confidently answer this based on available compliance documents. Let me connect you with a specialist." Inline filtering reduces hallucination-related user complaints by eighty-two percent in the first month.

Inline filters must be fast. A groundedness check that takes three seconds is too slow for interactive applications. Use lightweight NLI models, cache frequent claim-document pairs, and run checks asynchronously if user-facing latency is critical. Some systems return the response immediately and run groundedness checks in the background, flagging ungrounded responses in logs and alerting human reviewers after the fact.

**Batch groundedness analysis** runs on sampled production traffic after responses are delivered. Select a few hundred responses per day, run thorough groundedness checks, and log the results. Batch analysis does not block responses, but it surfaces trends. A gradual increase in ungrounded claims over two weeks is an early warning that the model or retrieval pipeline is degrading.

A legal research tool samples two hundred responses per day for groundedness analysis. Each response is checked for citation accuracy, claim entailment, and relevance. The percentage of responses with at least one ungrounded claim is plotted over time. The baseline is seven percent. In early 2026, the percentage climbs to fourteen percent over ten days. Investigation reveals that a retrieval index update introduced lower-quality documents. Groundedness monitoring catches the regression before it causes major user complaints.

## Hallucination Detection: The Extreme Case

Hallucinations are ungrounded claims that are also factually incorrect. A model that says "The capital of France is Berlin" is hallucinating. A model that invents a case name that does not exist is hallucinating. A model that fabricates a clinical trial result is hallucinating. Hallucination detection requires not just checking whether a claim is supported by documents but whether it is factually correct.

**Factual consistency models** detect contradictions between generated text and source documents. These models are trained to recognize when a generated statement contradicts or misrepresents information in the provided context. They are particularly effective for summarization tasks where the model is expected to restate document content without adding or altering facts.

A contract summarization tool uses a factual consistency model to check every summary against the original contract. If the summary says "The contract term is two years" but the contract says "The contract term is three years," the consistency model flags the hallucination. Detected hallucinations are blocked, logged, and used to retrain the model.

**Cross-checking against knowledge bases** verifies factual claims when you have a structured knowledge source. If the model claims "Product X costs forty dollars," check that claim against the product database. If the database says thirty-five dollars, the model hallucinated. Cross-checking works for any domain where authoritative data is available and queryable.

A customer support chatbot answers questions about product features and pricing. Every factual claim about products is automatically cross-checked against the product database. The model says "The Pro plan includes unlimited storage." The database says the Pro plan includes one terabyte of storage. The claim is flagged as incorrect, and the response is rewritten before sending. Cross-checking catches ninety-one percent of factual errors about structured product data.

**User correction signals** are indirect hallucination detectors. When users report that a response is incorrect, escalate to human support after receiving an answer, or leave negative feedback mentioning "wrong" or "incorrect," these are signals that the response might contain hallucinations. Correlate user correction signals with automated groundedness scores. Responses that score low on groundedness and also trigger user corrections are very likely to contain hallucinations.

A tax preparation assistant tracks user feedback and correlates it with groundedness scores. Responses with groundedness scores below 0.7 have a twenty-eight percent user correction rate. Responses with scores above 0.9 have a three percent correction rate. The correlation validates the groundedness metric. The team sets an alert threshold at 0.75 — responses below that score are flagged for review.

## Logging and Analyzing Groundedness Over Time

Groundedness is not binary. It is a distribution. Most responses are well-grounded. Some are partially grounded. A small percentage are completely ungrounded. Tracking the distribution over time reveals quality trends.

**Track groundedness as a continuous metric**. Log a groundedness score for every response — or for a sampled subset of responses. Compute mean groundedness, median groundedness, and the percentage of responses below critical thresholds. Plot these metrics as time series. A gradual decline in mean groundedness indicates systemic degradation. A sudden drop indicates a deployment problem.

A medical chatbot logs NLI-based groundedness scores for five hundred responses per day. Mean groundedness holds steady at 0.89 for three months. In mid-January 2026, mean groundedness drops to 0.76 over one week. Investigation reveals a prompt template change that encouraged the model to provide more context and background information. The added context was helpful but not grounded in retrieved documents. The team revises the prompt to emphasize grounding, and scores recover.

**Segment groundedness by query type and retrieval quality**. Not all queries are equally easy to ground. Queries on common topics with abundant documentation have high groundedness. Queries on rare topics with sparse documentation have lower groundedness. Segment groundedness metrics by retrieval relevance, query frequency, and document coverage. This reveals where groundedness problems are concentrated.

A legal research tool segments groundedness by practice area. Contract law queries have a mean groundedness score of 0.91. Intellectual property queries have a mean score of 0.79. The IP retrieval index is less comprehensive. The model struggles to ground claims when documents are sparse. The insight prompts the team to expand the IP index and retrain the model to better handle sparse evidence.

**Identify high-risk ungrounded patterns**. Cluster ungrounded responses to find common failure modes. Do ungrounded responses share common topics, common phrasing, or common retrieval characteristics? A pattern of ungrounded responses on a specific topic indicates a retrieval gap or a knowledge gap. A pattern of ungrounded responses using specific phrasing indicates the model learned to generate that phrasing without requiring supporting evidence.

A customer support chatbot clusters ungrounded responses. One cluster involves questions about product compatibility. The model consistently generates definitive statements like "Product A is fully compatible with Product B" without citing any documentation. The model learned this phrasing from training data, but product compatibility information is not in the retrieval index. The team adds compatibility data to the index and retrains the model to hedge when evidence is absent.

## Intervention Strategies for Ungrounded Responses

Detecting ungrounded responses is only useful if you act on the detection. There are multiple intervention points.

**Blocking ungrounded responses before delivery**. Set a groundedness threshold. Responses below the threshold are not sent to users. This is the most conservative strategy. It prevents hallucinations but might frustrate users with excessive "I cannot answer this" responses. Use blocking for high-stakes domains where hallucinations are unacceptable.

A healthcare advice chatbot blocks responses with groundedness scores below 0.85. Users receive a message: "I could not find reliable evidence to answer this question. Please consult a healthcare professional." Blocking reduces hallucination-related user complaints to near zero but increases escalation to human support by twelve percent. The team considers the trade-off acceptable for a medical application.

**Hedging and disclaimers for borderline groundedness**. Responses with moderate groundedness scores are delivered with caveats. "Based on available information, the answer may be X" or "I found limited evidence, but it suggests Y." Hedging preserves utility while signaling uncertainty.

A legal research tool delivers responses with groundedness scores between 0.7 and 0.85 but adds a disclaimer: "This answer is based on limited case law. Consult a legal professional for definitive advice." Users appreciate the honesty. They can choose to act on the information or seek additional confirmation.

**Post-delivery review and correction**. Deliver the response immediately but run groundedness checks in the background. If a response is later flagged as ungrounded, notify the user. "I made an error in my previous response. The correct information is..." Post-delivery correction is only viable when the consequences of errors are low and users are reachable.

A travel planning assistant runs groundedness checks asynchronously. Most responses pass. Responses that fail are flagged, and users receive follow-up corrections via email or in-app notification. The approach balances speed and accuracy — users get fast responses, and errors are corrected before they act on bad information.

Groundedness and hallucination detection are fundamental to trustworthiness. A system that generates fluent, plausible-sounding answers is impressive. A system that generates answers users can trust is valuable. The difference is groundedness. Without groundedness monitoring, you do not know whether your model is informing users or misleading them. With groundedness monitoring, you detect hallucinations early, understand their patterns, and intervene before trust is lost.

Next, we examine safety and policy compliance monitoring — how to detect when production outputs violate safety guidelines or regulatory requirements.

