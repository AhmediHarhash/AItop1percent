# 8.6 — Mitigation Strategies: Rollback, Fallback, and Graceful Degradation

The root cause was clear: the new model version was hallucinating product prices. The investigation had taken eighteen minutes. Now the engineering team faced the mitigation decision. They could roll back to the previous model — safe but slow, requiring a redeployment that would take twelve minutes. They could route traffic to a fallback rule-based system — fast but limited, providing only basic functionality. They could implement targeted filtering to catch and block hallucinated prices — complex but preserving most of the new model's improvements. The decision mattered. Each mitigation strategy had different speed, different user impact, and different risk. The team chose rollback. Twelve minutes later, the system was stable. They had given up the new model's 15 percent quality improvement to ensure reliability. Mitigation is about choosing the least bad option when no good options exist.

AI incidents require mitigation strategies that traditional software does not need. You cannot just restart a service when the problem is model behavior. You cannot just add caching when the issue is hallucinations. You need strategies that account for model versions, fallback systems, output filtering, and graceful degradation of capabilities. Each strategy has a speed-quality-risk tradeoff. The goal is to minimize ongoing damage while preserving as much functionality as possible.

## The Rollback Strategy

Rollback means reverting to the last known-good model version, prompt template, or configuration. It is the safest mitigation strategy but the slowest to execute and most disruptive to forward progress.

A healthcare documentation assistant had an incident in March 2025 where a new model deployment caused eval pass rates to drop from 93 percent to 79 percent. The root cause was identified: the new model had catastrophic forgetting of medical terminology. The mitigation was rollback. The engineering team reverted to the previous model version. Deployment took eight minutes. Eval pass rates returned to 93 percent. Total incident duration: twenty-six minutes. The rollback was successful, but it cost the team two weeks of work — the new model had other improvements that were now unavailable.

Rollback requires versioning infrastructure. You need to track: model versions, prompt versions, configuration versions, and the ability to deploy any previous version quickly. A financial advisory platform maintained a deployment history for the past 30 days with one-click rollback for any version. When an incident occurred in July 2025, the engineering team rolled back to the version from eighteen hours earlier. The rollback took four minutes. Without the one-click infrastructure, rollback would have required manually identifying the correct version, retrieving it from storage, and redeploying — a process that would have taken thirty to forty minutes.

Rollback is most effective when the incident is caused by a recent change. If the model deployed two hours ago is failing, rolling back to the model from three hours ago is likely to solve the issue. But if the incident is caused by data drift, external changes, or accumulating degradation over days, rollback may not help. A customer service chatbot had an incident in November 2025 where outputs were tonally inappropriate. The team rolled back the model. The issue persisted. They rolled back further. The issue persisted. Eventually they realized the problem was not the model — it was a shift in user expectations that had occurred gradually over weeks. Rollback was the wrong strategy. They needed retraining, not version reversion.

## The Traffic Routing Strategy

Traffic routing means shifting requests away from the failing component to a healthy component. For AI systems, this typically means routing from a new model to an old model, from one inference endpoint to another, or from an AI feature to a non-AI alternative.

A document analysis platform ran two model versions simultaneously during a gradual rollout: 80 percent of traffic on the new model, 20 percent on the old model. An incident occurred where the new model started generating incomplete summaries. The mitigation was immediate: shift 100 percent of traffic to the old model. The traffic shift took ninety seconds. No redeployment required. The new model continued running but stopped receiving production traffic. Total incident duration: eleven minutes. The traffic routing strategy was faster than rollback because no deployment was needed.

Traffic routing requires load balancer configuration and the ability to run multiple model versions simultaneously. A legal document assistant had this capability. When an incident occurred in September 2025, they shifted traffic from Claude Opus 4.5 to Claude Opus 4.0 in under two minutes. The older model had slightly lower quality but was stable. Users experienced a minor quality degradation instead of failures. The traffic shift provided immediate mitigation while the team investigated the root cause of the Opus 4.5 failures.

Traffic routing works best for gradual rollouts or A/B deployments where multiple versions are already running. If only one model version is deployed, traffic routing requires spinning up an alternative version, which takes time. A travel booking assistant learned this in January 2026. They ran only one model version in production. When an incident occurred, they wanted to route to the previous version. But the previous version was not running. They had to deploy it first, which took fourteen minutes. After the incident, they changed their deployment strategy to always keep the previous version warm and ready for emergency traffic shifts.

## The Fallback to Rules Strategy

Fallback to rules means switching from AI-generated outputs to rule-based, template-based, or hardcoded outputs. This strategy sacrifices quality and flexibility but provides guaranteed stability.

A customer service chatbot had an incident in May 2025 where the model started generating nonsensical responses. The mitigation was activating a rule-based fallback system. The fallback system used keyword matching and predefined templates to answer common questions. It could not handle complex queries or nuanced conversations, but it could provide basic support without hallucinating. The engineering team activated the fallback in three minutes. User satisfaction dropped from 4.1 to 3.2, but at least users were getting coherent responses. The fallback remained active for six hours while the team fixed the root cause.

Fallback systems require pre-built alternatives. You cannot design a rule-based system during an incident. A financial advisory platform maintained a rule-based investment recommendation system alongside their AI model. The rule-based system had been the original product before the AI model was introduced. When the AI model had an incident in October 2025, they switched to the rule-based system in under five minutes. The rule-based system was less sophisticated — it could not personalize recommendations or handle complex scenarios — but it was reliable. Users received generic but correct advice instead of AI-generated hallucinations.

Fallback to rules is most effective for well-defined, narrow use cases. If the AI task has clear boundaries and limited variation, a rule-based fallback is feasible. If the AI task is open-ended and requires reasoning, a rule-based fallback may be impossible. A content generation platform had an incident in December 2025 where the model was producing low-quality marketing copy. There was no rule-based fallback for creative content generation. The team had to either fix the model or take the feature offline. They chose to take it offline. The fallback strategy was not available for that use case.

## The Output Filtering Strategy

Output filtering means allowing the model to generate outputs but intercepting and blocking any outputs that fail validation checks. This strategy preserves functionality for valid outputs while preventing failures from reaching users.

A contract analysis platform had an incident in April 2025 where the model was occasionally hallucinating case citations. The team implemented output filtering: every generated citation was checked against a legal database. If the citation did not exist, the output was blocked and the user received an error message asking them to retry. The filter caught 97 percent of hallucinations. For the remaining 3 percent of requests, users received working outputs. Total mitigation time: twenty minutes to implement the filter. The filter was not perfect, but it reduced user impact from 100 percent to 3 percent while the team worked on a permanent fix.

Output filtering requires fast validation logic. If the filter takes three seconds to run, you are adding three seconds to every request. A healthcare documentation platform had an incident in August 2025 where the model was generating summaries that omitted critical information. They implemented a filter that checked whether certain required sections were present. The filter ran in 40 milliseconds. The latency impact was acceptable. Outputs missing required sections were flagged for human review instead of being sent to users immediately.

Output filtering works best when you can define clear validity criteria. If you can specify what a bad output looks like — missing fields, invalid formats, prohibited content — you can filter it. If bad outputs are semantically wrong but syntactically valid, filtering is harder. A customer service chatbot had an incident in November 2025 where the model was giving factually incorrect answers. The answers were well-formed, fluent, and confident. There was no syntactic signal to filter on. The team could not implement effective output filtering and had to resort to rollback.

## The Graceful Degradation Strategy

Graceful degradation means reducing the system's capabilities to a subset that still works. Instead of full functionality with high failure rate, you provide limited functionality with low failure rate.

A document summarization platform had an incident in June 2025 where the model was failing on long documents but working fine on short documents. The mitigation was input length restriction: limit inputs to 4,000 tokens instead of the usual 16,000 tokens. Users could still summarize short documents. Users with long documents received an error message explaining the temporary limitation. The mitigation preserved 70 percent of functionality while eliminating 95 percent of failures.

Graceful degradation requires the ability to detect which features are failing and disable them selectively. A travel booking assistant had an incident in February 2026 where the model's hotel recommendation feature was broken but the flight recommendation feature was working. They disabled hotel recommendations temporarily. Users could still search for flights. The hotel feature returned a message: "Hotel recommendations are temporarily unavailable. Please check back later." The mitigation preserved partial functionality instead of taking the entire system offline.

Graceful degradation is most effective when failures are isolated to specific features or input patterns. If the entire model is degraded uniformly, there is no subset of functionality to preserve. A content moderation platform had an incident in January 2026 where the model's accuracy had dropped across all content types. There was no graceful degradation available — they could not selectively disable certain moderation checks without creating safety gaps. They had to roll back the entire model.

## The Rate Limiting Strategy

Rate limiting means reducing the load on the failing system to prevent cascading failures. For AI systems experiencing resource exhaustion or latency issues, rate limiting can stabilize the system while preserving partial availability.

A legal document assistant had an incident in July 2025 where inference latency spiked from 800 milliseconds to 4,200 milliseconds. The root cause was a traffic surge overwhelming the inference servers. The immediate mitigation was aggressive rate limiting: reduce maximum concurrent requests from 200 to 50. Latency stabilized at 1,100 milliseconds. Many users experienced request rejections, but the users who got through received working outputs. Without rate limiting, the system would have collapsed entirely.

Rate limiting requires deciding who gets access and who gets rejected. A healthcare documentation platform had tiered rate limits based on customer priority. During an incident in March 2026, they implemented emergency rate limiting that prioritized hospital customers over clinic customers. Hospital requests had a 95 percent acceptance rate. Clinic requests had a 40 percent acceptance rate. The strategy was not fair, but it was aligned with business priorities. The hospital customers — the highest-revenue, highest-stakes users — experienced minimal disruption.

Rate limiting is a temporary mitigation. It reduces load to buy time for a permanent fix. A financial advisory platform implemented rate limiting during an incident in October 2025. The rate limits stayed in place for ninety minutes while the engineering team scaled up infrastructure. Once capacity was increased, rate limits were lifted. If rate limits had remained in place indefinitely, they would have become a feature degradation, not a mitigation.

## The Hybrid Mitigation Strategy

Most incidents do not require a single mitigation strategy. The most effective response often combines multiple strategies in sequence or in parallel.

A customer service chatbot had an incident in December 2025 where the model was generating inappropriate responses for 12 percent of queries. The mitigation was hybrid: immediate output filtering to block obviously inappropriate responses, traffic routing to shift 50 percent of load to the previous model version, and graceful degradation to disable the model's attempt at humor which was causing most of the inappropriate outputs. The three strategies ran in parallel. Output filtering caught 80 percent of inappropriate responses immediately. Traffic routing reduced the volume of requests hitting the failing model. Graceful degradation eliminated the feature that was most problematic. The combined mitigation reduced user-facing failures from 12 percent to 1.5 percent within eight minutes.

Hybrid strategies require coordination. You need to track which mitigations are active, when they were applied, and what their effects are. A document analysis platform used a mitigation checklist during incidents: rollback available? Traffic routing available? Fallback available? Output filtering feasible? Graceful degradation possible? Rate limiting necessary? The engineering team checked each option and applied the combination that minimized user impact while being fastest to implement. During an incident in May 2025, they determined that rollback would take twelve minutes, traffic routing was not available because only one version was deployed, but output filtering could be implemented in four minutes. They chose output filtering. The decision was based on speed and availability, not on some preference for one strategy over another.

## The Partial Rollback Strategy

Sometimes you do not need to roll back everything. If only one component is failing, roll back that component and leave everything else in place.

A travel booking assistant had an incident in August 2025 where hotel recommendations were failing but flight recommendations were fine. The issue was a new prompt template for hotels. The mitigation was rolling back only the hotel prompt template while keeping the flight prompt template unchanged. The partial rollback took three minutes and preserved the quality improvements in the flight feature. A full rollback would have reverted both features, sacrificing improvements that were working.

Partial rollback requires component-level versioning. You need to track versions for each prompt template, each model, each configuration independently. A contract analysis platform had this granularity. When an incident occurred in November 2025, they identified that the issue was specific to the contract summary generation feature, not the clause extraction feature. They rolled back only the summary generation model. The clause extraction model remained on the newer, better version. Users experienced a degradation in summary quality but an improvement in clause extraction compared to a full rollback.

Partial rollback is only possible if your system architecture supports independent versioning. If all features share a single model and single prompt, partial rollback is not feasible. A content moderation platform learned this in January 2026. They ran a single model that handled all content types. When an incident occurred affecting only political content moderation, they could not roll back just that capability. They had to roll back the entire model, even though 90 percent of its capabilities were working fine. After the incident, they redesigned the system to use separate models for different content types, enabling partial rollback for future incidents.

## The Canary Rollback Strategy

Canary rollback means rolling back gradually, starting with a small percentage of traffic, to verify that the rollback actually fixes the issue before applying it fully.

A healthcare documentation platform had an incident in April 2025. They rolled back the model. But the issue persisted. They had rolled back to a version that also had the bug, just less severely. They had to roll back further. The second rollback worked. The incident lasted forty-five minutes because the first rollback attempt was incorrect. After this incident, they implemented canary rollback: roll back 10 percent of traffic first, verify the issue is resolved for that traffic, then roll back 100 percent. The canary rollback adds five minutes to mitigation time but prevents rolling back to a version that does not fix the problem.

Canary rollback requires traffic segmentation and rapid metric evaluation. A legal document assistant had the infrastructure to route 10 percent of traffic to a specific model version and evaluate its performance within two minutes. When an incident occurred in September 2025, they used canary rollback. They rolled back 10 percent of traffic to the previous model. Metrics improved for that 10 percent. They rolled back the remaining 90 percent. Total mitigation time: eleven minutes, including the canary verification period. The five-minute verification cost was worth avoiding the risk of a failed full rollback.

## The Mitigation Documentation Requirement

Every mitigation action must be documented in real time. What did you change? When? What was the result? The documentation serves two purposes: it helps the team understand what has been tried, and it provides a record for post-incident review.

A customer service chatbot had an incident in June 2025 where the engineering team applied four different mitigations in rapid succession: traffic routing, output filtering, rate limiting, and partial rollback. But they did not document which was applied when or what the effect of each was. After the incident, during the post-mortem, they could not determine which mitigation had actually resolved the issue. The lack of documentation prevented learning. The next similar incident, they were not sure which mitigation to apply first.

A financial advisory platform used incident.io's timeline feature to automatically log every mitigation action. When an engineer executed a rollback, the action was timestamped and logged. When they enabled a feature flag to activate output filtering, the change was logged. When they adjusted rate limits, the change was logged. After the incident, the timeline showed exactly what had been tried, in what order, and what the metric effects were. The documentation enabled precise understanding of which mitigation strategies were effective.

## The Mitigation Validation Requirement

After applying a mitigation, verify that it worked. Do not assume success. Check metrics. Confirm that user impact has stopped.

A document analysis platform had an incident in October 2025. The engineering team rolled back the model. They assumed the rollback would fix the issue. They posted "mitigation applied" in the incident channel and started working on the root cause investigation. Twenty minutes later, they checked metrics. The issue was still happening. The rollback had not worked. They had wasted twenty minutes assuming the problem was solved. After this incident, they implemented a validation requirement: after every mitigation, explicitly check metrics within two minutes and confirm that the issue is resolved. If metrics do not improve, the mitigation failed and a different strategy is needed.

Validation should be quantitative, not qualitative. Do not rely on "it looks better." Check specific metrics: eval pass rate, error rate, latency, user complaints per minute. A travel booking assistant had an incident in December 2025. They applied output filtering. An engineer manually tested a few requests and saw that they were working. The engineer declared the mitigation successful. But production metrics still showed 8 percent failures. The manual test had been unrepresentative. The validation was wrong. The team later implemented automated validation: after mitigation, wait two minutes, compare current metrics to baseline, and confirm that metrics are within acceptable thresholds. Only then declare mitigation successful.

## The Mitigation Time Budget

Mitigation should be fast. If a mitigation strategy takes more than fifteen minutes to implement, consider whether a different strategy would be faster.

A healthcare documentation platform had an incident in July 2025. The engineering team wanted to implement sophisticated output filtering that checked generated summaries against source documents for factual accuracy. The filter would take two hours to build and test. Instead, they implemented a simpler filter that just checked for required sections. The simpler filter took twelve minutes to build and caught 60 percent of failures. It was not perfect, but it was fast. After the incident was resolved, they built the sophisticated filter for future use.

The mitigation time budget should be proportional to incident severity. For critical incidents, mitigation should take less than ten minutes. For high incidents, less than thirty minutes. For medium incidents, less than two hours. If you cannot implement a mitigation within the budget, apply the fastest available mitigation even if it is imperfect. A customer service chatbot had a critical incident in February 2026. The ideal mitigation was retraining the model with corrected data. But retraining would take six hours. They applied a fallback to rule-based responses in five minutes. The fallback was much lower quality, but it stopped the damage immediately. They retrained the model overnight and switched back to AI the next day.

## The Mitigation Reversal Plan

Some mitigations are temporary. You apply them to stop the bleeding, then remove them once the root cause is fixed. You need a plan for reversal.

A legal document assistant applied aggressive rate limiting during an incident in November 2025. The rate limits stabilized the system but reduced throughput by 70 percent. Once the root cause was fixed and capacity was increased, the rate limits needed to be removed. But the team forgot to remove them. The rate limits stayed in place for two days. Users complained about slow access. The engineering team investigated and realized the rate limits were still active. The lack of a reversal plan meant that a temporary mitigation became a lingering problem.

The reversal plan should be documented when the mitigation is applied. A contract analysis platform used a mitigation tracking sheet during incidents. Each mitigation had fields for: what was changed, when, why, expected duration, and reversal procedure. When output filtering was applied during an incident in January 2026, the sheet documented: "Output filtering active, blocking citations that fail database lookup, expected duration until model retraining complete, reversal procedure is to disable filter feature flag." Three days later, when retraining was complete, the team referred to the sheet and reversed the mitigation cleanly.

Once mitigation is applied and validated, the next challenge is communication. Who do you tell about the incident? What do you say? How often do you update them? AI incidents require careful communication because the failure modes are often hard to explain and users may not understand the nuances.

