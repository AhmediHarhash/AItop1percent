# Chapter 12 — Compliance, Audit, and Human-in-the-Loop Monitoring

Regulators do not care about your model architecture. They care about whether you can demonstrate that your AI system operates within policy, protects user data, and produces decisions that can be explained and justified. Compliance monitoring is not a checkbox — it is ongoing infrastructure that produces the audit trails, retention records, and explainability artifacts that auditors require. Human-in-the-loop monitoring tracks how often humans override AI decisions, how much corrections change outputs, and how quickly human feedback reaches production models. These signals tell you whether your AI system is augmenting human judgment or operating beyond it.

---

- 12.1 — Regulatory-Grade Logging: What Auditors Need to See
- 12.2 — Immutable Audit Trails: Tamper-Proof Record Keeping
- 12.3 — PII Detection and Redaction Logging
- 12.4 — Explainability Capture: Recording Why the Model Decided
- 12.5 — Data Retention Compliance: GDPR, HIPAA, and Beyond
- 12.6 — Human Override Rate Monitoring
- 12.7 — Escalation Frequency and Reviewer Disagreement Tracking
- 12.8 — Human Correction Delta: Measuring the Gap Between AI and Human
- 12.9 — Feedback Loop Latency: How Fast Human Signals Reach Models
- 12.10 — Audit Export and Regulator-Facing Reports

---

*The EU AI Act does not ask if your model is accurate. It asks if you can prove your model is accurate. Compliance monitoring produces that proof.*
