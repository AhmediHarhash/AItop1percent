# 7.4 — Alert Aggregation: Grouping Related Signals to Reduce Noise

The on-call engineer received fourteen pages in eight minutes. API errors in us-east-1. Model timeouts in us-east-1. Cache failures in us-east-1. Database connection failures in us-east-1. Load balancer health checks failing in us-east-1. Fourteen separate alerts, fourteen separate pages, fourteen separate acknowledgments required. The root cause was one network partition affecting one datacenter. The alerting system treated every symptom as an independent emergency. The engineer spent twelve minutes acknowledging alerts before starting investigation. By the time they understood the scope, the automatic failover had already routed traffic to us-west-2. The incident was over. The alert storm was just noise.

This is the aggregation problem. Real incidents produce cascading failures. A single root cause triggers failures in multiple systems. Without aggregation, each symptom generates an alert. The result is alert storms — dozens or hundreds of notifications in minutes, all describing the same underlying problem from different angles. Alert storms overwhelm engineers, bury critical information in noise, and slow response by forcing acknowledgment overhead before investigation can begin.

Effective aggregation groups related alerts into single notifications that capture incident scope without multiplying pages. The engineer should receive one alert that says "us-east-1 region experiencing widespread failures across API, model, cache, and database layers" not fourteen alerts that each describe one symptom. The aggregation must preserve information — which systems are affected, what is failing, how severe — while reducing cognitive load. This is the central challenge of alert aggregation: compress without losing critical signals.

## Spatial Aggregation: Grouping by Infrastructure

Spatial aggregation groups alerts by infrastructure location. When multiple services in the same region, datacenter, or availability zone experience failures simultaneously, aggregate them into a single regional alert. When multiple models running on the same hardware experience timeouts, aggregate them into a single hardware alert. The principle is that co-located failures often share root causes.

A streaming media AI company implemented spatial aggregation after experiencing alert storms during infrastructure failures. They had recommendation models running in six regions. When one region experienced network issues, every model in that region started failing health checks. Without aggregation, they received thirty-six alerts — six models times six alert types. With spatial aggregation, they received one alert: "us-east-1 region health degraded, affecting recommendation, personalization, and content classification models."

The implementation required tagging all metrics with infrastructure dimensions: region, availability zone, datacenter, rack, host. When alerts fired, the aggregation layer checked if multiple alerts were firing for resources in the same infrastructure location within a short time window. If yes, the alerts were grouped. The notification included the common infrastructure context and a summary of which services were affected. The engineer immediately understood scope — this is a regional issue, not a model issue. The investigation started from the right level.

The key tuning parameter is the time window. Alerts must fire within a short period to be considered related. If the window is too narrow — say thirty seconds — you will miss related alerts that fire slightly out of sync because monitoring systems poll at different intervals. If the window is too wide — say five minutes — you will incorrectly group unrelated alerts that happen to fire near each other. Industry practice suggests time windows between one and three minutes work for most infrastructure patterns.

The false aggregation problem is real. Sometimes multiple independent failures occur simultaneously in the same region. Aggressive spatial aggregation can hide the independence, leading engineers to assume a common cause when there is none. The mitigation is to make aggregated alerts expandable. The summary notification says "regional issue affecting multiple services." The detailed view shows each individual alert with its specific metrics and thresholds. The engineer can drill down if the summary is insufficient. The aggregation reduces initial noise but preserves full information for investigation.

Spatial aggregation works best for infrastructure failures. Network partitions, power issues, hardware failures, cloud provider incidents — these create co-located symptom patterns that aggregate cleanly. It works less well for application-level failures, where failures in different services might occur in the same infrastructure but have independent causes. A model failure and a database failure in the same region are not necessarily related. Aggregating them creates false assumptions. The aggregation logic must distinguish infrastructure causation from infrastructure coincidence.

## Temporal Aggregation: Grouping by Time

Temporal aggregation groups alerts that fire repeatedly for the same condition over time. If a metric crosses a threshold, fires an alert, recovers, then crosses the threshold again five minutes later, that should be two events in one alert, not two separate pages. The condition is flapping — oscillating around the threshold. Flapping alerts are low-value interrupts. They indicate instability but do not require multiple pages.

A financial services AI company dealt with flapping alerts in their fraud detection system. Their model confidence metric would occasionally hover near the alert threshold, crossing it briefly, recovering, then crossing again. Without temporal aggregation, this created alert sequences: fire at 2:15, recover at 2:18, fire at 2:22, recover at 2:24, fire at 2:29. Five alerts in fourteen minutes for the same underlying instability. Each alert woke the on-call engineer. None provided new information.

They implemented temporal aggregation with a suppression window. After an alert fired, subsequent alerts for the same condition within thirty minutes were suppressed. The engineer received one notification: "model confidence below threshold, flapping detected, five occurrences in past thirty minutes." The notification included timestamps and duration. The engineer understood that this was an unstable condition, not five independent failures. The response could be considered rather than reactive.

The suppression window must balance responsiveness with noise reduction. Too short and you still get alert storms during flapping. Too long and you might suppress alerts for genuinely different instances of the same problem. If a condition fires, is resolved through intervention, then fires again an hour later, that is probably a different incident that deserves a separate alert. If a condition fires, recovers spontaneously, and fires again five minutes later, that is probably flapping.

The implementation tracks alert state per condition. When an alert fires, the system records the condition, timestamp, and whether the alert is currently active or recovered. If the same condition fires again within the suppression window, the system updates the existing alert record with the new event instead of creating a new alert. The notification system can choose to send an update — "this alert has now fired three times" — or to remain silent until the suppression window expires or the condition stabilizes.

Temporal aggregation also applies to recurring alerts for chronic issues. If a model accuracy alert fires every day at the same time because a particular customer workload stresses the system, that is not an emergency. It is a known issue that needs fixing but does not require daily pages. The alerting system should recognize the recurrence pattern and adjust notification accordingly — perhaps moving the alert from Page to Review category after it recurs three times without resolution.

This requires memory across days or weeks. The alerting system must track alert history, identify recurrence patterns, and adapt behavior. A media AI company implemented this by maintaining an alert fingerprint — a hash of the condition, affected resources, and threshold. Alerts with the same fingerprint firing more than twice in seven days were flagged as recurring. The third occurrence triggered a different notification type: "chronic issue detected, this alert has fired three times in past week without resolution, requires sustained investigation." The notification went to engineering leadership, not just the on-call engineer. The chronic issue was surfaced as a systemic problem, not a series of one-off incidents.

## Logical Aggregation: Grouping by Causation

Logical aggregation groups alerts based on causal relationships. When alert A causes alert B, only alert A should page. Alert B is a symptom, not a root cause. An example: model timeouts cause API errors cause user-facing request failures. When the model times out, you should receive one alert about model timeouts, not three alerts about timeouts, API errors, and request failures. The downstream failures are expected consequences. Alerting on them is noise.

An e-commerce AI platform built a dependency graph for their services. Models depended on feature stores. APIs depended on models. User-facing services depended on APIs. They configured alert suppression rules based on the dependency graph. If a model alert fired, API and user service alerts were suppressed for five minutes. The reasoning was that API and user service failures were expected when the model failed. Alerting on expected consequences was redundant.

This worked well for cascading failures with clear causation. When the feature store went down, models failed, APIs failed, and user requests failed. The engineer received one alert: "feature store unavailable, causing downstream failures in model, API, and user service." The scope was clear. The investigation started at the root. The downstream alerts were logged but not paged. Once the feature store was restored, the downstream services recovered automatically.

The challenge is that dependency graphs are assumptions. Sometimes alert A and alert B fire simultaneously because they share a root cause, not because A causes B. If you suppress B because you assume A is the cause, you might miss the real root cause. A transportation AI company discovered this when their model and database both failed during a deployment. The model failure alert fired first. The database failure alert was suppressed because the system assumed model failure caused it. In reality, a network configuration change broke both simultaneously. The suppression hid the database failure, delaying diagnosis by twenty minutes.

The fix is to make suppression time-aware and conditional. Suppress downstream alerts only if they fire within a short time window after the upstream alert — say sixty seconds. If alert B fires more than a minute after alert A, they are probably independent. Also make suppression optional for critical systems. Database failures should page even if model failures fired first, because database failures are severe enough to warrant independent investigation. The suppression rule becomes: suppress expected low-severity consequences, but always page for high-severity conditions regardless of potential causes.

Logical aggregation requires maintaining and updating the dependency graph. Services change. New dependencies are introduced. Old dependencies are removed. If the dependency graph is stale, the suppression rules become wrong. An alert that should page is suppressed because of a dependency that no longer exists. An alert that should be suppressed pages because a new dependency was not documented. The graph must be kept synchronized with the actual system architecture. This is maintenance overhead. For teams that do not have architecture diagrams or service dependency graphs, logical aggregation is too complex to implement reliably.

## Content Aggregation: Grouping by Message Similarity

Content aggregation groups alerts with similar messages or conditions. If you receive ten alerts all saying "model latency exceeded threshold in different availability zones," those should aggregate into one alert: "model latency exceeded threshold across ten availability zones." The alerts are distinct events — different locations, different timestamps — but the content is nearly identical. Grouping them reduces repetition without losing information.

A retail AI company implemented content-based aggregation using alert message templates. Every alert had a template: "metric name exceeded threshold in location." Alerts with the same template and metric name firing within a time window were grouped. The aggregated alert listed all affected locations as a structured field. The notification format was: "model latency exceeded one second threshold in the following zones: us-east-1a, us-east-1b, us-east-1c, us-west-2a." The engineer understood the pattern immediately — this is a widespread latency issue, not a single-zone problem.

The aggregation logic used text similarity for less structured alerts. When alerts did not match templates exactly but had high text similarity — measured by edit distance or embedding cosine similarity — they were considered candidates for aggregation. A human reviewer could approve or reject the aggregation suggestion. Over time, approved aggregations became rules. The system learned which alert patterns should be grouped.

Content aggregation has the highest false positive risk. Alerts with similar text might describe genuinely different problems. "Model accuracy below ninety-five percent" could fire for two different models experiencing different issues. Aggregating them loses critical distinction. The mitigation is to include entity information in aggregation logic. Group alerts only if they have similar messages AND affect the same entity type or related entities. Alerts about different models should not aggregate even if the message is similar.

The benefit of content aggregation is catching patterns that other aggregation types miss. A security AI company used content aggregation to detect distributed attacks. They received alerts about unusual query patterns from seventeen different geographic regions within ten minutes. Each alert described a slightly different pattern, so they did not match any template. But content similarity flagged them as related. The aggregated view revealed a coordinated attack testing multiple exploit vectors. Without aggregation, the pattern would have been invisible in the noise of seventeen separate alerts.

## Aggregation Interfaces: Presenting Grouped Alerts

Aggregated alerts must be presented in ways that make scope and detail both accessible. The initial notification should be a concise summary. The engineer should be able to expand the summary to see individual alerts. The expansion should be available in the notification interface — no need to log into a separate dashboard or system.

A logistics AI platform used a two-tier notification format. The initial page was a summary: "Regional failure detected. Five services affected in us-east-1. Estimated user impact: high. Time window: 02:15 - ongoing." The summary included a link to expand details. Clicking the link showed a structured view of individual alerts: which services, which metrics, what thresholds, when each alert fired. The engineer could see the full picture or dive into specifics as needed.

The summary must include actionability guidance. If aggregated alerts all require the same response, state it clearly: "Investigate us-east-1 network connectivity." If aggregated alerts require different responses, flag that: "Multiple independent failures detected, requires triage to prioritize response." The worst aggregation is one that groups alerts without helping the engineer understand what to do. That is compression without insight.

Aggregation systems should also provide de-aggregation options. If an engineer receives an aggregated alert but suspects the grouped failures are actually independent, they should be able to split the aggregation and treat each alert separately. This is the escape hatch for false aggregations. The engineer's operational judgment overrides the system's grouping logic. In practice, de-aggregation is rare — most aggregations are correct — but the option must exist for the cases where aggregation misleads.

The next subchapter covers alert routing — the systems and policies that ensure the right alerts reach the right people, including primary on-call, backup, specialists, and escalation chains.

