# 5.6 — Citation and Source Coverage Monitoring

The model generates a confident answer. It cites three sources. The user clicks the first citation. The link is dead. The user clicks the second citation. The page exists but does not contain the passage the model claimed it did. The user clicks the third citation. The page has been updated since the system cached it, and the cited information is now outdated. The answer might be correct, but the user cannot verify it. The trust relationship is broken. This is the citation integrity failure, and it happens silently in production every day.

Citation quality is not the same as answer quality. A model can synthesize a factually correct answer from retrieved documents but cite them incorrectly — wrong page numbers, wrong sections, wrong URLs. Or the model can cite documents correctly at the time of indexing, but the documents move, change, or disappear before the user accesses them. The answer was right when generated. The citations are broken when accessed. The user experiences this as the system lying to them.

## The Citation Trust Problem

Users trust citations because citations are verification. If an answer includes citations, the user assumes they can click through and confirm the information. When citations break, users do not just lose trust in that specific answer. They lose trust in the entire system. A broken citation is worse than no citation because it wastes the user's time and signals that the system is not maintaining its knowledge base.

In February 2026, a medical information RAG cited clinical guidelines from a government health agency. The citations included URLs to specific guideline documents. The agency reorganized its website in March 2026, moving guideline documents to a new domain and changing URL structures. The RAG system's index still pointed to the old URLs. For six weeks, every citation to those guidelines returned a 404. Users received medically accurate information with broken citations. They could not verify the information. Complaints flooded support. "Your system cites sources that do not exist. How can we trust any of this?" The information was correct. The citations were stale. Trust collapsed.

The team did not know the citations were broken until users reported it. They had no monitoring for citation health. They added a weekly job that crawled every URL in the index and flagged 404 responses. The job found 14,000 broken citations across 230,000 indexed documents. Most were from site reorganizations, domain changes, or content deletions. The team spent two months updating citations, setting up redirects where possible, and removing documents whose sources were permanently gone. Citation health recovered. User trust took longer to rebuild.

## Citation Coverage: Are All Claims Supported?

**Citation coverage** measures whether every factual claim in the response has a supporting citation. Incomplete citation coverage means the model makes claims without providing sources. This is dangerous in domains where verification is important — healthcare, legal, financial, regulatory. A claim without a citation might be correct, but the user has no way to verify it.

Measuring citation coverage requires parsing the response into individual factual claims, then verifying that each claim has a citation. This is expensive and typically done with LLM-based automated evaluation. For a sample of responses, prompt an evaluator model to extract all factual claims and identify which claims have supporting citations. Citation coverage is the fraction of claims with citations.

A legal research RAG generated answers about case law. Evaluators extracted factual claims from a sample of 500 responses. On average, each response contained 4.2 factual claims. Of those, 3.1 had explicit citations. Citation coverage was 74 percent. The uncited claims were often procedural or contextual — "this issue has been litigated extensively" or "courts have split on this question." These claims were true but not supported by the cited cases. Users could not verify them. The team refined the system prompt to require citations for every factual claim, including procedural and contextual statements. Citation coverage improved to 91 percent. The remaining 9 percent were claims synthesized from multiple sources where no single citation was sufficient, a known limitation the team documented.

## Citation Accuracy: Do Citations Actually Support the Claims?

Citation accuracy measures whether the cited source actually contains the information the model claims it does. The model might cite Document A to support Claim X, but Document A does not mention Claim X. This is a hallucination at the citation level. The model generated a plausible citation that does not support the claim.

Measuring citation accuracy requires retrieving the cited document, extracting the relevant passage, and verifying that the passage supports the claim. For small-scale evaluation, humans do this. For large-scale evaluation, an LLM judge does this. Prompt the judge with the claim, the cited source, and the relevant passage. Ask: "Does this passage support the claim?" The judge returns yes or no. Citation accuracy is the fraction of citations judged as supporting their claims.

A financial news RAG cited market data and analyst reports. Evaluators sampled 300 responses with citations. For each citation, they retrieved the source document and verified whether the cited passage supported the claim. Citation accuracy was 83 percent. The 17 percent of inaccurate citations fell into three categories: **wrong section** — the document contained the information but in a different section than the model cited, **misinterpretation** — the document said something subtly different than the model claimed, and **stale data** — the document was correct when indexed but had been updated since, and the cached version no longer matched reality.

The team addressed wrong-section citations by improving chunking granularity. Instead of citing entire documents, the system cited specific sections. This forced the model to be more precise about where information was located. Misinterpretation errors were harder to fix and required better prompting to ensure the model accurately represented source content. Stale data errors required freshness monitoring and re-indexing, covered in earlier subchapters.

## Link Rot and Source Availability

Citations break over time. Websites reorganize. Content moves. Domains expire. Pages are deleted. The citation was valid when the document was indexed. It is invalid when the user clicks it. This is **link rot**, and it is pervasive. Studies of academic citations show that 20 to 30 percent of links break within five years. For web content, the rate is higher. RAG systems are vulnerable because they cache citations at indexing time but do not continuously validate them.

Tracking link rot requires periodic citation health checks. Crawl every URL in your index. Log the HTTP status code. If the response is 404 or 410, the citation is dead. If the response is 301 or 302, the citation is a redirect — follow it and update the indexed URL to the destination. If the response is 200, the citation is alive, but you still need to verify that the content has not changed in a way that invalidates the citation.

A policy documentation RAG indexed 18,000 documents with citations to external regulatory sites. The team ran a citation health check and found that 11 percent of citations returned 404 errors. Another 7 percent were redirects. The team updated URLs for redirects and flagged the 404 citations for manual review. Some were content that had been deleted permanently. Those documents were removed from the index. Others were content that had moved without proper redirects. The team manually found the new URLs and updated citations. Link rot dropped to 3 percent, but the team knew it would creep back up over time. They scheduled quarterly citation health checks to catch rot before users did.

## Citation Freshness vs Document Freshness

A document can be fresh — recently updated in the index — but the citations within it can be stale. The document content is current. The sources it references are not. This matters for documents that cite external sources. If your knowledge base includes reports, articles, or summaries that reference external research, those external references can become stale even if the document itself is updated regularly.

A research summary RAG indexed literature reviews written by domain experts. Each review summarized 20 to 50 research papers and included citations. The reviews were updated annually. The citations within the reviews often pointed to preprint servers, journal websites, or institutional repositories. Some of those sources moved or were replaced by final published versions with different URLs. The review document itself was fresh — updated in 2026. The citations within it were stale — pointing to 2024 preprint URLs that had been replaced by 2025 journal URLs.

The team added citation-level freshness tracking. When a document was re-indexed, the system crawled all citations within the document and verified they were still reachable. If a citation was broken, the system flagged it for review. The experts who authored the reviews were notified and updated citations in the next revision cycle. This caught citation staleness independently of document staleness.

## Phantom Citations: When the Model Cites What It Did Not Retrieve

The model is supposed to cite only the documents it retrieved. Sometimes it hallucinates citations — it generates a plausible-looking citation to a document that was not in the retrieved set. This happens when the model has been trained on data that included citations, and it learned the pattern of citation formatting without tying citations to actual retrieval.

Detecting phantom citations requires logging every document retrieved for a query, then verifying that every citation in the response corresponds to a retrieved document. If the model cites Document X but Document X was not retrieved, it is a phantom citation. The model fabricated the citation.

A scientific literature RAG retrieved papers from a database and generated summaries with citations. The team logged all retrieved papers and all generated citations. They found that 4 percent of citations were phantoms — the model cited papers that were not in the retrieved set. The citations looked plausible. They followed the correct format. They referenced real papers that existed in the database. But those papers were not retrieved for that specific query. The model had learned the pattern of citation formatting and generated citations that fit the pattern without verifying the papers were in context.

The team fixed this with constrained generation. The system prompt included a list of retrieved document IDs and instructed the model: "Only cite documents from this list." The model was also fine-tuned on examples where citations were explicitly constrained to retrieved documents. Phantom citation rate dropped to 0.3 percent, close to the irreducible rate caused by rare model errors.

## Citation Density and Overloading

Too few citations is a problem. Too many citations is also a problem. **Citation overload** occurs when the model cites so many sources that the user cannot process them. A response with 15 citations is overwhelming. Users do not click through all of them. They lose confidence because it looks like the model is uncertain and hedging by citing everything.

Citation density is the number of citations per claim or per paragraph. A healthy citation density depends on the domain. Legal and medical answers might require one citation per claim. Narrative or explanatory answers might require one citation per paragraph. Measuring citation density reveals whether the model is over-citing or under-citing.

A policy Q&A RAG generated answers with an average of 8.4 citations per response. Responses averaged three paragraphs. Citation density was 2.8 citations per paragraph. User engagement data showed that users clicked on the first citation 41 percent of the time, the second citation 18 percent of the time, and rarely clicked beyond the third citation. The model was generating 8.4 citations per response, but users engaged with two on average. The extra citations added cognitive load without adding value.

The team refined the prompt to prioritize the most relevant citations and limit total citations to four per response. Citation density dropped to 1.3 per paragraph. Users clicked on the first citation 52 percent of the time and the second citation 27 percent of the time. Engagement with citations increased because there were fewer to process. The responses were just as well-supported but less overwhelming.

## Monitoring Citation Quality at Scale

Track citation coverage, citation accuracy, link rot rate, phantom citation rate, and citation density. These metrics require sampling because verifying every citation for every query is impractical. Sample 500 to 1,000 responses per week. For each response, check whether claims have citations, whether citations support the claims, whether citation URLs are reachable, whether citations correspond to retrieved documents, and how many citations per response.

Segment by query type. Some query types naturally require more citations than others. Factual lookup queries might need one citation. Comparative analysis queries might need four. Segmented tracking prevents false alarms where citation density looks high because a particular query type justifiably requires many sources.

A healthcare RAG tracked citation metrics weekly. Citation coverage held steady at 89 percent. Citation accuracy was 91 percent. Link rot rate was 6 percent. Phantom citation rate was 1 percent. These were acceptable baselines. In April 2026, citation accuracy dropped to 78 percent over two weeks. Investigation revealed that a recent index update had ingested documents with poor structure. The chunking strategy failed on those documents, causing the model to cite whole documents instead of specific sections. When evaluators checked whether citations supported claims, they found that the documents contained the information somewhere but not in the section the model cited. Citation accuracy appeared low because the citations were too coarse. The team re-chunked the problematic documents. Citation accuracy recovered to 90 percent.

## The Citation Health Dashboard

Build a dashboard tracking citation metrics over time. Display citation coverage, citation accuracy, link rot rate, phantom citation rate, and citation density. Overlay document ingestion events, index rebuilds, and model updates. If citation quality drops after an index rebuild, the rebuild introduced broken citations. If phantom citation rate spikes after a model update, the new model is more prone to hallucinating citations.

A legal research platform built this dashboard and discovered that citation accuracy was highest for queries about recent cases and lowest for queries about cases from before 2020. Older case citations often pointed to archived versions of court websites that had since been redesigned. The citations were accurate when indexed in 2023, but by 2026 the archived URLs were broken. The team prioritized re-indexing older cases with updated citations. Citation accuracy for pre-2020 cases improved from 72 percent to 88 percent.

## The User Experience of Broken Citations

A user asks a question. The model answers confidently and cites three sources. The user clicks the first citation. 404. The user clicks the second citation. The page loads but does not contain the cited passage. The user clicks the third citation. The page has been updated and the passage now says something different than the model claimed. The user closes the system and googles the question manually. They do not come back. This is the compounding cost of poor citation quality. You lose the user not just for that query but for all future queries.

Citation quality is a user experience issue masquerading as a technical issue. Technically, the answer was correct. The model synthesized accurate information from the documents it retrieved. But the user cannot verify that, so from their perspective, the system is untrustworthy. Building citation monitoring is not optional. It is a prerequisite for user trust.

The next subchapter covers source quality signals — tracking when the sources in your knowledge base degrade in authority, accuracy, or relevance, even when the links still work and the content has not changed.

