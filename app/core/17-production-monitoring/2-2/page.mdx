# 2.2 — Distributed Tracing for AI Pipelines: Prompts, Retrievals, Tools, Guardrails

An AI request in production is not a single operation. It is a sequence of stages, each with its own latency, cost, failure modes, and performance characteristics. The user sends a question. Your system constructs a prompt. It calls your RAG pipeline to retrieve relevant context. It assembles the final prompt with system message, examples, retrieved context, and conversation history. It sends the prompt to your model provider. The model generates a response. Your guardrails evaluate the response for safety, correctness, and policy compliance. Your system formats the response and returns it to the user. Each stage can succeed, fail, or degrade silently. Without distributed tracing, you see only the total latency and the final outcome. With distributed tracing, you see every stage, every decision, and every bottleneck.

Distributed tracing for AI pipelines follows the same fundamental principles as tracing for microservices, but with AI-specific extensions. A **trace** represents a single request from start to finish. A **span** represents a single operation within that trace — prompt construction, retrieval, model call, guardrail check. Spans are organized hierarchically: the root span represents the entire request, and child spans represent sub-operations. Each span captures start time, end time, duration, status, and a set of attributes that describe what happened during that span.

The tracing standards that work for traditional distributed systems — OpenTelemetry, Jaeger, Zipkin — work for AI systems too. But AI systems need additional span attributes that capture token counts, model versions, prompt hashes, retrieval scores, tool call arguments, and guardrail decisions. The challenge is not whether to use distributed tracing — the challenge is what attributes to capture, how to structure spans for AI-specific operations, and how to keep trace volumes and costs manageable at scale.

## Span Structure for AI Request Pipelines

The root span represents the entire user-facing request. It starts when your system receives the user's query and ends when your system returns the final response. The root span captures request ID, user ID, session ID, endpoint, total latency, total tokens, total cost, and final status. This is the span you use to calculate SLAs, detect latency regressions, and monitor overall system health.

Beneath the root span, you create child spans for each major stage of your pipeline. **Prompt construction** is the first span. It captures how long it took to assemble the prompt, how many tokens the prompt contains before retrieval, which prompt template you used, and which conversation history you included. If you use multiple prompt templates based on query classification, this span tells you which template was selected. If your prompt construction logic fails — a missing variable, a malformed template, a database timeout fetching conversation history — this span captures the error.

**Retrieval** is the second span, and it is often the most complex. A single retrieval operation might involve query rewriting, embedding generation, vector search, reranking, and result filtering. You can represent this as a single span with detailed attributes, or as multiple child spans — one for query rewriting, one for embedding, one for vector search, one for reranking. The choice depends on whether you need per-stage latency breakdown. If retrieval is fast and reliable, a single span is sufficient. If retrieval is slow or failing, per-stage spans let you isolate whether the bottleneck is embedding, search, or reranking.

The retrieval span captures how many documents were retrieved, how many were returned after filtering, what the relevance scores were, and how many tokens the retrieved context added to the prompt. If retrieval returns zero results, this span should mark that as a warning. If relevance scores are unusually low, this span should capture that for later analysis. If retrieval times out, this span records the error and any fallback logic that triggered.

**Model inference** is the third span. It starts when you send the final prompt to your model provider and ends when you receive the response. This span captures which model you called, how many input tokens you sent, how many output tokens the model generated, how long the request took, and whether any errors occurred. If you retry the request due to rate limits or transient failures, each retry is a separate child span beneath the inference span, so you can see exactly how many retries occurred and how long each one took.

**Tool calls** appear as child spans of the inference span if your model uses function calling or tool use. Each tool call is a separate span. The span captures which tool was called, what arguments the model provided, what the tool returned, how long the tool took to execute, and whether the tool call succeeded or failed. If the model makes multiple tool calls in sequence, you see each one as a separate span in the trace. If the model makes tool calls in parallel, you see them as concurrent spans. This visibility is critical for debugging agentic systems where tool failures are the most common source of incorrect responses.

**Guardrail evaluation** is the fourth span. It runs after the model generates a response but before you return the response to the user. This span captures which guardrails evaluated the response, what scores they produced, and whether any guardrail blocked or modified the response. If multiple guardrails run in parallel, each is a child span beneath the guardrail evaluation span. If guardrails run sequentially with short-circuit logic — stop evaluating once one guardrail blocks — the trace shows you exactly which guardrail triggered and which ones never ran.

The final span is **response formatting**, which captures any post-processing you do before returning the response to the user. This might include markdown rendering, citation injection, or response truncation. Response formatting is usually fast, but it can fail — a malformed citation, a truncation bug, a serialization error. When it does, this span captures the error.

## Essential Attributes for AI Spans

Standard distributed tracing captures span name, start time, end time, status, and error messages. AI tracing needs more. Every span in an AI pipeline should capture **trace ID** and **span ID**, which are standard, but also **parent span ID** to establish the hierarchy, and **request ID**, which should match the request ID in your always-on instrumentation. This lets you correlate traces with logs, metrics, and external monitoring systems.

For the root span, capture **user ID** and **session ID** so you can trace a user's journey across multiple requests. Capture **endpoint** or **route** to distinguish between different features — chat, search, summarization, code generation. Capture **feature flags** if you use them, so you can correlate behavior changes with specific experiments. Capture **model routing decision** if your system chooses between multiple models based on query complexity or cost targets.

For the prompt construction span, capture **prompt template ID** or **prompt template version** so you know which template was used. Capture **prompt token count** before retrieval is added. Capture **conversation history length** in turns or tokens. If your prompt construction logic involves any dynamic decisions — which few-shot examples to include, which system message to use, which formatting to apply — capture those decisions as span attributes.

For the retrieval span, capture **query text** or a hash of the query text. Capture **number of documents retrieved**, **number of documents returned after filtering**, and **total tokens in retrieved context**. Capture relevance scores as an array or as summary statistics — minimum score, maximum score, average score. If you use reranking, capture the reranker model name and whether reranking changed the document order. If you use hybrid search, capture which search strategy was used — vector-only, keyword-only, or hybrid.

For the model inference span, capture **model provider**, **model name**, and **model version**. Capture **input token count**, **output token count**, and **total token count**. If your provider supports prompt caching, capture **cached tokens** and **cache hit rate**. Capture **sampling temperature**, **top-p**, **max tokens**, and any other generation parameters. If the model call fails, capture the error code and error message from your provider. If you retry, capture **retry count** and **final status**.

For tool call spans, capture **tool name**, **tool arguments**, **tool response**, **tool latency**, and **tool status**. Tool arguments and responses can be large, so consider hashing them or truncating them in the span, and storing full versions in sampled instrumentation. The key is to capture enough information to understand what the tool did, without overwhelming your tracing backend with massive payloads.

For guardrail spans, capture **guardrail name**, **guardrail score**, **threshold**, and **decision** — pass, warn, or block. If the guardrail is model-based, capture the model used for evaluation. If the guardrail modifies the response, capture what was modified. If the guardrail blocks the response, capture why.

## Correlation IDs and Context Propagation

Distributed tracing only works if you can connect spans across service boundaries. When your prompt construction service calls your retrieval service, the retrieval service needs to know which trace it belongs to. When your orchestration layer calls your model provider, your provider needs to return metadata that you can attach to the inference span. This requires **context propagation** — passing trace context from one service to the next.

The standard mechanism is HTTP headers. When your prompt construction service calls your retrieval service, it includes trace ID, span ID, and trace flags in HTTP headers. The retrieval service extracts those headers, creates a child span with the received trace ID, and sets its parent span ID to the received span ID. When the retrieval service completes, it returns data to the caller, and the caller records the retrieval span's end time and status.

OpenTelemetry defines standard headers for trace context propagation: `traceparent` and `tracestate`. If both your services use OpenTelemetry-compatible libraries, context propagation happens automatically. If you use custom instrumentation, you need to manually extract trace context from incoming requests and inject it into outgoing requests. This is tedious but essential. Without it, your traces are fragmented — you see the prompt construction span and the retrieval span as separate, disconnected traces, and you lose the ability to analyze end-to-end latency or understand causal relationships.

For model provider calls, context propagation is harder. Most model providers do not accept custom HTTP headers that propagate trace context. Instead, you rely on request IDs. Your inference span captures the request ID you sent to the provider and the request ID the provider returned. If the provider offers request-level logs or debugging endpoints, you can use the request ID to correlate your trace with their internal telemetry. If the provider does not expose per-request telemetry, you lose visibility into what happened inside their infrastructure, but you still capture everything that happened in your system.

For asynchronous operations — background jobs, queued tasks, webhook callbacks — context propagation requires embedding trace context in the message payload. When you enqueue a background job to reindex embeddings, include the trace ID and parent span ID in the job metadata. When the worker picks up the job, it extracts the trace context, creates a child span, and continues the trace. This lets you trace a user's request through synchronous operations, asynchronous processing, and back to a final response delivered via webhook or polling.

## Sampling Strategy for Distributed Traces

Traces are more expensive than logs. A single trace for a complex AI request might contain ten to twenty spans, each with dozens of attributes. At one million requests per day, storing full traces for every request creates terabytes of data and query latencies that make tracing unusable. Sampling is not optional at scale — it is required.

**Head-based sampling** makes the sampling decision at the start of the trace. When a request arrives, you decide whether to trace it based on a hash of the request ID, a random number, or a deterministic sampling rate. If you decide to trace it, you create spans for every operation in the request. If you decide not to trace it, you create no spans. Head-based sampling is simple, predictable, and computationally cheap. The downside is that you sample blindly — you trace some successful requests and miss some failed requests, purely by chance.

**Tail-based sampling** makes the sampling decision at the end of the trace. You buffer all spans in memory until the trace completes, then decide whether to keep the trace based on its characteristics. You can always keep traces that failed, traces that took longer than a threshold, traces that triggered guardrails, or traces for specific users or endpoints. Tail-based sampling gives you much better signal — you keep the traces that matter and discard the boring ones. The downside is cost and complexity. Buffering all spans in memory is expensive, and it requires coordination across distributed services to ensure all services agree on whether to keep or discard a trace.

Most production systems use a hybrid approach. You apply head-based sampling at a low rate — one percent or five percent — to capture a representative sample of all requests. You also apply tail-based rules to always keep traces that failed, traces that exceeded latency thresholds, or traces for flagged users. This gives you broad coverage for trend analysis and targeted coverage for incident debugging.

The sampling rate should be tuned based on traffic volume and tracing costs. If your tracing backend charges per span or per gigabyte ingested, monitor your costs closely. If query performance degrades because you are storing too many traces, increase your sampling rate. If you find yourself unable to debug incidents because relevant traces were not sampled, decrease your sampling rate or adjust your tail-based rules.

## Integrating Traces with Logs and Metrics

Traces are most powerful when combined with logs and metrics. A trace shows you the causal path of a single request. Logs show you detailed events within each span. Metrics show you aggregate behavior across many requests. All three are necessary. None is sufficient alone.

The integration point is the request ID. Your logs should include the request ID for every log line. Your traces should include the request ID as a span attribute. Your metrics should be tagged with request ID when possible, or at minimum with endpoint, model, and status. When you detect a latency spike in your metrics, you filter traces by time range and endpoint to find slow requests. You pick a representative trace and drill into its spans to see which stage was slow. You then filter logs by the trace's request ID to see detailed events — what the retrieval query was, what the model returned, what errors occurred.

Most modern observability platforms — Datadog, New Relic, Honeycomb, Grafana Cloud — support this workflow natively. You define a request ID field in your instrumentation schema. The platform automatically links traces, logs, and metrics by request ID. You can jump from a trace view to a log view to a metric view without manually constructing queries. This integrated experience is the difference between debugging an incident in five minutes and debugging it in five hours.

If you build your own observability stack, you need to design this integration yourself. Store request ID as a primary key or indexed field in your log storage. Tag metrics with request ID where cardinality allows, or use exemplars — a Prometheus feature that links metrics to traces by storing request IDs for a sample of metric data points. Build dashboards that let you filter traces by time range, endpoint, model, and error status, then drill into specific traces and their associated logs.

## AI-Specific Tracing Tools

OpenTelemetry is the industry-standard framework for distributed tracing, and it works well for AI systems if you instrument it correctly. But several tools have emerged specifically for AI observability, and they offer features that generic tracing tools do not.

**Langfuse** is an open-source observability platform designed for LLM applications. It understands prompts, completions, token counts, and model versions natively. You instrument your code with Langfuse's SDK, and it automatically creates traces with AI-specific attributes. Langfuse also provides a UI for browsing traces, analyzing prompt quality, and tracking cost over time. It integrates with LangChain, LlamaIndex, and other AI frameworks, so instrumentation is often a matter of adding a few lines of configuration.

**LangSmith** is a commercial observability platform from the creators of LangChain. It provides similar functionality to Langfuse — traces, logs, prompt versioning, cost tracking — with tighter integration into the LangChain ecosystem. LangSmith also supports dataset management, evaluation runs, and prompt playground features. If you are already using LangChain in production, LangSmith offers the fastest path to comprehensive observability.

**Braintrust** focuses on evaluation and observability. It treats every production request as a potential eval case, capturing prompts, completions, and user feedback. Braintrust also supports A/B testing, prompt versioning, and regression detection. It is particularly strong for teams that want to close the loop between production monitoring and evaluation — using real production data to improve your eval suite and using eval results to guide production decisions.

**Arize AI** specializes in model monitoring and drift detection for AI systems. It captures traces, but its primary focus is on model performance over time — detecting when accuracy degrades, when latency increases, or when cost spikes. Arize also supports embedding visualization, feature importance analysis, and fairness metrics. It is more ML-ops-focused than observability-focused, but it integrates well with tracing backends.

You do not need to choose one tool exclusively. Many teams use OpenTelemetry for infrastructure tracing, Langfuse or LangSmith for AI-specific tracing, and Datadog or Grafana for metrics and dashboards. The key is to ensure request IDs flow consistently across all tools, so you can correlate data from different systems.

## Tracing Multi-Agent and Multi-Model Workflows

Tracing becomes more complex when your AI system uses multiple models, multiple agents, or multi-step reasoning. A single user request might trigger a sequence of model calls: a classifier model decides the query type, a retrieval model embeds the query, a generation model produces a response, and an evaluation model scores the response quality. Each model call is a separate span, and the relationships between spans become critical.

For **sequential workflows**, spans are organized linearly. The root span represents the user request. Child spans represent each model call in order. Each child span waits for the previous span to complete before starting. Tracing this is straightforward — each span records its start time, end time, and parent span ID, and the trace naturally forms a linear sequence.

For **parallel workflows**, multiple spans execute concurrently. Your system might call multiple retrieval strategies in parallel — vector search, keyword search, and graph search — then merge the results. Or your system might call multiple models in parallel — GPT-5 and Claude Opus 4.5 — and use the first response or combine responses with a final aggregation step. In a parallel workflow, child spans have the same parent span ID, overlapping time ranges, and concurrent execution. Your tracing backend should visualize this as a horizontal fork-and-join diagram.

For **agentic workflows**, a model generates a plan, executes tools, evaluates results, and decides whether to continue or return a final answer. Each iteration is a loop in the trace. You can represent this as a sequence of child spans beneath the root span, where each span represents one iteration of the agent loop. Or you can create nested spans — an "agent iteration" parent span with child spans for tool calls, guardrail checks, and replanning. The choice depends on whether you need to analyze per-iteration performance or just overall agent latency.

For **multi-agent systems**, where different agents handle different parts of a task, each agent is a separate sub-trace. An orchestrator agent receives the user query, delegates subtasks to specialist agents, collects their responses, and synthesizes a final answer. Each specialist agent has its own trace, and the orchestrator trace links to them via parent-child relationships or explicit trace IDs stored in span attributes. This is the most complex tracing scenario, and it requires careful design to ensure you can navigate from the user request to any agent's internal operations.

## What Tracing Enables That Logs Cannot

Tracing is not a replacement for logs — it is a complement. Logs tell you what happened. Traces tell you how long each step took, which step failed, and how steps are causally related. Logs let you search for specific error messages or specific user IDs. Traces let you see the entire structure of a request and identify bottlenecks visually.

The specific capabilities tracing enables are: **latency attribution** — seeing which stage of your pipeline is slow, which tool call is timing out, which guardrail is adding unexpected latency. **Dependency analysis** — understanding how failures propagate through your system, which services are critical path, which services can fail without blocking the user. **Concurrency visualization** — seeing which operations run in parallel, which operations wait for others, where parallelism could improve performance. **Cardinality analysis** — understanding how many tool calls a typical request makes, how often retrieval is skipped, how frequently guardrails block responses.

Without tracing, you rely on logs and aggregate metrics. You can see that average latency increased, but you cannot see which stage caused the increase. You can see that error rates spiked, but you cannot see whether errors occurred early in the pipeline or late. You can see that costs are rising, but you cannot see whether the cause is prompt bloat, increased retrieval context, or more tool calls per request. Tracing answers these questions with a single query.

The next question is what metadata to capture in each trace and log event to make debugging fast and cost attribution possible — user IDs, session IDs, model versions, prompt hashes, and the dozens of other fields that turn raw telemetry into actionable insight.

