# 8.1 — AI Incident Taxonomy: The Failure Modes Unique to AI Systems

In early March 2025, a customer service platform's incident response team spent forty-three minutes arguing about whether they had an incident at all. User complaints had increased by 22 percent. Response quality scores had dropped from 4.2 to 3.8. But the system was up. Response times were normal. No errors in the logs. Half the team insisted it was a product issue, not an operational issue. The other half wanted to page engineering. By the time they decided it was an incident — a silent model degradation caused by a training data distribution shift — the problem had affected 18,000 customer interactions. The root cause was never in doubt. What took forty-three minutes was recognizing that AI systems fail in ways that traditional incident taxonomies do not capture.

Traditional software incidents have clear signatures. A service is down or it is up. An API returns 500 errors or it does not. Response times spike or they stay flat. AI incidents are different. The system can be technically healthy while delivering completely broken outputs. Latency can be perfect while quality collapses. Error rates can be zero while the model hallucinates confidential information. You need a different taxonomy to recognize AI failures quickly, classify them accurately, and route them to the people who can actually fix them.

## The Silent Degradation Class

The most dangerous AI incidents produce no traditional alerts. The model continues serving requests. Response times remain within SLA. Error rates stay at baseline. But the quality of outputs has degraded in ways that only domain experts or users can detect.

A healthcare documentation assistant spent six days generating clinically inaccurate summaries before anyone noticed. The model had started hallucinating medication dosages — not wildly wrong, but subtly incorrect. A nurse flagged it during a chart review. The engineering team investigated and found that a recent fine-tuning run had introduced catastrophic forgetting of medical domain knowledge. The system had served 4,200 incorrect summaries. Not a single technical metric had fired. No alert had triggered. The incident was invisible to traditional monitoring because the model had not crashed — it had just become subtly incompetent.

Silent degradation incidents share common characteristics. Output format remains valid. API responses are successful. But semantic quality has degraded. The model loses capabilities it previously had, develops new biases, or starts generating outputs that are technically well-formed but factually wrong. Detection requires content-quality monitoring, not just infrastructure monitoring. You need eval-based alerts that measure output correctness, not just system health. If your incident detection relies only on error rates and latency, this entire class of failure is invisible until users complain.

## The Hallucination Escalation Class

Hallucination incidents start small and compound. A model generates one incorrect fact. That fact gets cached. The cached output gets used as context in subsequent requests. The error spreads through the system, creating a cascade of increasingly confident falsehoods.

A legal research platform experienced this in November 2025. A user asked about a recent court ruling. The model hallucinated a case citation that did not exist. That hallucination got cached as a high-quality response. Over the next two hours, fourteen other users received the same fake citation. Three of them used it in client documents. The incident was not a single bad output — it was a systemic propagation of misinformation through the caching layer. The engineering team had to purge the cache, identify all affected sessions, and contact users who had received the hallucinated content. The technical fix took twenty minutes. The user outreach took three weeks.

Hallucination escalation incidents have distinct signatures. A single piece of incorrect information appears in multiple unrelated sessions. Confidence scores remain high. The model does not hedge or show uncertainty. It presents the hallucinated content with the same authority as factual content. Detection requires deduplication analysis across sessions — if the same specific claim appears in responses to semantically different queries, you likely have a hallucination spreading through cache or retrieval. Mitigation requires immediate cache invalidation and retroactive output review for all sessions containing the hallucinated content.

## The Context Poisoning Class

Context poisoning incidents occur when malicious or corrupted content enters the retrieval pipeline and contaminates outputs. The model itself is fine. The problem is what gets fed into the model as context.

A financial advisory platform suffered this in April 2025. An attacker had uploaded fake earnings reports to a publicly accessible document repository that fed the RAG system. The fake reports claimed that three mid-cap companies were about to declare bankruptcy. The RAG pipeline retrieved these documents as authoritative sources. For six hours, the model generated investment advice based on fabricated financial data. The incident was not a model failure — the model accurately summarized the content it was given. The failure was in the retrieval and validation pipeline. By the time the poisoned documents were identified, the platform had served advice to 230 users, twelve of whom had made trading decisions based on the false information.

Context poisoning incidents have a specific signature: outputs are factually incorrect, but the model cites real-seeming sources that do not pass validation. If you log retrieval sources and run automated fact-checking on high-stakes outputs, you can detect these incidents quickly. The model's confidence is not the problem — the model is doing exactly what it is designed to do. The problem is upstream, in what gets retrieved. Mitigation requires source validation, not model retraining. You need content verification pipelines that run before documents enter the retrieval index, and continuous monitoring that detects when retrieved content contradicts known-good reference data.

## The Prompt Injection Class

Prompt injection incidents occur when user inputs manipulate model behavior in ways that bypass safety controls. These are not accidental degradations — they are deliberate attacks that exploit how language models process instructions.

A customer support chatbot was compromised in August 2025. Users discovered that by phrasing requests in a specific way, they could get the model to ignore its refusal training and generate responses about competitors, internal processes, and even confidential customer data. The attack spread through online forums. Within eighteen hours, over 400 users had successfully extracted information the model was explicitly trained not to disclose. The engineering team had to take the system offline, retrain with adversarially robust examples, and implement input filtering that detected injection patterns. The incident cost six days of downtime and required contacting every user who had received improperly disclosed information.

Prompt injection incidents have telltale patterns. Users submit inputs with unusual structure — instructions within instructions, role-playing scenarios, or attempts to redefine the model's identity. Successful injections produce outputs that violate known policy constraints. If you log both inputs and outputs with policy-violation scoring, you can detect these incidents in real time. Mitigation requires input sanitization, output validation, and continuous red-teaming to discover new injection techniques before attackers do.

## The Latency Cascade Class

Latency cascade incidents occur when AI systems degrade under load in ways that compound. A slight increase in traffic causes slightly slower responses. Slower responses cause timeouts. Timeouts trigger retries. Retries increase load. Load increases latency further. The system spirals into failure even though no single component is broken.

A document analysis platform hit this in January 2026. Morning traffic increased by 30 percent — normal for the time of year. But the RAG pipeline's embedding generation was running slightly slower than usual due to a model update. Average response time increased from 800 milliseconds to 1,400 milliseconds. Client applications, configured with 1,500 millisecond timeouts, started timing out and retrying. The retry storm increased load by 60 percent. Response times hit 3,000 milliseconds. More timeouts. More retries. Within twelve minutes, the system was effectively down, not because of a failure but because of a positive feedback loop between latency and retry behavior. The fix was not scaling up — it was implementing exponential backoff and shedding load.

Latency cascade incidents have a distinct shape. Latency increases faster than traffic increases. Retry rates spike. Successful request rates drop while error rates climb. If you monitor the ratio of retry requests to original requests, you can detect cascades early. Mitigation requires aggressive timeout management, exponential backoff on retries, and load shedding that prioritizes original requests over retries. You cannot scale your way out of a latency cascade — you have to break the feedback loop.

## The Model Version Mismatch Class

Model version mismatch incidents occur when different parts of the system assume different model capabilities. One service expects a model that handles structured outputs. Another service sends requests to a model that does not. The mismatch is silent until it causes failures downstream.

A contract analysis platform experienced this in September 2025. The backend had upgraded from Claude Opus 4.0 to Claude Opus 4.5. The new model had better structured output capabilities, so the engineering team simplified parsing logic. But the load balancer was still routing 20 percent of traffic to the old model as part of a gradual rollout. Requests that hit the new model worked perfectly. Requests that hit the old model failed parsing, produced malformed outputs, and caused downstream services to crash. The system looked healthy 80 percent of the time and broken 20 percent of the time. Users experienced intermittent failures that were impossible to reproduce. The incident took four hours to diagnose because no one suspected that different model versions were running simultaneously.

Model version mismatch incidents have a probabilistic signature. Success rates sit at unexpected percentages — 75 percent, 82 percent, values that correlate with traffic distribution across model versions. Failures are not reproducible — the same input succeeds sometimes and fails other times. If you tag every request with model version and correlate error rates with version distribution, you can detect these mismatches immediately. Mitigation requires strict version consistency across all services or capability negotiation that ensures callers only request features the target model supports.

## The Concept Drift Collision Class

Concept drift incidents occur when the model's understanding of terms diverges from user expectations. The model has not degraded — it has learned something different than what users need.

A medical coding assistant experienced this in December 2025. The model had been fine-tuned on recent clinical documentation. During that period, medical terminology had shifted slightly — a specific diagnosis code's usage pattern changed in response to a regulatory update. The model learned the new pattern. But the hospital's workflow still used the old terminology. When clinicians entered traditional descriptions, the model suggested codes based on the new usage. From the model's perspective, it was correct — it was following 2025-2026 patterns. From the clinicians' perspective, it was wrong — it was not matching their institutional vocabulary. The incident lasted eight days and required dual-vocabulary support until the hospital updated its training materials.

Concept drift collision incidents have a specific signature: user complaints increase, but model accuracy on recent eval data stays high or improves. The model is performing well against new data and poorly against old expectations. If you maintain eval sets that represent both historical and recent usage patterns, you can detect drift collisions before they become incidents. Mitigation requires either retraining to preserve historical understanding or explicit version switching where users choose between legacy and current model behavior.

## The Cascading Context Loss Class

Cascading context loss incidents occur in multi-turn interactions where the model loses critical information from earlier turns, leading to increasingly incoherent or dangerous outputs.

A therapy chatbot platform experienced this in May 2025. The system supported hour-long conversations. Early turns established important safety context — a user disclosed they were feeling suicidal, and the model responded appropriately with crisis resources. But the context window was managed poorly. After thirty turns, the original disclosure had been truncated out of context. The model, no longer aware of the user's crisis state, returned to casual conversation. The user felt abandoned. The model had not failed catastrophically — it had failed gradually, losing the most important context first. The incident was only discovered when the user complained to their therapist, who reported it to the platform. The engineering team found that context truncation was deterministic — the oldest messages were always dropped first, regardless of content importance.

Cascading context loss incidents have a temporal signature. Errors occur late in conversations, not early. The model performs well for short interactions and degrades for long ones. If you track coherence scores, safety flag retention, and user satisfaction as a function of conversation length, you can detect when context management is failing. Mitigation requires intelligent context compression that preserves critical information — safety flags, user preferences, established facts — while truncating less important content.

## The Training Data Leak Class

Training data leak incidents occur when the model memorizes and regurgitates sensitive information from training data. These are not prompt injection attacks — they are intrinsic vulnerabilities caused by overfitting.

A customer support model in June 2025 started generating real customer email addresses when asked about example support tickets. The model had been fine-tuned on historical support conversations. It had memorized specific customer information. When users asked for examples of support interactions, the model retrieved memorized examples verbatim, including email addresses, account IDs, and partial credit card numbers. The incident was detected only after a user noticed they were seeing email addresses in supposedly anonymized examples. The engineering team had to take the model offline, retrain from scratch with proper data sanitization, and notify 1,800 customers whose information had been exposed.

Training data leak incidents have a signature: outputs contain information the model should not have access to — real names, real IDs, real contact information — in contexts where synthetic examples should appear. If you run pattern matching on outputs to detect real customer data formats, you can catch these incidents before users do. Mitigation requires training data sanitization, differential privacy during fine-tuning, and output filtering that detects and redacts memorized sensitive patterns.

## The False Negative Spiral Class

False negative spiral incidents occur when safety filters fail to catch violations, users discover the gaps, and exploit them at scale. One missed moderation decision becomes a systematic bypass as users share techniques.

A content moderation platform in October 2025 missed a specific class of policy violation. Users were posting hate speech using coded language the model had not been trained to detect. One user discovered the gap. They posted about it on a forum. Within six hours, 300 users were exploiting the same bypass. The moderation system saw no increase in violations because the content was not being flagged. Human reviewers eventually noticed patterns in user complaints and escalated to engineering. The bypass had been active for two days. The platform had failed to moderate 4,200 policy-violating posts because the false negative was systematic, not random.

False negative spiral incidents have a distinct shape: violations increase but detected violations do not. The gap between true violation rate and caught violation rate widens. If you sample unreviewed content and run secondary moderation passes, you can detect when the primary model is systematically missing a class of violation. Mitigation requires rapid retraining, immediate manual review of all content matching the bypass pattern, and proactive red-teaming to find similar gaps before users do.

## The Incident Classification Decision Tree

When an AI incident is detected, the first five minutes determine whether it takes one hour or ten hours to resolve. The key is rapid classification. Does this incident affect output quality, system availability, safety, or data integrity? Each class has different investigation paths, different mitigation strategies, and different escalation thresholds.

If users report quality problems but infrastructure metrics are normal, you have a silent degradation incident. If the same incorrect information appears in multiple sessions, you have a hallucination escalation incident. If outputs violate policies or expose sensitive data, you have either a prompt injection, context poisoning, or training data leak incident. If latency is spiking faster than load, you have a latency cascade. If failures are intermittent and unpredictable, you have a version mismatch. If user complaints are rising but eval scores are stable, you have a concept drift collision.

The taxonomy matters because each class requires different diagnostic tools. Silent degradation requires eval-based investigation. Hallucination escalation requires cache analysis. Prompt injection requires input log review. Latency cascades require retry rate monitoring. Version mismatches require deployment correlation analysis. Knowing the class tells you where to look first, which logs matter, and who needs to be involved. A clear taxonomy turns the chaotic first five minutes into a structured diagnostic process.

The next question is how fast you can detect incidents in the first place. Classification matters, but only if you know an incident is happening. Detection latency — the time between failure and awareness — determines how much damage occurs before response begins.

