# 9.1 — AI Service Level Objectives: Latency, Quality, Cost, Safety

Traditional SLOs measure availability and latency. An API either returns a response within 200 milliseconds or it does not. You can count successes, failures, and response times with precision. AI systems break this clarity. A model can return a response in 150 milliseconds with 95 percent availability and still fail catastrophically because the output told a customer to do something dangerous, leaked sensitive information, or provided factually incorrect guidance that cost the business a contract.

AI SLOs require a fourth dimension that traditional reliability engineering never needed: **output quality**. Your service can be perfectly available, perfectly fast, and perfectly cheap while simultaneously being perfectly useless or actively harmful. The SRE who designed Google's reliability practices would recognize your latency targets and error budgets. They would not recognize the reality that a 200-millisecond response containing medical misinformation is worse than a timeout.

The challenge is not philosophical. It is operational. You need SLOs you can measure in production, alert on when breached, and use to make real decisions about deployment gates, incident severity, and error budget consumption. This subchapter shows how to define AI SLOs that cover all four dimensions and how to operationalize them without creating alert fatigue or measurement overhead that collapses under production load.

## The Four-Dimension SLO Model

Traditional systems optimize for two dimensions: latency and availability. Add cost as a third dimension for cloud services. AI systems require a fourth: quality. Each dimension has different measurement characteristics, different acceptable thresholds, and different implications when breached.

**Latency** is the easiest to measure and the hardest to predict. A retrieval-augmented generation system might target P95 latency under 800 milliseconds, but actual latency depends on query complexity, retrieval results, context length, and model load. You can measure every request. You cannot predict every request. Your SLO must account for the fact that some queries are inherently slower without treating legitimate complexity as a failure.

**Availability** in AI systems is more nuanced than HTTP status codes. A 200 response with an empty string is not available. A 200 response that says "I cannot answer that question" might be appropriate refusal or might be a retrieval failure. You need to distinguish between legitimate model limitations and system failures. The traditional definition of availability as "returned a non-error response" does not capture whether the system actually worked.

**Quality** is the dimension that separates AI reliability engineering from traditional SRE. You need a metric you can calculate automatically for every production response without human review. That means automated eval scores, safety classifier confidence, or response validation checks. The metric must be fast enough to calculate in real-time or near-real-time and reliable enough that you trust it for alerting. A quality SLO might specify that 98 percent of responses score above 0.7 on your automated quality rubric, measured over a rolling 15-minute window.

**Cost** becomes an SLO dimension when token spend directly impacts margin. A customer service system might target average cost per conversation under 12 cents, measured daily. Breaching the cost SLO does not trigger an incident, but it does trigger investigation and potentially throttling of expensive features. Cost SLOs prevent the scenario where quality and latency are perfect but the service is losing money on every request.

## Defining Quality SLOs Without Human Review

The quality dimension is the hardest to operationalize because it requires judgment. You cannot measure quality the way you measure latency. You need a proxy metric that correlates with actual quality and can be calculated automatically for every response. The proxy must be good enough to trust for alerting but not so expensive that it doubles your inference cost.

The most common approach is a lightweight automated scorer that runs on every response. For a customer support system, this might be a small classifier that predicts whether the response is helpful, safe, and on-topic. The classifier was trained on thousands of human-labeled examples and runs in under 50 milliseconds. Your quality SLO specifies that 97 percent of responses must score above 0.75, measured over a rolling one-hour window. If quality drops below 97 percent for three consecutive measurement windows, you page the on-call engineer.

The threshold matters. Set it too high and you alert on noise. Set it too low and you miss real degradation. The right threshold comes from baseline measurement. Run the scorer on two weeks of production traffic while everything is working well. Find the P5 score—the value where 95 percent of responses score higher. That is your floor. Your SLO threshold sits slightly above the floor to give buffer for normal variance while catching actual degradation.

A second approach is rule-based validation. Every response must pass a set of checks: no personal identifiable information in the output, no refusals on valid queries, no responses shorter than 20 characters unless it is a legitimate short answer, no responses longer than 2000 tokens unless explicitly requested. These checks run in under 10 milliseconds and catch catastrophic failures without requiring a trained model. Your SLO specifies that 99.5 percent of responses pass all validation checks. Validation failures consume error budget even if the response subjectively seems fine.

A third approach is sampling with delayed measurement. You cannot run a full quality eval suite on every production response, but you can sample five percent of traffic and run deeper evaluations with 30-second latency. The sample must be representative—stratified by user segment, query type, and time of day. Your SLO specifies that 95 percent of sampled responses score above 0.8 on your full eval suite, measured with 15-minute delay. This catches quality degradation that lightweight scorers miss while keeping compute cost reasonable.

The key constraint is speed. If your quality measurement takes 10 seconds per response, you cannot use it for real-time SLO monitoring. You can use it for delayed alerting or offline analysis, but you need a faster proxy for real-time decisions. Most teams use a cascade: fast validation checks for real-time alerting, lightweight scorer for five-minute aggregate metrics, full eval suite for sampled delayed measurement.

## Composing SLOs Across Dimensions

Each dimension has its own SLO, but they interact. A system can meet latency SLO while violating quality SLO. A system can meet quality SLO while violating cost SLO. You need rules for how to handle multi-dimensional failures and how to prioritize when you cannot meet all SLOs simultaneously.

The most common pattern is tiered severity. A latency SLO breach alone is a P2 incident. A quality SLO breach alone is a P2 incident. A latency and quality SLO breach together is a P1 incident. An availability SLO breach is always P1 regardless of other dimensions. This captures the reality that some combinations are worse than the sum of parts.

A second pattern is conditional SLOs. Your latency SLO applies only to requests that meet the quality SLO. A fast garbage response does not count as meeting the latency target. This prevents the degenerate solution where you meet latency SLO by returning random strings. The implementation measures both dimensions and only counts requests that meet quality threshold toward latency SLO calculation.

A third pattern is composite SLIs. You define a single service level indicator that combines multiple dimensions. A request is "good" if it meets latency threshold AND quality threshold AND cost threshold. Your SLO specifies that 99 percent of requests are "good," measured over a rolling 24-hour window. This simplifies alerting but makes debugging harder because you need to decompose which dimension failed.

The interaction between quality and cost is particularly important. Improving quality often increases cost. Your quality SLO might specify that 98 percent of responses score above 0.7, but achieving that might require using a more expensive model or longer context windows. Your cost SLO might specify average cost under 10 cents per query. You cannot meet both by upgrading to the most expensive model. You need a decision framework for when quality takes precedence and when cost takes precedence.

The decision framework is usually segment-specific. Enterprise customers get higher quality SLO with relaxed cost SLO. Free-tier users get strict cost SLO with relaxed quality SLO. The system routes requests to different model tiers based on user segment and tracks SLOs separately. A quality SLO breach for enterprise customers is P1. A quality SLO breach for free-tier users is P3. This matches the business impact and prevents alert fatigue.

## Safety as a Non-Negotiable SLO

Safety is not a quality dimension. It is a separate dimension with zero error budget. A response that passes all quality checks but tells a user to harm themselves is not a quality failure, it is a safety failure. Safety SLOs have different thresholds, different alerting, and different escalation paths.

The most common safety SLO is classifier-based. Every response passes through a safety classifier that detects self-harm content, illegal activity instructions, privacy violations, and hate speech. Your safety SLO specifies that 99.99 percent of responses pass the safety classifier, measured over a rolling one-hour window. A single safety failure does not page on-call, but 10 failures in one hour does. A single high-confidence safety failure—where the classifier confidence exceeds 0.95—pages immediately.

The safety classifier must be fast and conservative. Fast means under 30 milliseconds P95 latency. Conservative means it flags borderline cases rather than letting them through. You would rather have a false positive that requires human review than a false negative that reaches a user. The safety SLO measures classifier pass rate, not human review outcomes. If the classifier flags 0.1 percent of responses and human review finds half are false positives, your safety SLO is still met.

A second safety dimension is refusal rate on prohibited queries. Your model should refuse to answer questions about how to build weapons, commit crimes, or evade safety measures. Your safety SLO specifies that the model refuses at least 95 percent of queries from a curated prohibited question set, measured daily by running the set through production. If refusal rate drops below 95 percent, you investigate whether fine-tuning or prompt changes weakened safety behavior.

A third safety dimension is PII leakage. Every response is scanned for email addresses, phone numbers, social security numbers, credit card numbers, and other PII patterns. Your safety SLO specifies that zero responses contain PII from training data or retrieval context. The scanner runs in under 20 milliseconds using regex and keyword matching. A single PII detection is a P1 incident regardless of SLO calculations.

The zero-tolerance nature of safety SLOs changes how you handle error budgets. A traditional SLO might allow 0.1 percent error rate, consuming error budget gradually. A safety SLO has effectively zero error budget. The "SLO" is really a hard constraint, and the measurement exists to detect violations quickly rather than to manage a budget. You do not decide whether to deploy based on safety error budget consumption. You decide whether to immediately roll back based on safety violation detection.

## Measuring SLOs Without Destroying Performance

Every measurement has cost. Latency measurement is cheap—log a timestamp at request start and request end. Quality measurement is expensive—run a model, execute validation rules, or sample for deeper eval. Cost measurement is cheap—sum token counts and apply pricing. Safety measurement is medium—run classifiers that are faster than the main model but not free. You need to measure all dimensions without adding so much overhead that you violate your latency SLO through measurement alone.

The first principle is asynchronous measurement where possible. Latency and availability must be measured synchronously because they are per-request properties. Quality and safety can often be measured asynchronously. Log the response, return it to the user, and measure quality in a background worker. This keeps measurement off the critical path. The tradeoff is delayed detection—if quality drops at 2:00 PM, you might not alert until 2:15 PM. For most systems, 15-minute detection delay is acceptable for quality SLO while immediate detection is required for availability SLO.

The second principle is sampling for expensive measurements. You do not need to run full quality evaluation on every response to detect SLO breaches. Sample five percent of traffic with stratification by user segment and query type. If the sample shows quality SLO breach, investigate the full population. If the sample meets SLO, assume the population does too. The sample size must be large enough for statistical significance—at least 1000 requests per measurement window for a 95 percent confidence interval within plus or minus two percent.

The third principle is cascading measurement. Run cheap checks synchronously, expensive checks asynchronously, and very expensive checks on samples. Every response gets validation rules in under 10 milliseconds. Every response gets logged for asynchronous lightweight scoring. Five percent of responses get full eval suite with 30-second latency. This gives you real-time detection of catastrophic failures, five-minute detection of quality degradation, and 15-minute detection of subtle quality issues, all without adding significant latency.

The fourth principle is pre-aggregation. Do not log every measurement and aggregate later. Aggregate in the service and emit metrics every 30 seconds. Your service tracks a rolling window of the last 1000 requests, calculates P50, P95, and P99 latency, calculates percentage meeting quality threshold, calculates average cost, and emits those aggregates. Your monitoring system stores aggregates, not raw measurements. This reduces logging volume by 100x while preserving the data you need for SLO calculation.

## Setting SLO Thresholds Without Guessing

The wrong way to set SLOs is to pick round numbers. "We will target 99.9 percent availability and 500 millisecond P95 latency." The right way is to measure current performance, understand user impact, and set thresholds that balance ambition with reality. An SLO you cannot meet is not a goal, it is a source of alert fatigue. An SLO you always meet is not useful, it is a vanity metric.

Start with baseline measurement. Run production for two weeks with no SLOs, just measurement. Collect latency, availability, quality, and cost metrics for every request. Calculate P50, P95, and P99 for each dimension. Identify daily and weekly patterns. Your baseline is not your SLO, it is your starting point.

Next, identify the threshold where user experience degrades. For latency, this is often driven by user research or competitive analysis. Users tolerate 1000 millisecond latency for complex queries but expect 300 milliseconds for simple queries. Your SLO threshold sits at the point where incremental latency significantly impacts satisfaction. For quality, the threshold comes from eval suite performance. If responses scoring below 0.7 are rated "not helpful" by users more than 50 percent of the time, your quality SLO threshold is 0.7 or higher.

Then, calculate the buffer. Your SLO threshold should sit above normal variance but below the point where you are constantly firefighting. If your P95 latency in baseline is 600 milliseconds with variance up to 800 milliseconds during peak load, your SLO threshold might be 900 milliseconds. This gives you 100 milliseconds of buffer beyond peak normal performance, allowing you to alert on real degradation without paging for expected variance.

Finally, validate with user impact. Deploy with draft SLOs for one week. When SLO is breached, investigate whether users noticed. If they did not, your threshold is too aggressive. If they noticed before your alert fired, your threshold is too permissive. Tune thresholds based on correlation between SLO breach and user-reported issues.

The thresholds will change over time. As you improve the system, tighten SLOs to maintain pressure for continuous improvement. As traffic grows or model costs increase, you might relax cost SLOs while tightening quality SLOs. SLOs are not static contracts, they are dynamic targets that reflect current system capabilities and business priorities.

## Alerting on SLO Breaches Without Alert Fatigue

An SLO breach does not always require immediate action. If quality SLO is 98 percent measured over 24 hours, a single bad hour that brings you to 97.5 percent does not justify paging someone at 3 AM. You need alerting rules that distinguish between noise and real problems.

The most common pattern is burn rate alerting. Measure how fast you are consuming error budget. If you are consuming budget at twice the normal rate, trigger a low-severity alert. If you are consuming budget at 10x the normal rate, page on-call. This catches fast-moving incidents without paging for slow degradation that can wait until morning.

For AI systems, quality SLO burn rate is particularly useful. Normal variance might drop quality from 98 percent to 97 percent over a few hours. That is 1x burn rate—you are consuming error budget at the expected rate. If quality drops from 98 percent to 94 percent in 15 minutes, that is 20x burn rate. Page immediately. The calculation is automatic: compare current error rate to baseline error rate over the same measurement window.

A second pattern is multi-window alerting. Require SLO breach in three consecutive measurement windows before alerting. A single five-minute window with quality at 96 percent does not page. Three consecutive five-minute windows with quality at 96 percent does page. This filters transient spikes while catching sustained degradation.

A third pattern is dimension combination. Alert only when multiple dimensions are breached simultaneously. A latency SLO breach alone is informational. A latency and quality SLO breach together is actionable. This reduces alert volume while increasing alert relevance. The downside is delayed detection of single-dimension failures, which is acceptable for dimensions like cost but not acceptable for safety.

The safety dimension bypasses normal alerting rules. A single high-confidence safety violation pages immediately regardless of SLO calculation. Safety alerting is not about error budget management, it is about immediate incident response. The alert includes the violating response, the classifier confidence scores, and the user ID for rapid investigation.

Your AI reliability SLOs sit at the intersection of traditional SRE practice and the messy reality that quality is subjective, measurement is expensive, and a technically successful response can still fail the user. The next subchapter covers error budgets specifically for model degradation—how to track quality decline over time and decide when degradation justifies rollback.

