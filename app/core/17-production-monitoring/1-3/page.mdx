# 1.3 — The Three Pillars of AI Observability: Traces, Metrics, Events

Observability for AI systems rests on three foundational pillars: traces, metrics, and events. Traces provide request-level visibility into what happened during a single interaction — what documents were retrieved, what reasoning the model followed, what tools were called, what output was generated. Metrics provide aggregated visibility into trends over time — average accuracy, p95 latency, error rates, cost per request, user satisfaction scores. Events provide discrete signals for specific occurrences — a safety policy violation, a drift detection alert, a user feedback submission, a retrieval cache miss. Each pillar captures different aspects of system behavior, and each pillar answers different questions. Together, they form the instrumentation necessary to understand, debug, and improve AI systems in production.

The three-pillar framework is not new. It originates from distributed systems observability, where traces, metrics, and logs are the standard instrumentation for complex services. The adaptation to AI systems requires extending each pillar to capture semantic information in addition to infrastructure information. Traditional traces show service calls and latencies. AI traces show prompt construction, retrieval results, reasoning steps, and generated outputs. Traditional metrics show request throughput and error rates. AI metrics show output quality, hallucination rates, and policy adherence. Traditional logs capture discrete events like errors and warnings. AI events capture semantic occurrences like feedback signals, drift detection, and content violations. The structure is the same, but the content is richer and more domain-specific.

## Traces: Request-Level Visibility Through the Full Workflow

A trace is a record of everything that happened during a single request. In traditional systems, a trace shows the path a request took through microservices — which services were called, in what order, with what latency, with what status codes. In AI systems, a trace shows the path a request took through the intelligence layer — what was the user query, what documents were retrieved, what was the constructed prompt, what reasoning did the model follow, what output was generated, what tools were called, what was the final response.

Traces are the foundation of debugging. When a user reports that the AI gave a wrong answer, the first step is to retrieve the trace for that request. The trace shows exactly what happened. You can see the user's query, validate that it was parsed correctly, inspect the retrieved documents and assess their relevance, read the constructed prompt and check for errors, examine the model's reasoning and identify logical flaws, review the generated output and compare it to expected behavior, trace tool calls and validate parameters. Every decision the system made is visible. The debugging process is no longer speculative — you have evidence.

The structure of a trace depends on the system architecture. For a simple RAG system, a trace might include the user query, embedding of the query, retrieval results with similarity scores, the assembled prompt with context, the model's generated response, and the final formatted output. For a complex agent, a trace might include the initial goal parsing, the planning step that selected a sequence of actions, each tool call with input parameters and return values, the reasoning updates after each action, the replanning logic if initial actions failed, and the final summary or deliverable. The deeper the system, the richer the trace.

Traces are expensive to store and query at scale. A trace for a complex agent workflow might be tens of kilobytes or more. A system processing millions of requests per day generates gigabytes of trace data. Not all traces are equally valuable. The trace for a request that completed successfully and received positive user feedback is less valuable than the trace for a request that was flagged as incorrect, violated a safety policy, or received negative feedback. A practical approach is to store all traces for a short retention window — seven to thirty days — and selectively retain traces for flagged requests indefinitely. This balances debuggability with cost.

Traces are also the input for post-hoc evals. You can sample traces from production traffic, run automated evaluations on the outputs, and measure quality metrics without re-running the model. This is faster and cheaper than live eval, and it allows you to evaluate quality metrics that were not instrumented at request time. A trace that includes the retrieved documents and the generated output can be evaluated for citation accuracy after the fact. A trace that includes tool call parameters can be evaluated for correctness against an expected action plan. Traces turn production traffic into an evaluation dataset.

The key insight is that traces provide granularity that metrics and events do not. Metrics tell you that accuracy dropped by 6 percent. Traces let you inspect individual requests to see why. Events tell you that a safety violation occurred. Traces let you see the full context that triggered the violation. Metrics and events tell you what happened at an aggregate or discrete level. Traces tell you what happened for a specific user at a specific moment. That level of detail is what makes debugging tractable.

## Metrics: Aggregated Quality, Cost, and Latency Signals

Metrics are aggregated measurements over time. They answer questions like: what is the average accuracy over the last hour? What is the 95th percentile latency? What percentage of requests violated safety policies? How much did inference cost today? Metrics compress individual data points into summary statistics that reveal trends, detect anomalies, and support decision-making. They are the foundation of dashboards, alerting, and operational monitoring.

For AI systems, metrics fall into four categories: quality metrics, performance metrics, cost metrics, and behavioral metrics. Quality metrics measure output correctness — accuracy, precision, recall, F1 score, hallucination rate, citation correctness, policy adherence, user satisfaction. Performance metrics measure execution characteristics — latency, throughput, error rate, timeout rate, retry rate, cache hit rate. Cost metrics measure resource consumption — inference cost per request, total daily spend, cost per user, cost per token, cost by model tier. Behavioral metrics measure user interaction patterns — query length distribution, session length, abandonment rate, feedback frequency, repeat query rate.

The challenge is that quality metrics are expensive to compute. Traditional performance metrics are emitted by the infrastructure as a side effect of execution — latency is measured by subtracting timestamps, throughput is counted by incrementing counters. Quality metrics require evaluation — comparing outputs to ground truth, running classifier-based checks, querying an LLM-as-judge, collecting human feedback. These evaluations have latency and cost. You cannot run a full eval suite on every production request in real time without introducing unacceptable overhead.

The solution is to measure quality metrics on a sample of traffic rather than on all traffic. Run lightweight checks on every request — length checks, regex-based policy checks, profanity filters — and log violations as events. Run expensive evals on a random sample of 1 to 10 percent of requests — LLM-as-judge evaluations, similarity checks against known-good answers, citation validation. Aggregate the results and track metrics over time. The sample gives you a statistically valid estimate of overall quality without the cost of evaluating every request.

The tradeoff is between granularity and cost. Sampling 1 percent of traffic gives you a rough trend line with high variance. Sampling 10 percent gives you a smoother trend line with lower variance but 10x the cost. The right sampling rate depends on request volume, quality variance, and operational budget. A high-volume system with stable quality can sample 1 percent and still detect significant shifts. A lower-volume system or one with high quality variance needs higher sampling to reduce noise. The principle is to sample enough to detect degradation before users complain, but not so much that the eval cost exceeds the inference cost.

Metrics are also the foundation of alerting. You define thresholds for critical metrics — accuracy below 85 percent, hallucination rate above 5 percent, latency above 500 milliseconds, cost per request above 10 cents — and configure alerts that fire when thresholds are breached. This enables proactive detection of degradation rather than reactive response to user complaints. The challenge is setting thresholds that are tight enough to catch real issues but loose enough to avoid alert fatigue. A threshold that is too tight fires on natural variance and trains engineers to ignore alerts. A threshold that is too loose misses degradation until it is severe.

The calibration process is iterative. Start with conservative thresholds based on baseline behavior. Monitor alert frequency and investigate every alert to determine if it was a true positive or a false positive. Tighten thresholds for metrics with low variance and high criticality. Loosen thresholds for metrics with high variance or low impact. Over time, the thresholds converge to a point where alerts are rare but meaningful. The goal is that every alert represents a real issue that requires investigation, and every issue is surfaced by an alert before it reaches users.

Metrics also enable trend analysis and forecasting. You can plot accuracy over the last 30 days and see if it is stable, improving, or degrading. You can compare cost per request before and after a model upgrade and quantify the savings. You can correlate latency with request volume and predict when you will hit capacity. You can track user satisfaction over time and measure the impact of product changes. Metrics turn operational intuition into data-driven decision-making.

## Events: Discrete Signals for Safety, Drift, and Feedback

An event is a discrete occurrence that is worth recording and potentially acting on. In traditional systems, events include errors, warnings, service starts and stops, deployments, configuration changes. In AI systems, events include safety policy violations, drift detection alerts, user feedback submissions, retrieval cache misses, tool call failures, model fallback activations, and content moderation flags. Events are distinct from metrics because they represent individual occurrences rather than aggregated measurements. You do not average events — you count them, filter them, correlate them, and respond to them.

Events provide visibility into occurrences that are too rare or too important to aggregate into metrics. A safety violation occurs infrequently, but every occurrence is critical and requires review. A drift detection alert fires when the input distribution shifts, which might happen once per week but signals a potential quality issue. A user downvotes a response, which is a single data point but represents explicit negative feedback. These occurrences need to be logged as discrete events so they can be investigated individually, not averaged into a metric that hides their significance.

Events are also the foundation of incident response. When a safety violation event fires, it triggers a review workflow. A trust and safety analyst inspects the input, the output, the policy that was violated, and the context of the interaction. If the violation is valid, the output is withheld and the user receives a fallback response. If the violation is a false positive, the policy is adjusted and the event is logged as a calibration data point. The event is not just a log entry — it is an input to an operational process.

Events enable correlation analysis. You can filter events by type, time range, user cohort, or system component and look for patterns. A spike in retrieval cache misses after a deployment suggests that the new model is querying different embeddings. A cluster of safety violations from a specific user suggests adversarial probing. A pattern of tool call failures on a specific API suggests that the API contract changed. Events are the raw material for debugging complex incidents where the root cause is not obvious from metrics alone.

Events also feed into model improvement workflows. User feedback events — thumbs up, thumbs down, explicit corrections — are the highest-quality signal for model performance. They represent real user preferences on real production outputs. These events can be collected into a feedback dataset, used to compute user satisfaction metrics, and fed into fine-tuning or reinforcement learning workflows. The event stream is not just operational telemetry — it is a training signal.

The challenge with events is volume. A high-traffic system might generate millions of events per day. Not all events are equally important. A policy violation event is critical. A cache hit event is routine. A user session start event is context. If all events are logged at the same priority, the critical ones get lost in the noise. The solution is event classification and filtering. Define event types with associated severity levels — critical, warning, info, debug. Log all critical and warning events. Sample info and debug events. Route critical events to alerting systems. Archive warning and info events for post-hoc analysis. The goal is to surface the important events in real time while retaining the full event stream for investigation.

Events also need to be structured. An unstructured log line like "safety violation occurred" is not actionable. A structured event with fields for event type, timestamp, user ID, request ID, policy violated, violation severity, input snippet, output snippet, and context is actionable. Structured events can be queried, filtered, aggregated, and routed to workflows. They turn raw occurrences into operational intelligence.

## What Each Pillar Captures and What It Misses

Each pillar has strengths and limitations. Traces provide complete visibility into individual requests but do not reveal trends or patterns. You can inspect a trace and see exactly what happened, but you cannot see whether the same issue is affecting thousands of other requests without examining thousands of traces. Traces are granular and rich but not aggregated. They answer "what happened here" but not "is this happening everywhere."

Metrics provide visibility into trends and patterns but do not reveal root causes. You can see that accuracy dropped by 8 percent, but you cannot see which queries were affected, what retrieval failures occurred, or what reasoning errors the model made. Metrics are aggregated and trend-revealing but not diagnostic. They answer "is something wrong" but not "why is it wrong."

Events provide visibility into discrete occurrences but do not reveal frequency or impact. You can see that a safety violation occurred, but you cannot see whether violations are increasing over time, whether they correlate with specific user cohorts, or whether they represent 0.01 percent or 5 percent of traffic. Events are discrete and actionable but not statistical. They answer "what just happened" but not "how often does this happen."

The implication is that you need all three pillars. Traces, metrics, and events are complementary. When a metric crosses a threshold and triggers an alert, you query events to see if there are related occurrences — spikes in safety violations, clusters of negative feedback, increases in retrieval failures. When events reveal a pattern, you inspect traces for affected requests to understand the root cause. When traces show an issue with a specific query, you check metrics to see if the issue is isolated or widespread. The three pillars form a closed loop: metrics detect issues, events provide context, traces enable diagnosis.

The operational workflow is: metrics alert on degradation, events filter to critical occurrences, traces diagnose root causes, fixes are deployed, metrics validate that the issue is resolved. Without metrics, you do not detect degradation until users complain. Without events, you do not know which occurrences to investigate. Without traces, you cannot diagnose root causes and are left speculating. The three pillars together enable proactive, evidence-based incident response.

## Building the Instrumentation Layer

Instrumentation is the process of adding code to your system that emits traces, records metrics, and logs events. For traditional systems, instrumentation is often automatic — APM tools provide libraries that wrap HTTP requests, database queries, and service calls and emit telemetry without manual intervention. For AI systems, instrumentation is mostly manual. You must explicitly log retrieval results, prompt construction, model outputs, reasoning steps, and tool calls. The infrastructure does not know what is semantically meaningful, so you must instrument the semantic layer yourself.

The instrumentation layer has three components: a logging library, a telemetry backend, and an analysis interface. The logging library is integrated into your application code and emits structured traces, metrics, and events. The telemetry backend ingests, stores, and indexes the data. The analysis interface provides dashboards, query tools, and alerting configuration. Together, these components form the observability stack.

For AI systems, several tools provide end-to-end observability stacks. Langfuse is an open-source observability platform for LLM applications, with support for tracing, metrics, and user feedback. LangSmith is Anthropic's commercial observability tool, tightly integrated with LangChain and focused on prompt engineering and debugging. Arize Phoenix is a platform for AI observability with strong support for embedding visualization, drift detection, and model performance tracking. Datadog and other APM vendors have added AI-specific integrations that layer semantic tracing on top of infrastructure monitoring. The choice depends on your architecture, budget, and team preferences.

The instrumentation process is iterative. Start by logging traces for every request — full input, full output, retrieval results, prompt construction. This gives you debuggability from day one. Add lightweight metrics — request count, latency, error rate, token count. This gives you basic operational visibility. Add event logging for critical occurrences — safety violations, policy flags, user feedback. This gives you incident detection. Over time, add richer metrics — eval-based quality scores, cost per request, user satisfaction. Add richer events — drift alerts, A/B test assignments, cache performance. The instrumentation grows as the system matures.

The cost of instrumentation is storage, latency, and engineering effort. Logging full traces for every request increases storage costs. Running evals on production traffic increases latency and compute costs. Writing instrumentation code and maintaining it as the system evolves consumes engineering time. These costs are real and must be managed. The benefit is visibility. Without instrumentation, you are operating blind. Every incident takes longer to detect, longer to diagnose, and longer to resolve. The cost of poor observability is higher than the cost of instrumentation.

The principle is that instrumentation is not optional. It is a core requirement for operating AI systems at scale. The earlier you invest in observability, the faster you can iterate, the more confidently you can deploy, and the better you can respond when things go wrong. Teams that treat observability as an afterthought discover during their first major incident that they lack the data needed to debug it. Teams that treat observability as foundational discover issues before they reach users and resolve them with evidence rather than speculation.

## How the Three Pillars Work Together in Practice

An example will clarify how traces, metrics, and events interact. A company runs a customer support AI with RAG over a documentation corpus. They instrument the system to emit traces for every request, compute quality metrics on a 5 percent sample, and log events for safety violations and negative user feedback.

In mid-January 2026, the accuracy metric begins to decline. Over three days, it drops from 89 percent to 81 percent. An alert fires because accuracy fell below the 85 percent threshold. Engineering investigates. They query the event stream for the same time period and see a spike in negative feedback events. They filter traces to requests with negative feedback and inspect a sample. They notice that many of the incorrect answers cite documents from an outdated product version. They query the retrieval system and discover that a recent index update failed to remove deprecated documents. They fix the index, re-run the embedding pipeline, and redeploy.

Within six hours of the fix, the accuracy metric recovers to 88 percent. Negative feedback events return to baseline. The incident is resolved. The detection window was three days — the metric declined gradually, and the alert fired when it crossed the threshold. Without the metric, detection would have taken longer. Without the events, they would not have known that users were giving negative feedback. Without the traces, they would not have identified the root cause as a retrieval issue with outdated documents. The three pillars together enabled detection, triage, diagnosis, and validation.

This workflow is standard for well-instrumented AI systems. Metrics provide early warning. Events provide triage signals. Traces provide diagnostic detail. Fixes are deployed based on evidence, not guesswork. Metrics validate that the fix worked. The loop closes, and the system returns to normal operation. Teams with full observability resolve incidents in hours. Teams without it resolve incidents in days or weeks, if they detect them at all.

The next subchapter will introduce an observability maturity model that describes five levels of AI observability, from blind operation with no AI-specific instrumentation to predictive operation with drift detection and proactive alerting. Most teams start at level 1 or 2. The goal is to reach level 4 or 5, where incidents are detected before they impact users and resolved with evidence rather than speculation.

