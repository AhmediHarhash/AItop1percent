# 11.2 — Real-Time Cost Tracking: Per-Request and Per-User Attribution

Real-time cost tracking is not about accounting precision. It is about knowing, within seconds of a request completing, how much that request cost and which user, feature, and model generated it. When a cost spike happens at 2pm on a Tuesday, you need to trace it back to the deployment, the API key, the prompt version, and the traffic pattern that caused it. That requires logging cost at the same granularity you log latency—per request, with full context, in real time.

## Per-Request Cost Calculation

Every API call to an LLM provider returns token counts: input tokens and output tokens. Those counts, combined with model pricing, give you the cost of the request. You calculate this at request time, log it as a metric, and emit it to your observability stack alongside latency and error rate.

A document summarization service calls Claude Sonnet 4.5 for every request. The API response includes input tokens and output tokens. The service multiplies input tokens by $0.003 per thousand tokens and output tokens by $0.015 per thousand tokens. The result is the estimated cost of the request, logged as a metric tagged with user ID, document type, and model name.

The calculation does not need to match the invoice exactly. Provider billing includes rounding, batching, and occasional promotional credits. What matters is directional accuracy: when token counts double, cost should approximately double. When you switch models, cost should reflect the new pricing. You are building a real-time signal, not an accounting ledger.

Most LLM providers return token counts in the API response. If a provider does not, you estimate token counts using a tokenizer library. The estimates are less precise but still valuable. A 10 percent estimation error is acceptable for observability. A three-week delay waiting for the invoice is not.

## Tagging for Attribution

Per-request cost is useless without context. You need to know which user, feature, endpoint, model, and deployment generated the cost. That means tagging every request with enough metadata to answer any question you will ask later.

A customer support assistant tags every request with user ID, customer ID, conversation ID, intent type, model name, prompt version, feature flag state, and environment. When cost spikes, the team filters by each dimension to find the source. They can see cost per customer, cost per intent, cost per prompt version, and cost per model. The tags turn a single cost-per-request number into a queryable dataset.

Tagging requires planning. You cannot add tags retroactively. If you log cost without user ID, you cannot later calculate cost per user from historical data. The tags you need depend on your architecture and your questions, but most teams need at least these: user or session identifier, endpoint or feature name, model name, prompt version or template ID, environment, and request ID for tracing.

Some teams tag requests with business dimensions: customer tier, subscription plan, geographic region, product feature. A multi-tenant SaaS platform tracks cost per customer tier. When they discover free-tier users cost twice as much per session as paid users, they add rate limits to free accounts and redesign the feature to reduce token usage for low-value traffic.

Tagging adds negligible latency if done correctly. You construct the tags from data you already have—user ID from authentication, model name from routing logic, endpoint from the request path. You do not make additional API calls or database queries. The tags are assembled at request time and logged with the cost metric.

## Real-Time Metric Emission

Cost metrics must flow to your observability platform within seconds, not hours. That means emitting them to the same system you use for latency and error rate: Datadog, Prometheus, Grafana Cloud, or your internal metrics pipeline.

A legal research platform emits cost metrics to Datadog using the same instrumentation that tracks API latency. Every completed LLM request generates a metric named llm.request.cost with tags for user ID, query type, model, and status. The metric appears in Datadog dashboards within 10 seconds. Engineers monitor cost per minute, cost per query type, and cost per model using the same tools they use for latency P95 and error rate.

The metric is a gauge or counter, not a log line. Log lines are useful for debugging individual requests, but metrics are what you graph, alert on, and aggregate. Most observability platforms support custom metrics with arbitrary tags. You treat cost as just another request-level metric, like duration or response size.

Some teams emit metrics asynchronously to avoid adding latency to the response path. After the LLM response is returned to the user, a background task calculates cost and sends it to the metrics pipeline. This keeps the critical path fast while still providing near-real-time visibility.

## Per-User and Per-Customer Cost Tracking

Per-request cost lets you measure aggregate spend, but per-user cost lets you identify outliers. A single user generating 100 times more cost than average might be an internal test account, an API integration gone wrong, or an attacker.

A SaaS analytics platform tracks cost per customer. In January 2026, one customer's daily cost spiked from $12 to $890. The team investigated and discovered the customer had built an internal dashboard that polled the API every 10 seconds, even when no users were active. The platform sent the customer a usage report and suggested caching. The customer fixed the integration, and their cost returned to baseline.

Per-user cost tracking requires storing or aggregating cost by user ID. Some teams use time-series databases like Prometheus or InfluxDB to store per-user cost metrics. Others aggregate in real time using stream processing. A high-throughput system might sample per-user costs—tracking exact costs for 10 percent of users and estimating the rest—to keep storage manageable.

Per-user cost alerts catch abuse early. A fintech assistant alerts when any user exceeds $50 in a single day. Most users cost less than $2 per day. The alert fires twice a month on average, almost always for legitimate reasons: a customer testing a new integration, a data migration script, or a batch job. But twice in 2025, it caught compromised API keys being used to resell access to the model.

## Per-Feature and Per-Endpoint Attribution

Not all features cost the same. Some use cheap models for simple tasks. Some use expensive models for complex reasoning. Some generate long outputs. Some run multi-step workflows. Per-feature cost tracking tells you which parts of your system are burning budget and whether they deliver enough value to justify it.

A document assistant has five features: summarization, Q&A, translation, formatting, and citation extraction. The team tracks cost per feature per day. Summarization costs $300/day and handles 12,000 requests. Q&A costs $1,800/day and handles 8,000 requests. Translation costs $4,200/day and handles 3,000 requests. Citation extraction costs $90/day and handles 15,000 requests.

The data reveals that translation is the most expensive feature per request and Q&A is the most expensive by total spend. Product reviews usage patterns and discovers that most translation requests are simple phrase translations that do not need GPT-5. They route short translation requests to Gemini 3 Flash, reducing per-request cost by 80 percent. Q&A stays on GPT-5 because the quality drop with cheaper models is unacceptable. They accept the cost because the feature drives retention.

Per-endpoint tracking works similarly. If your system has ten API endpoints, you tag every cost metric with the endpoint name. You can then see which endpoints cost the most, which have the highest cost per request, and which show cost growth over time. This is essential for API products where different endpoints serve different use cases with different cost profiles.

## Cost Per Session and Cost Per Conversation

For conversational systems, per-request cost is not enough. A chat session might involve twenty requests. What matters is the total cost of the session, not the cost of each individual message.

A customer support chatbot tracks cost per conversation. Each conversation has a unique session ID. Every request in the conversation logs its cost tagged with the session ID. At the end of the session, the system aggregates total cost across all requests. The median conversation costs $0.08. The 95th percentile costs $0.34. Conversations that exceed $1 are flagged for review.

The team reviews flagged conversations and finds two patterns. First, some users ask the same question repeatedly in different ways, running up costs without getting a useful answer—a UX problem, not a cost problem. Second, some conversations trigger long, multi-step reasoning chains that generate thousands of output tokens—a model selection problem. They add session-level cost budgets that limit total spend per conversation and route high-cost conversations to a fallback model after a threshold.

Per-session tracking requires maintaining session state—at minimum, a mapping from session ID to cumulative cost. Some teams store this in Redis. Others aggregate it in their logging pipeline. The implementation depends on scale and architecture, but the principle is the same: measure cost at the granularity that matches user experience.

## Handling Multi-Model and Multi-Provider Requests

Many systems call multiple models in a single user request. A search query might call a cheap embedding model, a retrieval step, and an expensive reasoning model. To track cost accurately, you log the cost of each model call separately, then sum them for total request cost.

A legal research assistant makes three LLM calls per query: a GPT-5-nano call to classify intent, a Claude Sonnet 4.5 call to retrieve relevant cases, and a GPT-5.1 call to generate the final answer. Each call logs its own cost with tags for model name and step name. The observability dashboard shows total cost per query and cost breakdown by model. The team discovers that 60 percent of cost comes from the final GPT-5.1 call and tests whether Claude Opus 4.5 delivers similar quality at lower cost.

Multi-provider tracking adds complexity when different providers report token counts differently or charge for API overhead. You normalize costs to a common unit—dollars per request—so you can compare across providers. This lets you measure whether switching providers would save money without losing quality.

## Cost Estimation for Streaming Responses

Streaming responses return tokens incrementally. The full token count is not known until the stream ends. You accumulate tokens as they arrive, calculate cost when the stream completes, and emit the metric.

A code assistant streams responses to users. As each token arrives, the client displays it and increments a running token count. When the stream ends, the client sends the final token count to the backend, which calculates cost and logs it. If the stream is interrupted or the client disconnects, the backend estimates cost from partial token counts.

Streaming adds latency to cost visibility—you cannot log cost until the response finishes—but the delay is typically under a second for most queries. For cost observability, that is fast enough. You see cost metrics within seconds of the request completing, which is real-time for production monitoring.

## Logging vs. Metrics: When to Use Each

Cost data lives in two places: structured logs and time-series metrics. Logs capture per-request details for debugging. Metrics aggregate costs for alerting and dashboards. You need both.

Logs include request ID, timestamp, user ID, model, input tokens, output tokens, calculated cost, latency, and any other context you might need to debug a specific request. When a user reports an expensive query, you search logs for their user ID, find the request, and see exactly what happened.

Metrics aggregate costs over time: cost per minute, cost per hour, cost per day. They include tags for grouping: model, endpoint, user tier, environment. You graph metrics on dashboards, set alerts on thresholds, and query them for trends. Metrics answer questions like "what is our current spend rate?" and "how much did Feature X cost this week?"

A document pipeline logs every request to a structured logging system with full details. It also emits a cost metric to Datadog every time a request completes. Engineers use Datadog for real-time monitoring and alerting. They use logs for root cause analysis when something goes wrong. The two systems are complementary, not redundant.

## Cost Attribution for Batch and Background Jobs

Not all AI workload is user-facing. Some systems run batch jobs, background tasks, or scheduled workflows. These jobs do not have user IDs or session IDs, but they still need cost attribution.

A content moderation system runs batch jobs every hour to re-check flagged content. Each job processes 10,000 items. The system tags every LLM request with job ID, batch ID, and task type. At the end of the job, it aggregates total cost and logs it with job metadata. The team tracks cost per job over time and alerts when a job exceeds expected cost by more than 25 percent.

Batch jobs often have predictable costs. If a job processes 10,000 items and each item costs $0.01, the job should cost around $100. When actual cost is $140, you investigate. Maybe the data changed and items are longer. Maybe a model upgrade increased pricing. Maybe a bug caused unnecessary retries. Cost tracking surfaces the anomaly so you can investigate.

## The Infrastructure for Real-Time Cost Tracking

Real-time cost tracking requires infrastructure: instrumentation to calculate cost, tagging to add context, metric emission to send data, and dashboards to visualize it. Most teams build this incrementally.

Start by logging token counts and calculating cost per request. Add tagging for user ID, model, and endpoint. Emit a single cost metric to your observability platform. Build a basic dashboard showing cost per minute and cost per model. Then expand: add per-user tracking, add per-feature attribution, add cost per session, add alerting. You do not need to build everything on day one. You need to start logging cost alongside latency so you can see it when problems happen.

Some teams use third-party platforms that provide cost tracking out of the box: Helicone, Portkey, Langfuse. These tools instrument API calls, calculate costs, and provide dashboards and alerts. They work well for smaller teams or teams that do not want to build custom instrumentation. Larger teams often build their own cost tracking because they need it integrated with existing observability and because they have custom attribution requirements.

The infrastructure does not need to be perfect. It needs to be good enough to catch cost spikes before they compound. Per-request cost logging with basic tagging is enough to provide 80 percent of the value. The remaining 20 percent comes from deep attribution, forecasting, and cost optimization, which we cover in later subchapters.

The next subchapter covers budget alerts—how to set thresholds and fire alerts when spend exceeds safe limits, so you catch runaway costs before the invoice arrives.

