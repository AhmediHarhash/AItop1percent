# 10.4 — Online A/B Testing for Model Variants

Your eval suite says the new model is better. Shadow mode says it works on production data. Canary rollout says it does not break with real users. But none of those answer the question that actually matters: does the new model improve the outcomes you care about? Does it increase user satisfaction, task completion, engagement, retention, revenue? A/B testing answers that question by running both models simultaneously on production traffic and measuring statistical differences in business metrics.

A/B testing is the truth test. Eval suite accuracy does not guarantee user satisfaction. Shadow mode cannot measure engagement. Canary rollout detects regressions but not improvements in hard-to-measure dimensions. A/B testing measures everything your instrumentation captures — thumbs up rates, task completion, time to resolution, follow-up query rate, churn — and tells you whether the new model is actually better by the definition of better that matters to your business.

In September 2025, an e-commerce recommendation platform deployed a new Llama 4 Maverick model that showed 12 percent better relevance scores in offline evaluation compared to their existing Llama 4 Scout model. They ran a 50/50 A/B test for three weeks. The result: relevance scores improved by 11 percent as predicted, but clickthrough rate stayed flat and conversion rate dropped by 3 percent. Investigation revealed that the new model was recommending products that were objectively more relevant to user search queries but less likely to result in purchases because they were higher-priced or had longer shipping times. The model optimized for relevance. The business needed purchase probability. A/B testing caught the misalignment between eval metric and business metric. The team retrained with purchase data in the reward model, tested again, and deployed a version that improved both relevance and conversion.

A/B testing is not optional for model changes that affect user-facing behavior. It is the only way to measure whether your model improvements translate to product improvements.

## The Infrastructure Required for A/B Testing at Scale

Running an A/B test requires infrastructure that can route traffic to multiple model variants, assign users consistently to variants, log which variant served each request, and join model outputs with downstream outcome metrics. The infrastructure has three components: variant assignment, data collection, and analysis pipeline.

Variant assignment determines which users see which model. The assignment must be random to ensure unbiased comparison, stable so users do not switch variants mid-test, and logged so you can attribute outcomes to the correct variant. The standard approach is hashing: hash the user ID with a stable salt, take the modulo of your variant count, and use the result to assign variant A or B. For a 50/50 test, users with even hash values get variant A and users with odd hash values get variant B. The hash ensures randomness. The user ID ensures stability. The salt ensures you can run multiple independent experiments without correlation.

Data collection captures both model outputs and downstream outcomes for every request. Model outputs include the response text, latency, token count, confidence scores, and any structured data the model returns. Downstream outcomes include user actions — clicks, purchases, form submissions, escalations to human support — and satisfaction signals — thumbs up/down, star ratings, follow-up queries. The data collection system must join model outputs with outcomes via a common request ID or user ID. If the model serves a response at 2pm and the user clicks a recommendation at 2:03pm, you need infrastructure that connects those events and attributes the click to the model variant that served the recommendation.

Analysis pipeline processes the collected data and computes statistical comparisons between variants. Mean and variance for quantitative metrics like latency, clickthrough rate, and conversion rate. Proportions and confidence intervals for binary metrics like refusal rate and error rate. Statistical tests to determine whether observed differences are real or noise — t-tests for continuous metrics, chi-square tests for categorical metrics, or more sophisticated methods like sequential testing or Bayesian inference. The analysis must account for multiple comparisons, variance across users, and the time required to reach statistical significance. The output is a dashboard showing key metrics for each variant, confidence intervals, and significance tests.

A document intelligence platform built A/B testing infrastructure using a Python routing layer, Postgres for event storage, and a nightly analysis pipeline in Apache Spark. The routing layer assigned users to variants via consistent hashing and logged assignments to Postgres. Every model request and every user action logged to Postgres with request IDs. The nightly pipeline joined model requests with user actions, computed metrics per variant, and ran statistical tests. The analysis fed a Grafana dashboard that showed 20 key metrics with 95 percent confidence intervals. The infrastructure cost three engineering weeks to build and enabled every subsequent model A/B test without additional engineering.

## Choosing Traffic Splits and Test Duration

The traffic split between variants and the test duration determine how quickly you reach statistical significance and how much risk you expose users to during testing. The decisions depend on expected effect size, baseline metric variance, traffic volume, and acceptable risk.

For exploratory tests where you do not know whether the new model is better, 50/50 splits maximize statistical power. Half of users get variant A, half get variant B. You reach statistical significance fastest because both variants accumulate data at the same rate. The downside is that 50 percent of users are exposed to the new model, which may be worse. If the new model is a regression, half your users suffer until you detect it and stop the test.

For confirmatory tests where you have strong evidence the new model is better — from eval suites, shadow mode, and successful canary — skewed splits like 80/20 or 70/30 reduce risk. Eighty percent of users stay on the proven baseline. Twenty percent test the new model. You still get enough data from the 20 percent to measure impact, but you limit exposure if the new model underperforms. The downside is slower time to significance because the smaller variant accumulates data more slowly.

Test duration depends on how long it takes to accumulate enough data to detect the effect size you care about with acceptable statistical power. If you need to detect a 3 percent improvement in task completion rate and your baseline rate is 65 percent, you need thousands of users per variant. If your traffic volume is 50,000 users per day, you reach significance in two to three days. If your traffic volume is 500 users per day, you need weeks. The math is determined by statistical power calculations before the test starts.

A customer support platform ran a 70/30 A/B test when deploying a new GPT-5.1 model in November 2025. They had high confidence from shadow mode and canary that the model was not a regression, so they used a skewed split to limit risk. Their goal was to detect a 5 percent improvement in resolution rate with 80 percent power and 95 percent confidence. Power calculations showed they needed 8,000 users per variant. At 70/30 split with 30,000 users per day, the 30 percent variant accumulated 9,000 users per day, reaching the threshold in one day. The baseline variant accumulated 21,000 users per day, far exceeding the requirement. They ran the test for three days to account for day-of-week variance, confirmed a 6 percent improvement in resolution rate, and ramped to full deployment.

## The Metrics That Matter in A/B Tests

A/B tests can measure any metric your instrumentation captures, but not all metrics are equally important. The metrics that matter are the ones that reflect the actual goals of deploying the new model. If the goal is improving user satisfaction, measure satisfaction. If the goal is reducing operational cost, measure escalation rate or human review rate. If the goal is increasing revenue, measure conversion or retention. Eval suite metrics are useful for development. Business metrics are the truth for deployment.

Primary metrics are the ones that directly measure the goal. User satisfaction score, task completion rate, conversion rate, retention rate, revenue per user. These are the metrics the test is designed to move. If the primary metric does not improve, the test fails, regardless of what else improves. A new model that is faster and more accurate but decreases user satisfaction is not better. User satisfaction is primary. Latency and accuracy are secondary.

Secondary metrics measure important dimensions that should not regress even if they are not the primary goal. Latency, error rate, refusal rate, escalation rate, cost per query. These are guardrail metrics. The new model must improve the primary metric without significantly regressing secondary metrics. A new model that increases task completion by 8 percent but doubles latency is not obviously better. You must evaluate the trade-off. A/B testing surfaces the trade-off explicitly so stakeholders can make informed decisions.

Tertiary metrics measure interesting dimensions that provide context but do not determine test success. Response length, citation count, reasoning token usage, conversation turn count. These metrics help you understand how the model achieves its results but do not directly measure user value. If the new model generates longer responses and improves satisfaction, the length increase explains the satisfaction gain. If the new model generates longer responses and satisfaction stays flat, the length increase is waste. Tertiary metrics are diagnostic, not evaluative.

A legal research platform defined three-tier metrics for their December 2025 A/B test. Primary metric: user-reported answer quality on a 1-to-5 scale, goal to improve by 0.3 points. Secondary metrics: citation accuracy must not drop below 94 percent, latency must not exceed 1.8 seconds at 95th percentile, refusal rate must not exceed 6 percent. Tertiary metrics: response length, citation count, and reasoning token usage. The test ran for two weeks. The new model improved answer quality by 0.4 points, maintained 96 percent citation accuracy, increased latency by 120 milliseconds to 1.65 seconds, and refusal rate stayed at 5.1 percent. All secondary metrics passed. The tertiary metrics showed the model was generating 18 percent more citations and using 22 percent more reasoning tokens. The team deployed based on primary and secondary metric success.

## Detecting Statistical Significance Without False Positives

A/B testing is a statistical exercise. You observe data from two variants and decide whether the differences are real or random noise. The standard approach is null hypothesis significance testing: assume there is no difference between variants, then calculate the probability of observing your data if that assumption is true. If the probability is low — typically below 5 percent — you reject the null hypothesis and conclude the difference is real. If the probability is high, you cannot conclude anything. The difference might be real but too small to detect with your sample size, or it might be pure noise.

The 5 percent threshold means you accept a 5 percent false positive rate. One time in twenty, you will conclude the new model is better when it is actually not better, just lucky. This is acceptable for low-stakes decisions but dangerous for high-stakes decisions. A content recommendation model that incorrectly concludes a new variant is better wastes some engineering effort and slightly degrades user experience until the mistake is caught. A medical diagnostic model that incorrectly concludes a new variant is better might harm patients. The acceptable false positive rate depends on the stakes.

Multiple comparisons multiply the false positive rate. If you test 20 metrics and use a 5 percent significance threshold for each, you expect one false positive even if all variants are identical. The solution is multiple comparison correction: adjust your significance thresholds using methods like Bonferroni correction, Benjamini-Hochberg procedure, or family-wise error rate control. The corrections reduce your false positive rate at the cost of reduced statistical power. You are less likely to falsely detect a difference, but you are also less likely to detect a real difference. The trade-off is fundamental.

Sequential testing solves a different problem: stopping the test as soon as you have enough evidence without waiting for a predetermined sample size. Traditional A/B testing requires you to decide sample size upfront and run the test until you reach it. Sequential testing lets you check the data continuously and stop as soon as significance is reached. The advantage is faster decisions. The disadvantage is complexity — naive peeking at the data increases false positive rates unless you use proper sequential testing methods like sequential probability ratio tests or Bayesian approaches that account for continuous monitoring.

A financial advisory platform used Bayesian A/B testing for model changes in 2025-2026. Instead of frequentist null hypothesis testing, they computed posterior probability distributions over the difference in key metrics between variants. If the posterior probability that the new model was better exceeded 95 percent, they concluded superiority. If the posterior probability that the new model was worse exceeded 95 percent, they concluded inferiority. If neither threshold was met, they concluded the test was inconclusive and ran longer. The Bayesian approach gave them continuous monitoring without inflated false positive rates and intuitive interpretations: "we are 97 percent confident the new model improves task completion by 4 to 9 percent."

## Handling A/B Tests That Show No Significant Difference

Most A/B tests conclude that the new model is significantly better or significantly worse. But some tests conclude there is no significant difference. The new model neither improves nor degrades key metrics within the statistical power of the test. This result is inconclusive, not informative. It means one of three things: the models are truly equivalent, the difference is real but smaller than your test can detect, or the difference is real but your test duration was too short.

If the models are truly equivalent on the metrics that matter, you choose based on secondary considerations. Cost, latency, maintainability, vendor relationship, model capabilities for future use cases. A model that performs identically but costs 30 percent less is better. A model that performs identically but has 200 millisecond better latency is better. A model that performs identically but gives you access to capabilities you will need in six months is better. Statistical equivalence does not mean the choice does not matter. It means business considerations dominate.

If the difference is real but smaller than your test can detect, you must decide whether detecting that difference matters. A test with 80 percent power to detect a 5 percent improvement in conversion rate will miss a 2 percent improvement even if it is real. If a 2 percent improvement is worth the engineering effort to deploy the new model, run a larger or longer test. If a 2 percent improvement is not worth the effort, conclude that the models are equivalent for your purposes and choose based on secondary considerations.

If test duration was too short, extend the test. Time-dependent metrics like retention or churn require weeks or months to measure. A test that runs for three days can measure immediate satisfaction and task completion but cannot measure whether users are still around 30 days later. If long-term outcomes matter, the test must run long enough to observe them. Most teams run short tests for immediate metrics and rely on long-term production monitoring to catch delayed regressions.

A content moderation platform ran a 50/50 A/B test for two weeks comparing Claude Opus 4.5 to a new fine-tuned variant. The test found no significant difference in accuracy, false positive rate, or false negative rate. The variants were statistically equivalent. The team chose the fine-tuned variant because it was 40 percent faster and cost 25 percent less to run, even though accuracy was identical. The A/B test gave them confidence that accuracy was not degrading, enabling them to optimize for cost and latency without risking quality.

## Designing A/B Tests for Heterogeneous Outcomes

Not all users react to model changes the same way. A model that improves outcomes for 80 percent of users but regresses outcomes for 20 percent may show no difference in aggregate metrics because the improvements and regressions cancel out. Heterogeneous treatment effects are common in AI systems because users have different needs, preferences, and usage patterns. A/B testing must account for this heterogeneity or risk missing important subgroup regressions.

Subgroup analysis splits users into segments and runs separate statistical tests per segment. Segment by user type — free versus paid, individual versus enterprise, new versus experienced. Segment by query type — simple versus complex, common versus rare, text versus multimodal. Segment by geography, language, device, or any other dimension that might correlate with differential impact. If the new model improves outcomes for most segments but regresses outcomes for one critical segment, aggregate metrics miss the problem. Subgroup analysis catches it.

The cost of subgroup analysis is multiple comparisons. If you test 10 segments, you run 10 statistical tests, increasing false positive risk. Apply multiple comparison corrections. Alternatively, preregister which subgroups you will test before running the experiment, treating exploratory subgroup analysis as hypothesis-generating rather than hypothesis-confirming. The discipline prevents cherry-picking subgroups after seeing the data.

A healthcare appointment scheduling assistant ran a 60/40 A/B test in October 2025 comparing GPT-5 to a new GPT-5.2 model. Aggregate metrics showed no significant difference in appointment booking rate. Subgroup analysis revealed that booking rate improved by 12 percent for new patients but decreased by 8 percent for established patients. Investigation found that the new model asked more clarifying questions, which helped new patients who were unfamiliar with the system but frustrated established patients who wanted faster service. The team built a hybrid system that used the new model for new patients and the old model for established patients. Subgroup analysis turned an inconclusive A/B test into a product insight that improved outcomes for both segments.

## When to Stop an A/B Test Early for Ethical Reasons

Most A/B tests run to completion, accumulating data until statistical significance is reached or the test duration expires. But some tests must stop early if evidence emerges that one variant is causing harm. Harm is not the same as underperformance. Underperformance means the new model is worse than the baseline. Harm means the new model is causing outcomes that are ethically or legally unacceptable regardless of comparison to baseline.

Early stopping criteria must be defined before the test starts. Error rates that exceed acceptable thresholds. Refusal rates that deny service to protected groups disproportionately. Outputs that violate content policy or legal requirements. Downstream outcomes that cause user harm — missed diagnoses, incorrect legal advice, financial loss. If these criteria are met, the test stops immediately, the harmful variant is removed from production, and incident response begins. Early stopping is not a statistical decision. It is a safety decision.

The challenge is distinguishing harm from noise. A single bad output is not evidence of systematic harm. A cluster of bad outputs might be. Early stopping requires human judgment guided by statistical evidence. If error rate spikes from 0.5 percent to 3 percent within the first hour of an A/B test, stop immediately. If error rate drifts from 0.5 percent to 0.7 percent over three days, investigate but do not necessarily stop. The magnitude and speed of degradation signal urgency.

A mental health support chatbot ran a 70/30 A/B test in early 2026 testing a new model with improved empathy scores. On day two, reviewers flagged three cases where the new model provided advice that could be interpreted as encouraging self-harm in response to users expressing suicidal ideation. The baseline model had escalated all three cases to human counselors. The team stopped the test immediately, removed the new model from production, and conducted a full safety review. Post-hoc analysis found that the empathy fine-tuning had reduced the model's escalation sensitivity. The team retrained with explicit safety constraints, tested in shadow mode and canary with heavy monitoring, then reran the A/B test successfully. Early stopping prevented potential tragedy.

## Transitioning from A/B Test to Full Rollout

An A/B test ends in one of three ways: the new model is significantly better, the new model is significantly worse, or the test is inconclusive. If the new model is significantly better, you transition to full rollout. If it is significantly worse, you abandon it. If the test is inconclusive, you make a business decision.

Transitioning to full rollout does not mean flipping a switch. The A/B test proved the model is better, but it did not prove the model is safe at 100 percent traffic. The standard transition path is ramping: move from 50/50 to 70/30, then 90/10, then 100/0 over several days or weeks, monitoring metrics at each stage. The ramp catches edge cases that did not appear during the test because they are rare enough that even 50 percent traffic did not trigger them.

Full rollout also requires sunsetting the baseline model. You must ensure no users are still assigned to the old variant, no cached responses are serving old model outputs, and all infrastructure dependencies are updated. The cutover must be clean. A system with 5 percent of users stuck on the old model due to stale feature flags will confuse support teams and break user experience for those users.

Documentation is not optional. Record what you tested, what you measured, what you found, and why you decided to deploy. Future engineers will ask why the system behaves the way it does. The A/B test is the evidence. Document primary and secondary metrics, statistical tests, significance levels, effect sizes, and any subgroup findings. The documentation is the institutional memory that prevents relitigating the same questions six months later.

A legal research platform completed a successful A/B test in January 2026 proving their new Claude Opus 4.5 model improved answer quality by 0.35 points on a 5-point scale with no secondary metric regressions. They transitioned to full rollout using a one-week ramp: 60/40 for two days, 80/20 for two days, 95/5 for three days, then 100/0. The final 5 percent stayed on baseline for an extra three days to serve as a holdout group for long-term impact measurement. After confirming stability at 100 percent, they decommissioned the baseline model endpoint and documented the test results in their internal wiki. The deployment succeeded without incident, and the documentation answered three stakeholder questions in the following two weeks about why the model's citation behavior had changed.

The next subchapter covers rollback criteria design, defining the thresholds and signals that trigger reverting to the previous model when a deployment goes wrong.

