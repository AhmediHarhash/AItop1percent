# 7.5 — Alert Routing: Getting the Right Alert to the Right Person

A database alert paged the machine learning team at 4 AM. The ML engineers woke up, acknowledged the alert, stared at database metrics they did not understand, and escalated to the infrastructure team twenty minutes later. The infrastructure engineer woke up, diagnosed a connection pool exhaustion issue, and fixed it in eight minutes. The total incident duration was thirty-four minutes. Twenty minutes was wasted on routing the alert to people who could not act on it. The ML team lost sleep unnecessarily. The infrastructure team started responding twenty minutes late. The routing failure turned a minor incident into a prolonged outage.

Alert routing is the problem of matching alerts to responders. Every alert should reach the person or team best positioned to investigate and mitigate. That person must have domain expertise, access to relevant systems, and authority to make changes. Routing alerts to the wrong people creates delays, requires escalation, and burns on-call capacity on issues that should have gone to someone else. Routing alerts to too many people creates diffusion of responsibility — everyone assumes someone else will handle it. Routing alerts to too few people creates bottlenecks and single points of failure.

The companies with effective alerting systems treat routing as a first-class problem, not an afterthought. They maintain explicit ownership maps, configure routing rules based on alert content and context, and design escalation paths that ensure every alert reaches someone who can act. This chapter teaches the routing patterns that work in organizations from small teams to large multi-team enterprises.

## Ownership-Based Routing: Mapping Alerts to Teams

Ownership-based routing starts with a service ownership map. Every service, model, API, and infrastructure component has a designated owner team. Every alert is associated with a component. The routing rule is simple: alert about component X goes to team Y, where Y owns X. The mapping is explicit, documented, and enforced by the alerting system.

A healthcare AI company with seven engineering teams and twenty-three production services built their ownership map in a structured data format. Each service had an entry specifying owning team, primary contact, and escalation chain. The alerting system queried this map when routing alerts. An alert about the diagnostic model went to the clinical AI team. An alert about the feature store went to the data platform team. An alert about API gateway performance went to the infrastructure team. The routing was deterministic and transparent.

The map included shared ownership for cross-cutting concerns. The model serving infrastructure was jointly owned by the infrastructure team and the ML platform team. Alerts about serving infrastructure went to both teams simultaneously. The reasoning was that diagnosis required collaboration — infrastructure team understood the compute and networking layers, ML platform team understood the serving framework and model lifecycle. Both perspectives were necessary for effective response. Shared ownership was the exception, not the rule. Most components had single owners to avoid diffusion of responsibility.

The maintenance burden is keeping the ownership map synchronized with organizational reality. Teams reorganize. Services are transferred between teams. New services are launched without updating ownership records. If the map is stale, alerts are routed incorrectly. An insurance AI company discovered this when a model ownership transfer was documented in an engineering announcement but not updated in the alerting system. Alerts for the transferred model kept paging the old team for six weeks. The old team escalated every alert to the new team. The routing failure created friction and delayed responses.

They made ownership updates part of the service transfer checklist. Before a service could be officially transferred, ownership in the alerting system, monitoring dashboards, and incident response playbooks had to be updated. The transfer was not complete until routing was verified — the new owning team received a test alert and confirmed they could respond. This procedural discipline kept the ownership map accurate.

Ownership-based routing works when ownership boundaries are clear and stable. In organizations with ambiguous ownership — multiple teams touching the same components, shared code bases, cross-functional services — ownership-based routing breaks down. Every alert requires escalation because no one team has full context. For those organizations, skill-based or context-based routing is more appropriate.

## Skill-Based Routing: Matching Alerts to Expertise

Skill-based routing directs alerts based on required expertise, not component ownership. An alert about model accuracy degradation goes to someone with ML debugging skills. An alert about database query performance goes to someone with database optimization skills. An alert about cryptographic validation failures goes to someone with security expertise. The routing considers what skills are needed to investigate, not which team owns the component.

A fintech AI platform used skill-based routing for their fraud detection system because incidents often required multiple expertise areas. A model performance alert might indicate data quality issues, requiring data engineering skills, or might indicate adversarial attacks, requiring security skills. Component ownership was clear — the fraud detection team owned the model — but response required pulling in specialists based on incident characteristics.

They implemented skill-based routing with a two-tier system. Primary routing went to the fraud detection team's on-call engineer. The alert included contextual information that suggested likely root cause category: data quality, model drift, adversarial input, infrastructure performance. The on-call engineer could immediately escalate to a specialist based on the suggested category. The escalation was assisted, not automatic — the engineer retained judgment about whether escalation was needed. But the routing system made the escalation path explicit and easy.

The specialist roster was maintained per skill area. Data quality specialists were available during business hours with escalation to senior engineers after hours. Security specialists were available twenty-four hours with rapid response SLAs. Infrastructure specialists were on-call rotation with the infrastructure team. The fraud detection on-call engineer did not need to know who to call — the routing system handled that based on selected skill category. The cognitive load was reduced from "figure out who can help" to "choose the right category."

Skill-based routing requires maintaining accurate skill registries. Engineers rotate, change teams, develop new skills, lose touch with old domains. If the skill registry is stale, routing directs alerts to people who can no longer help. A media AI company addressed this with quarterly skill surveys. Every engineer listed their current skill areas and confidence levels. The skill registry was rebuilt from survey responses. Stale entries were removed. New skills were added. The registry stayed aligned with current team capabilities.

The limitation of skill-based routing is that it introduces latency. Ownership-based routing pages the owner immediately. Skill-based routing pages a generalist who then escalates to a specialist. The escalation adds time. For critical incidents where every minute matters, this delay is costly. The trade-off is between immediate response by someone potentially lacking the right skills versus slightly delayed response by someone with exactly the right skills. For issues where diagnosis is complex, the delay is worthwhile. For issues where any responder can follow a runbook, ownership-based routing is faster.

## Context-Based Routing: Using Alert Content to Determine Destination

Context-based routing inspects alert content — metric names, threshold values, affected resources, time of day — to determine the appropriate recipient. The same component might generate alerts that should go to different teams based on context. API latency alerts during business hours might go to the application team. API latency alerts at 3 AM might go to the infrastructure team because the likely cause is resource constraints, not application code.

An e-commerce AI platform implemented context-based routing for their product recommendation system. Recommendation quality alerts were routed differently based on product category. Alerts affecting high-value categories — electronics, jewelry — paged the senior recommendation team regardless of time. Alerts affecting lower-value categories — household goods, office supplies — went to the general on-call rotation during business hours and were deferred to daily review after hours. The routing acknowledged that business impact varied by context, and on-call burden should match impact.

The routing rules used conditional logic. If alert metric is recommendation accuracy AND affected category is electronics or jewelry AND accuracy drop exceeds ten percentage points, then page senior team. If alert metric is recommendation accuracy AND affected category is household AND accuracy drop is less than fifteen percentage points, then send to review. The rules were complex but encoded real business priorities. Not all failures had equal urgency.

Context-based routing is powerful but fragile. Rules that are sophisticated enough to encode real decision-making are sophisticated enough to have bugs. A transportation AI company had a context-based rule that routed alerts differently on weekends versus weekdays. The rule implementation used local time zone instead of UTC. During daylight saving time transitions, the rule fired incorrectly, routing weekend alerts as if they were weekday alerts. The bug went unnoticed for months because daylight saving transitions are rare and testing did not cover time zone edge cases.

The testing burden for context-based routing is significant. Every conditional branch in routing logic must be tested. Edge cases must be identified and verified. Changes to routing rules must be reviewed and tested before deployment. The complexity is similar to application code complexity. If your routing rules span hundreds of lines of configuration or code, you have routing logic that needs software engineering discipline — version control, code review, automated testing, gradual rollout. Many teams are unprepared for this. They treat routing configuration as simpler than it is, leading to production bugs that cause alerts to reach wrong destinations.

A practical pattern is to keep routing rules simple and use routing as triage rather than final destination. Initial routing is coarse-grained — alerts about AI models go to AI team, alerts about infrastructure go to infrastructure team. The receiving team performs triage and re-routes if necessary. The re-routing is a human decision, not an automated rule. This keeps the automated routing simple and robust while allowing human judgment to handle edge cases.

## Geographic and Time Zone Routing: Follow-the-Sun Patterns

For organizations operating globally, alert routing must account for time zones. Waking engineers at 3 AM should be reserved for genuine emergencies. Issues that can wait until morning should route to engineers in time zones where it is currently business hours. Follow-the-sun routing directs alerts to whichever team is currently on-shift, reducing after-hours pages and distributing load globally.

A multinational insurance company with engineering teams in North America, Europe, and Asia implemented follow-the-sun routing for their AI underwriting systems. Review-level alerts routed to whichever region was currently in business hours. Page-level alerts routed to the primary owner team regardless of time zone — emergencies justify waking people up. The routing system maintained awareness of regional holidays and team availability. If the European team was unavailable due to a holiday, their alerts routed to the North American team during European daytime hours.

The implementation tracked on-call schedules per region and per team. The alerting system queried the schedule API to determine who was currently available in each region. Alerts were routed to available on-call engineers in time zones where current local time was between 8 AM and 6 PM. If no one was available in business-hours time zones, the alert routed to the primary owner team regardless of time zone. The logic prioritized letting engineers sleep while still ensuring critical alerts reached someone.

Follow-the-sun routing requires cultural and organizational investment. Teams in different regions must have sufficient expertise overlap to handle each other's alerts. Documentation must be comprehensive enough that an engineer in Singapore can debug a system primarily managed by a team in California. Handoffs must be explicit — when the Asia-Pacific shift ends and the European shift begins, context about ongoing incidents must transfer. These practices are standard in global operations centers. They are less common in engineering organizations, where teams often operate in regional silos.

The complexity increases with regulatory and data access constraints. An engineer in the EU might not have legal access to customer data stored in North America due to data residency regulations. If an alert requires examining customer data to diagnose, routing to an EU engineer creates a legal problem even if it solves a time zone problem. The routing system must be aware of data access boundaries and route accordingly. For some organizations, this complexity makes follow-the-sun routing impractical. They accept after-hours pages as a cost of compliance.

## Escalation Path Configuration: Who Gets Alerted When Primary Fails

Every routing configuration needs an escalation path. The primary recipient might be unavailable, might not acknowledge the alert, or might acknowledge but fail to resolve the issue. Escalation policies define what happens in each case: who gets notified next, how long to wait before escalating, and how many escalation levels exist before reaching executive leadership.

A healthcare AI company configured three-tier escalation for critical alerts. Primary on-call engineer received the alert first. If not acknowledged within five minutes, the backup on-call engineer received the alert. If neither acknowledged within ten minutes, the engineering manager and site reliability lead both received the alert. The escalation was automatic and time-based. No human needed to decide when to escalate. The system enforced response time requirements.

The escalation timing was tuned based on incident urgency. For alerts indicating active user harm — patients receiving incorrect medical information — the escalation was aggressive: five minutes to primary, two minutes to backup, immediate to leadership. For alerts indicating potential future harm — model accuracy declining but still above minimum thresholds — the escalation was relaxed: fifteen minutes to primary, thirty minutes to backup, business hours to leadership. The escalation speed matched the severity.

Escalation policies also handled resolution failures. If an alert was acknowledged but not resolved within a time threshold, the alert re-escalated. A financial services company set resolution time expectations based on alert category. Critical alerts required resolution or mitigation within thirty minutes. If the on-call engineer acknowledged the alert but the condition persisted after thirty minutes, the alert escalated to the engineering manager with a summary of actions taken. The escalation indicated that the incident exceeded the on-call engineer's capacity and required additional resources.

The design challenge is avoiding escalation fatigue at higher tiers. If every unacknowledged alert escalates to engineering managers, and alerts fire frequently, managers become de facto on-call engineers. This is unsustainable. The fix is to make primary acknowledgment reliable enough that escalation is rare. If alerts are well-calibrated, actionable, and reach the right primary recipient, acknowledgment rates should exceed ninety-five percent. Escalation becomes the exception, not the rule. When escalation does happen, it represents a genuine gap — the primary was unavailable, or the incident was more complex than usual. These are the cases where leadership involvement is appropriate.

## Notification Channel Configuration: Phone, SMS, Push, Email

Routing includes selecting notification channels. Critical alerts should use high-interrupt channels — phone calls, SMS. Lower-priority alerts can use lower-interrupt channels — push notifications, email. The channel selection affects how quickly humans notice and respond to alerts. Choosing the wrong channel delays response or creates unnecessary interruption.

A retail AI platform configured channel selection based on alert severity and time of day. Page-level alerts during on-call hours used phone calls — maximum interrupt, immediate attention. Page-level alerts during off-hours used SMS followed by phone call if not acknowledged within two minutes — slightly less aggressive to avoid waking engineers for alerts they had already acknowledged on mobile devices. Review-level alerts used push notifications during business hours and email during off-hours. Track-level monitoring never generated notifications — data was available in dashboards but did not push to any channel.

The implementation integrated with multiple notification services. PagerDuty for phone calls and SMS. Slack for push notifications and team channels. Email for daily digests and low-priority alerts. Each service had different reliability characteristics and latency profiles. Phone calls had the highest reliability but also the highest interrupt cost. Email had low interrupt cost but lower reliability — engineers do not check email at 3 AM. The channel selection balanced reliability and interrupt cost based on alert urgency.

Channel configuration must account for notification fatigue. If every alert uses the highest-interrupt channel, engineers become desensitized. The phone ringing at 3 AM stops being an emergency signal and becomes a routine annoyance. The interrupt loses effectiveness. The correct pattern is to reserve high-interrupt channels for genuine emergencies and use progressively lower-interrupt channels for lower-urgency alerts. The channel hierarchy reinforces the alert category hierarchy. Page-level alerts feel like pages. Review-level alerts feel like reviews. The medium matches the message.

One common mistake is allowing individuals to customize notification channels without constraints. An engineer disables phone notifications because they find them disruptive. Now critical alerts go to SMS, which the engineer does not notice while sleeping. The alert goes unacknowledged. The incident escalates. The customization that improved the engineer's comfort reduced the team's reliability. Channel configuration must be standardized for on-call roles. Individual preferences are acceptable for personal monitoring, not for on-call responsibilities.

The next subchapter covers escalation policies in depth — the rules and processes that ensure alerts reach resolution even when the primary responder is unavailable, unresponsive, or unable to resolve the issue alone.

