# 8.4 — The First Five Minutes: Initial Triage for AI Incidents

The alert fired at 2:34 AM: eval pass rate dropped from 91 percent to 78 percent. The on-call engineer opened the incident channel, started typing a message, then stopped. What should the message say? The engineer did not know the root cause yet. Did not know if it was still happening. Did not know if users were affected. Spent three minutes staring at dashboards trying to understand the scope before posting anything. By the time the first message went out — "investigating eval drop, unclear cause" — seven minutes had passed since the alert. Other engineers woke up, saw the vague message, and had to ask basic questions: Is this still happening? Which model? Which customers? The lack of structured initial triage turned a straightforward incident into a coordination nightmare that lasted six hours.

The first five minutes of an AI incident determine the shape of the entire response. If you gather the right information, escalate to the right people, and communicate clearly, the incident resolves quickly. If you waste time investigating the wrong things, involve the wrong stakeholders, or fail to establish basic facts, the incident drags on for hours. You need a triage protocol — a checklist of actions that happen in the first five minutes, regardless of incident type.

## The Incident Declaration Decision

The first decision is whether to declare an incident at all. Not every alert requires formal incident response. If an alert fires but metrics recover immediately, it is a transient blip. If an alert fires and metrics stay degraded, it is an incident.

A travel booking assistant had a policy: if an alert fires and metrics do not return to baseline within three minutes, declare an incident. In February 2026, an alert fired for increased latency. The on-call engineer watched the dashboard. Latency stayed elevated. At the three-minute mark, the engineer declared an incident, opened a channel, and started triage. The investigation revealed that a dependency service was restarting. Latency returned to normal after six minutes. The incident was resolved quickly, but the declaration decision was correct — the metrics had been degraded long enough to require coordinated response.

Contrast this with a document analysis platform that did not have a clear declaration policy. Alerts fired frequently. Engineers investigated individually. Sometimes they posted in Slack. Sometimes they did not. When a real incident occurred — model quality degrading due to a bad deployment — the response was fragmented. Different engineers investigated different symptoms in isolation. No one realized they were all working on the same incident until forty minutes had passed. The lack of a clear declaration threshold led to slow, uncoordinated response.

The declaration threshold should be based on severity and persistence. For critical alerts — safety violations, data exposure, total outages — declare immediately. For high alerts — quality degradation, significant latency increases — declare if metrics stay degraded for more than two minutes. For medium alerts — minor degradations, localized issues — declare if metrics stay degraded for more than five minutes. For low alerts — edge cases, minor bugs — do not declare an incident. Create a ticket and investigate during business hours.

## The Incident Channel and Initial Message

Once an incident is declared, create a dedicated communication channel immediately. The channel becomes the single source of truth for the incident. All updates, all decisions, all coordination happens there.

A customer service chatbot used incident.io for incident management. When the on-call engineer declared an incident, incident.io automatically created a Slack channel, invited relevant stakeholders, and posted a template for the initial message. The template had fields for: incident ID, time detected, affected system, current status, severity, and initial symptoms. The engineer filled out the template in under ninety seconds. Everyone joining the channel had context immediately. No one had to ask "what is happening?" The structured initial message eliminated the fifteen minutes of chaos that usually follows an incident declaration.

The initial message should be factual, not speculative. State what you know, not what you think. A financial advisory platform had an incident in July 2025 where the on-call engineer's initial message was "model seems to be hallucinating." This was speculation. Other engineers joined the channel and started investigating hallucination-specific issues. Twenty minutes later, it became clear the problem was not hallucination — it was stale retrieval data. The speculative initial message sent the response in the wrong direction. A better initial message would have been: "eval pass rate dropped from 94 percent to 79 percent at 03:14. Investigating cause. Latency and error rates normal."

The initial message should include: time of detection, affected metrics, severity classification, impacted systems or customers, and current status. It should not include: speculation about root cause, discussion of potential fixes, or assurances that it will be resolved quickly. The first message establishes facts. Diagnosis and resolution come later.

## The Immediate Scope Assessment

The second action in the first five minutes is scope assessment. How many users are affected? Which systems are impacted? Is it getting worse or staying stable? You need to answer these questions before you can decide who to involve and how to respond.

A contract analysis platform had a dashboard specifically for incident scope assessment. The dashboard showed: number of affected requests per minute, percentage of users experiencing errors, geographic distribution of impact, and which customers were affected. When an incident occurred in December 2025, the on-call engineer opened the scope dashboard and within ninety seconds determined: 6 percent of requests failing, affecting 14 customers, concentrated in US East region. This information went into the incident channel immediately. The engineering team decided to escalate to the infrastructure team because the regional concentration suggested a deployment or network issue. The scope assessment directed the investigation toward the right root cause.

Scope assessment requires pre-built tooling. You cannot afford to write custom queries during an incident. You need dashboards that show: affected request percentage, user distribution, geographic distribution, customer distribution, and temporal trend — is impact increasing, stable, or decreasing. If the trend is increasing, urgency escalates. If the trend is stable or decreasing, you have more time to investigate.

A healthcare documentation platform missed this. When an incident occurred in April 2025, the on-call engineer spent eight minutes writing SQL queries to determine scope. By the time scope was clear, the incident had expanded from 4 percent of requests to 11 percent. The delay in scope assessment led to delay in escalation, which led to extended user impact. The team later built a scope dashboard that required zero custom queries. The next incident was scoped in under two minutes.

## The Immediate Mitigation Check

The third action is checking whether immediate mitigation is possible. Can you roll back a recent deployment? Can you route traffic away from a failing component? Can you disable a feature that is causing problems? You do not need to understand the root cause to apply immediate mitigation.

A document summarization platform had an incident in November 2025 where eval pass rates dropped immediately after a model deployment. The on-call engineer did not spend time diagnosing why the new model was failing. The engineer immediately rolled back to the previous version. Eval pass rates returned to baseline within three minutes. Total incident duration: nine minutes. The root cause investigation happened later, during business hours, with no user impact.

Immediate mitigation is only possible if you have rollback capabilities built in advance. You need one-click rollback for model deployments. You need traffic routing controls that shift load between model versions. You need feature flags that disable problematic features without redeploying. A customer service chatbot had none of these. When an incident occurred, the only mitigation option was to take the system offline, fix the issue, and redeploy. The mitigation took four hours. A competitor with rollback capabilities resolved a similar incident in eight minutes.

The immediate mitigation check should happen within the first three minutes. If a recent change caused the issue, roll it back. If a specific component is failing, route around it. If a feature is causing harm, disable it. Do not wait to understand why something is broken before you stop it from causing more damage. Mitigation first, diagnosis second.

## The Escalation Decision

The fourth action is deciding who else needs to be involved. Not every incident requires waking up the VP of Engineering. But some do. The escalation decision should be based on severity, scope, and whether the on-call engineer has the authority and expertise to resolve the issue independently.

A travel booking assistant had clear escalation rules. Critical incidents: page the engineering manager immediately and notify the VP within fifteen minutes. High incidents: notify the engineering manager via Slack, page if no response in thirty minutes. Medium incidents: notify the engineering manager via Slack, no paging. Low incidents: no escalation required. In January 2026, an incident was classified as high severity. The on-call engineer sent a Slack message to the engineering manager. The manager responded within four minutes, joined the incident channel, and helped coordinate response. The escalation was smooth because the rules were clear.

Escalation rules should also cover when to involve non-engineering teams. If user impact is confirmed, escalate to product and customer success. If data exposure is suspected, escalate to legal and security. If the issue affects multiple customers, escalate to account management. A financial advisory platform had an incident in September 2025 where the engineering team spent two hours fixing a technical issue without notifying the customer success team. When customers started complaining, the customer success team had no context and gave inconsistent responses. The lack of early escalation turned a technical problem into a customer trust problem.

The escalation decision should happen within the first five minutes. If severity is high or critical, escalate immediately. If severity is medium and the on-call engineer does not have an obvious path to resolution, escalate. If severity is low, no escalation is required. The goal is to involve the right people early, not to solve everything independently before asking for help.

## The Baseline and Comparison Check

The fifth action is establishing a baseline. What did normal look like before the incident? What changed? You need comparison points to understand degradation magnitude and detect ongoing changes.

A content moderation platform had an incident in May 2025 where policy violation rates increased. The on-call engineer looked at the current rate: 2.3 percent. But without a baseline, that number was meaningless. Was 2.3 percent high or normal? The engineer spent six minutes finding historical data and determined that the baseline was 1.1 percent. The violation rate had doubled. This context changed the severity classification from medium to high and triggered immediate escalation.

The baseline check should be automated. Your incident dashboard should show: current value, baseline value, percentage change, and historical trend. If eval pass rate is 81 percent, the dashboard should also show: baseline 93 percent, change negative 13 percent, trend declining over past thirty minutes. A legal document assistant built this into their monitoring. When an incident occurred in March 2026, the on-call engineer had full baseline context within fifteen seconds. No manual queries. No hunting through old dashboards. The comparison data was right there.

The baseline check also establishes whether the incident is still active or has resolved itself. If current metrics are degraded relative to baseline, the incident is active. If current metrics have returned to baseline, the incident may have self-resolved, but you still need to understand why it happened to prevent recurrence. A healthcare documentation platform had an incident where eval pass rates dropped, recovered ten minutes later, and then dropped again. The initial recovery made the team think the issue was transient. The second drop revealed it was intermittent. Without continuous baseline comparison, they would have missed the pattern.

## The Hypothesis Generation Phase

Once basic facts are established — scope, severity, baseline comparison, immediate mitigation attempts — the first five minutes ends with hypothesis generation. What are the possible causes? What should you investigate first?

A customer service chatbot had a structured hypothesis generation process. When an incident occurred, the on-call engineer generated a list of potential causes in the incident channel: recent deployment, dependency failure, data pipeline issue, traffic spike, or external attack. The list was not exhaustive. It was a starting point. Other engineers joined the channel, saw the hypothesis list, and split up to investigate different possibilities. One engineer checked recent deployments. Another checked dependency health. A third reviewed traffic patterns. The parallel investigation reduced time to root cause from thirty minutes to eight minutes.

Hypothesis generation should be informed by the incident taxonomy. If eval pass rates dropped suddenly, likely causes are: recent model deployment, training data corruption, or catastrophic forgetting. If latency spiked, likely causes are: traffic surge, dependency slowness, or resource exhaustion. If policy violation rates increased, likely causes are: adversarial attack, model regression, or misconfigured filters. The taxonomy provides a shortlist of possibilities. The investigation tests each hypothesis in priority order.

The first five minutes should end with a posted hypothesis list in the incident channel. The list should be ordered by likelihood or by ease of testing. A financial advisory platform had an incident in October 2025 where the engineering team spent twenty minutes debating hypotheses in the incident channel before starting investigation. By the time they started testing, the incident had been active for twenty-five minutes. A better approach: post the hypothesis list in under ninety seconds, assign each hypothesis to an engineer, and start testing immediately. Discussion happens in parallel with investigation, not before it.

## The Five-Minute Triage Checklist

To ensure consistent triage, the first five minutes should follow a checklist. The checklist prevents skipped steps and ensures all critical information is gathered before deep investigation begins.

Minute one: Declare incident, create channel, post initial message with known facts. Minute two: Assess scope — affected users, systems, customers, regions. Minute three: Attempt immediate mitigation — rollback, traffic routing, feature disable. Minute four: Decide escalation — who needs to be paged, who needs to be notified. Minute five: Establish baseline comparison, generate hypothesis list, assign investigation tasks.

A contract analysis platform documented this checklist in their incident response runbook. When an incident occurred in August 2025, the on-call engineer followed the checklist step by step. The first five minutes were structured, efficient, and set up a successful investigation. The incident was resolved in forty minutes. Previous incidents without the checklist had taken two to four hours because the initial triage was chaotic and incomplete.

The checklist should be trained regularly. Run simulated incidents where engineers practice the first five minutes under time pressure. A travel booking assistant ran quarterly incident drills. The drill scenario gave engineers a synthetic alert and required them to complete the five-minute triage checklist. The drills revealed gaps in tooling — engineers could not assess scope quickly because the dashboard was slow. The team fixed the dashboard. The next real incident, the triage was fast and accurate.

## The Communication Cadence

During the first five minutes, communication should be frequent and factual. Post updates every ninety seconds even if the update is "still investigating, no new information." Silence creates anxiety and leads to duplicate effort as stakeholders start their own investigations.

A document analysis platform had an incident in January 2026 where the on-call engineer went silent for twelve minutes while investigating. Other engineers woke up, saw the initial message, and started investigating independently because they did not know what the on-call engineer was doing. Three people investigated the same hypothesis in parallel. When the on-call engineer finally posted an update, it was clear that coordination had broken down. The team later implemented a rule: post an update every ninety seconds during triage, even if the update is "no new information, still checking X."

The communication cadence should include: status updates, hypothesis test results, mitigation attempts, and escalation decisions. If an engineer tests a hypothesis and rules it out, post the result. If an engineer attempts a mitigation and it does not work, post the result. If an engineer escalates to another team, post the decision. The incident channel becomes a real-time log of everything the team knows and everything the team is trying.

A financial advisory platform used incident.io's timeline feature. Every action taken during the incident was automatically logged with a timestamp. The timeline made it clear who was working on what, what had been tried, and what had been ruled out. When new engineers joined the incident, they read the timeline and immediately understood the state of the response. The structured communication eliminated the "what is happening?" questions that usually slow down incident response.

## The Triage Handoff

If the on-call engineer cannot resolve the incident quickly, triage includes preparing for handoff. Another engineer may take over as incident commander. The handoff should be seamless, with full context transferred in under two minutes.

A healthcare documentation platform had an incident in July 2025 that started at 3:00 AM. The on-call engineer triaged the incident, attempted immediate mitigation, and determined it required deep model debugging. At 3:30 AM, the engineering manager woke up and took over as incident commander. The on-call engineer posted a handoff summary: incident timeline, scope, attempted mitigations, current hypothesis list, and what was still unknown. The engineering manager read the summary, asked two clarifying questions, and took over. The handoff took ninety seconds. There was no context loss, no repeated investigation, no confusion.

The handoff summary should be a single message with structured fields. Incident start time, current duration, severity, scope, user impact, attempted mitigations, tested hypotheses, current investigation focus, and open questions. A contract analysis platform used a handoff template that enforced this structure. When a handoff was needed, the outgoing incident commander filled out the template and posted it. The incoming commander had everything they needed to continue the response without asking for clarification.

## The Triage Failure Modes

Triage fails in predictable ways. The most common failure is analysis paralysis — spending too long trying to understand the root cause before taking any action. A content moderation platform had an incident in March 2026 where the on-call engineer spent eighteen minutes investigating the root cause before attempting any mitigation. By the time mitigation was applied, the incident had affected 14,000 requests. The engineer later admitted they felt they needed to understand the problem before acting. But understanding is not required for mitigation. If a recent deployment coincides with an incident, roll it back. If the issue persists, you have learned something. If it resolves, you have stopped the damage.

The second failure mode is inadequate communication. Engineers investigate quietly, post sparse updates, and leave stakeholders confused. A customer service chatbot had an incident where the on-call engineer investigated for thirty minutes without posting a single update. The engineering manager woke up, saw the initial message from thirty minutes ago, and had no idea if the issue was still happening. The manager paged two more engineers. When they joined the incident channel, they found that the on-call engineer had already resolved the issue but had not posted an update. The communication failure led to unnecessary escalation and wasted effort.

The third failure mode is incorrect severity classification. If an incident is classified as medium when it should be high, response is too slow. If classified as high when it should be medium, resources are wasted and alert fatigue increases. A travel booking assistant had an incident where eval pass rates dropped by 6 percent. The on-call engineer classified it as medium and investigated casually. Two hours later, user complaints started arriving. The severity was reclassified to high. The delayed classification led to delayed escalation and extended user impact. The issue should have been classified as high from the start based on the magnitude of the eval drop.

## The Triage Success Criteria

Successful triage is complete when: incident is declared and communicated, severity is classified, scope is assessed, immediate mitigation is attempted, escalation decisions are made, baseline comparison is established, hypothesis list is generated, and investigation tasks are assigned. If all of this happens in the first five minutes, the incident is set up for fast resolution.

A legal document assistant tracked triage completion time for every incident in 2025. Incidents where triage was completed in under five minutes had an average resolution time of 42 minutes. Incidents where triage took longer than ten minutes had an average resolution time of 3.2 hours. The correlation was clear: fast triage leads to fast resolution. The team invested in better tooling, clearer checklists, and more incident drills. Triage completion time improved from an average of eight minutes in Q1 to an average of four minutes in Q4. Incident resolution time dropped by 45 percent.

The triage phase ends when deep investigation begins. You have gathered the facts, attempted immediate fixes, and generated hypotheses. Now you need to find the root cause. Root cause investigation for AI incidents requires different techniques than traditional software debugging — the failure is often in model behavior, not code logic.

