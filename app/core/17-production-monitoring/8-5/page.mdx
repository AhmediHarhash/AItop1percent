# 8.5 — Root Cause Investigation: Debugging Production AI Failures

Twenty-three minutes into the incident, the engineering team had attempted three rollbacks, restarted two services, and increased inference capacity by 40 percent. Nothing worked. Eval pass rates stayed at 73 percent. The model was still producing incorrect outputs. The team had jumped straight from triage to random fixes without understanding what was actually wrong. At minute thirty, the engineering manager stopped everyone and said: "We need to find the root cause. Stop guessing." The team spent the next fifteen minutes doing systematic investigation. They discovered that a training data pipeline had introduced duplicates, causing the model to overfit to a small set of examples. The fix took eight minutes once they understood the problem. But they had wasted thirty minutes on guesses. Root cause investigation is not optional. It is the fastest path to resolution.

Traditional software debugging follows predictable patterns. You reproduce the issue, read stack traces, add logging, and trace execution flow. AI incident investigation is different. There is no stack trace for a hallucination. There is no error code for catastrophic forgetting. You cannot step through model inference with a debugger. You need investigation techniques specific to AI failures — techniques that diagnose model behavior, data issues, and prompt-context interactions.

## The Correlation Analysis Phase

The first investigation technique is correlation analysis. When did the incident start? What changed at that time? The goal is to find temporal correlation between the incident and a system change — a deployment, a configuration update, a traffic shift, or an external event.

A customer service chatbot had an incident in December 2025 where response quality degraded suddenly at 14:22. The investigation team pulled deployment logs. At 14:18, a new version of the prompt template had been deployed. Four-minute gap between deployment and degradation — strong temporal correlation. The team rolled back the prompt template. Quality returned to normal. Total investigation time: six minutes. The temporal correlation pointed directly to the root cause.

Correlation analysis requires comprehensive change logs. You need to know: every model deployment with timestamps, every prompt change, every configuration update, every dependency version change, every training data pipeline run. A financial advisory platform maintained a unified change log that recorded every modification to any component of the AI system. When an incident occurred in March 2026, the investigation team queried the change log for all changes in the thirty minutes before incident start. Three changes appeared: a model deployment, a cache configuration update, and a routine database index rebuild. The team tested each change in isolation. The cache configuration update was the root cause — it had introduced stale data into the retrieval pipeline.

If no changes occurred near incident start, widen the time window. Some issues have delayed effects. A training data pipeline ran at midnight. The resulting model was deployed at 6:00 AM. The model performed well initially but degraded over the course of the day as it encountered input distributions that differed from training data. The incident was detected at 2:00 PM. The temporal correlation was weak because fourteen hours had passed since the causal change. The investigation team had to consider changes from the past twenty-four hours, not just the past hour.

## The Input-Output Analysis Phase

The second investigation technique is examining the inputs and outputs where failure occurs. What are users asking? What is the model generating? Are there patterns in the failures?

A legal document assistant had an incident in May 2025 where eval pass rates dropped from 92 percent to 81 percent. The investigation team pulled a sample of 50 failed eval cases. They read the inputs and outputs. A pattern emerged: the model was failing specifically on queries about employment law. Queries about contract law, real estate law, and corporate law were fine. The pattern pointed to a domain-specific degradation. The team checked recent training data. They found that the latest fine-tuning run had included only 3 percent employment law examples, down from 15 percent in previous runs. The training data distribution had shifted, causing the model to lose employment law capability.

Input-output analysis requires logging infrastructure. You need to log: input text, output text, eval score, model version, and timestamp for every production request. During an incident, you need to query this data quickly. A healthcare documentation platform had a dashboard that let investigators filter production logs by eval score, time range, and model version. When an incident occurred in August 2025, the investigation team used the dashboard to pull all failed requests from the past hour. They reviewed 30 examples, identified a pattern — the model was generating summaries that omitted medication sections — and traced the issue to a prompt change that had inadvertently truncated medical data from context.

If failures are intermittent rather than consistent, sample both successful and failed cases. Compare them. What is different about the inputs that succeed versus the inputs that fail? A contract analysis platform had an incident where 18 percent of requests were failing. The investigation team sampled 20 failures and 20 successes. The difference was input length. Failures occurred on inputs longer than 8,000 tokens. Successes occurred on shorter inputs. The issue was context window truncation — long inputs were being cut off mid-sentence, causing the model to generate incomplete outputs. The fix was increasing the context window and implementing smarter truncation logic.

## The Model Behavior Analysis Phase

The third investigation technique is analyzing how the model is behaving differently from baseline. Is it less confident? More verbose? Using different vocabulary? Behavioral changes point to model-level issues — training problems, catastrophic forgetting, or prompt engineering failures.

A travel booking assistant had an incident in November 2025 where outputs were technically correct but tonally wrong. The model was generating overly formal language when users expected casual, friendly responses. The investigation team compared recent outputs to baseline outputs. They measured: average sentence length, use of contractions, use of exclamation points, and vocabulary complexity. Recent outputs had longer sentences, fewer contractions, and more complex vocabulary. The model's tone had shifted. The team traced this to a fine-tuning run that had used formal customer service transcripts as training data. The model had learned a formal tone from the data. The fix was retraining with a balanced mix of formal and casual examples.

Model behavior analysis requires behavioral metrics. You need to track: average output length, confidence score distribution, vocabulary diversity, sentiment, and domain-specific quality markers. A content moderation platform tracked the percentage of outputs that used hedging language — "might," "possibly," "could be." When this percentage increased from 8 percent to 22 percent after a model update, the investigation team knew the model had become less confident. They reviewed the update and found that the new model had been trained with more ambiguous examples. The model was hedging because it was genuinely less certain. The fix was retraining with clearer, more definitive examples.

If behavioral changes are subtle, use A/B comparison. Route traffic to both the failing model and a known-good baseline model. Compare outputs for the same inputs. A document summarization platform had an incident in July 2025 where outputs seemed slightly off but the team could not pinpoint the issue. They routed 10 percent of traffic to the previous model version and compared outputs. The failing model was generating summaries that focused on different aspects of the source documents than the baseline model. The failing model prioritized recent information. The baseline model prioritized frequent information. The difference pointed to a change in the training data's temporal distribution. The fix was rebalancing training data to match baseline priorities.

## The Data Pipeline Investigation Phase

The fourth investigation technique is tracing data flow through the system. Where does the model's input come from? How is context constructed? Are retrieval pipelines returning the right documents? Is training data clean?

A financial advisory platform had an incident in February 2026 where the model started generating outdated stock prices. The model itself was fine. The issue was upstream. The investigation team traced the data flow: user query, retrieval pipeline, vector database, embedding model, source documents. They checked each stage. The vector database was returning correct documents. But the source documents were stale. The data ingestion pipeline had stopped running two days earlier due to a credential expiration. The model was retrieving correct documents from an outdated index. The fix was refreshing the credentials and reindexing.

Data pipeline investigation requires observability into every component. You need to monitor: data ingestion status, index freshness, retrieval precision, embedding model version, and source document metadata. A healthcare documentation platform had a dashboard showing the timestamp of the most recent document in the retrieval index. When an incident occurred in October 2025, the investigation team checked the dashboard and immediately saw that the most recent document was three days old. The index was stale. This discovery took ninety seconds because the right metric was monitored.

If training data is suspected, you need to inspect recent training runs. What data was included? What was excluded? Were there duplicates? A customer service chatbot had an incident in April 2025 where the model started generating repetitive responses. The investigation team pulled the training data from the most recent fine-tuning run. They found that a data pipeline bug had caused 40 percent of the training examples to be duplicates. The model had overfit to the duplicated examples and was now generating them verbatim. The fix was deduplicating training data and retraining.

## The Dependency and External Service Investigation Phase

The fifth investigation technique is checking dependencies. AI systems rely on external services: vector databases, embedding models, API providers, rate limiters, and caching layers. If a dependency degrades, the AI system degrades.

A legal document assistant had an incident in January 2026 where latency increased from 900 milliseconds to 3,400 milliseconds. The model was fast. The issue was the vector database. The investigation team checked the vector database's metrics. Query latency had spiked. The vector database was overloaded due to a traffic surge from another service sharing the same instance. The fix was scaling up the vector database and isolating the AI service's instance from other traffic.

Dependency investigation requires monitoring of every external component. You need to track: API response times, error rates, rate limit headroom, cache hit rates, and database query latency. A content moderation platform had an incident in September 2025 where the model's policy violation detection rate dropped. The model was working correctly. The issue was the external content analysis API that provided additional signals. The API was returning errors, so the model was making decisions without the supplementary data. The fix was implementing retry logic and a fallback to model-only decisions.

If dependencies are functioning but behaving differently, check version changes. A travel booking assistant had an incident in June 2025 where the model started returning incorrect hotel ratings. The investigation team checked the hotel data API. The API had deployed a new version that changed the rating scale from 1-5 to 1-10. The model was still interpreting ratings on the old scale. The fix was updating the prompt to handle the new scale.

## The Load and Resource Investigation Phase

The sixth investigation technique is examining system load and resource utilization. Is the system under unusual load? Is it running out of memory, CPU, or GPU capacity? Resource exhaustion causes subtle failures in AI systems — timeouts, truncated outputs, or degraded quality due to reduced inference parameters.

A document analysis platform had an incident in March 2026 where outputs became less detailed. Eval pass rates dropped slightly but not dramatically. The investigation team checked resource utilization. GPU memory was at 94 percent. The inference engine was automatically reducing batch size and context length to avoid out-of-memory errors. The reduced context length caused the model to miss information, leading to less detailed outputs. The fix was scaling up GPU capacity.

Load investigation requires real-time resource monitoring. You need to track: CPU utilization, GPU utilization, memory usage, request queue depth, and inference latency distribution. A healthcare documentation platform had an alert that fired when GPU memory exceeded 85 percent. When an incident occurred in November 2025, the alert had fired twenty minutes before the quality degradation was detected. The resource alert was a leading indicator. The team later adjusted their incident response process to treat sustained high resource utilization as an incident trigger, not just a warning.

If load is normal but resource utilization is high, check for resource leaks. A customer service chatbot had an incident in July 2025 where memory usage climbed steadily over several hours until the service crashed. The investigation team reviewed code changes from the past week. They found that a recent update had introduced a memory leak in the context management logic. Old conversation contexts were not being released. The fix was patching the memory leak and implementing better resource cleanup.

## The Experiment-Based Investigation Phase

When correlation analysis, input-output analysis, and behavioral analysis do not reveal the root cause, the investigation moves to controlled experiments. Change one variable at a time and observe the effect.

A contract analysis platform had an incident in October 2025 where outputs were incorrect but the team could not identify a pattern. They ran experiments. First experiment: route traffic to the previous model version. Issue persists. The model is not the cause. Second experiment: use a different retrieval index. Issue persists. Retrieval is not the cause. Third experiment: use a simplified prompt template. Issue resolves. The prompt is the cause. The team analyzed the production prompt versus the simplified prompt. The production prompt had a new instruction that was causing the model to prioritize speed over accuracy. The fix was removing the instruction.

Experiments should be designed to isolate variables. If you change multiple things at once, you cannot tell which change mattered. A financial advisory platform ran an experiment during an incident in December 2025. They changed the model version and the prompt simultaneously. Quality improved. But they did not know which change was responsible. They had to run follow-up experiments: new model with old prompt, old model with new prompt. The follow-up experiments revealed that the model was the fix, not the prompt. The initial experiment wasted time by conflating variables.

Experiments require the ability to rapidly deploy changes to a subset of traffic. You need canary deployment capabilities, feature flags, and A/B testing infrastructure. A document summarization platform had all of this. When an incident occurred in May 2025, they ran four experiments in twelve minutes: different model version, different prompt, different retrieval strategy, different inference parameters. The fourth experiment solved the issue. Total time from incident start to root cause identification: nineteen minutes.

## The External Factor Investigation Phase

Sometimes the root cause is not in your system at all. It is an external change — a shift in user behavior, a change in upstream data sources, or an event that changes the distribution of incoming requests.

A travel booking assistant had an incident in August 2025 where the model started recommending inappropriate hotels. The investigation team could not find any system changes. The model had not changed. The data had not changed. But the outputs were wrong. The team expanded their investigation to external factors. They discovered that a major travel website had changed its hotel categorization system. The website was a source for the retrieval index. The categorization change had propagated into the index, causing the model to interpret hotel features differently. The fix was re-ingesting data from the website with updated parsing logic.

External factor investigation requires awareness of what happens outside your control. You need to monitor: upstream data source changes, user behavior shifts, traffic distribution changes, and external events that might affect input patterns. A content moderation platform monitored trending topics on social media. When trending topics shifted suddenly, the types of content submitted to the platform shifted. In June 2025, a major news event caused a surge in politically charged content. The model's policy violation detection rate increased because the input distribution had changed. The team recognized this as an external factor, not a system failure. They temporarily adjusted moderation thresholds to handle the shift.

If user behavior changes, the same model can perform differently. A customer service chatbot had an incident in January 2026 where user satisfaction dropped. The investigation team analyzed user queries. They found that users had started asking more complex, multi-part questions. The model was still optimized for simple, single-question interactions. The behavioral shift in users caused the degradation. The fix was retraining the model on more complex examples.

## The Hypothesis Elimination Process

Root cause investigation is most efficient when you systematically eliminate hypotheses. Start with the most likely causes. Test them. Rule them out. Move to the next hypothesis.

A healthcare documentation platform had an incident in April 2025. The hypothesis list: recent model deployment, prompt change, data pipeline issue, dependency failure, resource exhaustion. The investigation team tested the first hypothesis — they rolled back the model. Issue persisted. Hypothesis eliminated. They tested the second hypothesis — they rolled back the prompt. Issue persisted. Hypothesis eliminated. They tested the third hypothesis — they checked data pipeline logs. The pipeline had run successfully. Hypothesis eliminated. They tested the fourth hypothesis — they checked dependency health. One dependency, the medical terminology API, was returning stale data. Hypothesis confirmed. The investigation took fourteen minutes because they tested hypotheses in priority order and eliminated them quickly.

Hypothesis elimination requires discipline. Do not jump to the most interesting hypothesis. Start with the most likely hypothesis. Do not test multiple hypotheses simultaneously unless you can isolate their effects. Do not assume a hypothesis is wrong without testing it. A contract analysis platform had an incident in September 2025 where the team assumed the model was fine because no model deployments had occurred recently. They spent an hour investigating data and infrastructure. Eventually they tested the model hypothesis anyway. The model had degraded due to a silent weight corruption issue that occurred during routine replication. The assumption had cost an hour.

Each hypothesis test should produce a definitive result: confirmed, eliminated, or inconclusive. If the result is inconclusive, document why and move on. Do not spend twenty minutes trying to make an inconclusive test conclusive. A legal document assistant had an incident in November 2025 where the team tested whether a dependency was the cause. The test was inconclusive — the dependency was slow, but not slow enough to obviously cause the issue. The team documented the inconclusive result and moved on. Two hypotheses later, they found the real root cause. After resolution, they revisited the inconclusive test and confirmed that the dependency slowness was a contributing factor but not the primary cause.

## The Cross-Incident Pattern Recognition Phase

If the root cause is not immediately obvious, check whether similar incidents have occurred before. Historical incident patterns often reveal recurring issues.

A financial advisory platform maintained an incident database with tags for incident type, root cause, affected components, and resolution. When an incident occurred in July 2025, the investigation team queried the database for similar symptoms. They found three previous incidents with similar eval degradation patterns. All three had the same root cause: training data distribution shift. The team immediately focused their investigation on training data. They found the issue in eight minutes. Without the historical pattern recognition, they would have spent an hour testing unrelated hypotheses.

Cross-incident analysis requires good incident documentation. Every incident should be tagged with: symptoms, root cause, affected systems, and resolution. During an investigation, query the database for incidents with matching symptoms. If multiple past incidents had the same root cause, test that hypothesis first. A healthcare documentation platform had a recurring issue where eval pass rates dropped after weekend training runs. The pattern became clear after the fourth occurrence. The root cause was that weekend training runs used a different data pipeline configuration. The team fixed the configuration inconsistency and the issue stopped recurring.

If the current incident does not match any historical pattern, that itself is valuable information. It means you are encountering a new failure mode. Document it thoroughly so future investigations can benefit. A customer service chatbot had an incident in February 2026 that did not match any previous incident. The investigation took three hours because the team had no historical reference. But they documented the root cause, symptoms, and resolution in detail. Six months later, a similar incident occurred at a different company using the same model. That team found the documentation through a public post-mortem and resolved their incident in twenty minutes.

## The Investigation Time Budget

Root cause investigation should not take longer than the incident itself. If you have spent one hour investigating and still do not have a root cause, escalate or apply aggressive mitigation.

A document analysis platform had a rule: if root cause is not identified within forty-five minutes, take the system offline and route to a fallback. In October 2025, an incident occurred. The investigation team spent forty-five minutes testing hypotheses without finding the root cause. At the forty-five-minute mark, they stopped investigating and applied the fallback — routing traffic to a simpler, less capable but more reliable model. User impact stopped. The investigation continued offline with no pressure. The root cause was identified three hours later. The forty-five-minute budget prevented the team from spending three hours with active user impact.

The investigation time budget should be proportional to incident severity. For critical incidents, the budget is ten to twenty minutes before aggressive mitigation. For high incidents, the budget is thirty to sixty minutes. For medium incidents, the budget is two to four hours. If investigation exceeds the budget without resolution, mitigation takes priority. You can always investigate after users are unaffected.

Once the root cause is found, the next step is mitigation. How do you stop the damage? How do you restore normal operation? Mitigation strategies for AI incidents range from rollback to fallback to graceful degradation — each with different trade-offs.

