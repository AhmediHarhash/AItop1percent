# 14.11 â€” The Production Observability Readiness Checklist

Three days before launch, the engineering lead ran through the deployment checklist. Code reviewed. Tests passing. Infrastructure provisioned. Deployment scripts tested. Everything looked ready. Then someone asked: "If this breaks in production, how will we know? And if we know, what will we do about it?"

Nobody had good answers. They had built the product but not the observability infrastructure to operate it. They delayed launch by two weeks to instrument properly. Those two weeks were some of the highest-value engineering time they ever spent, because operating blind in production is how companies create preventable disasters.

The production observability readiness checklist is not aspirational. It is the minimum set of capabilities you must have before launching an AI system. Skip items at your own risk. Every item on this checklist exists because teams that skipped it regretted it.

## Instrumentation Readiness

Before launch, every critical component must be instrumented with metrics that reveal health, quality, and problems.

**Model quality metrics are live and validated.** You can measure correct answer rate, user satisfaction, refusal rate, and policy violation rate in real-time. You have tested these metrics in staging and confirmed they capture what you think they capture. You have baseline values from testing that define what "normal" looks like.

Without quality metrics, you will not know when the model degrades. You will discover quality problems when users complain, after damage has accumulated. Quality instrumentation is not optional.

**Latency metrics cover every critical path.** You track end-to-end latency from user request to response. You track model inference time separately from API overhead. You track P50, P95, and P99 latency, not just averages. You know what latency is acceptable and what crosses into user-impacting territory.

Without latency metrics, you will not know when the system slows down. Slow systems feel broken to users even when technically functional. Latency instrumentation is not optional.

**Error rates are tracked with sufficient granularity.** You distinguish between client errors, server errors, timeout errors, and model errors. You track error rates overall and per endpoint. You know what error rate is normal background noise and what indicates problems.

Without error rate tracking, you will not know when the system is failing. High error rates mean users cannot complete tasks. Error instrumentation is not optional.

**Cost metrics are live and attributed correctly.** You track cost per query, cost by model, cost by feature, and cost by user tier. You know how much you are spending in real-time. You have thresholds that define when cost is acceptable and when it exceeds budget.

Without cost metrics, you will discover budget overruns when the monthly bill arrives, too late to mitigate. Cost can spike from configuration errors, traffic surges, or routing mistakes. Cost instrumentation is not optional.

**Traffic metrics show usage patterns.** You track request volume overall and by endpoint. You track traffic segmentation by user cohort, geography, and subscription tier. You can detect traffic spikes, unusual patterns, and segment-specific surges.

Without traffic metrics, you cannot distinguish between normal growth and abnormal surges. You cannot detect bot attacks or viral usage patterns. Traffic instrumentation is not optional.

**Safety metrics monitor policy violations.** You track toxicity rate, harmful content rate, PII leakage rate, and content escalation rate to human reviewers. You know what rates are acceptable baseline and what rates indicate filter failures or attacks.

Without safety metrics, you will not know when the model produces harmful outputs until user complaints or regulatory action. Safety instrumentation is not optional.

## Dashboard Readiness

Metrics are worthless if nobody can see them. Dashboards must exist, be organized, and be accessible before launch.

**An executive dashboard summarizes top-line health.** In five seconds, someone should be able to look at one screen and know whether the system is healthy or broken. This dashboard shows request volume, average latency, error rate, cost, and quality score. Green means healthy. Red means broken. No interpretation required.

**A technical dashboard enables engineering diagnosis.** When the executive dashboard shows red, engineers open the technical dashboard to investigate. It shows latency distribution, error breakdown by type, model performance metrics, infrastructure health, and deployment history. This is the dashboard the on-call engineer uses.

**A quality dashboard tracks model behavior.** Correct answer rate by segment. User satisfaction trends. Refusal rate over time. Policy violation rate. Sample recent requests that triggered quality alerts. This is what Product and Trust and Safety monitor.

**A cost dashboard tracks spend in real-time.** Cost per query, cost by model, cost trending over the past day and week. Budget consumption percentage. Cost anomalies. This is what Finance and Engineering leadership monitor.

**Dashboards are linked and navigable.** From the executive dashboard, you can click through to technical details. From the technical dashboard, you can click through to specific logs or traces. Engineers under pressure should not waste time searching for information. Navigation should be obvious.

**Dashboards have been reviewed by actual users.** Before launch, walk through the dashboards with the people who will use them: on-call engineers, Product managers, Trust and Safety reviewers. Ask: "Can you interpret this? Is anything confusing? What information is missing?" Revise based on feedback.

Dashboards that looked great to the person who built them are often confusing to everyone else. User testing prevents this.

## Alerting Readiness

Alerts notify the team when intervention is needed. Poorly configured alerts either miss problems or create noise that gets ignored.

**Alerts exist for every critical failure mode.** Model quality degradation. Latency exceeding SLA. Error rate spiking. Cost exceeding budget. Traffic anomalies. Policy violation rate increasing. Each failure mode that could harm users or the business has a corresponding alert.

**Alert thresholds are tuned based on baseline behavior.** You do not guess at thresholds. You establish baselines during testing and set thresholds that distinguish real problems from normal variation. An alert that fires when correct answer rate drops below 85 percent should be set because 85 percent is meaningfully below your baseline, not because 85 percent sounds reasonable.

**Alerts specify severity and urgency.** Critical alerts page the on-call engineer immediately. High alerts notify in Slack within minutes. Medium alerts go into a queue for review during business hours. Low alerts are informational. Severity must be calibrated so critical really means critical.

**Alerts include actionable information.** An alert that says "model quality degraded" without context is useless. An alert that says "correct answer rate dropped from 88 percent to 79 percent in the past 10 minutes, affecting all user segments, potential causes: recent deployment, upstream service failure, data issue" gives the on-call engineer a starting point.

**Alert routing is configured and tested.** Critical alerts go to PagerDuty or Opsgenie and page on-call engineers. High alerts go to Slack channels monitored by relevant teams. You have tested alert routing in staging and confirmed alerts reach the right people.

**Alerts have been tuned to reduce false positives.** You ran the alerting system in staging for at least a week, observed what fired, and tuned thresholds. You do not launch with untested alerts because you do not know whether they will be signal or noise.

## Runbook Readiness

When an alert fires, the on-call engineer needs to know what to do. Runbooks provide that guidance.

**Runbooks exist for the five most common expected failure modes.** Based on your architecture and past experience, you have identified the five incidents most likely to occur: model quality degradation, latency spike, cost overrun, data pipeline failure, content filter misconfiguration. For each, you have a runbook.

**Each runbook follows the standard template.** Symptoms. Immediate actions. Diagnostic steps. Mitigation procedures. Escalation criteria. Verification steps. Consistent structure means engineers know where to look for information.

**Runbooks have been tested by walking through them.** Before launch, run simulated incidents in staging and follow the runbooks. Do the diagnostic steps actually reveal the problem? Do the mitigation procedures actually work? Are any steps outdated or unclear? Fix issues before launch.

**Runbooks are accessible under pressure.** They live in a place the on-call engineer knows: the team wiki, a runbook repository, linked from the on-call documentation. The on-call engineer should be able to find the correct runbook within 30 seconds of being paged.

**Escalation paths are documented and tested.** Runbooks specify when to escalate to the ML team, the data engineering team, Trust and Safety, Product, and leadership. Escalation contacts are current. You have confirmed these people are reachable and expect to be escalated to.

## On-Call Readiness

Someone must be responsible for responding when alerts fire. That someone must be prepared.

**On-call rotation is staffed and scheduled.** You know who is on-call this week and next week. The schedule is published. The on-call engineer has acknowledged their shift and confirmed they are available.

**On-call engineers have been trained.** They have completed the observability onboarding. They know the system architecture. They can navigate dashboards. They have read the runbooks. They have shadowed a previous on-call shift or completed practice incidents.

**On-call tooling is configured.** The on-call engineer has access to dashboards, logs, deployment tools, and communication channels. They can execute rollbacks, adjust configurations, and escalate issues. Access permissions are granted before their shift starts, not during an incident.

**Backup on-call coverage is identified.** If the primary on-call engineer cannot respond or needs help, who is backup? The backup is documented and aware of their role. You do not discover during an incident that the on-call engineer is unreachable and nobody knows who to escalate to.

**On-call communication channels are established.** When the on-call engineer needs help, where do they ask? A dedicated Slack channel? A phone tree? A PagerDuty escalation policy? Communication channels are tested before launch.

## Incident Response Readiness

When an incident occurs, the team must respond effectively. Incident response processes must exist before the first incident.

**Incident severity definitions are documented.** What constitutes a P0, P1, P2, P3 incident? Severity affects who gets notified, how quickly response is expected, and whether user communication happens. Definitions must be clear before incidents occur so people do not argue about severity during crises.

**Incident communication templates are prepared.** When an incident affects users, what do you tell them? Templates for user communication, status page updates, and internal stakeholder updates should exist. You adapt them during incidents, but you do not write from scratch under pressure.

**Incident documentation procedures are defined.** Who documents the incident timeline? Where does documentation live? What information must be captured? Clear procedures ensure incidents are documented consistently, which enables effective retrospectives.

**Postmortem process is established.** After incidents, the team meets to review what happened and what to improve. The postmortem process is defined: who facilitates, what questions are asked, how action items are tracked. Do not invent this process during your first retrospective.

## Compliance and Safety Readiness

If your AI system handles sensitive data, operates in regulated industries, or serves users in jurisdictions with strict AI regulations, compliance readiness is launch-critical.

**Data handling is instrumented for auditability.** If you must demonstrate that you handle personal data correctly, that you retain data for required periods, or that you delete data upon request, you have logging and metrics that prove compliance. You do not reconstruct compliance evidence after an audit starts.

**Safety monitoring meets regulatory requirements.** If regulations require monitoring for harmful outputs, bias, or fairness issues, that monitoring is live before launch. The EU AI Act requires systemic risk monitoring for high-risk AI systems. If that applies to you, compliance is not optional.

**Incident reporting procedures are defined.** Some regulations require reporting AI incidents to regulators within specific timeframes. If an incident occurs, who determines whether it is reportable, who drafts the report, and who submits it? These procedures must exist before incidents, not during them.

## Cross-Functional Coordination Readiness

AI systems affect multiple teams. Those teams must be coordinated before launch.

**Product has visibility into quality and user experience metrics.** They can see user satisfaction, escalation rates, and task completion rates. They know how to interpret these metrics and when to escalate concerns to Engineering.

**Trust and Safety has visibility into policy violations and safety metrics.** They can see toxicity rates, harmful content flags, and content escalations. They know how to escalate issues to Engineering and when to implement safety overrides.

**Finance has visibility into cost metrics.** They can see real-time spend and trending. They know the budget and when cost is approaching or exceeding it. They can trigger cost mitigation conversations before the monthly bill arrives.

**Leadership has visibility into top-line health.** They can check the executive dashboard and understand whether the system is operating well or experiencing issues. They receive notifications for major incidents.

**Cross-functional review meetings are scheduled.** The weekly cross-functional review is on everyone's calendar. Attendees know what metrics they are responsible for presenting.

## The Final Pre-Launch Review

One week before launch, conduct a pre-launch observability review. Walk through this entire checklist with the team. For each item, confirm it is complete or explicitly accept the risk of launching without it.

Document gaps. If you launch without completing an item, document why. "We are launching without anomaly detection because we prioritized other work, accepting that we might miss gradual degradation." Explicit risk acceptance is better than implicit gaps.

Set a post-launch review date. One week after launch, review how observability performed. Did alerts fire correctly? Were dashboards useful? Did runbooks work? What gaps did you encounter? Use early production experience to refine observability before issues become crises.

Observability readiness is not about perfection. It is about having the minimum capabilities to operate safely in production: see problems, diagnose them, fix them, learn from them. Teams that launch without these capabilities operate blind and discover their gaps during incidents when the stakes are highest. Teams that launch with observability readiness catch issues early, respond effectively, and improve continuously.

This is not a one-time checklist. As the system evolves, observability must evolve with it. New features require new instrumentation. New failure modes require new alerts. Changing team composition requires updated training. The checklist is a snapshot of readiness at launch. Maintaining readiness requires continuous investment, continuous improvement, and continuous discipline. That investment is what separates teams that operate reliably from teams that lurch from crisis to crisis, never quite understanding why.

