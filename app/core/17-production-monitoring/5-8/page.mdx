# 5.8 — Chunk Quality Degradation: When Your Splitting Strategy Breaks

Chunking is not a one-time decision. A chunking strategy that works perfectly for well-structured documents with consistent formatting fails catastrophically when applied to documents with tables, nested lists, code blocks, or dense technical notation. Your system chunks documents into 500-word passages. For narrative text, this works fine. For a document with a 300-row table followed by a short paragraph, the chunking logic slices the table mid-row, creating chunks that contain partial table rows with no context. The model retrieves these chunks. The chunks are semantically incoherent. The model cannot synthesize a meaningful answer from fragments of table rows. This is chunk quality degradation, and it is invisible until you inspect what the model is actually receiving.

Chunk quality is the difference between retrieving useful, self-contained passages that the model can reason over versus retrieving fragments, noise, and decontextualized snippets that the model cannot meaningfully process. High retrieval precision does not guarantee high chunk quality. You can retrieve the right document and still fail if the chunking strategy produced garbage from that document.

## The Fixed-Length Chunking Failure Mode

The simplest chunking strategy is fixed-length — split every document into 512-token chunks with 50-token overlap. This works acceptably for homogeneous narrative documents. It fails for documents with structure. A technical manual contains paragraphs, tables, diagrams, code listings, and numbered steps. Fixed-length chunking treats all of this as a token stream and splits at token boundaries without regard for document structure. A table gets split mid-row. A code listing gets split mid-function. A numbered procedure gets split between steps 3 and 4 with no indication that it is a partial sequence.

In August 2025, a developer documentation RAG chunked API reference documents using fixed 512-token chunks. API references contained endpoint descriptions, parameter tables, example requests, example responses, and error code lists. The chunking logic split these elements arbitrarily. One chunk contained half of a parameter table. The next chunk contained the other half of the parameter table followed by the start of an example request. A developer queried for parameters of a specific API endpoint. The retrieval system returned the chunk with the first half of the parameter table. The model synthesized an answer listing the parameters that happened to be in that chunk. It did not know there were more parameters in the next chunk. The answer was incomplete. The developer followed the incomplete documentation and encountered errors due to missing required parameters.

The team sampled 200 retrieved chunks and manually evaluated whether each chunk was semantically self-contained and useful. Only 61 percent were. The other 39 percent were fragments — partial tables, split code examples, incomplete lists, mid-paragraph breaks that lost context. Retrieval precision was high. Chunk quality was low. The model received the right documents but in a form that was impossible to use effectively.

The fix was structure-aware chunking. Instead of splitting at fixed token boundaries, the chunking logic identified document elements — paragraphs, tables, code blocks, lists — and treated each element as an atomic unit. If a table was 800 tokens, it became a single 800-token chunk, exceeding the 512-token target but preserving coherence. If a paragraph was 120 tokens, it became a 120-token chunk, under-utilizing the chunk size but remaining self-contained. Chunk quality improved to 87 percent. The model received coherent, usable passages.

## Tables and Structured Data

Tables are the most common cause of chunk quality failures. A table is a 2D structure. Chunking logic typically operates on 1D token streams. When you serialize a table into tokens and split it at a fixed boundary, you lose the 2D structure. A chunk might contain rows 1 through 5 of a table. The next chunk contains rows 6 through 10. The header row is in a previous chunk. The model sees rows 6 through 10 without column headers and cannot interpret the data. Or the chunk contains the header row and three data rows, but the table has 50 data rows, and the model does not know there is more data elsewhere.

A financial data RAG indexed earnings reports containing multi-page tables of quarterly revenue by product line. The chunking logic split tables into chunks of roughly 500 tokens. Each chunk contained 15 to 20 table rows. The table headers were in the first chunk. Subsequent chunks contained rows without headers. A query about Q3 revenue for Product X retrieved a chunk containing Q3 data rows but no headers. The model attempted to synthesize an answer from raw numbers without column labels. The answer was nonsensical — it could not identify which column was revenue, which was cost, which was profit.

The team implemented table-aware chunking. Tables were detected during indexing using HTML tags or markdown table syntax. Each table was extracted as a single chunk, regardless of token length. If a table exceeded 2,000 tokens, it was split vertically — the header row was repeated in each chunk, and each chunk contained a subset of data rows. This ensured that every chunk containing table data also included headers. The model could now interpret the numbers because it knew what each column represented. Chunk quality for table-heavy documents improved from 42 percent to 81 percent.

## Lists and Nested Hierarchies

Ordered and unordered lists are common in documentation, policies, and instructional content. Fixed-length chunking splits lists at arbitrary points. A chunk might contain items 1 through 7 of a 15-item list. The next chunk contains items 8 through 15. The model retrieves the first chunk and generates an answer based on items 1 through 7, not knowing items 8 through 15 exist. If the query is specifically about item 12, the first chunk is irrelevant and the second chunk is retrieved — but the model sees items 8 through 15 without the context of items 1 through 7.

Nested lists are worse. A policy document contains a section with a top-level list of requirements, each with sub-items, some with sub-sub-items. Fixed-length chunking splits this hierarchy at arbitrary nesting levels. A chunk contains a top-level item and three of its sub-items. The fourth sub-item is in the next chunk, disconnected from its parent item. The model retrieves the first chunk and does not realize the list continues.

A compliance documentation RAG chunked policy documents with nested lists. Evaluators sampled retrieved chunks and found that 28 percent of list-containing chunks were incomplete — they started or ended mid-list, or they contained sub-items without their parent items. Users querying about compliance requirements received partial lists. "Do I need to do X?" "Yes, you need to do A, B, and C." The actual requirement included D and E, but those items were in a different chunk that was not retrieved. Compliance failures resulted from incomplete information.

The team implemented list-aware chunking. Lists were detected by parsing markdown or HTML list structures. Each list was treated as an atomic unit unless it exceeded 1,500 tokens, in which case it was split at top-level item boundaries while preserving the hierarchy of sub-items under each top-level item. A chunk might contain top-level item 3 and all its sub-items. The next chunk would contain top-level item 4 and all its sub-items. This ensured that every chunk containing part of a list preserved the hierarchical context. Chunk completeness for list-heavy documents improved to 89 percent.

## Code Blocks and Technical Notation

Code examples, mathematical formulas, and technical diagrams are semantically atomic. Splitting them mid-content destroys meaning. A code function is 30 lines. Fixed-length chunking splits it at line 18. The chunk contains half a function. The model sees incomplete code and cannot reason about what the function does. A mathematical derivation spans two pages. Chunking splits it mid-equation. The model sees the first half of a proof with no conclusion.

A software documentation RAG chunked tutorials containing code examples. The chunking logic split code blocks at token boundaries. A code example demonstrating API authentication was 45 lines. The chunk contained lines 1 through 28. The remaining lines were in the next chunk. A developer queried for authentication examples. The retrieval system returned the first chunk. The model synthesized an answer showing incomplete code. The developer copied the code, ran it, and encountered syntax errors because the code was truncated. The developer assumed the documentation was wrong. The documentation was correct. The chunking was broken.

The team added code-aware chunking. Code blocks were detected by markdown fence syntax or indentation patterns. Each code block was treated as an atomic unit, never split. If a code block exceeded the chunk size limit, it became an oversized chunk. The assumption was that code must be complete to be useful. Incomplete code is worse than no code. After implementing code-aware chunking, the retrieval system surfaced complete, runnable code examples. Developer satisfaction with documentation retrieval improved measurably.

## Cross-Reference and Citation Context

Many documents contain cross-references — "as discussed in Section 3" or "see Table 5 for details." Fixed-length chunking can separate the cross-reference from the referenced content. A chunk contains a sentence referring to Table 5. Table 5 is three chunks away. The model retrieves the chunk with the reference but not the chunk with the table. The model cannot answer the query because it sees a reference to information it does not have.

A legal document RAG indexed contracts containing frequent cross-references between sections. A clause in Section 8 referred to definitions in Section 2. Chunking split the document such that Section 8 was in one set of chunks and Section 2 was in another. A query about the Section 8 clause retrieved Section 8 chunks. The model read "as defined in Section 2" but did not have Section 2. The model either ignored the cross-reference and synthesized an incomplete answer, or it acknowledged the cross-reference and stated "see Section 2 for definitions" without providing them. Either way, the answer was incomplete.

Solving cross-reference fragmentation requires either expanding chunks to include referenced content or dynamically retrieving referenced sections. The legal document team implemented reference-aware expansion. During chunking, the system parsed cross-references and included the referenced content as part of the chunk, even if that made the chunk larger. A chunk covering Section 8 also included the definitions from Section 2 that it referenced. This increased chunk size but preserved semantic completeness. Retrieval returned fewer chunks per query because each chunk was larger, but answer completeness improved because the model had all the context it needed.

## Measuring Chunk Quality

Chunk quality is hard to measure automatically. It requires human judgment or LLM-based evaluation. Sample retrieved chunks. For each chunk, evaluate whether it is semantically self-contained, coherent, and useful on its own. **Completeness** — does the chunk contain a complete thought, or is it a fragment? **Coherence** — does the chunk make sense when read independently, or does it require context from adjacent chunks? **Utility** — can the model synthesize a useful answer from this chunk, or is it too noisy, too dense, or too decontextualized?

A healthcare documentation RAG sampled 500 retrieved chunks per week. Human evaluators rated each chunk on completeness, coherence, and utility using a three-point scale. Aggregate chunk quality score averaged 2.3 out of 3. This was acceptable but not great. The team analyzed low-scoring chunks and found patterns: chunks containing partial tables, chunks with orphaned list items, chunks with references to figures not included in the chunk, chunks that started or ended mid-sentence due to tokenization edge cases. Each pattern informed a chunking refinement. After three months of iterative improvements, average chunk quality rose to 2.7 out of 3.

## Chunk Length Distribution

Not all documents should be chunked the same way. Narrative documents tolerate smaller chunks. Technical documents require larger chunks to preserve context. Monitoring chunk length distribution reveals whether your chunking strategy is producing consistent chunks or wildly variable ones. High variance means some documents are being over-chunked into tiny fragments while others are being under-chunked into massive blocks.

A product documentation RAG tracked chunk length distribution. Median chunk length was 420 tokens. 90th percentile was 680 tokens. 99th percentile was 3,200 tokens. The long tail consisted of chunks that could not be split without breaking semantic units — large tables, long code examples, dense technical appendices. These oversized chunks did not fit well in the context window. Queries retrieving oversized chunks consumed most of the token budget on a single chunk, leaving little room for other retrieved documents. The team set a hard chunk size limit of 1,500 tokens. Chunks exceeding the limit were split at the best natural boundary available — section breaks, paragraph breaks, or logical subsections. Chunk length variance decreased. Context window utilization became more predictable.

## Embedding Quality vs Chunk Quality

A poorly chunked document produces low-quality embeddings. The embedding model is trained to represent coherent passages. When you embed a fragment of a table or half a list, the embedding represents a semantically incomplete unit. The embedding might have high similarity to queries that match keywords in the fragment, but the fragment lacks the context needed to actually answer the query. Retrieval precision suffers because the embedding-query similarity does not reflect the usefulness of the chunk.

A technical support RAG embedded chunks of troubleshooting guides. The guides contained step-by-step procedures. Fixed-length chunking split procedures mid-sequence. Chunks contained steps 3 through 7 of a 12-step procedure. The embedding represented those five steps. A query about the full procedure retrieved chunks containing the middle steps. The model synthesized an answer that started with step 3, confusing users who had not completed steps 1 and 2. The retrieved chunks were relevant by keyword match but useless because they lacked the beginning of the procedure.

The team implemented procedure-aware chunking. Procedures were detected by parsing numbered or bulleted sequences. Each procedure was treated as an atomic unit. If a procedure was too long, it was split into logical phases — setup, execution, verification — with each phase forming a complete sub-procedure. Embeddings now represented complete procedures or complete phases. Retrieval returned actionable chunks. Answer completeness improved.

## The Chunk Quality Audit

Conduct regular chunk quality audits. Sample retrieved chunks monthly. Evaluate completeness, coherence, and utility. Identify patterns in low-quality chunks. Are they predominantly from specific document types? Specific authors? Specific ingestion pipelines? Pattern identification guides remediation. If table-heavy documents consistently produce low-quality chunks, the table handling logic needs improvement. If code-heavy documents fail, the code detection logic is insufficient.

An e-commerce product catalog RAG ran quarterly chunk quality audits. They sampled 1,000 chunks and rated them. In Q1, chunk quality was 2.5 out of 3. Low-quality chunks were predominantly from product spec sheets containing tables and bulleted feature lists. In Q2, the team implemented table-aware and list-aware chunking. Chunk quality improved to 2.7. In Q3, they discovered a new failure mode — product comparison pages with side-by-side tables were being chunked such that each chunk contained one column of the comparison table without the other columns. They refined chunking to keep comparison tables intact. Q4 chunk quality reached 2.8.

## When to Re-Chunk

Re-chunking your entire index is expensive and disruptive. You only do it when chunk quality has measurably degraded or when a significant portion of your documents are poorly chunked. The decision is informed by chunk quality metrics. If chunk quality is above 2.5 out of 3 and stable, tolerate the current chunking. If chunk quality is below 2.0 or declining, re-chunk.

Re-chunking also means re-embedding, because chunk boundaries change and embeddings are tied to specific chunk content. This is operationally similar to a full index rebuild. You run the new chunking logic, generate new embeddings, and cut over to the new index. Validate that retrieval quality improves after re-chunking. If it does not, the new chunking strategy is not better, and you should not deploy it.

A legal research platform re-chunked their case law index after discovering that 40 percent of chunks were fragments of court opinions that lacked sufficient context. Re-chunking took three weeks. Re-embedding took two weeks. The new index went live in November 2025. Retrieval quality improved by 14 percentage points. The operational cost was significant but justified by the user experience improvement.

The next subchapter covers multi-index coordination — observability when your RAG system retrieves from multiple knowledge sources with different schemas, freshness requirements, and quality profiles, and how to track which index is succeeding and which is failing.

