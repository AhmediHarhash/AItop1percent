# 4.7 — Baseline Management: Establishing and Updating Reference Distributions

Drift detection only works if you have a baseline — a reference distribution that represents correct, healthy behavior. But baselines are not static. Lock your baseline to training data from January 2025 and measure drift through December 2026, and you are comparing production to a snapshot that is nearly two years old. The world has changed. Your users have changed. Your product has changed. Some of that drift is appropriate. The question is not whether your production distribution matches your baseline. The question is whether your production distribution matches what correct looks like right now. Baseline management is the process of deciding when your reference distribution should evolve and when a divergence from baseline represents a problem to fix rather than a new normal to accept. Get this wrong and you will either alert on healthy evolution or miss genuine degradation.

In April 2025, a fraud detection system at a payments company noticed their false positive rate had increased from 1.8 percent to 2.4 percent over three months. Their drift monitoring showed significant input distribution shifts — transaction sizes, merchant categories, and geographic patterns had all changed. The team investigated for weeks, assuming the drift was causing the accuracy drop. It was not. The drift was real, but it was appropriate. E-commerce patterns had shifted as new shopping behaviors emerged. The model had adapted well. The false positive increase was caused by a threshold change made in February that the team had forgotten about. They had been comparing against a baseline from before the threshold change, treating post-change behavior as drift. The baseline should have been updated when the threshold changed. Instead, the team wasted three weeks chasing phantom drift while the real issue — the threshold — went unexamined. Baseline management failures waste time and obscure real problems.

## Establishing the Initial Baseline

Your initial baseline is the distribution from a period when your model was known to be performing well. For a newly launched model, this is typically the first few weeks after deployment, once initial bugs are fixed but before significant real-world drift has accumulated. For an existing model, this is a recent period where your performance metrics met targets and you had no major incidents. The baseline window should be long enough to capture natural variance but not so long that it includes outdated patterns. Two to four weeks is typical. Less than a week and you risk capturing noise. More than two months and you risk including drift you want to detect.

Do not use training data as your baseline unless your model launched so recently that production data is not yet available. Training data represents the world at training time, which might be months or years before deployment. Production data represents the world your model actually serves. The gap between training distribution and production distribution is often significant even at launch. If you baseline against training data, you will alert on differences that are not drift but simply the reality of deployment. Use production data from a validated period as your baseline. This ensures you are measuring drift from production norms, not from historical training conditions.

Segment your baseline by any dimension that has distinct behavior patterns. If your model serves multiple geographic regions and those regions have different input characteristics, establish per-region baselines. If your model handles multiple product lines with different user bases, establish per-product baselines. If your model has different accuracy profiles for different input types, establish per-type baselines. Segmented baselines prevent you from treating regional variance or product-specific patterns as drift. They also let you detect localized drift — a shift in one region or product line — before it becomes system-wide.

## The Baseline Update Trigger

The hardest decision in baseline management is when to update. Update too often and your baseline tracks every fluctuation, making drift detection useless. Update too rarely and your baseline becomes stale, making healthy evolution look like drift. The right update frequency depends on how fast your domain changes and how much tolerance you have for false positives.

The most common trigger is scheduled updates. Recompute your baseline every month or every quarter. This works well for domains with moderate change rates. A customer support chatbot might update baselines quarterly to capture evolving user language and new product features. A fraud detection system might update monthly to stay current with new fraud patterns. Scheduled updates are predictable and easy to operationalize. The downside is that they are not responsive to actual change. If significant drift happens two weeks after a baseline update, you wait another six weeks before the baseline reflects it.

Event-driven updates are better for systems where change is tied to known events. Update your baseline after every model retraining. Update after major product launches. Update after regulatory changes. Update after you adjust prompts or thresholds. Any deliberate change to your system that is expected to alter distributions should trigger a baseline update. This keeps your baseline aligned with system reality. You are not monitoring drift from an old configuration. You are monitoring drift from the current configuration. Event-driven updates require discipline — every system change must be logged and trigger the baseline update process. But they eliminate the problem of measuring drift from obsolete baselines.

Performance-based updates are the most sophisticated. Monitor your model's performance continuously. When performance is stable and meeting targets for an extended period — say, four weeks — that period becomes your new baseline. When performance degrades, do not update the baseline. The degradation is drift, and you want to detect it. This approach ensures your baseline always represents good performance. You never lock in degradation as the new normal. The challenge is defining "stable and meeting targets" rigorously enough that baseline updates do not happen during periods of silent drift.

## The Graceful Baseline Transition

When you update a baseline, do not switch abruptly. Abrupt switches create discontinuities in your drift metrics. A metric that showed 0.08 drift yesterday suddenly shows 0.02 drift today because the baseline changed. Your dashboards spike and dip unpredictably. Alerts stop firing even though behavior has not changed. The solution is graceful transition. Maintain both the old baseline and the new baseline for an overlap period — typically two to four weeks. Compute drift metrics against both. Gradually weight your alerts toward the new baseline while phasing out the old one.

A common approach is linear blending. For the first week after a baseline update, compute drift as 75 percent old baseline and 25 percent new baseline. For the second week, 50-50. For the third week, 25 percent old and 75 percent new. By week four, you are fully on the new baseline. This prevents alert disruption and gives your team time to recalibrate expectations. It also reveals whether the new baseline is stable. If drift metrics against the new baseline are noisy or erratic during the transition, the new baseline might be based on an atypical period and should be reconsidered.

Log all baseline updates with metadata. Record when the baseline was established, what period it covers, what event triggered the update, and what the key statistics are — mean, median, variance, KS statistics relative to the previous baseline. This log is your audit trail. When drift alerts fire six months later, you can trace back to understand what baseline was in effect and whether the alert is meaningful. Without this log, baseline updates become invisible, and future teams lose context on why drift monitoring behaves the way it does.

## When Drift Means Your Baseline Is Wrong

Sometimes what looks like drift is actually baseline error. You established a baseline during an atypical period — a holiday spike, a one-time campaign, a temporary product issue. The baseline captures that anomaly. When behavior returns to normal, drift detection alerts because the current distribution does not match the anomalous baseline. This is the inverted drift problem. Your model is healthy. Your baseline is wrong.

A retail recommendation system established a baseline in late November 2025, during peak holiday shopping. The baseline captured high purchase frequency, high average order values, and dense activity in gift categories. In January, drift monitoring went haywire. Every metric showed extreme drift — purchase frequency down, average order value down, gift category traffic down. The team initially thought the model had broken. It had not. The baseline was a holiday baseline being compared to normal shopping patterns. The fix was to establish a new baseline from a non-holiday period and retroactively recompute drift metrics.

The diagnostic signal for baseline error is that drift appears immediately after baseline establishment and persists without corresponding performance degradation. If you establish a baseline and drift metrics spike within days, your baseline is likely unrepresentative. If drift metrics stay elevated for weeks but performance is stable, your baseline is measuring difference from an anomaly, not drift from a norm. The solution is to reestablish the baseline using a period that better represents typical behavior. This is why baseline establishment should always include validation — compare the candidate baseline to multiple historical periods to ensure it is not an outlier.

## Multi-Baseline Strategies

Some systems need multiple concurrent baselines. A seasonal business — tax preparation, travel booking, retail — cannot use a single baseline year-round. January behavior is different from July behavior. Both are normal. Neither is drift relative to the other. The solution is seasonal baselines. Maintain separate baselines for each season or month. In January, measure drift relative to last January. In July, measure drift relative to last July. This eliminates seasonal patterns from drift detection while preserving the ability to catch genuine anomalies within each season.

Multi-geography systems often need per-region baselines. A global customer support chatbot has different input distributions in North America, Europe, and Asia-Pacific. Language patterns differ. Support issue types differ. Time zones create different usage patterns. Measuring all regions against a global baseline creates constant false positives. Instead, maintain per-region baselines and monitor drift within each region separately. You can still roll up to global drift metrics for executive dashboards, but your alerting should be region-specific.

Multi-product systems need per-product baselines when products have distinct user bases or use cases. A B2B SaaS platform with both small business and enterprise customers will see different usage patterns. Small business users have different input characteristics, different query types, and different accuracy expectations. Measuring both against a blended baseline obscures product-specific drift. Per-product baselines reveal when one segment is drifting while the other is stable. This localizes the problem and focuses investigation.

## Baseline Versioning and Rollback

Treat baselines as versioned artifacts. Baseline v1 covered January through February 2025. Baseline v2 covered March through April. Baseline v3 covered May through July. Each version is stored, documented, and retrievable. This enables rollback. If a baseline update causes alert chaos or reveals that the update was premature, you can roll back to the previous baseline while you investigate. Without versioning, baseline updates are one-way. You lose the ability to compare current behavior to historical norms beyond the most recent baseline.

Versioning also enables experimentation. You can compute drift metrics against multiple baseline versions simultaneously and compare the results. Which baseline produces the most actionable alerts? Which baseline has the fewest false positives? Which baseline aligns best with actual performance degradation? This analysis informs your baseline update strategy. You learn what baseline characteristics predict real drift versus noise. Over time, your baseline management becomes more sophisticated because you have historical data on what works.

Store baselines efficiently. You do not need to store every raw data point from the baseline period. Store the statistics: mean, median, percentiles, variance, and for distribution comparisons, either the full histogram or a sample of representative points. For embedding baselines, store the centroid, the covariance matrix, and a sample of embeddings for clustering. This reduces storage from gigabytes to megabytes while retaining everything you need for drift computation. If you ever need the raw baseline data for deeper analysis, you can usually reconstruct it from logs or archives.

## Communicating Baseline Changes

Baseline updates are operational changes that affect how your monitoring system behaves. If you update a baseline and do not communicate it, your team will be confused when drift metrics shift. An alert that fired yesterday stops firing today, and no one knows why. A metric that was stable suddenly shows variance. Without context, these changes look like bugs. With context, they are expected outcomes of baseline updates.

Document every baseline update in your team's communication channels. A simple message: "Baseline updated to v8, covering May 1 to May 28, triggered by model retraining on April 30. Drift metrics may shift over the next two weeks as the new baseline takes effect." This takes 30 seconds to write and prevents hours of confusion. Include the message in your monitoring dashboard as an annotation. When your team reviews drift charts, they should see markers indicating baseline updates. This connects behavior changes to their causes.

For systems with external stakeholders — compliance teams, auditors, executive leadership — baseline updates require formal documentation. Explain why the update was necessary, what period the new baseline covers, and how drift detection logic changed. This is especially important in regulated industries where drift detection is part of your model governance. Auditors need to understand that your baseline evolved appropriately, not that you manipulated it to hide drift. Transparent baseline management is part of responsible AI operations.

The next subchapter covers drift alert tuning — how to set thresholds and configure alerting logic to avoid false positives without missing real drift.

