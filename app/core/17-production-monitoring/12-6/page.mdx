# 12.6 — Human Override Rate Monitoring

Most teams think that low override rates mean the model is working well. If humans agree with the AI 95 percent of the time, the model must be accurate, right? Wrong. High agreement can mean the model is correct. Or it can mean humans are rubber-stamping AI decisions without real review. Or it can mean the model and humans share the same systematic bias. You cannot distinguish these scenarios without monitoring how overrides happen, who performs them, and what patterns emerge over time.

Human override rate is the percentage of AI recommendations that a human reviewer changes. A rate of 5 percent means humans override one in twenty decisions. This metric is meaningless in isolation. You need segmentation: override rate by reviewer, by decision type, by model confidence, by time of day, by case complexity. You need trends: is the rate increasing or decreasing? You need distribution: are overrides concentrated in specific reviewers or spread evenly? You need reasons: why did the human disagree?

Monitoring override rates is not just about measuring humans. It is about measuring the interaction between humans and AI. When override rates are stable and low, the system is probably working. When they spike, something changed — the model, the policy, the reviewer population, or the case mix. When they drift upward slowly, trust is eroding. When they cluster in specific reviewers, you have a training problem or a selection problem. Override rate monitoring surfaces all of this.

## What Override Rate Measures and What It Misses

Override rate measures disagreement. The AI says approve, the human says deny — that is an override. The AI says deny, the human says approve — that is also an override. Both are logged. The rate is the count of overrides divided by the count of reviews. Simple math. But the interpretation is complex.

A low override rate can indicate three different states. First, the model might be highly accurate, and humans rarely find mistakes. This is the ideal state. Second, humans might be under-reviewing, accepting AI recommendations without scrutiny. This is the performative oversight state. Third, the model might be overfit to human preferences, producing recommendations that humans agree with even when those recommendations are wrong. This is the bias amplification state. Override rate alone does not distinguish these.

A high override rate can also indicate three states. First, the model might be underperforming, making frequent errors that humans must correct. This is the broken model state. Second, reviewers might be over-correcting, applying inconsistent standards or personal preferences. This is the reviewer variance state. Third, the model and reviewers might be operating under different policies, with reviewers enforcing a newer or stricter policy. This is the policy drift state.

To distinguish these states, you need additional signals. Compare override rate to ground truth evaluations. If the model's precision on held-out test data is 92 percent but the override rate is 15 percent, either the test data is unrepresentative or reviewers are over-correcting. If test precision is 78 percent and override rate is 8 percent, reviewers are under-correcting. The gap between model performance and override rate reveals the quality of human review.

You also need to track override direction. If 90 percent of overrides change denials to approvals, the model is too conservative. If 90 percent change approvals to denials, the model is too permissive. Balanced overrides — 50 percent in each direction — suggest the model is near the decision boundary and humans are refining marginal cases. This is healthy. Imbalanced overrides suggest systematic miscalibration.

## Baseline Override Rates by Domain

Override rates vary by domain and task complexity. In content moderation, experienced reviewers override AI decisions 10 to 20 percent of the time for borderline content. In loan underwriting, override rates for routine applications are typically 3 to 7 percent. In medical triage, override rates for high-confidence recommendations are below 5 percent, but for low-confidence cases, they exceed 30 percent. These baselines help you interpret your own rates.

If your loan underwriting model has a 25 percent override rate, something is wrong. Either the model is broken, reviewers are poorly trained, or the policy is ambiguous. A rate this high means humans are making the real decision in one of every four cases. The AI is not a decision aid — it is a data aggregator with humans as the decision layer. This might be acceptable for highly complex cases, but not for routine ones.

If your content moderation model has a 2 percent override rate, something is also wrong. Either the model is extremely well-tuned, or reviewers are not engaging. Content moderation involves inherent ambiguity. Reasonable people disagree on edge cases. A 2 percent override rate suggests reviewers are not exercising judgment. They are clicking "approve" reflexively. Audit a sample. Watch reviewers work. Time how long they spend per case. If average review time is under 10 seconds, they are not reading the content.

Establish baselines during the pilot phase. Measure override rate on a known dataset with known ground truth. Compare reviewer decisions to ground truth. This tells you how often reviewers are correct when they override. If reviewers override 15 percent of AI decisions but are correct 85 percent of the time when they do, their judgment is valuable. If they are correct 50 percent of the time, they are guessing. Incorporate reviewer accuracy into your baseline expectations.

## Segmenting Override Rates by Reviewer

Not all reviewers have the same override rate. Some override 5 percent of cases. Others override 40 percent. This variance is expected — reviewers have different experience, training, and judgment. But excessive variance is a red flag. It indicates inconsistent standards or inadequate calibration.

Track override rate per reviewer. Plot the distribution. A normal distribution is healthy — most reviewers cluster around the median with a few outliers on each side. A bimodal distribution suggests two subpopulations: one group trusts the model, another does not. A heavily skewed distribution suggests training issues. If 80 percent of reviewers override 5 to 10 percent of cases but 20 percent override 30 to 40 percent, the high-override group needs investigation.

High override reviewers fall into three categories. First, they might be your best reviewers — experienced, detail-oriented, catching errors others miss. If their overrides align with ground truth more than other reviewers, they are high performers. Promote them. Have them train others. Second, they might be over-correcting — applying personal judgment instead of policy. If their overrides do not align with ground truth, they are adding noise. Retrain them. Third, they might be seeing harder cases. If case assignment is not random, high override rates might reflect case difficulty, not reviewer behavior.

Low override reviewers also fall into categories. They might be highly aligned with the model because the model performs well on their case mix. They might be under-reviewing, rubber-stamping decisions. Or they might be seeing easier cases. To distinguish, audit their work. Sample cases where they agreed with the AI. Check against ground truth. If agreement is correct 95 percent of the time, they are fine. If it is correct 70 percent of the time, they are under-reviewing.

Set acceptable ranges per reviewer based on case mix and experience. A senior reviewer on complex cases might have a baseline of 20 percent overrides. A junior reviewer on routine cases might have a baseline of 8 percent. Alert when a reviewer deviates more than 50 percent from their baseline for more than a week. This catches performance changes early.

## Tracking Override Rate by Model Confidence

Model confidence predicts override likelihood. Low-confidence predictions should have high override rates. High-confidence predictions should have low override rates. If this pattern does not hold, your confidence scores are not meaningful.

Segment override rate by confidence band. For confidence above 90 percent, override rate should be below 5 percent. For confidence 70 to 90 percent, override rate might be 10 to 15 percent. For confidence below 70 percent, override rate might exceed 25 percent. These thresholds depend on your domain, but the trend should be monotonic: higher confidence correlates with lower overrides.

If override rate is flat across confidence bands, your model's confidence is uncalibrated. A confidence score of 95 percent is no more reliable than 55 percent. This is a model problem. Recalibrate using Platt scaling or isotonic regression. Train a secondary model to map raw scores to calibrated probabilities. Validate on held-out data. Repeat until confidence predicts accuracy.

If override rate is inverted — higher confidence has higher overrides — your model is confidently wrong. It is most certain when it is most incorrect. This is worse than uncalibrated. This suggests the model learned spurious patterns and is confident in those patterns. Investigate feature importances. Check for data leakage. Check for bias. This pattern often emerges when a model overfits to shortcuts.

Monitoring this relationship over time detects model degradation. If override rate for high-confidence cases increases from 4 percent to 12 percent over three months, the model is drifting. Case mix might have changed. Policies might have changed. The model might be encountering distribution shifts. Whatever the cause, the model's high-confidence decisions are no longer trustworthy. Retrain or recalibrate.

## Override Rate by Decision Type and Case Complexity

Not all decisions are equally difficult. Routine loan applications with strong credit profiles are easy. Borderline applications with mixed signals are hard. Override rates should reflect this. Easy cases should have low override rates. Hard cases should have high rates. If override rates are flat across difficulty, humans are not differentiating. They are applying the same review effort to all cases.

Define case complexity metrics. For loans, complexity might be a function of credit score variance, debt-to-income ratio proximity to threshold, and employment history length. For content moderation, complexity might depend on borderline policy matches, context ambiguity, and user history. Assign a complexity score to each case. Segment override rate by complexity quintile.

In a well-functioning system, override rate increases with complexity. The bottom quintile — easiest cases — might have 3 percent overrides. The top quintile — hardest cases — might have 30 percent overrides. This is expected. Humans are spending their judgment on cases that need it. If override rate is uniform, humans are wasting time on easy cases or rushing through hard ones.

If override rate is high even for low-complexity cases, the model is broken at a fundamental level. It is failing on cases that should be straightforward. This suggests data quality issues, feature engineering errors, or fundamental misalignment between training data and production data. Do not tune the model. Rebuild it.

If override rate is low even for high-complexity cases, humans are under-reviewing complex cases. This might be due to time pressure, reviewer fatigue, or insufficient training on edge cases. Audit complex cases that were not overridden. Check against ground truth. If error rate is high, add mandatory review time minimums for complex cases. Force reviewers to spend at least 60 seconds on cases above a complexity threshold.

## Time-Based Patterns in Override Rates

Override rates change over time. Models drift. Policies change. Reviewers get tired. Case mix shifts. Monitoring trends catches these changes before they compound into systematic failures.

Plot override rate daily. Look for spikes. A spike might indicate a model deployment that introduced a bug. A spike on a specific date might correlate with a policy change that reviewers adopted but the model did not. A gradual upward trend over weeks indicates drift — the model is losing relevance and humans are compensating.

Look for weekly patterns. If override rate is higher on Mondays, reviewers might be applying weekend policy changes that are not yet in the model. If override rate is higher on Fridays, reviewer fatigue might be setting in — but this should cause lower overrides, not higher, if fatigue reduces scrutiny. Higher overrides on Fridays suggest reviewers are being more conservative as they close out the week.

Look for daily patterns. If override rate is higher in the afternoon, reviewer fatigue is likely. If override rate is higher in the morning, reviewers might be more alert and catching errors. If override rate is uniform across the day, time of day does not affect judgment. This is the ideal state.

Compare override rate to model version. When a new model is deployed, override rate should drop if the model improved or rise if it regressed. If override rate does not change, the model update had no effect. This might mean the update targeted a problem that does not affect the cases reviewers see, or it might mean the update did nothing. Check model performance metrics. If they improved but override rate did not drop, reviewers are not benefiting from the improvement.

Correlate override rate with case volume. If volume spikes and override rate drops, reviewers are rushing to clear the queue. They are under-reviewing. If volume spikes and override rate rises, the spike might be bringing in different types of cases that the model handles poorly. Investigate the case mix during the spike.

## Using Override Reasons to Diagnose Model Issues

When a human overrides the AI, they should provide a reason. The reason is logged. The reasons aggregate into patterns. These patterns diagnose what the model is getting wrong.

Reason codes should be structured. Free-text reasons are hard to analyze. Use a fixed set of codes: "insufficient credit history", "income verification failed", "policy exception applied", "model flagged incorrect feature". Each code maps to a category of model error or policy condition. When aggregating overrides, count by reason code.

If 60 percent of overrides cite "policy exception applied", humans are making discretionary decisions that the model cannot capture. This is expected for some decision types. But if exceptions are this frequent, consider encoding them in the model. If "first-time homebuyer exception" accounts for 30 percent of overrides, add a first-time homebuyer feature to the model.

If 40 percent of overrides cite "model flagged incorrect feature", the model is relying on features that humans consider irrelevant or misleading. Investigate which features. If the model flags zip code but humans override because zip code is not supposed to matter per policy, the model learned a spurious pattern. Remove the feature or penalize it during training.

If override reasons are evenly distributed with no dominant pattern, overrides are random noise. This suggests reviewers are applying personal judgment without consistent criteria. Retrain reviewers. Clarify policies. Provide examples of correct overrides.

If override reasons change over time, policies or case mix are shifting. A sudden increase in "policy exception applied" might indicate a new policy that reviewers know but the model does not. A decrease in "insufficient credit history" overrides might indicate that the model was retrained to handle this case better.

## Alerting on Override Rate Anomalies

Override rate should be stable within expected bounds. Deviations indicate problems. Set up automated alerts for anomalies.

Alert if override rate for any reviewer exceeds their baseline by more than 50 percent for three consecutive days. This catches reviewers who suddenly start overriding much more or much less. Investigate immediately. They might have encountered a batch of unusual cases, or they might have changed their review behavior.

Alert if aggregate override rate shifts by more than 20 percent week-over-week. A shift from 10 percent to 12 percent is noise. A shift from 10 percent to 15 percent is a signal. Check what changed: model version, reviewer population, case mix, policies. One of these changed and caused the shift.

Alert if override rate inverts by confidence band. If low-confidence cases have lower override rates than high-confidence cases for a full day, something is broken. Either confidence scores are wrong, or routing logic is wrong. This inversion should never happen in a well-functioning system.

Alert if override rate for a specific decision type drops to near zero. If loan denials are overridden 12 percent of the time on average but drop to 1 percent for a week, reviewers might have stopped reviewing denials. Check if denial volume spiked and reviewers are overwhelmed. Check if a policy change made denials less reviewable.

Alerts should route to both ML and Operations. ML owns the model. Operations owns the reviewers. Both must collaborate to diagnose. Many override rate issues are neither pure model problems nor pure reviewer problems — they are interaction problems.

The next subchapter addresses escalation frequency and reviewer disagreement tracking — monitoring when cases are sent to senior reviewers and measuring whether different reviewers agree with each other.

