# 2.6 — Cost Attribution: Tracking Spend by User, Feature, and Model

In August 2025, a SaaS company offering AI-powered document analysis crossed two million dollars in monthly AI spend. The CFO asked a simple question: where is the money going? The engineering team had no answer. They knew total spend. They knew which models they used. But they had no idea which features consumed what share of the budget, which customers drove the most cost, or which product experiments were burning money. They were flying blind on the second-largest line item in their P&L.

They spent two weeks adding cost attribution. They tagged every request with user ID, feature name, model name, and experiment variant. Within a month they discovered that one feature, an experimental AI summarization tool used by 8 percent of customers, was consuming 63 percent of their AI budget. The feature called Claude Opus 4.5 with 12,000-token context windows for every document, even single-page PDFs. The median customer using this feature cost them 140 dollars per month. The feature was priced at 29 dollars per month.

They shut down the feature, redesigned it with a smaller model and dynamic context sizing, and relaunched it three months later. AI spend dropped to 1.1 million dollars per month. Gross margin improved by 18 percentage points. The CFO stopped asking questions. The engineering team learned that you cannot optimize what you do not measure.

Cost attribution is the practice of tagging and aggregating AI costs at multiple dimensions so you can see exactly where money is spent and make data-driven decisions about pricing, model selection, and feature investment.

## The Four Dimensions of Cost Attribution

Every AI request should be tagged with at least four dimensions: user, feature, model, and timestamp. These four dimensions let you slice cost data in every way that matters for business and engineering decisions.

**User dimension** — Tagging by user ID or customer ID lets you calculate per-user cost. This is critical for understanding unit economics, identifying high-cost outliers, and setting usage-based pricing. A user who runs 500 queries per day with long contexts costs you far more than a user who runs 10 queries per day with short contexts. If both users pay the same subscription price, one is profitable and one is not. Without per-user attribution, you cannot see this.

**Feature dimension** — Tagging by feature or product area lets you see which parts of your product consume the most budget. A multi-feature AI product might have a chatbot, a document summarizer, a code generator, and a search assistant. Each feature has different usage patterns and cost profiles. The chatbot might use GPT-5-mini with 2,000-token contexts. The document summarizer might use Claude Opus 4.5 with 20,000-token contexts. The code generator might use Codex-Max with 8,000-token contexts. Feature-level attribution shows you where to focus optimization effort and where to charge premium prices.

**Model dimension** — Tagging by model and provider lets you compare costs across model choices. If you are experimenting with routing between GPT-5 and GPT-5-mini, model-level attribution shows you the actual cost difference. If you are considering switching from Claude Opus 4.5 to Gemini 3 Pro, you need to see not just the per-token price but the actual cost in production after accounting for context lengths, output lengths, and request volumes.

**Timestamp dimension** — Tagging with timestamp lets you track cost over time, identify cost spikes, and correlate cost changes with product changes or traffic patterns. A cost spike might correlate with a new feature launch, a bug that caused excessive retries, or a traffic surge from a marketing campaign. Without timestamps, you see the spike in your bill but cannot explain it.

These four dimensions combine to answer every cost question that matters. Which users cost the most? Which features drive spend? Which models are cheapest in practice? When did cost spike and why? Every question is answerable if you tag requests correctly.

## Tagging Requests for Attribution

Tagging happens at request ingestion. When a user request arrives, your system attaches metadata to the request object that flows through your entire pipeline. This metadata is logged with every model call, every retrieval operation, and every cost-generating event.

The tag structure looks like this: user ID, feature name, model name, timestamp, request ID. The request ID ties all operations within a single user request together. A single user request might make three model calls, two vector database queries, and one reranking operation. All six operations share the same request ID, user ID, and feature name. This lets you sum costs across all operations to get the true cost of serving that request.

Your instrumentation library should make tagging automatic. You set the tags once at the start of the request handler, and every instrumented operation inherits those tags. If you have to manually tag every model call, every vector search, and every database query, you will make mistakes and miss operations. Automatic tag propagation through context or thread-local storage is the only reliable approach.

A logistics company built a custom middleware that extracted user ID from the authentication token, inferred feature name from the API endpoint, and injected both into a request context object. Every downstream service read tags from context. This meant engineers writing new features did not have to remember to tag manually. Tagging was infrastructure, not application code.

## Aggregating Costs at Different Granularities

Once you tag every request, you aggregate costs at the granularities that drive decisions. The same underlying data supports multiple views.

**Per-user cost** — Sum all costs for a given user over a time window. This tells you which users are expensive, which are cheap, and what the distribution looks like. A B2B SaaS company found that 5 percent of their customers accounted for 68 percent of their AI spend. These customers were running automated scripts that called their API thousands of times per day. The company introduced tiered pricing with higher per-query costs for high-volume users. Revenue increased by 30 percent while AI spend stayed flat.

**Per-feature cost** — Sum all costs for a given feature over a time window. This tells you where your budget is going at the product level. A productivity tool found that their AI meeting summarizer, used by 40 percent of customers, consumed 71 percent of their AI budget because it transcribed and summarized hour-long meetings with Whisper and Claude Opus 4.5. They optimized by switching to Gemini 3 Flash for summarization and implementing smart chunking. Feature cost dropped by 55 percent with no detectable quality loss.

**Per-model cost** — Sum all costs for a given model over a time window. This tells you which models are actually expensive in your workload. A team experimenting with model routing found that GPT-5 was twice as expensive per token as GPT-5-mini, but their actual workload used GPT-5 for 18 percent of requests and GPT-5-mini for 82 percent. Total spend was split 60 percent GPT-5, 40 percent GPT-5-mini. The GPT-5 minority was consuming the majority of the budget because those requests used much longer contexts. Model-level attribution made this visible.

**Per-customer-segment cost** — Sum costs by customer segment, defined by plan tier, industry, or usage pattern. This tells you whether free-tier users are subsidized by paid users, whether enterprise customers cost more to serve than SMB customers, and whether your pricing reflects your costs. A legal tech company found that enterprise customers on unlimited plans cost four times as much to serve as mid-market customers on metered plans, but enterprise pricing was only 2.5 times higher. They adjusted enterprise pricing and introduced usage caps.

## Showback and Chargeback Models for Multi-Team Organizations

In large organizations, multiple product teams share a centralized AI platform. The platform team provides model access, guardrails, and observability. Product teams build features on top. Without cost attribution, the platform team has no way to allocate costs to product teams, and product teams have no visibility into their consumption.

**Showback** is the practice of showing each team what they cost without charging them. The platform team reports monthly: Team A consumed 140,000 dollars in AI costs, Team B consumed 85,000 dollars, Team C consumed 210,000 dollars. Teams see their impact but are not directly billed. Showback creates awareness and social pressure. Teams that see their costs rising ask questions and optimize. Teams that see themselves consuming far more than peers feel accountable.

**Chargeback** is the practice of actually billing each team for their usage. The platform team tracks costs in real-time and charges each product team's budget at the end of the month. Chargeback creates financial accountability. If Team C is consuming 210,000 dollars per month, that comes out of Team C's budget, not a shared pool. This aligns incentives. Teams optimize aggressively because their budget depends on it.

A fintech company with eight product teams implemented chargeback in early 2025. Within three months, total AI spend dropped by 34 percent. Teams that were previously indifferent to cost started caching aggressively, routing to smaller models, and reducing context lengths. One team discovered they were accidentally calling GPT-5 for a feature that worked fine with GPT-5-mini. The bug had existed for six months but nobody noticed until costs were attributed. Chargeback fixed it in two weeks.

The technical implementation of chargeback requires tagging every request with team ID, aggregating costs by team, and integrating with internal billing systems. Most organizations use a cost aggregation service that reads logs, sums costs by tag, and exports monthly reports. The platform team reviews the reports with finance and allocates charges. It is not glamorous work, but it is transformative for cost discipline.

## Tracking Cost Per Request and Cost Per User Session

Per-request cost is the finest-grained cost metric. It answers: how much did this one user request cost to serve? This metric is noisy because requests vary widely. A simple question might cost 0.02 dollars. A complex research query with retrieval, reranking, and a long model response might cost 1.40 dollars. But the distribution of per-request costs tells you what typical and extreme costs look like.

A customer support agent platform tracked per-request cost and found that p50 was 0.08 dollars, p95 was 0.62 dollars, and p99 was 3.20 dollars. The p99 was 40 times the p50. They investigated the p99 requests and found they were complex technical troubleshooting queries that retrieved dozens of documents and generated multi-step solutions. These queries were valuable but expensive. They introduced a tiered pricing model where standard support cost 0.15 dollars per query and premium support cost 0.50 dollars per query with no cap. High-cost queries moved to premium tier. Margins improved.

**Cost per user session** aggregates all requests within a session. A session might be a 20-minute conversation with 15 back-and-forth exchanges. Summing per-request costs across the session gives you session cost. This metric better reflects user behavior. Users do not think in requests. They think in conversations. A user who has a 15-turn conversation costing 2.40 dollars is different from a user who has one single request costing 2.40 dollars. The first user is engaged and exploring. The second user had one expensive query. Session cost captures this difference.

A therapy chatbot company tracked cost per session and found their median session cost was 0.90 dollars, but 10 percent of sessions exceeded 5 dollars. These long sessions involved users in crisis who chatted for over an hour. The company decided these sessions were the most valuable and should not be penalized with usage caps. They introduced session-based pricing: 5 dollars per session, unlimited turns within the session. This aligned pricing with value and made long sessions economically viable.

## Cost Attribution for Model Experiments and A/B Tests

When you run experiments, you need to know the cost of each variant. If Variant A uses GPT-5 and Variant B uses Gemini 3 Pro, and both variants have similar quality, the cost difference might be the deciding factor. Without cost attribution by experiment variant, you are guessing.

Tagging requests with experiment ID or variant name lets you calculate cost per variant. A legal document review tool ran an experiment routing 50 percent of traffic to Claude Opus 4.5 and 50 percent to Gemini 3 Pro. Both models performed well on their quality metrics. Claude had slightly better precision but cost 0.38 dollars per request at p50. Gemini had slightly lower precision but cost 0.16 dollars per request at p50. The quality difference was 2 percent. The cost difference was 58 percent. They chose Gemini and accepted the small quality trade-off.

Experiment-level cost attribution also catches cost regressions before they reach production. A team tested a new prompt variant that improved task accuracy by 4 percent. They celebrated the quality win and planned to roll it out. Then they checked experiment cost data and found the new prompt used 60 percent more tokens because it included additional examples. Cost per request increased by 52 percent. The 4 percent quality improvement was not worth a 52 percent cost increase. They iterated on the prompt, shortened the examples, and found a version that gave 3 percent quality improvement with only 8 percent cost increase. That version shipped.

## Real-Time Cost Dashboards

Cost attribution data is most valuable when it is real-time or near-real-time. Waiting until the end of the month to see cost breakdowns means you cannot react to cost spikes until damage is done. A real-time cost dashboard shows spend by user, feature, and model updating every few minutes.

A real-time dashboard lets you catch cost anomalies immediately. A customer accidentally launches a script that hammers your API with 10,000 requests per minute. Your dashboard shows their per-user cost spiking. You contact them within 20 minutes, they fix the bug, and you avoid a 30,000-dollar runaway cost incident. Without real-time visibility, you find out at the end of the month when the bill arrives.

Real-time dashboards also let you monitor experiments as they run. You launch a new feature to 10 percent of traffic. Within an hour, you see feature-level cost is 40 percent higher than expected. You investigate and find a bug that causes excessive retries. You fix the bug before rolling out to 100 percent of traffic. The cost attribution system saved you from a budget-breaking mistake.

The technical foundation for real-time cost dashboards is streaming log aggregation. Your services emit cost events to a message queue like Kafka. A stream processor consumes events, aggregates by tag dimensions, and writes to a time-series database. A dashboard queries the time-series database and displays costs by user, feature, and model with five-minute lag. This architecture is not trivial, but it is standard practice for high-scale AI platforms in 2026.

## The Cost of Not Attributing

A developer tools company spent 18 months with no cost attribution. They saw monthly bills from their model providers. They saw total spend growing. But they did not know which features drove growth, which customers cost the most, or where to optimize. When spend hit 800,000 dollars per month, the CFO demanded answers. The engineering team had none.

They spent a quarter instrumenting cost attribution, backfilling historical data, and building dashboards. What they found was shocking. One feature used by 12 percent of customers consumed 57 percent of spend. One customer, a hedge fund running automated market analysis, consumed 22 percent of total spend and paid 4,000 dollars per month while costing 76,000 dollars per month to serve. The feature was poorly designed and used the most expensive model for every request regardless of complexity. The customer was exploiting unlimited usage.

They fixed the feature, repriced the customer, and added usage caps for outliers. Within six months, spend dropped to 420,000 dollars per month while revenue stayed flat. Gross margin improved by 25 percentage points. Every dollar of that improvement was invisible until they implemented cost attribution. You cannot fix what you cannot see.

Next, we cover sampling strategies for high-volume systems, where logging every request is too expensive and logging nothing leaves you blind.
