# 14.2 â€” On-Call for AI Systems: What Changes from Traditional Services

On-call for traditional web services follows a known pattern. A service goes down. You check the logs. You restart the failed instance. You verify traffic is flowing. You write a postmortem. The playbook is mature. The tooling is proven. Engineers know what to expect.

Then they go on-call for an AI system. An alert fires: "Response quality degraded." They open the dashboard. Every technical metric looks fine. Latency normal. Error rate normal. Throughput normal. But user complaints are spiking. The model is producing subtly wrong answers that no error log captures, no health check detects, and no standard playbook addresses. Welcome to on-call for AI, where the failure modes do not map to traditional infrastructure and the debugging requires skills most engineers never learned.

## The Fundamental Difference: Silent Degradation

Traditional services fail loudly. The database crashes. The load balancer reports unhealthy instances. HTTP requests return 500 errors. You know something is wrong because the system stops working.

AI systems fail quietly. The model is still responding. Latency is fine. No errors in the logs. But the quality has degraded. It is producing confident nonsense. It is refusing safe requests. It is leaking training data. The system appears healthy while producing user-facing harm.

This changes the on-call engineer's job fundamentally. You cannot rely on technical health checks. You must monitor behavioral signals: user complaints, content policy violations, thumbs-down rates, escalation to human agents. The alert that pages you at 2am might say "toxicity rate increased 40 percent" or "correct answer rate dropped below 85 percent." These are not infrastructure problems you can fix by restarting a service. They are model behavior problems that require investigation, mitigation, and escalation.

## The On-Call Toolkit for AI Systems

An engineer on-call for AI needs different tools and different training than an engineer on-call for a traditional API.

**The Quality Dashboard** is your primary diagnostic tool. It shows real-time quality metrics: correct answer rate, policy violation rate, user satisfaction scores, escalation rate, refusal rate. When an alert fires, you open the quality dashboard first, not the error log. You are looking for patterns: did quality drop for all users or specific segments? Did it happen suddenly or gradually? Is it affecting all request types or specific queries?

**The Model Version Timeline** shows which model versions are serving traffic and when they changed. If quality degraded at 2:14am, you check whether a model deployment happened at 2:10am. If yes, you have a likely culprit. The mitigation is rollback. If no recent deployment, you are investigating something else: data drift, upstream service changes, or adversarial input patterns.

**The Sample Request Inspector** lets you examine actual requests and responses that triggered quality alerts. If the toxicity alert fired, you pull up recent responses flagged as toxic and read them. Are they actually toxic, or is the classifier misfiring? If the correct answer rate dropped, you examine recent incorrect responses. Is the model hallucinating? Refusing to answer? Giving outdated information? You need to see the actual outputs to understand the failure mode.

**The Traffic Segmentation View** breaks down quality metrics by user segment, geography, input language, and request type. If quality degraded only for Spanish-language requests, that is a different problem than degradation across all languages. If only enterprise customers are affected, that points to a specific integration issue. Segmentation turns a vague "quality is bad" signal into a specific "quality is bad for these users in this way" hypothesis.

**The Escalation Runbook** documents what to do for each type of quality failure. If toxicity spikes, you enable stricter content filtering and notify Trust and Safety. If correct answer rate drops, you roll back to the previous model version and notify the ML team. If cost spikes, you reduce traffic to expensive models and notify the finance team. The runbook is your decision tree when you are tired and pressured.

## What On-Call Engineers Can Actually Fix

Traditional on-call engineering empowers the on-call engineer to fix most problems. Restart the service. Scale up capacity. Roll back the bad deployment. Clear the disk space. Most incidents resolve without escalation.

AI on-call is different. Many incidents require escalation because the on-call engineer cannot retrain a model or rebuild a dataset at 3am. Understanding what you can fix versus what you must escalate prevents wasted time and dangerous improvisation.

You can fix configuration and traffic routing issues. If a bad model version deployed, you roll back. If traffic routing sends too much load to an expensive model, you adjust the router. If a content filter has incorrect thresholds, you tighten them temporarily. These are operational changes that do not require ML expertise.

You can implement immediate mitigations. If toxicity spikes, you enable the strictest content filter even though it increases false refusals. If latency spikes, you reduce batch sizes or shed load. If cost spikes, you route traffic away from the expensive model. These are blunt instruments that trade one metric for another, but they stop the bleeding while the team investigates root cause.

You cannot fix model quality issues. If the model is hallucinating, refusing safe requests, or producing subtly incorrect answers, you cannot fix that on-call. You can only mitigate by rolling back to a previous version or routing around the degraded model. The actual fix requires the ML team to investigate data issues, fine-tuning problems, or upstream service changes. Your job is to contain the damage and provide the ML team with diagnostic information when they arrive.

You cannot fix data pipeline issues. If the model is behaving strangely because the input data pipeline is broken, you can identify that as the likely cause, but fixing the pipeline is not an on-call task unless you happen to be the pipeline expert. You escalate to the data engineering team with evidence: quality degraded at the same time the data pipeline reported errors, here are the logs.

## The Handoff to Specialized Teams

AI incidents often require handoffs to specialized teams that traditional infrastructure incidents do not. The on-call engineer is the first responder, but not the resolver for many AI failure modes.

Trust and Safety handles content policy violations. If the model starts producing harmful content, racist outputs, or privacy leaks, you notify Trust and Safety immediately and they take over. Your job is to implement their mitigation instructions: enable stricter filters, disable certain features, or take the model offline if necessary.

The ML team handles model behavior issues. If the model is producing incorrect answers, hallucinating, or exhibiting degraded reasoning, you provide them with diagnostic data: when it started, which segments are affected, sample bad outputs, and what changed recently. They investigate whether this is a training issue, a data drift issue, or a prompt engineering problem.

The data engineering team handles pipeline failures. If input data is malformed, missing, or stale, you notify the data team and they debug the pipeline. You provide evidence: timestamps when quality degraded, examples of malformed inputs if visible, and correlation with pipeline alerts.

Product handles user communication. If the incident affects many users or will take hours to resolve, Product decides what to communicate externally, whether to disable features, and how to manage user expectations. You provide them with impact assessment: how many users affected, which features broken, estimated time to resolution.

The handoff is critical. AI incidents are multi-disciplinary. The on-call engineer triages, mitigates, and coordinates. The specialized teams resolve. Knowing when to hand off and to whom is half the skill.

## The AI-Specific Incident Template

When documenting an AI incident, the template differs from traditional infrastructure incidents. You need to capture information about model behavior, not just system health.

**Impact** includes traditional metrics: how many users affected, for how long, which features broken. But it also includes quality metrics: how much did correct answer rate drop, how many policy violations occurred, how many users complained or escalated to humans. The business impact of an AI incident is often quality degradation, not downtime.

**Symptoms** include both technical and behavioral signals. Traditional symptoms: latency increased, error rate spiked, traffic dropped. AI symptoms: toxicity rate increased, user satisfaction dropped, refusal rate increased, hallucination rate spiked. Both matter.

**Root cause** for AI incidents often involves model behavior, data quality, or interaction effects that traditional RCA frameworks do not cover. The root cause might be "model fine-tuned on data with temporal bias" or "upstream service started returning edge cases the model was never trained on" or "adversarial users discovered a jailbreak prompt." These are not infrastructure failures. They are AI-specific failure modes.

**Mitigation and resolution** distinguishes between what you did immediately to stop the harm and what the team did later to fix the underlying issue. Immediate mitigation: rolled back model, enabled stricter filters, shed traffic. Actual resolution: retrained model on better data, updated content policy classifier, improved prompt validation.

**Prevention** for AI incidents might include improving eval coverage, adding new quality metrics, updating fine-tuning procedures, or changing data validation. Traditional preventions like "add more monitoring" apply, but the monitoring needs to cover model behavior, not just infrastructure health.

## Training On-Call Engineers for AI

You cannot assume an engineer who is excellent at traditional on-call will automatically succeed at AI on-call. The skills overlap but are not identical. Training must cover AI-specific concepts.

Engineers need to understand how the AI system works at a conceptual level. They do not need to know the fine-tuning hyperparameters, but they need to know that models can degrade silently, that data drift causes behavior changes, that prompt changes affect outputs, and that model versions differ in capabilities. Without this mental model, they cannot interpret the alerts or form hypotheses about what went wrong.

They need to recognize the common AI failure modes: hallucination, toxicity, refusal, data leakage, degradation from distribution shift, performance issues from batch size changes. When an alert fires, they should have a checklist of likely causes to investigate. Did a model deploy recently? Did the input distribution change? Did someone modify prompts or content filters? Is this affecting all users or a segment?

They need hands-on practice with the AI-specific dashboards and tools before their first rotation. Shadow an experienced on-call engineer through a few incidents. Walk through historical incidents together, discussing what signals appeared, what the on-call engineer checked, and how they decided to escalate versus mitigate. Use past incidents as case studies.

They need the escalation paths and contact information for specialized teams. Who do you page for a Trust and Safety issue? Who handles data pipeline failures? Who approves rolling back a model? Who communicates with users during an outage? On-call engineers under pressure need this information at their fingertips, not buried in documentation they have to search for at 3am.

## The Emotional Difference: Ambiguity Under Pressure

Traditional on-call incidents have clear resolution criteria. The service is down or it is up. Traffic is flowing or it is not. You know when you have fixed the problem because the metrics return to normal.

AI on-call incidents often end ambiguously. You rolled back the model and toxicity dropped from five percent to two percent. Is that good enough? You tightened the content filter and refusals increased from eight percent to 15 percent. Did you make things better or worse? The model is still producing some incorrect answers, but fewer than before. Do you keep investigating or declare the incident resolved?

This ambiguity is cognitively draining. Engineers trained on binary success criteria struggle with AI's continuous quality spectrum. Training must prepare them for this. It is normal to resolve an AI incident without fully fixing the underlying issue. Your job on-call is to mitigate harm and stabilize the system. Full resolution might require days of ML work that happens after your rotation ends.

Document what you observed, what you changed, and what metrics improved. Hand off to the ML team with clear diagnostic information. Accept that you might not know the root cause and might not see the final fix. AI on-call is about damage control and information gathering, not about perfect resolution.

## Measuring On-Call Effectiveness for AI

Traditional on-call metrics measure time to detection, time to mitigation, time to resolution, and incident frequency. These still matter for AI systems, but you need additional metrics that capture AI-specific effectiveness.

**Time to correct triage** measures how long it took the on-call engineer to identify whether the incident was a model issue, a data issue, an infrastructure issue, or a configuration issue. Faster triage means faster escalation to the right team.

**Accuracy of mitigation** measures whether the on-call engineer's immediate actions improved the situation or made it worse. If they tightened a content filter and refusals spiked so much that user satisfaction dropped further, that mitigation failed. If they rolled back a model and quality returned to baseline, that mitigation succeeded.

**Completeness of diagnostic handoff** measures whether the on-call engineer provided the ML team with useful information. Did they identify which user segments were affected? Did they provide sample bad outputs? Did they correlate the degradation with recent changes? High-quality handoffs accelerate root cause analysis.

**Observability gap identification** measures how often on-call engineers flag missing dashboards, unclear alerts, or confusing metrics. The on-call rotation is your best source of observability feedback because it reveals what information is missing when you need it most under pressure.

These metrics help you improve both on-call training and observability infrastructure. If time to correct triage is consistently long, engineers need better training on AI failure modes. If mitigations frequently backfire, your runbooks need better decision trees. If diagnostic handoffs are incomplete, your dashboards are not surfacing the right information.

## Building the On-Call Culture for AI

On-call for AI systems requires more collaboration, more humility, and more documentation than traditional on-call. You will escalate more often. You will hand off to specialized teams. You will end shifts without fully understanding what happened. The culture must support this.

Celebrate good escalations, not just resolutions. An engineer who correctly identified a model behavior issue and escalated to the ML team with excellent diagnostic data did their job well, even though they did not fix the problem themselves. Recognize that.

Document every incident thoroughly, even the ambiguous ones. AI systems have long-tail failure modes that only become visible over time. The incident you half-resolved this week might provide the clue that solves a bigger mystery next month.

Debrief after every rotation. What did the on-call engineer struggle with? What dashboards were confusing? What runbooks were missing or stale? What escalation paths were unclear? Use this feedback to improve the on-call experience continuously.

Rotate everyone, including ML engineers. When the people who build models also carry the pager, they instrument better, write clearer runbooks, and build more operable systems. The empathy goes both directions: on-call engineers understand ML constraints better, and ML engineers understand operational constraints better.

AI on-call is harder than traditional on-call because the failure modes are less familiar and the fixes less obvious. But with the right tools, training, and culture, it becomes manageable. The next step is establishing the weekly review cadence that keeps the team aligned on what matters.

