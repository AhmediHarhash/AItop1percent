# 2.4 — Token Accounting: Input, Output, Cached, and Wasted Tokens

A fintech company running an AI-powered contract analysis tool received their March 2026 invoice from OpenAI and stared at a number that made no sense: $127,000. They had budgeted $40,000 based on their expected request volume and their estimated tokens per request. They immediately suspected billing errors, fraudulent usage, or a security breach. The actual cause was none of those. Their system was working exactly as designed — it was just designed badly. Every contract analysis request pulled the full contract text, 15,000 tokens on average, plus a system prompt of 2,000 tokens, plus few-shot examples totaling 3,000 tokens, plus conversation history averaging 4,000 tokens. Total input: 24,000 tokens per request. They were making 180,000 requests per month. The math was brutal and correct. They were spending $127,000 because they had never instrumented token accounting, never optimized their prompts, and never questioned whether they needed to send 24,000 input tokens to answer questions that required 200 output tokens.

When they finally added token tracking, they discovered that 40 percent of their input tokens were conversation history that the model never referenced. Another 25 percent were redundant few-shot examples that provided no marginal value beyond the first three examples. Another 15 percent were retrieval results ranked so low that they contributed nothing to the response. They cut their token usage by 60 percent in three weeks without degrading quality. Their monthly bill dropped to $51,000. They had been hemorrhaging $76,000 per month because they were not measuring where their tokens went.

Token accounting is not optional at scale. It is the difference between sustainable AI costs and runaway AI costs. Every token you send to a model provider costs money. Every token the model generates costs more money. If you do not track which tokens are necessary, which are redundant, and which are wasted, you have no basis for optimization. You are flying blind with a budget that compounds every time traffic grows.

## The Four Categories of Tokens

Not all tokens are created equal, and treating them as a single undifferentiated number obscures where your money goes. **Input tokens** are what you send to the model — system prompt, few-shot examples, retrieved context, conversation history, and the user's current query. Input tokens are priced lower than output tokens, but they are not free. For GPT-5, input tokens cost $0.005 per thousand tokens in early 2026. For a request with 20,000 input tokens, that is $0.10 per request just for input. At one million requests per month, input tokens alone cost $100,000.

**Output tokens** are what the model generates — the response you return to the user. Output tokens are priced higher than input tokens because generation is more computationally expensive than processing input. For GPT-5, output tokens cost $0.015 per thousand tokens in early 2026 — three times the cost of input. If your system generates 1,000 output tokens per request on average, that is $0.015 per request. At one million requests per month, output tokens cost $15,000. Total cost for input and output combined: $115,000 per month. This is not a small team's side project budget. This is a line item that CFOs care about.

**Cached tokens** are input tokens that the model provider recognizes as identical to a previous request and does not reprocess. Prompt caching, supported by OpenAI, Anthropic, and others, lets you send the same system prompt or the same retrieved documents across multiple requests and pay a reduced rate for the repeated tokens. Cached tokens cost 10 to 50 percent of regular input token costs, depending on the provider. If 80 percent of your input tokens are cacheable — a static system prompt, static few-shot examples — prompt caching can cut your input costs by 70 percent. But only if you structure your prompts to maximize cache hits and only if you track cache hit rates to verify it is working.

**Wasted tokens** are tokens you paid for but got no value from. Wasted tokens include: output tokens in a response that was rejected by a guardrail and never shown to the user. Input tokens in a retry attempt after the first request failed. Input tokens in retrieval context that the model never referenced. Output tokens that were truncated because the model hit the max token limit before finishing. These are not a separate billing category — your provider charges you for them as normal input or output tokens. But conceptually, they are waste. You paid for generation or processing that provided zero value. Tracking wasted tokens tells you where to optimize.

## Tracking Input Token Breakdown

Input tokens are not monolithic. A 20,000-token input might include 2,000 tokens of system prompt, 3,000 tokens of few-shot examples, 12,000 tokens of retrieved context, 2,000 tokens of conversation history, and 1,000 tokens of user query. If you log only the total input token count, you cannot optimize. You do not know whether the bloat is in retrieval, conversation history, or few-shot examples. If you log a breakdown, optimization becomes obvious.

**System prompt tokens** are the static instruction set you send with every request. For most systems, this is 500 to 3,000 tokens and rarely changes. You should log system prompt token count separately so you can measure it, optimize it, and detect regressions. If your system prompt grows from 1,500 tokens to 3,500 tokens because someone added verbose examples, you need to know immediately. A 2,000-token increase in the system prompt, at one million requests per month and $0.005 per thousand tokens, costs an extra $10,000 per month. If that change provided no quality improvement, it is pure waste.

**Few-shot example tokens** are the examples you include to guide the model's behavior. Few-shot examples are powerful for improving quality, but they are also expensive. If you include ten examples totaling 5,000 tokens, you are spending $0.025 per request on examples alone. At one million requests, that is $25,000 per month. The question is whether those examples provide $25,000 worth of quality improvement. The only way to know is to measure. Log how many few-shot tokens you send per request, run experiments with fewer examples, measure quality, and find the optimal trade-off.

**Retrieved context tokens** are the documents or chunks your RAG system injects into the prompt. This is often the largest and most variable component of input tokens. A request that retrieves five chunks totaling 2,000 tokens is cheap. A request that retrieves thirty chunks totaling 15,000 tokens is expensive. If your retrieval system over-retrieves — pulls more documents than necessary because your relevance threshold is too low — you are wasting money. Logging retrieved token counts per request, and correlating them with response quality, lets you tune retrieval depth to minimize cost without sacrificing quality.

**Conversation history tokens** grow with every turn in a multi-turn chat. Each time the user sends a message, you append the previous exchange to the prompt. After ten turns, you might have 10,000 tokens of history. Some of that history is necessary for context. Some of it is irrelevant to the current query. If you never prune history, history tokens will dominate your input costs. Logging history token counts per request lets you detect when history is bloating prompts and implement pruning strategies — summarize old turns, drop low-relevance turns, or use a sliding window that keeps only the most recent turns.

**User query tokens** are the smallest component, typically 50 to 500 tokens. You should still log them separately because query length can spike. If a user pastes a 5,000-token document into your chat interface and asks "summarize this," your query tokens jump by 10x. If you did not anticipate this, it creates a cost surprise. Tracking query token distribution lets you set limits, warn users about long queries, or route long queries to a cheaper model.

## Tracking Output Token Breakdown

Output tokens are what the model generates, and they are more expensive than input tokens. Tracking output token counts is essential, but tracking output token usage patterns is even more valuable. **Successful response tokens** are the tokens in a response that passed all guardrails, was returned to the user, and provided value. This is the baseline. Every other category of output tokens is overhead or waste.

**Rejected response tokens** are tokens in a response that a guardrail blocked. The model generated the response, you paid for those tokens, but the user never saw them because a toxicity filter, a PII detector, or a policy guardrail blocked the output. You then generate a fallback response or return an error, consuming more output tokens. If 5 percent of your responses are blocked and each blocked response averages 500 tokens, you are paying for 25,000 wasted output tokens per million requests. At $0.015 per thousand tokens, that is $375 per month. Not catastrophic, but avoidable if you improve your prompt or adjust your guardrail thresholds to reduce false positives.

**Truncated response tokens** occur when the model hits the max token limit before finishing. You set max_tokens to 2,000, the model generates 2,000 tokens, and the response ends mid-sentence. You paid for 2,000 output tokens, but the response is unusable. Users see incomplete answers and complain. You might retry with a higher max_tokens, consuming even more tokens. Logging truncation events — how often they occur, which prompts trigger them, which models hit limits — helps you set max_tokens appropriately and detect prompts that are too complex for the token budget.

**Retry response tokens** accumulate when you retry failed requests. A request times out, you retry. A request hits a rate limit, you retry. A request returns a malformed response, you retry. Each retry consumes output tokens. If your retry logic is aggressive — retry five times with exponential backoff — and your failure rate is 2 percent, you are consuming extra output tokens on every retry. Tracking retry token consumption separately from successful response tokens shows you how much your retries cost and whether you should optimize retry logic or fix the root cause of failures.

## Prompt Caching and Cache Hit Rate

Prompt caching is one of the most effective cost optimizations for AI systems, but it only works if you structure prompts correctly and if you track cache hit rates to verify that caching is actually happening. **Cache hits** occur when the model provider recognizes that part of your input — typically the system prompt and few-shot examples — is identical to a recent request and skips reprocessing those tokens. You pay a reduced rate for cached tokens, often 10 to 50 percent of normal input token costs.

To maximize cache hits, you need to send cacheable content in the same order, with the same formatting, in every request. If your system prompt is "You are a helpful assistant" in one request and "You are a helpful assistant." in the next request — note the period — the provider might treat them as different prompts and miss the cache. If you randomize the order of few-shot examples, every request is unique, and caching fails. If you include request-specific data — timestamps, request IDs — in the system prompt, every request is unique, and caching fails.

The way to structure prompts for caching is to front-load static content. Put your system prompt first, then your few-shot examples, then your retrieved context, then your conversation history, then the user query. The provider caches the prefix — the static content that appears in every request. The dynamic content — retrieved context, history, query — is not cached, but it comes after the cached prefix, so caching still works. If you intermix static and dynamic content, caching is less effective.

**Cache hit rate** is the percentage of cacheable tokens that were actually served from cache. If you send 5,000 cacheable tokens per request and your cache hit rate is 80 percent, you are paying reduced rates for 4,000 tokens and full rates for 1,000 tokens. If your cache hit rate is 20 percent, caching is barely helping. If your cache hit rate is 0 percent, caching is not working at all — maybe your prompts are too variable, maybe your traffic is too low to build a warm cache, or maybe you are not structuring prompts correctly.

Most providers expose cache hit rate in their API responses or in their usage dashboards. You should log cache hit rate per request and monitor it over time. If cache hit rate drops suddenly, something changed — a prompt template was updated, a new feature was deployed, or traffic patterns shifted. A 50 percent drop in cache hit rate can double your input token costs overnight.

## Identifying Wasted Tokens

Wasted tokens are the tokens you paid for but derived no value from. Some waste is unavoidable — retries after transient failures, guardrails blocking genuinely unsafe responses. But most waste is fixable. The first step is measurement. Log which tokens fall into each waste category and how often.

**Over-retrieval waste** happens when your RAG system pulls more documents than the model can use or needs. You retrieve twenty documents totaling 10,000 tokens, but the model only references the top three. The other 7,000 tokens were wasted. They consumed prompt space, increased latency, and cost money, but they provided zero value. To detect over-retrieval waste, you need to track how many documents were retrieved versus how many were cited in the response. If the model consistently cites only the top three documents, reduce your retrieval depth to five documents and measure whether quality degrades. If quality holds, you just cut retrieval costs by 50 percent.

**Redundant history waste** happens when you include conversation history that the model never references. After ten turns of chat, you might have 10,000 tokens of history in the prompt. If the user's current query only relates to the most recent turn, the other 9,000 tokens are wasted. To detect redundant history, you need to analyze which turns the model references in its response. This is hard to measure directly, but you can approximate it by running experiments: include full history versus include only the last three turns, measure quality, and see if full history provides any benefit. If not, prune history aggressively.

**Redundant example waste** happens when you include more few-shot examples than necessary. You might include ten examples because you assume more is better, but the model might perform just as well with three. To detect redundant example waste, run ablation studies: remove examples one at a time, measure quality, and find the minimum number of examples that maintains performance. Every example you remove saves tokens on every request forever.

**Guardrail rejection waste** happens when the model generates a response, you pay for output tokens, and then a guardrail blocks it. The tokens are wasted. To reduce guardrail rejection waste, you can improve your prompt to make unsafe responses less likely, or you can move some guardrails earlier in the pipeline — check the user query for policy violations before calling the model, so you never generate a response that will be blocked.

**Truncation waste** happens when the model hits max_tokens before finishing. You paid for the truncated tokens, but the response is incomplete and unusable. To reduce truncation waste, analyze which prompts or query types trigger truncation, increase max_tokens for those cases, or redesign the prompt to produce shorter responses.

## Cost Attribution by User, Feature, and Endpoint

Token accounting enables cost attribution: which users are expensive, which features are expensive, which endpoints are expensive. Without cost attribution, you cannot answer basic financial questions. A product manager asks "how much does the summarization feature cost per request?" You do not know. A finance team asks "which customers are driving our AI costs?" You do not know. An executive asks "should we raise prices for heavy users?" You cannot answer because you do not track per-user costs.

**Per-request cost** is calculated from token counts and model pricing. Input tokens times input token price, plus output tokens times output token price, plus any retrieval costs or tool costs. Sum these and log the total cost as a field. Now you can query cost per request, aggregate cost by any dimension — user, endpoint, model, feature flag — and detect cost anomalies.

**Per-user cost** is the sum of per-request costs for all requests by a specific user. Some users are cheap — they ask short queries, get short answers, use the system occasionally. Other users are expensive — they ask long queries, demand detailed answers, use the system constantly. If you charge a flat subscription fee, heavy users are subsidized by light users. If you want to introduce usage-based pricing, per-user cost data tells you where to draw the line.

**Per-endpoint cost** tells you which features are expensive. The chat endpoint might average $0.02 per request. The document summarization endpoint might average $0.30 per request because it processes 50,000-token documents. The code generation endpoint might average $0.15 per request. Knowing per-endpoint costs helps you prioritize optimization: focus on the endpoints with the highest total cost or the highest cost per request. It also helps product decisions: should you gate expensive features behind higher pricing tiers?

**Per-model cost** tells you which models are driving your bill. If you route requests to GPT-5, Claude Opus 4.5, and Llama 4, each has different pricing. GPT-5 might handle 60 percent of requests and consume 80 percent of costs because it is the most expensive. Llama 4 might handle 30 percent of requests and consume 5 percent of costs because it is cheap or self-hosted. If you want to cut costs, you need to know which models to optimize or replace.

## Detecting Cost Anomalies

Token accounting is not just for optimization — it is for incident response. Cost anomalies signal bugs, attacks, or misconfigurations. A sudden spike in input tokens per request means something changed. A sudden spike in output tokens per request means something changed. A sudden drop in cache hit rate means something broke. You need alerts that fire when token usage deviates from baseline.

**Input token spike alerts** fire when average input tokens per request exceed a threshold. If your baseline is 8,000 input tokens and it jumps to 30,000, something is wrong. Possible causes: your retrieval system is returning far more documents than usual. Your conversation history pruning logic broke, and you are including 50 turns of history. A user is submitting extremely long queries. Your prompt template changed and added bloat. Without an alert, you discover the problem when your monthly bill arrives. With an alert, you discover it within minutes and fix it before it costs thousands of dollars.

**Output token spike alerts** fire when average output tokens per request exceed a threshold. If your baseline is 800 output tokens and it jumps to 3,000, something is wrong. Possible causes: your prompt changed and the model is now generating verbose responses. Your max_tokens increased. A bug in your truncation logic means the model is generating until it hits the absolute max. An adversarial user is exploiting your system to generate long outputs. Again, early detection is the difference between a minor issue and a budget crisis.

**Cache hit rate drop alerts** fire when cache hit rate falls below a threshold. If your baseline cache hit rate is 85 percent and it drops to 30 percent, caching broke. Possible causes: your prompt template changed and the cacheable prefix is no longer stable. You introduced variability into the system prompt. Traffic patterns shifted and the cache is thrashing. A cache hit rate drop can double your input costs overnight, so detecting it quickly is critical.

**Wasted token alerts** fire when wasted tokens as a percentage of total tokens exceed a threshold. If 2 percent of your tokens are wasted due to guardrail rejections and it jumps to 15 percent, something changed. Possible causes: a new guardrail is too aggressive. A prompt change made unsafe responses more likely. An adversarial user is intentionally triggering guardrails. Wasted tokens are a leading indicator of quality problems or abuse.

## Storing and Querying Token Data

Token accounting data should be stored in your time-series metrics system — Prometheus, Datadog, Grafana Cloud, CloudWatch — so you can query it efficiently and visualize it in dashboards. For each request, emit metrics with tags for user ID, endpoint, model, prompt version, and any other relevant dimensions. Store input tokens, output tokens, cached tokens, total tokens, and estimated cost as separate metrics. Store cache hit rate as a gauge. Store wasted tokens as a counter.

With this instrumentation, you can answer questions like: what is the average input token count for the summarization endpoint? What is the p99 output token count for GPT-5? What is the total cost per day for each model? Which users consumed more than $100 in the last month? These queries run in milliseconds and power dashboards, alerts, and cost attribution reports.

You should also store detailed token breakdowns in logs or sampled traces for debugging. When you detect a cost anomaly, you need to drill into specific requests and see the full token breakdown: system prompt tokens, few-shot example tokens, retrieved context tokens, conversation history tokens, user query tokens. This level of detail does not belong in high-cardinality metrics — it would explode your metrics storage costs — but it belongs in sampled logs or traces so you can investigate outliers.

## Optimizing Based on Token Data

Once you have token accounting instrumentation, optimization becomes data-driven. You measure, you hypothesize, you experiment, you measure again. The fintech company that cut their bill from $127,000 to $51,000 did this methodically. They logged token breakdowns, identified that conversation history was 40 percent of input tokens, ran experiments with different pruning strategies, measured quality with their eval suite, and deployed the strategy that cut history tokens by 70 percent without degrading quality.

The same process applies to every token category. Measure how many few-shot examples you send. Run experiments with fewer examples. Measure quality. Deploy the minimum number that maintains quality. Measure how many documents you retrieve. Run experiments with fewer documents. Measure quality. Deploy the optimal retrieval depth. Measure how often responses are truncated. Increase max_tokens for prompts that truncate frequently. Measure whether truncation rate drops.

Token accounting is not a one-time project. It is an ongoing discipline. New features add new prompts. New prompts add new token costs. Traffic grows. Costs grow. You need continuous monitoring, continuous optimization, and continuous cost awareness. The teams that succeed at AI cost management do not treat tokens as an afterthought. They treat tokens as a first-class operational metric, as important as latency or error rate.

The next question is how to aggregate token data, trace data, and log data into dashboards and alerts that let you detect degradation, triage incidents, and optimize performance across the entire production system.

