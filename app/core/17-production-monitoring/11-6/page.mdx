# 11.6 — Model Cost Comparison: Measuring Price-Performance in Production

Model selection is not a one-time decision. Models get cheaper. New models launch. Your task evolves. A model that delivered the best price-performance in June might be outclassed by November. Model cost comparison means continuously measuring how much each model costs per successful outcome in production, so you can make informed decisions about switching, routing, or sticking with what you have.

## Price-Performance: The Metric That Matters

Cost per request tells you how much a model costs. Quality score tells you how well it performs. Neither metric alone tells you if you are using the right model. Price-performance combines them: cost per quality point, cost per successful request, or cost per user-valued outcome.

A search assistant uses Claude Opus 4.5 for all queries. Cost per query: $0.11. Quality score: 94 percent. In November 2025, the team tested GPT-5.1 on the same queries. Cost per query: $0.08. Quality score: 92 percent. Which model has better price-performance?

Claude Opus 4.5 costs 38 percent more per query but delivers 2 percentage points higher quality. GPT-5.1 costs 27 percent less but has slightly lower quality. Price-performance depends on how much the team values the 2 percentage point quality difference. If the quality difference is negligible to users, GPT-5.1 wins. If the quality difference matters, Claude Opus 4.5 is worth the premium.

The team ran an A/B test: half of users saw responses from Claude Opus 4.5, half from GPT-5.1. User ratings were statistically identical: 4.6 stars for both models. The 2 percentage point quality score difference did not translate to user-visible value. They switched to GPT-5.1 and saved $9,000/month.

Price-performance measurement requires defining what success means: user ratings, task completion, factual accuracy, user engagement. Once you define success, you calculate cost per successful outcome for each model and compare.

## Production-Based Model Comparison

Lab benchmarks compare models on static datasets. Production comparisons measure models on real traffic with real users. The two do not always agree. A model that scores higher on MMLU might perform worse on your specific use case. A model that wins in latency benchmarks might time out more often in production due to unpredictable load.

A document summarization service benchmarked five models in staging: GPT-5, Claude Opus 4.5, Claude Sonnet 4.5, Gemini 3 Pro, Llama 4 Maverick. All five performed well in offline evals. Claude Opus 4.5 had the highest quality score. GPT-5 was fastest. Llama 4 Maverick was cheapest.

The team deployed all five models to production with traffic splitting: 20 percent of requests to each model. They measured cost per request, latency, quality score, error rate, and user ratings. After two weeks, the production results diverged from staging.

Claude Opus 4.5 had the highest quality score in staging but similar user ratings to GPT-5 in production. GPT-5 was fastest in staging but had 1.2 percent higher error rate in production due to timeout issues during peak traffic. Llama 4 Maverick was cheapest but had 4 percent lower user ratings. Claude Sonnet 4.5 had the best balance: 10 percent lower cost than Claude Opus 4.5, statistically identical user ratings, and zero production issues.

The team adopted Claude Sonnet 4.5 as the default model. The decision was based on production data, not staging benchmarks. Production comparison revealed issues—timeout rates, user preferences, rare failure modes—that staging could not.

## Shadow Testing for Model Comparison

Shadow testing runs a candidate model alongside the production model without exposing users to the candidate's output. Every request is processed by both models. Users see the production model's response. The system logs both outputs for comparison.

A legal research tool uses GPT-5.1 in production. The team wants to test whether Claude Opus 4.5 delivers better quality at acceptable cost. They deploy Claude Opus 4.5 in shadow mode: for 10 percent of production traffic, the system calls both GPT-5.1 and Claude Opus 4.5, returns GPT-5.1's response to the user, and logs both responses with quality metrics.

After one week, the team has 12,000 side-by-side comparisons. Claude Opus 4.5 has 3 percentage points higher quality on automated metrics and identical ratings from human reviewers who evaluated a 500-query sample. Cost per query is 18 percent higher for Claude Opus 4.5.

The team runs a cost-quality tradeoff analysis. If they switch to Claude Opus 4.5, monthly spend increases from $18,000 to $21,200, but quality improves from 89 percent to 92 percent. They calculate that the 3 percentage point quality improvement reduces user escalations to human experts by 8 percent, saving $4,000/month in support costs. Net benefit: $800/month. They switch to Claude Opus 4.5.

Shadow testing eliminates risk. Users never see low-quality output from the candidate model. You gather production data without affecting user experience. The tradeoff is cost: you pay for both models during the test period. For high-stakes systems, the cost is worth it.

## Cost Variability Across Request Types

Not all requests cost the same. Some are short and simple. Some are long and complex. A model that is cheaper on average might be more expensive for the requests you care about most.

A customer support assistant handles two request types: simple FAQs and complex troubleshooting. Simple FAQs average 800 input tokens and 150 output tokens. Complex troubleshooting averages 2,400 input tokens and 600 output tokens.

The team tests GPT-5-mini and Claude Sonnet 4.5. GPT-5-mini costs $0.02 per simple FAQ and $0.09 per complex troubleshooting query. Claude Sonnet 4.5 costs $0.03 per simple FAQ and $0.12 per complex troubleshooting query. On average, GPT-5-mini is cheaper. But 70 percent of traffic is complex troubleshooting, where Claude Sonnet 4.5 delivers 6 percentage points higher quality.

The team implements hybrid routing: GPT-5-mini for simple FAQs, Claude Sonnet 4.5 for complex troubleshooting. This minimizes cost for low-value queries and maximizes quality for high-value queries. Total cost is 12 percent lower than using Claude Sonnet 4.5 for everything and quality is 4 percentage points higher than using GPT-5-mini for everything.

Model cost comparison must account for request type distribution. A model that wins on simple queries might lose on complex queries. Measure cost and quality separately for each important request type.

## Tracking Model Pricing Changes

Model pricing changes over time. Providers lower prices to stay competitive. New model versions launch at different price points. If you selected a model in June based on pricing, that decision might be wrong by December.

In August 2025, a document processing pipeline used GPT-5 at $3 per million input tokens and $15 per million output tokens. In November, OpenAI launched GPT-5.2 at $2 per million input tokens and $10 per million output tokens with similar quality. The team did not notice the price change for three weeks because they were not tracking model pricing updates.

They switched to GPT-5.2 and saved $7,000/month. The delay cost them $5,000 in unnecessary spend. The team implemented a pricing tracker: a script that checks provider pricing pages weekly and alerts when a model they use has a price change or when a new model launches in a relevant category.

Some teams track model pricing manually. Others use third-party aggregators that track pricing across providers. The method matters less than the discipline: model pricing is not static, and failing to track it means overpaying.

## Cost Comparison for Fine-Tuned Models

Fine-tuned models cost more per token than base models but often generate fewer tokens or require less context. Cost comparison must account for both per-token pricing and token efficiency.

A content moderation system uses GPT-5 base model with detailed prompts. Input tokens per request: 1,200. Output tokens: 80. Cost per request: $0.0048. Quality: 91 percent. The team fine-tunes GPT-5-mini on moderation data. Fine-tuned model pricing: 50 percent higher per token than GPT-5-mini base. But the fine-tuned model needs no few-shot examples and generates shorter outputs.

Fine-tuned GPT-5-mini input tokens: 400. Output tokens: 50. Cost per request: $0.0032. Quality: 92 percent. The fine-tuned model costs 33 percent less per request and delivers 1 percentage point higher quality despite using a smaller base model. The per-token price increase is more than offset by token efficiency.

Fine-tuned model cost comparison requires end-to-end measurement: total token usage multiplied by model-specific pricing. Per-token pricing alone is misleading.

## Comparing Models Across Providers

Multi-provider systems use models from multiple vendors: OpenAI, Anthropic, Google, Meta. Each provider has different pricing structures, token counting methods, and API overhead. Comparing costs across providers requires normalizing to a common metric.

A search platform uses GPT-5.1 from OpenAI and Claude Sonnet 4.5 from Anthropic. OpenAI charges per token with no rounding. Anthropic charges per token with rounding to the nearest ten tokens. For a request with 1,847 input tokens and 203 output tokens, OpenAI charges for exactly 1,847 and 203. Anthropic charges for 1,850 and 210.

The difference is small per request but compounds over millions of requests. The team tracks effective cost per request—actual invoice divided by request count—not calculated cost from token counts and published rates. Effective cost reveals discrepancies between expected and actual costs.

Cross-provider comparison also accounts for reliability. A cheaper model that times out 2 percent of the time might cost more than a slightly more expensive model with 0.1 percent timeouts if retries and lost requests are factored in.

## Cost-Quality Tradeoff Curves

Price-performance is not binary. Most systems can tolerate a range of cost-quality tradeoffs. Visualizing the tradeoff as a curve helps you find the optimal point.

A document assistant tests six models and measures cost per request and quality score for each. GPT-5.2: $0.09, 95 percent quality. Claude Opus 4.5: $0.11, 96 percent quality. Claude Sonnet 4.5: $0.07, 93 percent quality. GPT-5.1: $0.08, 94 percent quality. Gemini 3 Pro: $0.06, 91 percent quality. Llama 4 Maverick: $0.04, 87 percent quality.

The team plots cost vs. quality. The curve shows diminishing returns: moving from 87 percent to 91 percent quality costs $0.02 more per request. Moving from 91 percent to 93 percent costs $0.01 more. Moving from 93 percent to 96 percent costs $0.04 more. The steep part of the curve—93 percent to 96 percent—is where small quality gains require large cost increases.

The team decides 93 percent quality is acceptable. They adopt Claude Sonnet 4.5 at $0.07 per request. They could spend $0.04 more per request for 3 percentage points higher quality, but user testing shows the difference is not perceptible. The curve helps them avoid overpaying for imperceptible improvements.

## Measuring Opportunity Cost of Model Lock-In

Sticking with one model because it is familiar has opportunity cost. Every month you do not test alternatives, you might be overpaying or underperforming.

A customer success platform used GPT-5 from launch in March 2025 through November 2025. The team never tested alternatives because GPT-5 worked well and switching seemed risky. In December, a new engineer suggested testing Claude Sonnet 4.5. They ran a two-week shadow test. Claude Sonnet 4.5 delivered statistically identical quality at 22 percent lower cost.

They switched to Claude Sonnet 4.5 and saved $14,000/month. From March to November, they spent $126,000 more than they needed to. The opportunity cost of not testing alternatives was $126,000 over nine months.

The lesson: test alternatives at least quarterly, even when the current model works. Model landscape changes fast. Providers launch new models, drop prices, and improve performance. Testing every three months costs a few engineering hours and catches savings that compound over time.

## The Model Comparison Workflow

Model cost comparison is a repeatable workflow: define success metrics, select candidate models, deploy shadow or A/B testing, measure cost and quality in production, analyze price-performance, decide whether to switch.

A fintech assistant runs this workflow quarterly. Each quarter, the team identifies the top three or four models that might outperform the current production model. They deploy shadow testing for 10 percent of traffic over two weeks. They analyze results: cost per request, quality score, user ratings, latency, error rate. They calculate price-performance and decide whether to switch, stay, or route different request types to different models.

The workflow is systematic, not reactive. The team does not wait for a cost problem to arise. They proactively test alternatives and optimize before costs spiral. The quarterly cadence ensures they stay current with model releases and pricing changes.

## When Cost Comparison Reveals Quality Problems

Cost comparison sometimes surfaces quality issues you did not know existed. A model might have lower cost and similar aggregate quality but perform poorly on a specific subset of requests that matter disproportionately to users.

A search platform tested Llama 4 Scout as a cheaper alternative to GPT-5.1. Aggregate quality score: 91 percent for both models. Cost: Llama 4 Scout was 40 percent cheaper. The team prepared to switch. But during final review, they analyzed quality by query type. For factual queries, Llama 4 Scout scored 93 percent vs. GPT-5.1's 94 percent—acceptable. For queries requiring nuanced reasoning, Llama 4 Scout scored 84 percent vs. GPT-5.1's 92 percent—not acceptable.

Nuanced reasoning queries represented only 15 percent of traffic but drove 40 percent of user engagement. Switching to Llama 4 Scout would save money but harm the most valuable user interactions. The team adopted hybrid routing: Llama 4 Scout for factual queries, GPT-5.1 for reasoning queries. Cost dropped by 30 percent without sacrificing quality on the queries that mattered.

Cost comparison that ignores quality segmentation leads to bad decisions. Measure quality overall and by the dimensions you care about: query type, user segment, task complexity, domain.

The next subchapter covers cost forecasting—using production usage trends to predict future spend and plan budgets with confidence.

