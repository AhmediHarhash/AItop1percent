# 6.8 — Agent Self-Correction Frequency: A Signal of Instability

An agent that never reconsiders its decisions is rigid. An agent that reconsiders every decision is confused. Self-correction is necessary for robust reasoning. But too much self-correction indicates the agent does not trust its own outputs, cannot interpret data consistently, or is stuck oscillating between strategies.

Self-correction frequency measures how often an agent changes its plan, revises a conclusion, or re-evaluates a prior decision during a single trajectory. Occasional self-correction is healthy — the agent encounters new information and adjusts. Frequent self-correction is a symptom of instability. Monitoring self-correction frequency reveals when an agent's reasoning is deteriorating.

## The Endless Revision Loop

A contract analysis agent in late 2025 reviewed legal documents for compliance risks. The agent would extract clauses, assess each clause for risk, flag high-risk language, and generate a summary. In production, some requests took twelve times longer than expected. Logs showed the agent was revising its risk assessments repeatedly — flagging a clause as high-risk, reading more context, downgrading it to medium-risk, reading additional clauses, upgrading it back to high-risk, reconsidering again.

One request involved seventeen revisions across nine clauses before the agent settled on final risk ratings. The agent was not stuck in a loop — it was making forward progress — but it was excessively reconsidering decisions. Each revision added cost and latency. The final output was no more accurate than it would have been after two or three revisions.

The team implemented self-correction tracking: count how many times the agent revised a prior conclusion within a single trajectory. Healthy trajectories involved zero to two revisions. Trajectories with more than five revisions were flagged as unstable. Investigation revealed that the agent's risk-scoring prompt was ambiguous, giving the agent insufficient guidance on when a clause definitively qualified as high-risk. Clarifying the scoring criteria reduced median revisions from 3.2 to 1.1 per request.

## Tracking Decision Revisions

Self-correction occurs when the agent makes a decision, then later changes that decision within the same request. You detect this by logging each decision with a decision ID and tracking whether that decision is later overwritten or reversed.

A hiring agent in early 2026 screened resumes and assigned candidates to categories: "strong match," "possible match," or "not a match." Initially, the agent would categorize a candidate, continue reviewing the resume, and sometimes revise the category. Revision tracking logged every initial categorization and every subsequent change. The agent revised its categorization in eighteen percent of cases.

Revisions fell into two types: corrections and waffling. Corrections occurred when the agent initially misinterpreted data, then encountered clarifying information and fixed its mistake. Waffling occurred when the agent repeatedly moved candidates between categories without new information justifying the change — the agent was uncertain and kept reconsidering.

The team analyzed revision patterns. Corrections were valuable — the agent improved its accuracy by reconsidering early mistakes. Waffling was wasteful — the agent burned tokens reconsidering decisions without reason. They tuned the agent to allow one revision per candidate but flag any request involving more than two revisions as potentially unstable. Waffling dropped by sixty percent.

## Measuring Conclusion Confidence Variance

Agents often assign confidence scores to their decisions. High confidence means the agent is certain. Low confidence means the agent is uncertain. When an agent assigns high confidence to a decision, then later revises that decision, it indicates the initial confidence was miscalibrated.

You track confidence variance across revisions. If the agent assigns a confidence of 0.92 to a decision, then revises it to a different decision with confidence 0.88, the agent was overconfident the first time. Repeated overconfidence followed by revision suggests the agent's confidence scores are unreliable.

A fraud detection agent in mid-2025 scored transactions as risky or safe with associated confidence levels. The agent initially flagged a transaction as high-risk with 89 percent confidence, then revised it to low-risk with 84 percent confidence after retrieving additional data. Both confidence scores were high, but the conclusions contradicted each other. The agent was expressing certainty in incompatible assessments.

Confidence variance tracking flagged cases where the agent expressed high confidence in contradictory conclusions within the same trajectory. These cases indicated that the agent's confidence calibration was broken — it was not actually confident, it was just assigning high scores by default. The team retrained the confidence layer to better reflect uncertainty, which reduced false-confidence revisions by forty-two percent.

## Self-Correction Triggers and Justifications

Not all self-corrections are equal. Some are triggered by new evidence — the agent retrieves additional data that contradicts a prior conclusion. Some are triggered by re-evaluation — the agent reconsiders the same data and reaches a different conclusion. The latter is a red flag.

You log the reason for each self-correction. Did the agent retrieve new information, or did it simply reconsider existing information? If the agent revised a decision based on new data, that is responsive reasoning. If the agent revised a decision while looking at the same data it already processed, that is unstable reasoning.

A customer service agent in early 2026 handled product recommendations. The agent would recommend a product, then sometimes change the recommendation. Self-correction trigger logging showed that seventy percent of revisions occurred after the agent retrieved additional user preferences. Thirty percent occurred without new data — the agent reconsidered the same preferences and chose a different product.

The data-free revisions were investigated. They occurred when the agent's product ranking logic included randomness or when the agent used phrases like "on second thought" without clear justification. The team removed the randomness and required the agent to explicitly cite new information when revising a recommendation. Data-free revisions dropped to five percent.

## The Self-Doubt Spiral

An agent that doubts every decision will reconsider endlessly. This manifests as trajectories where the agent alternates between two or three conclusions without converging. Step five: choose option A. Step seven: reconsider, choose option B. Step nine: reconsider again, choose option A. The agent is stuck in a self-doubt spiral, wasting tokens on indecision.

You detect self-doubt spirals by tracking whether the agent returns to a previously rejected conclusion. If the agent considers option A, rejects it, then later reconsiders and accepts it, that is a signal of instability. Healthy reasoning involves evaluating options and committing. Unhealthy reasoning involves cycling through options repeatedly.

A financial planning agent in mid-2025 recommended investment allocations. In some cases, the agent would recommend allocation A, reconsider and recommend allocation B, then reconsider again and return to allocation A. The agent oscillated without making progress. Self-doubt detection flagged any trajectory where the agent returned to a previously dismissed option. Investigation showed that the agent's evaluation criteria were internally contradictory — one criterion favored allocation A, another favored allocation B, and the agent kept switching focus between criteria.

The team rewrote the evaluation logic to weight criteria consistently and break ties with a deterministic rule. Self-doubt spirals dropped from 4.7 percent of requests to 0.3 percent.

## Revision Impact on Cost and Latency

Every self-correction adds cost. The agent burns tokens reconsidering, re-evaluating, or replanning. Excessive self-correction directly inflates operational costs. You track cost-per-revision: how much does each self-correction add to the total cost of a request?

A research agent in early 2026 synthesized information from multiple sources. Self-correction added an average of 1,200 tokens per revision. Requests with zero revisions cost an average of 4,300 tokens. Requests with three revisions cost an average of 7,900 tokens — an 84 percent increase. The team prioritized reducing unnecessary revisions because each one represented nearly a doubling of marginal cost.

Latency impact is similar. Each revision adds time — the agent pauses to reconsider, makes additional model calls, or re-executes tool calls. Requests with zero revisions completed in median 2.1 seconds. Requests with four or more revisions took median 5.8 seconds. Self-correction frequency directly predicted response time.

## Healthy vs. Unhealthy Self-Correction Patterns

Not all self-correction is bad. Agents should revise decisions when they encounter contradictory evidence, discover mistakes, or learn new information that invalidates prior conclusions. The question is whether the self-correction improves accuracy or just burns cost.

You distinguish healthy from unhealthy self-correction by tracking whether revised decisions are more accurate than initial decisions. For tasks with ground truth, you compare initial decision accuracy to post-revision accuracy. If revisions improve accuracy by ten percentage points or more, they are healthy. If revisions improve accuracy by less than three points, they are marginal. If revisions do not improve accuracy at all, they are wasteful.

A document classification agent in mid-2025 categorized documents into topic areas. The agent self-corrected in fourteen percent of cases. Evaluation showed that revised classifications were correct 91 percent of the time, while initial classifications were correct 78 percent of the time before revision. The thirteen-point accuracy improvement justified the self-correction cost. The team did not reduce self-correction frequency; they let the agent revise when it judged necessary.

Contrast this with a pricing agent where revisions improved accuracy by only two percentage points — from 89 percent to 91 percent — while adding thirty percent to median cost. The marginal accuracy gain did not justify the cost. The team tuned the agent to reduce self-correction frequency, accepting the two-point accuracy trade-off.

## Flagging High-Revision Requests for Review

Requests with unusually high self-correction counts are candidates for human review. If the agent revised its conclusion seven times before completing, something went wrong — either the data was ambiguous, the task was unusually complex, or the agent's reasoning was unstable. You flag these requests and route them for quality assurance.

A medical coding agent in early 2026 assigned billing codes to patient encounters. The agent self-corrected in eight percent of cases on average. Requests with four or more self-corrections — one percent of all requests — were flagged for review by a human coder. Manual review showed that eighty percent of flagged cases involved ambiguous or incomplete documentation where the correct code was genuinely unclear. Twenty percent involved agent errors where the agent wavered between two codes and made the wrong final choice. Flagging high-revision requests caught a significant portion of mistakes before they reached billing.

## Self-Correction as a Training Signal

Agents that self-correct frequently are telling you something: the task is harder than the agent's initial strategy can handle, or the agent's initial reasoning is poorly calibrated. Self-correction frequency becomes a signal for where to focus training and prompt improvements.

You aggregate self-correction data by request type. If one category of requests triggers self-correction thirty percent of the time while others trigger it less than five percent of the time, that category is harder or more ambiguous. You prioritize improving the agent's performance on that category by adding demonstrations, refining instructions, or pre-training on similar examples.

A legal research agent in mid-2025 self-corrected on twelve percent of case law queries and thirty-two percent of statute interpretation queries. Statute interpretation was harder. The team created additional training examples focused on statute interpretation, clarified the interpretation framework in the agent's instructions, and added a validation step specific to statute queries. Self-correction on statute interpretation dropped to eighteen percent.

## Real-Time Self-Correction Limits

Some teams enforce self-correction limits: the agent is allowed to revise decisions up to N times per trajectory, where N is typically two or three. After hitting the limit, the agent must commit to its current conclusion or escalate. This prevents endless revision loops and forces the agent to reach closure.

A product recommendation agent in early 2026 was allowed two revisions per request. If the agent revised its recommendation twice and still wanted to reconsider a third time, the system blocked the third revision and forced the agent to finalize. In practice, less than one percent of requests hit the two-revision limit. The limit acted as a safeguard against runaway reconsideration without affecting normal operation.

## Confidence Thresholds and Revision Triggers

Some agents are configured to self-correct only when confidence is low. If the agent assigns a confidence score below a threshold — say, 0.7 — it is allowed to reconsider. If confidence is above the threshold, the agent must commit. This links self-correction to genuine uncertainty rather than indiscriminate second-guessing.

A credit risk agent in mid-2025 scored loan applications. The agent assigned a risk score and a confidence level. If confidence was below 0.75, the agent was prompted to gather additional data or reconsider its assessment. If confidence was above 0.75, the agent finalized immediately. This pattern ensured that self-correction occurred primarily when the agent was uncertain, not when it was confident but indecisive.

Confidence-based revision reduced overall self-correction frequency from nineteen percent to eleven percent of requests, while maintaining the same accuracy. The agent stopped reconsidering decisions it was already confident about and focused revisions on genuinely ambiguous cases.

## Longitudinal Self-Correction Trends

Self-correction frequency should stabilize over time as the agent improves. If self-correction frequency increases over weeks or months, it indicates model drift, degrading tool quality, or increasing task complexity. You track self-correction trends over time and investigate when trends move in the wrong direction.

A translation agent in early 2026 started with a self-correction rate of seven percent. Over three months, the rate climbed to fourteen percent. Investigation showed that the source content being translated had become more technical and ambiguous. The agent was encountering more cases where initial translations were uncertain and required revision. The team retrained the agent on technical content, which stabilized self-correction at nine percent.

Monitoring self-correction trends gives you early warning when agent performance is changing, even if final accuracy metrics remain stable. The agent might be working harder to maintain the same quality, which will eventually lead to cost or latency problems.

Self-correction is a double-edged signal. It shows the agent is capable of reconsidering and improving its reasoning. But it also reveals instability, indecision, or miscalibration. Monitoring self-correction frequency helps you distinguish healthy adaptability from wasteful uncertainty.

The next subchapter covers cost explosion detection — identifying when an agent starts burning budget far beyond expected levels and stopping it before financial damage compounds.

