# 5.7 — Source Quality Signals: Dead Links, Outdated Content, Authority Decay

The citation works. The page loads. The content exists. The information is wrong. The source was authoritative in 2024. By 2026, it is outdated, retracted, or superseded by newer research. Your RAG system does not know this. The document is in the index, the embedding is valid, the retrieval rank is high. The model cites the source confidently. The user acts on information that is no longer correct. This is source quality decay, and it is invisible to every metric that only checks whether links work.

Source quality is not binary. A source is not just "working" or "broken." A source can be technically accessible but factually compromised. The original study was retracted. The regulation was amended. The product was discontinued. The recommendation was revised. The standard was superseded. The content still exists at the original URL, still matches the cached version in your index, still retrieves with high similarity scores. The information is no longer true. Your RAG system confidently cites it as current.

## Retraction Tracking in Scientific Sources

In December 2025, a medical literature RAG system cited a 2023 oncology study in response to a query about treatment protocols. The study had been retracted in May 2025 due to data integrity issues. The retraction notice was published on the journal's website. The original paper was still accessible, marked with a retraction banner. The RAG system had indexed the paper in 2024, before the retraction. The system never re-crawled the paper to check for retraction notices. The cached version showed no retraction. The model cited the retracted study as if it were valid research. Physicians received treatment recommendations grounded in discredited data.

The team discovered the problem only when a physician cross-referenced the study and found the retraction notice. The team investigated and found that 0.3 percent of indexed papers had been retracted or corrected since indexing. That sounds small. In an index of 400,000 papers, it meant 1,200 papers were citing retracted or corrected research. Each of those papers was a potential source of misinformation.

The team implemented retraction monitoring. Once per week, the system queried major retraction databases — Retraction Watch, PubMed corrections, journal-specific feeds — and cross-referenced indexed papers against known retractions. Papers flagged as retracted were immediately removed from the retrieval index or tagged with retraction metadata that the model included in responses: "Note: this study has been retracted as of May 2025 due to data integrity concerns." Retraction-aware retrieval prevented the system from confidently citing discredited research as if it were current.

## Regulatory and Policy Supersession

Regulations change. Policies are amended. Guidelines are updated. The old version does not disappear. It remains accessible at its original URL, often in an archive section. Users querying for current regulations receive answers grounded in superseded versions. The answer is historically accurate but currently wrong.

A financial compliance RAG indexed banking regulations from government agencies. In March 2025, a key regulation was amended to reflect changes in reporting requirements. The agency published the updated regulation at a new URL and moved the old version to an archive with a note: "Superseded by version effective March 2025." The RAG system had indexed the old version. The system never detected the supersession because the old document was still accessible and its content had not changed — it was the same superseded regulation it had always been. The model cited the old version when answering questions about reporting requirements. Companies received guidance based on regulations that were no longer in effect. Compliance officers caught the errors. The firm faced scrutiny for providing outdated regulatory information.

Tracking supersession requires checking whether a document has been marked as superseded, archived, or replaced. For government documents, this often means parsing page headers, footers, or metadata for supersession notices. For standards organizations, this means checking for version numbers and effective dates. For internal policy documents, this means integrating with the document management system's versioning API.

The financial compliance team added supersession detection. Once per month, the system re-crawled every indexed regulatory document and parsed for supersession markers — phrases like "superseded by," "replaced by," "effective until," "archived version." Documents flagged as superseded were tagged with metadata indicating the supersession date and the URL of the current version. The retrieval system prioritized current versions and excluded superseded versions unless the user explicitly queried for historical regulations. Supersession-aware retrieval reduced regulatory misinformation to near zero.

## Authority Decay and Source Reputation

A source that was authoritative in 2024 might not be authoritative in 2026. The organization lost credibility. The author was discredited. The publication shifted focus or quality. The dataset was found to be flawed. Authority is not static. Monitoring authority requires tracking external signals about source reputation.

A business intelligence RAG indexed market research reports from dozens of firms. One firm, previously considered credible, published a series of reports in 2025 that were later found to contain fabricated data. The firm's reputation collapsed. Industry analysts stopped citing them. Other research firms issued corrections distancing themselves from the fabricated reports. The RAG system continued citing the firm's reports because they were in the index and had high retrieval scores for certain queries. Users received market analysis grounded in fabricated data. The firm's reports were still accessible. The content had not changed. The authority had evaporated.

Tracking authority decay requires external reputation signals. If a source is frequently cited by other high-authority sources, its authority is stable. If citations drop or if retractions and corrections reference the source negatively, its authority is decaying. If independent fact-checkers or domain experts flag the source as unreliable, it should be downranked or removed.

The business intelligence team integrated authority scoring into their retrieval pipeline. Each source was assigned an authority score based on citation frequency in trusted publications, absence of retraction notices, and inclusion in curated industry lists of credible firms. Authority scores were updated monthly. Sources with declining authority scores were downranked in retrieval. Sources flagged by multiple fact-checkers were removed entirely. This did not catch every fabricated report immediately, but it prevented the system from confidently citing sources whose reputation was actively collapsing.

## Content Staleness Without Document Changes

A document's content can become stale without the document itself changing. The document describes a product feature. The feature is deprecated. The document is never updated to reflect the deprecation. The document still exists, unchanged, at its original URL. The model cites it when users ask about the feature. Users receive information about a feature that no longer exists.

This is different from document staleness, covered earlier. Document staleness is when the source content changes and your cached version is outdated. Content staleness is when the source content does not change but the reality it describes does. The document is accurate as a historical record but inaccurate as a current description.

A SaaS documentation RAG indexed feature descriptions from a product knowledge base. In June 2025, the company deprecated a legacy API. The API documentation was not deleted. It remained in the knowledge base, marked as "legacy" in a sidebar note. The RAG system indexed the full text of the documentation, including the API reference. The sidebar note was not part of the embedded content. The retrieval system did not recognize that the API was deprecated. Users asked how to use the API. The model provided detailed instructions based on the documentation. Users followed those instructions and received errors because the API no longer existed. The documentation was technically correct — it accurately described how the API used to work. It was practically wrong because the API was no longer available.

Detecting content staleness requires understanding the context in which a document is still valid. Is this content current, historical, or deprecated? Deprecation markers — labels like "legacy," "archived," "deprecated," "no longer supported" — need to be extracted during indexing and included as metadata. Retrieval logic should filter or downrank deprecated content unless the query explicitly asks for historical information.

The SaaS team updated their indexing pipeline to extract deprecation markers from documentation. Documents tagged as legacy or deprecated were assigned a metadata flag. The retrieval system excluded deprecated content by default. Users querying for the legacy API received a response: "This API was deprecated in June 2025 and is no longer supported. Consider using the v2 API instead." The model stopped providing instructions for a non-functional API.

## Paywalls and Access Restrictions

A document is indexed from a source that was publicly accessible. The source later becomes paywalled or access-restricted. The citation points to content the user cannot reach without a subscription or login. The model's answer is correct and well-cited. The user cannot verify it because the citation leads to a paywall.

This is not a link rot problem — the content exists. This is an access problem. The user experience is similar to a dead link: the citation is useless. Tracking access restrictions requires periodic checks of whether indexed URLs are still publicly accessible. If a URL that was previously accessible now returns a 401 or 403 status, or if the page content includes paywall language, the source is restricted.

A news aggregation RAG indexed articles from news sites. Many articles were free at publication but moved behind paywalls after 30 days. The RAG system indexed articles when they were free. Users queried weeks later, after the articles had been paywalled. The model cited the articles. The users clicked and encountered paywall screens. The model's answer was correct, but the citations were inaccessible. Users could not verify the information without subscribing.

The team added access restriction detection. Once per week, the system re-crawled indexed URLs and checked for paywall indicators — HTTP 401/403 responses, redirects to login pages, or page content containing common paywall phrases. URLs flagged as paywalled were tagged with metadata. The system prompt instructed the model: "If a cited source is behind a paywall, inform the user and provide alternative free sources if available." Users still received paywalled citations when no free alternatives existed, but they were informed upfront rather than surprised after clicking.

## Tracking Source Diversity and Bias Concentration

A RAG system that retrieves from a narrow set of sources is vulnerable to source bias. If 80 percent of retrieved documents come from the same publisher or author, the system's perspective is skewed. Monitoring source diversity reveals whether retrieval is drawing from a balanced set of sources or over-relying on a few.

**Source concentration** is measured by calculating what percentage of retrieved documents come from the top 10 sources. If source concentration is high, you have a diversity problem. The retrieval system is biased toward certain publishers, authors, or domains, either because those sources dominate the index or because the ranking algorithm favors them.

A political news RAG retrieved articles from 200 news outlets. The team measured source concentration and found that 67 percent of retrieved articles came from five outlets. The index contained articles from all 200 outlets, but the ranking algorithm heavily favored large, frequently updated outlets. Smaller outlets were indexed but rarely retrieved. Users received a narrow perspective dominated by five sources. The team adjusted ranking to apply a source diversity penalty — if the top five retrieved articles all came from the same source, the 6th-ranked article from a different source was promoted. Source concentration dropped to 48 percent. Users received more balanced perspectives.

## The Source Quality Dashboard

Track retraction rates, supersession rates, authority decay signals, content staleness flags, access restriction rates, and source concentration. These metrics require periodic re-validation of indexed sources. You cannot check them once at indexing and assume they remain stable. Source quality degrades over time.

A pharmaceutical research RAG tracked source quality metrics monthly. Retraction rate held steady at 0.2 percent. Supersession rate was 1.1 percent per month, mostly from updated clinical guidelines. Authority decay was rare — fewer than five sources per year lost significant credibility. Content staleness was the biggest issue — 8 percent of indexed documents described products, protocols, or policies that had changed since the document was written. The team prioritized re-validation of high-traffic documents. Documents queried more than 100 times per month were re-validated weekly. Low-traffic documents were re-validated quarterly. This kept the most impactful content current while managing re-validation costs.

## The Cost of Ignoring Source Quality

Broken links are obvious. Source quality decay is silent. A user receives an answer grounded in a retracted study, a superseded regulation, or a discredited source. The answer looks authoritative. The citations look legitimate. The user acts on the information. The consequences are delayed. The patient follows treatment advice based on retracted research. The company implements compliance procedures based on superseded regulations. The investor makes decisions based on fabricated market data. The harm is real. The cause is invisible until someone cross-references the sources.

Monitoring source quality is not a nice-to-have. It is a requirement for any RAG system operating in domains where correctness matters. Healthcare, legal, financial, regulatory, scientific — these domains cannot tolerate source quality decay. The monitoring infrastructure is expensive. The alternative is professional negligence.

The next subchapter covers chunk quality degradation — how chunking strategies that worked when documents were structured and clean fail when documents are complex, poorly formatted, or multimedia-rich, and how to detect when your chunks stop making semantic sense.

