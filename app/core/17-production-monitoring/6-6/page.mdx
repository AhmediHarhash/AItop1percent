# 6.6 — Intermediate Reasoning Validation: Checking the Chain

The agent made twelve decisions. You validated the final output. You did not validate the eleven decisions that led to it. One of those eleven was wrong, and it poisoned every step that followed. By the time you checked the final answer, the damage was done.

Intermediate reasoning validation checks the agent's logic at each step of a trajectory, not just at the end. When the agent retrieves data, you validate that the data is relevant to the task. When the agent performs a calculation, you validate that the calculation is correct. When the agent makes a decision, you validate that the decision follows from the evidence. Catching errors early prevents cascading failures and reduces the cost of bad reasoning.

## The Cascading Interpretation Failure

A benefits enrollment agent in late 2025 helped employees select health insurance plans. The agent queried plan details, compared costs, assessed coverage, and recommended a plan. In one case, the agent recommended a high-deductible plan to an employee who had explicitly stated they could not afford high out-of-pocket costs.

Review of the trajectory showed the failure occurred at step two. The agent queried the plan database and retrieved details for five plans. One field in the data was labeled "annual maximum" and contained the value 8000. The agent interpreted this as the maximum the employee would pay per year — the out-of-pocket maximum. In reality, it was the maximum the insurance company would pay per year — a coverage cap. Based on this misinterpretation, the agent concluded the high-deductible plan had a low out-of-pocket maximum and recommended it.

Steps three through nine were all logically sound, given the incorrect interpretation from step two. The agent compared plans accurately, applied the user's preferences correctly, and synthesized a clear recommendation. But the recommendation was built on a misunderstanding. If intermediate reasoning validation had checked the agent's interpretation of "annual maximum" at step two, the error would have been caught before the agent invested seven more steps in flawed reasoning.

## Step-by-Step Data Interpretation Checks

Agents retrieve data and interpret it. Retrieval is easy to validate — did the API return a 200 status? Interpretation is harder — did the agent understand what the data means? Intermediate reasoning validation checks interpretation after every data retrieval step.

You implement interpretation checks using one of three methods: rule-based validators, schema-based validators, or model-based validators. Rule-based validators check for common misinterpretations — if the agent retrieves a field called "limit" from a financial API, the validator checks whether the agent interpreted it as an upper limit or a lower limit. Schema-based validators check whether the agent's interpretation aligns with the data schema — if the schema says a field contains currency in cents, the validator checks whether the agent treated it as cents or dollars. Model-based validators use a separate model call to ask: "Given this data and this interpretation, is the interpretation reasonable?"

A tax preparation agent in early 2026 retrieved tax form data from a database. One field was labeled "withholding" and contained negative values for some employees. The agent interpreted negative withholding as "the employee owes this amount to the IRS." The correct interpretation: negative withholding means the employee overpaid and is due a refund. The agent's interpretation was backwards.

Schema-based validation caught this. The database schema documented that negative values in the withholding field represent overpayments. The validator checked the agent's interpretation against the schema, detected the reversal, and flagged the step. The agent's plan was paused, the interpretation was corrected, and the trajectory continued correctly. Without intermediate validation, the agent would have told employees they owed money when they were owed refunds.

## Validating Calculations and Transformations

When agents perform calculations — adding, multiplying, converting units, applying formulas — those calculations can fail silently. The agent might use the wrong operands, apply the wrong formula, or introduce floating-point errors that propagate through subsequent steps. Validating calculations as they happen prevents numeric errors from compounding.

A mortgage agent in mid-2025 calculated monthly payments based on loan amount, interest rate, and term. The formula is standard. In production, the agent occasionally produced payments that were obviously wrong — twenty-dollar monthly payments on a 400,000-dollar loan. Investigation revealed floating-point precision errors: the agent divided the annual rate by 12 to get the monthly rate, then divided again by 100 to convert percentage to decimal, but the order of operations sometimes produced rounding errors that cascaded through the exponentiation step in the formula.

The team added a calculation validator: after the agent computed a monthly payment, the validator checked whether the payment was within a reasonable range given the loan parameters. A 400,000-dollar loan at four percent interest over thirty years should produce a payment between 1,700 and 2,000 dollars. A result of 20 dollars is impossible. The validator flagged it, logged the intermediate values, and the team identified the floating-point issue.

Calculation validation does not require checking every arithmetic operation. You validate results that feed into subsequent decisions. If the agent calculates an intermediate value but does not use it for anything, validation is optional. If the agent calculates a value and bases its next three steps on that value, validation is mandatory.

## Logical Consistency Checks Between Steps

Each step in an agent's trajectory should be logically consistent with prior steps. If step three retrieves a user's account balance and finds 120 dollars, step five should not attempt to process a withdrawal of 500 dollars without first handling insufficient funds. Logical consistency checks validate that the agent is not contradicting itself.

A customer service agent in early 2026 handled refund requests. The agent's trajectory: verify order status, check refund eligibility, calculate refund amount, process refund. At step two, the system returned "order not eligible for refund — outside return window." At step four, the agent processed a full refund. Steps two and four contradicted each other.

Logical consistency validation flagged this. The validator maintained a set of constraints derived from prior steps. When step two returned "not eligible," the validator added a constraint: no refund should be processed. When step four attempted to process a refund, the validator checked the constraints, found a violation, and halted execution. The agent's plan was reviewed, the contradiction identified, and the agent's logic was corrected to terminate after step two when eligibility checks fail.

Consistency checks are particularly important when agents interact with stateful systems. If the agent learns that a resource is locked, it should not attempt to modify that resource. If the agent learns that a user is not authenticated, it should not proceed with actions requiring authentication. These checks prevent agents from executing operations that are logically impossible given prior context.

## Real-Time Reasoning Scorecards

Some teams track a reasoning quality score that updates after each step. The score reflects how well the agent's trajectory aligns with correct reasoning patterns. High-quality steps increase the score. Low-quality steps decrease it. If the score drops below a threshold, the agent is paused for review or terminated.

A fraud detection agent in mid-2025 investigated suspicious transactions. Each step was scored: retrieving relevant data increased the score, retrieving irrelevant data decreased it, making a decision supported by evidence increased the score, making an unsupported decision decreased it. The agent started each request with a baseline score of 50. Successful data retrieval added five points. Redundant queries subtracted three points. Logical inferences added ten points. Contradictions subtracted fifteen points.

If the score dropped below 30, the agent was terminated and the case escalated to a human investigator. In practice, this happened in 1.8 percent of cases — situations where the agent retrieved contradictory information, made unsupported inferences, or repeated queries without making progress. The reasoning scorecard caught flawed trajectories before they reached completion.

## Evidence-Claim Matching

Agents make claims in their final outputs: "Your shipment will arrive Tuesday," "This transaction is fraudulent," "You are eligible for a refund." Each claim should be supported by evidence the agent retrieved during its trajectory. Evidence-claim matching validates that the agent's assertions are backed by data, not confabulated.

You log each piece of evidence the agent retrieves and tag it with what it supports. When the agent produces a final output, you parse the claims in that output and check whether each claim has corresponding evidence. Claims without evidence are flagged as unsupported.

A hiring agent in early 2026 screened resumes and produced summaries with statements like "Candidate has five years of Python experience" or "Candidate meets education requirements." Evidence-claim matching checked: did the agent retrieve a resume section mentioning five years of Python experience? Did it retrieve education details that meet the requirements? If the agent made a claim that did not appear in the retrieved data, the claim was flagged.

In one case, the agent stated "Candidate has management experience" despite the resume containing no management roles. The agent had inferred management experience from the candidate's senior title, which was a reasonable inference but not a fact stated in the resume. Evidence-claim matching flagged it, and the output was revised to say "Candidate has senior-level experience, which may include management responsibilities." The claim became more accurate and defensible.

## Validating Tool Output Parsing

Agents retrieve data from tools and parse it into structured formats. Parsing can fail silently — the agent extracts the wrong field, misinterprets the format, or hallucinates data that is not present. Validating parsing ensures the agent is working with accurate representations of tool outputs.

A legal document agent in late 2025 extracted clauses from contracts. The agent would call a document parsing tool that returned JSON with fields for clause type, clause text, and clause metadata. In some cases, the agent confused clause text with metadata and proceeded to analyze metadata as if it were contract language. The error was subtle — metadata often contained clause references and legal terminology — but it resulted in incorrect legal assessments.

Parsing validation checked whether the agent's extracted data matched the structure returned by the tool. If the tool returned a field labeled "metadata" and the agent treated it as "clause text," the validator flagged a mismatch. This required schema awareness — the validator knew what fields the tool returned and checked that the agent used each field appropriately.

## Probabilistic Consistency Scoring

Some intermediate reasoning checks are not binary pass-fail. They assess whether the agent's reasoning is probable given the data. Probabilistic consistency scoring uses heuristics or a secondary model to estimate the likelihood that the agent's reasoning is sound.

A medical triage agent in early 2026 assessed patient symptoms and recommended urgency levels. After the agent retrieved symptom data and before it made a recommendation, a consistency scorer evaluated: given these symptoms, how likely is this recommendation? If the patient reported chest pain and shortness of breath, and the agent recommended "low urgency, follow up in a week," the consistency score would be very low. The recommendation contradicted medical protocols.

Probabilistic scoring flagged recommendations with consistency scores below 0.4 on a scale of 0 to 1. These cases were escalated to a nurse reviewer before being sent to the patient. In practice, 2.1 percent of recommendations were flagged, and ninety percent of flagged cases were confirmed as incorrect or inappropriate. The consistency scorer caught reasoning errors that rule-based validators missed because they involved domain-specific knowledge encoded in the scoring model.

## Intermediate Validation Latency Costs

Every validation check adds latency. A check that calls a separate model adds 200 to 600 milliseconds. A rule-based check adds 5 to 50 milliseconds. You must balance thoroughness with speed. Validating every step exhaustively makes the agent too slow. Validating nothing makes the agent unreliable.

The common approach: classify steps as low-risk, medium-risk, or high-risk. Low-risk steps — retrieving static data, formatting output — receive lightweight or no validation. High-risk steps — making decisions, performing calculations that feed into recommendations, interpreting ambiguous data — receive thorough validation, including model-based checks if necessary.

A financial advisor agent in mid-2025 made investment recommendations. The team classified steps: retrieving market data was low-risk, calculating portfolio returns was high-risk, and generating explanatory text was low-risk. High-risk steps received full validation including consistency checks and calculation verification. Low-risk steps received only schema validation. This kept median latency at 2.8 seconds while catching ninety percent of reasoning errors.

## Human-in-the-Loop Intermediate Review

For high-stakes agents, some teams insert human review at specific steps. The agent executes part of the trajectory, pauses, presents its reasoning to a human, and waits for approval before continuing. This is slower than full automation but dramatically reduces the risk of consequential errors.

A loan underwriting agent in early 2026 paused after retrieving applicant data and generating a preliminary decision. A human underwriter reviewed the data, the agent's interpretation, and the preliminary decision. If the underwriter approved, the agent continued. If not, the underwriter corrected the agent's interpretation or overrode the decision. Median time to decision was four minutes with human review compared to ninety seconds without, but error rate dropped from 3.8 percent to 0.2 percent. For high-value loans, the accuracy improvement justified the latency cost.

## Building Validation Rules from Incident Data

Intermediate validation rules improve over time as you analyze incidents. When an agent produces a bad output, you trace the trajectory to find where the reasoning first went wrong. That failure point becomes a new validation rule. The system learns from mistakes.

A customer support agent in late 2025 occasionally escalated requests that could have been resolved automatically. Incident analysis showed that the agent was interpreting certain phrasing in customer messages as indicating complexity when the actual request was simple. For example, "I need urgent help with my bill" was escalated as a complex billing issue when it was usually just a request to view the bill.

The team added a validation rule: after the agent classifies a request's complexity, validate the classification by checking whether the request mentions specific complex keywords — "dispute," "error," "unauthorized," "fraud." If the classification is "complex" but none of those keywords appear, flag it for re-evaluation. This rule reduced unnecessary escalations by eighteen percent.

Intermediate reasoning validation is expensive — it adds latency, requires engineering effort, and increases system complexity. But for agents that make consequential decisions, it is non-negotiable. The alternative is waiting until the final output to discover that step three was wrong, by which time the agent has burned cost executing a flawed plan and produced an incorrect answer that might reach a user.

The next subchapter covers external API dependency tracking — monitoring the third-party services your agent relies on and detecting when those dependencies fail.

