# 11.4 — Cost Anomaly Detection: When Spend Spikes Unexpectedly

Manual budget thresholds catch obvious problems—spend that crosses a fixed dollar amount or exceeds a known safe limit. But they miss subtler anomalies: a 40 percent increase during a normally quiet period, a steady upward drift that never crosses the threshold, or a sharp drop that indicates silent failures. Cost anomaly detection uses statistical models to identify unusual patterns automatically, without requiring you to set thresholds for every possible scenario.

## Why Thresholds Are Not Enough

A threshold-based alert fires when spend exceeds a fixed value. If your system normally costs $60/hour, you set an alert at $120/hour. This works until your traffic pattern changes. Monday through Friday, $120/hour might be normal. Saturday at 3am, $120/hour is a catastrophic anomaly. A fixed threshold cannot adapt to time-of-day, day-of-week, or seasonal patterns.

In October 2025, a document summarization service saw spend increase from $50/hour to $75/hour on a Sunday morning. The threshold-based alert was set at $150/hour, so it did not fire. The team discovered the problem three days later when they reviewed weekly spend trends and noticed the sustained increase. By then, they had spent an extra $1,800 unnecessarily.

The root cause was a prompt change deployed Friday evening that increased output token counts by 35 percent. The change was tested in staging, which had no cost monitoring. It shipped to production over the weekend when traffic was low. Because weekend traffic is 60 percent lower than weekday traffic, absolute spend stayed well below the alert threshold. The anomaly was invisible to threshold-based alerts but obvious to anomaly detection: spend was 50 percent higher than typical Sunday levels.

Anomaly detection compares current spend to historical baselines that account for time patterns, traffic patterns, and seasonal variation. It answers the question "is this normal for right now?" rather than "is this above a fixed limit?"

## Statistical Anomaly Detection for Cost

Statistical anomaly detection models cost as a time series with expected behavior and deviations. The simplest approach is to calculate the mean and standard deviation of spend for each hour of the week over the last four weeks, then alert when current spend exceeds the mean by more than two or three standard deviations.

A customer support platform tracks hourly spend for 672 hours: 24 hours per day times 7 days times 4 weeks. For each hour of the week—Monday 9am, Monday 10am, Tuesday 9am, and so on—it calculates the mean and standard deviation of spend over the last four weeks. On Monday at 10am, if spend exceeds the four-week mean for Monday 10am by more than two standard deviations, the alert fires.

This approach adapts to day-of-week and time-of-day patterns. Weekday afternoons have higher baselines than weekend mornings. Traffic spikes during business hours do not trigger false alarms. Anomalies during low-traffic periods are detected even when absolute spend is low.

Standard deviation-based alerting assumes spend follows a roughly normal distribution. For most systems, this is close enough. If your spend distribution is highly skewed—most hours cost $10, but occasional hours cost $500—use percentile-based thresholds instead. Alert when spend exceeds the 95th or 99th percentile of historical spend for that hour of the week.

## Sliding Baseline Windows

Baselines must adapt as your system grows. A baseline calculated in September becomes stale by December if your traffic doubled. Sliding baseline windows recalculate the expected range every day or every week, so the baseline stays current.

A legal research tool uses a 30-day sliding window. Every day, it recalculates the expected spend for each hour of the week based on the last 30 days of data. If traffic grows 10 percent per month, the baseline grows with it. Anomaly detection catches deviations from the new normal, not deviations from three months ago.

Sliding windows introduce a risk: if a slow-growing anomaly persists long enough, it becomes part of the baseline. A bug that increases spend by 5 percent per week might go unnoticed because the baseline adjusts to it. To mitigate this, some teams use dual baselines: a short-term baseline (last seven days) and a long-term baseline (last 90 days). If short-term spend is normal but significantly higher than long-term spend, the alert flags a potential drift issue.

## Machine Learning-Based Anomaly Detection

Statistical methods work well for simple patterns, but machine learning models handle more complex seasonality, trends, and multi-dimensional patterns. Several observability platforms—Datadog, Grafana, and others—offer built-in ML-based anomaly detection.

A B2B SaaS platform uses Datadog's anomaly detection on its cost metrics. Datadog learns the normal cost pattern for each hour of the week, adjusts for trends and seasonal variation, and alerts when actual spend deviates from the forecast by more than a configurable threshold. The model adapts automatically as traffic grows or contracts.

The platform discovered three anomalies in December 2025 that threshold-based alerts had missed. First, spend on Friday afternoons was gradually increasing even though traffic volume was flat. Investigation revealed that users were asking longer, more complex questions over time. The team optimized prompts to reduce unnecessary verbosity.

Second, spend on one customer dropped by 60 percent over three days. The customer had not churned, but their API calls had shifted to a different endpoint that used a cheaper model. This was intentional, but the team had not been notified. The anomaly detection surfaced it, and the team confirmed with the customer that the change was expected.

Third, spend spiked by 200 percent on a Tuesday morning for 15 minutes. The team investigated and found a transient bug that caused retries for a specific request type. The bug resolved itself when the underlying data changed, but the anomaly detection caught it and prompted a fix to prevent recurrence.

ML-based anomaly detection requires less manual tuning than threshold-based alerts. You do not need to set thresholds for every metric. You configure sensitivity—how aggressive the model should be in flagging deviations—and the model handles the rest. The tradeoff is less transparency: you cannot easily explain why the model flagged a specific data point as anomalous.

## Per-Dimension Anomaly Detection

Aggregate cost anomaly detection catches system-wide problems, but per-dimension detection catches localized issues. A single user, feature, model, or endpoint might show anomalous cost even when total spend looks normal.

A fintech assistant runs anomaly detection on cost per user, cost per feature, and cost per model. In November 2025, cost for one feature spiked by 300 percent while total system cost increased by only 8 percent. The feature represented a small fraction of total traffic, so the aggregate anomaly detector did not fire. The per-feature detector caught it immediately.

The spike was caused by a change in the feature's prompt that added a multi-step reasoning chain. The reasoning improved quality but tripled token usage. The team reviewed the cost-quality tradeoff, decided the quality improvement was not worth the cost, and reverted to a simpler prompt with targeted improvements.

Per-dimension detection requires tracking cost by every dimension you care about—user ID, endpoint, model, feature, customer, geographic region—and running anomaly detection on each. This multiplies the number of metrics you monitor but catches problems that aggregate monitoring misses.

## Anomaly Detection for Cost-per-Unit Metrics

Absolute cost can increase because traffic increased or because cost per request increased. Anomaly detection on cost-per-unit metrics—cost per request, cost per user, cost per session—isolates efficiency changes from volume changes.

A document processing pipeline tracks cost per document. Normally, documents cost $0.06 to $0.10 depending on length and complexity. The team runs anomaly detection on cost per document, not total cost. When cost per document increases by more than 40 percent compared to the historical baseline for that document type, the alert fires.

In January 2026, the alert fired for financial statements, which normally cost $0.08 per document. Cost had increased to $0.13 per document. Total spend was within budget because financial statement volume was low that week, but the per-document cost anomaly revealed a problem: a model upgrade had increased pricing, and the team had not updated their cost estimates. They switched to a different model with similar quality and lower cost.

Cost-per-unit anomaly detection is especially valuable when traffic is unpredictable. A system that handles 1,000 requests one day and 10,000 the next cannot rely on absolute cost thresholds. Cost per request anomaly detection catches efficiency problems regardless of traffic volume.

## Detecting Gradual Cost Drift

Sharp spikes are easy to detect. Gradual drift is harder. If cost per request increases by 2 percent per week, it takes months before the cumulative impact is obvious. By the time you notice, you have overspent significantly.

Drift detection compares short-term trends to long-term baselines. A healthcare assistant calculates cost per interaction over the last seven days and compares it to the 90-day average. If the seven-day average is more than 15 percent higher than the 90-day average, the alert fires even if no single day looks anomalous.

In December 2025, the alert fired when seven-day average cost per interaction reached $0.19, compared to a 90-day average of $0.16. The team investigated and discovered that users were asking more follow-up questions per interaction, increasing session length. This was a positive engagement signal, but it came with a cost. The team decided to accept the higher cost but added session-level budgets to prevent runaway interactions from dominating spend.

Drift detection requires longer historical windows and slower response times than spike detection. It is not about catching problems in real time. It is about catching slow-growing issues before they become structural cost increases you cannot reverse without major changes.

## Anomaly Detection on Cost Ratios and Efficiency Metrics

Cost anomalies often show up in ratios: cost per quality point, cost per successful request, cost per engaged user. A system might have stable absolute costs but declining efficiency if quality drops or error rates rise.

A search platform tracks cost per successful query. When the API call succeeds and returns a high-quality result, the query is counted as successful. When the API times out, returns low-quality results, or fails, the query is counted as unsuccessful but still costs money. The team runs anomaly detection on cost per successful query.

In October 2025, cost per successful query increased by 30 percent over two days even though total cost stayed flat. The team investigated and found the model was timing out more frequently, causing more retries and lower success rates. The timeout issue was a model provider problem, not a bug in their system. They temporarily switched to a backup model until the provider resolved the issue.

Ratio-based anomaly detection catches problems that absolute cost metrics miss. A system that spends the same amount but delivers less value is underperforming. The cost itself is not the anomaly—the cost-to-value ratio is.

## Combining Threshold-Based and Anomaly-Based Alerts

Threshold-based alerts and anomaly-based alerts are complementary. Thresholds catch absolute overspend. Anomalies catch unusual patterns. Most teams need both.

A document assistant uses threshold alerts to enforce hard budget limits: if hourly spend exceeds $200, page the on-call engineer. It uses anomaly detection to catch unusual patterns: if spend is 2.5x higher than expected for this hour of the week, send a Slack notification. The threshold alert is a safety net. The anomaly alert is a warning system.

In practice, anomaly detection fires more often but with lower urgency. Anomalies might be false positives—a legitimate spike, a planned change, or random variation. Thresholds fire less often but with higher urgency. If spend crosses the hard limit, something is definitely wrong.

The two systems have different tuning requirements. Thresholds must be high enough to avoid false positives during normal peaks. Anomalies must be sensitive enough to catch meaningful deviations but not so sensitive that random noise triggers alerts. Most teams tune thresholds to catch the top 1 percent worst-case scenarios and tune anomalies to catch the top 5 to 10 percent unusual patterns.

## Alert Fatigue and Tuning Sensitivity

Anomaly detection generates more alerts than threshold-based alerting, which increases the risk of alert fatigue. If the detector fires ten times a day and nine are false positives, engineers stop responding.

The solution is tuning sensitivity and adding context. Sensitivity controls how aggressive the detector is. Higher sensitivity catches more anomalies but generates more false positives. Lower sensitivity misses subtle problems but reduces noise.

A customer success platform started with high sensitivity—any deviation more than 1.5 standard deviations triggered an alert. The alert fired 40 times in the first week. Most were false positives: legitimate traffic spikes, planned batch jobs, or random variation. The team raised sensitivity to 2.5 standard deviations, which reduced alerts to five per week, almost all of which were real issues.

Context reduces alert fatigue by helping engineers quickly triage. An alert that says "cost anomaly detected" requires investigation. An alert that says "cost is 3x higher than expected for Monday 10am, User X responsible for 70 percent of the spike, endpoint Y saw 500 requests in the last 10 minutes" can be triaged in seconds.

## Anomaly Detection for Cost Declines

Most anomaly detection focuses on cost increases, but cost declines can also indicate problems. A sudden drop in spend might mean the system is failing silently, rejecting requests, or timing out before completing work.

A legal research tool tracks cost per hour and alerts on both increases and decreases. In November 2025, cost per hour dropped by 50 percent over three hours. The team investigated and found the retrieval step was failing silently. The system was returning responses based on the fallback logic without calling the LLM, which saved money but delivered low-quality answers. Users had not yet reported the issue because the responses were plausible but unhelpful.

Anomaly detection on cost declines caught the problem before users noticed at scale. The team fixed the retrieval bug, cost returned to baseline, and quality returned to normal. Without cost monitoring, the failure might have persisted for days.

## The Infrastructure for Cost Anomaly Detection

Anomaly detection requires historical data, statistical or ML models, and integration with alerting systems. Most teams start simple and add sophistication over time.

A document service starts with a Python script that runs hourly. The script queries the last 30 days of hourly cost data from the metrics database, calculates mean and standard deviation for each hour of the week, compares current cost to the expected range, and sends a Slack message if an anomaly is detected. The script takes 30 lines of code and runs on a cron job.

As the system scales, the team migrates to Datadog's anomaly detection feature, which handles the calculation, alerting, and visualization. They configure sensitivity, define alert routing, and let Datadog manage the rest. The migration takes a day and reduces operational overhead.

Anomaly detection does not need to be sophisticated to be useful. A simple statistical model catches 80 percent of real anomalies. Advanced ML models catch the remaining 20 percent, but you do not need them on day one. Start with something simple. Add complexity when the simple version stops catching problems.

The next subchapter covers token waste analysis—finding and fixing inefficient prompts that burn tokens without improving quality or user experience.

