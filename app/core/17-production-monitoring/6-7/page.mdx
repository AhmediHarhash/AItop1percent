# 6.7 — External API Dependency Tracking: When Tools Fail

Your agent does not fail. Your agent's dependencies fail, and your agent inherits their failures. The agent itself is reliable, correctly implemented, and well-tested. But it calls twelve external APIs, and when any one of those APIs degrades, your agent's success rate degrades with it.

External API dependency tracking monitors every service your agent depends on — third-party data providers, internal microservices, databases, search indexes, authentication systems. You track uptime, latency, error rates, and response quality for each dependency. When a dependency fails, you attribute that failure correctly so you know whether to fix your agent or escalate to the team that owns the failing service.

## The Invisible Third-Party Degradation

A real estate agent in late 2025 helped users search for properties. The agent queried a third-party property listing API, a mortgage rate API, a school rating API, and a neighborhood data API. The agent itself ran on stable infrastructure with 99.7 percent uptime. But one week, user complaints spiked — the agent was returning incomplete property information.

Investigation revealed that the school rating API had degraded. It was still returning 200 status codes, so the agent logged the calls as successful. But the API was returning cached data from three months prior instead of current data. Users were seeing outdated school ratings, making decisions based on stale information, and blaming the agent.

The degradation was invisible to standard monitoring because the API did not return errors — it returned wrong data. The team implemented response validation: after calling the school rating API, check the timestamp on the returned data. If the data is more than fourteen days old, flag it. This caught the staleness issue within hours of the next occurrence. The agent began warning users when school data was outdated, and the team escalated the issue to the API provider.

## Tracking Per-Dependency Success Rates

You log every call to every external dependency with a success-or-failure status. Success means the call returned usable data within an acceptable timeframe. Failure means the call returned an error, timed out, or returned data that failed validation. You calculate success rates per dependency and per time window.

Dependencies have different reliability profiles. An internal database might maintain 99.9 percent uptime. A third-party geocoding API might average 98.2 percent uptime. A real-time stock price API might fluctuate between 96 and 99 percent depending on market hours. You establish baseline success rates for each dependency and alert when any dependency drops more than two percentage points below its baseline.

A logistics agent in early 2026 depended on six external APIs: shipment tracking, warehouse inventory, delivery routing, customer address validation, carrier rate calculation, and weather forecasting. The team tracked success rates for all six. Shipment tracking stayed above 99 percent. Weather forecasting fluctuated between 94 and 97 percent, which was within its normal range. One week, carrier rate calculation dropped from 97 percent to 89 percent. The team escalated to the carrier API provider and discovered they were experiencing database replication lag, which the provider fixed within four hours.

Without per-dependency tracking, the team would have seen overall agent success drop from 96 percent to 92 percent and spent hours debugging the agent's logic before realizing the problem was external.

## Dependency Latency Attribution

External dependencies contribute to overall agent latency. A fast agent that depends on slow APIs will be slow. You track how much time each dependency consumes per request. Latency attribution shows which dependencies are bottlenecks and whether those bottlenecks are consistent or intermittent.

A customer service agent in mid-2025 called an account lookup API, an order history API, and a returns eligibility API. Median agent latency was 2.1 seconds. Latency attribution showed: account lookup averaged 0.4 seconds, order history averaged 1.2 seconds, and returns eligibility averaged 0.3 seconds. Order history consumed sixty percent of total latency. When the team investigated, they found the order history API was querying an unindexed database table. Adding an index reduced order history latency to 0.5 seconds and overall agent latency to 1.4 seconds.

Dependency latency percentiles matter more than averages. An API with a median latency of 200 milliseconds but a p95 of 3.1 seconds is causing severe delays for five percent of requests. You track p50, p90, p95, and p99 latency for each dependency. When any dependency's p95 exceeds twice its p50, investigate. That is a signal of intermittent slowness or load-related degradation.

## Circuit Breaker Patterns for Failing Dependencies

When a dependency starts failing consistently, continuing to call it wastes time and increases latency. Circuit breaker patterns detect failing dependencies and temporarily stop calling them, allowing the agent to proceed with degraded functionality rather than waiting for doomed API calls to time out.

You implement a circuit breaker per dependency. The breaker tracks recent success and failure rates. If failures exceed a threshold — say, five consecutive failures or a failure rate above fifty percent over the last thirty seconds — the breaker opens. While open, the agent skips calls to that dependency and either uses cached data, proceeds without that data, or escalates the request.

A travel booking agent in early 2026 depended on a hotel availability API. The API experienced an outage one afternoon, returning 503 errors for every request. Initially, the agent continued calling the API, waiting two seconds per call before timing out, and eventually escalating every booking request after burning six seconds on retries. When the circuit breaker was implemented, the agent detected the API failure after three consecutive errors, opened the breaker, and began immediately escalating requests without wasting time on additional failed API calls. User experience improved — instead of waiting six seconds for a failure, users received an immediate "hotel search temporarily unavailable" message.

Circuit breakers reduce wasted latency during outages. They do not fix the root cause, but they prevent your agent from amplifying the dependency's failure by repeatedly attempting calls that will not succeed.

## Fallback Data Sources

Some dependencies have fallback alternatives. If a primary API fails, the agent can call a secondary source. Fallback strategies reduce dependence on any single external service. You configure fallbacks per dependency and track how often fallbacks are used.

A pricing agent in mid-2025 retrieved product prices from a primary pricing API and a secondary database cache. Under normal conditions, the agent called the API, which returned real-time prices. If the API failed or returned stale data, the agent queried the cache, which contained prices updated every fifteen minutes. The cache was less current but better than returning no price at all.

Fallback usage rate became a key metric. In normal operation, the agent used the fallback less than one percent of the time. When fallback usage jumped to twelve percent, it indicated that the primary API was unstable. The team monitored fallback rate as a leading indicator of primary dependency health. High fallback usage triggered alerts, prompting proactive investigation before the primary API failed completely.

## Dependency-Specific Error Classification

Not all API errors are the same. A 404 error means the resource does not exist — the agent might proceed with that knowledge. A 503 error means the service is overloaded — the agent should retry. A 401 error means authentication failed — retrying will not help. You classify errors per dependency and handle them accordingly.

You build error taxonomies for each external dependency: transient errors that warrant retries, permanent errors that require different logic, and ambiguous errors that need escalation. Transient errors include 503, 429, and timeout errors. Permanent errors include 404, 400, and 403 where the request was malformed. Ambiguous errors include 500, where the service failed but the cause is unclear.

A payment processing agent in early 2026 called a payment gateway API. The API returned several error types: insufficient funds (a permanent error indicating the transaction cannot succeed), rate limit exceeded (a transient error indicating retry after delay), and gateway timeout (a transient error indicating network issues). The agent handled each differently. Insufficient funds triggered an immediate user notification. Rate limit errors triggered a two-second delay and retry. Gateway timeouts triggered three retries with exponential backoff, followed by escalation if all retries failed.

Error classification reduced unnecessary retries — the agent stopped retrying errors that would never succeed — and improved retry success rates by targeting retries at errors that were likely to resolve.

## Health Check Pinging for Critical Dependencies

For critical dependencies, some teams implement active health checks: the system periodically pings the dependency even when the agent is not actively using it. If the health check fails, the system preemptively opens circuit breakers or switches to fallback strategies before user requests encounter the failure.

A fraud detection agent in mid-2025 depended on a real-time transaction scoring API. Every ten seconds, a background process sent a health check request to the API. If the health check failed twice in a row, the system flagged the API as degraded and began routing requests to a fallback scoring model. This allowed the agent to switch strategies before user requests hit the failing API, reducing user-visible failures.

Health check pinging adds overhead — you are making requests that do not serve user traffic. But for high-throughput agents where dependency failures cause cascading user-facing errors, the cost of health checks is negligible compared to the cost of failed user requests.

## Dependency SLA Tracking and Violation Alerts

When you depend on external services, you often have service level agreements that specify uptime, latency, and error rate guarantees. Dependency tracking validates whether those SLAs are being met and alerts when they are violated.

You configure SLA thresholds per dependency: uptime above 99.5 percent, p95 latency below 500 milliseconds, error rate below 0.5 percent. When any dependency violates its SLA, you generate an alert. For third-party APIs, you escalate to the provider. For internal services, you escalate to the owning team.

A financial data agent in early 2026 used a stock price API with an SLA of 99.7 percent uptime and p95 latency below 300 milliseconds. SLA tracking monitored the API continuously. One week, uptime dropped to 98.9 percent — still high, but below the SLA. The team escalated to the provider, who investigated and found a database scaling issue. The provider resolved it before uptime dropped further. Proactive SLA monitoring caught degradation early, preventing a more severe outage.

## Logging Dependency Response Quality

Some dependency failures are not errors — they are responses that technically succeed but are unusable. An API that returns an empty result set when it should return data, or returns data with missing fields, or returns malformed JSON that barely parses. These are silent failures. The API did not return an error code, but the agent cannot use the response.

You log response quality metrics for each dependency: completeness (do responses include all expected fields?), validity (do responses conform to the schema?), and freshness (is the data recent?). When quality metrics degrade, you flag it even if error rates remain low.

A product catalog agent in late 2025 queried a product database API. The API returned product details including name, price, availability, and image URL. One week, users reported that product images were not loading. The API was not returning errors — it was returning image URLs that pointed to broken or expired links. The agent logged successful API calls but users saw broken images.

Response quality validation checked whether image URLs were reachable. When fifty percent of returned image URLs failed to load, the system flagged the API. Investigation revealed that the product database team had migrated image hosting and failed to update URLs in the database. The quality validation detected a problem that error rate monitoring missed.

## Attributing Agent Failures to Dependency Failures

When your agent fails, you need to know why. If the failure was caused by a dependency, attribute it to that dependency. This prevents agents from being blamed for failures outside their control and directs remediation efforts to the correct system.

You log every failed agent request with a root cause attribution. If the agent failed because a tool call timed out, attribute the failure to the tool's API. If the agent failed because it received invalid data, attribute it to the data source. If the agent failed because of internal logic errors, attribute it to the agent itself.

A claims processing agent in early 2026 had an overall failure rate of 4.2 percent. Failure attribution showed: 2.1 percent of failures were due to timeout errors from the policy database, 1.4 percent were due to malformed responses from the eligibility API, 0.5 percent were due to agent logic errors, and 0.2 percent were due to user input issues. Sixty-two percent of failures were caused by external dependencies. The team shared failure attribution with the database and API teams, who prioritized fixes. External failure rates dropped, and overall agent failure rate fell to 2.8 percent without changing the agent's code.

## Dependency Observability Dashboards

You build a dependency health dashboard that displays the status of every service your agent relies on. Each dependency shows uptime, latency percentiles, error rate, and recent incidents. Engineers check the dashboard when agent success rates drop to quickly identify whether the problem is internal or external.

A healthcare agent system in early 2026 maintained a dependency dashboard with eleven external services: patient records database, medication database, lab results API, insurance eligibility API, provider directory, appointment scheduling API, prescription routing API, clinical decision support API, audit logging API, notification service, and identity verification API. When the agent's success rate dropped from 97 percent to 91 percent one morning, the dashboard immediately showed that the insurance eligibility API was experiencing elevated error rates. The team escalated to the eligibility API provider within ten minutes instead of spending hours debugging the agent.

## The Dependency Reliability Budget

Your agent's reliability is constrained by the reliability of its dependencies. If your agent depends on three APIs with ninety-five percent reliability each, the maximum possible agent reliability is 0.95 times 0.95 times 0.95, which equals 86 percent. Even if your agent's code is perfect, you cannot exceed 86 percent success because your dependencies fail fourteen percent of the time.

This is the dependency reliability budget. You calculate it by multiplying the success rates of all critical dependencies. The result is the theoretical maximum reliability your agent can achieve. If your reliability budget is too low, you either improve your dependencies, add fallbacks, or reduce the number of dependencies.

A document processing agent in mid-2025 depended on seven APIs, each with approximately 97 percent uptime. The reliability budget was 0.97 to the seventh power, which equals 81 percent. The agent could not reliably exceed 81 percent success without dependency improvements. The team prioritized the three least reliable dependencies, worked with their owners to improve them from 97 to 99 percent uptime, and raised the reliability budget to 91 percent. Agent success rate climbed accordingly.

Your agent is only as reliable as the tools it depends on. Monitoring dependencies is not optional. It is the foundation of understanding why your agent succeeds or fails in production.

The next subchapter covers agent self-correction frequency — tracking how often your agent changes its mind and whether that self-correction indicates instability or adaptability.

