# 10.10 — The Rollout Dashboard: Real-Time Deployment Health

The rollout dashboard is the central interface for monitoring model deployments from shadow mode through full production. It shows what percentage of traffic each variant is serving, what metrics each variant is producing, whether any variant is regressing, and whether rollout should continue or pause. The dashboard is not a passive reporting tool. It is the decision interface that engineering uses hourly or daily to make go-no-go calls on deployment progression.

Without a rollout dashboard, deployment monitoring is manual archaeology. Engineers query logs, export CSVs, build one-off analyses, paste numbers into Slack, and debate whether a 2.3 percent change in error rate is significant. The process takes hours. Decisions get delayed. Regressions propagate while teams argue about data. With a rollout dashboard, the data is live, the comparisons are automatic, and the decision criteria are explicit. Engineers open the dashboard, see that error rate is up 4 percent with 95 percent confidence, and roll back. The process takes five minutes.

In January 2026, a medical appointment scheduling platform deployed a new GPT-5.2 model during evening hours. The on-call engineer checked the rollout dashboard 30 minutes after deployment and saw latency at 95th percentile had increased from 1.2 seconds to 2.8 seconds. The dashboard showed the latency spike was statistically significant and exceeded the predefined Tier 2 rollback threshold. The engineer clicked the rollback button on the dashboard, reverting to the baseline model. Total user impact: 30 minutes of degraded latency affecting 5 percent of traffic. Without the dashboard, the engineer would have noticed slowness through user complaints or manual log queries hours later, affecting far more users. The dashboard turned a potential multi-hour incident into a 30-minute blip.

The rollout dashboard is not optional for teams deploying models to production. It is the infrastructure that makes disciplined, evidence-based deployment decisions possible at the speed production requires.

## Core Metrics That Every Rollout Dashboard Must Show

The rollout dashboard tracks metrics in three categories: correctness, performance, and user satisfaction. Every category must show absolute values per variant and comparison to baseline with statistical confidence. The dashboard updates in real time or near-real time, typically with one to five minute latency.

Correctness metrics answer whether the model produces accurate, safe, policy-compliant outputs. Error rate is the most fundamental — what percentage of requests return errors, malformed outputs, or exceptions. Refusal rate measures requests where the model declines to answer. Hallucination rate measures factually incorrect outputs for systems that can detect hallucinations automatically. Policy violation rate measures outputs that breach content guidelines. For each metric, the dashboard shows current value per variant, baseline value, difference, confidence interval on the difference, and whether the difference exceeds rollback thresholds.

Performance metrics answer whether the model meets latency and cost requirements. Median latency and 95th percentile latency reveal typical and worst-case response times. Timeout rate measures requests exceeding latency budgets. Throughput measures requests per second the model can handle. Token consumption measures cost per request. The dashboard shows these metrics per variant with comparisons to baseline and thresholds indicating acceptable ranges.

User satisfaction metrics answer whether users prefer the new model. Explicit satisfaction comes from thumbs up/down ratings or star scores. Implicit satisfaction comes from engagement — conversation length, follow-up query rate, task completion rate, session duration. The dashboard shows satisfaction metrics per variant with statistical tests indicating whether differences are significant.

A document intelligence platform's rollout dashboard had three sections. Top section: correctness metrics — error rate, refusal rate, malformed output rate — with red/yellow/green color coding based on thresholds. Middle section: performance metrics — median latency, 95th percentile latency, token cost per query — with line charts showing trends over the past 24 hours. Bottom section: user satisfaction — task completion rate, user rating average, follow-up query rate — with confidence intervals and significance indicators. The dashboard updated every two minutes. On-call engineers checked it every hour during active rollouts.

## Traffic Distribution and Variant Assignment Visibility

The dashboard must show how traffic is distributed across variants and confirm that distribution matches intent. A rollout intended to send 10 percent to the new model and 90 percent to baseline should show approximately 10/90 split in the dashboard. Large deviations indicate bugs in routing logic or unexpected traffic patterns.

Traffic distribution shows request count per variant, percentage per variant, and requests per minute per variant. The visualization should make mismatches obvious — a pie chart or stacked bar chart where 10 percent looks like 10 percent. Sample ratio mismatch detection automatically flags deviations exceeding 2 percentage points, alerting engineers that variant assignment may be broken.

Variant assignment visibility shows how users are assigned to variants. For user-ID-based assignment, show unique user count per variant. For session-based assignment, show session count. For request-level random assignment, show request count. The dashboard confirms that assignment logic is working and that variants are receiving comparable populations.

Ramp schedule visualization shows the planned traffic percentage over time. If the plan is to ramp from 5 to 10 to 25 to 50 to 100 percent over two weeks, the dashboard shows that schedule with current position marked. Engineers see whether the rollout is on schedule, ahead of schedule, or paused for investigation.

A customer support platform's dashboard showed traffic distribution as a horizontal stacked bar where baseline was blue and new variant was green. The bar updated live. When traffic was supposed to be 20/80, the bar showed approximately 20 percent green, 80 percent blue. During a deployment in November 2025, engineers noticed the bar showing 35/65 instead of 20/80. Investigation found a bug in the routing logic that was hashing user IDs incorrectly. They fixed the bug and restarted the rollout. The visual mismatch caught the problem within two hours of deployment.

## Statistical Comparison and Significance Indicators

Raw metric values do not answer the question engineers need answered: is the new variant significantly different from baseline, and if so, in what direction? The dashboard must compute statistical comparisons automatically and present them clearly.

Confidence intervals show the range of plausible values for metric differences. If error rate is 1.2 percent for baseline and 1.8 percent for new variant, the difference is 0.6 percentage points. The 95 percent confidence interval might be 0.2 to 1.0 percentage points, meaning the true difference is very likely in that range. The dashboard shows the confidence interval visually — error bars, shaded regions, or numerical ranges.

Significance indicators flag metrics that differ beyond random chance. A green checkmark means no significant difference. A yellow warning means difference is approaching significance thresholds. A red alert means significant degradation requiring investigation or rollback. Color coding makes decision-making fast — engineers scan for red, investigate yellow, ignore green.

Effect size magnitude matters more than statistical significance. A statistically significant 0.1 percent increase in error rate is not operationally important. A 3 percent increase is important regardless of statistical significance. The dashboard highlights effect sizes that exceed practical significance thresholds defined by the team — typically 10 to 20 percent relative change for most metrics.

Sequential testing or Bayesian updating provides continuous confidence estimates. Instead of waiting for predetermined sample size, the dashboard computes posterior probabilities or sequential test statistics continuously. "We are 87 percent confident the new model has higher error rate based on data so far." Confidence grows over time as more data accumulates. Engineers can see when confidence reaches decision thresholds.

A legal research platform's dashboard showed metric comparisons as horizontal bars with confidence intervals. Baseline was a vertical line at zero. New variant's difference was a horizontal bar extending left for decreases or right for increases. Confidence intervals were shaded regions around the bar. If the confidence interval overlapped zero, the metric was marked green — no significant difference. If the confidence interval excluded zero and effect size exceeded 5 percent, the metric was marked yellow or red depending on direction and magnitude. The visual encoding made statistical significance and practical significance both immediately obvious.

## Rollback Threshold Visualization and Alert Status

Rollback thresholds define when a deployment must pause or revert. The dashboard must show these thresholds explicitly and indicate how close current metrics are to crossing them. Engineers should never wonder whether a metric is bad enough to trigger rollback — the dashboard answers that question automatically.

Threshold lines appear on metric charts. If error rate threshold is 2 percent, the chart shows a horizontal red line at 2 percent. If the new variant's error rate approaches that line, engineers see the trend and can decide whether to roll back preemptively. If error rate crosses the line, automatic alerts fire or automatic rollback executes depending on tier.

Alert status shows which thresholds have been violated. A list of active alerts with severity — Tier 1 critical, Tier 2 investigation required, Tier 3 monitoring. Each alert links to the relevant metric chart and shows when the alert fired, what the threshold was, and what the current value is. Acknowledged alerts are marked as such so multiple engineers do not respond to the same alert.

Time to threshold estimates how long until a metric crosses a threshold given current trends. If error rate is increasing linearly at 0.05 percentage points per hour and threshold is 2 percent and current rate is 1.2 percent, time to threshold is 16 hours. The estimate helps engineers decide urgency — a threshold that will be hit in two hours requires immediate action, a threshold that will be hit in 20 hours can wait for morning investigation.

Rollback action buttons are prominently displayed. One click rolls back the new variant to 0 percent traffic, reverting all users to baseline. The button requires confirmation to prevent accidental clicks but is designed for speed — two clicks and rollback executes. During an incident, engineers should not be editing config files or running CLI commands. They should be clicking a button.

A healthcare diagnostics platform's dashboard showed rollback thresholds as red zones on metric charts. False negative rate threshold was 2 percent — anything above 2 percent was in the red zone. During a deployment in December 2025, false negative rate trended upward, entering the yellow zone at 1.7 percent. The dashboard estimated time to red zone as four hours. The on-call engineer investigated, found that the model was misclassifying a specific condition, and rolled back before reaching the threshold. The time-to-threshold estimate gave the engineer actionable lead time.

## Drill-Down Capabilities for Root Cause Investigation

When the dashboard shows a regression, engineers need to drill down to understand why. Drill-down capabilities let engineers segment metrics by subgroups, inspect sample outputs, and trace individual requests to understand root causes without leaving the dashboard.

Subgroup segmentation splits metrics by user attributes, query attributes, or context attributes. If error rate is elevated for the new variant overall, segment by user type, query length, language, or time of day. If error rate is high only for non-English queries, you have identified the root cause direction. The dashboard provides dropdown menus or filter interfaces for common segmentation dimensions.

Sample output inspection shows recent requests from each variant with their inputs, outputs, latencies, and error states. Engineers can randomly sample outputs or filter for specific criteria — errors only, refusals only, latencies above 3 seconds, outputs containing specific keywords. Inspecting sample outputs reveals patterns that aggregate metrics hide.

Request tracing links to detailed traces for individual requests. Click a request ID in the dashboard, see the full trace showing model invocation, intermediate steps, latencies per step, token counts, and final output. For multi-stage pipelines, the trace shows which stage caused high latency or errors. Request tracing accelerates debugging by connecting aggregate metrics to concrete examples.

Query trending shows which types of queries are driving metric changes. If refusal rate increased, trending shows which query topics or patterns are being refused more often. If latency increased, trending shows which query characteristics correlate with high latency. Trending analysis uses text clustering, keyword extraction, or topic modeling to group queries and compute metrics per group.

A financial advisory platform's dashboard included a drill-down panel. When engineers noticed elevated error rate for the new variant, they clicked the error rate metric, which opened a drill-down showing errors segmented by query length, user tier, and time of day. The segmentation revealed errors concentrated in queries longer than 500 tokens. Clicking that segment showed sample error outputs, which all indicated the model was timing out on long queries. The drill-down took three minutes and identified the root cause without querying logs or writing analysis scripts.

## Multi-Stage Rollout Progress Tracking

Model deployments typically progress through multiple stages — shadow mode, canary at 5 percent, canary at 15 percent, A/B test at 50/50, ramp to 100 percent. The dashboard must show which stage is currently active, what success criteria must be met to advance, and whether those criteria are currently satisfied.

Stage timeline shows past, current, and future stages. Past stages are marked complete with the date and duration they ran. Current stage is highlighted with elapsed time and remaining time until the next checkpoint. Future stages are grayed out with planned dates. The timeline gives engineers context about where the rollout is in its lifecycle.

Stage success criteria are explicitly listed. For the canary stage, criteria might be: error rate below 1.5 percent, latency below 2 seconds at 95th percentile, user satisfaction above 4.0, minimum 5,000 requests collected. The dashboard shows current values for each criterion and indicates whether each is satisfied. Green checkmarks for satisfied criteria, yellow warnings for borderline, red X for violated.

Automatic stage advancement occurs when all success criteria are met and minimum duration has elapsed. The dashboard shows a countdown: "Stage will advance in 4 hours if metrics remain healthy." When the countdown reaches zero, traffic percentage increases to the next stage automatically or after human approval depending on configuration. Automatic advancement removes human bottlenecks from rollout progression.

Manual stage advancement is available when automatic advancement is disabled. Engineers review the dashboard, confirm success criteria are met, and click "Advance to next stage." The dashboard logs who approved the advancement and when. Manual advancement gives teams control when they want human judgment in the loop.

A document analysis platform's dashboard showed rollout progress as a horizontal flow: Shadow mode, Canary 5 percent, Canary 15 percent, A/B 50/50, Ramp 100 percent. Each stage had a circle that turned green when complete. The current stage was highlighted with a progress bar showing time elapsed and success criteria status. During a deployment in January 2026, the rollout reached Canary 15 percent and stayed there for three days because latency was borderline yellow. On day three, latency improved, criteria turned green, and the dashboard showed "Ready to advance." Engineers approved advancement to A/B 50/50.

## Integration with Incident Response and On-Call Workflow

The rollout dashboard is not isolated monitoring — it is integrated with incident response and on-call workflow. Alerts from the dashboard page on-call, link to runbooks, and log to incident management systems. Actions taken on the dashboard — rollbacks, stage advancements, threshold adjustments — are logged and visible in incident timelines.

Alerting integration sends notifications to PagerDuty, Opsgenie, or Slack when thresholds are violated. Alerts include severity, affected metric, current value, threshold, and link to the dashboard. On-call engineers receive the alert on their phones, click the link, and see the full context in the dashboard without manually querying systems.

Runbook links appear on alerts. "Error rate exceeded Tier 2 threshold. See runbook: rollback procedure for model deployments." Engineers follow the runbook steps, which often reference dashboard actions — check subgroup segmentation, inspect sample outputs, decide whether to roll back or investigate further.

Incident timeline logging records all dashboard actions in the incident management system. If an engineer clicks rollback at 2:34pm, the incident timeline shows "2:34pm: Engineer initiated rollback from rollout dashboard. Variant traffic reduced to 0 percent." Post-incident reviews reference the timeline to understand response speed and decision quality.

Slack integration posts key events to team channels. "Deployment advanced to canary 15 percent." "Latency threshold violation detected, alert sent to on-call." "Rollback completed, all traffic reverted to baseline." The Slack posts keep the team aware of deployment status without requiring everyone to watch the dashboard.

A customer support platform integrated their rollout dashboard with PagerDuty and Slack. When error rate exceeded Tier 2 thresholds during a deployment, PagerDuty paged on-call with a link to the dashboard. The engineer opened the dashboard on their phone, saw error rate at 2.1 percent for the new variant, clicked rollback, and confirmed. The rollback executed in 15 seconds. Slack posted "Rollback initiated by Engineer X due to Tier 2 error rate threshold violation." The entire incident — from alert to resolution — took 90 seconds and was fully logged without manual incident filing.

## Historical Rollout Comparison and Learning

The dashboard should show not just the current deployment but also historical deployments for comparison. How does this rollout compare to the last three? Are metrics trending better or worse than typical rollouts? Historical context helps engineers calibrate expectations and identify anomalies.

Historical overlays plot past deployments on the same charts as the current deployment. If the current canary is at 5 percent for 12 hours, overlay the metrics from the previous three canaries at the same stage and duration. If current error rate is 1.3 percent and the previous three were 0.9, 1.0, and 1.1 percent, the current rollout is slightly elevated but within historical range.

Rollout success rate shows how many of the last ten rollouts reached 100 percent versus how many rolled back or paused. A low success rate indicates that deployment process or testing process needs improvement. A high success rate indicates the team has good discipline and effective safeguards.

Common failure modes are extracted from rollout history. If three of the last five rollbacks were due to latency regressions, latency is a recurring problem area requiring better load testing or performance optimization. If two rollbacks were due to subgroup failures in non-English languages, multilingual testing needs strengthening. The dashboard surfaces these patterns to guide process improvements.

Learning library links to post-rollout retrospectives. Each completed rollout — whether successful or rolled back — has a retrospective document summarizing what happened, what was learned, and what process changes were made. Engineers preparing for a new rollout can review past retrospectives to avoid repeating mistakes.

A legal research platform's dashboard included a historical comparison feature. When reviewing a new rollout, engineers could overlay metrics from the past five rollouts. During a deployment in December 2025, they noticed refusal rate was 6.2 percent at canary 5 percent, compared to historical average of 4.8 percent. The elevated rate was flagged yellow but not red. Engineers investigated, found the model was refusing a category of queries that the baseline model answered, and decided this was acceptable given improved accuracy in other areas. The historical context helped calibrate whether the difference was concerning or normal variance.

## Cost Visibility and Deployment Economics

Model rollouts affect costs — inference costs increase if the new model is more expensive, and A/B testing doubles costs during the test period. The dashboard must show cost impact in real time so teams understand the economic consequences of deployment decisions.

Cost per request shows the inference cost for each variant. If baseline costs 0.8 cents per request and new variant costs 1.4 cents per request, the dashboard shows both figures and the 75 percent increase. Cost is broken down by token usage — input tokens, output tokens, reasoning tokens for models that support extended thinking.

Total cost during rollout shows cumulative spend for the deployment so far. An A/B test running for five days at 50,000 requests per day with 1.2 cents per request total cost is 3,000 dollars. The dashboard tracks this running total. If cost budgets are exceeded, alerts fire.

Cost projection estimates total cost to complete the rollout. If the plan is to run A/B at 50/50 for two weeks then ramp to 100 percent over one week, and current spend is 500 dollars per day, projected total cost is 10,500 dollars. The projection helps teams decide whether the rollout is economically justified.

Cost-benefit analysis shows cost increase versus metric improvements. If the new model costs 40 percent more but improves task completion by 12 percent, is the trade-off worth it? The dashboard cannot make this judgment, but it presents the data clearly. Stakeholders decide whether the business value of 12 percent improvement justifies 40 percent cost increase.

A financial advisory platform's dashboard included a cost panel showing spend per variant, total spend during rollout, and projected spend to complete rollout. During an A/B test in late 2025, they noticed the new variant was costing 2.2 times as much as baseline due to increased output token usage. The dashboard projected that full rollout would increase monthly inference costs by 180,000 dollars. Product and engineering reviewed the cost-benefit: the new model improved conversion rate by 8 percent, generating an estimated 400,000 dollars additional monthly revenue. The cost increase was justified. The dashboard made the economic trade-off explicit and quantifiable.

## Customization for Different Deployment Types

Not all deployments are identical. Model version changes, prompt changes, parameter changes, and capability additions have different risk profiles and require different metrics. The dashboard should be customizable for deployment type.

Model version changes require comprehensive monitoring — correctness, performance, satisfaction, cost. The dashboard shows all metric categories because model changes can break behavior in any dimension.

Prompt changes require lighter monitoring focused on output quality and user satisfaction. Correctness and satisfaction metrics are critical. Performance metrics are less critical because prompts rarely affect latency significantly. The dashboard can hide or de-emphasize performance panels for prompt changes.

Parameter changes — temperature, top-p, max tokens — require performance and output distribution monitoring. Latency and token cost are affected. Output characteristics like length and diversity change. The dashboard highlights these dimensions.

Capability additions — enabling a new tool, enabling image input, enabling code execution — require monitoring of the capability-specific metrics. If adding database search capability, monitor search latency, search error rate, and whether users find the augmented responses more satisfying. The dashboard includes capability-specific panels.

The dashboard should support templates for common deployment types. Select "model version change" and get the full monitoring suite. Select "prompt change" and get a focused subset. Select "capability addition" and get capability-specific metrics plus standard metrics. Templates reduce configuration overhead and ensure the right metrics are monitored for each deployment type.

A document intelligence platform built deployment type templates into their dashboard. When starting a rollout, engineers selected deployment type from a dropdown. Model changes got the full dashboard. Prompt changes got a simplified dashboard focused on accuracy and user ratings. The templates ensured appropriate monitoring without requiring engineers to configure dozens of panels manually.

The rollout dashboard is the operational nervous system for model deployments. It centralizes data, automates analysis, visualizes trends, and enables fast decisions. Teams without rollout dashboards deploy blind. Teams with dashboards deploy with eyes open, catching problems early and building institutional confidence in their deployment discipline. The dashboard is not optional. It is the infrastructure that makes safe AI deployment possible at production scale.

