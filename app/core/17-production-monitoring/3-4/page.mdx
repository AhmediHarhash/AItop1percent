# 3.4 — Sampled Deep Evaluation: Batch Quality Measurement at Scale

Inline judges catch obvious quality problems. They miss subtle degradation — the shift in tone that makes responses less empathetic, the decrease in citation quality that makes answers less trustworthy, the increase in hedging language that makes advice less actionable. These problems are real. They degrade user experience. But they are below the resolution of a 200 millisecond inline judge running on a small model. You catch them with sampled deep evaluation: expensive, thorough quality checks on a statistically meaningful subset of traffic, run in batch, evaluated by powerful models or human reviewers.

Sampled deep evaluation balances cost and coverage. You cannot afford to run a ten-minute quality check on every request. You can afford to run it on fifty to five hundred requests per day, selected to represent the full distribution of production traffic. Those fifty to five hundred evaluations give you enough signal to detect quality trends, identify failure modes, and validate inline judges — if you sample correctly, if you evaluate rigorously, and if you act on what you find.

## Sampling Strategy: Representing the Full Distribution

Random sampling is simple but wasteful. You spend evaluation budget on easy, high-quality responses that teach you nothing. The teams that get value from sampled evaluation use stratified sampling: deliberately selecting responses from different segments of traffic to maximize information per evaluation.

**Stratify by predicted quality**. Use inline judge scores or proxy behavioral metrics to estimate which responses are likely to be good, mediocre, or bad. Sample disproportionately from the low-quality tail. If inline judges flag five percent of responses as potentially problematic, sample thirty percent of your deep evaluation budget from that five percent. You learn more from failures than from successes.

A customer support system runs deep evaluation on two hundred responses per day. Inline judges flag eight percent of responses as low-quality. The sampling strategy allocates sixty evaluations to low-scoring responses, eighty evaluations to medium-scoring responses, and sixty evaluations to high-scoring responses. The oversampling of low-quality responses catches subtle patterns that inline judges missed. The high-quality sample validates that inline judges are not missing good responses.

**Stratify by traffic segment**. Different user types, different prompt categories, and different times of day have different quality distributions. Sample proportionally across segments to ensure you detect problems in every part of your system. A quality regression that only affects five percent of users is invisible in pure random sampling but obvious in stratified sampling.

A legal research assistant stratifies deep evaluation across five practice areas, three user experience levels, and two response length categories. Each combination gets at least five evaluations per day. A quality problem in intellectual property for new users is detected within two days because that segment is explicitly sampled. Pure random sampling would take two weeks to accumulate enough examples to see the pattern.

**Stratify by disagreement**. When you run both inline judges and proxy behavioral metrics, look for cases where they disagree. High inline judge scores but low user engagement. Low inline judge scores but high user satisfaction. These disagreements are information-rich. They tell you what your automated metrics are missing. Allocate evaluation budget to disagreement cases.

A code completion tool samples responses where inline judge scores are high but user acceptance rate is low. Deep evaluation reveals the generated code is technically correct but stylistically inconsistent with the user's existing codebase. The inline judge cannot detect style mismatch. User behavior can. Sampling on disagreement surfaces the gap.

## Deep Evaluation Methodology: What to Measure and How

Deep evaluation is not "look at this response and decide if it's good." Deep evaluation is structured, consistent measurement of specific quality dimensions using explicit rubrics.

**Define a detailed rubric for each quality dimension**. Factual correctness is not one question. It is a set of questions. Are all factual claims accurate? Are claims appropriately qualified with uncertainty? Are sources cited? Are citations accurate and accessible? Is information current or outdated? Each question gets a separate score. The rubric turns subjective quality judgment into structured data.

A medical information chatbot evaluates responses on eight dimensions. Factual accuracy: Are medical claims correct according to current guidelines? Completeness: Does the response address all parts of the user's question? Safety: Does the response avoid advice that could cause harm? Tone: Is language empathetic, non-alarmist, and appropriate for a lay audience? Clarity: Is medical terminology explained? Citations: Are authoritative sources cited? Actionability: Does the response give clear next steps? Groundedness: Are all claims supported by the provided context or general medical knowledge? Each dimension is scored one to five with explicit criteria for each score level.

**Use LLM-as-judge with detailed prompts for scalable deep evaluation**. Human review is the gold standard but does not scale past a few dozen evaluations per day. For hundreds of evaluations, use a powerful model like GPT-5.1 or Claude Opus 4.5 as the evaluator. Give the model the rubric, the response, the original prompt, and any context the production model had access to. Ask the model to score each dimension and justify each score.

LLM-as-judge for deep evaluation is not the same as inline judges. Inline judges are fast and cheap. Deep evaluation judges are slow and expensive but much more thorough. An inline judge might use GPT-5-nano and run in 150 milliseconds. A deep evaluation judge might use GPT-5.1 with chain-of-thought reasoning and take fifteen seconds. The deep judge produces detailed, justified scores that humans can review and validate.

**Include human review for a subset of the deep evaluation sample**. Evaluate fifty responses per day with LLM-as-judge and ten responses per day with human reviewers. Use the human-reviewed responses to validate the LLM judge. Compute agreement rates between human scores and LLM scores. Retune LLM judge prompts when agreement falls below eighty percent. Human review is too expensive to scale but essential for calibration.

A financial advice chatbot runs GPT-5.1 as a deep evaluator on one hundred responses per day and human review on twenty responses per day. Agreement between GPT-5.1 and human reviewers is eighty-four percent for factual correctness, seventy-nine percent for tone, and ninety-one percent for compliance with financial regulations. The LLM judge is trustworthy for factual and compliance checks but less reliable for tone. The team weights human review more heavily for tone-related quality tracking.

## Temporal Patterns: When to Sample and How Often

Quality changes over time. Your sampling strategy must capture temporal patterns.

**Run continuous daily sampling** to detect quality trends. Evaluate the same number of responses every day, using the same stratification strategy. Daily consistency makes trends visible. A gradual quality decline from 4.3 to 3.9 over three weeks is obvious in a daily time series. It is invisible in sporadic, inconsistent sampling.

Track daily evaluation results in a time-series dashboard. Plot mean scores and score distributions over time. Mark model deployments, prompt changes, and infrastructure updates on the timeline. Correlate quality changes with system changes. A mean quality score drop from 4.4 to 3.8 two days after a model update is a clear signal. The same drop with no corresponding system change suggests external causes like adversarial traffic or data drift.

**Increase sampling frequency after deployments**. The highest-risk period for quality problems is the first seventy-two hours after a model update, a prompt change, or a significant infrastructure change. Double or triple your deep evaluation budget for three days post-deployment. Detect regressions before they compound.

A customer service copilot normally samples one hundred fifty responses per day for deep evaluation. After a model update, sampling increases to four hundred responses per day for three days. This post-deployment surge catches a subtle tone regression on the second day that would have taken ten days to detect under normal sampling.

**Run weekly deep dives on specific topics**. Use one day per week to focus deep evaluation on a specific user segment, prompt category, or failure mode. Rotate focus areas week by week. This targeted sampling catches problems that are too rare or too localized to appear in daily stratified sampling.

A legal document assistant runs themed deep evaluation every Friday. Week one focuses on contract generation. Week two focuses on legal research responses. Week three focuses on compliance questions. Week four focuses on high-risk advice that could expose users to legal liability. The rotating focus ensures every part of the system gets thorough scrutiny at least once per month.

## Analyzing Deep Evaluation Results: Turning Scores Into Action

Deep evaluation produces rich, structured data. The value is in what you do with it.

**Track score distributions, not just means**. A mean quality score of 4.2 is uninformative. A distribution where ninety percent of responses score 4.5 or higher and ten percent score below 3.0 tells you something specific: most responses are good, but there is a problematic tail. A distribution where all responses score 3.8 to 4.6 tells you something different: quality is consistent but mediocre. Mean scores hide distribution shape. Distribution shape reveals failure modes.

Plot score distributions as histograms or violin plots. Track how distributions change over time. A widening distribution indicates increasing inconsistency. A distribution shift toward lower scores indicates systematic degradation. A bimodal distribution indicates the system is performing well for some traffic and poorly for other traffic.

**Identify failure mode clusters**. Responses that score low on deep evaluation are not randomly distributed. They cluster around specific patterns. Use clustering algorithms or manual review to group low-scoring responses by failure mode. Some fail on factual accuracy. Some fail on tone. Some fail on completeness. Some fail on safety. Identifying the dominant failure modes tells you what to fix.

A medical chatbot runs deep evaluation on three hundred responses per week. Forty-two responses score below 3.5. Manual review of the low-scoring responses reveals three failure mode clusters. Sixteen responses failed because they cited outdated medical guidelines. Fourteen responses failed because they used overly technical language inappropriate for patients. Twelve responses failed because they did not address mental health aspects of questions that mentioned anxiety or depression. The clusters are specific and actionable. The team updates the retrieval index to prioritize recent guidelines, fine-tunes the model on plain-language medical explanations, and adds a mental health copilot to handle psychological dimensions.

**Correlate deep evaluation scores with inline judges and proxy metrics**. Deep evaluation is your ground truth. Inline judges and behavioral metrics are proxies. Measure how well the proxies predict the ground truth. A response that scores high on deep evaluation should score high on inline judges. A response that scores low on deep evaluation should trigger negative user behavior like retries or abandonment. If correlations are weak, your proxies are not measuring what matters.

Reweight your composite quality metrics based on observed correlations. A behavioral metric that strongly correlates with deep evaluation results should get higher weight in alerting. A behavioral metric with weak correlation should get lower weight or be dropped. Deep evaluation is the feedback loop that keeps automated quality monitoring accurate.

## Deep Evaluation for Edge Cases and Rare Events

Stratified sampling ensures common scenarios are covered. But rare, high-stakes scenarios also matter. Deep evaluation is the only practical way to monitor quality on edge cases that appear once per thousand requests.

**Maintain a watchlist of high-risk prompts and scenarios**. These are prompts that historically caused problems, prompts that involve sensitive topics, or prompts that require nuanced judgment. Flag any production request that matches a watchlist pattern. Route all flagged requests to deep evaluation, even if that means evaluating fewer routine requests.

A healthcare assistant maintains a watchlist of prompts involving self-harm, substance abuse, medication interactions, pregnancy, and pediatric care. Any request matching these patterns is automatically sent to deep evaluation within six hours. This targeted sampling ensures high-risk responses are checked thoroughly even though they represent less than two percent of total traffic.

**Run adversarial sampling**. Some users intentionally try to break your system with prompt injections, jailbreaks, or boundary-pushing requests. These adversarial requests often expose quality and safety problems. Sample aggressively from traffic flagged as potentially adversarial by input classifiers. Deep evaluation of adversarial traffic reveals both attack patterns and model weaknesses.

A content moderation assistant flags three percent of incoming traffic as potentially adversarial based on prompt patterns. All flagged traffic is sent to deep evaluation. Analysis reveals that adversarial prompts reliably cause the model to generate hedged, non-committal responses that do not actually answer the moderation question. The problem is not safety — the model is not producing harmful outputs. The problem is evasion — the model is refusing to make clear judgments. The team retrains the model to respond decisively to adversarial inputs.

**Use deep evaluation to validate major changes before full rollout**. Before deploying a new model, a new prompt, or a new retrieval strategy to all users, deploy to a small percentage of traffic and run intensive deep evaluation on that traffic. Evaluate several hundred to several thousand responses. Compare deep evaluation scores against the baseline. Only roll out fully if deep evaluation confirms quality improves or holds steady.

A legal research assistant tests a new retrieval ranking algorithm on five percent of traffic. Deep evaluation on six hundred responses from the test group shows factual accuracy improves from 4.2 to 4.6, but citation quality drops from 4.5 to 3.9. The new ranker retrieves better documents but the model is not citing them correctly. The team fixes the citation issue before rolling out to all users. Deep evaluation prevented a regression that inline judges did not catch.

## Cost Management for Sampled Deep Evaluation

Deep evaluation is expensive. Using GPT-5.1 or Claude Opus 4.5 to evaluate hundreds of responses per day costs real money. Managing that cost without sacrificing signal is a design problem.

**Use cascading evaluators**. Run a cheap, fast evaluator first. Only send responses that fail the cheap evaluator to the expensive, thorough evaluator. This reduces the number of expensive evaluations without reducing coverage of problematic responses. Ninety percent of responses pass the cheap evaluator and are marked as likely good. Ten percent are sent to the expensive evaluator for thorough review.

A financial advice chatbot runs a fast GPT-5-mini evaluator on three hundred responses per day. Responses scoring above 4.0 are marked as good with no further evaluation. Responses scoring below 4.0 are sent to a GPT-5.1 evaluator with detailed rubrics. This cascade reduces expensive evaluations by seventy percent while maintaining full coverage of low-quality responses.

**Amortize evaluation cost by reusing expensive evaluations across multiple purposes**. A deep evaluation produces scores on factual accuracy, tone, safety, completeness, and compliance. Use those scores to retrain inline judges, to validate proxy metrics, to debug specific failure modes, and to generate training data for model improvements. One evaluation serves four purposes. The per-purpose cost is one-quarter of the headline cost.

A customer support copilot runs deep evaluation on two hundred responses per day at a cost of 120 dollars per day. The evaluation data is used to retrain inline judges monthly, validate behavioral proxy metrics weekly, generate quarterly reports on quality trends, and create fine-tuning datasets for model improvements. The amortized value per dollar spent is high because the same evaluation data supports multiple workflows.

**Negotiate bulk evaluation pricing with model providers**. If you are sending thousands of deep evaluation requests per week, you are a high-volume customer. Model providers often offer discounted pricing for high-volume, low-latency-tolerance batch evaluation workloads. You save thirty to fifty percent on eval costs by negotiating batch pricing instead of using default API rates.

Sampled deep evaluation is the ground truth layer in production quality monitoring. Inline judges run on every request but only see surface features. Proxy metrics run on every request but only measure behavior. Deep evaluation is slow, expensive, and low-coverage — but it is accurate. The combination of all three creates a complete quality monitoring system: inline judges for real-time alerting, proxy metrics for user-centric signals, and deep evaluation for validation and failure mode analysis.

Next, we explore user feedback integration — how to turn explicit ratings and implicit signals into structured quality data.

