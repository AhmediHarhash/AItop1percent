# Chapter 2 — Telemetry Architecture and Instrumentation

You cannot observe what you do not measure. Telemetry architecture determines what signals your AI system emits, how those signals flow through your infrastructure, and whether the resulting data enables debugging or just accumulates storage costs. Get instrumentation right and you can trace any request through prompts, retrievals, tool calls, and guardrails to understand exactly what happened and why. Get it wrong and you have logs that tell you a request occurred without telling you anything useful about what went wrong. The instrumentation choices you make in the first month of production determine your debugging capabilities for the next year.

---

- 2.1 — Instrumentation Fundamentals: What to Log, What to Sample, What to Skip
- 2.2 — Distributed Tracing for AI Pipelines: Prompts, Retrievals, Tools, Guardrails
- 2.3 — The Request Context: Capturing Metadata That Enables Debugging
- 2.4 — Token Accounting: Input, Output, Cached, and Wasted Tokens
- 2.5 — Latency Decomposition: Where Time Goes in AI Requests
- 2.6 — Cost Attribution: Tracking Spend by User, Feature, and Model
- 2.7 — Sampling Strategies for High-Volume Systems
- 2.8 — Async Logging and Performance Impact Management
- 2.9 — Schema Design for AI Telemetry: Fields That Enable Analysis
- 2.10 — OpenTelemetry and AI: Extending Standards for LLM Workloads
- 2.11 — Retention Policies: How Long to Keep What

---

*The difference between debugging for five minutes and debugging for five hours is whether you logged the right fields in the first place.*
