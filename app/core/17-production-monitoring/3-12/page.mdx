# 3.12 — Quality SLOs: Setting Targets That Drive Action

A customer support chatbot team tracked inline quality scores, user satisfaction, and escalation rates. They built dashboards. They set up alerts. Yet when quality degraded over two months, no action was taken until user complaints reached crisis levels. The problem was not measurement. The problem was the absence of a target. No one knew what "good" meant. No threshold defined when intervention was required. Engineers saw quality drop from 4.4 to 4.0 and thought "4.0 seems fine." Product saw escalation rate rise from eighteen percent to twenty-six percent and thought "that is probably normal variance." Without explicit quality targets, the team had data but no accountability, visibility but no action.

Quality Service Level Objectives are the targets that define acceptable system performance. An SLO is not a hope or a guideline. It is a commitment. "Inline quality score will remain above 4.2 ninety-five percent of the time." "User satisfaction rate will remain above seventy percent every week." "Safety violation rate will remain below 0.5 percent every day." SLOs turn vague aspirations into measurable standards. They define what must be true for the system to be considered healthy. When an SLO is violated, action is required. SLOs create accountability, align teams on priorities, and make trade-offs explicit.

## Why AI Systems Need Quality SLOs

Traditional software has uptime SLOs and latency SLOs. AI systems need quality SLOs because availability and speed are insufficient. A system can be fast, available, and producing low-quality outputs. Quality SLOs extend SRE discipline to output correctness, safety, and user value.

**Quality is not monotonically improving**. Traditional software tends to get more reliable over time. Bugs are fixed, edge cases are handled, performance improves. AI systems degrade. Data drifts. Prompts change. Models are updated. Adversarial users probe new attack vectors. Without SLOs, degradation goes unnoticed until it is severe. With SLOs, degradation triggers action as soon as quality falls below the defined threshold.

A financial advice chatbot sets a quality SLO: inline judge score above 4.0 for ninety-eight percent of responses, measured daily. The SLO holds for four months. In early 2026, the SLO is violated on three consecutive days. Inline judge score drops to 3.9, then 3.8, then 3.7. The SLO violation triggers an incident. Investigation reveals a retrieval index update introduced lower-quality documents. The index is rolled back, and quality returns to 4.3 within six hours. Without the SLO, the degradation would have continued unnoticed.

**SLOs align cross-functional teams**. Engineering cares about latency and error rate. Product cares about user satisfaction. Legal cares about compliance. Security cares about safety violations. Without SLOs, each team optimizes for their own metrics, often at the expense of others. SLOs create shared accountability. Engineering cannot ship a low-latency update that degrades quality below the SLO. Product cannot launch a feature that increases safety violations above the SLO.

A healthcare chatbot defines three SLOs shared by Engineering, Product, and Trust and Safety. Factual correctness score above 4.5, owned by Engineering. User satisfaction above seventy-five percent, owned by Product. Safety violation rate below 0.3 percent, owned by Trust and Safety. No team can sacrifice another team's SLO to improve their own metric. Shared SLOs force collaboration and prevent siloed optimization.

**SLOs make trade-offs explicit**. Every AI system faces trade-offs. Quality versus latency. Safety versus utility. Coverage versus precision. Without SLOs, trade-offs are made implicitly, often by whoever happens to be making a decision. With SLOs, trade-offs are explicit and bounded. You can trade latency for quality as long as quality stays above the SLO. You can trade coverage for precision as long as user satisfaction stays above the SLO.

A legal research assistant faces a trade-off between response latency and groundedness. Stricter groundedness checks increase latency. The team sets two SLOs. Groundedness score above 0.85. P95 latency below four seconds. Any change that improves groundedness while keeping latency under four seconds is acceptable. Any change that degrades either metric below its SLO is rejected. The SLOs define the acceptable trade-off space.

## Defining Quality SLOs for Your System

Quality SLOs are not universal. They depend on your domain, your users, and your risk tolerance. Defining SLOs requires understanding what quality means for your specific system and what level of quality is necessary for the system to deliver value.

**Start with user expectations, not system capabilities**. Do not set SLOs based on what your system can currently achieve. Set them based on what users need. If your system currently scores 3.8 on inline quality but users need 4.5 to trust the system, the SLO is 4.5, not 3.8. The gap between current performance and the SLO defines your improvement roadmap.

A customer support copilot initially scores 3.6 on inline quality and has a user acceptance rate of sixty-two percent. User research reveals that agents begin to trust AI suggestions when acceptance rate exceeds seventy-five percent. The team sets a quality SLO targeting seventy-five percent acceptance, not sixty-two percent. The SLO drives a four-month improvement project. Quality improves to 4.3, and acceptance rate reaches seventy-eight percent. The SLO, not current capability, defined the target.

**Set SLOs on metrics that correlate with user value**. Quality metrics are proxies. They are useful only if they correlate with outcomes users care about. If inline quality score correlates with user satisfaction, set an SLO on inline quality score. If it does not, set an SLO on user satisfaction directly. SLOs on vanity metrics create false confidence.

A legal research assistant tests correlation between inline groundedness score and user satisfaction. Groundedness correlates at 0.81 with user satisfaction. Citation accuracy correlates at 0.67. Response length correlates at 0.23. The team sets SLOs on groundedness and citation accuracy. No SLO is set on response length because it does not predict user value. SLOs are set only on metrics that matter.

**Define achievable but meaningful thresholds**. An SLO that is never violated is too lenient. An SLO that is violated daily is too strict. The right threshold is achievable under normal operation but requires action when violated. A good rule of thumb: the SLO should be violated five to fifteen times per year. Frequent enough to drive continuous improvement. Rare enough that violations signal real problems.

A financial chatbot sets a user satisfaction SLO at seventy-two percent weekly. Historical data shows satisfaction ranges from sixty-eight percent to seventy-nine percent. The SLO threshold at seventy-two percent is the twenty-fifth percentile of historical performance. The SLO is violated eight times in the first year, each time prompting investigation and improvement. The threshold is meaningful but achievable.

**Set separate SLOs for different risk categories**. Not all outputs are equally critical. A factual error in low-stakes advice is annoying. A factual error in medical advice is dangerous. Set stricter SLOs for high-stakes interactions. Allow more tolerance for low-stakes interactions.

A healthcare chatbot defines two quality SLOs. For low-risk queries about general wellness and nutrition, inline quality score above 4.0. For high-risk queries involving symptoms, medications, or diagnoses, inline quality score above 4.7. Low-risk queries allow moderate quality. High-risk queries demand near-perfect quality. Separate SLOs reflect separate stakes.

## SLO Components: Metrics, Thresholds, and Time Windows

An SLO has three components. The metric being measured. The threshold that defines acceptable performance. The time window over which the threshold is evaluated.

**Choose a single metric per SLO**. Composite SLOs that combine multiple metrics are hard to interpret and hard to enforce. "Quality score above 4.0 AND user satisfaction above seventy percent" is two SLOs, not one. Separate them. Track them independently. Each SLO should have one clear, measurable metric.

A customer support chatbot defines four separate SLOs. Inline quality score above 3.8. User acceptance rate above seventy percent. Escalation rate below twenty percent. Safety violation rate below 0.5 percent. Each SLO is independent. Each has its own threshold and its own violation response. Separate SLOs are easier to monitor and enforce than composite SLOs.

**Express thresholds as percentiles or error budgets**. Absolute thresholds are brittle. "Quality score must always be above 4.0" is impossible. Every system has outliers and bad days. Percentile thresholds are more robust. "Quality score above 4.0 for ninety-five percent of responses" allows for five percent of responses to fall below the threshold without violating the SLO.

Error budgets are another approach. Define an acceptable error rate and budget for it. "No more than fifty low-quality responses per day" is an error budget. "No more than two safety violations per week" is an error budget. Error budgets make trade-offs explicit. You can spend your error budget on high-value experiments or improvements that temporarily degrade quality.

**Set time windows appropriate to the metric's volatility**. Fast-changing metrics like inline quality score should be evaluated over short time windows — hourly or daily. Slow-changing metrics like user satisfaction should be evaluated over longer time windows — weekly or monthly. The time window determines how quickly you detect violations and how much noise you tolerate.

A legal research assistant sets daily SLOs for inline groundedness score and hourly SLOs for safety violation rate. Groundedness is measured on every response and updates continuously. Daily evaluation is fast enough to detect problems. Safety violations are rare and high-stakes. Hourly evaluation ensures rapid detection. User satisfaction is measured weekly because feedback is sparse and noisy day-to-day.

## Enforcing SLOs: What Happens When They Are Violated

An SLO without consequences is a suggestion. An SLO with consequences is a commitment. Define what happens when an SLO is violated. Violations must trigger action, not just logging.

**SLO violations trigger incident response**. Treat an SLO violation the same way you treat a production outage. Declare an incident. Assemble a response team. Investigate root cause. Implement a fix. Document the incident and the resolution. SLO violations are not routine events. They are production failures.

A financial chatbot treats SLO violations as severity-2 incidents. When the inline quality SLO is violated, an incident is automatically declared in PagerDuty. The on-call engineer is paged. A Slack incident channel is created. The team investigates within thirty minutes. Root cause is documented in a post-incident review. SLO violations receive the same urgency as latency spikes or error rate increases.

**Define escalation paths for repeated violations**. A single SLO violation might be a transient issue. Three violations in a month is a systemic problem. Define escalation paths. After the third violation in thirty days, escalate to engineering leadership. After the fifth violation, escalate to executive leadership. Repeated violations demand structural fixes, not just tactical patches.

A customer support copilot escalates after three user satisfaction SLO violations in a rolling thirty-day window. First violation: engineering team investigates. Second violation: product manager is notified. Third violation: engineering director is notified and a dedicated task force is formed. The escalation path ensures repeated problems receive escalating attention.

**Freeze risky changes after SLO violations**. When an SLO is violated, the system is in a degraded state. Do not compound the problem by shipping additional changes. Freeze non-critical deployments until the SLO is restored. Focus all effort on recovery. Only changes that improve quality or fix the violation are allowed.

A healthcare chatbot implements a change freeze after any safety violation SLO breach. When the safety violation rate exceeds 0.5 percent, all model updates, prompt changes, and feature deployments are paused. Only changes directly addressing the safety issue are allowed. The freeze prevents the team from making quality worse while responding to a safety incident.

## Balancing SLOs and Innovation

SLOs create discipline. They also create tension. Tight SLOs limit experimentation. Loose SLOs allow degradation. The right balance depends on your system's maturity and your users' tolerance for quality variance.

**Use error budgets to allocate risk**. An error budget is the allowed amount of SLO violation. If your SLO is "quality above 4.0 for ninety-five percent of responses," your error budget is five percent. You can spend that budget on experiments, risky deployments, or aggressive optimizations. As long as you stay within budget, you are meeting the SLO.

A legal research assistant has a monthly error budget of two hundred low-quality responses, defined as groundedness score below 0.85. The team can spend the budget however they choose. Deploying a new retrieval algorithm might consume fifty responses of the budget. Testing a new prompt might consume thirty. As long as the total stays below two hundred, the SLO is met. Error budgets make trade-offs quantitative and transparent.

**Relax SLOs for experimental features**. New features do not have optimized quality on day one. Set relaxed SLOs for beta features and tighten them as the feature matures. A beta feature might have a user satisfaction SLO of sixty percent. The same feature after six months might have a user satisfaction SLO of seventy-five percent. Progressive SLO tightening balances innovation with quality.

A customer support chatbot launches a new multilingual feature in beta. The initial quality SLO is set at 3.5, lower than the main product's SLO of 4.0. The team collects data, iterates on the model, and improves quality. After three months, the multilingual SLO is raised to 3.8. After six months, it matches the main product at 4.0. Progressive tightening allows the feature to launch while ensuring it eventually meets production standards.

**Renegotiate SLOs when system or user needs change**. SLOs are not permanent. As your system improves, tighten SLOs. As user expectations increase, tighten SLOs. As you enter new domains or serve new user segments, redefine SLOs. SLOs should reflect current needs, not historical capabilities.

A financial chatbot initially set a quality SLO at 3.8. After two years of improvement, the system consistently scores above 4.3. The SLO is raised to 4.1 to reflect improved baseline performance. The new SLO ensures the team does not regress to previously acceptable but now outdated quality levels. SLOs ratchet upward as capabilities improve.

## Communicating SLOs to Stakeholders

SLOs are not just internal engineering commitments. They are promises to users, commitments to leadership, and alignment mechanisms across teams.

**Include SLOs in product documentation and user agreements**. If you promise users that responses will be factually correct ninety-five percent of the time, document that promise. Users know what to expect. Leadership knows what the team is accountable for. Documented SLOs create external accountability.

A healthcare chatbot documents three SLOs in its terms of service. Factual correctness above ninety percent, measured against clinical guidelines. Safety violation rate below 0.2 percent. Response latency below three seconds for ninety-nine percent of requests. Users see these commitments. The team is accountable for meeting them. Documented SLOs turn internal targets into external commitments.

**Report SLO compliance in regular reviews**. Include SLO compliance in weekly or monthly stakeholder updates. Show which SLOs were met. Show which were violated. Show what actions were taken. SLO reporting makes quality visible to leadership and ensures it remains a priority.

A legal research assistant includes SLO compliance in monthly executive reviews. The report shows four SLOs, compliance percentage for each, and trend over the last six months. SLOs that were violated are highlighted, with root cause analysis and corrective actions documented. Regular SLO reporting keeps quality top-of-mind for leadership and ensures resources are allocated to quality improvements.

**Use SLOs to set team priorities**. When multiple initiatives compete for resources, SLOs clarify priorities. If the system is meeting all SLOs, focus on innovation and new features. If an SLO is at risk of violation, prioritize fixing it over new work. SLOs prevent quality from being deprioritized in favor of features.

A customer support chatbot team uses SLOs to prioritize sprints. If all SLOs are green, the team allocates eighty percent of capacity to new features and twenty percent to quality improvements. If any SLO is yellow, the allocation shifts to fifty percent features and fifty percent quality. If any SLO is red, all work stops except for restoring the violated SLO. SLOs create dynamic prioritization that responds to system health.

## Evolving SLOs as Systems Mature

Early-stage systems need lenient SLOs that allow experimentation. Mature systems need strict SLOs that maintain high standards. SLO evolution tracks system maturity.

**Start with achievable SLOs and tighten over time**. A newly deployed model might have a quality SLO at 3.5. After six months of iteration, raise it to 4.0. After a year, raise it to 4.2. Progressive tightening drives continuous improvement without creating unachievable targets on day one.

A financial chatbot sets an initial user satisfaction SLO at sixty-five percent. After three months, user satisfaction consistently exceeds seventy-two percent. The SLO is raised to seventy percent. After six months, satisfaction consistently exceeds seventy-eight percent. The SLO is raised to seventy-five percent. Each SLO increase reflects improved baseline performance and ensures the team does not regress.

**Add new SLOs as capabilities expand**. Early systems might only track quality and latency. Mature systems add SLOs for safety, compliance, groundedness, and user satisfaction. As monitoring infrastructure improves, add SLOs that were previously unmeasurable.

A healthcare chatbot launches with two SLOs: inline quality score above 4.0 and latency below three seconds. Six months later, the team adds a groundedness SLO: ninety percent of factual claims supported by retrieved documents. A year later, a compliance SLO is added: zero HIPAA violations per month. SLO coverage expands as monitoring capability expands.

**Deprecate SLOs that no longer predict user value**. Not all SLOs remain useful. If a metric no longer correlates with user satisfaction or system health, remove it. SLO sprawl creates distraction and maintenance burden. Keep only SLOs that drive meaningful action.

A legal research assistant initially tracked an SLO on response length, believing concise responses were better. Analysis shows response length does not correlate with user satisfaction or quality scores. The SLO is deprecated. The team focuses on SLOs that matter: groundedness, citation accuracy, and user satisfaction. SLO deprecation keeps the monitoring system lean and focused.

Quality SLOs are the operational discipline that separates production-ready AI systems from prototypes. They define success, create accountability, and drive continuous improvement. Without SLOs, quality is a vague aspiration. With SLOs, quality is a measurable commitment that teams are held accountable for meeting. SLOs turn quality from a nice-to-have into a requirement.

