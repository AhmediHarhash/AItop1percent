# 2.8 — Async Logging and Performance Impact Management

Synchronous logging adds 50 milliseconds to every request. Async logging adds less than 1 millisecond.

That difference transforms user experience. In a conversational agent, 50 milliseconds is the difference between feeling responsive and feeling sluggish. At scale, 50 milliseconds per request across 10 million daily requests wastes 140 hours of cumulative user time every day. Users do not see "logging overhead." They see "slow product." And they leave.

An e-learning platform discovered this in late 2024. Their AI tutor logged every request synchronously. Each log write hit their database directly from the request handler. Under light load, logging added 30 milliseconds. Under peak load, logging added 120 milliseconds because the database was saturated. Users complained about lag. The team upgraded the database, doubled its capacity, and saw no improvement. The bottleneck was not database performance. It was the synchronous coupling between user requests and log writes.

They refactored to async logging. Logging calls became fire-and-forget. The request handler sent log events to an in-memory queue and returned immediately. A background worker drained the queue and wrote to the database in batches. Logging overhead dropped to 0.6 milliseconds. User-perceived latency dropped by 45 milliseconds at p50, 110 milliseconds at p95. Complaints stopped. NPS scores improved. The product felt faster even though the core AI remained unchanged.

Async logging is not optional for production AI systems. It is the only way to observe without degrading the thing you are observing.

## The Performance Cost of Synchronous Logging

Synchronous logging blocks the request handler until the log is written. If the log write takes 40 milliseconds, the user waits 40 milliseconds. If the log write fails and retries, the user waits even longer. If the logging system is slow, your product is slow. The user pays for your observability.

The cost compounds when you log at multiple stages. A request that logs at ingestion, after retrieval, after model inference, and at response time makes four synchronous log writes. If each write takes 20 milliseconds, you add 80 milliseconds to total latency. If each write retries once on failure, worst-case logging overhead is 160 milliseconds. That is catastrophic for user experience.

Synchronous logging also couples your application reliability to your logging system reliability. If your logging database goes down, and your application tries to write logs synchronously, every request fails or times out. Your application is down because observability is down. This is backwards. Observability should enhance reliability, not threaten it.

A healthcare chatbot team experienced this. Their logging system used a managed database service. The service had an outage. Their application tried to write logs synchronously, timed out after 5 seconds, and returned errors to users. For two hours, their chatbot was unavailable because their logging was unavailable. After the incident, they moved to async logging with a circuit breaker. The next time the logging database had an issue, logs were silently dropped, but the chatbot stayed online. Users never noticed.

## Fire-and-Forget: The Simplest Async Pattern

The simplest async logging pattern is fire-and-forget. The request handler calls a logging function. The function immediately returns. The log event is handled asynchronously in the background. The request handler never waits.

The implementation uses an in-memory queue. The logging function appends the log event to the queue in microseconds and returns. A separate background thread or worker process reads from the queue and writes logs to storage. If the queue is full, the logging function either drops the log or blocks briefly, depending on your overflow policy.

Fire-and-forget is sufficient for most logging. Logs are not transactional. If a log is lost, you lose observability for one request, but the user's request still succeeds. The trade-off is acceptable. You gain massive performance improvement in exchange for occasional log loss.

A ride-hailing service implemented fire-and-forget logging with a 10,000-event in-memory queue. Under normal load, the queue stayed under 500 events. Under peak load, it grew to 3,000 events. During an incident where their log storage was slow, the queue filled to capacity, and they started dropping logs. They lost 2 percent of logs during the incident. But user requests were unaffected. After the incident, they analyzed the 98 percent of logs they kept and identified the root cause. Fire-and-forget let them stay online while observability degraded gracefully.

## Buffered Writes for Reduced I/O

Instead of writing each log event individually, buffered logging accumulates events in memory and writes them in batches. If you receive 1,000 log events per second, instead of making 1,000 individual writes per second, you batch them into 10 writes of 100 events each. This reduces I/O load by 99 percent and improves write throughput.

Buffering works well when logs are sent to systems optimized for batch writes, like object storage, data warehouses, or message queues. These systems have high per-request overhead but high per-byte throughput. A single write of 100 events is far cheaper than 100 individual writes.

The trade-off is lag. Buffered writes add latency between when an event occurs and when it appears in storage. If you buffer for 10 seconds, logs are delayed by up to 10 seconds. For real-time alerting, this is unacceptable. For offline analysis and trend monitoring, it is fine. Most teams use short buffers, 1 to 5 seconds, to balance throughput and freshness.

A video streaming recommendation system buffered logs for 3 seconds or 500 events, whichever came first. Under high traffic, buffers flushed every 0.8 seconds because event count hit 500 quickly. Under low traffic, buffers flushed every 3 seconds. This kept lag bounded while maximizing batching efficiency. The team saved 60 percent on logging infrastructure costs because batching reduced write IOPS and storage API calls.

## Background Workers and Queue-Based Architectures

For high-scale systems, fire-and-forget with in-memory queues is not enough. If a process crashes, everything in its in-memory queue is lost. If you need higher durability, you use persistent message queues like Kafka, RabbitMQ, or AWS SQS.

Your application writes log events to the message queue. The queue persists events to disk and replicates across brokers. Background workers consume from the queue, process the events, and write to your log storage system. If a worker crashes, the queue retains the events and another worker picks them up. If your log storage is temporarily unavailable, events accumulate in the queue and are written when storage recovers.

This architecture decouples log production from log consumption. Your application is never blocked by logging. The queue absorbs traffic spikes. Workers scale independently of your application. If logging volume increases, you add more workers. If logging volume decreases, you scale down workers. Your application does not care.

A financial services platform used Kafka for async logging. Their application wrote 80,000 log events per second to Kafka. Ten background workers consumed from Kafka and wrote to their data warehouse. When their data warehouse had a maintenance window, Kafka buffered 6 hours of logs. When the warehouse came back online, workers drained the backlog in 40 minutes. The application was unaffected. Users experienced no degradation.

The durability guarantee of queue-based logging depends on your queue configuration. Kafka with replication factor 3 and acks equals all gives you extremely high durability, at the cost of higher write latency to Kafka. In-memory queues with no persistence give you zero durability, but microsecond latency. Most teams choose durable queues for critical logs like errors and cost events, and in-memory queues for high-volume non-critical logs like success events.

## Handling Log Loss Gracefully

Async logging will lose logs. Queues overflow. Workers crash. Networks partition. Accepting this is part of the design. The goal is not to prevent all log loss. The goal is to lose logs gracefully, to know when you are losing logs, and to ensure log loss does not break your application.

The first principle is fail-open for logging. If the logging system fails, the application continues. You do not return errors to users because logging is broken. You drop logs and move on. Users are more important than observability.

The second principle is monitoring log loss. Your logging system tracks how many events are produced, how many are written, and how many are dropped. If the drop rate exceeds a threshold, you alert. This tells you that observability is degraded, even if the application is fine.

The third principle is tiered durability. Critical logs like errors and high-value customer requests go through a durable path with Kafka and replication. Non-critical logs like sampled success events go through a best-effort path with in-memory queues. This ensures that the logs you need most are least likely to be lost.

A logistics company tiered their logging: error logs and premium customer logs went to Kafka with replication. Sampled success logs went to an in-memory queue. During a network partition, the in-memory queue filled and they dropped 18 percent of success logs. But they kept 100 percent of error logs and premium customer logs. This was the correct trade-off. When they debugged the incident afterward, they had all the critical data. The lost success logs did not matter.

## Circuit Breakers for Observability

A circuit breaker for logging prevents cascading failures. If the logging system is slow or unavailable, the circuit breaker opens and stops sending log events. This protects your application from being dragged down by a failing dependency.

The circuit breaker tracks logging failures: timeouts, errors, queue overflows. If the failure rate exceeds a threshold, the breaker opens. While open, all logging calls return immediately without attempting to log. After a cooldown period, the breaker enters a half-open state and allows a few logging calls through. If they succeed, the breaker closes and normal logging resumes. If they fail, the breaker opens again.

Circuit breakers ensure that a logging outage does not become an application outage. When your log storage is down, the breaker opens, logs are dropped, but your application stays fast and responsive. When log storage recovers, the breaker closes and logging resumes.

A social media company implemented circuit breakers for their logging. During a log database incident, the circuit breaker opened after 30 seconds. Logging stopped. The application stayed healthy. Users experienced no impact. The incident lasted 90 minutes. When the database recovered, the circuit breaker closed within 2 minutes and logging resumed. They lost 90 minutes of logs but served users without interruption. Without the circuit breaker, request latency would have spiked to multi-second timeouts and the application would have been effectively down.

## Measuring Logging Overhead

You cannot optimize what you do not measure. Logging overhead should be instrumented and monitored. The metrics that matter are: logging call duration at p50, p95, p99; queue depth over time; event drop rate; worker lag.

**Logging call duration** measures how long the application waits when making a logging call. For async logging, this should be under 1 millisecond at p50 and under 10 milliseconds at p99. If you see logging call duration growing, your queue is full or your async mechanism is broken.

**Queue depth** measures how many events are waiting in the queue. A growing queue depth means workers are not keeping up. If queue depth grows unbounded, you will eventually drop logs. Monitoring queue depth lets you scale workers before you hit capacity.

**Event drop rate** measures what percentage of log events are dropped due to overflow, failures, or circuit breakers. A small drop rate, under 1 percent, is acceptable. A drop rate above 5 percent means your logging system is undersized or broken.

**Worker lag** measures how far behind workers are. If workers are processing events from 30 seconds ago, your lag is 30 seconds. High lag delays alerting and dashboards. Monitoring lag helps you size your worker pool.

A marketplace platform monitored all four metrics. They set an alert for logging call duration p99 above 20 milliseconds. When the alert fired, they investigated and found that their in-memory queue was undersized. They increased queue size from 5,000 to 20,000 events. Logging call duration dropped back under 5 milliseconds. The alert caught a performance degradation before users noticed.

## The Trade-Off Between Freshness and Performance

Async logging introduces lag. Events do not appear in logs immediately. They appear after queue transit time, worker processing time, and batch write time. This lag ranges from milliseconds to seconds depending on your architecture.

For offline analysis, lag does not matter. If you are analyzing yesterday's logs to understand user behavior, whether the logs arrived 100 milliseconds or 5 seconds after the event is irrelevant. For real-time alerting, lag matters. If your alerting system needs to detect error rate spikes within 10 seconds, and your logging lag is 30 seconds, you cannot meet your SLO.

Most teams split logging into two paths: a fast path for critical events that need low-latency alerting, and a slow path for bulk events that tolerate higher lag. The fast path uses low-latency queues, immediate writes, and no batching. The slow path uses batching, buffering, and high-throughput writes.

A fraud detection system sent fraud alerts through a fast path with less than 500-millisecond lag. They sent all other logs through a slow path with 10-second lag. Fraud alerts triggered automated actions, so low lag was critical. Other logs were for analysis, so lag was acceptable. The two-path design let them optimize for both latency and cost.

## Structured Logging for Efficient Parsing

Async logging systems work best with structured logs. Structured logs use a consistent schema, usually JSON, with typed fields. Unstructured logs are plain text, requiring regex parsing and manual interpretation. Structured logs are machine-readable, queryable, and aggregatable.

When you log a request, you emit a structured event with fields: timestamp, user ID, feature name, model name, latency, cost, error status, prompt length, response length. Your background workers parse these fields without ambiguity. Your analytics system indexes on these fields. Your dashboards query by these fields.

Structured logging also makes it easy to evolve your schema. You add new fields without breaking existing logs. Old logs do not have the new field, so you handle it as null. New logs include the field. This forward compatibility is critical for long-lived systems.

A healthcare platform used structured logging from day one. Every log event was a JSON object with 30 standard fields. When they added cost attribution, they added three new fields: model name, input token count, output token count. Old logs did not have these fields. New logs did. Their analytics system handled both gracefully. They could analyze cost trends from the moment they added the fields, without backfilling old logs.

## When to Use Synchronous Logging

Async logging is the default. But there are cases where synchronous logging is correct.

**Compliance-critical logs** — If regulations require that every action is logged before it takes effect, you log synchronously. A financial trading system might be required to log every trade before executing it. If the log write fails, the trade does not execute. This is synchronous by design. The user waits for the log write. But the cost is necessary for regulatory compliance.

**Critical state changes** — If a log represents a state change that must be durable before proceeding, you log synchronously. An access control system that logs permission grants might require the log to be written before the permission takes effect. If the system crashes after granting permission but before writing the log, there is no audit trail. Synchronous logging ensures the log is durable before the state change commits.

**Low-volume, high-value events** — If you log 10 events per hour and each event is critical, synchronous logging is fine. The performance cost is negligible at that volume. A system that logs only administrative actions, like creating a new user or changing system configuration, can log synchronously without impacting user experience.

For everything else, async logging is the right choice. It decouples observability from performance, protects your application from logging system failures, and scales to millions of events per day without slowing down users.

Next, we turn to trace correlation and request linking, the techniques that let you follow a single user request across multiple services, multiple model calls, and multiple retries, so you can debug complex distributed failures.
