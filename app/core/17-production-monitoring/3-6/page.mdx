# 3.6 — Confidence and Uncertainty Metrics: When the Model Knows It Does Not Know

The most dangerous outputs are not the ones that are obviously wrong. They are the ones that are wrong and delivered with confidence. A model that says "I am not sure, but here is my best guess" is useful. A model that says "The answer is definitely X" when the answer is actually Y is a liability. The teams that build trustworthy AI systems measure not just correctness but confidence calibration — whether the model's expressed certainty matches the actual likelihood that its answer is correct.

Confidence and uncertainty metrics answer the question: does the model know when it does not know? A well-calibrated model expresses high confidence on questions it answers correctly and low confidence on questions it answers incorrectly. A poorly calibrated model is confident even when wrong, or uncertain even when right. Calibration is measurable, trainable, and essential for any AI system where incorrect answers have consequences. You measure calibration in production by logging model confidence scores, comparing those scores to actual correctness, and detecting when the relationship breaks down.

## Sources of Confidence Information

Confidence comes from multiple sources. Some models expose internal uncertainty through logprobs or token probabilities. Some models generate explicit confidence statements in their responses. Some systems infer confidence from external signals like retrieval relevance or input ambiguity.

**Token-level logprobs** are the most direct measure of model uncertainty. For each generated token, most language models can return the log probability of that token given the preceding context. Low logprobs indicate the model was uncertain about that token. High logprobs indicate confidence. Aggregate token-level logprobs into response-level confidence scores by averaging, taking the minimum, or using entropy-based measures.

A legal research assistant uses GPT-5.1 to generate case summaries. For each summary, the system logs the mean token logprob and the minimum token logprob. Responses with mean logprob above negative 0.8 are considered high confidence. Responses with mean logprob below negative 2.0 are flagged as low confidence. Analysis shows that low-confidence responses have a thirty-four percent error rate, while high-confidence responses have a six percent error rate. Logprobs are a strong predictor of correctness.

Not all models expose logprobs. Claude models, as of 2026, do not return per-token probabilities through public APIs. Closed models often restrict access to internal uncertainty measures. For these models, you must infer confidence from other signals.

**Explicit confidence statements** are generated when you prompt the model to express its certainty. After generating an answer, ask the model: "How confident are you in this answer? Rate from 1 to 5." Or use chain-of-thought prompting: "Explain your reasoning and state your confidence level." The model generates a self-assessed confidence score along with the answer.

Self-assessed confidence is less reliable than logprobs. Models are trained to be helpful and sometimes express confidence even when uncertain. But self-assessed confidence is better than no confidence signal, and it works with models that do not expose logprobs. The key is calibration. Measure how well self-assessed confidence correlates with actual correctness. If the model says it is ninety percent confident, is it correct ninety percent of the time? If not, recalibrate.

A customer support chatbot prompts the model to rate its confidence on every response. Responses labeled "high confidence" are sent directly to users. Responses labeled "medium confidence" are sent with a disclaimer. Responses labeled "low confidence" are escalated to human agents. Initial deployment shows the model is overconfident — "high confidence" responses are correct only eighty-three percent of the time, not the expected ninety-five percent. The team recalibrates by lowering the threshold for what counts as high confidence.

**Retrieval relevance scores** are confidence proxies for RAG systems. If the retrieved documents are highly relevant to the query, the model is likely to generate a correct, grounded response. If retrieved documents are marginally relevant, the model is working with weak evidence. Retrieval scores are leading indicators of response confidence.

A medical Q and A system retrieves documents from clinical guidelines and research papers. For each query, the system logs the top retrieval score, the mean retrieval score of the top five documents, and the gap between the top score and the second score. Responses generated from high-relevance retrievals have an eight percent error rate. Responses generated from low-relevance retrievals have a thirty-one percent error rate. Retrieval relevance predicts correctness even before the model generates the response.

## Measuring Confidence Calibration in Production

Confidence is only useful if it is calibrated. A model that assigns a ninety percent confidence score should be correct ninety percent of the time. A model that assigns a fifty percent confidence score should be correct fifty percent of the time. Calibration measures whether confidence scores match reality.

**Collect ground truth correctness labels on a sample of production responses**. Use human review, automated verification, or user feedback to determine whether responses are correct. Log the confidence score for each response. Plot predicted confidence against observed correctness. A perfectly calibrated model produces a straight diagonal line. An overconfident model sits above the line. An underconfident model sits below the line.

A financial advice chatbot logs self-assessed confidence scores on all responses. Each day, human reviewers evaluate one hundred responses for correctness. Predicted confidence is plotted against actual correctness. At seventy percent predicted confidence, the model is correct sixty-two percent of the time. At ninety percent predicted confidence, the model is correct seventy-eight percent of the time. The model is overconfident across the board. The team recalibrates by applying a correction function to raw confidence scores before displaying them to users.

**Track calibration error as a time-series metric**. Calibration is not static. A model that was well-calibrated at deployment can become miscalibrated as data drifts, prompts change, or user behavior evolves. Compute calibration error weekly or daily. Expected Calibration Error is a standard metric: divide confidence scores into bins, compute the mean predicted confidence and the actual correctness rate in each bin, and measure the difference. Rising calibration error is an early warning that the model's uncertainty estimates are degrading.

A legal contract review tool tracks calibration error every week. For six months, ECE hovers around 0.04 — well-calibrated. In January 2026, ECE jumps to 0.12 over two weeks. Investigation reveals a prompt update that caused the model to express higher confidence without improving actual accuracy. The prompt is reverted, and calibration returns to baseline.

**Segment calibration by context**. A model might be well-calibrated on simple questions and poorly calibrated on complex questions. A model might be well-calibrated for frequent query types and poorly calibrated for rare query types. Track calibration separately for different user segments, prompt categories, and response characteristics. Calibration errors are often localized to specific contexts.

A customer support chatbot is well-calibrated on billing questions — predicted confidence matches actual correctness closely. It is poorly calibrated on technical troubleshooting questions — it expresses high confidence even when giving incorrect instructions. Segmented calibration reveals the problem. The team fine-tunes the model specifically on technical troubleshooting with ground truth labels. Calibration improves in that domain while remaining stable in others.

## Using Confidence to Route Decisions

Confidence scores enable smarter routing. High-confidence responses can be sent directly to users. Low-confidence responses can be escalated, hedged, or blocked.

**Automatic escalation based on confidence thresholds**. Define a confidence threshold below which responses are not safe to send without human review. In a customer support copilot, responses with confidence below seventy percent are flagged for agent review. In a medical chatbot, responses with confidence below eighty-five percent are flagged with a disclaimer: "This is my best understanding, but please consult a healthcare professional." Thresholds turn continuous confidence scores into discrete decisions.

Thresholds must be tuned to balance risk and cost. A very high threshold minimizes incorrect responses but escalates too much, overwhelming human reviewers or frustrating users with excessive hedging. A very low threshold minimizes escalations but allows incorrect responses through. The right threshold depends on the cost of errors versus the cost of escalation.

A tax preparation assistant sets different confidence thresholds for different advice types. Advice on basic deductions requires seventy percent confidence. Advice on complex business expenses requires ninety percent confidence. Advice on audit risk requires ninety-five percent confidence. The thresholds reflect the stakes. Low-stakes advice tolerates more uncertainty. High-stakes advice demands high confidence.

**Hedging language based on confidence levels**. Instead of blocking low-confidence responses, deliver them with appropriate hedging. A high-confidence response says "The answer is X." A medium-confidence response says "Based on available information, the answer is likely X." A low-confidence response says "I am not certain, but X is a possibility." Hedging matches language to uncertainty.

Users trust systems that acknowledge uncertainty more than systems that are always confident. A model that says "I am not sure" when it is genuinely uncertain is more trustworthy than a model that always sounds authoritative. Confidence-aware hedging builds trust.

A legal research assistant generates responses with confidence-calibrated hedging. High-confidence responses cite case law definitively: "The precedent is established in Smith v. Jones." Medium-confidence responses add qualifiers: "The most relevant precedent appears to be Smith v. Jones." Low-confidence responses acknowledge gaps: "I found limited precedent on this issue, but Smith v. Jones may be relevant." Users report higher trust in the system after hedging is introduced because the system no longer sounds overconfident on ambiguous questions.

**Fallback strategies for low-confidence scenarios**. When confidence is low, the system can offer alternatives instead of committing to a single answer. Present multiple possibilities. Suggest the user refine their query. Offer to escalate to a human. Low-confidence responses do not have to be failures. They can be opportunities to gather more information or provide more nuanced answers.

A travel planning assistant detects low confidence when the user's query is ambiguous. Instead of guessing, the system responds: "I found several options. Are you interested in budget travel, luxury travel, or something in between?" The user clarifies, and the system responds with high confidence. Low-confidence detection triggers clarification, turning a potential error into a better user experience.

## Monitoring Confidence Drift

Confidence distributions change over time. A model that initially expressed appropriate uncertainty might become overconfident as prompts evolve or underconfident as data drifts. Monitoring confidence drift is as important as monitoring quality drift.

**Track the distribution of confidence scores over time**. Plot mean confidence, median confidence, and the percentage of responses in high, medium, and low confidence buckets. A shift toward higher confidence without a corresponding improvement in correctness indicates overconfidence drift. A shift toward lower confidence without a corresponding increase in errors indicates underconfidence drift.

A financial advice chatbot tracks the percentage of responses in each confidence category. In September 2025, fifteen percent of responses are low confidence, sixty percent are medium confidence, twenty-five percent are high confidence. By December 2025, the distribution shifts to eight percent low, forty-eight percent medium, forty-four percent high. Confidence is increasing. Correctness is not. The model is becoming overconfident. Investigation reveals that a prompt template change removed explicit instructions to express uncertainty, causing the model to default to confident language even on ambiguous queries.

**Correlate confidence drift with deployment events**. Model updates, prompt changes, and data pipeline changes can all affect confidence calibration. Mark these events on your confidence time series. Confidence shifts that coincide with deployments are likely caused by those deployments. Confidence shifts that occur gradually over weeks without corresponding deployments suggest external drift.

A medical chatbot experiences a gradual increase in low-confidence responses over two months. No model updates or prompt changes occurred during that period. Investigation reveals that user queries became more complex and more ambiguous over time. The model correctly detected that it was being asked harder questions. The confidence drift was appropriate. The team adjusts by improving the model's ability to handle complex queries rather than recalibrating confidence thresholds.

**Alert on sudden confidence shifts**. A ten percentage point increase in high-confidence responses over three days is abnormal. A sudden increase in low-confidence responses is equally abnormal. Sudden shifts indicate something broke. Either the model's calibration broke, or the input distribution changed dramatically. Both require investigation.

A customer service copilot alerts when the percentage of high-confidence responses drops from eighty-two percent to sixty-three percent in two days. Investigation reveals a surge in adversarial traffic — users intentionally submitting ambiguous or nonsensical queries. The model correctly detected uncertainty. The alert prompts the team to investigate the traffic source and deploy input filtering.

## Training Models to Express Uncertainty

Confidence is not purely a measurement problem. It is also a training problem. Models can be fine-tuned to express uncertainty more accurately.

**Fine-tune on examples with explicit confidence labels**. Collect a dataset of prompts, responses, and ground truth correctness labels. Train the model to generate confidence-aware responses. Show examples where the model says "I am confident the answer is X" and X is correct. Show examples where the model says "I am uncertain, but I think the answer is Y" and Y is incorrect. The model learns to associate its internal uncertainty with explicit language.

A legal research assistant fine-tunes Claude Sonnet 4.5 on three thousand query-response pairs labeled for correctness. The fine-tuned model generates responses that include explicit confidence statements. Post-fine-tuning, self-assessed confidence correlates with actual correctness at 0.73, up from 0.58 before fine-tuning. The model learned to recognize when it was guessing versus when it had strong evidence.

**Use reinforcement learning from human feedback to reward well-calibrated uncertainty**. Train the model not just to be correct but to be correctly confident. Responses that are correct and confident receive high reward. Responses that are incorrect and tentative receive moderate reward — the model was wrong, but it acknowledged uncertainty. Responses that are incorrect and confident receive low reward — the model was wrong and overconfident. Over time, the model learns to hedge when uncertain.

A healthcare advice chatbot uses RLHF to improve calibration. Human reviewers rate responses on two dimensions: correctness and confidence appropriateness. A response that says "You should definitely do X" when X is incorrect receives the lowest rating. A response that says "I am not certain, but you might consider X" when X is incorrect receives a higher rating. The model learns that inappropriate confidence is worse than admitting uncertainty.

**Prompt engineering for uncertainty expression**. Before fine-tuning, prompt the model to express uncertainty explicitly. Include instructions like "If you are uncertain, say so" or "Rate your confidence on a scale from 1 to 5" in system prompts. Simple prompt changes can significantly improve confidence expression, especially in models that are already trained on uncertainty-aware datasets.

A tax preparation assistant adds a confidence prompt to every request: "After answering, state your confidence level and explain any sources of uncertainty." The model begins generating responses like "I am eighty percent confident in this answer. The main source of uncertainty is that your question involves a recent tax law change, and guidance is still evolving." The prompt alone improves user trust without requiring model retraining.

Confidence and uncertainty metrics are the difference between a model that provides answers and a model that provides trustworthy answers. Correctness is necessary but not sufficient. A system that is correct ninety percent of the time but expresses one hundred percent confidence is dangerous. A system that is correct ninety percent of the time and correctly identifies the uncertain ten percent is trustworthy. Confidence calibration is measurable, improvable, and essential for production AI systems that users depend on.

Next, we explore groundedness and hallucination detection — how to identify when the model generates claims that are not supported by evidence.

