# 4.5 — Statistical Methods for Drift Detection: KS Tests, PSI, Jensen-Shannon

Most teams start drift monitoring by staring at dashboards and guessing whether a distribution looks different. This works for dramatic shifts — if your median input length doubles, you will notice. It fails for subtle shifts — if your 75th percentile shifts by 12 percent over six weeks, you will miss it until performance degrades. Statistical tests quantify drift. They give you a number that represents how much a distribution has changed, and they tell you whether that change is likely to be random noise or meaningful signal. The three most useful tests for production AI monitoring are the Kolmogorov-Smirnov test for continuous distributions, the Population Stability Index for bucketed distributions, and Jensen-Shannon divergence for probability distributions. Each has different strengths and appropriate use cases. Understanding when to use which test is the difference between drift monitoring that catches real problems and drift monitoring that generates alerts you ignore.

## The Kolmogorov-Smirnov Test

The KS test measures the maximum distance between two cumulative distribution functions. You have a baseline distribution — token counts from last month, for example. You have a current distribution — token counts from this week. The KS test computes the largest vertical gap between the two cumulative distributions and returns a statistic between 0 and 1. A KS statistic of 0 means the distributions are identical. A KS statistic of 1 means they have no overlap. In practice, values above 0.1 indicate meaningful drift for most use cases.

The KS test works best for continuous numeric features: token counts, embedding dimensions, confidence scores, latency measurements, any value that can take many different numeric values. It is distribution-agnostic — it does not assume your data is normally distributed or follows any particular shape. This makes it robust for real-world AI systems where distributions are often multimodal, skewed, or irregular. The test is also sensitive to both location shifts and shape changes. If your median shifts but your variance stays the same, the KS test detects it. If your median stays the same but your variance doubles, the KS test detects that too.

A financial risk scoring system monitoring input drift on loan amount distributions used the KS test to detect a shift from small personal loans to larger business loans. Their baseline distribution from January through March 2025 had a median loan amount of $18,000. In April, the KS statistic jumped from typical values around 0.03 to 0.19. Investigation revealed that a product change had opened business loan applications to a new customer segment. The input distribution had fundamentally shifted. The risk model, trained on personal loan data, was miscalibrated for business loans. The KS test caught this within the first week of the shift. Without statistical testing, the team might not have noticed until performance metrics degraded weeks later.

The KS test comes with a p-value that tells you the probability the observed difference could occur by chance. A p-value below 0.05 is the standard threshold for statistical significance. But in production monitoring, do not rely solely on p-values. With large sample sizes, even tiny shifts become statistically significant. With small sample sizes, large shifts might not reach significance. Use the KS statistic itself as the primary signal. A KS value above 0.1 deserves investigation regardless of p-value. A KS value below 0.05 is usually noise unless you have other corroborating signals.

## The Population Stability Index

The Population Stability Index measures drift for categorical or bucketed data. You divide your continuous variable into bins — deciles are common — or you use natural categories for discrete variables. For each bin, you compute the proportion of samples in that bin for your baseline period and your current period. The PSI formula is the sum across all bins of the difference in proportions, weighted by the log ratio of proportions. The result is a single number. PSI below 0.1 indicates minimal drift. PSI between 0.1 and 0.25 indicates moderate drift worth monitoring. PSI above 0.25 indicates significant drift requiring action.

PSI is the industry standard for monitoring drift in credit scoring, fraud detection, and other financial models because it handles categorical predictions naturally. A fraud model that outputs risk scores from 1 to 10 can bucket the score distribution into deciles and track how the distribution across buckets shifts over time. PSI is also useful for monitoring classification label distributions. A customer support intent classifier with eight intent categories computes PSI by treating each intent as a bin. If the proportion of queries classified as "billing issue" goes from 14 percent to 22 percent, PSI captures that shift.

A healthcare documentation system tracking output drift on clinical note length used PSI by bucketing note lengths into quintiles: 0-50 words, 51-120 words, 121-200 words, 201-350 words, and 351-plus words. Their baseline distribution from June 2025 showed 8 percent in the first bucket, 31 percent in the second, 38 percent in the third, 18 percent in the fourth, and 5 percent in the fifth. By September, the distribution had shifted to 12 percent, 28 percent, 29 percent, 22 percent, and 9 percent. PSI calculated at 0.18 — moderate drift. Investigation revealed that a model update had made outputs more verbose. The team adjusted their prompt to counteract the shift and restored the distribution to baseline.

The challenge with PSI is choosing the right number of buckets. Too few buckets and you lose resolution — a shift within a bucket goes undetected. Too many buckets and you get noisy estimates because some buckets have very few samples. Ten buckets is a reasonable default for continuous variables. For categorical variables, use the natural categories. PSI also requires that every bucket in the baseline has at least some samples. If a bucket has zero baseline samples but gains samples in the current period, PSI becomes undefined. Handle this by combining sparse buckets or using a small smoothing constant.

## Jensen-Shannon Divergence

Jensen-Shannon divergence measures the similarity between two probability distributions. Unlike KS tests and PSI, which are primarily diagnostic tools, JS divergence is a true distance metric. It ranges from 0 to 1, where 0 means identical distributions and 1 means completely disjoint. JS divergence is symmetric — the distance from distribution A to distribution B is the same as from B to A. It is also bounded, which makes thresholds interpretable. A JS divergence above 0.15 typically indicates meaningful drift. Above 0.3 indicates severe drift.

JS divergence is particularly useful for monitoring embedding distributions and high-dimensional feature spaces. If you are embedding inputs with a sentence transformer and monitoring drift in embedding space, JS divergence computed on the distribution of embeddings gives you a single number that captures semantic drift. It is also useful for comparing token distributions in text models. If you track the frequency distribution of the top 1000 tokens in your baseline period and compare it to the current period, JS divergence quantifies vocabulary drift.

A content moderation system using Llama 4 Maverick embeddings to detect harmful content tracked JS divergence on embedding centroids across weekly windows. Their baseline JS divergence week-over-week was typically below 0.08, indicating stable input distributions. In late 2025, JS divergence jumped to 0.23 in a single week. Investigation revealed a coordinated spam campaign using new linguistic patterns the model had not seen in training. The embedding space had shifted because the semantic content of inputs had changed. The team updated their moderation rules and added the new patterns to their training set. JS divergence returned to baseline within two weeks.

JS divergence is more computationally expensive than KS tests or PSI because it requires density estimation. For discrete distributions like token frequencies or classification labels, computation is straightforward — just normalize counts into probabilities. For continuous distributions, you need to bin the data or estimate densities with kernel methods. The added cost is usually worth it when you need a true distance metric for comparison across multiple distributions or over time. JS divergence also composes well with clustering and anomaly detection — you can use it to measure how far a current distribution is from a set of known-good distributions.

## Choosing the Right Test

Use the **KS test** when you have continuous numeric features and you want a simple, interpretable measure of whether the overall distribution has shifted. KS tests are fast, require no parameterization, and work on raw data. They are the default choice for monitoring input features like token counts, confidence scores, or numeric metadata. The downside is that KS tests are univariate — you run one test per feature. If you have 50 features, you run 50 tests. This multiplies your false positive risk unless you adjust significance thresholds.

Use **PSI** when you have categorical outputs or when you want to track shifts in bucketed distributions with interpretability for business stakeholders. PSI is standard in regulated industries because auditors understand it. It is also useful when you want to monitor not just that drift occurred but where in the distribution it occurred — which buckets gained mass and which lost it. The downside is that PSI requires choosing bucket boundaries, and poor choices can mask drift or create false positives.

Use **JS divergence** when you have high-dimensional data, when you need a metric that composes across multiple distributions, or when you are monitoring embeddings or token distributions. JS divergence is the most mathematically principled of the three, but it is also the least interpretable for non-technical stakeholders. A KS statistic of 0.15 is easier to explain than a JS divergence of 0.15, even though both indicate drift. JS divergence shines when you are building automated systems that need to compare many distributions programmatically.

## Multi-Feature Drift Detection

Most models use dozens or hundreds of features. Monitoring drift feature-by-feature creates alert fatigue. If you run a KS test on 100 features every day with a significance threshold of 0.05, you will get 5 false positives per day even if no drift is happening. The solution is to combine signals. Compute drift statistics for all features, but alert only when multiple features drift simultaneously or when a feature drifts for multiple consecutive days. A single feature drifting one day is noise. Ten features drifting the same day is signal. One feature drifting for seven consecutive days is signal.

Use dimensionality reduction to monitor high-dimensional drift with a single metric. Compute the principal components of your feature space on the baseline distribution. Project current data into that principal component space. Measure drift on the first few principal components using KS tests or JS divergence. This collapses hundreds of features into a handful of metrics while preserving the directions of maximum variance. If drift is happening, it will show up in the principal component projections.

Use multivariate distance metrics for holistic drift assessment. Compute the Mahalanobis distance or the Maximum Mean Discrepancy between baseline and current distributions in the full feature space. These metrics account for correlations between features and give you a single score representing overall distributional shift. The challenge is interpretability — if the metric spikes, you still need to drill down to individual features to understand what changed. Use multivariate metrics for alerting, then use univariate metrics for diagnosis.

## Thresholds and Alerting Logic

No universal threshold works for all systems. A KS statistic of 0.15 might be catastrophic drift for one model and normal variance for another. The right threshold depends on your baseline variance, your tolerance for false positives, and the cost of missing real drift. Start by measuring drift statistics during a period you know was stable. Compute the distribution of KS statistics, PSI values, or JS divergences across that period. Use the 95th or 99th percentile of that distribution as your threshold. This ensures your alerts fire only when drift exceeds historical variance.

Implement tiered alerting. A KS statistic between 0.1 and 0.15 triggers a low-priority notification. A KS statistic above 0.15 triggers a medium-priority alert. A KS statistic above 0.25 or drift sustained for more than three days triggers a high-priority alert requiring immediate investigation. Tiered alerting reduces alert fatigue while ensuring critical drift gets attention. Most teams ignore alerts if they are all the same severity. Differentiation drives action.

Use adaptive thresholds for systems with known cyclical patterns. If your input distribution changes predictably every Monday or every quarter, your baseline should reflect that. Compute separate baselines for each day of the week or each season. Compare Mondays to Mondays, not Mondays to Fridays. Compare Q4 to last Q4, not Q4 to Q2. Adaptive thresholds prevent recurring patterns from being misidentified as drift while still catching genuine anomalies within each cycle.

## When Tests Disagree

Sometimes different tests give conflicting signals. KS test shows no drift, but PSI shows moderate drift. JS divergence shows drift, but business metrics look stable. When tests disagree, investigate the mismatch. KS test is sensitive to tail behavior, while PSI is sensitive to middle-of-distribution shifts. If KS is quiet but PSI is alerting, check whether the drift is concentrated in the bulk of the distribution. If JS divergence is alerting but KS is quiet, check whether the drift is multivariate — correlated features shifting together in ways that KS tests on individual features do not catch.

Use multiple tests as complementary signals, not interchangeable ones. KS test, PSI, and JS divergence measure different aspects of distributional change. A mature drift monitoring system runs all three and uses the combination to triangulate. If all three alert, drift is certain. If one alerts, investigate. If none alert but business metrics degrade, you have concept drift — the inputs and outputs look stable, but the relationship between them has changed. Statistical tests catch input and output drift. They do not catch concept drift. That requires outcome tracking, which we covered in the previous subchapter.

The next subchapter covers embedding space monitoring — how to detect semantic drift in high-dimensional representations that statistical tests struggle to measure directly.

