# 7.6 — Escalation Policies: When Alerts Go Unanswered

The critical alert fired at 2:47 AM. The primary on-call engineer was asleep with their phone on silent — a policy violation, but it happened. Five minutes passed. The alert escalated to the backup on-call engineer. The backup was on a flight to a conference, phone in airplane mode. Five more minutes passed. The alert escalated to the engineering manager. The manager was awake, saw the alert, acknowledged it, but had not touched the codebase in two years and could not diagnose the issue. By the time someone with actual debugging capability joined at 3:04 AM, the model had been serving corrupted predictions for seventeen minutes. Three hundred fourteen customers received incorrect risk assessments. The escalation policy worked exactly as designed. The outcome was catastrophic because the policy escalated to authority instead of capability.

Escalation policies determine what happens when alerts go unacknowledged or unresolved. They are the safety net that ensures incidents are not ignored. But escalation is not just about moving alerts up a reporting chain until someone with sufficient seniority pays attention. Effective escalation routes alerts to people who can actually investigate and resolve the issue, coordinates multi-person response when needed, and distinguishes between acknowledgment failures and resolution failures.

## Time-Based Escalation: Handling Unacknowledged Alerts

The most basic escalation is time-based acknowledgment escalation. If the primary recipient does not acknowledge within a threshold, notify the secondary recipient. If the secondary does not acknowledge, notify the tertiary. The time thresholds are short — typically three to five minutes per tier — because unacknowledged critical alerts represent failures in the on-call system, not just delayed responses.

A logistics AI company configured three-tier acknowledgment escalation with progressive time thresholds. Primary on-call had five minutes to acknowledge. If they did not, the backup on-call received the alert and had three minutes to acknowledge. If neither acknowledged, the alert went to the engineering manager and the director of engineering simultaneously, with a one-minute acknowledgment requirement. The shortening thresholds reflected increasing urgency — if two engineers failed to acknowledge, something was wrong with the on-call process, not just with individual availability.

The key question is: who should be in the escalation chain? The obvious answer is the organizational hierarchy — engineer to senior engineer to engineering manager to director. This is wrong. The escalation chain should follow capability and availability, not authority. A senior engineer who is on vacation should not be in the escalation chain that week. A manager who has never operated the system should not be the last line of response. The escalation chain must be composed of people who can actually respond effectively.

A fintech company learned this during a production incident where the escalation chain was purely hierarchical. The primary engineer did not acknowledge. The alert escalated to the team lead, who was at a family event and acknowledged but could not investigate meaningfully. The alert escalated to the engineering manager, who acknowledged, read the runbook, and determined the issue required database access they did not have. The alert escalated to the director of engineering, who acknowledged and started paging random team members until someone with database access answered. Total time to effective response: thirty-eight minutes. The escalation policy sent the alert to four people in sequence. None of them could fix the problem. The person who eventually fixed it was not in the escalation chain at all.

They redesigned escalation to follow capability. The primary on-call engineer was always someone with full system access and recent operational experience. The backup was also a full-capability engineer from the same team. The third tier was the on-call engineer from a related team who had been cross-trained on the system. Only the fourth tier was management — and management's role was to coordinate external resources if the incident exceeded the team's capacity, not to debug the system themselves. The escalation chain prioritized getting the alert to someone who could act, then getting leadership visibility. The order matters.

Acknowledgment escalation must include notification of the escalation to the original recipient. If the primary engineer does not acknowledge within five minutes and the alert escalates, the primary engineer should receive a notification: "you did not acknowledge within threshold, this alert has escalated to backup." This serves two purposes. First, it alerts the primary that their acknowledgment failed — perhaps they thought they acknowledged but the action did not register. Second, it creates accountability — the escalation is not silent. If an engineer consistently fails to acknowledge, the pattern is visible.

## Capability-Based Escalation: Routing to Skills When Primary Cannot Resolve

Acknowledgment escalation handles situations where no one responds. Resolution escalation handles situations where someone responds but cannot resolve the issue. The engineer acknowledges the alert, investigates, and determines they lack the expertise to fix it. Manual escalation at that point is inefficient. The alerting system should support assisted escalation — the engineer can escalate to a specialist with one action.

A healthcare AI company implemented capability-based escalation through their incident management interface. When an engineer acknowledged an alert, the interface displayed likely escalation paths based on alert type. A model accuracy alert offered escalation to the ML team or the data engineering team. An API performance alert offered escalation to the infrastructure team or the database team. The engineer selected the relevant escalation path, added context about what they had already tried, and triggered the escalation. The specialist received the alert with full history and context.

The escalation was not automatic — the engineer had to judge that escalation was necessary. But the system made escalation low-friction. Without assisted escalation, the engineer would need to look up who to contact, find their phone number or Slack handle, copy alert details, and manually reach out. That process takes time and introduces errors. Assisted escalation made it a single button press. The faster escalation happens, the faster incidents are resolved.

Capability-based escalation requires maintaining a mapping of alert types to required skills. A model serving timeout might require ML platform expertise, container orchestration expertise, or network debugging expertise depending on root cause. The alert content provides hints — if CPU utilization is high, escalate to platform team for optimization analysis; if network errors are elevated, escalate to infrastructure for connectivity debugging. The hints are not deterministic. They are suggestions that the responding engineer can accept or override based on investigation findings.

A media AI platform used machine learning to improve escalation suggestions. They trained a model on historical incidents: alert type, initial investigation findings, eventual resolution, which team resolved it. The model predicted which team was most likely to resolve the current incident based on pattern matching against past incidents. The prediction was displayed as a suggested escalation path. Over six months, the suggestion accuracy was seventy-three percent — not perfect, but better than random guessing. Engineers used the suggestion as a starting point and refined based on their own assessment.

The risk with assisted escalation is that it becomes a crutch. Engineers acknowledge alerts, spend thirty seconds looking at dashboards, and immediately escalate rather than investing in serious investigation. This creates load on specialist teams and degrades the effectiveness of primary on-call. The mitigation is to track escalation patterns. If an engineer escalates more than forty percent of their alerts, that indicates insufficient training or poor alert routing. The alert might be reaching the wrong primary team. Or the primary engineer might need more training to handle common failure modes. Either way, high escalation rates are a signal that the system needs adjustment.

## Multi-Tier Escalation: Coordinating Complex Incidents

Some incidents require multiple people or teams simultaneously. A model failure that affects multiple downstream services, a security breach that spans infrastructure and application layers, a data corruption that requires data engineering, ML, and compliance involvement. These incidents need coordinated response from the start, not sequential escalation. Multi-tier escalation pages multiple recipients in parallel based on incident characteristics.

An insurance company configured multi-tier escalation for high-severity alerts that indicated cross-system impact. An alert classified as "multi-service outage" paged three teams simultaneously: the API team, the model team, and the infrastructure team. Each team received the alert with context indicating their likely relevance. The teams joined a shared incident channel automatically created by the incident management system. Coordination happened in real-time. The API team investigated user-facing symptoms. The model team checked model health. The infrastructure team verified compute and network layers. The parallel investigation was faster than sequential escalation would have been.

The challenge is avoiding over-notification. If every complex incident pages ten teams, most of those teams will find they are not needed. That creates alert fatigue and trains teams to ignore multi-tier pages. The threshold for multi-tier escalation must be high. Only incidents with clear evidence of multi-system impact should trigger parallel notification. For incidents where impact is uncertain, start with single-team notification and escalate as investigation reveals scope.

Multi-tier escalation should include a designated incident commander — someone responsible for coordinating the response, making decisions, and communicating status. Without coordination, parallel response becomes chaotic. Three teams investigate independently, reach conflicting conclusions, and take contradictory actions. The incident commander does not need to be the most senior person. They need to be someone with communication skills and authority to make calls. A fintech company rotated incident commander responsibility across senior engineers. The on-call incident commander received multi-tier alerts and was responsible for convening teams, managing the incident channel, and driving toward resolution.

The incident commander role prevents diffusion of responsibility. When multiple teams receive an alert, each team might assume another team is leading the response. The result is that no one takes ownership. Designating a commander explicitly assigns ownership. The commander is responsible for ensuring response happens, even if the commander is not doing the technical work themselves. This is a coordination role, not a hero role. The best incident commanders spend most of their time facilitating others, not typing commands.

## Escalation to External Parties: Vendors, Customers, and Executives

Some incidents require notification beyond the engineering team. Model failures that violate contractual SLAs must be reported to affected customers. Security incidents must be reported to compliance teams and potentially regulators. Infrastructure failures caused by cloud provider issues require engagement with vendor support. Escalation policies must include rules for when and how to notify external parties.

A healthcare AI company configured automatic escalation to their compliance team for any alert indicating potential HIPAA violations. Alerts about unauthorized data access, data exfiltration attempts, or audit log failures triggered immediate notifications to the compliance officer and legal counsel. The technical team resolved the incident. The compliance team assessed regulatory impact and determined reporting obligations. The escalation happened in parallel — compliance did not wait for technical resolution to begin their assessment.

External escalation timing is critical. Notify too early and you create alarm before understanding severity. Notify too late and you violate reporting obligations or damage trust. The general pattern is to notify internal escalation tiers immediately but delay external escalation until sufficient information is available. A transportation AI company waited fifteen minutes after alert acknowledgment before escalating to customer-facing teams. The delay gave the technical team time to assess scope and prepare clear communication. Customers received accurate information about impact and estimated resolution time, not panicked notifications about unconfirmed issues.

Executive escalation follows similar logic. Engineering leadership should be notified of major incidents early, but not every Page-level alert justifies waking a VP at 3 AM. The threshold for executive escalation is typically impact-based: estimated user impact exceeds a threshold, financial impact exceeds a threshold, or duration exceeds a threshold. A retail AI platform escalated to executives if an incident affected more than ten thousand users, had financial impact exceeding fifty thousand dollars, or remained unresolved after one hour. Those thresholds captured truly major incidents while avoiding executive notification for routine pages.

Executive escalation should include summarized context, not raw technical alerts. Executives need to know: what is broken, how many users are affected, what is being done, what help is needed. They do not need to know that the p99 latency of the feature embedding service exceeded two seconds causing downstream timeout cascades. The escalation message format is different from the technical alert format. A global insurance company used a template for executive escalation: "Incident Summary: brief description. User Impact: number and severity. Response Status: team working on it and progress. Estimated Resolution: time or unknown. Assistance Needed: any executive action required." The format enabled fast executive decision-making without requiring technical depth.

## Escalation Monitoring and Continuous Improvement

Escalation policies themselves need monitoring. How often does escalation happen? At which tier do incidents typically get resolved? Are certain engineers or teams escalating more than others? These patterns indicate policy effectiveness and opportunities for improvement.

A financial services company tracked escalation metrics monthly. They measured acknowledgment time by tier — how long until primary acknowledged, how long until backup acknowledged when primary failed, how long until management acknowledged when backups failed. They measured resolution tier — what percentage of incidents were resolved at primary level, at backup level, at specialist level, at executive level. They measured escalation reasons — unresponsive primary, insufficient capability, multi-system coordination, external party involvement.

The analysis revealed patterns. Forty-seven percent of incidents were resolved at primary level without escalation — good, that is the goal. Thirty-two percent required escalation to backup or specialist — acceptable, indicates some incidents exceed primary capability. Fifteen percent required multi-team coordination — worth examining whether better initial routing could reduce this. Six percent reached executive escalation — appropriately rare for major incidents. But eight percent of primary pages went unacknowledged and escalated due to no response. That was too high. It indicated either on-call engineers were not reliably reachable or alert volume was high enough that engineers were overwhelmed.

The investigation focused on the eight percent unacknowledged rate. Interviews with on-call engineers revealed that a subset of alerts were firing during commutes when engineers were driving and could not safely respond to phones. The company adjusted policy to allow thirty-minute acknowledgment windows for alerts during typical commute hours, with immediate backup notification in parallel. This reduced unacknowledged escalation to three percent — still present for genuine edge cases, but no longer indicating systematic problems.

Escalation policy effectiveness is visible in time-to-resolution distributions. If incidents that escalate take significantly longer to resolve than incidents handled at primary level, the escalation policy is introducing overhead. If incidents that escalate resolve quickly once they reach the right tier, the escalation policy is working — it is getting alerts to capable responders. A media AI company found that incidents escalated from the application team to the ML team resolved thirty percent faster than incidents the application team tried to handle themselves. That indicated the initial routing was wrong. Certain alert types should have gone directly to the ML team, skipping the application team entirely. They updated routing rules based on this finding.

The next subchapter covers alert fatigue prevention — the techniques and organizational practices that prevent escalation policies from being undermined by engineers who have learned to ignore alerts because of overwhelming volume or poor signal quality.

