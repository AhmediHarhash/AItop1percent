# Chapter 3 — Quality and Safety Metrics in Production

Latency and error rates tell you if your system is running. Quality and safety metrics tell you if your system is working. The difference matters because an AI system can have perfect uptime while producing outputs that damage users, violate policies, or erode trust. Quality measurement in production requires signals you can compute without human review of every output — proxy metrics that correlate with correctness, inline judges that run on every request, and sampled deep evaluation that grounds your automated signals in human truth. Safety measurement requires even more vigilance because the cost of missing a safety failure is measured in harm, not just inconvenience.

---

- 3.1 — Production Quality Signals: What You Can Measure Without Human Review
- 3.2 — Proxy Metrics: Using Behavior Signals to Infer Quality
- 3.3 — Inline Evaluation: Running Lightweight Judges on Every Request
- 3.4 — Sampled Deep Evaluation: Batch Quality Measurement at Scale
- 3.5 — User Feedback Integration: Explicit Ratings, Implicit Signals
- 3.6 — Confidence and Uncertainty Metrics: When the Model Knows It Does Not Know
- 3.7 — Groundedness and Hallucination Detection in Production
- 3.8 — Safety and Policy Compliance Monitoring
- 3.9 — Adversarial Signal Detection: Prompt Injection and Jailbreak Attempts
- 3.10 — Abuse Traffic Patterns: Detecting Coordinated Attacks at Runtime
- 3.11 — Quality Dashboards: What to Show and How Often to Refresh
- 3.12 — Quality SLOs: Setting Targets That Drive Action

---

*The teams that measure quality in production catch regressions in hours. The teams that do not catch them in user complaints weeks later.*
