# 9.7 — Capacity Planning for LLM Systems

Capacity planning for web servers is arithmetic. Each server handles 1,000 requests per second. You need to serve 50,000 requests per second. You provision 50 servers plus 25 percent headroom, giving you 62 servers. The math is clean, the relationship is linear, and scaling is predictable. Capacity planning for LLM systems is not arithmetic. It is probabilistic forecasting with non-linear cost curves, highly variable latency, and unpredictable demand patterns.

A single query might consume 150 tokens and cost 0.002 dollars, or it might consume 8,000 tokens and cost 0.12 dollars. Your average cost per query is 0.008 dollars, but your P99 cost is 0.18 dollars—a 20x difference. Latency varies by 5x depending on query complexity, retrieval results, and model load. Your demand doubles every six months, but the distribution of query types shifts as users discover new ways to use the system. You need to plan capacity not just for volume, but for variance, tail behavior, and evolving usage patterns.

This subchapter covers how to forecast LLM demand, how to provision for variance and tails, how to balance cost and quality in capacity decisions, and how to avoid the two failure modes: under-provisioning that causes outages and over-provisioning that wastes money.

## Forecasting Demand in the Presence of Growth and Shift

Traditional capacity planning assumes demand grows predictably. You measure traffic for six months, fit a growth curve, and project forward. LLM demand grows unpredictably because usage is not just increasing, it is changing. Early adopters use your system for simple FAQ queries. Six months later, power users are submitting complex multi-turn conversations with uploaded documents. The volume grew 2x, but the compute requirement grew 4x.

The first challenge is separating volume growth from complexity growth. Volume is straightforward—requests per day, conversations per month, users per week. Complexity is harder to measure. One proxy is average token count per request. If average tokens increase from 300 to 650 over six months, complexity is growing even if request volume is flat. A second proxy is retrieval depth. If average documents retrieved per query increases from two to five, complexity is growing. A third proxy is session length. If average turns per conversation increase from three to eight, complexity is growing.

Your forecast must model both dimensions. Volume might grow 50 percent over the next quarter based on user acquisition. Complexity might grow 30 percent based on power user behavior. Combined, your capacity requirement grows 95 percent, not 50 percent. If you plan capacity based on volume alone, you under-provision by 45 percent.

The second challenge is predicting usage shifts. You launch a new feature—document upload, voice input, multi-turn conversations. Adoption is slow initially, then suddenly spikes. Your capacity planning cannot wait until adoption is complete to react. You need to forecast adoption curves based on analogous features. If your last major feature reached 40 percent adoption in three months, assume this feature will follow a similar curve. Provision capacity to handle 40 percent adoption by month three.

The third challenge is seasonal and event-driven variance. E-commerce support systems see 3x traffic during November-December. Tax preparation systems see 10x traffic during March-April. Product launch days see 5x traffic for 48 hours then return to baseline. Your capacity planning must account for known peaks and provision for them in advance. You cannot rely on auto-scaling to handle a 3x spike in two hours. Auto-scaling handles unexpected spikes. Known spikes require pre-provisioning.

The forecasting model combines historical growth trends, complexity growth trends, feature adoption curves, and seasonal patterns. The output is not a single number but a range: expected load, P75 load, P95 load. You provision for P75 load normally and P95 load during known peak periods. This balances cost with reliability.

## Provisioning for Variance and Tail Latency

Average load is not the capacity target. Average load is the baseline. Your capacity target must account for variance. If your average load is 1,000 queries per minute but your P95 load is 1,800 queries per minute, provisioning for 1,000 queries per minute means you are under-capacity 5 percent of the time. You need to provision for P95 or accept frequent degraded performance.

The first decision is which percentile to provision for. P50 is too low—you are under-capacity half the time. P99 is too expensive—you provision for load spikes that happen once per week. The standard target is P90 or P95. You provision enough capacity to handle P95 load without degradation. The top 5 percent of load either triggers graceful degradation or is absorbed by auto-scaling with slightly elevated latency.

The second decision is how to measure load. Requests per minute is one metric, but it does not capture query complexity. A better metric is tokens per minute, which accounts for both volume and size. If your average query is 500 tokens and you receive 1,000 queries per minute, your load is 500,000 tokens per minute. If query complexity grows and average becomes 750 tokens while volume stays flat, your load is now 750,000 tokens per minute—a 50 percent increase with no volume change.

A third metric is compute-normalized load. Different models require different compute. A query to Claude Opus 4.5 requires roughly 3x the compute of a query to Claude Sonnet 4.5. If 80 percent of your traffic uses Opus and 20 percent uses Sonnet, your compute-normalized load is higher than if 20 percent uses Opus and 80 percent uses Sonnet, even with identical token counts. You need to measure not just token volume but token-times-model-cost.

The provisioning calculation then becomes: what infrastructure capacity supports P95 compute-normalized token throughput with acceptable latency? You measure current throughput at various capacity levels. At 10 instances, you handle 400,000 tokens per minute with 600 millisecond P95 latency. At 20 instances, you handle 900,000 tokens per minute with 550 millisecond P95 latency. At 30 instances, you handle 1,400,000 tokens per minute with 520 millisecond P95 latency. The scaling is not linear—efficiency improves with instance count due to better load distribution.

If your P95 compute-normalized load is 1,200,000 tokens per minute, you need at least 27 instances to handle it with acceptable latency. You provision 30 instances to provide 8 percent headroom. This ensures you meet latency SLO at P95 load.

## Balancing Cost and Over-Provisioning

The more capacity you provision, the better your latency and reliability, and the higher your cost. Under-provisioning causes outages. Over-provisioning wastes money. The optimal provisioning point depends on the cost of downtime versus the cost of idle capacity.

For a revenue-generating system—e-commerce support, customer-facing chatbot—downtime costs money. If your system drives 5 million dollars per month in revenue and an outage costs 10 percent of daily revenue, a four-hour outage costs 55,000 dollars. If over-provisioning by 30 percent costs 8,000 dollars per month in additional infrastructure, the over-provisioning pays for itself if it prevents one outage every seven months.

For an internal tool—employee support, documentation search—downtime costs productivity but not direct revenue. You can tolerate occasional degraded performance. Provisioning for P75 load instead of P95 load saves 20 percent of infrastructure cost. Employees experience slightly slower responses during peak periods, but no one is blocked. The cost savings of 6,000 dollars per month might be acceptable for occasional minor inconvenience.

The decision framework is cost-of-downtime divided by cost-of-over-provisioning. If the ratio is above 10, over-provision aggressively. If the ratio is below 2, provision conservatively. If the ratio is between 2 and 10, provision for P90 load with auto-scaling to P99.

A second consideration is lead time for scaling. If you can auto-scale from 20 to 40 instances in three minutes, you can provision conservatively and rely on auto-scaling for spikes. If auto-scaling takes 15 minutes—due to model load time, instance boot time, or rate limits—you must provision more aggressively. The slower your scaling, the more headroom you need.

A third consideration is cost structure. If you pay per token to a hosted provider, over-provisioning does not increase cost—you only pay for what you use. If you self-host models, over-provisioning means paying for idle GPU capacity. The cost penalty is different, and the provisioning strategy differs accordingly.

For hosted models, the capacity planning question is primarily about rate limits and prioritization. You need enough rate limit headroom to handle P95 load. For self-hosted models, the question is how many GPUs to provision. Each GPU costs 2,000 dollars per month. Buying an extra GPU for headroom costs real money even if it sits idle.

## Auto-Scaling Policies for Unpredictable Spikes

Capacity planning handles predictable growth and known peaks. Auto-scaling handles unpredictable spikes. Your product gets mentioned on social media and traffic triples in 20 minutes. A major incident in your industry drives support queries up 5x. You cannot predict these events, but you can prepare to scale automatically when they happen.

The most common auto-scaling trigger is CPU utilization. When average CPU across instances exceeds 70 percent for three minutes, add 50 percent more instances. When average CPU drops below 40 percent for 10 minutes, remove 25 percent of instances. This is borrowed from traditional web infrastructure and works reasonably well for LLM serving.

A better trigger is request queue depth. When queue depth exceeds 100 requests for five minutes, scale up. When queue depth drops below 20 for 10 minutes, scale down. Queue depth directly measures whether you are keeping up with demand, which CPU utilization only approximates. A system can have low CPU utilization because it is bottlenecked on network or memory, while queue depth clearly shows backlog.

A third trigger is latency. When P95 latency exceeds 1.5x target latency for three consecutive measurement windows, scale up. This catches capacity issues that manifest as slowness rather than errors. The threshold is 1.5x rather than 2x because latency degrades gradually. By the time latency is 2x baseline, you are deep into saturation. At 1.5x, you still have time to scale before users experience severe degradation.

The scaling speed matters. Adding one instance at a time is too slow when you need 10 more instances. Adding 10 instances at once is too aggressive when you only need three. The most common policy is proportional scaling: add 50 percent of current instance count, minimum two instances, maximum 10 instances. If you have six instances and need to scale, add three. If you have 30 instances and need to scale, add 10.

The scale-down policy must be conservative. Scale up quickly and aggressively. Scale down slowly and cautiously. You would rather pay for five extra minutes of unused capacity than scale down too quickly and trigger a second spike. The asymmetry reflects the cost-risk tradeoff: unused capacity costs money, insufficient capacity causes outages.

## Capacity Planning for Multi-Tenant Systems

When you serve multiple customers from shared infrastructure, capacity planning becomes allocation. How much capacity does each customer get? What happens when one customer spikes and consumes capacity needed by others? You need policies for resource allocation, isolation, and oversubscription.

The most common approach is tiered capacity allocation. Enterprise customers get dedicated capacity—specific instances reserved for their traffic. Paid customers get shared capacity with guaranteed minimums—at least 100 queries per minute, but they can burst higher if capacity is available. Free-tier customers get best-effort capacity—whatever is left after enterprise and paid customers are served.

The allocation is enforced through priority queues. Incoming requests are tagged with customer tier. The scheduler processes enterprise requests first, paid requests second, free requests last. If the system is at capacity, free requests are queued or rejected while enterprise and paid requests continue flowing. This ensures that paying customers never experience degradation due to free-tier traffic spikes.

A second approach is oversubscription. You provision 100 instances but allocate 150 instances worth of guarantees to customers, betting that not all customers will use their full allocation simultaneously. This is the airline overbooking model. It works if usage patterns are uncorrelated—different customers peak at different times. It breaks if customers are correlated—if everyone spikes simultaneously, you cannot fulfill guarantees.

The oversubscription ratio depends on usage correlation. If customers are geographically distributed across time zones, correlation is low and you can oversubscribe 2:1 safely. If customers are all in the same industry with synchronized busy periods, correlation is high and you should oversubscribe 1.2:1 at most.

A third approach is per-customer rate limiting. Each customer has a maximum queries per minute limit based on their plan. The limit is enforced regardless of available capacity. This simplifies planning—you provision for the sum of all rate limits. The downside is that capacity sits unused if customers do not use their full allocations.

The multi-tenant capacity plan must account for growth across customers. As customer count grows, aggregate capacity requirement grows. As individual customers grow, their allocation grows. You need to forecast both dimensions. If you add 10 customers per month and each customer grows 20 percent per quarter, your capacity requirement grows from both sources.

## Cost Optimization Through Spot Instances and Model Batching

Capacity is expensive. A single A100 GPU costs 2,000 dollars per month on AWS. An 8-GPU instance costs 16,000 dollars per month. If you need 10 such instances to serve peak load, you are spending 160,000 dollars per month on compute. Any optimization that reduces instance count by 20 percent saves 32,000 dollars per month.

The most significant optimization is spot instances for self-hosted models. Spot instances cost 50 to 70 percent less than on-demand instances but can be reclaimed with two minutes notice. For auto-scaling capacity, spot instances are perfect. Your baseline capacity is on-demand instances that are never reclaimed. Your burst capacity is spot instances that handle spikes. If spot instances are reclaimed, auto-scaling provisions on-demand instances as fallback.

The implementation requires spot-aware auto-scaling. When scaling up, provision spot instances first. If spot instances are unavailable or reclaimed, provision on-demand instances. When scaling down, remove spot instances first. This keeps cost low during normal operation while maintaining reliability during spot scarcity.

A second optimization is request batching. Processing 10 requests sequentially through a model takes 5,000 milliseconds. Processing 10 requests as a batch takes 1,200 milliseconds. Batching increases throughput by 4x, allowing you to serve the same load with 75 percent fewer instances. The tradeoff is latency—batched requests wait for a batch to fill before processing.

The batching policy balances throughput and latency. You batch up to 10 requests or 100 milliseconds, whichever comes first. During high load, batches fill within 20 milliseconds and you get full throughput benefit. During low load, batches timeout at 100 milliseconds and you get minimal throughput benefit but keep latency acceptable.

A third optimization is dynamic model selection. Not every query needs your most expensive model. During capacity planning, you allocate a mix of model tiers. Sixty percent of capacity is Opus-class models for complex queries. Thirty percent is Sonnet-class models for moderate queries. Ten percent is small models for simple queries. A classifier routes queries to the appropriate tier in real-time, maximizing throughput per dollar.

The cost optimization must not compromise reliability. Spot instances save money but introduce reclamation risk. Batching saves money but increases latency. Dynamic model selection saves money but risks misclassification. Each optimization includes safeguards: spot instances have on-demand fallback, batching has timeout limits, model selection has quality thresholds.

Your capacity planning balances growth forecasting, variance provisioning, cost optimization, and reliability targets to ensure you can serve demand without outages or waste. The next subchapter covers load shedding—what to do when demand exceeds even your best capacity planning.

