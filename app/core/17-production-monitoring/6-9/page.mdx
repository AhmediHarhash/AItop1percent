# 6.9 — Cost Explosion Detection: Agents That Burn Budget

An agent system that costs 1,400 dollars per day suddenly costs 19,000 dollars per day. No traffic spike. No infrastructure failure. Just agents making poor decisions about when to stop, when to retry, and how many steps to take. By the time finance notices the overage, you have burned 80,000 dollars on runaway reasoning.

Cost explosion detection monitors token consumption, API call volume, and compute usage in real time, flagging anomalies before they accumulate into budget disasters. Agents are uniquely vulnerable to cost explosions because they are autonomous — once they start down an expensive path, they continue until stopped by a step limit, timeout, or cost circuit breaker.

## The Sixteen-Thousand-Dollar Morning

A document analysis agent in late 2025 processed contracts for a legal tech company. Expected cost: three cents per contract. One Friday morning, the finance team received an alert: the company's model API bill had hit 16,200 dollars in four hours — twenty-three times the normal daily spend.

Investigation revealed that a small percentage of agents had entered high-cost trajectories. A contract with unusually complex nested clauses triggered the agent to perform deep recursive analysis, making 340 model calls and consuming 890,000 tokens to analyze a single document. Twelve contracts that morning followed the same pattern. The agents were not stuck in loops — they were executing valid but catastrophically expensive reasoning strategies.

The disaster could have been prevented with cost explosion detection. The first high-cost trajectory should have triggered an alert within minutes. The agent should have been terminated or throttled after the first few high-cost requests. Instead, the system continued running until the bill exceeded a threshold that triggered a manual finance alert hours later.

## Real-Time Per-Request Cost Tracking

You log the cost of every agent request in real time. As the agent makes tool calls and model invocations, you accumulate cost. When cost exceeds a threshold, you take action — throttle the agent, terminate it, escalate it, or flag it for review.

Cost thresholds are set per request type. A simple account lookup should cost less than 0.002 dollars. A complex research task might cost up to 0.15 dollars. You define expected cost ranges for each category. Requests that exceed the high end of the range by more than three times are flagged as cost explosions.

A customer support agent in early 2026 handled inquiries with an expected cost between 0.01 and 0.04 dollars per request. The team set a hard limit at 0.12 dollars — three times the high end. Any request approaching 0.12 dollars was terminated and escalated to human support. In practice, 0.2 percent of requests hit the limit. Manual review showed that ninety percent of terminated requests involved edge cases that required human judgment anyway. Terminating them early saved cost without degrading user experience.

## Detecting Token Burn Rate Anomalies

Token consumption rate predicts final cost. An agent that has consumed 50,000 tokens in the first twenty seconds of a request is on track to consume far more by the time it finishes. You monitor token burn rate — tokens consumed per second — and flag requests where burn rate is abnormally high.

You calculate baseline burn rates for each request type. A document summarization agent might consume 8,000 tokens per request over an average duration of twelve seconds, for a burn rate of 667 tokens per second. If a request is consuming 2,400 tokens per second at the ten-second mark, it is burning tokens three times faster than baseline and is likely heading toward a cost explosion.

Burn rate anomaly detection flags requests where current burn rate exceeds baseline by more than two times. These requests are monitored closely, and if burn rate does not decrease within thirty seconds, the agent is terminated.

A financial modeling agent in mid-2025 built investment projections. Baseline burn rate was 450 tokens per second. One request hit 1,300 tokens per second within the first fifteen seconds. Burn rate detection flagged it. The agent was executing a recursive calculation that was expanding exponentially. The system terminated the agent at forty seconds, limiting cost to 0.31 dollars instead of the projected 2.10 dollars if the agent had run to completion.

## Step Depth and Cost Correlation

Cost correlates with step depth. More steps mean more tool calls, more model invocations, and higher cost. You track the relationship between step depth and cost for each request type. When a request's cost is significantly higher than expected given its step depth, something is wrong.

A hiring agent in early 2026 used an average of 6.2 steps per request at a median cost of 0.028 dollars, or roughly 0.0045 dollars per step. A request that took fourteen steps and cost 0.19 dollars was an outlier — it cost 0.014 dollars per step, three times the baseline. Investigation showed the agent was making redundant tool calls at each step, calling the same resume database three times per step instead of once. The high per-step cost revealed inefficiency that step depth alone did not capture.

## Cost Circuit Breakers

When cost exceeds a critical threshold, you stop the agent immediately. Cost circuit breakers are similar to step limit enforcement, but they trigger on cost rather than step count. The advantage: cost circuit breakers account for the fact that not all steps are equally expensive. An agent that makes fifteen cheap tool calls might be fine, while an agent that makes five expensive model calls with huge contexts might be burning budget.

You set cost circuit breakers per request type. For low-value requests — account lookups, simple queries — the breaker might trip at 0.05 dollars. For high-value requests — research reports, complex analysis — the breaker might trip at 0.50 dollars. When a request hits the circuit breaker, the agent is terminated, the request is logged as a cost overrun, and the user receives an error or escalation.

A legal research agent in mid-2025 set cost circuit breakers at 0.40 dollars for standard research and 1.20 dollars for deep research. In three months, 0.4 percent of requests hit the standard breaker and 0.1 percent hit the deep research breaker. Manual review showed that nearly all breaker-triggered requests involved ambiguous or overly broad queries that would have required human assistance anyway. The circuit breakers prevented those requests from burning excessive cost before escalation.

## Identifying Cost Hotspots by Tool

Not all tools cost the same. A tool that calls GPT-5 with a 50,000-token context costs far more than a tool that queries a local database. You track cost attribution per tool, identifying which tools are the primary cost drivers.

Cost attribution shows you where money is being spent. A research agent might attribute sixty percent of cost to a web search tool that uses a large model to rank and synthesize results, thirty percent to a document extraction tool, and ten percent to everything else. If web search cost suddenly jumps to eighty percent, you investigate what changed.

A product recommendation agent in early 2026 used five tools: catalog search, user preference retrieval, inventory check, pricing calculation, and recommendation synthesis. Cost attribution showed catalog search and recommendation synthesis each accounted for forty percent of cost. The other three tools together accounted for twenty percent. When total cost increased by thirty percent over two weeks, tool-level attribution revealed that recommendation synthesis cost had doubled. Investigation showed the agent was generating longer, more detailed explanations, consuming more output tokens per recommendation. The team tuned the output length constraint, and cost returned to baseline.

## Anomalous Tool Call Volume

Cost explosions often result from calling tools too many times. An agent that makes forty API calls to complete a request that normally takes eight calls is burning cost through sheer volume. You track tool call volume per request and flag requests where volume is abnormally high.

Tool call volume thresholds are set per request type. A customer service request should use between five and twelve tool calls. A request that uses thirty tool calls is either handling an unusually complex case or stuck in inefficiency. You flag high-volume requests for review.

A logistics agent in mid-2025 tracked shipments using an average of 7.4 tool calls per query. Requests using more than twenty tool calls were flagged. Investigation showed that high-volume requests occurred when tracking numbers were ambiguous or misformatted. The agent would try multiple variations, querying the tracking API repeatedly. The team added input validation to detect malformed tracking numbers before the agent started querying, reducing high-volume requests by seventy percent.

## Cost Budgeting Per User Session

Some agents handle multi-turn conversations. Cost accumulates across turns. A user who has a fifteen-turn conversation with an agent can burn significantly more budget than a user who asks one question and leaves. You track cumulative cost per session and enforce per-session cost limits.

A customer service agent in early 2026 allowed up to 0.50 dollars of cost per user session. Most sessions used 0.08 to 0.15 dollars. Sessions approaching 0.50 dollars received a warning: "We have spent significant resources on your request and will escalate to a human specialist for more efficient assistance." This message framed the escalation positively while preventing runaway session costs.

Per-session cost budgeting also revealed patterns of abuse or misuse. A small number of users repeatedly engaged the agent in long, meandering conversations that provided no value but consumed budget. Per-session limits contained the damage.

## Model Selection and Cost Explosion

Agents that route requests to different models based on complexity can miscalculate and route to expensive models unnecessarily. A request that should be handled by a small, cheap model gets routed to a large, expensive model, inflating cost.

You track model selection decisions and validate that expensive models are used only when necessary. If an agent routes a simple request to GPT-5 when GPT-5-mini would suffice, you flag it. Over time, you tune the routing logic to prefer cheaper models and escalate to expensive models only when justified.

A document summarization agent in mid-2025 used GPT-5-mini for documents under 5,000 words and GPT-5 for longer documents. Cost tracking showed that twelve percent of GPT-5 usage was for documents under 3,000 words — cases where GPT-5-mini would have been sufficient. The routing logic was adjusted to use GPT-5 only for documents above 7,000 words or documents flagged as complex by a preliminary classifier. GPT-5 usage dropped by thirty percent, and median cost per request decreased by eighteen percent without measurable quality degradation.

## Pre-Execution Cost Estimation

Before executing an agent's plan, you can estimate cost based on the planned steps. The agent generates a plan: retrieve data from three sources, perform analysis, synthesize results. You estimate how many tokens each step will consume and predict total cost. If predicted cost exceeds a threshold, you either optimize the plan, escalate the request, or warn the user.

Cost estimation is imperfect — actual cost depends on data size, tool performance, and whether the agent needs to replan. But even rough estimates catch obviously expensive plans before execution.

A market research agent in early 2026 estimated cost before executing analysis plans. Plans predicted to cost more than 0.80 dollars were flagged for review. A human operator would review the plan, decide whether the cost was justified, and either approve it, modify it, or route the request to a more efficient workflow. This pre-execution check prevented high-cost plans from running unattended, reducing cost overruns by forty-three percent.

## Detecting Gradual Cost Inflation

Cost explosions are not always sudden. Sometimes cost creeps up gradually over weeks or months as agents learn inefficient patterns, tools degrade, or traffic characteristics change. You track average cost per request over time and alert when cost trends upward.

A sales agent in mid-2025 started with a median cost of 0.021 dollars per request. Over eight weeks, median cost climbed to 0.037 dollars — a seventy-six percent increase. No single day showed a spike, so daily monitoring missed it. Trend analysis caught the inflation. Investigation revealed that the agent was using more verbose outputs over time, consuming more output tokens per request. The team clarified output length constraints, and cost returned to baseline.

Gradual cost inflation is insidious because it does not trigger acute alerts. Tracking trends over rolling windows — comparing this week's cost to the average of the prior four weeks — surfaces gradual degradation before it becomes severe.

## Cost Attribution to User Behavior

Not all cost explosions are the agent's fault. Some result from user behavior — users submitting unusually complex requests, asking repetitive questions, or engaging in adversarial interactions. You track cost per user or per session to identify users who consume disproportionate resources.

A tutoring agent in early 2026 tracked cost per student. Most students consumed 0.15 to 0.40 dollars per session. One student consumed 4.20 dollars in a single session, asking the agent to solve seventy-three practice problems step-by-step. The agent complied, burning tokens on repetitive explanations. The system flagged the session, and the team implemented a per-session question limit to prevent resource exhaustion.

## Alerting on Cost Anomalies

Real-time alerting is essential. You set alert thresholds: if cost per request exceeds three times baseline, alert the on-call engineer. If total daily cost exceeds projected spend by fifty percent by midday, escalate to leadership. Alerts enable intervention before cost damage becomes catastrophic.

Alert fatigue is a risk. Too many false positives and teams ignore alerts. You tune thresholds to balance sensitivity and specificity. A threshold that triggers on five percent of requests is too sensitive. A threshold that triggers on 0.1 percent of requests is appropriate for catching true anomalies.

A customer service agent in mid-2025 set cost anomaly alerts at five times baseline per-request cost. This threshold triggered on 0.3 percent of requests, almost all of which were true cost explosions. The engineering team reviewed alerts within minutes and terminated runaway agents before cost exceeded 0.50 dollars per request.

## Post-Incident Cost Analysis

When a cost explosion occurs, you log the full trajectory, analyze what went wrong, and implement safeguards. Cost explosions are not random. They result from specific agent behaviors, tool inefficiencies, or edge cases. Post-incident analysis identifies patterns and prevents recurrence.

A healthcare agent experienced a cost explosion when processing a patient's medical history that spanned forty years and included thousands of records. The agent attempted to load and analyze the entire history in one request, consuming 1.2 million tokens and costing 9.40 dollars. Post-incident analysis led to a change: the agent now analyzes medical histories in time windows, processing the most recent two years in detail and summarizing older records. Similar requests now cost under 0.60 dollars without losing clinical relevance.

Cost explosions are not acceptable operational events. They are preventable with real-time monitoring, circuit breakers, and cost-aware agent design. Agents that burn budget without bounds are not production-ready. Cost discipline is part of agent reliability.

The next subchapter covers human handoff triggers — identifying when an agent should stop attempting to solve a problem and escalate to a human specialist.

