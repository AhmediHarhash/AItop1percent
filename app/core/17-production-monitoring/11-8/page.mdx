# 11.8 — Cost Governance: Policies and Controls for Multi-Team Systems

Cost governance is what prevents one team's experiment from burning the entire quarter's budget. In single-team systems, informal coordination works. In multi-team systems—ten product teams, twenty engineers, four environments, dozens of features—informal coordination fails. Cost governance provides the policies, controls, and accountability structures that let teams move fast without creating budget chaos.

## The Problem Governance Solves

Without cost governance, AI costs behave like unmanaged cloud infrastructure: every team deploys what they want, optimizes for their own metrics, and assumes someone else is tracking aggregate spend. Costs spiral. Blame is diffuse. By the time Finance notices, the quarter is over and the budget is blown.

In October 2025, a multi-product SaaS company had five product teams using a shared LLM infrastructure. Each team built features independently. No team tracked aggregate cost. No one owned the overall budget. October spend reached $89,000—78 percent over the $50,000 budget. Finance escalated to Engineering leadership. Engineering blamed Product. Product blamed Engineering. No one could explain which features drove the overage or who approved them.

The company implemented cost governance in November. Each team received a monthly budget. Costs were tracked per team, per feature, and per environment. Teams that exceeded their budget needed approval from their VP to continue. Any new feature that would increase costs by more than 10 percent required cost-impact review before launch. By December, spend was $52,000—within budget—and every team knew their contribution to the total.

Cost governance is not about restricting innovation. It is about visibility, accountability, and preventing surprises. Teams can still experiment, but they do so with awareness of costs and consequences.

## Budget Allocation by Team

The simplest governance model allocates budget per team. Each team receives a monthly or quarterly budget. They manage spend within that budget. If they exceed it, they escalate or reduce usage.

A customer-facing products organization has three product teams: Support, Sales, and Success. Total quarterly AI budget: $150,000. Support handles 60 percent of traffic and receives $90,000. Sales handles 25 percent and receives $37,500. Success handles 15 percent and receives $22,500. Each team tracks their own spend and is responsible for staying within their allocation.

Budget allocation reflects expected usage, strategic priority, and historical spend. A team launching a new feature might receive a higher allocation. A team optimizing an existing feature might receive a flat or reduced allocation. Reallocation happens quarterly based on actuals and roadmap.

Per-team budgets create accountability. When a team knows they own $30,000 per month, they optimize differently than when the budget is a shared pool. They question whether every prompt template needs five examples or three. They test cheaper models. They add caching. Accountability drives cost discipline.

## Cost Approval Thresholds

Not all costs require approval. A feature that adds $100/month does not need executive sign-off. A feature that adds $10,000/month does. Cost approval thresholds define what requires review and who approves it.

A fintech company sets three approval tiers. Tier 1: costs under $500/month require no approval beyond team lead. Tier 2: costs between $500 and $5,000/month require product manager approval. Tier 3: costs over $5,000/month require VP approval and Finance review. Teams estimate cost impact before launch using cost-per-request data from staging or production shadow testing.

Approval thresholds balance speed and control. Low thresholds—every change requires approval—slow down development. High thresholds—only massive changes require approval—risk budget overruns. Most teams set thresholds at 5 to 10 percent of monthly budget for automatic approval and 20 to 30 percent for executive approval.

Approval processes include cost-benefit analysis. A feature that costs $8,000/month but increases conversion by 4 percent might be approved. A feature that costs $8,000/month but improves a rarely-used workflow might not. The approval process forces teams to justify cost, not just quality.

## Environment-Based Cost Controls

Production, staging, and development environments have different cost profiles. Production serves real users and must prioritize quality. Staging tests changes and should minimize cost. Development environments should have strict cost limits to prevent accidental overruns from local testing.

A document processing platform implements environment-based controls. Production has no cost limits but is monitored continuously. Staging has a daily budget of $200. If staging spend exceeds $200 in a day, further API calls are rate-limited. Development environments have a daily budget of $20 per engineer. If an engineer's dev environment exceeds $20, their API key is suspended until they acknowledge a warning.

The development environment limit prevents accidental cost spikes from infinite loops, unthrottled batch processing, or engineers forgetting to shut down test scripts. In September 2025, an engineer left a test script running overnight. The script called GPT-5 in a loop with no rate limiting. By morning, it had generated $1,100 in costs. After implementing the $20 daily limit, similar incidents are caught within an hour and cost less than $25.

Staging budgets prevent pre-production testing from dominating spend. Teams test prompt changes, run eval suites, and validate new features in staging. Without limits, staging can cost as much as production. With a daily budget, teams optimize staging usage: they run targeted tests instead of exhaustive sweeps, cache model responses, and use cheaper models for non-critical testing.

## Feature Flags and Cost Gates

Feature flags control which users see which features. Cost gates extend this: a feature can be enabled but subject to cost limits. If the feature exceeds its budget, it is automatically disabled or falls back to a cheaper implementation.

A search assistant has a premium reasoning feature that uses GPT-5.1 for complex queries. The feature has a daily budget of $600. When daily spend exceeds $600, the feature falls back to Claude Sonnet 4.5, which costs 40 percent less. Users still get answers, but the system preserves the budget. The next day, the budget resets and the premium feature re-enables.

Cost gates prevent runaway spend from new features. A feature that was tested on 100 users might behave differently at 10,000 users. If cost grows faster than expected, the cost gate protects the overall budget while the team investigates. Without cost gates, a popular feature can burn the quarter's budget in a week.

Cost gates are implemented at the application layer, not the provider layer. The application tracks cumulative spend for the feature, compares it to the budget, and decides whether to invoke the expensive model or fall back. This requires real-time cost tracking, which we covered in earlier subchapters.

## Per-User and Per-Request Rate Limits

Cost governance includes rate limiting: caps on how much a single user or request can cost. Rate limits prevent abuse, contain anomalies, and enforce fair usage policies.

A B2B SaaS platform sets per-user rate limits. Free-tier users can make 100 API calls per day. Paid users can make 10,000 calls per day. Enterprise users have no hard limit but trigger alerts if they exceed expected usage by 3x. A free-tier user who attempts 101 calls in a day receives a rate limit error and is prompted to upgrade.

Per-request rate limits cap token usage. A document summarization service limits input tokens to 8,000 per request. Requests that exceed this are rejected with an error explaining the limit. This prevents users from submitting 50,000-token documents that cost $2.50 per request. The limit was set based on the 99th percentile of legitimate usage. Less than 1 percent of requests exceed it, and those are typically errors or abuse.

Rate limits must be tuned to avoid harming legitimate usage. Set them based on historical data: the 95th or 99th percentile of usage is a good starting point. Monitor rate limit errors. If 5 percent of requests hit the limit, the limit is too low. If 0.01 percent hit it, the limit is set correctly.

## Cost Chargeback Models

Chargeback models allocate costs back to the teams or departments that generated them. This creates financial accountability and aligns incentives: teams that optimize costs reduce their own budgets, not someone else's.

A multi-product company tracks AI costs per product line. Each product line is a separate P&L. Monthly AI costs are charged back to the product line based on usage. Product A generated 60 percent of requests and is charged 60 percent of costs. Product B generated 30 percent and is charged 30 percent. Product C generated 10 percent and is charged 10 percent.

Chargeback creates pressure to optimize. Product A's leadership sees $18,000/month in AI costs on their P&L. They ask their engineering team to reduce it. The team implements caching, switches to cheaper models for low-priority requests, and optimizes prompts. Costs drop to $13,000/month. Product A keeps the savings. Without chargeback, there is no financial incentive to optimize.

Chargeback requires accurate cost attribution. You must track costs per team, per product, or per customer. This requires tagging requests with the relevant metadata, as covered in earlier subchapters. Chargeback also requires Finance and Engineering alignment on how costs are allocated and reported.

## Alerting and Escalation Policies

Cost governance includes who gets alerted when budgets are exceeded and what actions they take. Without clear escalation policies, budget overruns are discovered too late to fix.

A healthcare AI platform defines three escalation levels. Level 1: any feature exceeds its daily budget by 20 percent. Alert the feature owner via Slack. Level 2: any team exceeds their monthly budget by 50 percent by mid-month. Alert the engineering manager and product manager. Level 3: aggregate company spend exceeds quarterly budget projection by 10 percent. Alert VP Engineering, VP Product, and CFO.

Each level has a response playbook. Level 1: feature owner reviews cost spike, determines if it is expected traffic or an anomaly, and decides whether to take action. Level 2: team leadership reviews spending trends, identifies high-cost features, and prioritizes optimization work. Level 3: executive team reviews overall spend, reallocates budgets if necessary, and escalates to CEO if the issue cannot be resolved within existing budgets.

Escalation policies prevent budget problems from being ignored. When alerts fire and no one responds, budgets are exceeded by default. When alerts trigger defined actions, teams catch and fix problems before they compound.

## Cost Review Cadences

Cost governance includes regular reviews: weekly for operational teams, monthly for leadership, quarterly for strategic planning. Each review has a different focus and different participants.

A customer support organization holds weekly cost reviews with the engineering team. The team reviews the last week's spend, identifies anomalies, checks forecast accuracy, and discusses upcoming changes that might affect cost. The review takes 20 minutes. It is not about solving every problem—it is about maintaining awareness and catching issues early.

Monthly reviews include product and engineering leadership. The review covers total spend vs. budget, cost per feature, cost trends, forecast for the next month, and any features that exceeded their cost estimates. The team decides whether to reallocate budgets, prioritize cost optimization, or accept higher costs in exchange for value delivered.

Quarterly reviews are strategic. They include Finance, VP Engineering, VP Product, and sometimes the CEO. The review covers total spend vs. annual budget, cost as a percentage of revenue, cost efficiency trends, model selection decisions, and budget allocation for the next quarter. Quarterly reviews align cost management with business goals.

Regular reviews prevent cost governance from becoming a one-time exercise. Cost patterns shift. Traffic grows. Features launch. Without ongoing reviews, governance policies become stale and ineffective.

## Governance Without Bureaucracy

Cost governance risks slowing teams down. If every feature requires three levels of approval, two weeks of cost estimation, and a committee review, teams avoid building anything new. Effective governance balances control with speed.

Fast approval for low-risk changes. A feature that costs under $200/month should not require a VP meeting. Automate approvals: if the cost estimate is below the threshold, the feature is automatically approved. Reserve manual review for high-cost or high-uncertainty features.

Self-service cost estimation. Teams should not need to ask Finance or Engineering leadership to estimate cost. Provide tools: cost calculators based on traffic projections and model pricing, dashboards showing cost per request for existing features, historical data on similar features. Teams estimate their own cost impact and include it in launch planning.

Clear policies documented in a runbook. What requires approval? Who approves it? How do you estimate cost? What happens if you exceed budget? If every team interprets governance differently, it is not governance—it is chaos. Document the policies, link them in the wiki, and train new team members.

Governance should feel like guardrails, not roadblocks. Teams should be able to move fast within the guardrails and escalate when they need to go outside them. If governance feels oppressive, teams will circumvent it. If governance feels absent, costs will spiral.

## Cost Governance in Startups vs. Enterprises

Startups and enterprises need different governance models. Startups have fewer teams, faster iteration cycles, and less formal structure. Enterprises have more teams, more compliance requirements, and more stakeholders.

A 20-person startup implements lightweight governance: a shared spreadsheet tracking cost per feature, a weekly Slack update on total spend, and a rule that any feature expected to cost over $1,000/month requires founder approval. That is enough. The team is small, communication is direct, and everyone knows what everyone else is building.

A 2,000-person enterprise implements formal governance: per-team budgets, multi-tier approval workflows, chargeback to business units, monthly reviews with Finance, quarterly budget planning integrated with corporate FP&A. The complexity is necessary because the scale is large, the stakeholders are many, and informal coordination fails.

Most teams need something in between. Start with simple policies: track costs, set budgets, require approval for large changes. Add complexity only when the simple version breaks. Governance should scale with organizational complexity, not precede it.

## When Governance Fails

Governance fails when policies are ignored, when alerts are dismissed, or when accountability is unclear. A budget alert that fires every day becomes background noise. An approval process that everyone bypasses is not a process.

In November 2025, a document platform had a governance policy requiring VP approval for features costing over $3,000/month. Three features were launched in November that cost $4,200, $5,800, and $3,700 per month. None received VP approval. The teams assumed approval was implicit because the features were in the roadmap. The VP assumed teams would escalate if they needed approval. By month-end, spend was $14,000 over budget.

The company tightened governance: features over $3,000/month cannot deploy to production without VP sign-off in the deployment pipeline. The sign-off is a required field in the release checklist. Attempts to deploy without it are blocked. The policy is no longer optional—it is enforced by tooling.

Governance policies must be enforceable. A policy you cannot or will not enforce is worse than no policy because it creates the illusion of control without the reality. If you set a rule, enforce it. If you cannot enforce it, do not set it.

The next subchapter covers the cost-quality dashboard—unified views that let teams see cost, quality, latency, and error rate in one place, enabling informed tradeoff decisions.

