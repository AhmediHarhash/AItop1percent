# 2.7 â€” Sampling Strategies for High-Volume Systems

You cannot afford to log everything. You cannot afford to log nothing. Sampling is the art of logging enough.

At low scale, you log every request. Every prompt, every response, every latency measurement, every cost event. Full logging is simple. Full logging works. But full logging does not scale. A system serving 10 million requests per day generates terabytes of logs. Storage costs explode. Query performance degrades. Your logging system becomes more expensive than your model calls.

A ride-sharing company faced this in mid-2025. Their AI routing assistant served 40 million requests per day. Full logging cost them 180,000 dollars per month in storage and 60,000 dollars per month in log query costs. The logs were too large to search efficiently. Debugging a single incident required waiting 20 minutes for queries to complete. The logging system designed to help them debug was actively hindering debugging.

They implemented stratified sampling. They logged 100 percent of errors, 100 percent of slow requests, 10 percent of requests from premium users, and 1 percent of requests from standard users. Log volume dropped by 94 percent. Storage costs dropped to 12,000 dollars per month. Query performance improved by 30 times. And they still had enough data to debug incidents, monitor quality, and understand user behavior. Sampling saved them 228,000 dollars per month.

Sampling is not a compromise. It is a requirement for operating AI systems at scale.

## Uniform Random Sampling

The simplest sampling strategy is uniform random sampling: keep X percent of requests, discard the rest. Every request has an equal probability of being logged. If you sample at 5 percent, you log roughly 1 out of every 20 requests.

Uniform sampling works when your workload is homogeneous. If every request looks similar, costs similar amounts, and has similar latency, then a random 5 percent sample accurately represents the full population. You can calculate average latency, average cost, error rate, and quality metrics from the sample and extrapolate to the full population.

The implementation is trivial. Generate a random number between 0 and 1 for each request. If the number is less than your sample rate, log the request. Otherwise, discard it. This adds microseconds of overhead and requires no coordination between servers.

But uniform sampling has a fatal flaw: it discards rare events. If errors occur in 0.1 percent of requests, and you sample at 1 percent, you capture only 1 in 1,000 errors. That is not enough to debug. If tail latency affects 0.5 percent of requests, and you sample at 2 percent, you see only 1 in 50 tail latency events. You miss the patterns that matter most.

Uniform sampling works for understanding the average case. It fails for understanding failures, outliers, and rare but important events.

## Error-Biased Sampling: Always Log Failures

The first rule of sampling at scale: always log errors. Errors are rare, high-signal, and critical for reliability. An error rate of 0.5 percent means 50,000 errors per day in a 10-million-request system. You need every one of those errors logged to understand failure modes, identify patterns, and fix root causes.

Error-biased sampling keeps 100 percent of failed requests and samples successful requests at a lower rate. If you sample successes at 2 percent and failures at 100 percent, you get full error visibility while keeping log volume manageable.

The technical implementation requires evaluating request outcome before deciding whether to log. Your logging logic checks: did this request return an error code? Did the model call fail? Did a timeout occur? Did guardrails block the response? If yes, log everything about the request. If no, apply standard sampling.

A financial analysis platform ran 25 million requests per day with a 0.3 percent error rate. They sampled successful requests at 1 percent and logged all errors. This gave them 75,000 error logs per day and 250,000 success logs per day. The error logs were enough to debug every failure mode. The success logs were enough to monitor quality and cost. Total log volume was 325,000 events per day, a 98.7 percent reduction from full logging.

Error-biased sampling also catches cascading failures. When one component starts failing, error rate spikes. Full error logging means you see the spike immediately and can trace it to the root cause. With uniform sampling, you might sample only 3 errors out of 300 during the spike and miss the pattern.

## Tail-Biased Sampling: Prioritize Slow Requests

Tail latency is where user pain lives. A request that takes 12 seconds to complete is far more important to understand than a request that takes 0.8 seconds. Tail-biased sampling keeps 100 percent of slow requests and samples fast requests at a lower rate.

You define a latency threshold. Requests slower than the threshold are logged fully. Requests faster than the threshold are sampled. If your p95 latency is 3 seconds, you might set the threshold at 4 seconds and log everything above that. For requests under 4 seconds, sample at 2 percent.

Tail-biased sampling ensures you have full visibility into the experiences that frustrate users. When a user complains that your system is slow, you have logs for every slow request in their session. You can trace the latency decomposition, identify the bottleneck, and fix it. With uniform sampling, you might have no logs for the user's slowest requests.

A healthcare documentation assistant used tail-biased sampling with a 5-second threshold. Requests above 5 seconds were logged fully. Requests below 5 seconds were sampled at 3 percent. They found that 2 percent of requests exceeded the threshold. Full logging of those 2 percent gave them enough data to identify three distinct slow-request patterns: retrieval timeouts when querying large patient records, model timeouts when generating long reports, and database lock contention during peak hours. They optimized all three. P95 latency dropped by 40 percent.

## Stratified Sampling by User Tier or Feature

Not all users are equally important. Premium users paying 500 dollars per month deserve better observability than free-tier users paying nothing. Not all features are equally important. Your core product feature deserves more logging than an experimental beta feature.

Stratified sampling assigns different sample rates to different strata. You define strata by user tier, feature, geography, or any dimension that matters for your business. Each stratum has its own sample rate.

A common stratification is by user tier: log 100 percent of enterprise customers, 20 percent of pro customers, 5 percent of standard customers, and 1 percent of free-tier users. This ensures full visibility for your highest-value customers while controlling overall log volume.

Another common stratification is by feature: log 100 percent of requests for your core product, 10 percent for stable features, and 50 percent for experimental features. Core product gets full observability. Stable features get enough logging to detect regressions. Experimental features get elevated logging during the validation phase.

A B2B software company used both dimensions. They stratified by customer segment and feature area. Enterprise customers using the core product were logged at 100 percent. SMB customers using experimental features were logged at 25 percent. Free-tier users on stable features were logged at 2 percent. This gave them high-resolution data where it mattered most and low-resolution data where it mattered least.

The implementation requires tagging requests with stratum identifiers and looking up sample rates per stratum. Your logging middleware checks the user tier and feature name, retrieves the corresponding sample rate from a configuration service, and applies it. This adds a few milliseconds of overhead but gives you fine-grained control over log volume.

## Head-Biased Sampling: Recent Requests Matter More

Debugging almost always focuses on recent events. When users report a problem, you care about logs from the last hour, not logs from last month. When an incident occurs, you need logs from the incident window, not logs from three weeks ago. Head-biased sampling takes advantage of this by keeping more recent logs and discarding older logs.

One approach is time-windowed sampling: log 100 percent of requests for the last 24 hours, 10 percent of requests from 1 to 7 days ago, and 1 percent of requests older than 7 days. Your logging system starts with full retention, then down-samples older data in the background. Recent data is always high-resolution. Historical data is lower-resolution but still sufficient for trend analysis.

Another approach is adaptive retention: when log volume is low, keep everything. When log volume spikes, increase the sample rate to keep storage costs bounded. This ensures you never run out of log storage during a traffic surge while still logging as much as possible.

A video streaming platform used time-windowed sampling. They logged 100 percent of requests for 48 hours, 5 percent for 7 days, and 1 percent for 30 days. When incidents occurred, they had full data for the incident window. When analyzing long-term trends, they had 1 percent samples going back a month, which was enough for statistical analysis. Storage costs were predictable and bounded.

## Combining Sampling Strategies

The most effective sampling systems combine multiple strategies. You use error-biased sampling to catch all failures, tail-biased sampling to catch all slow requests, and stratified sampling to give premium users higher logging rates, then apply uniform sampling to everything else.

A logistics AI platform combined four strategies:
- 100 percent of errors logged
- 100 percent of requests slower than 6 seconds logged
- 50 percent of enterprise customer requests logged
- 10 percent of pro customer requests logged
- 2 percent of standard customer requests logged

The combined effect: total log volume was 8 percent of full logging, but they had complete visibility into failures, tail latency, and high-value customers. The 8 percent sample was strategically chosen. They could debug any incident, answer any customer question, and monitor quality with high confidence.

The technical implementation layers these strategies. Your logging decision logic evaluates in order: Is this an error? If yes, log. Is this a slow request? If yes, log. Is this a premium user? If yes, apply premium sample rate. Otherwise, apply default sample rate. Each layer is independent and composable.

## Calculating Sample Rates Based on Volume and Budget

Sample rates are not arbitrary. They are calculated based on request volume, log storage costs, and acceptable statistical confidence. If you serve 10 million requests per day and want statistical confidence on quality metrics, you need at least 10,000 sampled requests per day for a confidence interval of plus-or-minus 1 percent at 95 percent confidence. That implies a minimum sample rate of 0.1 percent.

If your log storage costs 0.02 dollars per gigabyte per month, and each logged request is 5 kilobytes, then 10,000 requests per day is 50 megabytes per day, 1.5 gigabytes per month, costing 0.03 dollars per month. That is negligible. But if you serve 500 million requests per day and sample at 0.1 percent, you are logging 500,000 requests per day, 2.5 terabytes per month, costing 50 dollars per month. Still manageable.

The formula is: sample rate equals target sample count divided by daily request volume. If you want 100,000 sampled requests per day and you serve 50 million requests per day, your sample rate is 0.2 percent. If you want 1 million sampled requests per day, your sample rate is 2 percent.

You also need to account for stratification. If 20 percent of your traffic is premium users logged at 100 percent, and 80 percent is standard users, then your effective overall sample rate is much higher than the nominal rate for standard users. The weighted average matters for budgeting.

A travel booking platform calculated they could afford 20 dollars per day in log storage. Each logged request cost 0.0001 dollars to store. That meant they could log 200,000 requests per day. They served 30 million requests per day. They allocated 100 percent logging to errors and slow requests, which consumed 60,000 of their daily budget. That left 140,000 for sampled successes. They set the sample rate at 0.5 percent for standard users and 10 percent for premium users. The math worked out to exactly their budget.

## Monitoring Sample Quality and Coverage

Once you implement sampling, you need to monitor that your sample is representative. Sampling introduces bias. If the bias is small and understood, sampling works. If the bias is large and hidden, sampling gives you misleading data.

The primary metric is coverage by important dimensions. What percentage of your users appear in the sample? What percentage of your features? If you have 100,000 active users and only 200 appear in your daily logs, you have a coverage problem. If you have 10 features and only 3 appear in your logs, you have a coverage problem.

You also monitor for under-sampling of rare but important events. If your error rate is 0.5 percent but your logs show an error rate of 0.05 percent, your error-biased sampling is broken. If your p99 latency is 8 seconds but your logs show p99 of 3 seconds, your tail-biased sampling is broken.

A common technique is dual-stream logging: log 100 percent of metadata and a small percentage of full payloads. Metadata includes user ID, feature name, latency, cost, error status, and model name. Full payloads include prompts and responses. Metadata is cheap to store. Payloads are expensive. You use metadata for aggregate metrics and coverage monitoring. You use payloads for deep debugging. This hybrid approach gives you both breadth and depth.

## The Cost of Over-Logging and Under-Logging

Over-logging costs money and performance. A social media company logged 100 percent of their 200 million daily AI requests for six months. Log storage cost 320,000 dollars per month. Log queries were so slow that debugging took hours. Engineers started avoiding the logs and debugging by intuition instead. The logging system became a hinderance.

Under-logging costs visibility and confidence. A fintech startup sampled at 0.1 percent to save money. When a major quality issue arose affecting 0.5 percent of requests, they had almost no logs for the affected requests. They could not reproduce the issue. They could not identify the root cause. They spent two weeks flying blind. The quality issue persisted. Customers churned. The company lost more revenue from the quality issue than they saved on logging.

The right sampling rate is the one that gives you enough data to debug any incident, answer any question, and monitor quality with confidence, while keeping log costs a small fraction of your total infrastructure spend. For most teams, that means logging between 1 percent and 10 percent of requests with strategic bias toward errors, tail latency, and high-value users.

Next, we examine how to implement async logging so that observability does not slow down your production requests.
