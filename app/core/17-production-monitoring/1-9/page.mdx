# 1.9 â€” Observability Ownership: Who Builds, Who Operates, Who Responds

The alert fired at 2:47 AM. Model output quality had dropped below threshold for twelve consecutive minutes. The on-call engineer acknowledged the page, opened the runbook, and immediately hit a problem: the runbook said to notify the AI Product team, but nobody knew who that was. The Slack channel listed in the runbook had been archived three months ago. The escalation path pointed to an engineering manager who had left the company. By the time the on-call engineer found someone who knew anything about the AI system, forty minutes had passed and the quality drop had resolved itself. The postmortem identified the root cause as a prompt deployment that reverted automatically after fifteen minutes when a canary check failed. The observability system worked. The ownership model did not.

This is the observability ownership problem. You can build perfect instrumentation, flawless dashboards, and precisely tuned alerts, and still fail operationally because nobody knows who is responsible for what. Observability without clear ownership produces alert fatigue, ignored dashboards, and incidents that escalate to the wrong people. Effective observability requires explicit answers to three questions: who builds the observability infrastructure, who operates and maintains it, and who responds when it detects problems. The answers are not the same. The platform team that builds the infrastructure is not the product team that responds to incidents. The engineering team that operates the dashboards is not the executive team that reviews weekly metrics. Ownership must be explicit, documented, and enforced. Anything less leads to the 2:47 AM escalation failure.

## The Platform-Builds-Product-Owns Model

The most successful ownership model at scale is platform-builds-product-owns. The platform engineering team builds and maintains the observability infrastructure: the instrumentation libraries, the telemetry pipeline, the storage layer, the query interfaces, and the alerting framework. They provide the system. They do not provide the signals. The product engineering teams own the signals specific to their AI systems: which metrics to track, which thresholds to alert on, which dashboards to maintain, and which runbooks to write. They provide the domain knowledge. They do not build the infrastructure.

This division works because it aligns responsibility with knowledge and capacity. Platform teams have the expertise to build reliable infrastructure but lack the domain knowledge to define good vs bad model behavior for every AI product. Product teams have the domain knowledge but lack the capacity to build telemetry pipelines and storage layers. Platform teams should not decide that a sentiment score below zero point seven requires an alert. They do not know the product well enough to set that threshold. Product teams should not build custom logging pipelines for every AI feature. They do not have the infrastructure expertise or the time.

The boundary is instrumentation vs configuration. Platform provides instrumentation: SDKs that make it easy to log inputs, outputs, latencies, token counts, and semantic metrics. Product provides configuration: which inputs to log, which outputs to track, which thresholds trigger alerts, and which dashboards matter for their feature. A product team launching a contract summarization feature should not write custom logging code. They should call platform-provided functions that automatically capture inputs, outputs, model versions, and latencies. But they should define what "good summarization" looks like, what quality score requires escalation, and what the runbook says to do when quality drops.

Concretely, platform owns the observability infrastructure roadmap, the uptime of telemetry systems, the reliability of dashboards, and the training materials that teach product teams how to use the system. Product owns the specific metrics for their features, the alert thresholds, the on-call rotation, and the incident response. If the telemetry pipeline goes down, platform is paged. If a model quality alert fires, product is paged. This separation keeps incentives aligned. Platform teams are measured on infrastructure reliability and adoption. Product teams are measured on feature quality and uptime. Both teams need observability to succeed, but for different reasons.

## RACI Matrices for AI Observability and Why They Matter

RACI matrices feel like bureaucratic overhead until you experience an incident where nobody knows who makes decisions. RACI stands for Responsible, Accountable, Consulted, Informed. For every observability decision and operational task, you need explicit answers. Who is Responsible for doing the work? Who is Accountable for the outcome? Who must be Consulted before decisions are made? Who must be Informed after decisions are made? Without this clarity, decisions stall, work gets duplicated, and incidents escalate to the wrong people.

For building new observability capabilities, Platform is Responsible and Accountable. Product is Consulted to ensure the capabilities meet their needs. Leadership is Informed when capabilities ship. For defining metrics and thresholds for a specific AI feature, Product is Responsible and Accountable. Platform is Consulted to ensure metrics are technically feasible and performant. Leadership is Informed when alerts are configured. For responding to a model quality incident, the Product team that owns the feature is Responsible and Accountable. Platform is Consulted if the issue involves infrastructure. Leadership is Informed if the incident is customer-facing or high-severity.

For maintaining dashboards, ownership depends on the dashboard type. Executive dashboards showing company-wide AI health are owned by Platform, with Product Consulted on what metrics matter. Feature-specific dashboards showing per-product quality trends are owned by Product, with Platform Consulted on technical feasibility. Shared dashboards showing cross-cutting metrics like cost, latency, or compliance are owned by Platform, with Product Informed but not Consulted unless their features are outliers.

Write this down. Do not assume everyone shares the same mental model. A two-page RACI document that lists every observability responsibility and assigns roles prevents more incidents than most alerting improvements. Include specific examples: who is Responsible for writing the runbook when a new AI feature launches? Who is Accountable for ensuring alert thresholds are tuned after the first week in production? Who must be Consulted before changing the telemetry sampling rate? Who must be Informed when a dashboard is deprecated? If your team cannot answer these questions in thirty seconds, you do not have an ownership model. You have an ownership vacuum.

## The Danger of Orphaned Dashboards and Unowned Alerts

The most common observability failure is not missing dashboards. It is dashboards nobody looks at. In mid-2025, a healthcare company had forty-seven dashboards tracking AI system behavior. A six-month audit found that thirty-two of those dashboards had not been viewed in over three months. Eleven dashboards were tracking metrics for features that no longer existed. Eight dashboards were duplicates with slightly different queries. The total cost of maintaining these dashboards was fourteen thousand dollars per year in infrastructure and query costs. The value was zero.

Orphaned dashboards are a symptom of unclear ownership. Someone builds a dashboard during a project. The project ships. The dashboard remains. Six months later, nobody remembers why it exists or what decisions it was supposed to inform. Alerts follow the same pattern. A team sets up an alert during development. The alert fires occasionally but never indicates a real problem. The team starts ignoring it. A year later, the alert is still firing, still being ignored, and still paging someone who has no idea what to do. This is alert fatigue by accumulation.

Prevent this with an ownership review cadence. Every quarter, Platform reviews all dashboards and alerts. For each dashboard, identify the owner. If the owner no longer exists or the dashboard has not been viewed in ninety days, mark it for deprecation. For each alert, identify the on-call rotation it pages. If the alert has not fired in ninety days or has fired without leading to action, mark it for tuning or removal. Deprecate or fix everything marked. This review should take one engineer one day per quarter. The cost is trivial. The benefit is massive.

Make ownership visible in the observability system itself. Dashboards should display the owner team, the last update date, and the last view date. Alerts should display the on-call rotation, the escalation path, and the link to the runbook. If someone opens a dashboard and sees it was last updated eighteen months ago, they immediately know it is stale. If someone receives an alert and the runbook link returns a 404, they immediately know ownership has broken down. Visibility creates accountability. Invisible ownership creates orphaned infrastructure.

## Embedding Observability Engineers in AI Product Teams

For organizations building multiple high-stakes AI products, embedding observability engineers in product teams scales ownership better than centralized models. An embedded observability engineer is a platform team member who spends fifty to eighty percent of their time working with one or two product teams, helping them define metrics, tune alerts, build dashboards, and respond to incidents. They bring platform expertise into the product context. They translate product requirements into platform capabilities. They are the bridge between "we need to know if summarization quality is dropping" and "we will track semantic similarity between summaries and source documents, alert if the seven-day moving average drops below zero point eight-two, and page the product on-call."

This model works at scale because it distributes platform knowledge without duplicating platform engineering. A five-person platform team can support twenty product engineers by embedding one platform engineer with every four product engineers. The embedded engineer handles instrumentation questions, reviews metric definitions, debugs telemetry issues, and represents product needs in platform planning. The product team gets expert help without building their own observability stack. The platform team gets product feedback without losing context across dozens of features.

The embedded engineer is not on the product on-call rotation. They are Consulted when incidents involve observability infrastructure, but they are not Responsible for responding to model quality alerts. This distinction matters. If the embedded engineer is on-call for product issues, they become a product engineer who happens to know observability. The goal is the opposite: a platform engineer who deeply understands product context. They train product engineers to respond to alerts, not respond to alerts themselves.

Embedding works best when the embedded engineer rotates every six to twelve months. Permanent embedding risks the engineer losing touch with platform evolution and becoming siloed in one product domain. Rotation keeps platform skills sharp and spreads observability expertise across the organization. A product team that has had an embedded observability engineer for six months should be self-sufficient in defining metrics and tuning alerts. If they still depend on the embedded engineer for basic observability tasks after six months, the embedding failed. The goal is to build product team capability, not create permanent dependencies.

## The On-Call Rotation Question for AI Observability

Who gets paged when an AI observability alert fires? This is not a philosophical question. It is a design question that determines whether your observability system accelerates incident response or creates escalation chaos. The answer depends on what the alert detects. An alert for infrastructure failure pages the platform on-call. An alert for model quality degradation pages the product on-call. An alert for cost threshold breach pages whoever owns the budget. An alert for compliance violation pages both product and legal. The escalation path must be explicit in the alert definition, not discovered during the incident.

For product teams with dedicated on-call rotations, AI observability alerts go to the same rotation that handles application errors and performance degradation. AI is not a separate operational concern. It is part of the product. If the product on-call gets paged when API latency spikes, they should also get paged when model quality drops. If the product on-call does not have context to respond to AI alerts, your on-call rotation is understaffed or undertrained. Fix the training, do not create separate on-call rotations for AI vs non-AI issues.

For product teams without dedicated on-call rotations, AI observability alerts should still page someone. The default should be the product engineering lead or the feature owner. If that person is not available, the alert escalates to the broader engineering on-call. The worst outcome is AI alerts going to a shared channel where they get lost among deployment notifications and infrastructure logs. Alerts that do not page a specific person do not get actioned. They get acknowledged and forgotten.

For high-stakes AI systems where degradation has immediate user impact or regulatory consequences, consider follow-the-sun on-call coverage. If your AI system serves global users and model quality degradation affects all regions simultaneously, a single-region on-call rotation means incidents during off-hours take longer to resolve. Follow-the-sun coverage distributes on-call responsibility across time zones so someone is always awake and available. This increases operational cost but decreases incident resolution time. The trade-off makes sense for Tier 1 and Tier 2 systems. It does not make sense for Tier 3 systems where delayed response is acceptable.

Document the on-call escalation path in every alert. When the alert fires, the recipient should immediately know: am I the right person to respond to this? If not, who is? If I cannot resolve this in fifteen minutes, who do I escalate to? If this turns into a major incident, who is the incident commander? These questions should have answers before the alert fires, not during the incident. Write runbooks that answer them. Test the runbooks quarterly by running incident simulation exercises. The simulation should start with an alert firing and end with the right person resolving the issue using the runbook. If the simulation reveals gaps, fix them before a real incident exposes the same gaps.

## Anti-Pattern: Everyone Owns It Meaning Nobody Owns It

The most dangerous observability ownership anti-pattern is the everyone-owns-it model. Leadership declares that observability is a shared responsibility. Every team is expected to instrument their systems, maintain their dashboards, and respond to their alerts. In theory, this distributes ownership and prevents silos. In practice, this creates a tragedy of the commons where nobody feels accountable because everyone is nominally responsible.

The failure mode is predictable. Teams instrument the minimum required to ship. Dashboards are built during development and never updated. Alerts are set to defaults nobody tuned. When an alert fires, teams assume someone else is handling it. When a dashboard shows concerning trends, teams assume someone else is investigating. Six months later, you have extensive observability infrastructure and no operational discipline. The infrastructure exists but nobody uses it effectively.

This anti-pattern appears most often in organizations transitioning from small to medium scale. When the company had twenty engineers, everyone really did own everything. Informal communication worked. When the company has eighty engineers, informal communication breaks down. What worked at twenty people fails at eighty. The symptom is incidents where multiple teams are involved but nobody knows who is in charge. The root cause is ownership that was never formally transitioned from informal shared responsibility to explicit role-based accountability.

Fix this by making ownership specific. Not "everyone on the AI team owns observability." Instead: "Platform owns infrastructure, Product owns metrics and alerts, Engineering leadership owns quarterly ownership reviews." Not "all teams should monitor their AI systems." Instead: "The Summarization team owns the summarization quality dashboard, the Translation team owns the translation quality dashboard, Platform owns the cost and latency dashboards." Specificity creates accountability. Vagueness creates diffusion of responsibility.

## Making Ownership Visible and Enforced

Ownership does not matter if it is not visible and enforced. Visibility means anyone in the organization can look up who owns a dashboard, an alert, a metric, or a runbook. Enforcement means ownership is reviewed regularly and ownership gaps trigger action. Both require tooling and process.

For visibility, your observability platform should support ownership metadata. Every dashboard has an owner field showing the team and the individual responsible. Every alert has an owner field showing the on-call rotation and the escalation path. Every metric has an owner field showing who defined it and when it was last reviewed. This metadata should be searchable. If someone asks "who owns the model quality dashboard for contract summarization," they should get an answer in fifteen seconds from the observability platform, not from asking around Slack.

For enforcement, ownership should be a required field. You cannot create a dashboard without assigning an owner. You cannot create an alert without defining an escalation path. If an owner leaves the company or changes roles, their ownership assignments are automatically flagged for reassignment. If a dashboard goes unviewed for ninety days, it is automatically flagged for review. If an alert fires and nobody acknowledges it within fifteen minutes, it escalates automatically.

Enforcement also requires periodic reviews. Every quarter, Platform reviews all observability assets and reports ownership gaps to engineering leadership. Gaps include dashboards without owners, alerts with broken escalation paths, runbooks with outdated information, and metrics nobody is tracking. Leadership assigns someone to fix each gap. The review should take two hours per quarter. The alternative is ownership decay that turns good infrastructure into ignored infrastructure over eighteen months.

Ownership is not a one-time decision. It is an ongoing operational discipline. The team that builds ownership into their observability culture from day one avoids the escalation chaos, the ignored alerts, and the 2:47 AM incident where nobody knows who to page. The team that assumes ownership will sort itself out discovers during incidents that it has not.

Next, you need a reference architecture that ties all these concepts together: how signals are captured, how they move through your system, where they are stored, how they are queried, and how they drive action. The 2026 AI observability architecture is not a single tool. It is a layered system where each layer has a specific responsibility.
