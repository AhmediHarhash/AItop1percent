# 7.2 — Alert Categories: Critical, Warning, and Informational

Most alerting systems claim to have multiple severity levels. In practice, they have one: whatever fires gets ignored until it becomes an outage. The problem is not that teams choose wrong severity levels. The problem is that severity levels do not map to operational behavior. Critical, high, medium, low — these labels describe how bad something is, not what humans should do about it. Behavior is what matters. Effective alert categories predict and prescribe action.

The companies with reliable AI systems use category systems that directly encode response expectations. One global financial services firm simplified their AI alerting to three categories: **Page**, **Review**, and **Track**. Page means interrupt immediately, acknowledge within five minutes, investigate now. Review means examine during the next business day, batch with other issues, prioritize by impact. Track means the system records it, humans see it in weekly retrospectives if at all. The categories do not describe severity. They describe workflow.

This reframing solves the alert fatigue problem at the design level. When you categorize by action instead of severity, every alert rule must answer a concrete question: what should the human do when this fires? If the answer is "investigate immediately," it is a Page. If the answer is "look at it tomorrow," it is Review. If the answer is "maybe notice a trend over time," it is Track. Anything else is not an alert — it is monitoring data that belongs in dashboards, not notification channels.

## Page-Level Alerts: The Interrupt Contract

A Page-level alert is a contract. The system promises: this condition requires immediate human action to prevent or mitigate user harm. The human promises: I will acknowledge within five minutes and begin investigation immediately. Both sides must honor the contract. If the system pages for conditions that do not require immediate action, engineers stop trusting it. If engineers do not respond to pages immediately, the system becomes decorative.

An education AI platform spent eight months refining their Page-level alert criteria. They started with thirteen alerts categorized as critical. They applied a four-part test to each alert. First: does this condition indicate current or imminent user impact? Second: can a human take action within the next thirty minutes that meaningfully improves outcomes? Third: will delaying response until morning make the situation materially worse? Fourth: is the condition rare enough that engineers can sustain immediate response over time? All four must be true. Only three of the original thirteen alerts passed.

The three that remained were genuinely critical. Model returning invalid safety classifications — immediate user harm, could be fixed by routing traffic to backup model, delay risked dangerous content reaching users, occurred less than once per month. API response rate dropping below fifty percent — users experiencing failures, could be mitigated by scaling infrastructure or disabling expensive features, delay caused extended outage, occurred twice per quarter. Data pipeline completely stopped — no predictions possible, required manual intervention to restart, delay extended outage duration, occurred once per quarter.

The other ten alerts failed at least one criterion. Model confidence scores drifting lower — might indicate future problems but no current user impact. Token usage exceeding projections — actionable but not immediately, could wait until morning. Cache hit rate dropping — interesting but not user-facing unless it caused latency. Unusual query patterns — worth investigating but not urgent. All ten moved to Review category. Pages per month dropped from forty-two to four. Response time to actual pages improved from twelve minutes to ninety seconds. The engineers started treating every page as an emergency because every page was an emergency.

The mechanism is that scarcity creates urgency. When pages are rare, each page receives full attention. When pages are common, each page receives divided attention. The division is not conscious. It is automatic cognitive triage. Humans cannot maintain high alert for frequent stimuli. The body habituates. The response dampens. This is not a training problem. It is biology. The only solution is to make pages rare enough that habituation does not occur. Industry data from incident response studies suggests the threshold is approximately five pages per week per engineer. Above that, response quality degrades measurably.

This standard forces hard choices. Many conditions feel important enough to warrant immediate notification. Most are not. The test is not "should we know about this?" The test is "must we act on this right now?" The difference is the entire alerting discipline. Should-know is monitoring. Must-act-now is paging. Conflating them destroys both. You end up with monitoring systems that interrupt humans and paging systems that are ignored. Separating them creates monitoring systems that inform and paging systems that command.

## Review-Level Alerts: The Daily Digest

Review-level alerts represent conditions that require human attention but not immediate response. These go to batched reports — daily digests, morning briefings, team channel posts — reviewed during business hours by engineers with full context and unrushed judgment. The key distinction from Page-level: delaying response until the next business day does not materially worsen outcomes.

A logistics AI company built their Review category around the concept of degradation without outage. Their route optimization model occasionally produced routes that were suboptimal but valid — ten percent longer than expected, or using more vehicles than necessary, but still delivering all shipments on time. These degradations indicated problems: stale training data, incorrect features, broken optimization heuristics. They required investigation and fixes. They did not require 3 AM pages. The next morning was soon enough.

They created a daily operations report that summarized all Review-level conditions detected in the previous twenty-four hours. Model quality metrics below baseline. API latency elevated but not critical. Cache effectiveness declining. Unusual error patterns. Infrastructure utilization trending upward. Each condition included current values, historical context, and suggested investigation steps. The report went to the on-call engineer and the broader team. It was reviewed during morning standup. Issues were triaged, assigned, and tracked. Response happened within one business day. User impact was minimal because the conditions had not yet crossed into outage territory.

The pattern that makes Review category work is context richness. Page-level alerts must be sparse — just enough information to trigger immediate action. Review-level alerts can be detailed — full context, historical trends, related signals. When an engineer reviews a daily digest, they are not responding to an interrupt. They are analyzing a situation with their full cognitive capacity. The format can be dense. The analysis can be nuanced. The response can be considered. This is the opposite of a page, where the goal is minimal information for maximum speed.

Review alerts also serve a prediction function. Many Page-level incidents are preceded by Review-level signals. Model confidence declining gradually before failing completely. API latency creeping up before spiking. Error rates increasing slowly before exploding. The Review category catches these early signals when intervention is still easy. If every Review alert is addressed promptly, many Page alerts never fire. This is the correct pattern. You want your monitoring to detect problems while they are still small and your alerting to handle the cases where problems became big before detection.

The failure mode is treating Review as "low priority Page" — a category that should interrupt engineers but not as urgently. This creates a third tier of notification that engineers learn to ignore. Review is not a Page at all. It is scheduled reporting. It does not go to paging systems. It goes to communication channels that humans check regularly but not immediately. Slack, email, issue trackers. The medium is the message. Using paging systems for Review-level alerts trains engineers to ignore paging systems. Using reporting systems for Review-level alerts trains engineers to incorporate review into their daily workflow.

## Track-Level Alerts: The Retrospective Layer

Track-level alerts are not really alerts. They are monitoring events that are recorded for trend analysis but do not trigger any immediate notification. These conditions are logged, aggregated, and surfaced during weekly or monthly retrospectives. The purpose is not to trigger action but to enable pattern recognition over time.

A healthcare AI company used Track-level monitoring for any model behavior that fell within acceptable parameters but deviated from typical patterns. Model selected the secondary algorithm instead of primary more frequently than usual — within acceptable range, might indicate subtle training drift. User queries included more negation words than typical — within normal variation, might indicate changing user needs. Response latencies were at the seventy-fifth percentile more often — still fast enough, might indicate early infrastructure strain. None of these conditions required immediate action. All provided useful signal for longer-term system understanding.

Track data was aggregated weekly and reviewed in a one-hour team session. The review looked for patterns: metrics that trended in concerning directions over multiple weeks, anomalies that repeated, correlations between seemingly unrelated signals. Many weeks, the review found nothing actionable. Some weeks, it surfaced early indicators of problems that became Review-level concerns. Occasionally, it revealed opportunities: user behavior changes that suggested feature improvements, model performance patterns that informed training priorities.

The key is that Track-level monitoring does not generate notifications. It generates data. The data is stored, indexed, visualized, but not pushed to humans. Humans pull it when they have time and context to interpret it meaningfully. This inverts the normal alerting model. Normal alerts push information to humans. Track monitoring makes information available for humans to pull. The distinction matters. Push creates interruption and context-switching cost. Pull creates optionality and focused analysis.

Track monitoring should be comprehensive. This is where you put all the metrics that seem important but do not meet the bar for Page or Review. Model internal states. Feature importance distributions. Embedding space characteristics. User interaction patterns. Infrastructure resource utilization. Query complexity distributions. All of it is worth tracking. None of it justifies interrupting a human. The comprehensiveness of Track monitoring is what allows Page and Review to be sparse. You are not ignoring those signals. You are consuming them on a different timescale.

The implementation pattern is to log everything to a time-series database with rich metadata, then build dashboards and query tools that let engineers explore the data during scheduled analysis sessions. The exploration is open-ended. What changed this week? What correlations exist? What anomalies appeared? The questions drive the investigation. The Track data provides answers. This is how you gain deep system understanding without drowning in alerts.

## The Reclassification Problem: When Categories Drift

Alert categories are not permanent. A condition that starts as Track can become Review if it trends poorly. A Review condition can escalate to Page if thresholds are crossed. A Page condition can downgrade to Review if system changes make immediate response less critical. The category system must support reclassification based on operational experience.

A retail AI company tracked alert reclassification as a key reliability metric. Every quarter, they reviewed their alert rules and categorization. Alerts that fired frequently moved down in category or were deleted entirely. Alerts that preceded incidents but were not paging moved up in category. Alerts that never fired were deleted. The review was data-driven: how many times did this alert fire? How many times did it correspond to real incidents? How long until humans responded? What actions were taken? The answers informed reclassification decisions.

Over two years, their Page-level alert count decreased from eighteen to six. Their Review-level alert count held steady at around fifteen. Their Track-level monitoring expanded to cover seventy metrics. The system became more selective at the top and more comprehensive at the bottom. This is the correct evolution. As you understand your system better, you need fewer interrupts and more instrumentation. The interrupt budget is constant — human attention. The instrumentation budget is flexible — storage and compute are cheap.

The reclassification discipline also prevents alert accumulation. Most alerting systems accumulate rules over time. Every incident generates new alerts. Few alerts ever get deleted. The result is a growing pile of increasingly irrelevant rules. Scheduled reclassification creates forcing functions: justify this alert's category or move it down. If you cannot justify a Page, make it Review. If you cannot justify Review, make it Track. If you cannot justify Track, delete it entirely. The bias should be toward deletion.

One useful heuristic: any alert that has not fired in six months is a candidate for reclassification or deletion. Either the condition truly never happens — in which case the alert is unused safety infrastructure that might be worth keeping at Track level — or the condition happens but the alert never catches it, which means the alert is misconfigured and needs repair or removal. Alerts that fire regularly justify their existence through use. Alerts that never fire require active justification. The burden of proof is on keeping them.

## The Cross-Team Problem: Category Systems at Scale

Alert categories become complicated in organizations with multiple teams and multiple systems. The Page-level alert for the model team is Track-level noise for the infrastructure team. The Review alert for product is critical for compliance. Category systems must account for audience.

An insurance company with eight engineering teams and fourteen AI systems solved this through routing rules that considered both category and audience. Each alert rule specified a category — Page, Review, or Track — and a primary audience — model team, API team, data platform team, or product team. The combination determined notification behavior. Page-level alerts went to the on-call engineer for the primary audience. Review-level alerts went to that team's digest channel. Track-level alerts were logged but not routed. Cross-cutting alerts could specify multiple audiences, but each audience could override the category for their own context.

This system acknowledged that severity is contextual. A data pipeline failure is a Page for the data platform team — they need to fix it immediately. It is Review for the model team — they need to know, but their models are still serving from cached data. It is Track for the product team — interesting context but not actionable. One condition, three categories, based on who is receiving the information and what they can do about it. The category system must be flexible enough to handle this reality.

The implementation was a routing layer between the monitoring system and notification systems. The monitoring system fired events with metadata: condition type, affected system, current metrics, historical context. The routing layer evaluated rules that combined the event metadata with team responsibility matrices to determine category and audience. The output was targeted notifications: pages to specific engineers, digests to specific channels, log entries to specific databases. The same event could generate all three or none, depending on the rules.

This approach prevents the common failure mode where every team receives every alert because no one wants to miss something important. That pattern creates alert fatigue at scale. Every team is overwhelmed with notifications that are critical for some other team. The routing layer implements the principle that alerts should be targeted, not broadcast. Broadcast creates noise. Targeted routing creates signal. The system should know who needs to act and notify only them. Everyone else can see the data in Track systems or retrospectives.

## The Documentation Problem: Categories Without Context

Category labels are useless without documentation. An alert marked "Page" tells you to respond immediately. It does not tell you what to do. An alert marked "Review" tells you to examine it tomorrow. It does not tell you what you are looking for. Category systems must be paired with runbooks that explain the intended response.

Every Page-level alert should have a runbook that answers five questions. What condition triggered this alert? What user impact is occurring or imminent? What investigation steps should you take first? What mitigation actions are available? When should you escalate? The runbook should be linked directly from the alert notification. The on-call engineer should be able to access it within seconds of receiving the page. The goal is to minimize time from alert to effective action.

A transportation AI company templated their Page-level runbooks to ensure consistency. Every runbook started with impact statement: what users are experiencing. Then immediate checks: three specific things to verify right now. Then mitigation options: ordered list of actions from safest to most aggressive. Then escalation criteria: when to page the next level. Then deep investigation: if mitigation works, what to examine for root cause. The template ensured that every runbook provided both immediate guidance and longer-term learning.

Review-level alerts need lighter documentation. The focus is not immediate action but informed analysis. The documentation should explain what the metric means, what normal ranges look like, what causes it to vary, and what actions have been effective in the past. The engineer reading a Review alert during morning standup is not under time pressure. They can read more context. The documentation can be richer. Historical trends, related metrics, past incident reports, links to code or architecture docs.

Track-level alerts need almost no documentation. These are raw data points. The documentation is the dashboard that visualizes them and the query tools that let engineers explore them. The weekly retrospective provides the context. Engineers looking at Track data are doing open-ended investigation. They are not following a runbook. They are forming hypotheses and testing them against the data. The documentation should explain what the data represents and how it is collected, but it does not need to prescribe actions.

The maintenance burden is real. Every documented alert is a page that needs updating when the system changes. This is another reason to minimize Page and Review alert counts. Every additional alert is not just a notification rule. It is a documentation commitment. If you cannot commit to maintaining the runbook, you should not commit to the alert. Undocumented Page-level alerts are worse than no alerts. They create panic without direction.

The next subchapter covers threshold setting — the technical decisions that determine when alerts actually fire, including static, dynamic, and anomaly-based approaches that match different operational needs.

