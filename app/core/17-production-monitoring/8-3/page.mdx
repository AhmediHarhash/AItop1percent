# 8.3 — Incident Severity Classification for AI Failures

The alert fired at 3:17 AM. Model confidence scores had dropped by 14 percentage points. The on-call engineer woke up, opened the dashboard, and faced a question with no clear answer: how bad is this? The system was still serving requests. Latency was normal. Error rates were baseline. But something had changed. Was this a page-the-VP situation or a create-a-ticket-for-Monday situation? The engineer spent twenty minutes investigating, decided it was not critical, and went back to sleep. By morning, the issue had escalated. The confidence drop was a leading indicator of hallucination — the model was generating outputs it was less certain about, and many were factually wrong. What looked like a minor metrics deviation at 3:17 AM was a severity-one incident by 8:00 AM. The classification failure cost five hours of response time.

Traditional software has clear severity definitions. Severity one means the service is down. Severity two means degraded performance. Severity three means a bug with workarounds. AI incidents do not fit these categories cleanly. The system can be fully operational while producing catastrophically bad outputs. Latency can be perfect while the model exposes confidential data. You need a severity framework that accounts for output quality, safety, and business impact, not just availability.

## The Four-Tier AI Severity Framework

AI incident severity should be classified across four tiers: critical, high, medium, and low. Each tier has specific response requirements, escalation paths, and resolution timelines. The classification is based on three dimensions: user impact, data exposure risk, and system availability.

Critical incidents involve immediate user harm, data breaches, or total system failure. A healthcare prior authorization system in December 2025 had a critical incident when the model started approving medically inappropriate procedures. The outputs were not just wrong — they were dangerous. Patients could have received unnecessary surgeries. The incident required immediate escalation to executive leadership, coordination with clinical teams, and notification to affected patients. Resolution timeline: minutes to hours. The team took the system offline, rolled back to the previous model, and manually reviewed every authorization issued in the past six hours. Critical incidents require all-hands response and external communication.

High incidents involve significant degradation, policy violations, or partial service outages that affect large user populations but do not cause immediate harm. A legal document assistant in May 2025 had a high incident when the model started hallucinating case citations. The outputs were wrong and could mislead users, but the impact was not immediate physical harm. Clients would catch the fake citations during their own review processes. The incident required engineering response within one hour, product team notification, and user communication within four hours. Resolution timeline: hours. High incidents require dedicated engineering focus but do not always require executive escalation.

Medium incidents involve localized degradation, minor policy violations, or issues affecting small user populations. A travel booking assistant in March 2026 had a medium incident when the model started using overly casual language with business travel customers. The outputs were functional but tonally inappropriate for the context. User satisfaction dropped, but no bookings failed. The incident required investigation within four hours and resolution within one business day. Medium incidents are handled by the on-call rotation without broader escalation unless they persist or worsen.

Low incidents involve minor bugs, edge case failures, or issues with minimal user impact. A content summarization tool in July 2025 had a low incident when the model started generating summaries that were five words longer than the configured maximum. The outputs were still useful. Users did not complain. But the behavior violated the specification. The incident was logged and queued for the next sprint. Resolution timeline: days to weeks. Low incidents do not require immediate response but should be tracked to detect patterns.

## The User Impact Dimension

User impact is the primary severity driver. If users are harmed, misled, or unable to complete critical tasks, severity is high. If users are mildly inconvenienced or do not notice the issue, severity is low.

A financial advisory platform had an incident where the model's latency increased from 1,200 milliseconds to 2,800 milliseconds. The increase was significant — more than double. But users did not complain. The experience was still acceptable. The engineering team classified it as medium severity and investigated during business hours. They found that a dependency service was responding slowly and fixed it within six hours. No users were notified. The impact was minimal.

Contrast this with an incident at a customer service chatbot where response times increased from 800 milliseconds to 1,400 milliseconds. The absolute increase was smaller, but users noticed. Satisfaction scores dropped. Complaint volume spiked. The team classified it as high severity because user impact was measurable and immediate. They escalated to the platform team, identified a resource contention issue, and resolved it within ninety minutes. The difference was not the metrics — it was how users experienced the change.

User impact assessment requires understanding what users care about. For a real-time voice assistant, 200 milliseconds of added latency is critical. For a document analysis tool, 200 milliseconds is irrelevant. For a content moderation system, a 2 percent increase in false negatives is critical. For a marketing copy generator, a 2 percent increase in awkward phrasing is medium severity. You cannot classify severity from metrics alone. You need context about what those metrics mean for user experience.

## The Data Exposure Risk Dimension

Data exposure risk drives severity independent of user impact. If an incident involves leaking sensitive data, exposing confidential information, or violating privacy policies, severity is automatically high or critical regardless of how many users are affected.

A contract analysis platform had an incident in September 2025 where the model started including snippets of other users' contracts in generated outputs. The issue affected only three users. The functional impact was minimal — the snippets were brief and did not disrupt the primary output. But the data exposure risk was critical. The platform was leaking confidential legal information across customer boundaries. The team classified it as critical, took the system offline, notified affected users, and conducted a full audit of all outputs generated during the exposure window. The incident required legal review and external communication to regulators.

Data exposure incidents have binary severity: either data was exposed or it was not. If it was exposed, severity is at least high, usually critical. If confidential data, PII, or regulated information appeared in outputs where it should not have, the response is immediate. If the exposure was theoretical — the model could have leaked data but did not — severity drops to medium or low depending on how close the system came to actual exposure.

A customer service chatbot had an incident in November 2025 where adversarial testing revealed that the model could be prompted to disclose customer email addresses under specific conditions. No actual user had triggered the vulnerability. No data had been exposed in production. But the risk was real. The team classified it as high severity, fixed the vulnerability within four hours, and ran a retroactive scan of production logs to confirm no exploitation had occurred. Potential exposure is not as severe as actual exposure, but it is severe enough to require immediate remediation.

## The System Availability Dimension

System availability is the third severity dimension. If the system is completely down, severity is critical. If the system is degraded but functional, severity depends on the degree of degradation and the criticality of the use case.

A document summarization platform had an incident in April 2025 where 15 percent of requests were failing with timeout errors. The system was 85 percent operational. For a non-critical internal tool, this might be medium severity. But the platform served legal teams working under tight deadlines. The 15 percent failure rate was blocking critical work. The team classified it as high severity, escalated immediately, and resolved it within two hours by scaling up inference capacity.

System availability severity is context-dependent. A 5 percent error rate for a marketing tool is medium severity. A 5 percent error rate for a medical diagnosis system is critical. A 50 percent latency increase for an internal tool is low severity. A 50 percent latency increase for a customer-facing real-time assistant is high severity. The same metric degradation has different severity depending on what the system does and who depends on it.

The framework requires pre-defining availability thresholds for each severity tier. For a healthcare system, greater than 1 percent error rate is high severity, greater than 5 percent is critical. For a content generation tool, greater than 10 percent error rate is medium severity, greater than 25 percent is high. These thresholds should be documented, reviewed quarterly, and updated as user expectations evolve.

## The Composite Severity Decision

Most incidents do not fit cleanly into one dimension. A typical incident affects user experience, carries some data risk, and degrades availability. Composite severity is determined by the highest severity dimension, not the average.

A travel booking assistant had an incident in January 2026 where the model was generating incorrect hotel recommendations for 8 percent of queries. User impact was moderate — most users noticed and searched again. Data exposure risk was none. Availability was 92 percent. Each dimension individually would be medium severity. But the combination — measurable user impact plus non-trivial availability degradation — pushed the composite severity to high. The team responded immediately.

The rule is: if any dimension is critical, composite severity is critical. If any dimension is high and no dimension is critical, composite severity is high. Only if all dimensions are medium or low does composite severity drop to medium. This conservative approach ensures that incidents are not under-classified because of averaging across dimensions.

A contract analysis platform had an incident where model latency increased by 300 percent — from 600 milliseconds to 2,400 milliseconds. User impact was high. But data exposure risk was none. System availability was 100 percent. The team initially classified it as medium severity because the system was still working. But users were abandoning sessions due to slow responses. The product team reclassified it as high severity based on user impact alone, and engineering prioritized it immediately. The lesson: any single dimension can drive severity up. No dimension can pull severity down when another dimension is high.

## The Temporal Severity Escalation Rule

Severity is not static. An incident that starts as medium severity can escalate to high or critical if it persists or worsens. A healthcare documentation assistant had an incident in October 2025 where eval pass rates dropped from 94 percent to 89 percent. The initial classification was medium severity because the degradation was modest and no user complaints had been received. The engineering team investigated, found no obvious cause, and continued monitoring.

Over the next three hours, eval pass rates dropped further to 82 percent. User complaints started arriving. The team reclassified the incident as high severity and escalated to senior engineering. Within another hour, eval pass rates hit 71 percent and the model started generating clinically incorrect summaries. The incident was reclassified as critical. The system was taken offline. What started as a medium-severity metrics anomaly became a critical patient safety issue.

Temporal escalation rules should be explicit. If a medium-severity incident persists for more than four hours, reclassify as high. If a high-severity incident persists for more than two hours, reclassify as critical. If severity indicators worsen — user complaints increase, error rates rise, data exposure risk emerges — reclassify immediately. The goal is to avoid complacency. A persistent or worsening incident is more severe than an equivalent snapshot would suggest.

## The Safety and Policy Violation Severity Override

Any incident involving safety violations or policy breaches is automatically high or critical severity, regardless of frequency or user impact. A content moderation platform in June 2025 had an incident where the model failed to flag a single piece of prohibited content. Just one miss. But the content was illegal in the jurisdiction where the platform operated. The team classified it as critical, conducted a full audit of all content processed in the past week, and reported the incident to legal and compliance teams. One violation was enough to trigger maximum severity response.

Safety and policy violations cannot be averaged or diluted. If the model generates a single output that violates HIPAA, that is critical. If the model exposes a single user's PII to another user, that is critical. If the model generates content that violates platform policies in one out of 10,000 requests, that is still high severity. The frequency does not matter. The existence of the violation is sufficient.

This severity override applies even when the violation is caught before user impact. A customer service chatbot had an incident in August 2025 where the model generated an output containing another user's account number. The output was caught by an automated redaction filter before reaching the user. No data was actually exposed. But the model had attempted to expose data. The team classified it as high severity, investigated the root cause, and implemented additional training to prevent similar outputs. The filter had worked. But the underlying model behavior was still unacceptable.

## The Cross-Customer Severity Multiplier

For multi-tenant systems, incidents affecting multiple customers are more severe than incidents affecting a single customer. A SaaS contract analysis platform had an incident in March 2026 where one customer's fine-tuned model was generating incorrect outputs. The issue was isolated to that customer. Severity was high because the outputs were wrong and the customer was relying on them for legal work. But the incident did not escalate beyond the customer success and engineering teams.

Two weeks later, a similar incident occurred, but it affected the base model used by all customers. The root cause was the same. The impact per customer was identical. But the scope was 200 customers instead of one. The team classified it as critical, escalated to executive leadership, and coordinated a company-wide response with customer notifications, rollback plans, and manual review of affected outputs. The per-customer impact did not change. The severity changed because of the multiplier effect.

The cross-customer multiplier applies when incidents affect shared infrastructure, base models, or common data pipelines. If an incident affects N customers, severity increases by one tier when N crosses specific thresholds. For 1-10 customers: base severity. For 11-50 customers: increase by one tier. For greater than 50 customers: increase by two tiers, with a floor of high severity. These thresholds should be tuned based on your customer base size and risk tolerance.

## The Severity Classification Checklist

When an incident is detected, the on-call engineer should evaluate severity using a checklist, not intuition. The checklist ensures consistent classification across different responders and different types of incidents.

Is any user harmed, misled, or blocked from critical tasks? If yes, severity is at least high. Is sensitive data exposed, leaked, or accessible to unauthorized users? If yes, severity is at least high. Is the system producing outputs that violate safety or policy constraints? If yes, severity is at least high. Is the system completely down or is error rate greater than the critical threshold? If yes, severity is critical. Is the issue affecting multiple customers or shared infrastructure? If yes, increase severity by one tier.

If all answers are no, evaluate degradation magnitude. Is the metric deviation greater than 25 percent from baseline? If yes, severity is medium. Is the issue reproducible and affecting a meaningful percentage of users? If yes, severity is medium. If the issue is isolated, intermittent, and has minimal user impact, severity is low.

A financial advisory platform documented this checklist in their incident response runbook. When an alert fired, the on-call engineer ran through the checklist in under two minutes and made a classification decision. The classification determined response time, escalation path, and communication requirements. The checklist removed ambiguity and reduced the time spent debating severity during the critical first minutes of an incident.

## The Reclassification Trigger Points

Severity should be reclassified as new information emerges. An incident that initially appears low severity may be reclassified as high when the scope becomes clear. A travel booking assistant had an alert for increased latency. Initial investigation suggested the issue was isolated to a single region. Severity: medium. Further investigation revealed the issue was present in all regions but masked by traffic distribution. Severity: high. Later, the team discovered that the latency increase was causing booking failures, not just slow responses. Severity: critical. The incident was reclassified twice as the team's understanding evolved.

Reclassification triggers should be explicit. Reclassify to higher severity if: user complaints exceed five per hour, error rate increases by more than 50 percent, data exposure is discovered, or the issue persists for longer than expected resolution time. Reclassify to lower severity if: user complaints stop, metrics return to baseline even without intervention, or the issue is confirmed to be isolated with no risk of expansion.

The reclassification decision requires the same rigor as initial classification. Document the reason for reclassification. Update stakeholders. Adjust response plans. A content moderation platform had an incident where they reclassified from high to medium severity after confirming that a suspected policy violation was actually a false positive from the detection system. The model had not violated policies. The monitor had misclassified a benign output. The reclassification allowed the team to reduce response urgency and focus on fixing the monitoring issue rather than treating it as a safety incident.

## The Cost of Misclassification

Under-classifying severity leads to slow response, inadequate resources, and extended user impact. Over-classifying severity leads to alert fatigue, wasted effort, and eventual desensitization. The goal is accurate classification, not conservative classification.

A healthcare documentation platform had a history of over-classifying incidents. Every metrics deviation was treated as high severity. The engineering team was paged frequently for issues that turned out to be transient noise. After six months, engineers stopped responding with urgency. When a real critical incident occurred — the model was generating clinically incorrect summaries — the initial response was slow because the team assumed it was another false alarm. The incident lasted four hours longer than it should have because over-classification had trained the team to treat all alerts as non-urgent.

Under-classification is equally dangerous. A customer service chatbot had an incident where eval pass rates dropped by 8 percent. The on-call engineer classified it as low severity because users had not complained yet. The engineer created a ticket for Monday and went back to sleep. By morning, user complaints had arrived. The issue was not transient — the model was generating incorrect information consistently. The engineering team spent the next day in crisis mode, fixing an issue that should have been addressed immediately. The classification error cost twelve hours of response time and significant user impact.

Accurate classification requires calibration. Review past incidents quarterly. Were high-severity incidents actually high-severity in retrospect? Were low-severity incidents correctly classified? Adjust thresholds and checklists based on historical data. A document analysis platform reviewed 30 incidents from 2025 and found that they had under-classified five and over-classified three. They adjusted their severity thresholds to better align with actual user impact and data exposure risk. The next quarter, classification accuracy improved from 73 percent to 91 percent.

Once severity is classified, the next challenge is the first five minutes of response. What do you do immediately? What information do you gather? Who do you escalate to? The initial triage determines whether the incident is resolved quickly or spirals out of control.

