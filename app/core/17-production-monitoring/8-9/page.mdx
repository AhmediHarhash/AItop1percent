# 8.9 — Incident Metrics: MTTD, MTTR, and AI-Specific KPIs

The VP of Engineering asked a simple question: "How reliable are we?" The engineering manager pulled up a dashboard showing 99.8 percent uptime. The VP said: "That tells me the system is available. It does not tell me the system works correctly." The traditional reliability metrics — uptime, error rate, latency — were green. But incidents were happening. Models were degrading. Users were complaining. The metrics were measuring the wrong things. Three weeks later, the engineering team deployed a new dashboard with AI-specific incident metrics: mean time to detect quality degradation, mean time to restore correct outputs, incident frequency by failure mode, and percentage of incidents caught by monitoring versus users. The new dashboard told a different story. Detection latency averaged 42 minutes. Twenty-eight percent of incidents were reported by users before monitoring detected them. The system was available. But it was not reliably correct. The metrics changed how the team thought about reliability.

Traditional incident metrics focus on availability and performance. For AI systems, you need metrics that capture output quality, detection speed, and the gap between technical health and actual correctness. Incident metrics serve two purposes: they tell you how well your incident response is working, and they reveal which parts of your system are most fragile. The metrics you track shape what you optimize for. If you only measure uptime, you optimize for availability. If you measure detection latency and output correctness, you optimize for reliability.

## Mean Time to Detect: The First Critical Metric

Mean time to detect — MTTD — measures how long it takes from the first failure to awareness that an incident is happening. For AI systems, MTTD is often the dominant contributor to total incident duration because failures are ambiguous and hard to detect.

A healthcare documentation platform tracked MTTD for every incident in 2025. The average was 38 minutes. Breaking it down by detection method: incidents detected by automated monitoring had MTTD of 11 minutes, incidents detected by user complaints had MTTD of 64 minutes, incidents detected by proactive review had MTTD of 4.2 days. The metric revealed a clear pattern: automated detection was 6x faster than user-reported detection and 550x faster than review-based detection. The insight drove investment decisions. The team expanded automated monitoring to cover more failure modes, reducing the percentage of user-reported incidents from 31 percent to 12 percent over six months.

MTTD should be broken down by incident severity. A financial advisory platform tracked MTTD separately for critical, high, medium, and low severity incidents. They found that critical incidents had MTTD of 7 minutes, high incidents had MTTD of 22 minutes, medium incidents had MTTD of 88 minutes. The difference made sense — critical incidents had tighter monitoring and more obvious symptoms. But the team set a goal: reduce MTTD for high-severity incidents to under 10 minutes. They achieved it by implementing tighter thresholds and faster alert routing.

MTTD can also be broken down by failure mode. A customer service chatbot tracked MTTD for different incident types: hallucination incidents, latency incidents, safety violation incidents, and quality degradation incidents. Hallucination incidents had MTTD of 15 minutes because they were caught by content-based monitoring. Latency incidents had MTTD of 3 minutes because they were caught by infrastructure monitoring. Quality degradation incidents had MTTD of 51 minutes because they required eval-based detection. The breakdown revealed that quality degradation was the hardest failure mode to detect quickly. The team invested in continuous eval to reduce that MTTD.

## Mean Time to Restore: The Second Critical Metric

Mean time to restore — MTTR — measures how long it takes from detection to full resolution. For AI incidents, MTTR includes investigation time, mitigation time, and validation time.

A legal document assistant tracked MTTR for all incidents in 2025. The average was 73 minutes. Breaking it down by resolution method: incidents resolved by rollback had MTTR of 34 minutes, incidents resolved by traffic routing had MTTR of 18 minutes, incidents resolved by output filtering had MTTR of 61 minutes, incidents resolved by retraining had MTTR of 9.4 hours. The metric revealed that mitigation strategy massively affected resolution time. The team optimized for fast mitigation strategies. They implemented one-click rollback and pre-deployed multiple model versions to enable traffic routing. MTTR dropped to 41 minutes over six months.

MTTR should be broken down by incident complexity. A contract analysis platform tracked MTTR separately for incidents with known root causes versus incidents with novel root causes. Known root cause incidents had MTTR of 29 minutes because the team followed established runbooks. Novel root cause incidents had MTTR of 127 minutes because they required exploratory investigation. The difference highlighted the value of documentation and runbooks. The team invested in expanding their incident playbook with more failure modes and diagnostic procedures.

MTTR can also be broken down by time of day. A travel booking assistant found that incidents occurring during business hours had MTTR of 38 minutes, while incidents occurring overnight had MTTR of 94 minutes. The overnight incidents had slower response because fewer engineers were awake and coordination was harder. The team decided this was acceptable — overnight incidents affected fewer users. But they documented on-call procedures more thoroughly to help overnight responders work independently without waiting for help.

## Mean Time Between Incidents: The Stability Metric

Mean time between incidents — MTBI — measures the average time between incidents. High MTBI indicates a stable system. Low MTBI indicates fragility.

A document summarization platform tracked MTBI over 2025. In Q1, MTBI was 6.2 days. In Q2, after significant stability investments, MTBI increased to 11.8 days. In Q3, MTBI dropped to 4.1 days after a major model upgrade introduced new failure modes. In Q4, after stabilizing the new model, MTBI increased to 14.3 days. The trend line showed that system stability improved over the year, but major changes caused temporary destabilization. The metric informed release strategy. The team now implements major changes in smaller increments to avoid the destabilization spikes.

MTBI should be broken down by component. A customer service chatbot tracked MTBI separately for model-related incidents, infrastructure-related incidents, and data-related incidents. Model incidents had MTBI of 8.3 days. Infrastructure incidents had MTBI of 21.7 days. Data incidents had MTBI of 4.9 days. The data showed that data pipelines were the least stable component. The team prioritized data infrastructure improvements, implementing better monitoring, validation, and error handling. Data-related MTBI increased to 12.1 days over the next quarter.

MTBI can be misleading if incident severity is not considered. A system could have high MTBI but all incidents could be critical. A healthcare documentation platform tracked weighted MTBI, where critical incidents counted triple, high incidents counted double, medium incidents counted normal, and low incidents counted half. The weighted MTBI gave a better picture of overall stability because it accounted for incident impact, not just frequency.

## Incident Frequency by Failure Mode: The Pattern Metric

Tracking how often each type of incident occurs reveals which failure modes are most common and which parts of your system are most fragile.

A financial advisory platform tracked incident counts by failure mode in 2025: silent degradation (14 incidents), hallucination (9 incidents), latency cascade (7 incidents), data staleness (6 incidents), model version mismatch (4 incidents), and prompt injection (2 incidents). The distribution showed that silent degradation was the most common failure. The team focused their reliability efforts on better quality monitoring and continuous eval. The next year, silent degradation incidents dropped to 6, while overall incident count dropped from 42 to 28.

Incident frequency should be tracked over time to detect trends. A legal document assistant tracked incidents by month. They noticed that data-related incidents spiked every quarter-end. Investigation revealed that quarter-end data processing loads stressed the data pipelines, causing failures. The team improved pipeline capacity and implemented better load distribution. The quarterly spike disappeared.

Incident frequency can also be correlated with system changes. A travel booking assistant tracked how many incidents occurred within 48 hours of a deployment. In Q1 2025, 41 percent of incidents occurred within 48 hours of a deployment. The team tightened pre-deployment testing and implemented longer canary periods. By Q4, only 18 percent of incidents occurred within 48 hours of deployment. The correlation metric showed that deployment quality had improved.

## Detection Source Distribution: The Monitoring Effectiveness Metric

What percentage of incidents are detected by automated monitoring versus user reports versus proactive review? The distribution reveals monitoring effectiveness.

A contract analysis platform tracked detection sources for all incidents in 2025: 62 percent detected by automated monitoring, 29 percent reported by users, 9 percent found during proactive review. The team set a goal: get to 80 percent automated detection. They expanded monitoring coverage, implemented user feedback monitoring, and added more eval-based alerts. By end of year, the distribution was 78 percent automated, 17 percent user-reported, 5 percent review-detected. The shift meant incidents were caught earlier with less user impact.

Detection source should be correlated with MTTD. A document summarization platform found that automated detection had MTTD of 9 minutes, user reports had MTTD of 47 minutes, and proactive review had MTTD of 6.3 days. The correlation confirmed that automated detection was not just more common but also faster. The insight justified continued investment in automated monitoring even though it was more expensive than relying on user reports.

Detection source can also be correlated with incident severity. A healthcare documentation platform found that critical incidents were 89 percent automated detection, high incidents were 68 percent automated, medium incidents were 44 percent automated. The team realized that their monitoring was good at catching severe issues but missed milder degradations. They added monitoring for medium-severity failure modes.

## False Positive Rate: The Alert Quality Metric

How often do alerts fire when no real incident is occurring? High false positive rates lead to alert fatigue and desensitization.

A customer service chatbot tracked alert false positive rate: alerts fired 127 times in Q1 2025, actual incidents were declared 34 times, false positive rate was 73 percent. The high rate meant engineers were ignoring alerts. The team tuned thresholds, improved aggregation logic, and removed noisy metrics. In Q2, alerts fired 52 times, actual incidents were 29, false positive rate dropped to 44 percent. Still high, but improving. By Q4, false positive rate was 18 percent. Engineers started trusting alerts again.

False positive rate should be tracked per alert type. A financial advisory platform found that latency alerts had a 9 percent false positive rate, while eval degradation alerts had a 51 percent false positive rate. The difference was because latency was a stable metric while eval had natural variance. The team implemented better statistical methods for eval-based alerting, reducing false positives to 22 percent.

False positive rate must be balanced against false negative rate. If you tune thresholds too conservatively to reduce false positives, you increase false negatives — real incidents that do not trigger alerts. A legal document assistant tracked both rates. When they reduced false positives from 38 percent to 12 percent, false negatives increased from 6 percent to 19 percent. The tradeoff was unacceptable. They accepted a 20 percent false positive rate to keep false negatives below 10 percent.

## Root Cause Distribution: The Failure Source Metric

What causes incidents most frequently? Model issues? Data issues? Infrastructure issues? The distribution guides where to invest in reliability.

A travel booking assistant tracked root causes for all incidents in 2025: model behavior issues (37 percent), data pipeline failures (28 percent), infrastructure problems (18 percent), prompt engineering mistakes (12 percent), dependency failures (5 percent). The distribution showed that model and data were the primary reliability risks. The team invested in better model testing and data pipeline monitoring. The next year, the distribution shifted: model issues dropped to 24 percent, data issues dropped to 19 percent, overall incident count dropped by 33 percent.

Root cause distribution should inform team structure and expertise. A healthcare documentation platform found that 42 percent of incidents were caused by domain-specific model failures — the model misunderstanding medical concepts. The insight led to hiring domain experts into the engineering team and creating a specialized medical AI team. Incidents caused by domain misunderstanding dropped to 18 percent within six months.

Root cause distribution can also reveal technical debt. A contract analysis platform found that 31 percent of incidents were caused by configuration errors. The high percentage indicated that configuration management was brittle. The team implemented configuration-as-code, mandatory review, and automated validation. Configuration-related incidents dropped to 7 percent.

## User Impact Percentage: The Blast Radius Metric

What percentage of users or requests were affected by each incident? The metric reveals how well you contain failures.

A document summarization platform tracked user impact for all incidents in 2025. The median incident affected 8 percent of users. The 90th percentile incident affected 34 percent of users. The worst incident affected 89 percent of users. The distribution showed that most incidents were localized, but some were widespread. The team analyzed the high-impact incidents and found a pattern: they occurred when failures hit shared infrastructure or base models. The team invested in better fault isolation and canary deployments to reduce blast radius. The next year, 90th percentile impact dropped to 19 percent.

User impact should be correlated with detection speed. A customer service chatbot found that incidents detected within 10 minutes affected a median of 4 percent of users, while incidents detected after 30 minutes affected a median of 19 percent. The correlation was clear: faster detection meant smaller impact. The insight justified aggressive investment in detection speed even though it increased costs.

User impact should also be tracked in absolute numbers, not just percentages. A financial advisory platform tracked both. An incident that affected 5 percent of users sounded small, but it was 2,300 users. The absolute number made the impact tangible and influenced severity classification and response urgency.

## Recurrence Rate: The Persistent Failure Metric

How often do incidents recur? High recurrence rate indicates that fixes are superficial or that root causes are not being addressed.

A legal document assistant tracked incident recurrence in 2025. They defined recurrence as: an incident with the same root cause occurring within 90 days of a previous incident. Recurrence rate was 23 percent in Q1. The team implemented a policy: every incident post-mortem must identify preventive measures, not just corrective measures. Recurrence rate dropped to 11 percent by Q4.

Recurrence should be tracked at multiple levels. A travel booking assistant tracked: exact recurrence — same root cause, same symptom, similar recurrence — same root cause, different symptom, and class recurrence — different root cause, same failure mode. Exact recurrence was 8 percent. Similar recurrence was 19 percent. Class recurrence was 34 percent. The breakdown showed that they were preventing exact repeats but not addressing broader vulnerability patterns.

Recurrence rate should trigger escalation. A healthcare documentation platform had a policy: if an incident recurs within 30 days, the second occurrence is automatically escalated to director-level review. The escalation ensures that recurring issues receive senior attention and adequate resources for permanent fixes.

## Incident Cost: The Business Impact Metric

How much does each incident cost in engineering time, lost revenue, customer impact, and reputation damage? The metric justifies reliability investments.

A contract analysis platform tracked incident costs in 2025. The average incident cost was calculated as: engineering hours spent times hourly rate, plus customer impact estimated as affected user-hours times value per user-hour, plus any direct revenue loss from service unavailability. The average incident cost was 14,200 dollars. The median was 6,800 dollars. The worst incident cost 127,000 dollars. The annual total was 612,000 dollars. The cost data justified a 200,000 dollar investment in reliability tooling and monitoring. If the investment reduced incident frequency by 30 percent, it would pay for itself in one year.

Incident cost should include hidden costs. A financial advisory platform found that customer churn after incidents was 2.3 times baseline churn rate. An incident that affected 500 users led to 12 cancellations over the next 60 days. Each cancellation represented 8,000 dollars in annual revenue. The hidden cost was 96,000 dollars — eight times the direct engineering cost of the incident. The full cost accounting changed priorities. Incidents that seemed minor from an engineering perspective were significant from a business perspective.

Incident cost can be compared across incident types to prioritize prevention efforts. A customer service chatbot found that hallucination incidents cost 3.2 times more than latency incidents, even though latency incidents were more frequent. Hallucinations caused trust damage and customer churn. Latency spikes were annoying but did not drive cancellations. The team prioritized hallucination prevention over latency optimization.

## Improvement Trends: The Progress Metric

Are incident metrics improving over time? Tracking trends shows whether reliability investments are working.

A document summarization platform tracked quarterly trends for key metrics: MTTD dropped from 41 minutes in Q1 to 19 minutes in Q4, MTTR dropped from 86 minutes to 52 minutes, incident frequency dropped from 18 incidents per quarter to 11 incidents per quarter, user-reported detection dropped from 34 percent to 16 percent. The trends showed consistent improvement. The dashboard visualized trends to make progress visible to the entire team.

Trends should be reviewed regularly. A legal document assistant reviewed incident metrics in monthly engineering all-hands meetings. The visibility created accountability and celebrated progress. When MTTD dropped below 20 minutes for the first time, the team celebrated. When incident frequency increased in one month due to a bad deployment, the team discussed what went wrong and how to prevent it.

Trends can reveal the impact of specific changes. A healthcare documentation platform implemented continuous eval in April 2025. They tracked MTTD before and after. Before continuous eval, MTTD averaged 38 minutes. After continuous eval, MTTD averaged 14 minutes. The specific improvement attributed to a specific change justified the ongoing cost of continuous eval.

Once you have metrics showing how well you respond to incidents manually, the next question is how much of incident response can be automated. Can runbooks execute themselves? Can mitigation happen without human decisions? Incident automation reduces MTTR and removes human error from the critical path.

