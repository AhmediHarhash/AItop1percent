# 6.3 — Tool Call Success Rates and Latency Attribution

Your agent has a 98 percent success rate. Its tools have a 76 percent success rate. The agent is failing a quarter of its attempted actions, but your dashboards show green because you are measuring model calls instead of tool calls.

An agent is only as reliable as the tools it depends on. When a tool fails, the agent must handle the failure — retry, switch strategies, or give up. Tool failures compound across multi-step trajectories. An agent that makes eight tool calls with a ninety percent per-call success rate has only a 43 percent chance of completing the entire sequence without a failure. Tool reliability is not an infrastructure concern. It is an agent success concern.

## The Hidden Failure Layer

A fintech agent in late 2025 helped customers check account balances, transfer funds, and review transaction history. The agent called a balance API, a transfer API, and a transaction query API. Engineering monitored the agent's model invocations and saw 99.1 percent success. Customer complaints told a different story — fifteen percent of requests were failing or producing incomplete answers.

Investigation revealed the problem. The balance API succeeded 98 percent of the time. The transfer API succeeded 89 percent of the time. The transaction query API succeeded 81 percent of the time. When the agent tried to perform a complex task that required all three APIs, the probability of completing without an API failure was 0.98 times 0.89 times 0.81, which equals 71 percent. Nearly thirty percent of complex requests hit at least one API failure, even though the agent itself never crashed.

The team had been monitoring agent uptime, not tool uptime. The agent could invoke tools successfully one hundred percent of the time and still fail thirty percent of requests because the tools themselves returned errors. Tool call success rates became a primary dashboard metric, tracked per tool and per request type.

## Per-Tool Success Rate Tracking

You log every tool invocation with a success or failure status. Success means the tool returned usable data. Failure means the tool returned an error, timed out, or returned malformed data the agent could not parse. You track these rates per tool, per day, and per request type.

**Database query tools** should succeed above 97 percent. If a query tool drops below 95 percent, either the database is unstable or the agent is issuing malformed queries. Both require immediate attention.

**Search tools** should succeed above ninety percent. Search failures often result from index problems, query timeouts, or network issues. A search tool at 85 percent success is degrading agent performance across every request that needs retrieval.

**External API tools** vary widely. A weather API might succeed 99.5 percent of the time. A third-party credit check API might succeed 92 percent of the time due to rate limits and occasional downtime. You set per-tool thresholds based on historical performance and contractual SLAs.

A customer service agent in early 2026 used nine different tools: CRM lookup, order database query, product catalog search, inventory check, shipping tracker, refund API, email sender, ticket creator, and knowledge base search. The team tracked success rates for all nine. CRM lookup stayed above 99 percent. Shipping tracker fluctuated between 94 and 97 percent. Inventory check dropped to 88 percent one week when a vendor's API had reliability problems.

The inventory check degradation caused a secondary effect: when the inventory tool failed, the agent could not answer stock availability questions and escalated to humans. Escalation rate jumped from eight percent to nineteen percent. The root cause was a third-party API, but the symptom was increased human workload. Monitoring tool success rates surfaced the problem two days before the vendor notified the company of the API issues.

## Failure Attribution in Multi-Tool Trajectories

When a request fails, you need to know which tool failure caused it. An agent that calls five tools and fails might have encountered a failure on tool one, tool three, or tool five. The failure attribution tells you where to focus remediation.

You tag each failed request with the step number and tool name where the failure occurred. First-tool failures indicate problems with request validation or initial data retrieval. Mid-trajectory failures indicate problems with follow-up queries or conditional logic. Final-step failures indicate problems with output formatting or submission APIs.

A hiring agent in mid-2025 screened resumes using a sequence of tools: resume parser, skills database query, experience validator, culture fit assessment, and candidate scoring. When a screening failed, the team needed to know which step broke. Failure attribution showed that resume parser failures accounted for forty percent of all screening failures, skills database timeouts accounted for thirty percent, and the other three tools together accounted for thirty percent. The team prioritized improving the resume parser and adding retries to the skills database, which together would address seventy percent of failures.

## Retry Logic and Success-After-Retry Rates

When a tool fails, most agents retry before giving up. Retry logic hides tool unreliability from immediate dashboards but not from user experience. A tool that succeeds on the second retry is still slower than a tool that succeeds on the first attempt. You track both first-attempt success rates and success-after-retry rates.

**First-attempt success** is the percentage of tool calls that succeed on the first invocation. This is the ideal case — no wasted latency, no wasted cost.

**Success-after-retry** is the percentage of tool calls that succeed after one or more retries. The difference between first-attempt and success-after-retry is the retry tax — the proportion of requests that burn extra time and cost due to initial failures.

A logistics agent in early 2026 queried a package tracking API with a first-attempt success rate of 91 percent. With two retries, success-after-retry climbed to 98 percent. The seven-point gap meant that seven percent of requests required retries, each adding 400 to 800 milliseconds of latency. For a system handling ten thousand requests per day, that was seven hundred requests experiencing retry delays. The team worked with the API provider to diagnose the transient failures and improved first-attempt success to 96 percent, cutting the retry tax in half.

## Latency Attribution Per Tool

An agent that takes 3.2 seconds to respond might be spending 0.8 seconds on model calls, 1.9 seconds on tool calls, and 0.5 seconds on internal processing. If you only monitor total latency, you cannot optimize the bottleneck. Latency attribution breaks total time into per-tool contributions.

You instrument each tool invocation with start and end timestamps. After each request completes, you calculate how much time each tool consumed. The latency breakdown shows which tools are slow, which tools are fast, and which tools dominate overall response time.

A research agent in late 2025 used four tools: web search, academic database query, content extraction, and summarization. Median latency was 4.1 seconds. Latency attribution showed: web search averaged 1.8 seconds, academic database query averaged 1.4 seconds, content extraction averaged 0.6 seconds, and summarization averaged 0.3 seconds. The agent was spending nearly half its time on web search and a third on database queries. Model calls and other logic accounted for less than ten percent of total latency.

The team optimized by caching frequent search results and running web search and database query in parallel when possible. Median latency dropped to 2.9 seconds without changing agent logic or degrading quality. The latency attribution identified where time was being spent. The engineering work removed the bottlenecks.

## Per-Tool Latency Percentiles

Average latency hides tail behavior. A tool that averages 200 milliseconds but has a 95th percentile of 2.1 seconds is causing severe delays for five percent of requests. You track latency percentiles per tool — p50, p90, p95, p99 — and alert when any tool's tail latencies degrade.

A payment processing agent in early 2026 called a fraud detection API for every transaction. The API had a median latency of 180 milliseconds and a p95 of 310 milliseconds — acceptable performance. One week, p95 latency jumped to 1.9 seconds while median stayed at 190 milliseconds. The fraud detection provider had introduced a new machine learning model that occasionally took much longer to evaluate high-risk transactions. Five percent of requests were experiencing ten times normal latency.

The team implemented a timeout: if the fraud detection API did not respond within 800 milliseconds, the agent would proceed with a default risk score and flag the transaction for asynchronous review. This prevented tail latencies in one tool from degrading the entire agent's p95 latency. The fraud detection provider eventually optimized their model, but the timeout protection remained as a safeguard against future regressions.

## Tool Call Volume and Rate Limits

Some tools have rate limits. When an agent exceeds the rate limit, tool calls fail with rate limit errors, not data errors. You track tool call volume per minute and per tool to detect when your agent is approaching or hitting rate limits.

A social media monitoring agent in mid-2025 used the Twitter API and LinkedIn API to gather brand mentions. Both APIs had rate limits: Twitter allowed 180 requests per fifteen-minute window, LinkedIn allowed 500 requests per day. The agent operated within limits during testing but hit Twitter rate limits in production when request volume surged. Twenty percent of Twitter tool calls started failing with rate limit errors during peak hours.

The team implemented request throttling: the agent queued Twitter API calls and spread them across the fifteen-minute window. If the queue grew beyond a certain depth, the agent would delay responses or reduce the number of searches per request. Rate limit errors dropped to under one percent. The agent became slower during peak hours but remained reliable.

## Cascading Tool Failures

When one tool fails, it often causes subsequent tool calls to fail or produce poor results. An agent that fails to retrieve a user's account ID in step one will fail every query in steps two through five that depend on that account ID. Cascading failures inflate overall failure rates and obscure the root cause.

You track dependency chains: which tools depend on the outputs of prior tools. When a failure occurs, you identify whether it is a root failure or a cascaded failure. Root failures indicate tool problems. Cascaded failures indicate that the agent did not handle an earlier failure gracefully.

A travel booking agent in early 2026 followed a sequence: search flights, check seat availability, verify pricing, reserve seat, confirm booking. If flight search failed, the agent would proceed to check seat availability using stale data from a previous session, fail, attempt to verify pricing with no valid flight ID, fail again, and eventually error out at the reservation step. One root failure caused four cascaded failures.

The team added validation: if any step failed, the agent would check whether downstream steps depended on that step's output. If so, the agent would skip the dependent steps and immediately escalate or return an error rather than executing steps guaranteed to fail. Cascading failure rates dropped from twelve percent to three percent. The agent still encountered the same number of root failures, but it stopped wasting time and tokens on doomed follow-up actions.

## Tool Health Dashboards

You build a real-time dashboard showing tool health across your agent system. Each tool displays its success rate, median latency, p95 latency, call volume, and recent error types. When a tool degrades, the dashboard highlights it immediately.

Tool health dashboards differ from infrastructure dashboards. Infrastructure dashboards show whether a service is up. Tool health dashboards show whether an agent can successfully use that service. A database might be up with 99.9 percent uptime while the agent's query tool has an 87 percent success rate because the agent is issuing queries that violate the schema or time out under load.

A healthcare agent system in early 2026 operated a tool health dashboard with sixteen tools. Engineering and operations reviewed it every morning. When the eligibility check tool dropped from 97 percent to 91 percent success, the team investigated within hours and discovered that a recent schema change in the upstream eligibility database had broken the query logic. The fix took thirty minutes. Without the dashboard, the degradation would have persisted for days until customer complaints surfaced the pattern.

## Defining Tool-Specific SLAs

Not all tools need the same reliability. A tool that retrieves decorative information can tolerate lower success rates. A tool that processes financial transactions needs near-perfect reliability. You define per-tool SLAs based on the tool's role in the agent's workflow.

**Critical tools** must succeed above 99 percent. Failures block the agent from completing its core task. Examples: authentication, transaction execution, primary data retrieval.

**Important tools** must succeed above 95 percent. Failures degrade the agent's output quality but do not prevent completion. Examples: supplementary data sources, formatting tools, optional enrichment APIs.

**Optional tools** can tolerate success rates above ninety percent. Failures are inconvenient but not damaging. Examples: logging, analytics, decorative metadata retrieval.

You set alerting thresholds at these SLA boundaries. When a critical tool drops below 99 percent, you page on-call engineers. When an important tool drops below 95 percent, you send a warning. When an optional tool drops below ninety percent, you log it for later review.

Tool reliability determines agent reliability. An agent with perfect internal logic and flawless reasoning will still fail if its tools are unreliable. Monitoring tool success rates and latency contributions is not optional. It is the foundation of agent observability.

The next subchapter covers loop detection — identifying when an agent gets stuck repeating the same actions and burning tokens without making progress.

