# 3.2 — Proxy Metrics: Using Behavior Signals to Infer Quality

In August 2025, a healthcare platform deployed a new symptom checker that passed all pre-deployment evals. Within three weeks, patient engagement dropped eighteen percent. User session length decreased from 4.3 minutes to 2.1 minutes. Completion rate on symptom intake forms fell from seventy-nine percent to fifty-four percent. The model was technically correct — it generated medically accurate responses with proper citations. But it had become terse, clinical, and hard to understand. Direct quality metrics looked fine. Proxy behavioral metrics caught the problem immediately.

Proxy metrics measure user behavior that correlates with quality but does not directly measure the output itself. They answer the question: are users acting like the model is giving them good responses? When users abandon sessions, retry prompts, ignore suggestions, or escalate to human support, they are signaling that quality is insufficient. The advantage of proxy metrics is speed and scale. You measure every user interaction automatically, in real time, with no human review required. The disadvantage is interpretation. User behavior is noisy, context-dependent, and influenced by factors beyond model quality. The teams that use proxy metrics effectively combine multiple signals, segment by context, and validate against direct quality measures.

## Engagement Signals: How Users Interact With Responses

Engagement signals measure whether users are consuming, using, and acting on model outputs. Low engagement indicates the output is not valuable, not trustworthy, or not usable.

**Session continuation rate** measures what percentage of users continue interacting after receiving a response. A user asks a question, gets an answer, and then asks a follow-up question. That is positive engagement. A user asks a question, gets an answer, and immediately leaves. That is negative engagement. Track continuation rate over time. A drop from seventy-six percent to sixty-two percent means something changed in how users perceive the responses.

Segment continuation rate by prompt type, user type, and response characteristics. Power users have different engagement patterns than new users. Complex queries have different continuation patterns than simple queries. A drop in continuation for new users but not power users indicates an onboarding quality problem. A drop in continuation for long responses but not short responses indicates verbosity is driving users away.

**Response acceptance rate** measures whether users explicitly accept, use, or act on the model's output. In a code completion tool, acceptance means the user keeps the suggested code. In a writing assistant, acceptance means the user incorporates the suggestion. In a customer support copilot, acceptance means the agent sends the drafted response. Acceptance rate is one of the strongest proxies for quality because it directly measures whether the user trusted the output enough to use it.

A legal contract review tool suggests clause revisions. Acceptance rate holds steady at eighty-three percent for four months. Over two weeks, it drops to sixty-nine percent. The model is still generating suggestions with correct legal language and proper formatting. But lawyers are rejecting more suggestions. Investigation reveals the model started suggesting overly conservative revisions that clients would never accept. The model was technically correct but contextually inappropriate. Acceptance rate caught it before any formal quality eval.

**Time spent reading responses** measures whether users are actually consuming the content. If a model generates a 300 word answer and the user scrolls past it in three seconds, they did not read it. If they spend forty seconds on it, they probably read most of it. Track median read time, read time distribution, and the percentage of responses where estimated read time is implausibly short.

Compare actual read time against expected read time based on content length and complexity. A two hundred word response should take at least twenty-five to thirty-five seconds to read. If median observed read time is eight seconds, users are not reading. They are scanning, skipping, or ignoring. That correlates with low perceived quality even if the content is objectively correct.

## Retry and Reformulation Signals: When Users Try Again

Retry signals measure whether users attempt to get a better response by asking again, rephrasing, or escalating. High retry rates indicate the first response was insufficient.

**Immediate retry rate** measures what percentage of users submit another prompt within thirty seconds of receiving a response. A user asks for a Python code example, gets one, and immediately asks for the same thing phrased differently. That is a retry. It signals the first response missed the mark. Track retry rate overall and segmented by prompt similarity. A user asking a completely different follow-up question is not a retry. A user asking the same question with minor rephrasing is a retry.

A travel planning assistant has a retry rate of nine percent for three months. It spikes to twenty-three percent over five days. Users are not getting satisfactory responses on the first attempt. Investigation shows the model started assuming all travel is domestic unless explicitly stated otherwise. International travel queries required multiple retries before users figured out they needed to specify country names explicitly. Retry rate detected the instruction-following regression.

**Escalation to human support rate** measures what percentage of interactions end with the user requesting human help. In customer support copilots, measure how often agents override the suggested response and write their own. In consumer chatbots, measure how often users click "talk to a human" after interacting with the bot. Escalation rate is a strong negative signal. It means the AI was not sufficient.

Track escalation rate by issue type and user segment. Some issues inherently require human judgment and should have high escalation rates. Other issues should be fully automatable. A billing question chatbot should have an escalation rate below fifteen percent. A complex policy interpretation chatbot might have an escalation rate of sixty percent. What matters is the baseline and the delta. A five percentage point increase in escalation rate over two weeks is a quality problem regardless of the absolute level.

**Edit and revision rate** measures how much users modify model outputs before using them. In writing assistants, measure what percentage of suggested text is kept verbatim versus edited. In code generators, measure how much generated code is modified before running. In drafting tools, measure how many characters are changed. High edit rates indicate the model is giving outputs that are close but not quite right.

A marketing copy generator has an average edit rate of thirty-two percent — users keep about two-thirds of suggested text and modify one-third. Edit rate climbs to fifty-four percent over three weeks. Users are now modifying more than half of every suggestion. The model is still generating plausible copy, but it no longer matches brand voice, tone, and style as well as it used to. Edit rate detected a fine-tuning degradation that formal quality evals missed because they measured factual correctness, not stylistic fit.

## Abandonment Signals: When Users Give Up

Abandonment signals measure when users stop interacting because the quality is too low to justify continued effort. Abandonment is the ultimate negative signal. The user concluded the system is not worth their time.

**Mid-session abandonment rate** measures what percentage of users start a task but leave before completing it. A user begins a multi-turn conversation, gets three responses, and then closes the window without finishing. That is abandonment. Track abandonment rate by session depth. Abandonment in the first turn is different from abandonment in the fifth turn. Abandonment after receiving a response is different from abandonment while waiting for a response.

A tax preparation assistant has a mid-session abandonment rate of fourteen percent — one in seven users who start the process do not finish. Abandonment rate climbs to twenty-eight percent over ten days. Users are leaving after two or three interactions. Investigation shows the model started asking redundant questions, asking for information it already had, and giving generic advice instead of personalized guidance. The experience became tedious and untrustworthy. Abandonment rate was the first signal.

**Task completion rate** measures whether users accomplish what they set out to do. If a system is designed to help users complete a form, complete a booking, or generate a report, measure what percentage of sessions end with task completion. Low completion rates indicate something in the interaction is broken. It might be quality, usability, or trust — but it is something.

Track completion rate separately for different task types and user experience levels. First-time users have lower completion rates than returning users. Complex tasks have lower completion rates than simple tasks. What matters is relative change. A drop from sixty-eight percent to fifty-one percent completion over two weeks is a red flag.

**Return visit rate** measures whether users come back. If a system provides value, users return. If quality degrades, users stop returning. Track daily active users, weekly active users, and retention curves. A product that retains seventy-three percent of users from week one to week two suddenly drops to fifty-eight percent retention. Something changed in the user experience, and quality is the first suspect.

Segment return rate by user cohort and acquisition channel. A drop in retention for new users but not existing users indicates an onboarding quality problem. A drop in retention for users acquired through a specific marketing campaign indicates that campaign set incorrect expectations, leading to disappointment with actual quality.

## Comparative User Feedback: Explicit Signals

Some proxy metrics come from explicit user feedback rather than implicit behavior. These are noisier but often more specific in what they reveal.

**Thumbs up and thumbs down rates** measure explicit approval and disapproval. Track the percentage of responses that receive positive feedback, negative feedback, and no feedback. Track the ratio of positive to negative. A system with a seventy percent positive rate and eight percent negative rate has a 8.75 to 1 positive-to-negative ratio. That ratio drops to 4.2 to 1 over a week. Negative feedback is increasing faster than usage is growing. Quality is degrading.

Correlate feedback with response characteristics. What kinds of responses get negative feedback? Long responses? Short responses? Responses with citations? Responses without citations? Identify patterns. A legal research tool starts getting negative feedback specifically on responses that cite older case law. Users expect recent precedents. The model is retrieving from a stale index. Feedback patterns reveal the specific failure mode.

**User-submitted feedback comments** are sparse but high-signal. When users voluntarily write feedback, they are usually reporting something important. Track feedback volume, sentiment, and topic clusters. An increase in feedback volume often precedes measurable quality degradation. Users notice problems and report them before the problems become statistically obvious in behavioral metrics.

Use topic modeling or simple keyword extraction to cluster feedback. A medical assistant starts receiving feedback comments with keywords like "confusing," "hard to understand," and "too technical." The model shifted toward more clinical language. A code assistant starts receiving feedback with keywords like "verbose," "over-commented," and "too much boilerplate." The model started generating overly cautious, defensive code. Feedback topic clusters tell you what kind of quality problem you have.

**Comparison preference rates** measure whether users prefer the current model over a previous version or alternative. Run side-by-side comparisons on a sample of traffic. Show users two responses and ask which is better. Track preference rate over time. A new model version initially has a sixty-two percent preference rate over the old version. Preference rate drops to forty-nine percent over a month. The new model is degrading, and users can tell.

Use preference comparisons to validate other proxy metrics. If behavioral metrics say quality is declining, but users still prefer the current model in side-by-side tests, the behavioral metrics might be responding to something other than quality. If behavioral metrics look stable, but preference rate is declining, you have a problem that behavioral metrics are not sensitive enough to detect.

## Combining Proxy Metrics Into a Quality Score

No single proxy metric is sufficient. Users abandon sessions for many reasons. They give negative feedback when they are frustrated with the product, not just the model. Proxy metrics work when you combine them into a composite score that filters noise and amplifies signal.

**Define a weighted composite quality proxy** from multiple behavioral signals. Weight each signal by how strongly it correlates with direct quality measures on your system. Weight each signal by how quickly it responds to quality changes. Faster signals get higher weight for real-time alerting. Slower, more reliable signals get higher weight for trend analysis.

A financial advice chatbot combines five proxy metrics. Session continuation rate gets twenty percent weight. Escalation to human advisor rate gets thirty percent weight — it is the strongest signal. Response acceptance rate gets twenty-five percent weight. Retry rate gets fifteen percent weight. Negative feedback rate gets ten percent weight. The composite score catches eighty-seven percent of quality degradations within twenty-four hours, compared to forty-three percent detection rate using any single metric alone.

**Segment composite scores by user context**. Different users tolerate different quality levels. Different tasks require different quality standards. A composite score for a compliance task should weight precision and accuracy heavily. A composite score for a creative task should weight user satisfaction and engagement heavily. Build separate composite scores for separate use cases.

Track composite scores at multiple timescales. Hourly scores catch acute problems. Daily scores filter transient noise. Weekly scores reveal trends. Monthly scores measure long-term drift. Alert on hourly scores, investigate daily scores, report weekly scores, and set strategy based on monthly scores.

## Validating Proxy Metrics Against Ground Truth

Proxy metrics are only useful if they actually correlate with quality. Periodically validate proxy metrics against direct quality measures to ensure the correlation holds.

**Run continuous sampled human review** on a subset of production traffic. Measure true quality on fifty to two hundred responses per day. Correlate human quality ratings against proxy metrics for those same responses. Compute correlation coefficients. A proxy metric with a correlation above 0.6 is useful. A correlation below 0.3 is noise. Drop low-correlation metrics from your composite score.

Track correlation over time. A proxy metric that used to correlate well might stop correlating as user behavior or product usage patterns change. A writing assistant initially found that response length negatively correlated with quality — shorter suggestions were better. Six months later, after a product redesign encouraged longer-form writing, the correlation reversed. Shorter suggestions were now worse. Correlations are not static. Measure them continuously.

**Run A and B tests with intentional quality degradation**. Deploy a deliberately worse model variant to a small percentage of traffic. Measure how quickly proxy metrics detect the degradation. Measure whether proxy metrics correctly identify the degraded variant as worse. Proxy metrics that fail to detect known quality problems are not useful. Proxy metrics that flag quality problems that do not exist create alert fatigue.

Test proxy metrics against different kinds of quality degradation. Factual incorrectness. Tone problems. Format errors. Verbosity. Brevity. Hallucinations. Different proxy metrics are sensitive to different failure modes. Build a portfolio of proxy metrics that together cover all the failure modes you care about.

Proxy metrics bridge the gap between rare, expensive human quality evaluation and the need for real-time quality monitoring at scale. When validated and combined correctly, they give you continuous quality visibility with no human in the loop. When used naively, they create false confidence and missed regressions. The difference is rigor in metric selection, validation, and interpretation.

Next, we explore inline evaluation — running lightweight AI judges on every request to measure quality without human review.

