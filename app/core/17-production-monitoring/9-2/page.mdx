# 9.2 — Error Budgets for Model Degradation

Error budgets work because they turn reliability into a resource you can spend. You have 99.9 percent uptime SLO. That gives you 43 minutes of downtime per month. You can spend that budget on risky deploys, infrastructure migrations, or planned maintenance. When the budget is exhausted, you stop taking risks and focus on stability. The math is clean, the incentives are clear, and everyone understands the tradeoff.

Model degradation breaks this clarity. Your quality SLO specifies that 98 percent of responses must score above 0.7 on your automated eval. That gives you a two percent error budget. But model degradation is not like downtime. Downtime is binary—the service works or it does not. Degradation is gradual. Quality drops from 98.5 percent to 98.2 percent to 97.8 percent over three days. At what point do you stop deployment? At what point do you roll back? The error budget is being consumed, but slowly, and the cause might be shifting user behavior rather than model failure.

The challenge is that traditional error budget policies assume errors are acute events. An outage consumes budget, then ends. Model degradation consumes budget continuously until you intervene. You need different rules for how to measure degradation, when to escalate, and when to declare the budget exhausted.

## Tracking Degradation as Continuous Budget Consumption

Traditional error budgets measure discrete failures. A request succeeds or fails. You count failures, divide by total requests, and calculate percentage. If your SLO is 99 percent success rate, you have one percent error budget. Each failed request consumes a portion of that budget proportional to the total traffic.

Model degradation requires tracking partial failures. A response that scores 0.65 when your threshold is 0.7 is not a complete failure. It is a partial failure. A response that scores 0.4 is a larger failure than a response that scores 0.65. You need an error budget accounting system that captures the magnitude of degradation, not just the count of failures.

The most common approach is threshold-based budget consumption. Any response below quality threshold consumes budget. A response at 0.69 consumes the same budget as a response at 0.4. This is simple to implement and matches how you calculate SLO compliance. If quality SLO is 98 percent above 0.7 and you measure 97 percent above 0.7, you consumed two percent of error budget when you were allowed one percent. You are over budget.

The problem with threshold-based consumption is that it treats all failures equally. A response at 0.69 is nearly correct. A response at 0.4 is garbage. If your error budget is consumed entirely by 0.69 responses, you might have a calibration problem but not a crisis. If your error budget is consumed by 0.4 responses, you have a crisis. You need to distinguish between near-threshold failures and catastrophic failures.

A more sophisticated approach is magnitude-weighted budget consumption. Each response below threshold consumes error budget proportional to how far below threshold it is. If threshold is 0.7, a response at 0.6 consumes 0.1 units of budget. A response at 0.3 consumes 0.4 units of budget. Your error budget is now measured in "quality points below threshold" rather than "count of failures." This captures the severity of degradation while remaining automatically calculable.

The calculation works like this. Your SLO specifies 98 percent of responses above 0.7. Assume 100,000 requests per day. Your error budget allows 2,000 responses below threshold per day, or equivalently, a maximum cumulative deficit of 2,000 quality points if every failing response is at 0.6. If you have 1,000 responses at 0.4, you consume 300 quality points per response times 1,000 responses equals 300,000 quality points, which vastly exceeds your budget even though only one percent of requests failed. This approach catches catastrophic failures faster than count-based budgets.

## Baseline Drift and Budget Accounting

The hardest problem in model degradation budgets is separating model failure from baseline drift. Your model was trained on data from January through March 2025. It is now June 2026. User queries have changed. Language has evolved. The distribution has shifted. Quality might drop from 98.5 percent to 97.8 percent not because the model broke, but because the world changed.

Traditional error budgets do not care why requests fail. A failure is a failure. Model degradation budgets need to care because the intervention is different. If quality degradation is due to distribution shift, you need to retrain or fine-tune on recent data. If quality degradation is due to a bad deployment, you need to roll back. You cannot decide which intervention to apply without understanding the cause.

The most common approach is baseline tracking. Measure quality on a static eval set every hour. This set does not change—it represents the distribution you designed the model for. If quality on the static set remains stable at 98.5 percent while production quality drops to 97.8 percent, the gap is distribution shift. If quality on the static set drops to 97.8 percent along with production quality, the model itself degraded.

When distribution shift consumes error budget, you do not roll back. You acknowledge that the budget consumption is expected and schedule a retraining cycle. You might create a separate budget category for shift-related degradation with different escalation policies. Shift-related degradation triggers a P3 ticket for the model engineering team to refresh training data. Model-related degradation triggers a P1 incident to roll back immediately.

The challenge is that distribution shift and model degradation can happen simultaneously. Your model might degrade due to a bad deployment while simultaneously encountering shifted distribution. The static eval set catches model degradation, but you also need to sample recent production traffic and run it through the previous model version. If the previous version performs better on recent traffic, the current deployment broke something. If the previous version performs the same on recent traffic, the issue is distribution shift.

## Error Budget Policies for Gradual Degradation

Traditional error budget policies are simple. When you exhaust the budget, you freeze risky changes and focus on reliability. AI error budget policies need to account for the fact that gradual degradation might consume budget over weeks, and declaring a freeze based on cumulative degradation from distribution shift is counterproductive.

The most common policy is windowed budgets. You have a 24-hour budget, a seven-day budget, and a 30-day budget. Exhausting the 24-hour budget pages on-call and triggers rollback consideration. Exhausting the seven-day budget freezes new deployments until quality recovers. Exhausting the 30-day budget triggers a mandatory post-mortem and retraining cycle. This distinguishes between acute incidents and chronic degradation.

A second policy is burn rate thresholds. Your monthly error budget allows two percent of requests to fail quality threshold. Normal burn rate is 0.07 percent per day. If you burn through 0.2 percent in a single day—three times normal rate—you trigger investigation even if total budget is not exhausted. If you burn through one percent in a single day—14 times normal rate—you page immediately and consider rollback. This catches fast-moving degradation without waiting for cumulative budget exhaustion.

A third policy is segmented budgets. Enterprise customers have a separate error budget from free-tier users. A model deployment that degrades enterprise quality by 0.5 percent consumes more budget than a deployment that degrades free-tier quality by 2 percent. This aligns error budget consumption with business impact and prevents scenarios where low-impact degradation triggers the same response as high-impact degradation.

The freeze decision is the hardest call. If you exhaust error budget due to gradual distribution shift over 30 days, freezing deployments does not help. The model needs retraining, and freezing deployments delays the fix. If you exhaust error budget due to a bad deployment that degraded quality by five percent in six hours, freezing deployments is exactly right. You need decision criteria that distinguish between scenarios.

The criteria are usually velocity-based. If error budget is exhausted in under 24 hours, freeze deployments immediately. If error budget is exhausted over seven to 30 days, do not freeze, but prioritize retraining. If error budget is exhausted in one to seven days, investigate root cause before deciding on freeze. This gives you a response framework that matches degradation velocity to intervention urgency.

## Separate Budgets for Quality, Safety, and Cost

You cannot use a single error budget for all SLO dimensions. A cost overrun is not the same as a safety violation. A quality degradation is not the same as a latency spike. Each dimension needs its own budget with its own policies.

Quality error budget allows two percent of responses below threshold, measured over rolling 30 days. When exhausted, you freeze model deployments but not infrastructure deployments. Consuming 50 percent of quality budget triggers a review of recent changes. Consuming 100 percent triggers mandatory rollback consideration.

Safety error budget is effectively zero. Any safety violation consumes infinite budget. The "budget" exists for tracking purposes, but there is no concept of acceptable safety violation rate that you manage over time. A single safety violation triggers investigation. Ten violations in one hour trigger immediate rollback.

Cost error budget allows 10 percent overage on target cost per query, measured monthly. When exhausted, you do not roll back, but you do throttle expensive features or route more traffic to cheaper models. Cost budget exhaustion is a business problem, not a reliability problem. It triggers finance and product discussions, not incident response.

Latency error budget allows one percent of requests to exceed threshold, measured over rolling 24 hours. When exhausted, you investigate infrastructure issues, but you do not necessarily roll back model changes. Latency degradation is often due to load, not model quality, and the fix is scaling rather than rollback.

Availability error budget allows 0.1 percent request failure rate, measured over rolling 24 hours. When exhausted, you page on-call immediately and initiate incident response. Availability is the only dimension where traditional error budget policies apply directly.

The multi-budget system means you can be over budget on cost while under budget on quality, or over budget on quality while under budget on latency. Your response depends on which budgets are exhausted and why. The decision tree is complex, which is why you need clear policies written down before an incident, not decided during an incident.

## Automatic Rollback Triggers Based on Budget Velocity

The most dangerous situation is fast-moving quality degradation that consumes error budget faster than humans can respond. You deploy a model change at 10 AM. By 10:30 AM, quality has dropped from 98 percent to 92 percent. Your error budget is exhausted in 30 minutes. You need automatic rollback.

The rollback trigger is usually burn rate plus magnitude. If you consume more than 50 percent of 24-hour error budget in under one hour AND quality drops by more than three percent absolute, trigger automatic rollback. The system does not wait for human approval. It reverts to the previous model version, pages on-call, and logs the decision.

The rollback is model-only, not infrastructure. You do not roll back the entire deployment. You switch model serving to point at the previous model version while keeping all infrastructure changes. This minimizes blast radius while addressing the immediate quality issue. If rollback does not restore quality within 15 minutes, you escalate to full deployment rollback.

Automatic rollback requires confidence in your quality measurement. You cannot automatically roll back based on a metric that is noisy or unreliable. The metric must be stable, well-calibrated, and tested. Before enabling automatic rollback, you run shadow mode for two weeks where the system calculates when it would have triggered rollback and logs the decision without taking action. You review the logs to ensure no false positives.

A second requirement is fast validation. After automatic rollback, the system must validate that quality recovered. If quality remains degraded after rollback, the issue is not the recent deployment—it is something else. The system must detect this within five minutes and escalate to human responders rather than thrashing between versions.

A third requirement is rollback limits. The system can automatically roll back once per hour. If it triggers rollback more than twice in 24 hours, it disables automatic rollback and pages on-call for manual investigation. Repeated rollbacks indicate a deeper problem that automatic systems cannot fix.

## Communicating Budget Status Without Creating Panic

Error budgets are transparent by design. Everyone on the team can see current budget status, burn rate, and time until exhaustion. This transparency creates accountability and aligns incentives. For AI systems, transparency can also create panic.

If your quality error budget shows 80 percent consumed over the last 20 days due to gradual distribution shift, that is not a crisis. It is expected. If you display "ERROR BUDGET 80 PERCENT EXHAUSTED" on the team dashboard, people who do not understand the context will panic and demand immediate action. You need to communicate budget status with enough context that people understand whether the situation is normal or urgent.

The most common approach is color-coded status with explanation. Green means under 30 percent budget consumed at expected burn rate. Yellow means 30 to 70 percent consumed or elevated burn rate. Red means over 70 percent consumed or critically elevated burn rate. Each status includes a one-sentence explanation: "Elevated burn rate due to deployment at 2 PM, currently investigating" or "Gradual consumption due to distribution shift, retraining scheduled for next week."

A second approach is budget forecasting. The dashboard shows current budget, burn rate, and projected exhaustion date. If burn rate is normal and exhaustion date is 20 days away, that is not urgent. If burn rate is 10x normal and exhaustion date is four hours away, that is urgent. The forecast gives context that raw budget percentage does not.

A third approach is comparison to baseline. The dashboard shows current error rate and baseline error rate from the same period last month. If current error rate is 1.8 percent and baseline is 1.5 percent, you are slightly elevated but not in crisis. If current error rate is 5 percent and baseline is 1.5 percent, you are in crisis. The comparison helps people distinguish between normal variance and real problems.

The communication must reach the right people with the right urgency. Quality budget status goes to model engineers and on-call. Cost budget status goes to finance and product. Safety budget status goes to trust and safety team and legal. Availability budget status goes to SRE and infrastructure. Different audiences need different levels of detail and different escalation paths.

Your model degradation error budgets transform subjective quality into managed risk. The next subchapter defines what actually counts as an error in LLM systems—the service level indicators that feed your error budget calculations.

