# 12.8 — Human Correction Delta: Measuring the Gap Between AI and Human

An override is binary. The AI said yes, the human said no. Override rate counts how often this happens. But it does not measure how wrong the AI was. Did the human flip a borderline 51 percent confidence decision, or did they reject a 95 percent confidence recommendation? Did the human adjust a loan amount by 5 percent, or did they cut it in half? Override rate treats all disagreements equally. Human correction delta measures the magnitude of disagreement.

Correction delta matters because not all overrides are equal. If the AI recommends approving a loan for 50,000 dollars and the human approves it for 48,000 dollars, the decisions are aligned. The human made a small adjustment. This is refinement, not rejection. If the AI recommends 50,000 and the human approves 20,000, the decisions are misaligned. The human fundamentally disagrees with the AI's assessment. Override rate sees both as overrides. Correction delta sees one as minor, the other as major.

Measuring correction delta requires quantifying decisions on a continuous scale. For binary decisions — approve or deny — delta is zero or one. For continuous decisions — loan amounts, risk scores, content moderation severity — delta is the difference between AI and human outputs. For multi-class decisions — approve, review, deny — delta depends on the distance between classes. This measurement tells you whether the AI is close to correct or wildly wrong when humans intervene.

## Defining Delta for Different Decision Types

Binary decisions have limited delta. The AI says approve. The human says deny. The delta is one decision boundary crossed. You can still measure how far from the boundary the AI was. If the AI's confidence was 52 percent for approval, it was near the boundary. If confidence was 92 percent, it was far from the boundary. Log both the override and the confidence at time of override. High-confidence overrides indicate the AI is confidently wrong. Low-confidence overrides indicate the AI is uncertain and humans are making the final call.

Regression decisions have natural delta. The AI predicts a loan default probability of 8 percent. The human assesses it as 15 percent. The delta is 7 percentage points. The AI predicts a customer lifetime value of 1,200 dollars. The human assesses it as 800 dollars. The delta is 400 dollars. For regression, compute delta as the absolute difference between AI and human predictions. Also compute relative delta: the difference divided by the human's value. A 400 dollar difference on an 800 dollar prediction is 50 percent relative delta. A 400 dollar difference on a 4,000 dollar prediction is 10 percent. Relative delta normalizes across scales.

Ordinal decisions have rank-based delta. Content moderation might have five severity levels: none, low, medium, high, critical. The AI rates a post as medium. The human rates it as high. The delta is one level. The AI rates it as low. The human rates it as critical. The delta is three levels. Count the number of levels between AI and human judgments. Larger deltas mean larger disagreements.

Multi-dimensional decisions require vector delta. A hiring decision might include multiple scores: technical skill, culture fit, leadership potential. The AI rates a candidate as 7, 6, 5 on a 10-point scale. The human rates them as 8, 4, 7. The delta is the Euclidean distance between the vectors: square root of one squared plus two squared plus two squared, which equals 3. Or use Manhattan distance: sum of absolute differences, which equals 5. Either metric quantifies the total disagreement across all dimensions.

For any decision type, also track delta direction. Is the human consistently more conservative than the AI, or more permissive? If the AI predicts default risk at 8 percent and humans average 12 percent, humans are more risk-averse. If humans average 5 percent, they are more risk-tolerant. Direction matters for recalibration. A model that is systematically biased in one direction can be corrected with a constant offset or a scaling factor.

## Correction Delta Distribution and What It Reveals

The distribution of correction delta is more informative than the average. A mean delta of 10 percent could come from uniform 10 percent adjustments across all cases, or from 90 percent of cases with zero delta and 10 percent with 100 percent delta. The first distribution suggests the AI is consistently slightly off. The second suggests the AI is mostly correct but catastrophically wrong on a subset of cases.

Plot the distribution. A narrow distribution centered near zero means the AI and humans agree most of the time, with small refinements. A wide distribution means high variance — humans make large corrections frequently. A bimodal distribution means two regimes: cases where AI and humans agree, and cases where they strongly disagree. A skewed distribution means the AI is biased in one direction.

A narrow distribution is ideal for low-stakes decisions. If you are predicting email response times and humans adjust the AI's prediction by an average of 2 percent with standard deviation of 3 percent, the AI is doing fine. Humans are making cosmetic adjustments. A wide distribution is acceptable for high-stakes decisions where humans are expected to apply significant judgment. If you are pricing complex insurance policies and humans adjust the AI's premium by an average of 8 percent with standard deviation of 15 percent, that might be reasonable. Pricing is hard. Humans add value.

A bimodal distribution is a red flag. It means the AI works well for some cases and fails for others. Segment by case type. If the AI is accurate for standard applications and wildly wrong for non-standard applications, route non-standard applications to human-first workflows and use the AI only for standard cases. Do not let the AI make recommendations it cannot support.

Track the distribution over time. If the standard deviation of delta is increasing, the AI is drifting. Its predictions are becoming more variable relative to human judgment. If the mean delta is shifting from zero to positive or negative, the AI is developing a systematic bias. Retrain or recalibrate.

## High-Delta Cases as Model Failure Signals

Cases where correction delta is large are cases where the model failed. These are your most valuable training examples. If the AI recommended a 100,000 dollar loan and the human approved 40,000, the AI made a 60 percent error. This case contains information the model does not have or does not use correctly. Analyze it.

Flag all cases where delta exceeds a threshold — say, two standard deviations from the mean. These are outlier corrections. Export them. Review them manually. Look for patterns. Do they share features? Do they involve specific scenarios the model was not trained on? Do they reveal data quality issues?

If high-delta cases cluster around a specific feature value, that feature is causing model errors. Suppose 40 percent of high-delta loan corrections involve applicants with recent job changes. The model is not handling job stability correctly. It might be over-weighting income and under-weighting employment duration. Retrain with additional features or reweighted examples.

If high-delta cases involve missing data, your missing data handling is wrong. The model might be imputing missing values incorrectly or ignoring them. Suppose the AI assigns high loan amounts to applicants with missing credit history, assuming missing means irrelevant. Humans see missing as risky and reduce amounts. Fix the imputation strategy. Treat missing as a signal, not noise.

If high-delta cases involve rare combinations of features, your model has not seen enough training examples. A loan application from a young applicant with high income and no credit history is rare. The model extrapolates poorly. Humans apply judgment. Collect more examples of rare cases, or exclude rare cases from AI recommendation and route them directly to human review.

Use high-delta cases to build a challenging evaluation set. If you retrain the model, test it specifically on past high-delta cases. Can it now get them right? If delta on these cases decreases, the retrain worked. If delta remains high, the retrain did not address the root cause. Iterate.

## Correction Delta by Model Confidence

Correction delta should correlate with model confidence. High-confidence predictions should have low delta. Low-confidence predictions should have high delta. If this relationship holds, confidence is a useful signal for deciding when to involve humans.

Segment correction delta by confidence band. For predictions with confidence above 90 percent, compute average delta. For confidence 70 to 90 percent, compute average delta. For confidence below 70 percent, compute average delta. Plot the results. You should see monotonic decrease: higher confidence correlates with lower delta.

If delta is flat across confidence bands, confidence is not meaningful. A 95 percent confidence prediction is no more accurate than a 55 percent confidence prediction. Recalibrate the model. Use Platt scaling or isotonic regression. After recalibration, recheck the relationship. Confidence should now predict delta.

If delta is higher for high-confidence predictions than low-confidence ones, the model is miscalibrated in the opposite direction. It is most confident when it is most wrong. This is dangerous. It encourages over-reliance on the AI. If users see 95 percent confidence, they trust the recommendation. If that recommendation is wrong by 50 percent, trust collapses. Fix this immediately. Investigate whether the model is overfitting, whether confidence is computed incorrectly, or whether training data contains systematic errors.

Use delta-by-confidence analysis to set routing rules. If delta for confidence below 70 percent averages 30 percent, route those cases to human review. If delta for confidence above 90 percent averages 3 percent, allow the AI to decide. The middle range — 70 to 90 percent — might need human oversight or spot-checking depending on risk tolerance.

## Tracking Delta by Reviewer and Reviewer Consistency

Not all reviewers produce the same delta. Some reviewers make small adjustments. Others make large adjustments. This variance matters. If one reviewer consistently produces high delta, they either see harder cases, have different standards, or distrust the AI.

Measure average correction delta per reviewer. Plot the distribution. If most reviewers cluster around 5 percent delta and one reviewer averages 40 percent delta, investigate. Check case mix. If the high-delta reviewer sees the same cases as others, they are applying a different standard. Calibrate them. Have them review the same cases as other reviewers and compare.

If the high-delta reviewer is correct — their adjustments align with ground truth better than the AI or other reviewers — they are your most skilled reviewer. Promote them. Have them train others. Understand their judgment process. Incorporate their reasoning into policies or model features.

If the high-delta reviewer is incorrect — their adjustments do not align with ground truth — they are over-correcting. They might distrust the AI, or they might be applying personal preferences. Retrain them. Show them cases where the AI was correct and they adjusted unnecessarily. Show them the cost of over-adjustment.

Also measure delta consistency per reviewer. Compute the standard deviation of delta for each reviewer. A reviewer with low mean delta and low standard deviation makes small, consistent adjustments. A reviewer with low mean delta and high standard deviation sometimes agrees, sometimes makes large corrections. Inconsistency is a problem. It means the reviewer's judgment is not stable. Investigate why. Are they guessing? Are they applying inconsistent policies? Do they need better tools or information?

## Correction Delta as a Retraining Signal

Correction delta is richer than binary labels. When you retrain a model, binary labels tell you "this was wrong." Correction delta tells you "this was wrong by this much, in this direction." This is more informative. It enables regression-based retraining and recalibration.

If the AI predicts loan amounts and humans adjust them, use the adjusted amounts as regression targets. Retrain the model to predict the human-adjusted amount, not the original AI amount. If the model consistently predicts 10 percent higher than humans approve, apply a 10 percent discount factor at inference time. This corrects systematic bias without retraining.

If the correction delta has a learnable pattern, train a secondary model. The first model makes a prediction. The second model predicts the correction delta based on the prediction and case features. Combine them: final prediction equals first model output plus second model delta. This is stacking. It works when corrections are predictable from features the first model did not use effectively.

If corrections are rare but large, treat high-delta cases as hard negatives. Include them in the training set with high sample weight. The model will pay more attention to them. This reduces errors on the cases that matter most. Do not just add more data randomly. Add the data where the model currently fails.

Track delta reduction over time. After a retrain, measure delta on a held-out set. If delta decreases, the retrain worked. If delta stays the same, the retrain did not address the issue. If delta increases, the retrain made the model worse. Use delta as the eval metric, not just accuracy or override rate. Accuracy can improve while delta gets worse if the model shifts its errors from one type to another.

## Building Correction Delta Monitoring Dashboards

Correction delta monitoring requires different dashboards than override rate monitoring. The metrics are continuous, not binary. The visualizations are distributions and trends, not just counts.

Dashboard one: delta distribution. Show a histogram of correction delta for the past week. Overlay the prior week's distribution for comparison. Highlight shifts. Show summary statistics: mean, median, standard deviation, 90th percentile. This tells you whether corrections are getting larger or smaller.

Dashboard two: delta by confidence. Show average delta for each confidence band. Show delta distribution within each band. Highlight bands where delta is high despite high confidence. This tells you whether confidence is trustworthy.

Dashboard three: delta by case type. Show average delta for each case type or segment. Highlight types with high delta. This tells you which cases the model handles poorly.

Dashboard four: delta by reviewer. Show average delta and delta standard deviation for each reviewer. Highlight reviewers with high delta or high variance. This tells you who needs calibration.

Dashboard five: high-delta cases. List the top 20 cases by delta from the past week. Include features and human reasoning. This is the manual review queue. These cases deserve investigation.

Set alerts based on delta thresholds. If mean delta exceeds 15 percent for three consecutive days, alert the model team. If 90th percentile delta exceeds 50 percent, alert that extreme errors are occurring. If a single reviewer's delta exceeds 40 percent, alert the ops team. Correction delta is a leading indicator. It catches model degradation before it becomes a crisis.

The next subchapter covers feedback loop latency — measuring how long it takes for human corrections to reach the model retraining pipeline and improve the AI.

