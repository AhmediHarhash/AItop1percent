# 14.3 — Weekly Review Cadence: The Metrics That Matter

Most teams that fail at production observability have excellent dashboards. They instrument everything. They collect millions of data points. They build beautiful visualizations. And then they never systematically review what the data is telling them.

The dashboards sit unused except during incidents. Trends go unnoticed until they cross critical thresholds. Gradual degradation accumulates week after week because nobody is comparing this week to last week. The team has observability infrastructure but not observability practice. The weekly review is what closes that gap.

## Why Weekly Is the Correct Cadence

Daily reviews are too frequent. Most meaningful changes in AI systems happen over days, not hours. Reviewing metrics daily generates noise and meeting fatigue without providing proportional insight. You see random variation, not real trends.

Monthly reviews are too infrequent. Problems that emerged two weeks ago have already caused user harm by the time you notice them in a monthly review. Trends that started three weeks ago have had time to compound. Monthly reviews catch problems too late to prevent damage.

Weekly reviews are the minimum viable cadence for staying ahead of degradation. A problem that emerges on Tuesday gets caught by Friday's review. A trend that started last week gets investigated this week before it escalates. Week-over-week comparisons are long enough to filter out daily noise but short enough to catch issues before they metastasize.

The weekly cadence also aligns with human attention patterns. Teams can sustain a one-hour weekly commitment. They cannot sustain a daily 15-minute commitment — it degrades into a box-checking exercise within two weeks. Weekly creates rhythm without burnout.

## The Agenda: What Gets Reviewed Every Week

The weekly review follows a consistent structure. Consistency matters because it creates memory. The team knows what to expect. They know what to prepare. They know which metrics will be discussed. This predictability enables meaningful preparation and prevents the meeting from becoming an unstructured conversation.

**Production health overview** comes first. The Observability Lead presents high-level metrics: total request volume, average latency, error rate, cost per query, model utilization. These are the top-line indicators of system health. The question being answered is: compared to last week, are we handling more traffic, responding faster, seeing fewer errors, and spending less per query? Week-over-week comparison is mandatory. Absolute numbers without comparison provide no insight.

**Quality metrics review** comes second. This is the core of the AI-specific review. Correct answer rate, user satisfaction scores, thumbs-down rate, escalation rate, refusal rate, policy violation rate. Each metric is compared to last week, and any movement of more than five percentage points in either direction triggers discussion. Why did correct answer rate drop from 89 percent to 85 percent? Why did refusals increase from seven percent to 12 percent? If the change correlates with a deployment or configuration change, investigate. If not, flag for deeper analysis.

**Segment-specific trends** comes third. Overall metrics can mask problems in specific segments. The team reviews quality metrics broken down by user cohort, geography, language, and request type. If overall correct answer rate held steady but Spanish-language correct answer rate dropped 10 percentage points, that is a problem masked by aggregate data. Segment analysis catches disparate impact that aggregate metrics hide.

**Incidents and near-misses** comes fourth. The on-call engineer from the previous week presents any incidents, degradations, or near-misses. Near-misses matter as much as actual incidents. If an alert almost fired but did not quite cross the threshold, that is a signal. If quality degraded but stayed within SLA bounds, that is still worth discussing. Near-misses are leading indicators. Ignoring them means waiting until the leading indicator becomes a lagging indicator.

**Observability gaps** comes fifth. Did the on-call engineer struggle to diagnose an issue because a dashboard was missing? Did a quality degradation go unnoticed because no alert covered it? Did an incident take longer to understand because logs were incomplete? The weekly review is where these gaps get documented and prioritized. Every week, the team should identify at least one observability improvement to make. If you are not continuously improving observability, you are falling behind the product's evolution.

**Action items from last week** comes last. Did the team commit to investigating a cost spike? Did someone volunteer to update a stale runbook? Did a Domain Monitor say they would tighten a threshold? The final five minutes review whether last week's commitments were completed and why if they were not. Accountability without follow-up is theater.

## The Metrics That Matter: What to Track Week Over Week

Not all metrics deserve weekly attention. Some metrics are lagging indicators reviewed monthly or quarterly. Some are real-time indicators monitored continuously. The weekly review focuses on the metrics that show meaningful change week to week and predict upcoming problems.

**Request volume trends** tell you whether usage is growing, stable, or declining. A 20 percent week-over-week increase in requests means you need to start thinking about capacity. A 15 percent decline means you need to investigate whether users are abandoning the feature. Sudden changes in volume correlate with quality issues, cost issues, and capacity issues. Volume is the multiplier on everything else.

**Latency distribution** matters more than average latency. If average latency is stable but P95 latency increased 30 percent, something is wrong. The weekly review looks at P50, P95, and P99 latency week over week. If the tail is expanding, investigate before it crosses SLA thresholds. Latency degradation happens gradually and then suddenly. Weekly monitoring catches the gradual phase.

**Quality by segment** prevents aggregate metrics from hiding problems. The team reviews correct answer rate, user satisfaction, and refusal rate broken down by user cohort, request type, and geography. If quality is stable for most users but degrading for a specific cohort, that is both a product problem and a fairness problem. Segment-specific trends appear weeks before they affect aggregate metrics.

**Cost per query trends** reveal inefficiency early. If cost per query increased 12 percent week over week but quality held steady, you are getting worse ROI. Investigate whether traffic shifted toward expensive models, whether caching stopped working, whether batch sizes decreased, or whether retry logic is triggering more often. Cost creep happens slowly unless monitored weekly.

**Alert signal-to-noise ratio** measures observability quality. If an alert fired 15 times last week but only two firings indicated real problems, that alert is 87 percent noise. The weekly review tracks which alerts fired, which indicated real issues, and which were false positives. Noisy alerts get tuned or retired. Missing alerts get added. This continuous tuning is what keeps the alerting system useful.

**User escalation rate** is the leading indicator of user dissatisfaction. If users are escalating to human support more often this week than last week, something is wrong with the AI system. Maybe it is refusing more often. Maybe it is giving incorrect answers more often. Maybe it is slower. Escalation rate rises before churn metrics move, giving you time to fix issues before users leave.

## How to Run the Review Without It Becoming a Status Meeting

The failure mode of the weekly review is that it degrades into a status meeting where people read dashboards aloud without insight or action. Preventing this requires discipline from the Observability Lead.

**Prepare metrics in advance.** Every Domain Monitor prepares their section before the meeting. They have already compared this week to last week, identified anomalies, and formed hypotheses about causes. The meeting is for discussion, not for data discovery. If someone is calculating week-over-week percentages during the meeting, preparation failed.

**Lead with anomalies, not with "everything is fine."** If a metric is stable week over week and behaving as expected, say that in one sentence and move on. Spend time on anomalies: metrics that moved significantly, trends that started or reversed, segments that diverged from overall patterns. The review is for finding problems, not for confirming that problems do not exist.

**Require hypotheses, not just observations.** When someone says "correct answer rate dropped from 89 percent to 85 percent," the next sentence must be a hypothesis about why. "This correlates with the model deployment on Tuesday" or "this coincides with increased traffic from free-tier users" or "I do not know yet but I will investigate this week." Observation without hypothesis is data without insight.

**Assign owners for every action item.** If the team decides to investigate a cost spike, someone's name goes next to it. If a dashboard needs updating, someone's name goes next to it. If a threshold needs tuning, someone's name goes next to it. Anonymous action items do not get done. Named action items have accountability.

**Time-box the meeting strictly.** The weekly review is one hour, no exceptions. If a topic requires more than 10 minutes of discussion, it becomes a separate deep-dive meeting with a smaller group. The weekly review is for breadth and triage, not for depth and resolution. Respecting the time limit keeps the team engaged and prevents meeting fatigue.

## What Gets Escalated from the Weekly Review

The weekly review identifies issues, but not all issues get resolved in the review. Some require deeper investigation, specialized expertise, or cross-functional coordination. Knowing what to escalate and to whom is critical.

**Persistent quality degradation** escalates to the ML team. If correct answer rate has declined for three consecutive weeks and the team cannot identify a reversible cause like a bad deployment, the ML team needs to investigate whether data drift, distribution shift, or model degradation is happening.

**Cost trends that exceed budget** escalate to Finance and Engineering leadership. If cost per query is increasing week over week and the team cannot mitigate through routing or caching improvements, leadership needs to know that the budget will be exceeded and that architectural changes might be necessary.

**Segment-specific quality gaps** escalate to Product and Trust and Safety. If a specific user cohort or demographic is experiencing lower quality, Product decides whether this is acceptable or requires prioritized fixes. If the gap creates fairness concerns, Trust and Safety weighs in on mitigation.

**Recurring incidents without clear fixes** escalate to Engineering leadership. If the on-call engineer has been paged three weeks in a row for the same issue and the team has not identified a permanent fix, something is wrong with prioritization or resourcing. Leadership needs to allocate time to address root cause.

**Observability debt** escalates to Engineering leadership if it accumulates faster than the team can pay it down. If the weekly review identifies five new observability gaps every week but the team only closes two, the gap widens. Leadership needs to allocate dedicated time for observability improvements or accept increasing operational risk.

## Metrics That Do Not Belong in the Weekly Review

Some metrics are important but do not warrant weekly attention. Reviewing them weekly generates noise without insight because they do not change meaningfully week to week.

**Long-term accuracy trends** are better reviewed monthly or quarterly. Whether the model's accuracy on a benchmark dataset changed from 87.3 percent to 87.1 percent this week is not operationally meaningful. Review these trends over longer horizons when you have enough data to distinguish signal from noise.

**Infrastructure health metrics** like CPU utilization, memory usage, and disk space are monitored continuously by automated systems. Unless an alert fired or an incident occurred, there is no value in reviewing them weekly. They belong in the on-call engineer's dashboard, not the weekly review.

**Rare edge cases** that occur once or twice per week do not warrant discussion unless they indicate a pattern. If one user reported a bizarre failure that happened once and never repeated, document it and move on. If the same bizarre failure starts happening three times per week, then it is a trend worth discussing.

**Aspirational metrics** that the team is not yet acting on should not be in the weekly review. If you are tracking a metric but do not yet have a plan for what to do when it moves, remove it from the review. It creates cognitive load without enabling action. Add it back when you are ready to act on it.

## Making the Weekly Review Sustainable

The weekly review is a long-term commitment. For it to remain effective, it must be sustainable without burning out the team.

**Rotate the presenter role.** The Observability Lead facilitates, but different Domain Monitors present their areas each week. Rotation distributes the preparation load and prevents any one person from becoming the bottleneck. It also ensures that multiple people stay fluent in interpreting production metrics.

**Keep the format consistent.** Use the same agenda every week. The same metric order. The same dashboard templates. Consistency reduces cognitive load. The team can focus on interpreting the data instead of figuring out what they are supposed to be looking at.

**Celebrate improvements, not just problems.** If the team implemented a fix last month and this week's data shows it worked, call that out. If cost per query declined after routing optimizations, celebrate. If correct answer rate improved after dataset refinements, recognize the team that made it happen. The weekly review should not feel like an endless catalog of failures.

**Accept that some weeks are boring.** Not every week will have major insights. Sometimes metrics are stable, no incidents occurred, and everything is running smoothly. That is success. Do not invent problems to justify the meeting. Say "things look good, let's move on" and end early. A 30-minute review when everything is stable is better than a 60-minute review filled with invented concerns.

The weekly review is the heartbeat of operational observability. It catches degradation early, surfaces trends before they become crises, and keeps the team aligned on what matters. Next, we zoom out to the monthly deep dive, where longer-term patterns and systemic issues come into focus.

