# 8.11 — Learning from Incidents: Feeding Production Failures Back Into Eval

The incident was resolved. The model was rolled back. The post-mortem was complete. The engineering team understood what went wrong: the model had failed on a specific class of legal queries involving multi-party contracts. The eval suite had tested single-party and two-party contracts but not multi-party contracts. The model passed eval. It failed in production. Three weeks later, another deployment occurred. The new model passed the same eval suite. It deployed to production. Within two hours, it failed on the same class of multi-party contract queries. The same failure mode. The same eval gap. The team had learned from the incident during the post-mortem, but they had not encoded that learning into the eval suite. The knowledge stayed in their heads. It did not prevent recurrence. Six months later, after a third incident with the same root cause, the team finally added multi-party contract test cases to eval. The next deployment caught the issue before production. The incident became impossible. That is the goal: turn every production failure into a test case that prevents the failure from recurring.

Incidents reveal what your eval missed. If an issue made it to production, your eval did not test for it. The incident is a signal: this failure mode is real, it happens in production, and it must be added to your eval. Learning from incidents means systematically converting production failures into eval test cases, updating eval datasets, and expanding eval coverage to close the gaps. Without this feedback loop, you repeat the same incidents indefinitely. With this feedback loop, your eval gets stronger after every incident, and your system gets more reliable over time.

## The Incident-to-Eval Pipeline

The simplest implementation is a manual process: after every incident, add test cases to eval that would have caught the issue. This works for small teams with low incident frequency.

A healthcare documentation platform had a policy: within 48 hours of every incident resolution, the incident owner must add at least three test cases to the eval suite that would have detected the incident before production. In March 2025, an incident occurred where the model omitted medication sections from summaries. The incident owner added test cases: summaries must include medication sections if medications are present in source documents, medication sections must list all medications mentioned, and medication dosages must match source documents. The next model deployment ran these new eval cases. A regression was caught in staging that would have caused the same incident. The eval addition prevented recurrence.

The manual process scales poorly. A financial advisory platform had 42 incidents in 2025. If each incident required manually adding test cases, that was 42 separate engineering tasks. Many were delayed or forgotten. By Q4, only 60 percent of incidents had resulted in eval updates. The team automated the process. After every incident, an automated system created a GitHub issue titled "Add eval for incident X" with a template describing the incident, the failure mode, and suggested test cases. The issues were triaged weekly. Completion rate increased to 91 percent.

The incident-to-eval pipeline should be part of the incident lifecycle. A contract analysis platform had incident stages: detection, triage, mitigation, resolution, post-mortem, and eval update. An incident was not considered fully closed until the eval update was complete. The lifecycle enforcement ensured that learning happened systematically. In 2025, every incident resulted in eval updates. Incident recurrence rate dropped from 19 percent to 7 percent.

## The Failure Case Extraction

The most direct way to update eval is to extract the exact input that failed in production and add it to eval with the correct expected output.

A customer service chatbot had an incident in July 2025 where the model generated an inappropriate response to a specific user query. The engineering team extracted the query from production logs, determined the correct response, and added it to eval as a new test case. The eval suite now permanently tested that exact input. Future model versions were required to handle it correctly. The extraction took eight minutes. The test case prevented recurrence forever.

Failure case extraction requires logging. You need to log: user input, model output, and whether the output was correct. A legal document assistant logged every production request with input, output, eval score, and user feedback. When an incident occurred, they queried logs for all requests that failed during the incident window. They extracted 30 failure cases and added all 30 to eval. The next model deployment was tested against those 30 cases. Two failures were caught in staging.

Failure case extraction must handle PII and confidential data. You cannot add real user data to eval if it contains sensitive information. A healthcare documentation platform had an incident involving patient data. They extracted the failure cases, anonymized patient names and identifiers, and added the anonymized cases to eval. The anonymization preserved the structural complexity that caused the failure while removing confidential information. A financial advisory platform built an automated anonymization pipeline that replaced customer names with synthetic names, replaced account numbers with synthetic IDs, and replaced specific company names with generic placeholders. The pipeline enabled systematic failure case extraction without privacy violations.

## The Failure Mode Generalization

Adding the exact failure case to eval is necessary but not sufficient. You also need to generalize: what class of inputs does this failure represent? What other similar inputs might also fail?

A document summarization platform had an incident where the model failed on a document with nested bullet points. The engineering team added the specific failing document to eval. But they also generated 20 synthetic documents with nested bullet points of varying depths and structures. The generalization meant that eval tested not just the one failure case but the entire class of nested-structure documents. The next model deployment was tested against all 20 cases. Three additional failure modes were discovered and fixed before production.

Failure mode generalization requires domain expertise. An engineer can extract the failure case mechanically. But generalizing requires understanding what made the input difficult and what other inputs share that difficulty. A customer service chatbot had an incident where the model misunderstood a query with double negatives. The engineering team added the specific query to eval and also generated ten more queries with double negatives, triple negatives, and complex negation structures. The generalization required linguistic expertise to identify the difficulty pattern.

Generalization should be systematic, not ad hoc. A travel booking assistant built a failure mode taxonomy: input length extremes, ambiguous language, multi-intent queries, domain-specific terminology, non-standard formatting, and edge-case numerical values. When an incident occurred, the team classified the failure mode and generated test cases covering the entire category. In September 2025, an incident involved a query with ambiguous language. The team added the specific query plus 15 synthetically generated ambiguous queries covering the category. The generalization took 40 minutes but provided much broader coverage than adding only the single failure case.

## The Eval Dataset Rebalancing

Incidents often reveal that eval datasets are unbalanced. If a failure mode is rare in eval but common in production, incidents will occur repeatedly.

A legal document assistant had an incident in November 2025 involving multi-party contracts. The post-incident analysis revealed that multi-party contracts represented 18 percent of production queries but only 3 percent of eval cases. The imbalance meant that models were not adequately tested on this important query type. The team rebalanced: they oversampled multi-party contract cases in eval to reach 20 percent representation. The next model deployment was tested on a realistic distribution. Failures on multi-party contracts dropped from 14 percent to 4 percent.

Rebalancing requires continuous production monitoring. You need to know: what is the current distribution of production queries, and how does it compare to eval distribution? A healthcare documentation platform built a dashboard comparing production query distribution to eval distribution across five dimensions: medical specialty, document length, patient complexity, urgency level, and query type. The dashboard highlighted mismatches. When production distribution shifted — more oncology queries, fewer cardiology queries — the eval team rebalanced to match.

Rebalancing is not just about matching production distribution. You should oversample rare but critical cases. A financial advisory platform had a query type — requests involving deceased account holders — that represented 0.8 percent of production queries. Matching production distribution would mean 0.8 percent of eval cases. But failures on deceased account holder queries were extremely costly due to sensitivity and regulatory requirements. The team set eval representation at 5 percent, 6x higher than production frequency. The oversampling ensured that this critical edge case was thoroughly tested.

## The Regression Test Suite

Every production failure should become a permanent regression test. Regression tests ensure that once a failure is fixed, it never recurs.

A contract analysis platform maintained a regression test suite separate from the main eval suite. The regression suite contained every failure case from every production incident. The suite had 127 test cases as of December 2025. Every model deployment was required to pass 100 percent of regression tests. Main eval could tolerate some failures — the team accepted 93 percent pass rate. But regression tests were absolute. If a model failed any regression test, deployment was blocked. The policy meant that once a bug was fixed, it stayed fixed.

Regression test suites grow over time. A customer service chatbot's regression suite grew from 14 cases in January 2025 to 83 cases by December 2025. The growth was a good sign — it meant they were learning from failures. But it also meant longer test times. The team implemented prioritization: regression tests from critical and high-severity incidents ran on every deployment, regression tests from medium and low-severity incidents ran weekly. The prioritization kept deployment speed acceptable while maintaining coverage.

Regression tests should be reviewed periodically. Some failures become irrelevant as the system evolves. A document summarization platform reviewed their regression suite quarterly. In April 2025, they removed eight test cases that were no longer relevant because the underlying model architecture had changed in ways that made those failures impossible. The pruning kept the suite focused on current risks.

## The Eval Coverage Expansion

Incidents reveal entire dimensions of behavior that eval is not measuring. Coverage expansion means adding new types of tests, not just new test cases within existing categories.

A healthcare documentation platform had an incident in June 2025 where the model generated summaries with incorrect temporal ordering — events were described out of sequence. The eval suite tested factual accuracy and completeness but not temporal coherence. The incident revealed a coverage gap. The team added a new eval dimension: temporal coherence. Every summary was checked to ensure events were described in chronological order. The new dimension caught five failures in the next deployment that the old eval would have missed.

Coverage expansion should be informed by incident patterns. A financial advisory platform analyzed 38 incidents from 2025 and found that 16 involved failures the eval did not test for: tone appropriateness, demographic fairness, handling of edge-case numerical values, behavior under ambiguous inputs, and consistency across similar queries. The team added eval coverage for all five dimensions. The expanded eval caught 40 percent more issues in pre-production testing over the next six months.

Coverage expansion should not wait for incidents. Proactive red-teaming can identify coverage gaps before they cause incidents. But incidents provide the clearest signal about what is missing. A legal document assistant had a policy: after every incident, ask "what dimension of quality does our eval not currently measure?" If the answer is a dimension that matters, add it. In 2025, this policy led to adding six new eval dimensions: legal citation accuracy, clause completeness, jurisdictional appropriateness, handling of conflicting clauses, document version consistency, and response to incomplete inputs. Each addition came from an incident where eval had missed a failure.

## The Production Sampling Strategy

You cannot add every production request to eval. The volume is too high. You need a sampling strategy that captures representative and problematic cases.

A customer service chatbot sampled production requests for eval addition using four criteria: random sampling at 0.5 percent for general coverage, all requests that received negative user feedback, all requests where the model's confidence was below 70 percent, and all requests flagged by automated content policies. The sampling strategy ensured that eval included both typical cases and edge cases. In 2025, they added 2,400 production-sampled cases to eval. The sampled cases improved eval-production correlation from 0.81 to 0.89.

Production sampling should oversample failures. A travel booking assistant sampled 10 percent of successful production requests and 100 percent of failed production requests. The oversampling meant that eval was biased toward difficult cases. The bias was intentional — eval should test the model's limits, not just its average performance. The failure-heavy eval caught regressions earlier and more reliably.

Production sampling requires quality review. You cannot blindly add production cases to eval because production outputs are not always correct. A document analysis platform had a process: production cases were sampled, human reviewers labeled them with correct outputs, and only cases with verified correct outputs were added to eval. The review step ensured eval quality. Without review, eval would include cases with incorrect labels, making eval results meaningless.

## The Eval Freshness Requirement

Eval datasets decay. Production query distributions shift. User expectations evolve. An eval suite that is accurate today becomes less accurate over time if not updated.

A healthcare documentation platform had a policy: eval datasets must include production cases from the past 90 days. Older cases were retained for historical coverage, but the most recent 90 days of production represented at least 40 percent of eval. The freshness requirement meant that eval tracked production reality. When production query patterns shifted — more telemedicine queries in 2025 — the eval automatically reflected the shift because recent production cases were continuously added.

Eval freshness can be measured as the age of the most recent eval case. A financial advisory platform tracked this metric. In Q1 2025, the median age of eval cases was 147 days. Eval was stale. The team implemented a continuous eval update process where production cases were added weekly. By Q4, median age dropped to 32 days. The fresher eval caught production-relevant issues more reliably.

Freshness requirements must balance stability and relevance. If eval changes too frequently, model performance becomes non-comparable across time. You cannot tell if a performance drop is because the model got worse or because eval got harder. A legal document assistant balanced this by maintaining two eval suites: a stable historical suite that never changed, used for long-term performance tracking, and a rolling production suite that updated monthly, used for production-readiness testing. Both suites ran on every deployment. The dual-suite approach provided both stability and freshness.

## The Human-in-the-Loop Eval Update

Some incidents reveal failures that cannot be captured in automated eval. The failure requires human judgment to detect. The eval update must include human review.

A contract analysis platform had an incident in August 2025 where the model generated legally correct but strategically poor contract language — technically accurate but unfavorable to the client. Automated eval checked legal correctness. It did not check strategic quality. The incident revealed a need for human-in-the-loop eval. The team added a monthly review process where legal experts reviewed 200 randomly sampled model outputs for strategic quality. The human review caught issues automated eval missed.

Human-in-the-loop eval is expensive and slow. A customer service chatbot considered adding human review for tone and empathy after an incident where the model's responses were technically correct but emotionally inappropriate. They ran a cost analysis: human review of 1,000 cases per week would cost 40 hours of reviewer time, approximately 2,400 dollars per week, 125,000 dollars per year. The cost was justified for a customer-facing product where tone mattered. They implemented human-in-the-loop tone eval. Tone-related incidents dropped from 1.8 per month to 0.3 per month.

Human-in-the-loop eval should be used selectively for dimensions that matter and cannot be automated. A document summarization platform decided not to add human review for aesthetic quality of summaries because aesthetic preferences were subjective and not critical to functionality. They focused human review on factual accuracy, which was objective and critical. The prioritization kept costs manageable while addressing the highest-risk gaps.

## The Eval-Production Feedback Loop

The ultimate goal is a closed loop: production failures inform eval updates, eval updates improve model quality, improved models reduce production failures, and the cycle continues.

A healthcare documentation platform measured the feedback loop effectiveness by tracking: percentage of incidents that resulted in eval updates, correlation between eval scores and production performance, and incident recurrence rate. In Q1 2025, 67 percent of incidents resulted in eval updates, eval-production correlation was 0.78, and recurrence rate was 22 percent. By Q4, after systematically implementing the feedback loop, 94 percent of incidents resulted in eval updates, correlation improved to 0.87, and recurrence rate dropped to 9 percent. The closed loop was working.

The feedback loop should be measured and optimized. A financial advisory platform tracked loop latency: time from incident occurrence to eval update deployment. In Q1, loop latency averaged 18 days. Incidents happened, post-mortems occurred, eval updates were discussed, but implementation was slow. The team prioritized reducing loop latency. They automated eval case generation from incident reports, implemented continuous eval deployment, and assigned dedicated ownership for eval updates. By Q4, loop latency dropped to 4.3 days. Faster loop meant faster learning.

The feedback loop creates compounding reliability. Each incident makes eval better. Better eval catches more issues before production. Fewer issues reach production. Fewer incidents occur. The cycle reinforces itself. A legal document assistant tracked total incident count by quarter: Q1 2025 had 14 incidents, Q2 had 11 incidents, Q3 had 9 incidents, Q4 had 6 incidents. The declining trend was not because they stopped deploying — deployment frequency was constant. It was because eval was improving, catching issues earlier, and preventing incidents.

## The Incident Prevention Metric

The ultimate success metric for incident-to-eval feedback is how many future incidents are prevented. This is hard to measure directly but can be approximated.

A customer service chatbot tracked "eval catches" — issues found during pre-production testing that would have caused incidents if deployed. In Q1 2025, before implementing systematic incident-to-eval feedback, they caught 3.2 issues per quarter in pre-production testing. In Q2, after implementing feedback, they caught 7.1 issues per quarter. In Q3, 9.8 issues. In Q4, 11.3 issues. The increasing catch rate suggested that eval was becoming more effective at finding production-relevant problems. Each caught issue was a prevented incident.

Incident prevention can also be measured by recurrence rate. A document summarization platform tracked how often the same failure mode caused multiple incidents. In early 2025, recurrence rate was 24 percent — nearly one in four incidents was a repeat of a previous failure. After implementing systematic eval updates, recurrence rate dropped to 8 percent. The reduction meant that once a failure was fixed and added to eval, it rarely recurred.

The prevention metric justifies the investment in incident-to-eval feedback. A healthcare documentation platform calculated: average incident cost was 18,000 dollars in engineering time, user impact, and reputation damage. If the incident-to-eval process prevented six incidents per year, it saved 108,000 dollars per year. The process cost approximately 30,000 dollars per year in engineering time for eval updates. The ROI was 3.6x. The prevention metric made the investment clearly worthwhile.

## The Organizational Discipline

The incident-to-eval feedback loop only works if it is treated as mandatory, not optional. It requires organizational discipline.

A contract analysis platform made eval updates a required step in the incident lifecycle. Incidents could not be closed until the eval update was complete. The engineering manager reviewed all incidents weekly and followed up on any without eval updates. The discipline meant that 98 percent of incidents resulted in eval updates. Without the discipline, the number would have been much lower.

The discipline must be supported by tooling. A travel booking assistant built automated tracking: when an incident was marked resolved, a reminder fired 24 hours later asking whether eval had been updated. If not, another reminder fired after 48 hours. If still not updated after 72 hours, the incident owner's manager was notified. The automated nagging ensured that eval updates did not slip through the cracks.

The discipline must be celebrated, not just enforced. A financial advisory platform highlighted eval updates in weekly team meetings. When an engineer added test cases to eval based on an incident, it was called out as good work. The celebration reinforced that learning from incidents was valued. The cultural emphasis on learning made the incident-to-eval process feel like quality improvement, not compliance overhead.

Learning from incidents closes the loop between production reality and testing. Every failure in production becomes a test case that prevents recurrence. Every incident strengthens your eval. Every stronger eval makes your system more reliable. The feedback loop is the difference between repeating the same mistakes forever and systematically eliminating entire classes of failures. Incidents are expensive. Learning from them is how you make the expense worth it.

