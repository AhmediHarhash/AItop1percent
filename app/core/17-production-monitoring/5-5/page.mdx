# 5.5 — Context Window Utilization: Truncation Tracking and Waste Detection

You retrieve twenty documents. You concatenate them into a prompt. The prompt exceeds the model's context window. The system truncates the last seven documents. The model never sees them. One of those seven documents contained the exact answer to the user's query. The model synthesizes an answer from the thirteen documents it did see, none of which were as relevant. The answer is plausible but incomplete. The user does not know that the system retrieved the right document and then discarded it before the model could use it. This is the context window waste problem, and it is completely invisible in standard retrieval metrics.

Your retrieval precision might be perfect. Your recall might be high. Your ranking might be optimal. But if the model's context window is too small to fit all the retrieved documents, or if the documents you retrieve are so verbose that only a fraction of them fit, the model operates on a subset of the retrieved context. If that subset does not include the most relevant passages, answer quality degrades. The failure mode is not retrieval failure. The failure mode is context budget mismanagement.

## The Truncation Failure Pattern

In September 2025, a legal research RAG system retrieved ten case law documents per query. Each document was a full court opinion, ranging from 8,000 to 40,000 tokens. The system concatenated them in ranked order and sent them to a model with a 128,000-token context window. The system prompt and query used 2,000 tokens. That left 126,000 tokens for retrieved documents. On average, the first three documents consumed 90,000 tokens. The remaining seven documents were truncated or omitted entirely. The retrieval system ranked documents correctly. The model never saw the documents ranked 4th through 10th because the top three were so long that they filled the context window.

For some queries, the 4th or 5th ranked document was more directly relevant than the 2nd or 3rd. The ranking model scored documents based on overall relevance, but relevance is not uniform across an entire 30,000-token opinion. A case might be highly relevant in its analysis section but include thousands of tokens of procedural history that are irrelevant to the user's query. The retrieval system retrieved the entire opinion. The model received the entire opinion, including the irrelevant procedural sections, which consumed context budget that could have been used for the 4th-ranked document's highly relevant analysis section.

The team did not realize this was happening until they added context window utilization tracking. They logged how many tokens each retrieved document consumed and whether any documents were truncated. They discovered that 34 percent of queries resulted in at least one retrieved document being fully truncated — retrieved but not seen by the model. For queries where truncation occurred, answer quality was measurably worse. Recall at 10 was 0.88, but effective recall — fraction of queries where a relevant document was both retrieved and seen by the model — was 0.71. The retrieval system was working. The context budget was failing.

## Measuring Context Window Waste

Context window waste is the fraction of the context window consumed by irrelevant content. If you retrieve five documents, the model's context window has space for 50,000 tokens, and the five documents total 50,000 tokens, you have zero unused capacity but potentially high waste if large portions of those documents are not relevant to the query. Measuring waste requires passage-level relevance judgments. For each retrieved document, identify which passages are relevant to the query. Calculate the token count of relevant passages versus total token count. Waste is the fraction of tokens that are not in relevant passages.

A healthcare documentation RAG retrieved clinical guidelines. Guidelines were comprehensive documents covering multiple scenarios. A user querying about a specific condition received a 15,000-token guideline document, of which 1,200 tokens were relevant to the condition. The other 13,800 tokens covered different conditions, background information, and references. The waste ratio was 92 percent. The model's context window was dominated by irrelevant content. The model had to synthesize an answer while wading through 13,800 tokens of noise to find 1,200 tokens of signal.

The team implemented passage extraction. Instead of retrieving full guidelines, they chunked guidelines into condition-specific sections and retrieved only the relevant sections. A query that previously retrieved 15,000 tokens now retrieved 2,000 tokens — the relevant section plus a small amount of surrounding context for coherence. Waste ratio dropped to 40 percent. The same context window budget now fit five queries' worth of content instead of one. The model had more signal and less noise. Answer quality improved, and the system could support higher query concurrency because each query consumed less context capacity.

## Truncation Tracking as a Leading Indicator

Truncation is a hard failure. The document was retrieved but not seen. Tracking truncation rates tells you how often retrieval effort is wasted. Log every query. Log how many documents were retrieved. Log how many documents fit in the context window after accounting for system prompt, query, and generation budget. Log how many were truncated. If truncation rate is above 10 percent, your retrieval strategy is returning more documents than your context window can handle, or your documents are too verbose.

A customer support RAG retrieved fifteen help articles per query. The model's context window was 32,000 tokens. Articles varied in length from 500 to 8,000 tokens. On average, twelve articles fit. Three were truncated. Truncation rate was 20 percent. For queries where the 13th, 14th, or 15th article was the most relevant, the model never saw it. The team reduced retrieval count from fifteen to ten. Truncation rate dropped to 5 percent. Recall at 10 was nearly identical to recall at 15 because the 11th through 15th ranked documents were rarely the most relevant. The retrieval precision improved because the system stopped returning marginally relevant documents that would not fit anyway.

Truncation tracking also exposes document length variance problems. If most documents are 2,000 tokens but a few are 30,000 tokens, retrieving one long document can crowd out nine shorter documents. If the long document is highly relevant, this is fine. If the long document is moderately relevant and several shorter documents are also moderately relevant, you lose coverage by spending the entire context budget on one document.

A legal contract RAG had this problem. Most contracts were 5,000 to 10,000 tokens. Certain contracts — master service agreements, enterprise licenses — were 60,000 tokens. When one of those large contracts was retrieved, it consumed most of the context window. Other relevant contracts were truncated. The team set a per-document token limit of 12,000 tokens. Documents exceeding the limit were chunked into sections. The retrieval system could return multiple sections from the same large document, or sections from several documents, without one document monopolizing the context window. Truncation rate dropped from 28 percent to 9 percent.

## Ranking for Context Budget

Traditional ranking optimizes for relevance score. Context-aware ranking optimizes for relevance per token. A document with a relevance score of 0.9 and 20,000 tokens might be worse than a document with a relevance score of 0.85 and 3,000 tokens if the context window is constrained. The shorter document delivers nearly the same relevance at a fraction of the cost, leaving budget for additional documents.

**Relevance density** is relevance score divided by token count. Ranking by relevance density prioritizes documents that deliver the most signal per token consumed. This is especially important when the model has a limited context window or when inference cost is proportional to context length. A system optimizing for relevance density will prefer concise, high-signal documents over verbose, moderately relevant ones.

A technical support RAG ranked documents by raw relevance score. The top-ranked document for a query about API authentication was a comprehensive API guide — 18,000 tokens covering every endpoint and feature, with a relevance score of 0.92. The 2nd-ranked document was a focused authentication tutorial — 2,200 tokens covering only authentication, with a relevance score of 0.89. The system retrieved the guide first. It consumed the majority of the context window. The tutorial was truncated. The model synthesized an answer from the guide, which included the needed information but also 15,000 tokens of irrelevant API details. The answer was verbose and unfocused.

The team switched to relevance density ranking. The tutorial ranked first — relevance score 0.89 divided by 2,200 tokens equals density of 0.0004. The guide ranked second — relevance score 0.92 divided by 18,000 tokens equals density of 0.00005. The tutorial fit comfortably in the context window. The guide was retrieved but ranked lower. If the context budget allowed, the model also saw the guide. If not, the model had the tutorial, which was sufficient. Answer quality improved because the model encountered the most relevant information first and did not have to extract signal from a sea of tangentially related content.

## Multi-Turn Context Accumulation

In multi-turn conversations, context accumulates. The first turn uses 5,000 tokens. The second turn adds another 4,000 tokens of retrieved content. The third turn adds 3,500 more. By turn five, the conversation history plus retrieved documents exceed the context window. The system either truncates early turns, losing conversation context, or truncates recent retrievals, losing the information needed to answer the current query. Both are bad.

Tracking context window utilization across turns reveals when conversations hit the limit. Log the token count for conversation history, system prompt, current query, and retrieved documents. If the total exceeds the model's context window, something gets truncated. Most systems truncate the oldest turns first, preserving recent context. This works until a user refers back to something from turn two that has been truncated. The model no longer has that context and cannot answer accurately.

A customer service chatbot had a 32,000-token context window. The system prompt was 1,500 tokens. Each turn of conversation added 2,000 to 5,000 tokens. By turn seven, the conversation history alone was 30,000 tokens. The system continued retrieving documents for each turn, but there was no room for them. Retrieved documents were truncated immediately. The model answered based purely on conversation history and system instructions, without access to the knowledge base. Users asked factual questions. The model gave generic answers because it had no retrieved context. The team did not realize this was happening until they logged context window usage per turn. Turn seven and beyond had zero tokens allocated to retrieved documents. The retrieval system was running, returning results, but the model never saw them.

The solution was context window management. After turn five, the system stopped appending full conversation history and instead summarized early turns into 1,000-token summaries. This kept total conversation history under 15,000 tokens, leaving 16,500 tokens for retrieved documents. The model could still access the knowledge base in later turns. Answer quality stabilized instead of degrading as conversations lengthened.

## Monitoring Context Budget Allocation

Build a dashboard showing how context tokens are allocated per query. Break down by system prompt, query, conversation history, retrieved documents, and generation budget. If retrieved documents consistently consume less than 50 percent of available tokens, you might be under-retrieving — there is budget to retrieve more documents and improve coverage. If retrieved documents consistently consume more than 90 percent of available tokens, you are at risk of truncation and might need to retrieve fewer or shorter documents.

An insurance Q&A RAG tracked context allocation and found that retrieved documents used only 35 percent of available tokens on average. The system retrieved five documents. The context window could fit twelve. The team increased retrieval count to ten. Coverage improved. Queries that previously had insufficient context now had more retrieved documents to draw from. Answer completeness increased. The extra documents did not hurt precision because the ranking model was accurate — the 6th through 10th ranked documents were still relevant, just less so than the top five. Having them available gave the model more context to synthesize comprehensive answers.

Another system found that retrieved documents consumed 95 percent of available tokens. Generation budget — tokens reserved for the model's response — was squeezed to 5 percent of the window. The model frequently hit generation limits mid-sentence, producing truncated answers. The team reduced retrieval from eight documents to six, freeing up token budget for generation. Answer truncation rates dropped from 18 percent to 3 percent. Retrieval coverage dropped slightly, but answer completeness improved because the model had room to finish its responses.

## The Token-Aware Retrieval Strategy

Not all retrieval strategies are token-aware. Many systems retrieve a fixed number of documents regardless of their length. Token-aware retrieval sets a token budget and retrieves as many documents as fit within that budget. If you have a 20,000-token budget for retrieved content and your documents average 3,000 tokens, you retrieve six documents. If a query retrieves documents averaging 6,000 tokens, you retrieve three documents. The number of documents varies, but the token consumption is stable.

This prevents the failure mode where retrieving five short documents works fine, but retrieving five long documents causes truncation. Token-aware retrieval adapts to document length. The model always receives a full context budget of retrieved content, but the document count varies based on document verbosity.

A news aggregation RAG implemented token-aware retrieval with a 30,000-token budget for articles. Most articles were 1,500 to 2,500 tokens. The system retrieved ten to fifteen articles. Breaking news queries often retrieved long-form investigative pieces of 8,000 tokens. The system retrieved three to four articles instead. In both cases, the model received 30,000 tokens of retrieved content. The document count adapted to fit the budget. This avoided the problem where one long article consumed the entire window, and it avoided the problem where many short articles created redundancy and noise.

## The Cost of Waste

Context window waste is not just a quality problem. It is a cost problem. Most model APIs charge by token. If you send 50,000 tokens to the model and only 10,000 tokens are relevant, you paid for 40,000 tokens of waste. At scale, this is significant. A system processing one million queries per day, with an average of 30,000 tokens per query and a 60 percent waste ratio, is paying for 18 billion wasted tokens per day. At current pricing, that is thousands of dollars daily in pure waste.

Reducing waste reduces cost. Passage extraction, relevance density ranking, token-aware retrieval, and truncation prevention all reduce the number of irrelevant tokens sent to the model. A fintech platform reduced context window waste from 58 percent to 31 percent by implementing passage extraction and relevance density ranking. Token consumption per query dropped by 40 percent. At their query volume, this saved 290,000 dollars per month in model API costs. The retrieval quality also improved. Reducing waste was both a cost optimization and a quality improvement.

## The Observability Requirements

Track tokens per query component. Log system prompt tokens, query tokens, conversation history tokens, retrieved document tokens, and generated response tokens. Sum them. Compare to the model's context window limit. If the sum exceeds the limit, log which component was truncated. If retrieved documents were truncated, log how many documents fit versus how many were retrieved. Track truncation rate over time. If it trends upward, your retrieval strategy is retrieving more or longer documents while your context budget remains constant.

Track waste ratio. For a sample of queries, measure what fraction of retrieved tokens are in passages judged relevant. If waste is high, your retrieval is returning whole documents when you need specific passages. Track context allocation per component. If retrieved documents consume less than 40 percent of the window, you are under-utilizing retrieval. If they consume more than 90 percent, you are at risk of truncation and starving the generation budget.

The next subchapter covers citation and source coverage monitoring — ensuring that the model's cited sources are comprehensive, accurate, and actually support the claims made in the response.

