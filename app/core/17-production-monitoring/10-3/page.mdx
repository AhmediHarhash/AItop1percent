# 10.3 — Canary Rollout for LLM Changes: Gradual Exposure Patterns

Canary rollout means sending a small percentage of real production traffic to the new model while the majority continues using the baseline. Unlike shadow mode, users now see the new model's outputs. Unlike full deployment, only a controlled subset of users are exposed. The canary is your early warning system. If the new model breaks, only 5 percent of users see the breakage. You detect the problem, roll back, and investigate. The other 95 percent never knew anything went wrong.

The term comes from coal mining, where canaries detected toxic gas before it killed miners. In AI deployment, the canary detects regressions before they destroy user trust. The canary is not sacrificial — you monitor carefully and pull it back at the first sign of trouble. But the canary takes the risk that you cannot eliminate through testing alone. Production is the final exam. Shadow mode proved the model works on production data. Canary rollout proves it works with production users.

## The Traffic Routing Mechanics of Canary Deployment

Canary rollout requires infrastructure that routes some requests to the new model and some to the old model, with precise control over the split percentage. The routing decision happens before the model receives the request. Once a user is assigned to a variant, they must stay on that variant for the duration of the canary period to maintain consistency.

The simplest routing strategy is random assignment. For each incoming request, generate a random number. If it falls below your canary threshold — say 5 percent — route to the new model. Otherwise route to the old model. Random assignment distributes the canary traffic evenly across all users and query types, giving you representative coverage. The downside is that individual users may see inconsistent behavior if different requests in the same session hit different variants. A user asks three questions. The first two hit the old model, the third hits the new model. The response style changes mid-conversation. Users notice inconsistency, even if both models are high quality.

Session-based routing solves the consistency problem. Hash the user ID or session ID, and use the hash to determine variant assignment. All requests from the same user or session route to the same variant for the canary duration. Users get consistent experience. The downside is uneven distribution — if your canary is 5 percent and you have 20 power users who collectively generate 30 percent of queries, random hashing might assign all of them to the baseline, giving your canary minimal coverage of high-volume users. Session-based routing works best when traffic is distributed across many users rather than concentrated in a few.

Segment-based routing gives you explicit control over which users enter the canary. Route internal employees first, then low-tier customers, then high-tier customers. Route users in low-stakes geographies before high-stakes geographies. Route users with high tolerance for experimentation before users with zero tolerance. Segment-based routing lets you sequence risk deliberately. The downside is complexity — you need user segmentation infrastructure and logic to manage it. Most teams start with random or session-based routing and graduate to segment-based routing as their deployment sophistication increases.

A financial planning platform used segment-based canary routing when deploying a new GPT-5.1 model in December 2025. They started with internal employees — 30 people using the system daily. After three days with no issues, they expanded to free-tier users — 2,000 users generating 5,000 queries per week. After one week with acceptable metrics, they expanded to paid individual customers — 8,000 users generating 40,000 queries per week. After two weeks, they expanded to small business customers — 1,200 customers generating 80,000 queries per week. Enterprise customers were last, entering canary only after four weeks of proven stability. The segmentation gave them escalating confidence with controlled risk at each stage.

## The Metrics That Define Canary Success or Failure

Canary rollout succeeds or fails based on metrics you define before deployment begins. The metrics must be measurable, automated, and actionable. If a metric regresses beyond a threshold, rollout pauses or reverts. The metrics fall into four categories: correctness, performance, user satisfaction, and downstream impact.

Correctness metrics measure whether the model produces accurate, safe, and policy-compliant outputs. Error rate is the most basic — what percentage of requests return errors or malformed responses. Refusal rate measures how often the model declines to answer when it should answer, or answers when it should refuse. Hallucination rate measures factually incorrect outputs. Policy violation rate measures outputs that breach content guidelines. For systems with human review, override rate measures how often reviewers reject or edit the model's outputs. All correctness metrics should improve or stay flat during canary rollout. If error rate increases by 30 percent, the canary fails.

Performance metrics measure latency, throughput, and resource consumption. Median latency and 95th percentile latency reveal whether the new model is slower. Timeout rate measures requests that exceed latency budgets. Token consumption measures inference cost. Memory utilization and GPU utilization measure resource efficiency. Performance regressions are hard trade-offs — sometimes a better model is slower. But dramatic performance regressions destroy user experience regardless of quality gains. A model that is 40 percent more accurate but takes 3 seconds instead of 1 second may not be worth deploying. Canary metrics tell you whether the trade-off is acceptable in practice.

User satisfaction metrics measure how users react to the new model. Explicit satisfaction comes from thumbs up/down ratings, star ratings, or feedback forms. Implicit satisfaction comes from engagement metrics — conversation length, follow-up query rate, task completion rate, retention. If users consistently rate the new model lower, give it fewer thumbs up, or abandon tasks more often, the model may be technically better but experientially worse. Canary rollout catches this gap. A customer support model showed 8 percent higher accuracy in canary rollout but 12 percent lower user satisfaction because it gave more cautious answers that required users to take more steps. The accuracy win did not justify the satisfaction loss. The team rolled back.

Downstream impact metrics measure effects beyond the model's direct output. Escalation rate measures how often users escalate to human support. Retry rate measures how often users rephrase and try again because the first response was unsatisfying. Error report rate measures user-submitted bug reports. Sales conversion rate, signup rate, churn rate — any business metric the model might influence. A content recommendation model showed stable accuracy and latency during canary but a 6 percent drop in clickthrough rate because the new model recommended more niche content that was high-quality but less engaging. Canary caught the downstream impact that accuracy metrics missed.

## Setting Regression Thresholds That Trigger Rollback

Before canary rollout begins, you must define the thresholds that trigger automatic or manual rollback. The thresholds are not arbitrary. They reflect the acceptable risk tolerance for the model's criticality and the statistical confidence you need to distinguish real regressions from noise.

Critical regressions trigger automatic rollback without human approval. Latency exceeding 5 seconds at 95th percentile. Error rate exceeding 2 percent. Policy violation rate exceeding 0.1 percent. Content that triggers legal or safety escalations. These thresholds represent catastrophic failure. The system detects the regression, reverts to the baseline model, pages on-call, and logs the incident. Automatic rollback prevents prolonged user harm when something breaks badly.

Moderate regressions trigger alerts and investigation without automatic rollback. Latency increasing by 300 milliseconds. Refusal rate increasing by 15 percent. User satisfaction dropping by 8 percent. These regressions are serious but not catastrophic. The system alerts on-call, presents comparison data, and waits for human decision. The human reviews evidence, decides whether the regression is acceptable trade-off or deployment blocker, and either continues canary or rolls back manually. Manual rollback preserves human judgment for ambiguous cases.

The thresholds must account for baseline variance. If your baseline model's error rate fluctuates between 0.5 percent and 1.2 percent, a canary error rate of 1.1 percent is not a regression — it is normal variance. The threshold must be calibrated to baseline behavior. Statistical techniques like confidence intervals, control charts, or sequential testing help distinguish signal from noise. A healthcare model used sequential probability ratio testing during canary rollout, defining thresholds as "95 percent confidence that error rate increased by more than 2 percentage points relative to baseline." The statistical rigor prevented false alarms from normal variance while catching real regressions quickly.

A legal contract analysis platform defined three-tier rollback thresholds for their December 2025 canary. Tier 1 — automatic rollback: error rate above 1 percent, latency above 4 seconds at 95th percentile, any output citing a case that does not exist. Tier 2 — manual investigation required: error rate increase of 0.3 percentage points, latency increase of 400 milliseconds, refusal rate increase of 10 percent. Tier 3 — monitor but proceed: error rate increase of 0.1 percentage points, latency increase of 100 milliseconds, refusal rate increase of 5 percent. During the canary, they hit a Tier 2 threshold on day three when refusal rate increased by 12 percent. Investigation revealed the model was refusing contract clauses with complex nested conditionals. The team adjusted prompts, reran canary, and succeeded.

## The Canary Duration and Ramp Strategy

How long should canary rollout last, and how fast should you ramp up the traffic percentage? The answers depend on traffic volume, metric variance, and risk tolerance. Low traffic volumes need longer observation to accumulate enough data for statistical confidence. High variance in baseline metrics needs longer observation to distinguish regression from noise. High-stakes models need slower ramps with more checkpoints.

The initial canary typically starts at 1 to 5 percent of traffic. One percent minimizes blast radius but requires higher traffic volumes to generate sufficient signal. Five percent generates faster signal but exposes more users to potential regressions. A system serving 100,000 requests per day at 5 percent canary generates 5,000 canary requests per day — enough to detect differences as small as 2 percent with reasonable confidence within 48 hours. A system serving 1,000 requests per day at 5 percent generates 50 canary requests per day — too few to detect anything but catastrophic regressions in 48 hours.

Canary duration at each stage depends on reaching statistical confidence for the metrics that matter. If you need to detect a 5 percent change in error rate with 95 percent confidence and your baseline error rate is 1 percent, you need roughly 8,000 requests per variant. At 5 percent canary on 100,000 requests per day, you accumulate 5,000 canary requests per day and reach confidence in two days. At 5 percent canary on 10,000 requests per day, you need 16 days. The math determines the timeline.

Ramp strategy defines how you increase exposure over time. Conservative ramp goes 1 percent, 5 percent, 10 percent, 25 percent, 50 percent, 100 percent with checkpoints at each stage. Moderate ramp goes 5 percent, 15 percent, 50 percent, 100 percent. Aggressive ramp goes 10 percent, 50 percent, 100 percent. The ramp speed should match model criticality and stakeholder risk tolerance. Healthcare models ramp conservatively. Content recommendation models ramp aggressively.

A customer support platform used a four-stage ramp for a new Claude Sonnet 4.5 model. Stage 1: 5 percent for three days, 15,000 canary requests, looking for catastrophic regressions. Stage 2: 15 percent for five days, 75,000 canary requests, looking for moderate regressions and user satisfaction changes. Stage 3: 50 percent for seven days, 350,000 canary requests, looking for subtle downstream impacts. Stage 4: 100 percent after confirming all metrics stable. Total rollout time was 16 days from first canary request to full deployment. Product managers initially complained about the timeline. After rollout succeeded without incident, they adopted the same ramp for all future model changes.

## Handling Regressions Discovered During Canary

When canary rollout detects a regression, you have three options: rollback and fix, rollback and rethink, or proceed with acceptance. The decision depends on regression severity, fix feasibility, and business impact.

Rollback and fix applies when the regression has a clear cause and a feasible solution. The model times out on long documents because you changed the tokenization strategy — roll back, fix tokenization, redeploy. The model refuses a category of legitimate requests because training data was imbalanced — roll back, add balanced examples, retrain, redeploy. The model cites sources incorrectly because a post-processing regex broke — roll back, fix regex, redeploy. These are engineering problems with engineering solutions. Canary caught the regression before it reached most users. You fix it and try again.

Rollback and rethink applies when the regression reveals a fundamental problem with the new model's approach. The model is more accurate but slower because it does more reasoning, and users hate the latency — you cannot fix this by tuning inference parameters. The model refuses less often but produces riskier outputs, and stakeholders reject the risk — you cannot fix this by adjusting refusal thresholds. The model improves quality but costs 3x more to run, and finance rejects the spend — you cannot fix this by optimizing infrastructure. These are not bugs. These are trade-offs where the new model's characteristics do not align with product requirements. Rollback is permanent. You rethink the model strategy before trying again.

Proceed with acceptance applies when the regression is real but acceptable as a trade-off for other gains. The model is 100 milliseconds slower but 15 percent more accurate, and stakeholders accept the latency cost for the quality gain. The model refuses 8 percent more requests but eliminates a category of policy violations, and trust and safety accepts the refusal increase. The model generates longer responses but user satisfaction is stable because users prefer detail over brevity. These are trade-offs where the business explicitly chooses the new model despite the regression. Canary provides the evidence to make informed trade-off decisions.

A document analysis platform detected a regression during canary rollout in January 2026. The new model showed 6 percent higher accuracy but 12 percent higher refusal rate on contracts with redacted sections. Investigation revealed the model was treating redactions as insufficient context and refusing to analyze rather than inferring from unredacted sections. The team debated. Legal preferred the higher refusal rate because inferring from partial data created liability risk. Product preferred the lower refusal rate because users expected the model to work with redacted contracts. The decision was rollback and rethink. The team built a new version that explicitly asked users whether to infer from partial data or refuse, giving users control over the trade-off. The third version succeeded in canary and deployed fully.

## Canary Rollout for Prompt Changes Versus Model Changes

Canary rollout applies to any change that affects model behavior — model version changes, prompt changes, parameter changes, pipeline changes. But prompt changes and model changes have different risk profiles and different canary strategies.

Model changes — switching from GPT-5 to GPT-5.1, from Claude Opus 4 to Opus 4.5, or deploying a new fine-tuned variant — are high-risk changes that require full canary rigor. The model's capabilities, failure modes, and output distributions may differ substantially. You need long canary duration, conservative ramp, and comprehensive metric monitoring. A model change canary might run for two weeks at 5 percent, one week at 15 percent, one week at 50 percent, then full rollout — four weeks total.

Prompt changes — editing system instructions, adding few-shot examples, rewording task descriptions — are medium-risk changes that allow faster canary. The underlying model is unchanged, so catastrophic failures are less likely. But prompts dramatically affect behavior, so canary is still necessary. A prompt change canary might run for two days at 10 percent, three days at 50 percent, then full rollout — five days total. If metrics stay stable, confidence builds faster than with model changes.

Parameter changes — adjusting temperature, top-p, frequency penalty, max tokens — are low-risk changes that allow fast canary. The model and prompt are unchanged. You are tuning generation behavior within known bounds. A parameter change canary might run for one day at 25 percent, one day at 50 percent, then full rollout — two days total. Some teams skip canary for parameter changes entirely, using shadow mode plus rapid rollout.

The risk stratification determines canary duration, but the principle is the same: expose a small percentage first, measure impact, and expand only after confirming safety. The difference is how much evidence you need before expanding. Model changes need weeks of evidence. Prompt changes need days. Parameter changes need hours to days. All need some canary period to catch the unexpected.

## The Human Monitoring Required During Canary

Canary rollout is not set-it-and-forget-it automation. Humans must monitor metrics, review anomalies, read sample outputs, and make judgment calls that automation cannot make. The level of human involvement depends on model criticality and canary stage.

During the first 24 hours of any canary, someone on-call should review metrics every two to four hours. Look for obvious breakage — error spikes, latency spikes, user complaints. Scan the logs for anomalies. Read a sample of canary outputs manually to gut-check whether the model is behaving as expected. Early detection prevents small problems from becoming large problems. A healthcare diagnostic model had an engineer on-call checking metrics every two hours during the first 48 hours of canary. On hour six, the engineer noticed escalation rate had increased by 20 percent. Investigation found the model was escalating cases with incomplete patient histories rather than asking clarifying questions. The team adjusted the prompt, restarted canary, and caught the problem before it affected more than 200 users.

During steady-state canary, automated monitoring takes over with humans reviewing dashboards daily. The dashboard shows key metrics trended over time, comparison to baseline, regression thresholds, and top anomalies. If metrics stay stable and anomalies are rare, daily review is sufficient. If metrics start drifting or anomalies increase, review frequency increases. A legal research platform set up a daily canary review meeting during the 50 percent ramp stage. Three engineers reviewed the dashboard, discussed any concerning patterns, and made go/no-go decisions for the next ramp stage. The discipline of daily review prevented rationalizing away small regressions that could compound into large failures.

User feedback during canary is signal, not noise. Users report bugs, express confusion, or complain about changes. Every piece of feedback from canary users should be triaged. Some feedback reveals regressions the metrics missed. Some feedback is subjective preference that does not indicate a problem. Some feedback is feature requests unrelated to the model change. But you cannot distinguish signal from noise without reading the feedback. A content moderation model received 40 pieces of user feedback during canary rollout. Thirty-eight were unrelated to the model change. Two reported that the model was flagging sarcasm as hate speech. Manual review confirmed the regression. The metrics had not caught it because the volume was too low. User feedback caught it. The team rolled back.

The next subchapter covers online A/B testing, where both models run at steady traffic splits and you measure statistical differences in business metrics.

