# 10.8 — Experiment Analysis: Detecting Real Differences from Noise

Every A/B test faces the same fundamental question: is the difference you observe between variants real, or is it random noise that will disappear with more data? A new model shows 4 percent higher user satisfaction than the baseline. Is that a genuine improvement worth deploying, or statistical noise from sampling variance? Experiment analysis is the discipline of answering this question with mathematical rigor instead of intuition. Intuition fails. Math does not lie.

In September 2025, a financial planning platform ran an A/B test comparing GPT-5 to a new fine-tuned GPT-5.1 variant. After three days, the new variant showed 6 percent higher task completion rate. The product team declared victory and pushed for full rollout. The data team ran statistical tests and found that with the sample size collected, the observed difference had a 32 percent probability of being pure noise. The test needed to run for 10 more days to reach statistical confidence. Product pushed back, arguing that 6 percent improvement was obviously meaningful. Data held firm. The test ran for 13 total days. Final result: 2 percent improvement with 95 percent confidence — real but much smaller than initial observation suggested. If they had deployed after day three based on intuition, they would have expected 6 percent improvement and gotten 2 percent, damaging stakeholder trust in future experiments.

Experiment analysis prevents premature conclusions that waste engineering effort deploying changes that do not actually work. It also prevents failing to deploy changes that do work because early noise looked like failure. The discipline separates signal from noise using statistical methods that account for variance, sample size, and the multiple ways humans fool themselves with data.

## The Null Hypothesis and Statistical Significance

Experiment analysis begins with the null hypothesis: assume there is no real difference between variants. Any observed difference is due to random sampling. Under this assumption, calculate the probability of observing a difference as large as you actually observed. If that probability is very low — conventionally less than 5 percent — reject the null hypothesis and conclude a real difference exists. If the probability is high, you cannot conclude anything. The variants might be different, but your data does not prove it.

The 5 percent threshold means you accept a 5 percent false positive rate. One experiment in twenty, you will conclude a difference exists when it does not. This is called a Type I error. The threshold is conventional, not fundamental. High-stakes decisions use stricter thresholds like 1 percent. Low-stakes decisions use looser thresholds like 10 percent. The choice trades false positive risk against statistical power — stricter thresholds reduce false positives but increase false negatives.

Statistical significance does not mean practical significance. A difference can be statistically significant but too small to matter. An A/B test with 100,000 users per variant will detect a 0.5 percent improvement in task completion with high confidence. But is 0.5 percent improvement worth the engineering cost to deploy and maintain a new model? That is a business decision, not a statistical one. Always report both statistical significance and effect size magnitude.

The p-value quantifies significance. It is the probability of observing your data or more extreme data if the null hypothesis is true. A p-value of 0.03 means: if the variants were truly identical, there is a 3 percent chance of observing a difference this large due to random sampling. Since 3 percent is below the conventional 5 percent threshold, you reject the null hypothesis. A p-value of 0.15 means 15 percent chance under the null hypothesis — not rare enough to conclude a real difference.

A customer support platform ran an A/B test comparing two prompt variants. Variant A showed 73.2 percent task completion. Variant B showed 75.8 percent task completion. The difference was 2.6 percentage points. They ran a chi-square test and obtained p-value 0.04. Conclusion: the difference is statistically significant at the 5 percent level. Effect size is 2.6 percentage points. Stakeholders decided 2.6 percentage points justified deploying variant B.

## Effect Size, Confidence Intervals, and Practical Significance

P-values tell you whether a difference exists. Confidence intervals tell you how large the difference is and how certain you are about that size. A 95 percent confidence interval for a difference means: if you repeated the experiment many times, 95 percent of the resulting intervals would contain the true difference. The interval quantifies uncertainty.

Effect size is the magnitude of the difference you care about. For conversion rate, effect size is percentage point difference. For latency, effect size is milliseconds difference. For user satisfaction score, effect size is point difference on the rating scale. Report effect size alongside significance. "Variant B improves task completion by 2.6 percentage points, 95 percent confidence interval from 0.2 to 5.0 percentage points, p equals 0.04." The interval tells you the difference is real and probably between 0.2 and 5.0 points, with 2.6 as best estimate.

Wide confidence intervals indicate high uncertainty. A confidence interval from 0.1 to 8.3 percentage points means the true effect could be tiny or large — you do not know which. Narrow intervals indicate low uncertainty. An interval from 2.3 to 2.9 percentage points means you are confident the effect is around 2.6 points. Uncertainty shrinks with more data. If your interval is too wide for decision-making, collect more data.

Minimum detectable effect is the smallest effect size you can reliably detect with your sample size. Before running an experiment, calculate: to detect a 3 percent improvement with 80 percent power and 95 percent confidence, how many users do you need per variant? Power analysis tools like G*Power or online calculators provide the answer. If you need 8,000 users per variant and you only have 1,000, the experiment is underpowered — you will not detect a 3 percent improvement even if it exists.

A legal research platform ran an A/B test measuring citation accuracy. Baseline showed 94.2 percent accuracy. New variant showed 95.1 percent accuracy. Effect size: 0.9 percentage points. They computed a 95 percent confidence interval: 0.1 to 1.7 percentage points. The interval excludes zero, confirming statistical significance. But stakeholders questioned whether 0.9 percentage points justified the deployment cost. Data team presented business impact: 0.9 percentage points means 45 fewer incorrect citations per 5,000 queries, preventing approximately three customer complaints per month. Quantifying practical impact helped stakeholders decide deployment was worthwhile.

## Multiple Comparisons and the False Discovery Problem

When you test many metrics or many variants simultaneously, your false positive rate multiplies. Test 20 metrics at the 5 percent significance level, and you expect one false positive even if all variants are identical. Test 10 variants pairwise — 45 comparisons — and false positives become almost guaranteed. The multiple comparisons problem is the statistical tax for testing everything.

Multiple comparison correction adjusts significance thresholds to control overall false positive rate. Bonferroni correction divides your significance threshold by the number of comparisons. Testing 20 metrics with 5 percent per-test threshold becomes 0.25 percent per-test threshold to maintain 5 percent overall false positive rate. Bonferroni is conservative — it reduces false positives but increases false negatives. You miss real effects because the corrected threshold is very strict.

Benjamini-Hochberg procedure controls false discovery rate instead of family-wise error rate. It allows some false positives but guarantees they represent a controlled proportion of all discoveries. If you test 20 metrics and find five significant results, Benjamini-Hochberg ensures at most one is a false positive. The procedure is less conservative than Bonferroni and more practical for exploratory analysis.

The best solution is preregistration: decide which metrics matter before running the experiment. Designate one primary metric that determines success or failure. Designate a few secondary metrics that must not regress. Test only those metrics with standard thresholds. Do not mine the data for surprising findings and then claim they were significant. Exploratory analysis is fine for generating hypotheses. It is not valid for confirming hypotheses.

A healthcare diagnostics model ran an A/B test with one primary metric — false negative rate — and three secondary metrics — false positive rate, escalation rate, and latency. They preregistered these metrics and significance thresholds before starting the test. During analysis, they also noticed that response length differed between variants. They reported the response length difference as exploratory finding, not as statistically significant, because it was not preregistered. The discipline prevented claiming significance for a metric chosen post-hoc because it looked interesting.

## Sequential Testing and Early Stopping

Traditional A/B testing requires deciding sample size upfront and running until you reach it. Sequential testing allows checking data continuously and stopping as soon as sufficient evidence accumulates. The advantage is faster decisions. The disadvantage is complexity — naive peeking inflates false positive rates unless you use proper sequential methods.

Naive peeking means checking p-values repeatedly and stopping as soon as p is below 0.05. This inflates false positive rate to roughly 30 percent instead of 5 percent because you get many chances to find a random fluctuation that looks significant. If you peek 10 times during an experiment, one of those peeks will likely show p less than 0.05 by chance even if no real difference exists.

Sequential probability ratio test corrects for continuous monitoring. It computes a likelihood ratio at each data point and stops when the ratio exceeds a threshold corresponding to your desired error rates. SPRT is mathematically rigorous and stops as soon as sufficient evidence is available. The average sample size to reach a decision is smaller than fixed-sample testing, accelerating experiments.

Bayesian sequential testing computes posterior probability that one variant is better than the other. Stop when posterior probability exceeds a threshold — typically 95 percent for superiority or 5 percent for futility. Bayesian methods provide intuitive interpretation: "we are 97 percent confident variant B is better than variant A." They naturally handle continuous monitoring without inflated error rates.

Many teams use informal stopping rules: check data daily, stop if the p-value has been below 0.05 for three consecutive days. This is better than naive peeking but not as rigorous as SPRT or Bayesian methods. The informal rules add conservatism by requiring sustained significance, reducing false positives from transient noise.

A content moderation platform used Bayesian sequential testing for prompt A/B tests. They computed posterior probability daily. If probability that variant B was better exceeded 95 percent, they declared success and ramped to 100 percent. If probability that variant B was worse exceeded 95 percent, they declared failure and disabled variant B. Otherwise they continued collecting data. The Bayesian approach let them stop most tests within four days instead of running fixed two-week durations.

## Variance Reduction Techniques for Faster Results

Experiments reach statistical confidence faster when measurement variance is low. Variance reduction techniques decrease noise, allowing smaller sample sizes or shorter durations to detect the same effect size. The techniques include stratification, covariate adjustment, and variance-weighted metrics.

Stratification divides users into segments with similar baseline behavior and analyzes each segment separately. Segment by user tenure, usage frequency, or baseline engagement level. Compute treatment effect per segment, then aggregate. Stratification removes between-segment variance, which does not reflect treatment impact, focusing analysis on within-segment variance, which does.

Covariate adjustment uses user attributes or baseline behavior as covariates in regression models. Regress outcome metric on variant assignment plus covariates like user tenure, past activity level, or demographics. The regression isolates the treatment effect after accounting for covariate influence. Covariate adjustment can reduce variance by 30 to 60 percent in high-heterogeneity populations.

Variance-weighted metrics assign more weight to observations with lower variance. Users with stable, predictable behavior contribute more to the analysis than users with erratic behavior. Variance weighting improves statistical power without collecting more data.

CUPED is a variance reduction technique that subtracts pre-experiment baseline from post-experiment outcome. If user A had 80 percent task completion before the experiment and 85 percent during, their adjusted outcome is 85 minus 80 equals 5 percentage points improvement. If user B had 60 percent before and 65 percent during, their adjusted outcome is also 5 percentage points. CUPED removes user-level variance that existed before the experiment, leaving only variance due to treatment. CUPED typically reduces variance by 30 to 50 percent, allowing experiments to conclude in half the time.

A document summarization platform implemented CUPED for A/B tests in late 2025. They measured user satisfaction during the week before each experiment as baseline. During the experiment, they computed adjusted satisfaction as post-experiment satisfaction minus pre-experiment baseline per user. Analyzing adjusted satisfaction instead of raw satisfaction reduced variance by 42 percent. Experiments that previously required 10,000 users now required 5,800 users, accelerating test timelines by 40 percent.

## Detecting Interaction Effects and Subgroup Differences

Aggregate results hide subgroup heterogeneity. A treatment that improves outcomes for 70 percent of users but harms 30 percent may show neutral or slightly positive aggregate effect. Subgroup analysis detects these hidden patterns.

Subgroup analysis splits users by attributes — new versus returning, free versus paid, mobile versus desktop — and analyzes treatment effect per subgroup. If the new model improves task completion for paid users by 8 percent but degrades it for free users by 3 percent, aggregate analysis shows 5 percent improvement, missing the free user regression. Subgroup analysis catches it.

Interaction effects occur when treatment impact depends on another variable. Maybe the new model works better for long queries but worse for short queries. Maybe it improves outcomes for English but not Spanish. Detect interactions by including interaction terms in regression models or by comparing treatment effects across subgroups with statistical tests.

The cost of subgroup analysis is multiple comparisons. Testing 10 subgroups generates 10 tests, requiring multiple comparison correction. Preregister important subgroups to maintain statistical validity. Use exploratory subgroup analysis to generate hypotheses for future confirmatory experiments.

Subgroup differences sometimes reveal opportunities for personalization. If the new model is better for segment A but worse for segment B, deploy it only to segment A. If it is better for query type X but worse for query type Y, route by query type. Subgroup analysis turns one-size-fits-all deployments into tailored solutions.

A financial advisory platform analyzed an A/B test by user segment and discovered the new model improved outcomes for users with portfolios above 100,000 dollars but degraded outcomes for users with portfolios below 50,000 dollars. The model had been trained on advice for high-net-worth individuals and generalized poorly to smaller portfolios. They deployed the model only to high-net-worth users and retrained with more representative data before expanding to all users. Subgroup analysis prevented deploying a model that would have harmed their largest user segment.

## Common Pitfalls That Invalidate Experiment Results

Well-designed experiments can still produce invalid results if implementation or analysis has subtle flaws. Sample ratio mismatch, carryover effects, novelty effects, and selection bias are common pitfalls that teams miss.

Sample ratio mismatch occurs when the observed split between variants differs from the intended split. You designed a 50/50 test but observed 48/52 or 45/55. Small deviations are noise. Large deviations indicate a bug in randomization logic, bot traffic, or systematic dropout. Investigate any mismatch exceeding 2 percentage points. A mismatch of 40/60 invalidates the experiment because the variants are not receiving comparable traffic.

Carryover effects occur when users assigned to one variant retain memory or behavior from a previous variant. If a user saw model A last week and now sees model B, their behavior with model B may be influenced by expectations set by model A. Carryover is especially problematic for within-user experiments where the same user sees both variants over time. Between-user experiments avoid carryover by ensuring each user sees only one variant.

Novelty effects occur when users react to newness rather than quality. The new model gets higher satisfaction in the first three days because users find change interesting, but satisfaction regresses to baseline after two weeks. Novelty effects bias short-term experiments. Run experiments for at least one week, preferably two, to let novelty wear off.

Selection bias occurs when users assigned to variants differ systematically. Randomization should make variants comparable. If variant A gets more engaged users by chance, variant A will show better metrics regardless of treatment quality. Check for balance by comparing baseline characteristics between variants before the experiment starts. If variants differ on key covariates, use stratification or covariate adjustment to correct.

A customer support platform detected a sample ratio mismatch in November 2025. The intended split was 50/50. Observed split was 42/58. Investigation found that mobile app users were disproportionately assigned to variant B due to a bug in the mobile SDK's randomization logic. The team fixed the bug, restarted the experiment, and confirmed 50/50 split. Without detecting the mismatch, they would have compared 42 percent of users on variant A to 58 percent on variant B, producing biased results.

## Communicating Results to Non-Technical Stakeholders

Experiment results must be communicated clearly to stakeholders who may not understand p-values, confidence intervals, or statistical power. Effective communication focuses on practical significance, visualizes uncertainty, and avoids jargon.

Lead with the decision: "We should deploy variant B because it improves task completion by 3 percentage points with high confidence." Follow with evidence: "We tested 18,000 users per variant for two weeks. Task completion was 72 percent for variant A and 75 percent for variant B. The 3 percentage point difference is statistically significant with 95 percent confidence. The confidence interval ranges from 1.8 to 4.2 percentage points, meaning the true improvement is very likely in that range."

Visualize the results. Show bar charts with confidence intervals. Show time series of metrics during the experiment. Show distributions of outcomes per variant. Visuals communicate faster than tables of numbers. Confidence intervals should always appear on charts to show uncertainty.

Avoid jargon. Do not say "p equals 0.03." Say "the probability that this result is due to chance is 3 percent." Do not say "null hypothesis." Say "we assumed the variants were identical and calculated how surprising our data would be under that assumption." Translate statistical concepts into plain language.

Quantify business impact. A 3 percentage point improvement in task completion means 1,500 more successful tasks per week at current traffic, preventing 300 escalations to human support, saving 60 hours of support labor per week, worth approximately 4,000 dollars weekly. Translating statistical results into business metrics helps stakeholders make informed decisions.

A legal research platform presented A/B test results to executive stakeholders using a three-slide deck. Slide 1: decision and key result in plain language. Slide 2: chart showing citation accuracy per variant with confidence intervals. Slide 3: business impact quantification — fewer incorrect citations, fewer customer complaints, higher retention. The deck communicated statistical findings without requiring stakeholders to understand p-values or power analysis. Executives approved deployment in 15 minutes.

## Post-Experiment Validation and Long-Term Monitoring

An experiment proves a treatment works in the test period. It does not prove the treatment will work forever. Post-experiment validation monitors whether the effect persists after full rollout. Long-term effects sometimes differ from short-term effects due to user adaptation, seasonal changes, or population shifts.

Holdout groups extend the experiment indefinitely. Keep 5 percent of users on the baseline variant even after deploying the new variant to 95 percent. Monitor metrics for both groups over weeks or months. If the treatment effect persists, confidence increases. If it fades, investigate why. Holdouts answer the question: does this improvement last?

Long-term monitoring tracks metrics after full rollout to detect drift. If the new model showed 3 percent improvement during the experiment but metrics return to baseline three months later, something changed. User behavior adapted. Data distribution shifted. Model performance degraded. Long-term monitoring catches these changes.

Replication in new contexts validates external validity. If a treatment worked in the US, does it work in EU? If it worked in summer, does it work in winter? If it worked for query type X, does it work for query type Y? Replication experiments answer these questions and prevent overgeneralizing from one context to all contexts.

A healthcare diagnostics model deployed a new variant after a successful A/B test in October 2025. They maintained a 5 percent holdout group on the baseline variant for six months. Monitoring showed the treatment effect persisted through December but started fading in January. Investigation found that physician behavior had adapted — they learned the new variant's strengths and started asking different types of questions, shifting the distribution in ways the model handled less well. The team retrained the model with the new query distribution, ran a follow-up experiment, and confirmed sustained improvement.

The next subchapter covers multi-variant testing, the methods for comparing more than two options simultaneously, and the statistical and operational challenges that arise when testing three, five, or ten variants at once.

