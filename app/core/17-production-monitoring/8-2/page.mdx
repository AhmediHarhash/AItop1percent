# 8.2 — Detection Latency: How Fast Can You Know Something Is Wrong

The finance team discovered the problem on Monday morning. The AI-powered expense categorization system had been miscategorizing transactions since Thursday afternoon. Four business days. Thousands of transactions. The problem was not subtle — the model was tagging personal expenses as business travel and business meals as office supplies. But no alert had fired. No metric had crossed a threshold. The engineering team had dashboards showing latency, error rates, throughput, and availability. All green. The model was technically healthy. It was just wrong. Detection latency — the time between when the failure began and when the team knew about it — was 96 hours. By the time they started fixing it, the damage was already done.

For traditional software, detection latency is measured in seconds or minutes. A service goes down, an alert fires, someone gets paged. For AI systems, detection latency is often measured in hours or days. The failure is ambiguous. The signals are weak. The metrics you monitor are not the metrics that matter. You need a different approach to detection — one that catches semantic failures as fast as infrastructure failures.

## The Three Detection Modalities

AI incidents can be detected through three channels: automated metrics, user feedback, and proactive review. Each has different latency characteristics. The goal is not to pick one — it is to layer all three so that the fastest available signal triggers response.

Automated metric-based detection fires when dashboard thresholds are breached. If you monitor eval pass rates, policy violation rates, or output quality scores, you can detect incidents within minutes. A travel booking assistant ran continuous eval on a sample of production outputs. In February 2026, eval pass rates dropped from 94 percent to 76 percent over a fifteen-minute window. The alert fired. The on-call engineer investigated and found that a dependency service was returning stale hotel pricing data. The RAG pipeline was retrieving outdated information, causing the model to generate incorrect recommendations. The incident was caught in seventeen minutes. Automated detection is fast, but only for failures that manifest in metrics you are already monitoring.

User feedback detection relies on complaints, support tickets, or explicit error reports. This is slower than automated detection but catches problems that metrics miss. A legal document assistant had an issue in July 2025 where the model started using British spelling in contracts that required American spelling. The problem was subtle. Eval scores were unaffected because the outputs were grammatically correct and semantically accurate. But clients noticed. The first support ticket came in three hours after the issue started. The support team escalated to product. Product escalated to engineering. By the time the engineering team started investigating, the incident was eight hours old. User feedback detection is slower, but it catches semantic problems that automated metrics miss entirely.

Proactive review detection happens when humans manually inspect outputs looking for problems. An insurance claims processing system ran daily audits where domain experts reviewed a random sample of 200 AI-generated claim summaries. In November 2025, an auditor noticed that the model had started omitting specific types of medical claims from summaries. The problem had existed for eleven days before the audit caught it. Proactive review has the highest detection latency, but it is often the only way to catch subtle domain-specific errors that neither metrics nor users flag immediately.

## The Detection Latency Equation

Detection latency is not just about how fast your monitoring runs. It is about the compound delay across four stages: signal generation, signal aggregation, threshold evaluation, and alert delivery. If each stage adds latency, the total can become substantial even when individual components are fast.

Signal generation is how quickly raw system events become analyzable data. If you run eval on every production request, signal generation latency is near zero. If you run eval on a batch of requests every hour, signal generation latency is up to sixty minutes. A healthcare documentation platform ran eval every four hours to save costs. When a model degradation occurred, the earliest it could be detected was four hours after the first bad output. The engineering team later moved to continuous eval on 5 percent of traffic, reducing signal generation latency to five minutes on average. The cost increase was 8 percent. The detection latency improvement was 48x.

Signal aggregation is how quickly individual data points become trends. If you alert on single failed requests, you get high false positive rates. If you wait for statistical significance, you add latency. A customer service chatbot required five consecutive eval failures before alerting. The goal was to avoid false alarms from transient issues. But when a real degradation occurred, the system waited for five failures. At a request rate of 120 per minute, five failures took three minutes under normal conditions. But during the degradation, request rates dropped because users were retrying failed sessions. Five failures took eleven minutes. The aggregation logic that was designed to reduce noise ended up adding detection latency when it mattered most.

Threshold evaluation is how often aggregated metrics are checked against alert conditions. If you evaluate thresholds every minute, you add up to one minute of latency. If you evaluate every ten minutes, you add up to ten minutes. A document analysis platform checked thresholds every five minutes. When a latency cascade began, the first threshold breach occurred 90 seconds into the incident. But the next threshold evaluation was not scheduled for another three minutes. The alert fired four and a half minutes after the incident started. Continuous threshold evaluation — checking conditions as soon as new data arrives — would have fired the alert in under two minutes.

Alert delivery is how quickly humans learn that something is wrong. If alerts go to Slack and someone is watching, delivery latency is seconds. If alerts go to email and no one checks for an hour, delivery latency is an hour. A content moderation system had email-based alerting. An incident occurred at 6:00 PM on a Friday. The email was sent immediately. The on-call engineer was in a meeting and did not see the email until 9:30 PM. Detection latency was three and a half hours, not because monitoring was slow, but because alert delivery did not reach a human. Switching to PagerDuty with immediate notification reduced delivery latency to under one minute.

## The False Negative Problem

The fastest detection system in the world does not matter if it misses the incident entirely. False negatives — incidents that occur but do not trigger alerts — are the silent killer of detection latency. The incident is happening. The system is failing. But your monitoring says everything is fine.

A financial advisory platform had sophisticated monitoring. They tracked latency, error rates, model confidence scores, and eval pass rates. But they did not monitor retrieval precision. In October 2025, the RAG system started retrieving less relevant documents due to a vector database configuration change. The model still generated outputs. Confidence scores were still high. Eval pass rates were unaffected because the eval was measuring fluency, not factual accuracy. But outputs were increasingly based on irrelevant context. Users noticed. The engineering team did not. Detection latency was four days because the failure mode was not covered by any metric.

False negatives occur when your metrics do not align with actual failure modes. If you monitor latency but not quality, you miss silent degradation. If you monitor overall eval pass rates but not segmented pass rates, you miss demographic disparities. If you monitor average confidence but not confidence calibration, you miss overconfident hallucinations. Every metric you do not monitor is a potential blind spot. The question is not whether your metrics are good — it is whether your metrics cover all the ways the system can fail.

The solution is comprehensive coverage. You need metrics for output quality, safety, retrieval precision, confidence calibration, demographic fairness, latency, error rates, and cost. Each metric should have thresholds tuned to fire before user impact becomes severe. If a metric does not align with a known failure mode, either add a metric that does or remove the metric to reduce noise. The goal is to make false negatives rare — to ensure that every incident class in your taxonomy has at least one metric that detects it.

## The User Feedback Signal

Users detect incidents faster than you do. They see every output. They experience every failure. The question is how quickly their feedback reaches you and how quickly you recognize it as an incident signal.

A customer service chatbot in March 2026 started generating overly formal responses after a model update. The outputs were technically correct. Eval scores were stable. But users found the tone off-putting. Satisfaction scores dropped from 4.1 to 3.6 over two days. The product team saw the scores but assumed it was normal variance. The support team saw complaints but did not escalate. The engineering team had no visibility into user sentiment. Detection latency was six days because the signal existed but was not routed to the people who could act on it.

User feedback becomes a detection signal only if it is monitored continuously and correlated with model changes. If you track support ticket volume, sentiment, and keyword frequency in real time, you can detect incidents as soon as users notice them. A travel booking assistant had a dashboard showing support ticket volume by category. When ticket volume for incorrect pricing spiked by 40 percent in one hour, the support team immediately escalated to engineering. The engineering team investigated and found that the RAG pipeline was retrieving cached pricing data that had expired. The incident was caught in 75 minutes — much faster than it would have been detected through eval alone, because the eval did not check pricing freshness.

The challenge is distinguishing signal from noise. Users complain for many reasons. Most complaints are not incidents. But when complaint volume spikes, when new error patterns emerge, or when sentiment drops suddenly, those are incident signals. You need baselines. If average support ticket volume is 40 per hour and it hits 70 per hour, something is wrong. If sentiment is usually 4.0 and it drops to 3.4 in two hours, something changed. Automated anomaly detection on user feedback metrics turns subjective complaints into objective incident signals.

## The Canary Deployment Advantage

Canary deployments reduce detection latency by isolating changes to a small percentage of traffic. If a new model fails, it fails on 5 percent of requests instead of 100 percent. The incident is smaller and easier to detect.

A document summarization platform deployed model updates to 5 percent of traffic for thirty minutes before full rollout. In January 2026, they deployed a new version of Claude Opus 4.5 with updated prompts. Within eight minutes, eval pass rates on canary traffic dropped from 92 percent to 81 percent. The engineering team immediately rolled back the canary. Total affected requests: 1,200. If they had deployed to 100 percent of traffic, the same issue would have affected 15,000 requests before detection. The canary reduced both impact and detection latency because the team was watching canary metrics closely during the deployment window.

Canary deployments work best when canary traffic has the same monitoring as production traffic. You need separate metrics for canary requests. You need automated comparison between canary and baseline performance. You need alert thresholds tuned tightly enough to catch regressions quickly but loosely enough to avoid false positives from small sample variance. A content moderation platform ran 10 percent canary deployments with automated comparison. If canary policy violation rates diverged from baseline by more than 3 percentage points for more than five minutes, the deployment automatically rolled back. This caught four incidents in 2025 before they reached full production.

## The Role of Synthetic Traffic

Synthetic traffic — automated test requests sent continuously to production systems — provides a detection signal independent of user behavior. If real traffic is low, synthetic traffic still runs, detecting incidents even when users are not active.

A legal research platform sent 200 synthetic queries per hour to production. Each query had a known correct answer. In September 2025, at 2:00 AM, synthetic traffic started failing. Eval pass rates dropped from 96 percent to 68 percent. Real user traffic was minimal at that hour. Without synthetic traffic, the incident would not have been detected until morning when user traffic resumed. Instead, the alert fired at 2:14 AM. The on-call engineer investigated and found that a dependency service had deployed a breaking API change. The issue was fixed by 3:30 AM, before most users woke up.

Synthetic traffic reduces detection latency for low-traffic periods and catches issues that only manifest under specific conditions. The synthetic requests should cover edge cases, rare query types, and policy boundaries. If your real traffic is 95 percent simple queries and 5 percent complex queries, your synthetic traffic should be 50 percent complex queries, because those are where failures are most likely. A customer support chatbot sent synthetic adversarial queries — attempts to extract confidential information, inject prompts, and bypass safety filters — every ten minutes. These queries never appeared in real traffic. But they caught three incidents in 2025 where safety controls had degraded due to model updates.

## The Detection Latency Budget

Detection latency is not free to reduce. Faster detection requires more compute, more complex pipelines, more alerting infrastructure. The question is how much detection latency you can afford.

For safety-critical systems, detection latency should be measured in minutes. If your model is processing medical diagnoses, legal advice, or financial transactions, a four-hour detection latency means four hours of incorrect outputs with real consequences. You need continuous eval, real-time thresholds, and immediate alerting. A healthcare prior authorization system ran eval on every production request. The compute cost was 18 percent of total inference cost. But detection latency for quality issues was under five minutes. The team considered the cost justified because undetected failures led to denied coverage and patient harm.

For non-critical systems, detection latency can be measured in hours. If your model is generating marketing copy or internal summaries, a four-hour detection latency is annoying but not dangerous. You can run eval on batches, check thresholds less frequently, and save compute costs. A content summarization tool for internal use ran eval on 10 percent of traffic every two hours. Detection latency averaged three hours. The engineering team accepted this because the impact of undetected failures was low and the cost savings were substantial.

The key is to align detection latency with failure impact. If a failure causes user harm, financial loss, or regulatory exposure, you need fast detection. If a failure causes mild inconvenience, slower detection is acceptable. The mistake is applying the same detection latency budget to every system. High-stakes systems need subsecond to minute-scale detection. Low-stakes systems can tolerate hour-scale detection.

## The Compound Effect of Latency Stages

Detection latency is not dominated by a single slow stage — it is the sum of all stages. A system with five-minute signal generation, five-minute aggregation, five-minute threshold evaluation, and five-minute alert delivery has twenty-minute detection latency. Optimizing one stage does not matter if the others remain slow.

A document analysis platform reduced signal generation latency from ten minutes to one minute by moving from batch eval to streaming eval. But threshold evaluation still ran every ten minutes. Total detection latency dropped from thirty minutes to twenty minutes — an improvement, but not the 10x improvement they expected. They later moved to continuous threshold evaluation and reduced total latency to six minutes. The lesson: you have to optimize the entire detection pipeline, not just the slowest component.

The detection latency budget should be allocated across stages based on where reduction is cheapest. If signal generation is expensive to speed up but threshold evaluation is cheap, optimize threshold evaluation first. If alert delivery is the bottleneck, fix alerting before optimizing metrics. A content moderation platform had one-minute signal generation, one-minute aggregation, and ten-minute alert delivery because alerts went to email. They switched to PagerDuty and reduced total detection latency from twelve minutes to three minutes with no changes to metrics or thresholds.

## The Incident That Was Never Detected

Some incidents are never detected by monitoring. They are discovered through user escalations, audits, or post-mortems for unrelated incidents. These represent the worst-case detection latency — effectively infinite.

A contract analysis platform in August 2025 had an incident where the model started generating biased language in diversity clauses. The outputs were legally correct but contained subtle phrasing that favored specific demographic groups. The engineering team had no metric for bias in legal language. User feedback was minimal because clients did not notice until their own legal reviews. The incident was discovered four months later during an unrelated audit. By then, the model had generated 14,000 contracts with biased language. The platform had to offer free contract review services to affected clients and implement demographic fairness eval that should have existed from the start.

These undetected incidents are the strongest argument for comprehensive metric coverage. Every known failure mode should have a metric. Every metric should have a baseline and a threshold. Every threshold should fire alerts that reach humans. If an incident happens and your system does not detect it, your monitoring has a gap. Close that gap before the next incident.

Once you can detect incidents quickly, the next challenge is deciding how serious they are. Not every incident requires the same response. Severity classification determines who gets paged, how fast you respond, and when you communicate with users.

