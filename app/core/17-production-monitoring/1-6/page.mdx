# 1.6 â€” What to Measure: The Core Signal Categories for AI Systems

AI systems generate thousands of potential metrics. Latency, token count, cache hit rate, model version, retrieval precision, safety filter confidence, user feedback scores, retry attempts, abandonment rate, cost per request, error distribution, context length, tool invocation success rate, prompt template version, vector search recall. Most teams measure too much or too little. Too much and you drown in noise, chasing false alarms and burning alert fatigue. Too little and you miss the signal that would have told you the system was degrading three days before users started complaining.

The answer is not to measure everything. The answer is to measure the right things and organize them by purpose. There are five core signal categories in production AI systems: infrastructure signals, quality signals, safety signals, cost signals, and behavioral signals. Each category serves a different purpose, triggers different response patterns, and requires different collection infrastructure. You need all five. Skip one and you have a blind spot.

## Infrastructure Signals: Is the System Functioning?

Infrastructure signals measure whether the system is up and responsive. This is the foundational layer. If your API is returning 500 errors or timing out, nothing else matters. Users cannot experience quality if they cannot get a response.

The core infrastructure metrics are request rate, error rate, latency distribution, and availability. Request rate tells you volume and load patterns. You need to know normal traffic to detect anomalies. A 10x spike in traffic might be organic growth, a bot attack, or a retry storm from a misconfigured client. You cannot tell the difference without baseline data. Error rate tells you what percentage of requests are failing. An error rate above 1 percent in a production system is a crisis. An error rate above 5 percent means something is fundamentally broken. Latency distribution tells you how fast the system responds. Median latency is useful. P95 and P99 latency are more useful because they show tail behavior. A system with 200ms median and 8-second P99 has a serious problem that median alone would hide. Availability is uptime: the percentage of time the system is reachable and functional.

But AI systems have additional infrastructure signals that traditional web services do not. You need to measure model endpoint health independently from application health. If you route requests to three model providers and one is timing out, you need to detect that and shift traffic. You need to measure inference queue depth if you are batching requests. A growing queue means you cannot keep up with load. You need to measure vector database query latency separately from model inference latency. Slow retrieval is a different problem from slow generation and requires different fixes.

You also need to measure dependency health. Most AI systems call multiple external services: model APIs, vector databases, SQL databases, caching layers, embedding services, tool APIs. Each dependency can fail independently. If your system calls an external API for real-time data and that API starts returning stale responses, your infrastructure metrics will look fine, but your outputs will be wrong. You need to track dependency response times, error rates, and staleness indicators.

Infrastructure signals are high-volume and low-latency. You are measuring every request. You need sub-second granularity to detect spikes and catch transient issues. You need to store these metrics efficiently and query them quickly. Most teams use time-series databases like Prometheus or managed services like Datadog. The metrics are numeric and aggregate well. You do not need to store raw request data to track error rates and latency percentiles.

## Quality Signals: Are the Outputs Good?

Quality signals measure whether the system is producing correct, relevant, useful outputs. This is the layer most teams underinvest in because it is harder to measure than infrastructure. Latency is objective. Correctness is contextual. But contextual does not mean unmeasurable. You can measure quality. You just need proxies and sampling strategies.

The core quality metrics depend on your system architecture. For RAG systems, you measure groundedness, relevance, and answer completeness. Groundedness is whether the response is supported by the retrieved context. Relevance is whether the retrieved context is actually related to the query. Answer completeness is whether the response fully addresses the question or leaves gaps. For agentic systems, you measure task success rate, tool selection accuracy, and action correctness. Task success rate is whether the agent completed the user's goal. Tool selection accuracy is whether the agent picked the right tool for each step. Action correctness is whether each tool invocation used the right parameters.

For conversational systems, you measure coherence, consistency, and context retention. Coherence is whether the response makes sense given the conversation history. Consistency is whether the system contradicts itself across turns. Context retention is whether the system remembers relevant information from earlier in the conversation. For summarization systems, you measure factual accuracy, coverage, and conciseness. Factual accuracy is whether the summary contains no fabricated information. Coverage is whether the summary captures the key points. Conciseness is whether the summary is appropriately short given the content.

You cannot run expensive quality checks on every request. A human review of every output would cost more than the system generates in value. You need automated proxies and sampling strategies. Automated proxies are fast checks that correlate with quality. For groundedness, you might use an LLM-as-judge prompt that compares the response to the retrieved chunks and outputs a binary supported or unsupported label. For relevance, you might measure semantic similarity between the query and the top retrieved chunks. For coherence, you might use a fine-tuned classifier that detects nonsense responses. These proxies are imperfect but cheap and scalable.

Sampling strategies let you run expensive checks on a representative subset of traffic. You might route 1 percent of production requests to a human review queue. You might prioritize high-stakes interactions: requests from enterprise customers, requests that triggered safety filters, requests with negative user feedback. You might stratify your sample to ensure coverage of different query types, different user segments, and different time windows. The goal is to get enough signal to detect quality degradation without evaluating every request.

Quality signals are lower-volume than infrastructure signals but higher-dimensionality. You are measuring multiple aspects of quality, often with multiple methods per aspect. You need structured storage that lets you slice by quality dimension, by automated versus human assessment, and by request attributes. Many teams use observability platforms like Langfuse or LangSmith that are purpose-built for AI quality metrics.

## Safety Signals: Are the Outputs Harmful?

Safety signals measure whether the system is producing content that violates policies, harms users, or creates liability. This category includes content that is hateful, violent, sexually explicit, self-harm-promoting, personally identifiable, or otherwise restricted by your acceptable use policy. It also includes adversarial behavior: prompt injection attempts, jailbreak attempts, and data exfiltration attempts.

The core safety metrics are policy violation rate, false positive rate, and adversarial attempt rate. Policy violation rate is the percentage of outputs that your safety classifiers flag as violating policy. This metric has a problem: it conflates true positives with false positives. If your violation rate suddenly spikes, you do not know if users started asking harmful questions or if your classifier started misfiring. You need to track false positive rate separately by sampling flagged outputs and having humans review them. A false positive rate above 5 percent means your classifier is too sensitive and you are blocking legitimate use cases. Adversarial attempt rate is the percentage of inputs that show signs of adversarial intent: attempts to bypass filters, extract training data, or manipulate the system into harmful outputs.

Safety signals also include near-miss detection. A near-miss is an output that did not violate policy but came close. The model generated a response that your filter flagged with medium confidence but allowed through. Or the model started to generate a policy-violating response but your prefix filter caught it and regenerated. Near-misses are leading indicators. If near-miss rates are rising, violation rates will follow unless you adjust thresholds or retrain classifiers.

You need to measure safety at multiple layers. Input safety: are users submitting policy-violating prompts? Output safety: is the model generating policy-violating content? Context safety: if you are running RAG, does your retrieved context contain harmful material that could influence generation? Tool safety: if you are running agents, are tool invocations creating risk through side effects? Each layer needs independent measurement because failures can occur independently. Your input filter might be perfect while your output filter is miscalibrated. Your output filter might be perfect while your retrieved context contains PII.

Safety signals require real-time measurement and fast response. If a user submits a prompt that your classifier flags as a jailbreak attempt, you need to block it before the model generates a response. If the model generates content that violates policy, you need to block it before it reaches the user. This means safety classifiers run in the request path, adding latency. You need fast classifiers or you need to accept the latency cost. Most production systems target sub-100ms safety classification latency.

Safety signals also require audit trails. If a user reports a policy violation or a regulator asks for evidence of compliance, you need to produce logs showing what happened, which filters ran, what they detected, and what action was taken. This means storing flagged interactions with full context: the prompt, the response, the filter scores, the policy that was triggered, and the timestamp. Retention policies matter. GDPR might require you to delete user data after a certain period. Your legal team might require you to retain policy violation logs for years.

## Cost Signals: What Is This System Spending?

Cost signals measure how much money the system is burning. AI systems have variable costs that scale with usage. Every token you send to GPT-5 costs money. Every vector search costs compute. Every embedding API call costs money. Cost per request can vary by 100x depending on model choice, prompt length, and retrieval strategy. If you are not measuring cost, you are flying blind financially.

The core cost metrics are cost per request, cost per user, cost per session, and total monthly cost. Cost per request is the foundation. It aggregates model inference costs, embedding costs, vector search costs, and any other per-request infrastructure costs. You need to track this at a granular level: by model, by user segment, by feature, by request type. A single power user running 200 requests per day with long contexts might cost 50x more than 200 typical users. If you are not measuring per-user cost, you cannot tell.

Cost per session is useful for conversational systems where a single user interaction spans multiple turns. The first turn might cost 10 cents. By turn ten, with full conversation history in context, the cost might be 80 cents. You need to know when sessions become cost-prohibitive. Cost per user is useful for subscription services. If your product costs users 20 dollars per month and your AI costs average 35 dollars per user per month, your unit economics are broken. You need to detect this before you scale.

You also need to measure cost composition: what is driving your spend? Is it model inference, embeddings, vector search, or tool API calls? If 70 percent of your cost is model inference, optimizing retrieval will not help. If 60 percent of your cost is embeddings, switching from GPT-5 to Llama 4 will not help. You need to know where the money goes to know where to optimize.

Cost signals interact with quality signals. The cheapest model that meets your quality bar is the right model. But you cannot know what meets your quality bar unless you measure quality. Most teams start with expensive models, measure quality, then experiment with cheaper alternatives and measure again. If GPT-5-mini produces equivalent quality to GPT-5 on 80 percent of your requests, you route those requests to the cheaper model and save 60 percent on inference costs. But you only know this if you measure both cost and quality.

Cost signals also detect abuse and anomalies. A single user account making 10,000 requests per day might be a legitimate power user or a compromised account running a bot. A sudden 10x spike in embedding API calls might be organic growth or a misconfigured batch job generating embeddings for the same data repeatedly. You need alerting on cost anomalies just like you alert on error rate anomalies.

## Behavioral Signals: How Are Users Responding?

Behavioral signals measure how users interact with the system. This category includes explicit feedback like thumbs-up and thumbs-down ratings, implicit feedback like retry rates and abandonment rates, and engagement metrics like session length and feature adoption. Behavioral signals are your ground truth for user satisfaction. Infrastructure can be perfect, quality can be high, safety can be airtight, costs can be low, and users can still hate the product. Behavioral signals tell you if the system is actually working for users.

The core behavioral metrics are user feedback scores, retry rate, abandonment rate, session length, and feature adoption. User feedback scores are explicit: the user rates the response as helpful or not helpful. You need to track the percentage of responses that receive positive feedback and the percentage that receive negative feedback. You also need to track the percentage of responses that receive no feedback. If 95 percent of your responses go unrated, you do not have enough signal. You need to prompt for feedback strategically: after complex queries, after tool invocations, after long sessions.

Retry rate is the percentage of requests where the user rephrases and resubmits after receiving a response. A high retry rate means users are not getting useful answers on the first try. Abandonment rate is the percentage of sessions where the user leaves without completing their goal. If 40 percent of users abandon after two turns, your system is not meeting their needs. Session length is the number of turns per session. Very short sessions might indicate quick success or immediate failure. Very long sessions might indicate complexity or user frustration. You need to segment by outcome to interpret the signal.

Feature adoption measures which capabilities users are actually using. If you built an agentic system with ten tools and users only invoke two of them, either the other eight tools are not useful or users do not know they exist. If you built a RAG system with advanced filtering options and users never use the filters, the feature is wasted effort. Behavioral signals tell you what users value.

You also need to measure negative behaviors: requests that trigger errors, requests that time out, requests that receive low feedback scores, requests that get retried, requests that lead to abandonment. Each negative behavior is a failure mode. You need to measure frequency, distribution, and trends. If your timeout rate doubled over the last week, something changed. If your low-feedback-score rate is 30 percent for one user cohort and 5 percent for another, you have a segmentation problem.

Behavioral signals are noisy and delayed. A user who has a bad experience might not leave feedback immediately. They might retry once, get frustrated, and abandon. Or they might continue using the system while silently dissatisfied. You need to combine explicit and implicit signals to get the full picture. A lack of negative feedback does not mean users are happy. It might mean they are apathetic.

## Organizing Signals: The Dashboard Structure

You cannot put every metric on one dashboard. You need a hierarchy. Most teams organize monitoring into three tiers: executive dashboards, operational dashboards, and diagnostic dashboards. Executive dashboards show business-level metrics: daily active users, total requests, cost per user, quality scores, safety incident count. These update hourly or daily. Operational dashboards show real-time system health: error rate, latency, request rate, model endpoint status, alert status. These update every minute. Diagnostic dashboards show deep metrics for investigation: per-model latency distributions, per-feature cost breakdowns, per-query-type quality scores, per-tool success rates. These update in real time and support filtering, drilling, and trace lookup.

The hierarchy matters because different roles need different views. Executives need to know if the system is healthy and whether key metrics are trending in the right direction. They do not need to know that the P99 latency for GPT-5-mini requests is 200ms higher than yesterday. Engineers need to know that. When you are on-call and an alert fires, you need the operational dashboard to tell you what is broken. When you are debugging a specific issue, you need the diagnostic dashboard to show you exactly what happened.

The next question is not what to measure but what happens when you do not. Flying blind is not a neutral state. It is a high-cost state. The costs are concrete, measurable, and often larger than the cost of building observability in the first place.

