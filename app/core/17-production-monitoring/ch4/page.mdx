# Chapter 4 — Drift Detection and Distribution Monitoring

Your AI system was validated against a specific distribution of inputs. Users do not care. They send whatever they send, and their behavior changes over time in ways your validation set never anticipated. Drift is the slow divergence between the world your system was built for and the world it actually operates in. Input drift means users are asking different questions. Output drift means your model is answering differently. Concept drift means the relationship between questions and correct answers has changed because the world itself changed. Detecting drift before it becomes degradation is the difference between proactive maintenance and reactive firefighting.

---

- 4.1 — What Is Drift and Why It Kills AI Systems Slowly
- 4.2 — Input Drift: When User Behavior Changes
- 4.3 — Output Drift: When Model Behavior Changes
- 4.4 — Concept Drift: When the World Changes
- 4.5 — Statistical Methods for Drift Detection: KS Tests, PSI, Jensen-Shannon
- 4.6 — Embedding Space Monitoring: Detecting Semantic Shift
- 4.7 — Baseline Management: Establishing and Updating Reference Distributions
- 4.8 — Drift Alert Tuning: Avoiding False Positives Without Missing Real Drift
- 4.9 — Drift Response Playbooks: What to Do When Drift Is Detected
- 4.10 — Segment-Level Drift: Catching Localized Distribution Shifts
- 4.11 — Temporal Patterns: Seasonal, Weekly, and Event-Driven Drift

---

*Drift is not a bug. It is physics. The question is whether you detect it before your users do.*
