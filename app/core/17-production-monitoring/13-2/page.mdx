# 13.2 â€” Gateway-Based Tools: Helicone, Portkey, and the Proxy Pattern

Gateway observability adds fifteen milliseconds to every request. For most applications, this latency is invisible. For the real-time trading platform that used LLMs to generate market commentary, fifteen milliseconds meant the difference between timely information and stale data. They tested Helicone in staging, measured the added latency, and removed it before production launch. Gateway tools solve real problems, but they solve them at an architectural cost that not every application can afford.

The proxy pattern is conceptually simple. Instead of calling OpenAI directly, you call the gateway. The gateway logs the request, forwards it to OpenAI, logs the response, and returns it to your application. You get complete observability with minimal code changes. Your application barely knows the gateway exists.

## How Gateway Architecture Works

The request path changes from application to provider into application to gateway to provider. Every byte that flows between your code and the model provider passes through the gateway first. This positioning gives the gateway complete visibility. It sees the full prompt, every token of the response, all metadata, timing information, and error conditions. Nothing happens that the gateway does not observe.

Implementation requires changing one configuration value. Your code currently points to api.openai.com. You change it to point to your gateway instance at gateway.yourcompany.com. The gateway forwards requests to api.openai.com on your behalf. To your application, the gateway looks identical to OpenAI's API. The same endpoints exist. The same authentication works. The same response format returns. The only difference is the destination URL.

This simplicity makes gateway tools the fastest path to comprehensive observability. A team with fifty services making LLM calls can add observability to all of them by updating fifty configuration values. No SDK imports. No code instrumentation. No testing whether the wrapper library works correctly with async contexts or streaming responses. Just point to the gateway and everything starts getting logged.

## What Helicone Does Well

Helicone positions itself as the zero-effort observability layer. The marketing is mostly accurate. A developer can add Helicone to an existing application in under five minutes. Create an account, get an API key, change the OpenAI endpoint from api.openai.com to oai.helicone.ai, add the Helicone API key as a header, and requests start flowing through. The Helicone dashboard immediately shows request counts, token usage, costs, and latency distributions.

The tool shines for teams that need basic visibility quickly. You get cost tracking by user, by endpoint, by model. You get request logs with full prompt and response history. You get caching that reduces duplicate requests automatically. You get rate limiting that protects you from runaway costs. All of this works without writing instrumentation code.

The architectural choice means Helicone sees everything but understands nothing. It knows you sent a request to GPT-5.1. It knows the prompt was eight hundred tokens. It knows the response took one point three seconds. It does not know whether the response was correct, whether it matched your quality criteria, or whether the user found it helpful. Gateway tools provide infrastructure observability for AI. They do not provide semantic observability.

Teams using Helicone typically combine it with other tools. Helicone handles the raw request logging and cost tracking. A platform tool like Langfuse handles the business logic tracing and quality evaluation. The gateway provides the foundation. Additional instrumentation adds the context.

## What Portkey Adds to the Pattern

Portkey extends the gateway pattern with routing intelligence. The gateway does not just forward requests to one provider. It can send them to different providers based on rules you configure. Your application asks for a completion. Portkey decides whether to send it to GPT-5.1, Claude Opus 4.5, or Gemini 3 Pro based on cost, latency, or custom logic.

This capability matters for teams running multi-provider strategies. The customer support automation company routed simple classification tasks to Gemini 3 Flash for speed and cost. They routed complex reasoning tasks to Claude Opus 4.5 for quality. They routed everything else to GPT-5.1 as a balanced default. Portkey handled the routing logic. Their application code stayed provider-agnostic.

Portkey also provides automatic fallback. If OpenAI returns a 429 rate limit error, Portkey retries the request against Anthropic automatically. If Anthropic is experiencing elevated latency, Portkey shifts traffic to Google. Your application sees reliable responses even when individual providers experience issues. The gateway absorbs the complexity of multi-provider operations.

The cost is additional configuration complexity. Someone needs to define the routing rules. Someone needs to monitor whether the rules produce the intended behavior. Someone needs to update the rules when provider capabilities or pricing changes. Gateway intelligence is powerful but not free.

## The Latency Tax in Detail

The fifteen-millisecond overhead is not constant. It varies with geography, network conditions, and gateway load. The e-commerce company tested Helicone from their US-East data center. Median added latency was twelve milliseconds. Ninety-ninth percentile latency was forty-five milliseconds. For their use case, this was acceptable. Their AI features already had multi-second total latency. Twelve more milliseconds did not matter.

The financial services company tested the same setup. They measured added latency from their London office to Helicone's European gateway. Median latency was eight milliseconds. They measured latency from their Singapore office to the same gateway. Median latency was eighty milliseconds. Gateway latency depends heavily on physical proximity between your infrastructure and the gateway's infrastructure.

The mitigation is deploying gateways close to your compute. Helicone offers regional deployments. Portkey supports self-hosted instances. If you run your application in AWS US-East, run your gateway in AWS US-East. If you run multi-region, run gateways in each region and route traffic to the closest one. Gateway latency becomes negligible when the gateway sits inside your VPC.

## Request Modification and Caching

Gateways sit in the request path. This means they can modify requests before forwarding them. Helicone provides automatic prompt caching. If you send the same prompt twice, Helicone returns the cached response without calling OpenAI. You save the API cost and reduce latency to near-zero for repeated requests.

This behavior is magical when it works and dangerous when it does not. The weather application sent requests like "What is the weather in Seattle?" multiple times per day. Caching seemed perfect. Users asked the same question repeatedly. Why call the LLM every time? The problem emerged when users asked the question at 8am and again at 2pm. The cached response from 8am was stale by afternoon. Weather changes. The cache did not.

Cache invalidation is the hard problem. Helicone allows setting cache TTL, but determining the right TTL requires understanding your use case deeply. Some prompts produce stable answers that stay valid for days. Some prompts produce answers that go stale in minutes. Gateway caching works when your requests are truly idempotent. It fails when context or time matters.

Request modification also enables prompt enrichment. Portkey can inject additional context into prompts automatically. You send a basic request. The gateway adds organization-specific instructions, formatting requirements, or safety guidelines before forwarding it. Your application code stays simple. The gateway handles the complexity.

The risk is drift between what your code intended and what the gateway actually sent. Debugging becomes harder when the gateway modifies requests silently. Teams using request modification should log both the original and modified versions so issues can be traced.

## Multi-Tenant Isolation and Key Management

The SaaS platform served three hundred enterprise customers from a single application. Each customer needed AI features. Each customer had their own OpenAI API key because of cost allocation requirements. Managing three hundred API keys in application code was untenable.

Portkey solved this by managing keys at the gateway level. Each customer's requests included a customer identifier. The gateway looked up the appropriate API key and used it for that request. The application code never touched individual customer keys. Key rotation happened at the gateway without application deployments. Key usage tracking happened automatically per customer.

This architecture works well for B2B platforms where customers bring their own LLM provider accounts. It works poorly for B2C applications where you use a single organizational account. The complexity is justified only when multi-tenancy is a hard requirement.

## Privacy and Compliance Implications

Gateway tools see every prompt and every response. This creates privacy concerns that differ from SDK-based tools. When you use Langfuse's SDK, you decide what to log. You can redact sensitive fields. You can skip logging certain request types entirely. When you use a gateway, every byte passes through. Redaction requires gateway-level configuration, not application-level code.

Helicone offers prompt anonymization that strips PII automatically before storing logs. The feature works by running NER models against every prompt, identifying entities that look like names or email addresses, and replacing them with placeholders. This adds latency and is not perfectly accurate. False positives mean legitimate content gets redacted. False negatives mean PII leaks through.

The financial services company decided gateway tools were incompatible with their compliance requirements. They could not send customer financial data through a third-party proxy, even briefly. They needed on-premise observability or nothing. They deployed self-hosted Arize Phoenix instead.

Teams in regulated industries should evaluate gateway tools carefully. The convenience is real. The compliance risk is also real. A tool that requires sending sensitive data to external infrastructure may be legally impossible regardless of its technical capabilities.

## Self-Hosted Gateway Options

LiteLLM is the open-source answer to Helicone and Portkey. It provides the same proxy pattern with the option to run entirely within your infrastructure. You deploy LiteLLM to your Kubernetes cluster. Your applications call the LiteLLM endpoint. LiteLLM forwards requests to providers and logs everything to your database.

The advantages are control and privacy. No data leaves your network. You can modify the gateway code if needed. You pay no per-request fees. The disadvantages are operational complexity. You now operate a distributed proxy layer. You need monitoring for the proxy itself. You need to handle scaling, failover, and upgrades. The engineering effort is substantial.

The healthcare company ran self-hosted LiteLLM because their privacy requirements left no alternative. They deployed it to three regions with automatic failover. They used Postgres for log storage and exported logs to their data lake hourly. They assigned one engineer half-time to operate the system. This staffing level proved barely sufficient. When LiteLLM released a version with breaking changes, migration took two weeks.

Self-hosted gateways make sense when you have strong privacy requirements and platform engineering capacity. They are the wrong choice for small teams focused on product velocity.

## When Gateway Tools Are the Right Choice

Gateway architecture shines in specific situations. If you need observability across many services and do not want to instrument each one individually, gateways provide comprehensive coverage with minimal effort. If you need multi-provider routing and automatic failback, gateways handle this complexity transparently. If you need cost tracking without building custom infrastructure, gateways provide it immediately.

Gateway tools struggle with semantic understanding. They log requests but do not know whether the AI is doing the right thing. They track costs but do not track quality. They measure latency but do not measure correctness. Teams that need deep AI-specific observability will always need more than a gateway alone.

The multi-tool strategy is common in 2026. Use a gateway for universal request logging and cost tracking. Use a platform tool for quality evaluation and business logic tracing. Use your existing APM for infrastructure health. Each tool does what it does best. The integration overhead is real but manageable. Understanding platform tools and how they complement gateway approaches is the next critical piece.

