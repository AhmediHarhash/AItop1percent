# 13.4 â€” Enterprise Integration: Datadog, New Relic, and Existing APM

The SRE team at the logistics company managed infrastructure for two hundred services. They had fifteen years of institutional knowledge encoded in Datadog dashboards, alert configurations, and runbooks. When the AI product team wanted to add specialized AI observability tools, the SRE team resisted. Another tool meant another dashboard to check, another alert channel to monitor, another login to manage. They wanted AI metrics in Datadog alongside everything else. The AI team explained that Datadog could not provide the semantic depth they needed for AI-specific observability. Both teams were right. The tension between unified tooling and specialized capabilities defines enterprise AI observability in 2026.

## What Enterprise APM Tools Handle Well

Datadog, New Relic, Dynatrace, and similar platforms excel at infrastructure observability. They monitor CPU, memory, disk, and network across your entire fleet. They track API response times, error rates, and throughput. They provide distributed tracing for microservices. They integrate with your existing incident management, on-call rotation, and escalation policies. Your SRE team knows these tools intimately and trusts them completely.

Adding AI services to this infrastructure monitoring is straightforward. Your AI service makes HTTP requests to OpenAI. Datadog sees those requests like any other external API call. It tracks latency, error rates, and request volumes. It graphs these metrics alongside your database queries, cache hits, and message queue throughput. Your on-call engineer gets a unified view of system health without context-switching between tools.

The insurance claims system ran on AWS with full Datadog instrumentation. When they added AI-powered document extraction, Datadog immediately started tracking the new service. The dashboards showed request rates climbing, latency staying stable, and error rates at expected levels. When OpenAI experienced elevated error rates one afternoon, Datadog alerted the team within seconds. The existing monitoring infrastructure extended to AI features without additional work.

## Where Enterprise APM Falls Short

Datadog knows you called OpenAI. It knows the request took eight hundred milliseconds. It knows you received a 200 OK response with fifteen hundred tokens. It does not know whether the response was correct, relevant, or useful. It cannot tell you that prompt quality degraded, that outputs became less specific, or that the model started hallucinating at higher rates. Infrastructure health and AI quality are orthogonal concerns.

The content generation platform looked healthy in New Relic. Response times were fast. Error rates were low. Throughput was steady. Customer complaints were increasing because the AI-generated product descriptions had become generic and unhelpful. The quality degradation happened without any infrastructure signals. New Relic could not detect it because it was not an infrastructure problem.

This gap is fundamental. Enterprise APM tools measure what they can instrument through standard telemetry. Request timing, status codes, resource utilization. They do not analyze prompt content, evaluate semantic coherence, or detect subtle quality shifts. These capabilities require AI-specific tooling.

## Datadog AI Observability Extensions in 2026

Datadog recognized this gap and released AI Observability features in late 2025. The extensions provide LLM-specific metrics beyond basic request monitoring. They track token usage by model and endpoint, calculate costs automatically based on current provider pricing, and detect anomalies in token distributions that might indicate prompt issues.

The integration works through instrumentation libraries similar to platform tools. You import the Datadog LLM library, wrap your OpenAI calls with their decorators, and metrics start flowing. The advantage is that metrics appear in your existing Datadog dashboards. You can create unified views that show infrastructure health and AI-specific metrics side-by-side.

The financial analysis company used Datadog AI Observability to track cost per analysis type. They correlated token usage with customer tier and feature usage. They built dashboards that showed AI costs alongside compute costs, making cost optimization decisions easier. The unified view helped finance teams understand total infrastructure costs without needing to aggregate across multiple platforms.

The limitation is depth. Datadog tracks token counts and costs well. It struggles with semantic quality evaluation, prompt drift detection, and retrieval quality analysis. The tool provides visibility but not the deep AI-specific analytics that specialized platforms deliver.

## New Relic's LLM Monitoring Approach

New Relic took a different approach with their AI Monitoring product. Instead of building comprehensive AI observability features, they focused on integration with existing AI tools. New Relic acts as the aggregation layer. You continue using LangChain, LangSmith, or your chosen framework. New Relic ingests telemetry from those tools and displays it alongside infrastructure metrics.

This architecture acknowledges that enterprise APM tools cannot replace specialized AI platforms. Instead, they become the unified interface. Your SRE team looks at New Relic for everything. New Relic pulls AI metrics from specialized tools and presents them in familiar formats. The SRE team does not need to learn a new platform.

The media company ran their AI content moderation system with this architecture. LangSmith instrumented their agent workflows and captured detailed traces. New Relic ingested summary metrics from LangSmith via API integration. The operations team saw moderation volumes, accuracy rates, and system health in New Relic. When they needed to debug specific quality issues, they drilled through to LangSmith for detailed trace analysis. Each tool did what it did best.

## The Unified Dashboard Problem

Building unified dashboards that combine infrastructure and AI metrics is harder than it looks. The engineering team at the translation service wanted a single dashboard showing translation throughput, latency, error rates, translation quality scores, and cost per language pair. Half those metrics came from Datadog. Half came from Langfuse. Neither tool could display metrics from the other natively.

The solution required exporting data from both tools into a data warehouse and building custom dashboards in Grafana. Datadog metrics exported to Snowflake every five minutes. Langfuse metrics exported via webhook to the same schema. Grafana queried Snowflake and rendered unified visualizations. The system worked but required significant engineering effort to build and maintain.

The alternative is accepting tool fragmentation. Infrastructure metrics live in Datadog. AI quality metrics live in Langfuse. Unified cost reporting happens in your business intelligence tools by joining data exports. Your team learns to use multiple tools for different questions. This is the reality at most organizations with mature AI systems in 2026.

## Alert Routing and Incident Management

Enterprise APM platforms integrate deeply with incident management workflows. Datadog can page your on-call engineer, create a PagerDuty incident, post to Slack, and update a status page automatically. Specialized AI observability tools typically cannot. They can send webhooks or emails but lack the mature incident integrations that enterprise tools provide.

The customer support platform needed to alert when their AI response quality dropped below thresholds. They tracked quality in Langfuse but managed incidents in PagerDuty. Langfuse could not create PagerDuty incidents directly. They built a webhook bridge. Langfuse sent webhooks to a small service they maintained. That service translated Langfuse alerts into PagerDuty API calls. The architecture worked but added fragility.

Organizations with mature incident response processes want AI alerts flowing through the same channels as infrastructure alerts. This requirement often drives teams toward enterprise APM tools even when those tools provide less AI-specific depth. The operational maturity and existing integrations outweigh the specialized capabilities.

## The Cost Visibility Challenge

Finance teams want unified cost reporting. They want to see compute costs, data transfer costs, database costs, and AI costs in one place. Enterprise APM tools provide infrastructure cost visibility. AI platform tools provide model inference costs. Neither provides the complete picture.

The SaaS company tracked infrastructure costs in Datadog's Cloud Cost Management. They tracked AI costs in Helicone. Producing a unified cost report required exporting data from both tools and joining it manually. The finance team complained that AI costs were invisible in their standard reporting. The engineering team had the data but in the wrong format and wrong system.

The solution space is evolving. Some organizations export all cost data to their BI tools and build unified reports there. Some organizations use specialized FinOps platforms that integrate with both infrastructure and AI cost sources. Some organizations accept fragmented cost visibility as the price of using specialized tools. No perfect solution exists in 2026.

## Custom Metrics and Extensibility

Enterprise APM platforms excel at custom metrics. You can send arbitrary metrics via StatsD or API. You can build dashboards that combine standard metrics with custom business metrics. Datadog will graph anything you send it. This extensibility makes enterprise APM tools powerful integration hubs.

The legal document system calculated a custom quality score for every AI-generated summary. They sent that score to Datadog as a custom metric. Datadog graphed quality scores over time, alerted when scores dropped, and correlated quality changes with infrastructure events. The quality metric was AI-specific. The monitoring infrastructure was standard APM.

This pattern works for teams that want unified monitoring without adopting specialized AI platforms. Calculate your AI quality metrics in application code. Send them to your existing APM as custom metrics. Build dashboards and alerts using familiar tools. You lose the detailed trace analysis and prompt debugging that specialized tools provide. You gain simplicity and unified operations.

## The Self-Hosted APM Option

Large enterprises often run self-hosted APM infrastructure for compliance or cost reasons. They deploy Grafana, Prometheus, and Loki in their own data centers. They own the complete monitoring stack. Adding AI observability to these stacks requires finding tools that integrate with open-source monitoring infrastructure.

Arize Phoenix integrates naturally with Prometheus and Grafana. It exports metrics in Prometheus format. It provides pre-built Grafana dashboards for AI workloads. Teams running open-source APM stacks can add Phoenix without adopting a new monitoring paradigm. The AI metrics appear in the same Grafana instance that displays everything else.

The healthcare company ran entirely self-hosted monitoring for HIPAA compliance. They used Prometheus for metrics, Loki for logs, and Grafana for visualization. When they added AI features, they deployed Arize Phoenix alongside their existing stack. Phoenix sent metrics to Prometheus. Grafana dashboards combined infrastructure and AI metrics. The architecture preserved their compliance requirements while adding AI observability.

## Multi-Tool Integration Patterns

Most organizations in 2026 run multiple observability tools. Enterprise APM for infrastructure, specialized platforms for AI quality, gateway tools for cost tracking, and BI tools for business reporting. The question is not whether to use multiple tools but how to integrate them effectively.

The integration pattern that works reliably is treating your data warehouse as the central hub. Every tool exports data to the warehouse. Business intelligence and reporting query the warehouse. This architecture decouples tools from each other. Datadog does not need to know about Langfuse. Langfuse does not need to know about your BI tools. Everything flows through the warehouse.

The e-commerce company implemented this with Snowflake. Datadog metrics exported to Snowflake via their native integration. Langfuse traces exported via custom webhook to a Lambda function that wrote to Snowflake. Helicone cost data exported daily via API to Snowflake. Tableau queried Snowflake for unified reporting. Each tool operated independently. The warehouse provided unity.

## When Enterprise APM Is Enough

For some AI use cases, enterprise APM tools provide sufficient observability. If your AI features are simple request-response patterns with clear success criteria, infrastructure monitoring plus custom quality metrics may suffice. If you call one LLM per user request, check the output against validation rules, and return a result, you do not need detailed trace analysis or prompt debugging.

The calculator application used GPT-5 to parse natural language math queries. The request flow was simple. Parse the query, call GPT-5 for structured extraction, validate the result is valid math, compute the answer, return it. When parsing failed, it failed obviously with clear error patterns. Datadog monitored request rates and latency. Custom metrics tracked parse success rates. The team never needed specialized AI observability tools.

The threshold is complexity. Simple AI features can live within enterprise APM infrastructure. Complex AI features with multi-step reasoning, retrieval, tool use, and business logic require specialized observability. The key is being honest about your complexity level before choosing tools. Understanding build versus buy decisions for AI observability is the next essential topic.

