# Chapter 8 — Incident Detection and Response

An alert fires. Something is wrong. What happens next determines whether a potential outage becomes a resolved incident or an escalating crisis. AI incident response differs from traditional incident response because AI failures are often ambiguous. Is the model hallucinating or did the retrieval layer return bad context? Is this a model provider issue or a prompt regression? Is the quality drop affecting all users or just a specific segment? The first five minutes of an AI incident are spent figuring out what kind of problem you have before you can even start fixing it. Good incident response processes account for this ambiguity with structured triage, clear severity classification, and playbooks that guide investigation before they prescribe solutions.

---

- 8.1 — AI Incident Taxonomy: The Failure Modes Unique to AI Systems
- 8.2 — Detection Latency: How Fast Can You Know Something Is Wrong
- 8.3 — Incident Severity Classification for AI Failures
- 8.4 — The First Five Minutes: Initial Triage for AI Incidents
- 8.5 — Root Cause Investigation: Debugging Production AI Failures
- 8.6 — Mitigation Strategies: Rollback, Fallback, and Graceful Degradation
- 8.7 — Communication During AI Incidents: Internal and External
- 8.8 — Post-Incident Review: The AI-Specific Questions to Ask
- 8.9 — Incident Metrics: MTTD, MTTR, and AI-Specific KPIs
- 8.10 — Incident Automation: Runbooks That Execute Themselves
- 8.11 — Learning from Incidents: Feeding Production Failures Back Into Eval

---

*The best incident response teams are the ones that turn every incident into an eval case. The failure that happens once becomes the test that prevents it from happening again.*
