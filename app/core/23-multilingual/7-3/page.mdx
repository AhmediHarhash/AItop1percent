# 7.3 â€” Instruction Following Across Languages: Where Models Lose Compliance

In early 2025, a European insurance company launched a claims-processing assistant in eight languages. The system prompt contained eight formatting constraints: respond with a numbered list of next steps, keep each step under 40 words, use formal register, do not use first person, include a legal disclaimer at the end, bold the action verb in each step, limit the response to five steps maximum, and close with a reference number placeholder. The prompt had been refined over six weeks using English test cases and performed beautifully. In English, the model followed all eight constraints 94 percent of the time. The team ran a quick check in French and German, saw reasonable output, and shipped.

Within the first month, the compliance data told a different story. French compliance averaged 87 percent -- the model occasionally exceeded five steps or dropped the disclaimer. German was at 83 percent -- the formal register held, but the 40-word step limit was ignored because German compound words made steps longer. Spanish dropped to 79 percent. Japanese hit 72 percent -- the model often switched from numbered steps to flowing paragraphs and ignored the bold formatting entirely. Thai compliance ran at 64 percent -- the model followed the numbered list format about half the time, rarely included the disclaimer, and occasionally inserted English sentences. Arabic was at 61 percent -- the model produced prose instead of numbered steps, ignored the step count limit, and the bold formatting was inconsistent.

The prompt was identical across all languages. Only the user's language changed. The engineering team had assumed that instruction following was a model capability that worked uniformly. It does not. It is a capability that degrades along a predictable gradient, and the gradient follows the model's training data distribution.

## The Compliance Gradient

Instruction following is not a binary capability. It is a spectrum that correlates with how much instruction-following training data the model saw in each language. This creates what practitioners call **The Compliance Gradient**: a predictable pattern of degradation from English through high-resource languages to low-resource languages.

At the top of the gradient sits English, where frontier models in 2026 follow well-written instructions above 90 percent of the time for most constraint types. English is the language where RLHF was conducted, where the largest volume of instruction-tuning data exists, and where human evaluators rated model compliance during training.

The first drop occurs in Tier 1 non-English languages -- French, German, Spanish, Portuguese, Italian, Dutch, and Mandarin Chinese. These languages have substantial representation in training data and benefit from linguistic similarity to English (for European languages) or massive corpus size (for Mandarin). Instruction compliance in these languages typically runs 5 to 12 percentage points below English. The model still follows most instructions, but compound or nuanced directives start to slip.

The second drop comes with Tier 2 languages -- Japanese, Korean, Russian, Arabic, Turkish, Polish, Vietnamese, Hindi. These languages have meaningful training data but differ more substantially from English in grammar, script, or both. Compliance drops 12 to 25 percentage points below English. Complex formatting instructions fail more often. Multi-step conditional logic in the prompt is frequently simplified by the model.

The steepest drop occurs in Tier 3 languages -- Thai, Burmese, Swahili, Tagalog, Yoruba, Khmer, Lao, Amharic, and most languages with fewer than 50 million speakers and limited digital presence. Compliance can fall 25 to 40 percentage points below English. The model may effectively ignore complex instructions and revert to its default generation behavior, which is itself biased toward English-style output.

This gradient is not a fixed property of the language. It is a property of the model's training data distribution. As multilingual training improves, the gradient shifts. GPT-5 and Claude Opus 4.6 show a narrower gradient than their predecessors, especially for Tier 2 languages where training data has expanded significantly. But the gradient persists, and for Tier 3 languages it remains steep enough to be a production-critical concern.

## Which Instructions Fail First

Not all instructions degrade equally. There is a hierarchy of fragility, and knowing it tells you where to focus your testing and mitigation effort.

**Length constraints fail early.** An instruction like "keep your response under 150 words" relies on the model's internal calibration of what "150 words" means. In English, this calibration is reasonably accurate. In Japanese, the concept of a "word" is linguistically ambiguous -- Japanese does not use spaces between words, so the model must decide how to count. In German, compound words mean a single "word" can contain what English would express in three or four words. In Thai, word boundaries are not marked by spaces, and the segmentation is context-dependent. The result is that word-count instructions are unreliable across languages, with the model undershooting in some languages and overshooting in others. Character-count or sentence-count instructions are slightly more stable but still vary.

**Formatting constraints fail next.** Numbered lists, bullet points, headers, bold text, and structured section layouts are all formatting conventions that the model learned primarily from English-language text. When asked to produce a numbered list in Arabic, the model may comply but switch to Arabic numerals inconsistently, mix right-to-left text direction with left-to-right numeral placement, or produce a layout that looks correct in plain text but renders poorly in a right-to-left interface. In Japanese, the model may substitute native numbering conventions or switch to a flowing-paragraph format that feels more natural in Japanese but violates the prompt's formatting rules.

**Tone and register constraints degrade in parallel.** An instruction to use "formal but friendly" tone is culturally anchored. The model interprets this through the lens of its training data for each language. In German, "formal" may produce output that uses the Sie form consistently but sounds stiff. In Japanese, the model may oscillate between politeness levels that do not map to the English concept of "formal but friendly" at all. The instruction is not ignored -- it is reinterpreted through a cultural lens the model learned from training data, and the reinterpretation may not match what the product team intended.

**Compound instructions simplify.** When a prompt contains multiple simultaneous constraints -- "respond with a numbered list, keep each item under 30 words, use formal register, and include a disclaimer at the end" -- the model in English can hold all four constraints simultaneously. In non-English languages, the model's ability to juggle multiple constraints decreases. It will follow the most salient constraint (often the format instruction) and relax or ignore the others. The more constraints you stack, the more you lose in non-English languages.

**Conditional instructions fail most.** An instruction like "if the user asks about pricing, respond with a table; if the user asks about features, respond with a numbered list" requires the model to parse the condition and select the appropriate format. This two-step process -- condition evaluation plus format execution -- fails at two to three times the rate in non-English languages compared to English. The model may apply the wrong format, ignore the condition and use a default format, or partially execute both formats in a confused hybrid.

## Why Compliance Degrades: The Training Data Explanation

The mechanism behind the compliance gradient is straightforward once you understand how instruction following is trained.

Modern language models acquire instruction-following capability through a multi-stage training process. The base model learns language patterns from massive pretraining corpora. Then supervised fine-tuning (SFT) on instruction-response pairs teaches the model to follow specific types of instructions. Then RLHF or DPO aligns the model's outputs with human preferences, including the preference for following instructions accurately.

Each of these stages has an English bias. The pretraining data is 40 to 60 percent English. The SFT instruction datasets are overwhelmingly English -- many contain 80 percent or more English examples. The RLHF human evaluators rated primarily English outputs. The result is that the model's instruction-following "muscle" was built almost entirely on English reps. It generalizes to other languages through the model's cross-lingual transfer capabilities, but that generalization is imperfect and decays with linguistic distance from English.

Think of it this way: the model learned that when it sees the instruction "respond with a numbered list," it should produce output with the pattern "1. First point\n2. Second point\n3. Third point." It learned this pattern from thousands of English examples. When the target output language is French, the model can transfer this pattern with high accuracy because French numbered lists look almost identical to English ones. When the target language is Japanese, the transfer is harder because Japanese text layout conventions differ, the numbering style may differ, and the model saw fewer examples of numbered-list output in Japanese during SFT and RLHF.

This is why the compliance gradient follows the training data distribution rather than the inherent difficulty of the language. Finnish is a linguistically complex language with agglutinative morphology, but if a model has substantial Finnish instruction-tuning data, instruction compliance in Finnish will be higher than in a simpler language that has less training data. The gradient is about exposure, not complexity.

## The Simplification Principle

The most effective mitigation for cross-lingual compliance degradation is also the most counterintuitive: simplify your instructions for non-English contexts.

Most teams assume that if an instruction works in English, the fix for non-English failure is to make the instruction more explicit, more detailed, more specific. In practice, the opposite is true. Complex instructions with five or six constraints degrade faster across languages than simple instructions with two or three constraints. Adding detail to compensate for non-compliance often makes it worse, because the additional detail adds more English-centric formatting assumptions that the model struggles to transfer.

**The Simplification Principle** states: for each additional language you support beyond English, reduce the number of simultaneous constraints in your prompt until compliance remains above your quality threshold. If your English prompt has eight constraints and achieves 94 percent compliance, but your Japanese compliance is at 72 percent, do not add more instructions to push Japanese compliance up. Remove constraints until the remaining ones are followed reliably, then solve the removed constraints through other mechanisms -- post-processing, output validation, or per-language prompt variants.

In practice, this often means maintaining two or three complexity tiers of your system prompt. The English tier contains all constraints. The Tier 1 non-English tier contains the most critical constraints with simplified wording. The Tier 2 and Tier 3 tier contains only the essential constraints -- the ones where non-compliance causes a user-facing failure or a safety risk.

## Repetition and Reinforcement

When simplification alone is not enough, repetition helps. The model's attention mechanism gives more weight to instructions that appear multiple times in the prompt. If a specific constraint is critical -- a safety boundary, a required disclaimer, a format that your downstream parser depends on -- repeat it.

The effective pattern is to state the constraint once in the system prompt's instruction section, then restate it in the target-language style section (if you use the hybrid architecture from subchapter 7.2), then reinforce it one more time immediately before the generation point. Three repetitions of a critical constraint can improve compliance by 10 to 15 percentage points in Tier 2 languages. More than three repetitions show diminishing returns and waste token budget.

Repetition works because the model's cross-lingual instruction following is weaker but not absent. Each instance of the instruction adds weight to the constraint during generation. The model that might ignore a single instance of "include a disclaimer at the end" in Thai is more likely to comply when it sees the instruction three times, including once in Thai.

## Examples Over Rules

A second mitigation that consistently outperforms rule-based instructions is replacing rules with examples. Instead of telling the model "format your response as a numbered list with each item under 30 words," show it an example of the desired output in the target language.

The reason this works is fundamental to how language models generate text. Rules require the model to interpret the instruction and then construct output that satisfies the rule. Examples allow the model to pattern-match, which is a stronger capability. When the model sees a Japanese numbered list in the prompt, it can reproduce the pattern even if it would not have constructed that pattern from an English rule alone.

The tradeoff is token cost. Examples consume more tokens than rules, and in high-fertility languages the cost is amplified. A Japanese example that demonstrates the desired format might cost 150 to 200 tokens, compared to 30 tokens for an English formatting rule. But if the rule achieves 72 percent compliance and the example achieves 89 percent compliance, the extra tokens are worth the investment. Chapter 7.4 covers few-shot example strategies in detail.

## Per-Language Prompt Optimization

The most resource-intensive but highest-quality approach is to optimize the prompt separately for each language. This goes beyond translation. It means rewriting instructions in ways that work better in the target language, adjusting constraints to match the target language's conventions, and testing each language's prompt independently.

For example, a word-count constraint in English might become a character-count constraint in Japanese. A "formal but friendly" tone directive in English might become a specific politeness level directive in Japanese (using the concept of keigo levels rather than the English "formal/friendly" spectrum). A numbered-list format instruction in Arabic might include explicit direction about numeral style and text alignment.

This approach produces the highest compliance rates because each language's prompt is tuned to that language's strengths and conventions. The cost is that you now maintain as many prompt versions as you support languages, each requiring testing, iteration, and ongoing maintenance. For most teams, per-language optimization is justified only for their top three to five languages by user volume. The remaining languages get simpler prompts with fewer constraints, accepting lower compliance in exchange for maintainable operations.

## The Compliance Testing Framework

You cannot manage what you do not measure. Every team serving multiple languages needs a compliance testing framework that tracks instruction following per language, per constraint.

Build a test suite that contains twenty to thirty representative queries per supported language. For each query, define the expected constraints: the output should be in language X, formatted as a numbered list, with N or fewer items, in formal register, with a disclaimer at the end. Run every query against your system prompt and score each constraint as pass or fail.

The output is a compliance matrix: languages on one axis, constraints on the other, compliance percentages in each cell. This matrix immediately reveals your weak spots. You will likely find that some constraints are robust across all languages (simple language-matching instructions, for example) while others fall apart outside of English (complex formatting, conditional logic, length limits).

Use the matrix to prioritize. If a constraint drops below your quality threshold in a specific language, you have four options: simplify the constraint, reinforce it with repetition, replace it with an example, or handle it in post-processing. The matrix tells you which constraints need which treatment in which languages, turning an overwhelming multilingual quality problem into a manageable engineering backlog.

## When Compliance Failure Is a Safety Issue

Some instruction-following failures are cosmetic -- the numbered list becomes a bulleted list, the response is slightly longer than specified. Others are dangerous. If your system prompt includes instructions like "never provide medical diagnoses," "do not generate content about weapons," or "always include a disclaimer that this is not legal advice," these are safety constraints. When they fail, the failure is not a formatting inconvenience. It is a liability.

Safety-critical instructions must be held to a higher compliance standard than formatting instructions, and that standard must be met across every supported language. If your safety compliance is 98 percent in English but 82 percent in Thai, you have a safety gap for Thai users that could expose your organization to regulatory action, particularly under the EU AI Act's requirements for high-risk systems.

The mitigation for safety-critical instructions is defense in depth. Do not rely solely on the system prompt to enforce safety. Add an output classification layer that checks every response -- regardless of language -- for safety violations before it reaches the user. Use the system prompt as the first line of defense and the classifier as the second. The system prompt will catch most violations in most languages. The classifier catches the ones that slip through, especially in languages where the system prompt's influence is weaker.

This layered approach is more expensive than relying on the system prompt alone. But the alternative -- a safety gap that scales with the number of languages you support -- is a risk most organizations cannot afford.

## The Path Forward

Instruction-following degradation across languages is not a problem you solve once and forget. It is an ongoing operational reality that requires monitoring, measurement, and periodic re-optimization as models change.

Each new model release shifts the compliance gradient. GPT-5.1 may handle Japanese formatting better than GPT-5 but regress on Arabic conditional instructions. Claude Opus 4.6 may improve Thai compliance by ten percentage points compared to Claude Opus 4.5 but change the way it interprets German formality levels. Your compliance matrix from six months ago may no longer reflect the current model's behavior.

The discipline is to re-run your compliance test suite after every model update, treat the compliance matrix as a living document, and allocate engineering time for prompt adjustments as part of every model migration. The teams that maintain this discipline deliver consistent multilingual quality. The teams that test once and assume stability discover, months later, that a model update quietly degraded their Korean compliance from 85 percent to 68 percent, and nobody noticed until Korean users started leaving.

The next subchapter addresses a specific technique for improving instruction compliance in non-English contexts: few-shot examples, including how to select them, which language they should be in, and how to manage the token budget tradeoffs they create.