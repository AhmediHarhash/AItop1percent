# Chapter 11 — Fine-Tuning and Adaptation for Multilingual Systems

Multilingual fine-tuning is not the same problem as monolingual fine-tuning with more data. Cross-lingual transfer means that fine-tuning on one language affects performance on every other language the model supports, sometimes improving languages you never trained on and sometimes destroying capabilities you assumed were stable. Catastrophic forgetting hits multilingual models harder because the capability surface is larger — every language, every task, every cultural nuance is a dimension that can degrade silently. Data imbalance between high-resource and low-resource languages creates quality cliffs that standard training recipes do not address, and most teams discover those cliffs only after deployment.

---

- **11.1** — When Multilingual Fine-Tuning Is Necessary vs When Prompting Suffices
- **11.2** — Cross-Lingual Transfer: How Fine-Tuning One Language Affects Others
- **11.3** — The Catastrophic Forgetting Problem in Multilingual Models
- **11.4** — Data Collection for Low-Resource Languages: Strategies and Pitfalls
- **11.5** — Synthetic Data Generation for Multilingual Training
- **11.6** — PEFT Methods for Multilingual Adaptation: LoRA, Language-Specific Adapters
- **11.7** — Model Merging for Cross-Lingual Capability Transfer
- **11.8** — Multilingual Training Data Balancing: Ratios, Curricula, and Sampling
- **11.9** — Evaluation Gating for Multilingual Fine-Tuned Models
- **11.10** — Distillation for Multilingual Small Models
- **11.11** — The Multilingual Fine-Tuning Decision Tree

---

*Fine-tuning a multilingual model without cross-lingual eval is not optimization — it is gambling with every language your users depend on.*
