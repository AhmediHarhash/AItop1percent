# 4.10 â€” Building the Multilingual Eval Dashboard: Per-Language, Per-Dimension Tracking

Most teams have one number for quality. One metric, one dashboard tile, one green-yellow-red indicator that tells leadership everything is fine or everything is not. For a monolingual product, a single number is reductive but tolerable. For a multilingual product, a single number is actively dangerous. It hides the most important information your team needs: which languages are degrading, which dimensions are failing, and whether the gap between your strongest and weakest languages is growing or shrinking.

A single quality score of 82 percent across twelve languages might mean all twelve languages are performing at roughly the same level. It might also mean English is at 94 percent, French and German are at 88 percent, and Thai, Vietnamese, and Arabic are at 61 percent. The average looks acceptable. Three markets are in crisis. The team that trusts the single number deploys with confidence. The team that sees the matrix deploys with precision.

## Why a Matrix, Not a Number

The multilingual eval dashboard is a matrix because multilingual quality is inherently two-dimensional. One axis is language. The other axis is quality dimension -- fluency, accuracy, cultural fit, completeness, format compliance, cross-lingual consistency. Every intersection of language and dimension tells a distinct story.

Korean fluency at 4.1 out of 5 with Korean accuracy at 68 percent means the model produces beautiful Korean that contains factual errors. Thai accuracy at 89 percent with Thai cultural fit at 2.4 out of 5 means the model gets the facts right but delivers them in ways that feel foreign to Thai users. Arabic format compliance at 63 percent with Arabic fluency at 4.3 means the model writes elegant Arabic but breaks structured output formats -- a problem caused by right-to-left text interacting with left-to-right formatting tokens.

Each of these intersections points to a different root cause and a different intervention. A single composite number cannot distinguish between them. An engineering team that sees "Korean quality is 74 percent" has no actionable path forward. An engineering team that sees "Korean accuracy is 68 percent while Korean fluency is 4.1" knows exactly where to focus: the model's factual knowledge in Korean, not its language generation capability.

The dashboard exists to turn ambiguous quality signals into specific engineering tasks.

## Dashboard Structure

The core view of the dashboard is a table. Rows represent languages, ordered by tier and then by traffic volume within each tier. Columns represent quality dimensions: fluency, accuracy, cultural fit, completeness, format compliance, and cross-lingual consistency. Each cell contains three pieces of information: the current score, the trend direction over the last four evaluation cycles, and the distance from the target threshold.

**Current score** is the most recent evaluation result for that language on that dimension. For fluency and cultural fit, this is typically a rating on a five-point scale. For accuracy and completeness, this is a percentage. For format compliance, this is a pass rate. For cross-lingual consistency, this is a percentage of parallel test cases where the language produced substantively equivalent output to the English reference.

**Trend direction** is one of three states: improving, stable, or declining. Calculate trend by comparing the current score to the trailing four-cycle average. A difference of more than one standard deviation in the positive direction is improving. More than one standard deviation in the negative direction is declining. Within one standard deviation is stable. Display trends as simple arrows or directional indicators -- the reader should grasp the trajectory in a glance.

**Distance from threshold** is the gap between the current score and the minimum acceptable score for that language-dimension pair. A positive distance means the score exceeds the threshold -- quality is above the bar. A negative distance means the score is below threshold -- quality has failed and requires action. Color-code cells based on this distance: green for scores comfortably above threshold, yellow for scores within 10 percent of threshold, red for scores below threshold. The color pattern across the matrix tells the quality story before anyone reads a single number.

Below the matrix, include a row of weighted composite scores -- one per language. This composite applies your use-case-specific dimension weights to produce a single per-language quality number. The composite is useful for executive reporting and high-level prioritization, but it should always be expandable to its component dimensions. A composite that drops from 80 to 73 should immediately reveal which dimension drove the decline.

## Per-Language Trend Tracking

Current scores are snapshots. Trends are narratives. A language at 78 percent accuracy that has been climbing from 65 percent over four months is a success story. A language at 78 percent accuracy that has been sliding from 86 percent over the same period is an emerging crisis. The numbers are identical. The trajectories are opposite.

Your dashboard should store and display historical data for every language-dimension pair going back at least six months. Weekly data points for languages in active development, monthly for stable languages. The historical view enables three critical capabilities.

First, regression attribution. When a score drops, you need to identify which change caused it. Overlaying quality trend data with deployment timestamps shows exactly when the regression began. If Korean accuracy was stable at 82 percent for six weeks, then dropped to 71 percent in the week after a system prompt update, the prompt update is your prime suspect. Without historical trend data, you are guessing at causes.

Second, improvement validation. When your team invests in improving a specific language-dimension pair, the trend data confirms whether the improvement is real and durable. A score that jumps from 68 to 79 after a prompt change and then slides back to 72 over the following three weeks suggests the improvement was superficial -- the prompt change addressed symptoms rather than root causes. A score that jumps and holds indicates a genuine fix.

Third, drift detection. Model quality drifts over time even without any changes on your side. API providers update their models, retrain on new data, adjust safety classifiers. These external changes can shift your quality scores without any deployment on your end. Continuous trend tracking catches this drift. A gradual decline of one or two points per month across multiple languages may indicate a provider-side model update. Without trend data, this slow degradation goes unnoticed until the cumulative decline becomes severe.

## Alert Conditions

A dashboard that must be manually checked is a dashboard that gets ignored. Your multilingual eval dashboard needs active alerting -- automated notifications that surface problems before they become crises.

Configure alerts for four conditions.

**Threshold breach.** Any language-dimension pair drops below its minimum acceptable threshold. This is the most basic alert and the most urgent. A threshold breach means quality in that language on that dimension has fallen below the bar your team has set. The alert should include the current score, the threshold, the trend direction, and a link to the specific failing test cases. Route threshold breach alerts to the engineering team responsible for that language tier.

**Rapid decline.** Any language-dimension pair drops more than two standard deviations from its trailing average within a single evaluation cycle. This catches sudden regressions even when the score is still above threshold. A language at 88 percent that drops to 79 percent in one cycle has not breached a threshold of 75 percent, but the decline is abnormal and warrants investigation. Rapid decline alerts catch regressions early, before they compound.

**Cross-language divergence.** The gap between your highest-performing and lowest-performing language on any dimension exceeds a specified range. If your best language on accuracy is at 93 percent and your worst is at 59 percent, the 34-point gap means your users are receiving dramatically different quality depending on their language. Divergence alerts surface equity problems -- situations where some user populations are underserved relative to others.

**Stale data.** Any language-dimension pair has not been evaluated within its scheduled cadence. If a Tier 1 language is supposed to be evaluated weekly and the last data point is three weeks old, something has broken in the evaluation pipeline. Stale data alerts prevent false confidence -- the absence of bad news is not good news if you have not checked recently.

## Drill-Down: From Dashboard to Root Cause

The dashboard is the starting point of investigation, not the endpoint. When a cell turns red, the team needs to drill down from the aggregate score to the specific failing cases to the root cause.

The first drill-down level shows the individual test cases that make up the score for that language-dimension pair. Sort them by score, worst first. The team sees exactly which questions, which scenarios, which capability areas are failing. If Korean accuracy is at 68 percent, the drill-down might reveal that the model is accurate on general knowledge questions but failing on financial domain questions -- a much more specific and actionable finding than "Korean accuracy is low."

The second drill-down level shows the model's actual output for a failing test case alongside the expected output and the evaluation rubric. The team can read the response, compare it to the reference, and identify the nature of the failure. Is the model hallucinating facts? Omitting key information? Providing information from the wrong jurisdiction? The specific failure pattern determines the intervention.

The third drill-down level shows historical performance on that specific test case. Has it always been wrong, or did it recently regress? If it recently regressed, which deployment corresponds to the regression? This level connects quality failures to engineering changes, completing the diagnostic loop.

Build these three drill-down levels into the dashboard from the start. A dashboard that shows red cells but cannot explain why is a frustration tool, not a quality tool. The team that sees "Korean accuracy is failing on financial domain questions after the October 15 prompt update" has a clear path to resolution. The team that sees "Korean accuracy is 68 percent" has a meeting.

## Comparison Views: Before and After, Model vs Model

Beyond real-time monitoring, the dashboard should support comparison views that show quality changes across two points in time or two system configurations.

**Before-and-after comparison.** When the team makes a change -- a prompt update, a model swap, a retrieval modification -- the comparison view shows every language-dimension pair side by side: score before the change and score after. Cells where quality improved are highlighted in one color. Cells where quality degraded are highlighted in another. The comparison view makes the cross-lingual impact of any change instantly visible.

Without this view, the team must mentally reconstruct the before-and-after picture from trend data, which is error-prone and time-consuming. With it, the product manager can look at a single screen and say "the prompt change improved English accuracy by 5 points, held French and German steady, and degraded Korean cultural fit by 8 points." That sentence takes five seconds to produce from a comparison view and thirty minutes to produce from raw trend data.

**Model-versus-model comparison.** When evaluating a model upgrade or a provider switch, run your full eval suite against both the current model and the candidate. Display the results side by side. This comparison view is essential for model selection decisions in multilingual products, because the overall benchmark numbers published by model providers rarely reflect your specific language mix and domain. A model that benchmarks 3 percent better overall might benchmark 6 percent worse in your three most important non-English languages. The model-versus-model view catches these hidden trade-offs before the switch happens.

Build comparison views as standard dashboard features, not as ad-hoc analyses. Every change to your system should trigger a comparison. The cost of generating comparison data is marginal -- you are already running evaluations. The cost of not comparing is decisions made on incomplete information.

## The Weakest Language View

Your system's quality is not your average language score. It is your weakest language score. A product that performs brilliantly in eight languages and terribly in two is a product that fails 20 percent of its user base. The users in those two languages do not experience your average quality. They experience the worst quality.

The weakest language view is a dashboard perspective that sorts languages by their composite score, worst first, and highlights the bottom performers. This view answers the question that matters most for user equity: which users are getting the worst experience, and how much worse is it compared to the best?

Display the weakest language view alongside the system-wide average. If the average is 83 percent and the weakest language is at 59 percent, the 24-point gap is your quality equity problem. Set organizational targets not just for average quality but for minimum quality -- no language should fall below a certain floor, regardless of its traffic volume. A language that represents 2 percent of your traffic still represents real users who deserve a functional product.

The weakest language view also reveals resource allocation priorities. Investment in improving your weakest language from 59 to 72 percent has more impact on overall user experience than improving your strongest language from 91 to 94 percent. The marginal return on quality investment is highest at the bottom of the distribution, not the top. Teams that optimize for average quality tend to polish languages that are already good while neglecting languages that are struggling. The weakest language view corrects this bias by making the struggling languages impossible to ignore.

## Stakeholder Views

Different stakeholders need different views of the same underlying data. A single dashboard layout that works for everyone does not exist. Build at least three views.

**The engineering view** shows the full matrix with all dimensions, all languages, drill-down capability, and trend data. Engineers need the most granular view because they are diagnosing specific problems and planning specific interventions. The engineering view should also show deployment markers on trend graphs so engineers can correlate quality changes with code changes.

**The product view** shows per-market quality -- aggregated by the markets your product serves rather than by raw language. A product manager for the Japanese market cares about Japanese quality across all dimensions but does not need to see Vietnamese data. The product view groups languages by market, shows the weighted composite score, and highlights dimensions that are below target. Product managers use this view to make feature-priority and market-investment decisions. If Japanese cultural fit has been declining for three months, the product manager needs to know -- and needs enough detail to justify resource allocation in the next planning cycle.

**The leadership view** shows the risk surface -- how many languages are above threshold, how many are at risk, how many are below threshold, and the trend in each category. Leadership does not need to know that Korean accuracy is 68 percent. Leadership needs to know that three of twelve supported languages are below quality threshold, two are trending downward, and the quality equity gap is widening. The leadership view should fit on a single screen and communicate the quality story in thirty seconds. Include a one-sentence status summary: "9 of 12 languages above threshold. Arabic and Thai below threshold. Korean trending down, approaching threshold."

**The compliance view** is relevant for products operating in regulated markets or under frameworks like the EU AI Act. The compliance view maps quality dimensions to regulatory requirements and highlights any language-dimension pair that falls below a compliance-relevant threshold. If the EU AI Act requires that your high-risk AI system demonstrate consistent performance across all populations it serves, the compliance view shows exactly which languages meet that requirement and which do not. Compliance officers use this view to prepare for audits, respond to regulator inquiries, and identify compliance gaps before they become enforcement actions.

The four views draw from the same data store and the same evaluation results. They differ only in aggregation, filtering, and presentation. Build the data pipeline once, build the views as lightweight presentation layers on top.

A common mistake is building only the engineering view and expecting other stakeholders to extract what they need. They will not. Product managers will stop checking a dashboard that requires them to mentally filter twelve languages and six dimensions to find their market's data. Leadership will stop checking a dashboard that requires them to scroll through hundreds of cells to assess overall risk. Each stakeholder needs their view built for them, optimized for the decisions they make. The investment in building multiple views is small compared to the cost of stakeholder disengagement.

## Connecting Eval Pipelines to the Dashboard

The dashboard is only as good as the data that feeds it. If evaluation results flow into the dashboard manually -- someone runs an eval, exports a spreadsheet, uploads the numbers -- the data will be stale, inconsistent, and eventually absent. The connection between your evaluation pipelines and your dashboard must be automated.

The architecture has three components. First, the evaluation pipeline itself -- the system that runs test cases against the model, collects responses, and computes scores. This pipeline should produce structured output: language, dimension, test case identifier, score, timestamp, model version, and any metadata needed for drill-down. Second, a data store that receives and indexes evaluation results. A time-series database or a structured data warehouse works well -- anything that supports efficient querying by language, dimension, time range, and model version. Third, the dashboard application that reads from the data store and renders the views described above.

The evaluation pipeline should run on its own schedule in addition to being triggered by deployments. Deployment-triggered evaluations catch regressions caused by your changes. Scheduled evaluations -- daily for Tier 1 languages, weekly for Tier 2 -- catch drift caused by external factors like provider model updates. Both feed into the same data store and appear on the same dashboard.

Automate the alert system as part of the data pipeline. When new evaluation results arrive in the data store, a post-ingestion process checks them against threshold and trend rules, fires alerts for any conditions that are met, and logs the alert status. The team should never need to manually check the dashboard to discover a quality problem. Problems should come to them.

## What the Dashboard Cannot Tell You

A dashboard reveals patterns. It does not explain causes. When Thai cultural fit drops from 3.4 to 2.7, the dashboard shows the decline. It cannot tell you that the decline happened because a new system prompt included American holiday references, or because a retrieval pipeline change surfaced English-language documents for Thai queries, or because the model provider's latest update degraded Thai cultural knowledge.

Diagnosis requires human investigation. The dashboard's role is to make the investigation efficient -- to point the team at the right language, the right dimension, the right time window, and the right failing cases. The drill-down capability described above is the bridge between "something is wrong" and "here is what went wrong."

Teams that treat the dashboard as a self-sufficient quality management system will be disappointed. Teams that treat it as the starting point for targeted investigation and the measurement system for confirming that interventions worked will find it indispensable.

## Dashboard Adoption: Making It Part of the Workflow

A dashboard that exists but nobody checks is infrastructure that costs money without providing value. Adoption requires more than building the tool. It requires embedding it into the team's operational rhythm.

Include dashboard review in your weekly engineering standup. Spend five minutes reviewing the quality matrix -- which cells changed color, which trends shifted, which alerts fired since the last meeting. This ritual creates accountability: when the team knows that last week's red cell will be discussed in this week's meeting, the cell gets investigated between meetings.

Include dashboard snapshots in deployment checklists. Before any deploy, the deploying engineer should review the most recent dashboard state and confirm that no Tier 1 languages are below threshold. After the deploy, the engineer should check the post-deployment evaluation results and confirm no new regressions. This takes two minutes and catches problems that would otherwise propagate.

Include dashboard summaries in monthly business reviews. When leadership sees multilingual quality data alongside revenue data, user growth data, and customer satisfaction data, quality becomes a business conversation rather than an engineering conversation. A market lead who sees that their language's quality score dropped 7 points last month will ask questions. Those questions create organizational pressure to invest in quality improvement. Without the dashboard in the business review, multilingual quality remains invisible to the people who allocate resources.

The dashboard succeeds when it becomes the shared language for multilingual quality discussions across the organization. When a product manager says "Thai cultural fit is in the yellow zone" and every engineer, every market lead, and every executive knows exactly what that means and where to look, the dashboard has achieved its purpose.

The next subchapter addresses a problem the dashboard will reveal but cannot solve on its own: the gaps in your evaluation coverage -- languages and dimensions where you have too few test cases to trust the scores the dashboard displays.