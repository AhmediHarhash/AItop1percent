# 11.2 — Cross-Lingual Transfer: How Fine-Tuning One Language Affects Others

In late 2024, a legal technology company fine-tuned Llama 3 70B on 12,000 examples of Japanese contract analysis. The results were impressive. Japanese legal summarization quality jumped from 58 to 87 on their internal eval, and the team celebrated a successful deployment. Three weeks later, their Korean enterprise clients started filing complaints. Contract summaries in Korean had become noticeably worse — missing key clauses, generating awkward phrasing, occasionally mixing in Japanese legal terms. The team had never evaluated Korean after the Japanese fine-tuning. When they finally did, Korean quality had dropped from 74 to 61. Chinese had fallen from 71 to 64. The Japanese fine-tuning had improved one language by pulling capability away from its closest linguistic neighbors.

This is not a bug. It is the fundamental physics of multilingual models, and any team that fine-tunes for one language without evaluating all others is building on sand.

## The Mechanism: Shared Representations in a Shared Space

To understand why fine-tuning Japanese affects Korean, you need to understand how multilingual models store language knowledge.

Modern large language models do not maintain separate internal modules for each language. Instead, they develop shared representations — patterns of neural activation that encode meaning across languages. The concept "breach of contract" has a representation in the model's internal space that is close to the Japanese, Korean, Chinese, Spanish, and French expressions of the same concept. This is by design. Shared representations are what allow a model trained predominantly on English to generate passable output in dozens of other languages. The model learns abstract semantic structures that transcend any single language, and it uses those structures to generalize.

The problem emerges when you fine-tune. When you update the model's weights to improve Japanese contract analysis, you are adjusting the very representations that are shared across languages. The gradient updates that push the model toward better Japanese legal output also shift the embedding space that Korean, Chinese, and other languages rely on. Think of it as a shared foundation beneath many buildings. Reinforcing the foundation under the Japanese wing changes the load distribution for every other wing.

The degree of impact depends on how much representational infrastructure the languages share. Languages that share writing systems, grammatical structures, or vocabulary tend to share more internal representations. Japanese and Korean share significant grammatical structure — both are subject-object-verb languages with similar agglutinative morphology. Chinese, Japanese, and Korean share a writing system heritage through Chinese characters. When you fine-tune on Japanese, the shared infrastructure with Korean and Chinese means those languages feel the shift most acutely.

## Positive Transfer: The Free Lunch That Sometimes Exists

Cross-lingual transfer is not always destructive. In some configurations, fine-tuning on one language improves performance on related languages you never trained on. This is positive transfer, and when it works, it is enormously valuable.

The mechanism is straightforward. When you fine-tune on Japanese legal text, you are teaching the model both Japanese-specific patterns and language-independent legal reasoning. The legal reasoning — how to identify key clauses, how to structure a summary, how to flag risk language — transfers to other languages because it lives in the shared representational layer. A team fine-tuning on Spanish customer support data found that Portuguese customer support quality improved by 12 points without a single Portuguese training example. The languages share enough vocabulary, grammar, and cultural context that Spanish fine-tuning effectively served as a noisy training signal for Portuguese.

Research presented at ACL 2025 demonstrated that language diversity in fine-tuning data improves cross-lingual transfer across the board. Training on more diverse language sets — mixing languages from different families and scripts — consistently produced better results on held-out languages compared to training on a single language or on closely related languages only. The finding suggests that diversity forces the model to learn more generalizable representations rather than language-specific shortcuts.

The positive transfer effect is strongest within language families. Romance languages transfer well to each other. Germanic languages transfer well to each other. Turkic languages transfer well to each other. A 2025 study on model merging for cross-lingual transfer showed that merging a Turkish-tuned model with a base instruction model yielded a 15.5 percent accuracy improvement on Turkish benchmarks and a 4.7 percent improvement on Malagasy — a language from an entirely different family — without any Malagasy training data. The researchers used only 16,700 Turkish examples and achieved gains that transferred across language family boundaries.

But positive transfer is not guaranteed, and counting on it without measurement is gambling.

## Negative Transfer: When One Language Cannibalizes Another

Negative transfer is the dark side, and it is more common than positive transfer in production settings. The pattern is consistent: fine-tuning on language A degrades performance on languages B through Z, with the degradation strongest in languages that share the most representational infrastructure with A.

The degradation follows a predictable hierarchy. Languages in the same script family suffer most. When you fine-tune heavily on Latin-script languages, CJK languages tend to degrade, and vice versa. Languages with similar grammar suffer more than languages with different grammar. Fine-tuning on one agglutinative language (like Turkish or Finnish) can degrade other agglutinative languages while leaving isolating languages (like Vietnamese or Mandarin) relatively unscathed.

A healthcare company serving Southeast Asia fine-tuned their model on Thai medical dialogue — 8,000 examples of patient-doctor conversation summaries. Thai quality jumped by 25 points. Vietnamese, which shares geographic proximity but belongs to a different language family and uses a Latin-derived script, dropped by only 3 points. But Lao, which is closely related to Thai linguistically and uses a similar Brahmic-derived script, dropped by 14 points. Burmese, another language using a Brahmic-derived script, dropped by 9 points. The closer the linguistic relationship, the sharper the degradation.

The most dangerous negative transfer is the kind that does not show up in your primary metrics. If you are evaluating the fine-tuned model only on the task you fine-tuned for — Japanese legal summarization, Thai medical dialogue — you see the improvement and nothing else. The degradation hides in languages and tasks you are not measuring. This is why cross-lingual evaluation is not optional. It is the only way to know whether your improvement in one language came at the expense of others.

## The Script Boundary

Script differences create a partial natural barrier to transfer — both positive and negative. Languages that share a script tend to share more internal representations, because the tokenizer processes them similarly and the model develops shared subword patterns. Languages on different sides of a script boundary share fewer representations, which means less positive transfer but also less negative interference.

This creates a practical principle: fine-tuning on Latin-script languages is less likely to catastrophically degrade CJK languages, and vice versa. Fine-tuning on Arabic-script languages is less likely to damage Devanagari-script languages. The barrier is not absolute — there is always some transfer across script boundaries because the shared semantic layer operates above the level of individual characters — but it is meaningful enough to inform your fine-tuning strategy.

The practical implication is that if you must fine-tune for multiple languages, grouping your fine-tuning by script family reduces the risk of cross-boundary degradation. Fine-tune your Latin-script languages together, your CJK languages together, your Arabic-script languages together. Each group benefits from positive within-group transfer while the script boundary provides some insulation against cross-group interference.

However, the script boundary is weaker in newer models. GPT-5, Claude Opus 4.6, and Gemini 3 Pro have been trained on more balanced multilingual data than their predecessors, and their shared semantic representations are deeper and more script-independent. This means more positive transfer across script boundaries — but also more negative transfer risk. The same architectural improvement that lets a model understand Korean concepts through Japanese fine-tuning also means that Japanese fine-tuning can more easily disrupt Korean representations.

## Measuring Transfer: The Transfer Matrix

You cannot manage what you do not measure, and for multilingual fine-tuning, what you need to measure is the **transfer matrix**.

The transfer matrix is a structured record of how every supported language's performance changes after each fine-tuning run. Before fine-tuning, you run your eval suite across all supported languages and record the baseline scores. After fine-tuning, you run the identical eval suite and record the new scores. The difference between post-fine-tuning and pre-fine-tuning scores, for each language, is the transfer effect.

A positive number means the language benefited from the fine-tuning. A negative number means it degraded. A number near zero means it was unaffected. The matrix has one row per fine-tuning run and one column per language, and over time it reveals patterns that inform every subsequent fine-tuning decision.

Building the transfer matrix requires that your eval suite covers every language you support, not just the language you are fine-tuning for. This is the part most teams skip. They have a robust eval suite for their fine-tuning target language and a skeleton eval — or no eval at all — for the others. When the transfer matrix has gaps, you are flying blind. The language you did not measure is the language that degraded the most.

The eval suite for non-target languages does not need to be as deep as the target language eval. A representative sample of 200 to 500 examples per language, covering your core tasks, is enough to detect meaningful degradation. The point is not to comprehensively evaluate every language after every fine-tuning run. The point is to catch drops larger than five points before they reach production.

## The Language Family Effect

Language families are your strongest predictor of transfer direction and magnitude.

Romance languages — Spanish, French, Italian, Portuguese, Romanian — share vocabulary, grammar, and discourse patterns. Fine-tuning on Spanish produces the strongest positive transfer to Portuguese (typically 8 to 15 points on task-specific metrics), followed by Italian and French (5 to 10 points), with decreasing transfer to Romanian and Catalan. The same family also means the highest negative transfer risk if you over-specialize: heavy fine-tuning on formal Castilian Spanish can degrade Latin American Spanish variants.

Sino-Tibetan languages show strong transfer between Mandarin and Cantonese but weaker transfer to Tibetan and Burmese. Japanese, despite extensive Chinese character borrowing, belongs to a different language family (Japonic), and the transfer pattern is intermediate — strong for kanji-heavy vocabulary, weak for grammar.

Turkic languages — Turkish, Azerbaijani, Uzbek, Kazakh — show remarkably strong positive transfer. Fine-tuning on Turkish can improve Azerbaijani by 10 to 20 points because the languages are mutually intelligible in many registers. This makes Turkic languages an ideal case for single-language fine-tuning with multi-language benefit.

Indo-Aryan languages — Hindi, Bengali, Marathi, Gujarati, Punjabi — also transfer well within the family, but the Devanagari-to-other-script transition (Bengali uses a different script from Hindi) introduces a partial barrier that reduces transfer magnitude by roughly 30 to 40 percent compared to same-script pairs.

Knowing these patterns lets you design fine-tuning strategies that maximize positive transfer and minimize negative interference. If you need to improve both Turkish and Azerbaijani, fine-tune on Turkish alone and measure. If you need to improve both Thai and Vietnamese, you probably need separate fine-tuning because the languages are too distant for reliable positive transfer.

## Designing for Transfer

Armed with the transfer matrix and language family knowledge, you can make fine-tuning decisions that account for cross-lingual effects.

The first principle is to fine-tune on the highest-resource member of a language family first, then measure transfer to the rest. If the positive transfer is sufficient, you avoid fine-tuning on the lower-resource languages entirely. This saves data collection cost, training cost, and maintenance cost.

The second principle is to include a small proportion of non-target language data in every fine-tuning batch. If you are fine-tuning on Japanese, include 10 to 15 percent Korean and Chinese examples. This does not need to be task-specific data — general-quality examples in those languages are enough to maintain the shared representations. Research on language-diverse fine-tuning from EMNLP 2025 confirmed that even small amounts of diverse language data during fine-tuning significantly reduce negative transfer to non-target languages.

The third principle is to set transfer thresholds before you start. Decide in advance how much degradation you will tolerate in each non-target language. A five-point drop in a language that represents 2 percent of your traffic might be acceptable. A five-point drop in a language that represents 25 percent of your traffic is not. These thresholds are business decisions, not engineering decisions, and they should be agreed upon with product and business stakeholders before fine-tuning begins.

The fourth principle is to use parameter-efficient fine-tuning methods that limit the scope of weight changes. We will explore these methods in detail in subchapter 11.6, but the preview is that techniques like LoRA and language-specific adapters can reduce cross-lingual interference by restricting which weights are updated during fine-tuning.

## What Your Team Should Do Monday

If you are planning a multilingual fine-tuning project, start by building your transfer matrix. Run your eval suite across all supported languages. Record the baselines. Then, after any fine-tuning run, re-run the full suite and record the changes. Make the matrix a required artifact in your fine-tuning pipeline — no fine-tuned model ships to production without a completed transfer matrix showing the impact on every supported language.

Set your transfer thresholds. Define the maximum acceptable degradation for each language, weighted by business importance. A model that improves Japanese by 25 points but degrades Korean by 15 points might be a bad trade if Korean is a larger market. A model that improves Spanish by 20 points and degrades Portuguese by 3 points is probably a good trade regardless.

And if you are early in your multilingual journey, remember: cross-lingual transfer is a feature, not just a risk. The same mechanism that can degrade your Korean quality can also improve it for free, if you design your fine-tuning strategy to harness positive transfer rather than stumbling into negative transfer. The difference is measurement, planning, and treating every language as a stakeholder in every fine-tuning decision.

The next subchapter addresses the most severe form of negative transfer — catastrophic forgetting — where fine-tuning does not just degrade other languages but effectively erases them from the model's capabilities.
