# 4.11 â€” Eval Coverage Gaps: Detecting Which Languages Are Under-Tested

How do you know which languages are under-tested? You count. You open your evaluation data, and for each language, you count the number of eval cases per quality dimension. Then you stare at the numbers and accept what they tell you. Most teams that perform this audit for the first time discover a coverage landscape that is wildly uneven -- hundreds of test cases for English, a respectable set for their top two or three languages, and single digits for everything else. Some dimensions have zero coverage in some languages. Zero. Not low coverage. Zero test cases. Zero data points. Zero basis for any quality claim.

A quality score for a language-dimension pair with five test cases is not a measurement. It is a guess wearing a number's clothing. And if your dashboard shows Thai cultural fit at 80 percent based on five eval cases, your team might believe Thai cultural fit is strong. In reality, you have no idea. The five cases might all test the same narrow scenario. They might miss every cultural norm that Thai users actually encounter. The 80 percent gives your team false confidence and your Thai users a product that nobody has seriously evaluated.

## The Coverage Audit

The coverage audit is a structured inventory of your evaluation assets, organized by language and quality dimension. It answers two questions: how many eval cases do you have, and how much of the quality surface do those cases actually cover?

Start with a simple count. Build a table with one row per language and one column per quality dimension. In each cell, write the number of eval cases you have for that combination. This count alone is revealing. A typical result for a team supporting twelve languages might look something like this: English has 250 cases for accuracy, 180 for fluency, 120 for cultural fit, 150 for completeness, 90 for format compliance, and 100 for cross-lingual consistency. French has 80 for accuracy, 60 for fluency, 40 for cultural fit, 50 for completeness, 30 for format compliance, and 45 for consistency. Thai has 22 for accuracy, 15 for fluency, 8 for cultural fit, 12 for completeness, 6 for format compliance, and 10 for consistency. Bengali has 7 for accuracy, 4 for fluency, 0 for cultural fit, 3 for completeness, 2 for format compliance, and 0 for consistency.

The pattern is universal. Coverage declines steeply as you move from high-resource to low-resource languages. Cultural fit is the most under-covered dimension because it requires native-speaker expertise to design test cases. Format compliance is under-covered because teams assume it transfers from English. Cross-lingual consistency is under-covered because it requires parallel test infrastructure that most teams have not built.

But count alone is insufficient. The second layer of the audit examines coverage breadth. Twenty accuracy test cases that all ask factual questions about geography provide twenty data points on one narrow capability. They tell you nothing about accuracy on medical questions, financial questions, legal questions, or any other domain your product serves. Coverage breadth measures how many distinct scenarios, topics, and capability areas your test cases span.

Map your eval cases to a taxonomy of scenarios relevant to your product. If your product is a customer support assistant, your scenario taxonomy might include account management, billing inquiries, technical troubleshooting, product returns, policy questions, and complaint handling. For each language, count how many scenarios have at least one test case. A language with 40 eval cases that cover all six scenario categories has better coverage breadth than a language with 60 eval cases that only cover three categories.

## Common Coverage Patterns

Three patterns appear in nearly every coverage audit. Recognizing them helps you prioritize where to invest.

**The cliff pattern.** Coverage drops off a cliff after the first few languages. English is thoroughly covered. The next two or three languages have reasonable coverage, typically 30 to 50 percent of English coverage. Every language after that has coverage in single digits as a percentage of English. The cliff usually corresponds to the languages that were included in the initial product launch -- they received evaluation investment during development. Languages added later were launched with minimal evaluation because the team was under time pressure and assumed that model capabilities would generalize.

**The cultural fit desert.** Cultural fit has the lowest coverage of any dimension across all languages, including English. Cultural fit test cases require deep cultural expertise to design -- you cannot generate them from a template or translate them from English. They require someone who understands the social norms, communication expectations, and sensitivity landscape of each culture. This expertise is expensive and hard to source, so teams defer building cultural fit cases indefinitely. The result is that the dimension most likely to cause user alienation is the dimension least likely to be measured.

**The format compliance assumption.** Format compliance is tested thoroughly in English and assumed to transfer to other languages. Teams reason that structured output requirements are language-independent -- if the model can produce a numbered list in English, it can produce one in Arabic. In practice, format compliance degrades significantly for languages with non-Latin scripts, right-to-left text direction, or complex character compositions. Arabic, Hebrew, Thai, Japanese, and Korean all exhibit format compliance failures that English testing would never reveal. A team that skips format compliance testing in these languages discovers the problem when their downstream parser breaks on production output.

## The Danger of Untested Dimensions

A dimension with zero eval cases is not a dimension with unknown quality. It is a dimension where quality is uncontrolled. You have no feedback loop, no regression detection, no improvement mechanism. The model's behavior on that dimension in that language is whatever the model happens to do, and nobody is checking.

Consider the concrete example of cultural fit for Arabic. If you have no cultural fit eval cases for Arabic, you have no idea whether your system handles references to Islamic holidays appropriately, whether it uses the right level of formality in professional contexts, whether it avoids topics that are culturally sensitive in Arabic-speaking markets, or whether its examples and analogies reflect the lived experience of Arabic-speaking users. Your system might be handling all of these perfectly. It might be violating cultural norms on every response. You do not know. You cannot know. And you cannot improve what you do not measure.

The danger compounds over time. Every model update, every prompt change, every retrieval modification could be shifting cultural fit behavior in Arabic. Without eval cases, you have no way to detect these shifts. A regression that would trigger an alert if you had coverage goes completely unnoticed. Over months, small regressions accumulate. By the time a user complaint or a market review surfaces the problem, the model's behavior in that dimension may have drifted far from acceptable, and the team has no baseline data to compare against.

Untested dimensions are also the dimensions where the model is most likely to fail, precisely because the absence of testing means the absence of feedback that would drive improvement. Dimensions you test improve because test results guide prompt engineering, few-shot example selection, and fine-tuning priorities. Dimensions you do not test stagnate or decay because nothing in your development process pushes them toward better performance.

## Using Production Data to Reveal Coverage Gaps You Did Not Know You Had

The coverage audit described above examines your existing eval cases. But the most dangerous coverage gaps are the ones where you do not even have a category to count against -- scenarios your eval suite does not consider because your team did not anticipate them.

Production query logs reveal these blind spots. Sample a thousand queries per language from production and categorize them by topic, intent, and complexity. Then compare the distribution of real user queries against the distribution of your eval cases. The mismatches are your hidden coverage gaps.

A healthcare assistant team performed this exercise for their Hindi eval suite and discovered that 28 percent of Hindi queries concerned Ayurvedic remedies and their interactions with prescribed medications. Their eval suite had zero cases testing this scenario. The model was responding to these queries -- 280 times per thousand Hindi queries -- with zero quality measurement. Some responses were likely helpful. Some were likely dangerous. Nobody knew which, because the eval suite was built from an English template that never imagined the query pattern.

Another team supporting Arabic for a legal information product found that 19 percent of Arabic queries concerned Islamic finance principles -- profit-sharing, interest-free lending, Sharia compliance. Their eval suite tested general financial literacy concepts imported from English. The Arabic-specific financial knowledge that their users most needed was entirely unmeasured.

Production data analysis should run quarterly. Each cycle compares the evolving distribution of real queries against the evolving inventory of eval cases. New query patterns that emerge -- seasonal topics, responses to regulatory changes, new product features -- should generate new eval cases within one evaluation cycle. If your team waits longer, the gap between what users ask and what you test widens, and your quality scores become increasingly disconnected from user experience.

## Coverage Targets by Language Tier

Set explicit minimum coverage targets for each language tier and each quality dimension. These targets create accountability and make coverage gaps actionable rather than merely observable.

**Tier 1 languages** -- your top three to five languages -- need 100 or more eval cases per dimension. At this level, scores are statistically meaningful, trends are detectable, and regressions are reliably caught. Aim for coverage breadth of 80 percent or more of your scenario taxonomy in each dimension. Tier 1 coverage should be comparable to your English coverage. If English has 200 accuracy cases spanning fifteen scenario categories, French and Japanese should have at least 100 accuracy cases spanning twelve or more scenario categories.

**Tier 2 languages** -- your next five to ten languages -- need 50 or more eval cases per dimension. This level provides meaningful scores and catches major regressions, though trend detection is less precise than at Tier 1 levels. Aim for coverage breadth of 60 percent or more of your scenario taxonomy.

**Tier 3 languages** -- the remainder -- need a minimum of 20 eval cases per dimension. Below 20, statistical noise dominates the signal. A score computed from fewer than 20 cases can swing 15 or more points based on a single test case outcome, making it unreliable for decision-making. Twenty cases is the floor below which your quality number is not a quality number -- it is random variation pretending to be measurement.

For any language-dimension pair below the Tier 3 minimum of 20 cases, mark it as "insufficient coverage" on your dashboard rather than displaying a score. Displaying a score computed from seven test cases gives the impression of measurement where none exists. Displaying "insufficient coverage" makes the gap visible and creates pressure to close it.

## Prioritizing Gap Closure

You cannot close every coverage gap simultaneously. Resources are finite, and some gaps matter more than others. Prioritize based on three factors.

**Traffic volume.** A coverage gap in a language that handles 100,000 queries per day affects more users than a gap in a language that handles 500 queries per day. Close high-traffic gaps first. Every day you operate without adequate coverage in a high-traffic language is a day you are flying blind at scale.

**Risk exposure.** A coverage gap in a safety-critical dimension -- accuracy in a medical product, cultural fit in a market with strict cultural norms, format compliance for a language that feeds downstream systems -- carries more risk than a coverage gap in a low-stakes dimension. A zero-coverage cultural fit gap in Arabic for a product serving Saudi Arabia is a higher priority than a low-coverage fluency gap in Portuguese for a product serving casual users.

**Regulatory requirement.** Some coverage gaps are not just risky but non-compliant. The EU AI Act requires that high-risk AI systems be tested for bias and quality across the populations they serve. If your product operates in an EU market and you have zero eval coverage for a quality dimension in the language of that market, you have a compliance gap as well as a quality gap. Regulatory coverage gaps should be closed before any product launch or audit deadline, regardless of traffic volume.

Build a prioritized backlog of coverage gaps. Each item specifies the language, the dimension, the current case count, the target case count, and the estimated effort to close the gap. A cultural fit gap in Arabic that requires going from 0 to 50 cases needs native-Arabic cultural experts and will take four to six weeks. A format compliance gap in Korean that requires going from 12 to 50 cases can be partially automated using template generation and will take one to two weeks. The backlog makes coverage investment plannable and trackable.

## Practical Approaches to Building Coverage Quickly

Closing coverage gaps requires eval cases, and building eval cases requires expertise, time, and budget. Here are the practical strategies teams use to build coverage efficiently without sacrificing quality.

**Native-speaker crowdsourcing.** For languages where you do not have in-house expertise, contract native-speaker evaluators through annotation platforms or language service providers. Give them a clear brief: the product domain, the scenario taxonomy, the quality dimensions, and examples of strong eval cases from your English or Tier 1 suite. Ask them to write eval cases -- not translate English cases, but create native-language cases that reflect the scenarios real users in their market would encounter. A skilled native-speaker contractor can produce 15 to 25 high-quality eval cases per day. Two weeks of contracted work produces enough cases to bring a language from zero to Tier 3 coverage across most dimensions.

**Production failure mining.** When users in a specific language report problems -- through support tickets, feedback forms, or satisfaction surveys -- convert each reported failure into an eval case. The failure is already a validated quality signal: a real user encountered a real problem in a real scenario. Turning it into an eval case preserves the signal and adds it to your regression suite. Over months, production failure mining builds a coverage set that is naturally aligned with the problems your users actually experience.

**LLM-assisted case generation with human validation.** Use a strong multilingual model to generate candidate eval cases for under-covered language-dimension pairs. Provide the model with your scenario taxonomy, your existing eval cases as examples, and instructions to generate culturally appropriate cases in the target language. Then have native speakers review and validate each generated case, keeping the ones that are realistic and well-formed, discarding the ones that are culturally off or linguistically awkward. This hybrid approach produces cases three to five times faster than fully manual creation, at the cost of validation effort. The key constraint: never put LLM-generated cases into your eval suite without native-speaker validation. An eval case that contains cultural mistakes or unnatural language contaminates your measurement.

**Cross-team sharing.** If your organization has multiple products serving the same languages, share eval infrastructure. A customer support product and an e-commerce product serving Japanese users have overlapping eval needs -- both need Japanese fluency cases, Japanese cultural fit cases, and Japanese format compliance cases. The domain-specific cases differ, but the language-quality cases can be shared, reducing the total investment needed across the organization.

## The Eval Debt Concept

Coverage gaps are a form of debt. This book calls it **eval debt** -- the accumulated deficit between the evaluation coverage you have and the evaluation coverage you need. Like technical debt, eval debt compounds. Like technical debt, it is easy to accumulate and painful to pay down. Unlike technical debt, eval debt is invisible until something breaks.

Eval debt accumulates in predictable ways. Every new language you launch without full evaluation coverage adds debt. Every new product feature you ship without updating eval cases in all languages adds debt. Every model update you deploy without verifying quality across all dimensions adds debt. The debt grows with every change, because each change could have introduced a quality problem that your insufficient coverage failed to detect.

The compound interest on eval debt is undetected regressions. Each undetected regression makes your product slightly worse for users in that language. Over months, the accumulated regressions can make the product significantly worse than when it launched -- even though every individual change seemed harmless. The team has no record of the degradation because there was no measurement to record it. When the problem finally surfaces through user complaints or a market review, the team discovers that quality has drifted far from acceptable, and rebuilding requires far more effort than maintaining would have cost.

The way to manage eval debt is the same as technical debt: acknowledge it, track it, and pay it down deliberately. Add a coverage metric to your dashboard that shows the current case count versus the target for every language-dimension pair. Track the total debt -- the sum of all gaps between current and target coverage -- as an organizational metric. Set quarterly targets for debt reduction. Celebrate when coverage targets are met. Make eval debt visible to leadership so that coverage investment competes fairly with feature development for resources.

## Monitoring Coverage Over Time

Coverage is not a one-time achievement. It is a continuous metric that should increase over time and never decrease without a deliberate decision.

Several forces erode coverage. Product evolution adds new capabilities that existing eval cases do not test. A customer support assistant that adds a new billing dispute resolution feature needs new eval cases for that feature in every language -- and if the eval team does not build them, the feature launches with zero coverage in every language. Model updates can make existing eval cases less relevant if the model's failure modes shift. An eval case designed to catch a specific error pattern may no longer trigger that error after a model update, reducing the effective coverage even though the case count stays the same.

Track two coverage metrics over time. Absolute coverage -- the raw count of eval cases per language per dimension -- should trend upward as your team builds new cases. Effective coverage -- the percentage of eval cases that are still relevant and discriminating -- should be monitored through periodic case audits. An eval case that has passed at 100 percent for twelve consecutive evaluation cycles may no longer be testing a relevant failure mode. It is not wrong, but it is consuming evaluation resources without providing signal. Replace it with a case that tests a current failure mode.

Set a coverage growth target. A reasonable target is 10 to 20 percent coverage growth per quarter, concentrated in the highest-priority gaps identified by your prioritized backlog. At this rate, a team starting with 30 percent of ideal coverage reaches 80 percent within two years. The growth rate is slow enough to be sustainable alongside regular product development and fast enough to materially reduce eval debt.

Review coverage metrics in the same cadence as your quality metrics. If your team reviews quality scores monthly, review coverage metrics monthly. Coverage and quality are complementary: quality scores tell you how the product is performing where you are measuring. Coverage metrics tell you how much of the quality surface you are actually measuring. A product with high quality scores and low coverage is not a high-quality product. It is a product with high quality in the areas it happens to be testing and unknown quality everywhere else.

One useful practice is the **new feature coverage check**. Whenever the product ships a new capability -- a new query type, a new output format, a new domain -- the eval team should immediately assess whether eval cases exist for that capability in every supported language. If they do not, the feature has launched with zero coverage in those languages. Add the missing cases to the coverage backlog with high priority. The longer a feature runs without eval coverage, the more production data accumulates without quality measurement, and the more potential regressions go undetected.

Another practice is **sunset auditing** -- removing eval cases that are no longer relevant. If your product deprecated a feature, the eval cases for that feature should be retired. If a model update fundamentally changed a failure pattern, the eval cases designed to catch the old pattern should be replaced with cases targeting the new pattern. Stale eval cases inflate your coverage count without providing signal. A team with 150 cases that includes 30 stale cases has an effective coverage of 120. Tracking effective coverage separately from absolute coverage keeps the count honest.

## The Coverage Confidence Relationship

There is a direct relationship between coverage depth and the confidence you can place in your quality scores. This relationship is not linear -- it follows a diminishing-returns curve that has practical implications for how much coverage to build.

At zero cases, confidence is zero. You know nothing. At 10 cases, you can detect catastrophic failures but not subtle regressions. A 10-case score can swing by 10 percent based on a single case outcome. At 20 cases, you cross the minimum viable measurement threshold. Scores become somewhat stable, and large regressions are reliably detected. At 50 cases, trends become meaningful. You can detect a 5-point regression with reasonable confidence over two evaluation cycles. At 100 cases, you have robust measurement. Scores are stable, trends are clear, and regressions of 3 points or more are reliably caught within a single evaluation cycle. Beyond 200 cases, additional coverage adds marginal confidence for most dimensions -- the incremental value of the 201st case is much smaller than the incremental value of the 21st case.

This curve means that coverage investment should be concentrated at the bottom of the distribution. Moving Bengali from 4 cases to 24 cases -- crossing the minimum viable threshold -- provides more value per dollar than moving French from 100 cases to 150 cases. The diminishing-returns curve argues for breadth-first coverage: bring every language above the minimum threshold before deepening coverage in languages that are already adequately covered.

The exception is safety-critical dimensions in high-traffic languages. Accuracy in a medical product serving Japanese users warrants coverage well beyond 100 cases, because the cost of an undetected accuracy regression in that context is severe enough to justify the diminishing-returns investment.

## Closing the Loop

Eval coverage gaps are the silent vulnerability in every multilingual AI system. They allow quality problems to persist undetected, regressions to accumulate unchallenged, and teams to operate with false confidence about languages they have barely tested. The coverage audit, the prioritized backlog, and the coverage growth targets described in this subchapter provide the mechanism for closing those gaps systematically.

The discipline of coverage management connects everything in this chapter: the native-language eval sets from subchapter 4.2, the dimension-specific metrics from subchapter 4.7, the regression testing from subchapter 4.9, and the dashboard from subchapter 4.10. Without adequate coverage, none of those systems work. With it, they form a complete evaluation infrastructure that tells you the truth about your product's quality in every language you serve. The coverage audit is not a one-time exercise. It is a recurring discipline that ensures your measurement infrastructure grows alongside your product. A team that audits coverage quarterly, closes gaps deliberately, and tracks eval debt as a first-class metric will always know which languages it can trust and which languages are operating on faith.

The next chapter shifts from measurement to action -- the translation, localization, and cultural adaptation practices that determine whether your multilingual product merely works in other languages or genuinely belongs in other cultures.