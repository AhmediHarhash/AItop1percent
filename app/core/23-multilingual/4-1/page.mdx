# 4.1 â€” Why Translated Evals Produce Misleading Results

The fastest way to build a multilingual eval suite is to translate your English evals. It is also the fastest way to get misleading results. The translation approach is seductive because it is cheap, fast, and produces numbers that look reasonable. You take the two hundred evaluation cases that you carefully designed for English, run them through a translation API or hand them to a translator, and suddenly you have two hundred evaluation cases in Spanish, Japanese, Arabic, and Hindi. The eval runs. The model scores 85% in Spanish. The team celebrates multilingual coverage. Except the 85% is a lie -- not because the model cannot perform in Spanish, but because the evaluation itself is testing the wrong things in the wrong way.

This subchapter explains why translated evals produce misleading results, what they miss, and what it costs you when you trust them.

## What Teams Typically Do

The standard approach goes like this. A team builds a thorough English eval suite. It tests summarization quality, factual accuracy, instruction following, tone consistency, and domain-specific knowledge. The eval cases are carefully constructed by native English speakers who understand the product, the users, and the edge cases that matter. The suite works. It catches regressions, validates improvements, and gives the team confidence that their English product meets quality standards.

Then the product expands to new languages. The deadline is tight. The budget for evaluation is a fraction of the original English eval budget, because evaluation is already built -- it just needs to be translated. A batch of eval cases goes to a machine translation service or a human translator. The translated cases come back. Someone spots-checks a few for obvious errors. The suite runs. Numbers appear on a dashboard. The team marks multilingual evaluation as complete.

The problem is not laziness. It is a reasonable assumption that turns out to be wrong: that a good evaluation case in English becomes a good evaluation case in another language through translation. This assumption fails for at least five distinct reasons, each of which independently produces misleading results.

## Reason One: Cultural Assumptions Do Not Translate

English eval cases carry cultural context that their creators rarely notice because it matches their own cultural frame. A question about tipping practices at restaurants assumes a culture where tipping exists. A scenario involving a 401k retirement plan assumes American financial instruments. A test case about Thanksgiving dinner assumes a holiday that most of the world does not celebrate. A factual question about "the president" without specifying a country assumes the United States.

When these cases are translated, the cultural assumptions travel with them. The Japanese version of a question about 401k plans tests whether the model can discuss American retirement accounts in Japanese -- not whether the model understands Japanese pension systems, which is what a Japanese user would actually ask about. The Arabic version of a question about Thanksgiving tests the model's ability to describe an American holiday in Arabic, not its ability to discuss Eid, Ramadan, or regional holidays that Arabic-speaking users care about.

The result is that translated evals test the model's ability to discuss English-culture topics in another language. This is a valid capability, but it is not the capability your users need. A customer support bot serving Japanese users needs to handle questions about Japanese holidays, Japanese business practices, Japanese address formats, and Japanese honorific conventions. A translated English eval suite tests none of these. The model could score 95% on translated evals and still fail catastrophically on the actual queries Japanese users submit.

## Reason Two: Translation Quality Contaminates Eval Quality

The quality of the translation directly affects the quality of the evaluation, and most translations are not good enough to serve as eval cases. Machine translation produces text that is grammatically correct but often unnatural -- sentences that a native speaker would never write, word choices that are technically valid but contextually wrong, structures that follow English syntax patterns rather than the target language's natural patterns.

This translated text has a name in linguistics: **translationese**. Translationese is text that is technically in the target language but reads like a translation. It uses loan structures from the source language, selects word orders that mirror English rather than following the target language's conventions, and produces sentences that a native speaker would describe as "correct but weird."

When your eval cases are in translationese, you are testing the model's ability to handle awkward, unnatural phrasing -- not its ability to handle the fluent, natural queries that real users submit. A model might score well on translationese inputs because the model itself was trained partly on translationese -- web-crawled parallel text, translated documents, and other bilingual data that shares the same unnatural patterns. The model recognizes translationese because it has seen plenty of it. But real user queries are not translationese. They are native-language text with native-language patterns, and the model's performance on native input can differ significantly from its performance on translated input.

Research on multilingual evaluation published at NAACL and EMNLP in 2025 confirmed this pattern: human assessments revealed that many machine-translated evaluation cases fell below claimed quality standards, with source sentences that were too domain-specific and contained jargon or culturally specific expressions that translated unnaturally into target languages. The researchers found that evaluation scores on translated test sets systematically overstated model capabilities compared to scores on native-language test sets.

## Reason Three: Multiple Valid Answers Collapse to One

English eval cases typically have a reference answer -- the correct response that the model's output is compared against. When the eval case is translated, the reference answer is also translated. But translation is not a one-to-one mapping. A single English sentence can be validly translated into multiple target-language sentences, each with different word choices, different emphasis, and different formality levels, all equally correct.

If your evaluation compares the model's output against a single translated reference, it penalizes every valid alternative phrasing. The model might produce a response that is more natural, more fluent, and more culturally appropriate than the translated reference -- and receive a lower score because it used different words. This is especially severe for languages with rich morphology or flexible word order. Arabic, with its root-and-pattern morphological system, can express a single concept in many grammatically valid ways. Japanese, with its multiple levels of formality and honorific systems, can translate a single English instruction into a polite form, a casual form, a humble form, or a respectful form, each appropriate in different contexts. A translated reference captures one of these forms and implicitly declares the others wrong.

The consequence is that translated evals systematically undercount correct responses. The model might be performing well -- producing fluent, accurate, culturally appropriate output -- but the eval score says otherwise because the reference answer uses a different but equally valid phrasing. Teams that see low scores on translated evals may invest in fixing problems that do not exist, optimizing for a specific phrasing that is no better than what the model already produces.

## The Translation Trap

These three failure modes combine into a pattern this book calls **The Translation Trap**: the false confidence that comes from translated evals showing acceptable scores, paired with the false alarm that comes from translated evals showing low scores on valid output.

The trap works in both directions. When translated evals show high scores, the team believes multilingual quality is strong. In reality, the high scores may reflect a match between the model's training distribution and the translationese patterns in the eval -- not actual quality in the language. When translated evals show low scores, the team believes multilingual quality is poor and invests in fixing problems. In reality, the low scores may reflect valid outputs being penalized for using different phrasing than the translated reference -- not actual quality failures.

Either way, the team's decisions are driven by misleading signals. Resources are misallocated. Real problems go undetected while phantom problems consume engineering time. The team has evaluation coverage on paper -- twelve languages, two hundred cases each -- but the coverage is illusory. It measures something, but not the thing that matters.

A mid-sized B2B product team learned this the hard way in late 2025. They had translated their English eval suite of 150 cases into Spanish, Portuguese, French, and German. The model scored between 82% and 88% across all four languages. The team considered multilingual quality solved and moved on to other priorities. Three months later, they began receiving complaints from Spanish-speaking enterprise customers. The complaints were specific: the model used formal register when the customer's context called for informal, it referenced American date formats instead of day-month-year, and it repeatedly suggested solutions that were legally relevant in the United States but meaningless in Mexico and Colombia. None of these failure modes were captured by the translated eval suite, because the eval cases were fundamentally American scenarios expressed in Spanish words.

When the team built a native Spanish eval suite -- fifty cases written by Spanish-speaking evaluators reflecting actual customer scenarios from Latin American markets -- the model's score dropped from 86% to 71%. The fifteen-point gap was the distance between what translated evals measured and what actually mattered.

## What Translated Evals Miss

Beyond the three core failure modes, translated evals have systematic blind spots that native-language evals cover.

**Cultural knowledge.** A model serving Korean users should understand Korean business etiquette, Korean educational systems, Korean holidays, and Korean social norms. Translated English evals test none of this. They test whether the model can answer American questions in Korean.

**Idiomatic usage.** Every language has idioms, colloquialisms, and expressions that do not translate from English. A native-language eval tests whether the model can understand and produce these natural expressions. A translated eval tests whether the model can handle literal translations of English idioms, which is a different and less useful capability.

**Formality register.** Japanese, Korean, Thai, Hindi, and many other languages have elaborate formality systems where the appropriate level of politeness depends on the social context. A translated English eval case has no formality signal -- it was written in English, which has minimal formality distinction. The translated version picks one formality level, usually the default one the translator chose. The eval does not test whether the model selects the right formality level for the context, which is one of the most important quality dimensions for users of these languages.

**Script-specific formatting.** Date formats, number formats, address structures, name ordering conventions, currency symbols, and punctuation conventions differ by language and region. Translated evals inherit English formatting conventions. A model that correctly uses Japanese date ordering, or puts the family name before the given name in Korean, or uses the correct currency symbol for Brazilian Real, may score lower on a translated eval that expects English formatting patterns.

**Regulatory and legal context.** Privacy regulations, consumer protection laws, financial disclosure requirements, and content restrictions vary by country. A model serving German users must understand GDPR implications. A model serving Brazilian users must understand LGPD. Translated English evals test knowledge of American regulatory frameworks expressed in the target language, not knowledge of the local regulatory framework that actually applies.

## When Translated Evals Are Acceptable

Translated evals are not worthless. They serve a legitimate purpose when used correctly and when their limitations are explicitly acknowledged.

As a rough first filter, translated evals can detect catastrophic failures: a model that cannot generate coherent text in a language, that crashes on non-Latin input, or that reverts to English when prompted in another language. These failures are so severe that even a translated eval catches them. If a model scores below 50% on translated evals in a language, you have a fundamental capability problem that needs to be addressed before native evals become relevant.

As a consistency check, translated evals can verify that a model's core capabilities -- instruction following, output formatting, basic reasoning -- work across languages. If the model follows instructions correctly in English but ignores them in Arabic, translated evals detect this pattern, because the instruction-following capability is language-independent.

As a supplement to native evals, translated evals add volume. If you have fifty native-language eval cases and want to expand coverage quickly, adding a hundred translated cases gives you broader coverage, as long as you weight the native cases more heavily in your quality assessment and treat the translated cases as a secondary signal.

The rule is simple: never use translated evals as your only multilingual evaluation. Never report translated eval scores without the "translated" label. And never make quality decisions based solely on translated eval results when native eval data is available.

## The Standard You Should Hold

Every language you serve in production deserves at least a core set of native-language, culturally grounded evaluation cases. "Core" does not mean hundreds of cases on day one. It means enough cases -- twenty to fifty -- to test the capabilities your users actually need, written by native speakers who understand both the language and the domain, reflecting real-world scenarios from the actual market you serve.

Building native eval sets is more expensive than translating. It requires native speakers with domain knowledge, cultural context, and evaluation expertise. Subchapter 4.2 provides the detailed methodology. But the cost of native eval sets is a fraction of the cost of shipping a product that fails in ways your translated evals cannot detect. The team that spent three months responding to Spanish customer complaints could have prevented those complaints with two weeks of native eval development.

The investment compounds over time. A native eval set improves with every cycle: you add cases that reflect real user failures, remove cases that no longer test relevant capabilities, and build a suite that genuinely measures your product's quality in each language. A translated eval set, no matter how many cycles you run it through, remains a shadow of the English original -- structurally incapable of testing the dimensions that matter most.

The next subchapter teaches you how to build those native-language evaluation sets from scratch: how to recruit the right evaluators, how to design culturally grounded test cases, and how to establish quality standards that reflect what real users in each market actually need.