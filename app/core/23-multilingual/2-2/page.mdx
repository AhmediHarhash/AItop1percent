# 2.2 — Benchmark Literacy: MMMLU, MMLU-ProX, INCLUDE, and What They Actually Measure

Your model scores 90% on MMMLU. Does that mean it works in Japanese? It means the model answered 90% of translated English knowledge questions correctly when those questions were rendered in Japanese. It does not mean the model can draft a Japanese business proposal, summarize a Japanese legal filing, or navigate the honorific layers that Japanese customer service requires. The distance between a benchmark score and production-quality multilingual output is enormous, and teams that treat the benchmark as the finish line rather than the starting line consistently ship products that fail in languages where the score looked fine.

Multilingual benchmarks have improved dramatically between 2024 and 2026. The field has moved from translated English tests to natively authored, culturally grounded evaluations. But each benchmark measures something specific, misses something important, and can be gamed in ways that do not translate to real-world performance. Understanding what each benchmark actually measures — and what it does not — is the difference between using benchmarks as a useful signal and using them as an expensive source of false confidence.

## MMMLU: The Translated Baseline

**MMMLU** — Massive Multitask Multilingual Language Understanding — was OpenAI's extension of the original MMLU benchmark into 14 languages using professional human translations. Each language version contains approximately 15,908 multiple-choice questions across 57 subjects, from abstract algebra to world religions. The translations are high quality by translation standards: professional translators with subject matter expertise, quality review processes, and consistency checks.

The fundamental problem with MMMLU is not the translation quality. It is the assumption that translating an English question preserves its evaluative value. A question about the United States Constitution tests legal knowledge in English and translation fidelity in French. A question referencing the Federal Reserve tests economics in English and cultural familiarity in Japanese. A question about common law precedent is meaningful in English, partially meaningful in languages spoken in common law jurisdictions, and nearly meaningless in languages spoken in civil law jurisdictions. The knowledge being tested is English-centric. The language is not.

This cultural bias inflates the scores of models that are good at English knowledge in non-English languages and deflates the scores of models that have strong native-language knowledge but weaker English cultural grounding. A model that knows everything about Japanese labor law, Japanese corporate governance, and Japanese medical practice but has limited knowledge of American history will score lower on MMMLU Japanese than a model with the opposite profile. For teams building products that serve Japanese users in Japanese contexts, the second model is worse. But the benchmark says it is better.

MMMLU remains useful as a baseline. It tells you whether a model can process and reason about text in a given language at all. A model that scores below 50% on MMMLU in a language is probably not ready for production use in that language. But the ceiling MMMLU can measure is limited. A score of 85% versus 88% on MMMLU tells you almost nothing about which model will perform better on your actual task.

## MMLU-ProX: Harder Questions, More Languages, Same Cultural Limitations

**MMLU-ProX**, published at EMNLP 2025, addresses two weaknesses of MMMLU: question difficulty and language coverage. It extends the harder MMLU-Pro question set — which emphasizes reasoning over recall — into 29 typologically diverse languages, with 11,829 identical questions per language. The translation process uses multiple large language models for initial translation followed by expert human review to ensure accurate expression, consistent terminology, and cultural relevance.

The results from MMLU-ProX are sobering. Performance gaps between English and the lowest-scoring languages reach 24.3 percentage points even for state-of-the-art models. High-resource European languages like French and German show gaps of 5 to 8 points. CJK languages show gaps of 10 to 15 points. Arabic and Hindi show gaps of 12 to 18 points. Swahili, Yoruba, and other low-resource languages show gaps exceeding 20 points.

What MMLU-ProX adds to the picture is a sharper measure of reasoning capability. Because the questions are harder — requiring multi-step reasoning rather than factual recall — the gaps it reveals are more predictive of real-world performance on complex tasks. A model that maintains 85% English accuracy and 78% French accuracy on MMLU-ProX is more likely to handle complex French tasks than one that achieves the same spread on MMMLU, because the underlying capability being measured is closer to what production tasks demand.

The limitation remains cultural grounding. MMLU-ProX starts from English questions and translates them. The expert review process catches translation errors and ensures terminology consistency, but it does not replace the questions with ones that test native-language knowledge. A Turkish user taking a quiz about Ottoman economic history would find that MMLU-ProX tests something quite different from what matters to them. For this reason, MMLU-ProX is best treated as a measure of cross-lingual reasoning transfer — how well a model can apply its reasoning abilities in a non-English language — rather than a measure of native-language competence.

## INCLUDE: The Native-Language Breakthrough

**INCLUDE**, published as a Spotlight paper at ICLR 2025, represents a fundamental shift in multilingual evaluation design. Instead of translating English questions, INCLUDE compiles questions from academic, professional, and occupational license examinations that were originally written in their native languages. The benchmark covers 44 languages, contains 197,243 questions, and spans 15 different scripts.

The design philosophy is simple and powerful: if you want to know whether a model can function in Japanese, test it on questions that Japanese professionals answer in Japanese — the Japanese bar exam, Japanese medical licensing exams, Japanese civil service tests. These questions test knowledge that matters in Japanese society, expressed in the way native Japanese speakers express it, using cultural references and domain vocabulary that no translation from English would produce.

The consequence of this design is that INCLUDE reveals much larger performance gaps than translated benchmarks. A model that scores 85% on MMMLU Japanese might score 72% on INCLUDE Japanese, because the INCLUDE questions test Japanese-specific knowledge that the model never learned from its primarily English training data. For low-resource languages, the gaps are even more dramatic. Models that score 65% on MMMLU Swahili may score below 45% on INCLUDE Swahili, because the MMMLU score reflected the model's ability to answer translated English questions rather than its actual Swahili knowledge.

For model selection, INCLUDE provides the most realistic signal of how a model will perform on real tasks in a given language. If your product serves Turkish lawyers, INCLUDE's Turkish legal exam questions are closer to your actual use case than MMMLU's translated American legal questions. If your product serves Indonesian healthcare workers, INCLUDE's Indonesian medical licensing questions tell you more than any English-origin benchmark.

The limitation of INCLUDE is coverage unevenness. Not all 44 languages have the same number or difficulty of questions. Languages with robust professional examination systems — Japanese, Korean, French, German, Chinese — have deep, high-quality question sets. Languages where professional licensing is less standardized or less digitized may have thinner coverage. Additionally, INCLUDE does not measure generation quality — it is still a multiple-choice benchmark. A model that can pick the right answer on a Japanese medical question may still generate poor-quality Japanese medical prose.

## BenchMAX: Measuring Cross-Lingual Reasoning Parity

**BenchMAX**, published at EMNLP 2025, takes a different approach entirely. Rather than measuring absolute performance per language, BenchMAX measures the gap between English and non-English performance across 10 diverse tasks including instruction following, code generation, long context understanding, mathematical reasoning, and tool use. It covers 17 languages with native-speaker annotation for quality assurance.

The critical contribution of BenchMAX is the GAP metric — a quantitative measure of how much capability a model loses when operating in a non-English language. A model might score 82% on English instruction following and 74% on German instruction following, yielding an 8-point gap. Another model might score 79% and 76%, yielding a 3-point gap. The second model is worse in absolute terms on both languages but more linguistically equitable. For products that must deliver consistent quality across languages, the second model may be the better choice.

BenchMAX also reveals a finding that challenges the assumption that scale solves multilingual performance: the performance gaps between English and other languages cannot be reliably closed by simply scaling up model size. Larger models improve absolute performance in all languages, but they do not proportionally narrow the gap. A 70-billion-parameter model may outperform a 7-billion-parameter model in both English and Arabic, but the Arabic deficit relative to English may be nearly identical in both models. This means you cannot solve your multilingual quality problem by upgrading to a bigger model. You need a model that was specifically trained or fine-tuned for your target languages.

## What All These Benchmarks Miss

Even the best multilingual benchmarks share limitations that no current benchmark addresses.

Generation quality is the biggest blind spot. Every major multilingual benchmark uses multiple-choice or short-answer formats. None of them measures whether the model can generate fluent, natural, culturally appropriate prose in a given language. A model that can select the correct answer on a Japanese medical exam may produce Japanese text that sounds robotic, uses unnatural word order, or mixes formality registers in ways that no native speaker would accept. For most production applications — chatbots, content generation, customer support, document drafting — generation quality matters more than knowledge recall, and no published benchmark measures it.

Cultural appropriateness is another gap. Benchmarks test whether the model knows facts and can reason. They do not test whether the model understands that a formal business greeting in Korean requires specific honorific patterns, that an informal response to a Saudi customer is a cultural offense, that humor in Brazilian Portuguese follows different rules than humor in European Portuguese, or that a medical explanation in Hindi needs to account for Ayurvedic concepts alongside Western medicine. Cultural failures are the ones that generate social media backlash and viral screenshots. No benchmark catches them.

Safety across languages is systematically under-measured. Models that reliably refuse harmful English prompts may comply with harmful prompts in low-resource languages because their safety training was conducted primarily in English. The MMMLU, MMLU-ProX, and INCLUDE benchmarks do not test safety at all. If your product has safety requirements, you need a separate multilingual safety evaluation that goes beyond any published benchmark.

Task-specific performance is the final gap. Your product does not ask the model to answer multiple-choice questions. It asks the model to summarize a legal brief in French, classify customer sentiment in Arabic, generate product descriptions in Japanese, or answer medical questions in Hindi. No published benchmark measures whether a model can do any of these specific tasks in any specific language. Benchmarks tell you whether the model has the foundation to potentially succeed. Only task-specific, language-native evaluation tells you whether it actually will.

## The Benchmark Tourism Problem

A growing concern in the multilingual AI community is what practitioners call **benchmark tourism** — the practice of optimizing models for benchmark performance in ways that do not transfer to real-world tasks. A model provider can boost their MMMLU score by training on question formats similar to MMLU, increasing exposure to the specific knowledge domains MMLU tests, or fine-tuning on translated educational content that mirrors benchmark questions. The MMMLU score goes up. The model's ability to draft natural French prose or handle colloquial Japanese does not change.

Benchmark tourism is particularly pernicious in multilingual evaluation because the gap between benchmark performance and real-world performance is already larger for non-English languages. An English benchmark score of 90% usually corresponds to genuinely strong English capabilities — the model has been trained extensively on English and the benchmark measures a meaningful subset of that training. A Hindi benchmark score of 75% might correspond to a model that can parse Hindi well enough to select correct multiple-choice answers but cannot generate fluent Hindi paragraphs because its Hindi training data was too narrow.

The defense against benchmark tourism is straightforward in principle and demanding in practice: never select a model based solely on published benchmark scores. Always supplement with your own language-native, task-specific evaluation. The benchmarks eliminate obviously incapable models. Your evaluation identifies the genuinely capable ones.

## How to Use Benchmarks in Your Selection Process

Treat multilingual benchmarks as a three-stage filter, not a ranking system.

The first stage is elimination. Use MMMLU and MMLU-ProX to eliminate models that cannot handle your target languages at all. A model scoring below 55% on MMLU-ProX for a language you need is not a candidate, regardless of its English score.

The second stage is gap analysis. Use BenchMAX's GAP metric and MMLU-ProX's per-language breakdowns to identify which models maintain the narrowest performance gaps across your target language set. If you need French, Japanese, and Arabic, look for the model with the smallest worst-case gap across those three languages, not the model with the highest score in any one of them.

The third stage is cultural relevance testing. Use INCLUDE where available to check whether the model has native-language knowledge, not just translated English knowledge. A model that scores well on INCLUDE in your target languages has been trained on data that includes native-language content, which predicts better real-world performance than high scores on translated benchmarks.

After these three stages, you should have two to four candidate models. From here, the only evaluation that matters is your own: task-specific prompts in your target languages, evaluated by native speakers, measured against the quality criteria your product requires. Benchmarks got you to the short list. Your evaluation selects the winner.

The next subchapter introduces the framework that organizes this entire selection process — the Language Tier System that categorizes languages by the quality gap you should expect and the investment required to close it.
