# 12.2 — The EU AI Act and Multilingual Obligations for GPAI Systems

If your AI system serves EU users in multiple languages, the EU AI Act does not treat multilingual support as a feature. It treats it as a compliance obligation. The distinction matters more than any technical decision you will make this year, because a feature can be deprioritized, delayed, or descoped. An obligation cannot. And unlike most regulations that affect AI products at the edges — consent banners, data deletion workflows, audit logging — the EU AI Act's multilingual implications cut straight through the center of your system architecture, touching training data documentation, per-language evaluation, transparency disclosures, and downstream deployer responsibilities. Teams that discover these obligations after launch are not just behind schedule. They are out of compliance.

## What the EU AI Act Requires from GPAI Providers

The EU AI Act creates a specific regulatory category for **General Purpose AI models** — models designed to perform a wide range of tasks across multiple domains. If you build, train, or substantially modify a foundation model that is made available on the EU market, you are a GPAI provider, and Article 53 imposes a set of obligations that became applicable on August 2, 2025.

The obligations start with technical documentation. Annex XI of the Act specifies what that documentation must contain, and language coverage is not optional metadata buried in an appendix. The documentation template requires providers to describe the approximate size of training data by modality — text, image, audio, video — and to describe language coverage with special attention to the official languages of the European Union. There are twenty-four official EU languages. If your model was trained predominantly on English data with thin coverage of Estonian, Maltese, or Bulgarian, your documentation must say so. The Act does not require that your model perform equally across all EU languages. It requires that you know and disclose how it performs.

This disclosure obligation creates a cascade of technical requirements that most teams have not internalized. You cannot document per-language coverage without measuring per-language coverage. You cannot measure per-language coverage without per-language evaluation sets. You cannot build meaningful per-language evaluation sets without native-speaking evaluators. The documentation requirement, which reads as a paperwork exercise, is actually a mandate for multilingual evaluation infrastructure. If you do not have that infrastructure, you cannot produce the documentation. If you cannot produce the documentation, you are not compliant.

The training data summary is a separate obligation under Article 53. Providers must publish a sufficiently detailed summary about the content used for training, following a template provided by the AI Office. This template explicitly asks for language and demographic coverage information. The summary is public-facing. Your competitors, your customers, and regulators can read it. If you claim broad multilingual capability in your marketing but your training data summary reveals that 94% of your data is English, the gap between claim and disclosure becomes a regulatory problem and a reputational one.

## The August 2, 2026 Compliance Deadline and Systemic Risk

GPAI obligations under the Act follow a staggered timeline, and the most consequential deadline is August 2, 2026. On that date, the European Commission's enforcement powers enter full application. The Commission will have the authority to enforce compliance with GPAI obligations, including the power to impose fines.

For GPAI models classified as having **systemic risk** — defined as models trained with computational resources exceeding ten to the power of twenty-five floating point operations, or models designated as systemic risk by the Commission based on other criteria — the obligations are heavier. Providers of systemic risk models must conduct model evaluations including adversarial testing, track and report serious incidents, and implement cybersecurity protections. The multilingual dimension of systemic risk is particularly important: if your model is widely deployed across EU member states in multiple languages, the Commission is more likely to classify risks as systemic because language-specific failures can affect entire national populations simultaneously.

Consider what this means in practice. A GPAI model that hallucinates medical information has a safety problem. A GPAI model that hallucinates medical information specifically in Romanian — because its Romanian training data was thin and poorly curated — has a systemic risk that affects an entire national population. The Act does not distinguish between intentional harm and negligent language coverage gaps. Both create the same downstream risk, and both trigger the same obligations.

Models placed on the market before August 2, 2025 have a grace period until August 2, 2027 to bring their documentation into compliance. But if you are building or fine-tuning a model in 2026 and deploying it to EU markets, you have no grace period. You must comply from the date of deployment.

## The GPAI Code of Practice

The GPAI Code of Practice, published on July 10, 2025, translates the Act's legal requirements into practical compliance guidance. Signing the Code is voluntary, but the Commission has signaled that signatories who follow the Code will be presumed to comply with GPAI obligations — creating a strong incentive to adopt its framework.

The Code establishes a Model Documentation Form that signatories commit to completing before placing a GPAI model on the EU market. This form requires current documentation reflecting any material changes, preserved for a minimum of ten years after the model's initial release. For multilingual systems, the ten-year preservation requirement means your per-language evaluation results from 2026 must still be available and auditable in 2036. If you are running per-language evals in an ad hoc fashion — checking Japanese quality when someone complains, running French evals when a customer asks — you do not have the documentation infrastructure to satisfy this requirement.

The Code also distinguishes between information intended for downstream providers, the AI Office, and national competent authorities. This three-audience disclosure structure means your per-language documentation must serve different purposes. Downstream providers need enough information about per-language capabilities to meet their own obligations. The AI Office needs enough detail to assess systemic risk. National competent authorities need enough specificity to evaluate compliance in their language and jurisdiction. A single marketing-grade capability description does not satisfy any of these audiences.

The September 2025 Q and A document from the AI Office further clarified that "language coverage" in the documentation context means not just listing which languages the model can process, but providing meaningful information about relative capability levels. Saying "this model supports twenty-four EU languages" without disclosing that its quality varies from 92% in English to 51% in Maltese is insufficient. The disclosure must be honest enough that downstream deployers and end users can make informed decisions about which languages to trust and which to supplement or avoid.

## Downstream Deployer Obligations

The EU AI Act does not only regulate model providers. It also imposes obligations on **deployers** — organizations that use AI systems in their products and services. If you fine-tune a GPAI model, integrate it into a customer-facing application, or build a product on top of a third-party model, you are a deployer with your own compliance requirements.

For multilingual systems, the deployer obligations create a chain of accountability that many teams have not mapped. When you take a base model from OpenAI, Anthropic, Google, or Meta and fine-tune it for your use case, you become what the Act calls a downstream modifier. You inherit the provider's transparency obligations for the aspects of the model you changed. If your fine-tuning improved English quality but degraded German quality, you own that degradation. The base model provider documented their German quality level. You changed it. The delta is your responsibility.

This chain of accountability means deployers need their own per-language evaluation infrastructure, independent of whatever the model provider claims. If the provider's documentation says their model scores 88% on German task accuracy, and your fine-tuned version scores 72% on the same metric, you need to know that before deployment. "We trusted the provider's documentation" is not a compliance defense when you modified the model and shipped the modified version without verifying per-language performance.

Deployers of high-risk AI systems face even stricter requirements. Under Article 26, deployers must use the system in accordance with the instructions for use provided by the provider, monitor the system's operation, and inform the provider or distributor if they have reason to believe the system presents a risk. For multilingual high-risk systems — medical diagnosis tools, legal advisory systems, financial credit scoring — the monitoring obligation extends to every language the system processes. If your medical diagnosis assistant works reliably in French but produces inconsistent results in Dutch, and you discover this through monitoring, you have a positive obligation to report it and to stop deploying in Dutch until the issue is resolved.

## Transparency Requirements: What Users Must Be Told

Article 13 of the EU AI Act requires that high-risk AI systems be designed and developed with sufficient transparency to enable deployers to interpret the system's output and use it appropriately. The instructions for use must include information about the system's characteristics, capabilities, and limitations of performance, including the level of accuracy, robustness, and cybersecurity.

For multilingual systems, "capabilities and limitations of performance" includes per-language variation. You cannot market a product as a "multilingual AI assistant" available in twelve languages if quality varies from 94% in English to 58% in Greek without disclosing this disparity. The marketing claim creates a user expectation. The undisclosed quality gap violates the transparency obligation. The user who relies on your Greek-language legal analysis, believing it meets the same standard as the English version, has been misled.

The practical implication is that your product must surface language-specific capability information to users. This does not mean burying a disclaimer in your terms of service. It means providing information that is, in the Act's language, "concise, complete, correct, clear, relevant, accessible, and comprehensible." For a multilingual product, that standard requires per-language capability disclosure in a format that users can actually find and understand. A support article listing known limitations by language, a quality indicator within the product interface, or a transparency page that reports per-language evaluation scores — the format is yours to choose, but the obligation to inform is not.

The transparency requirement also applies to how the system handles language detection and switching. If your system automatically detects the user's language and responds in that language, but the detection is unreliable for certain languages or language pairs, that unreliability is a limitation that users need to know about. A Croatian user whose inputs are misclassified as Serbian and answered in Serbian register has experienced a system limitation that the Act requires you to disclose and address.

## High-Risk AI Systems and Per-Language Quality Standards

The EU AI Act defines high-risk AI systems as those used in specific domains where errors carry significant consequences: biometric identification, critical infrastructure management, education, employment, essential services, law enforcement, migration, and justice. AI systems deployed in these domains must meet rigorous quality standards — and those standards apply in every language the system claims to support.

This is where the multilingual obligation becomes most concrete and most consequential. A medical triage system that classifies patient symptoms must meet accuracy thresholds in every language it processes. If it performs at 96% accuracy in English but 71% in Polish, and a Polish-speaking patient receives an incorrect triage recommendation, the system has failed a high-risk obligation. The provider cannot argue that the system works well in its primary language. The Act evaluates performance in the language the system actually processed, not the language the provider prefers.

The practical architecture this demands is expensive and rigorous. For each high-risk application, you need evaluation datasets in every supported language, calibrated to the domain. A medical triage eval set in Portuguese is not the same as a general-purpose Portuguese eval set. It must contain medical terminology, symptom descriptions, and clinical reasoning patterns that Portuguese-speaking patients and clinicians actually use. Building these per-language, per-domain evaluation sets requires native-speaking domain experts — Portuguese-speaking physicians, Polish-speaking legal professionals, Greek-speaking financial analysts — and those experts are not interchangeable with general-purpose annotators.

The cost of non-compliance in high-risk domains is not just regulatory fines. It is professional liability, reputational damage, and in medical or safety-critical contexts, potential harm to individuals. The teams that build high-risk multilingual AI systems without per-language quality infrastructure are not making a budget trade-off. They are accepting risk they may not survive.

## Reasonably Foreseeable Misuse in Multilingual Contexts

The EU AI Act requires providers to consider "reasonably foreseeable misuse" when designing and documenting their systems. For multilingual AI, this concept has a specific and often overlooked application: if users predictably use your system in a language it handles poorly, that usage pattern is a foreseeable risk you must address.

The reasoning is straightforward. If you deploy a customer service chatbot in Belgium, a country with three official languages — Dutch, French, and German — users will interact with it in all three. If your chatbot performs well in French but poorly in German, and you know this from your evaluation data, the German-language failures are not edge cases. They are foreseeable. Belgium has a substantial German-speaking community concentrated in the East Cantons. Deploying a chatbot that fails for that community, when you knew it would fail, is a failure to address a foreseeable risk.

The "reasonably foreseeable" standard extends beyond your intended market. If you deploy an AI system in the Netherlands, you should foresee that some users will interact in Frisian, a co-official language. If you deploy in Finland, you should foresee interactions in Swedish, which is an official language alongside Finnish. If you deploy across the EU broadly, you should foresee usage in all twenty-four official languages, regardless of which languages you intended to support. The Act does not limit your obligations to languages you chose. It extends them to languages your users predictably use.

This does not mean you must support every conceivable language. It means you must either support a language adequately or transparently disclose that you do not support it and implement safeguards to prevent users from relying on unsupported languages for consequential decisions. A system that detects an unsupported language and says "I cannot provide reliable results in this language" is handling foreseeable misuse responsibly. A system that silently produces low-quality output in an unsupported language, allowing users to rely on it without knowing the risk, is not.

## The Practical Compliance Architecture

Building compliance infrastructure for multilingual GPAI obligations is not a single project. It is an operating model with four continuous components.

The first component is **per-language capability measurement**. You need automated evaluation pipelines that run regularly — at minimum before every model update, prompt change, or pipeline modification — and produce per-language scores across your quality dimensions. These scores must be stored historically, because the documentation requires showing how performance changes over time, and the ten-year retention requirement means your evaluation infrastructure must be durable.

The second component is **documentation generation and maintenance**. Your technical documentation, training data summary, and transparency disclosures must reflect the current state of the system, not the state at launch. If a model update improves your Bulgarian quality from 68% to 82%, your documentation must be updated. If a fine-tuning pass degrades Romanian, that degradation must be documented. The Code of Practice requires that documentation "remain current, reflecting any material changes." For a multilingual system where quality shifts with every model update, "current" means continuous documentation updates, not annual reviews.

The third component is **downstream deployer communication**. If you provide a GPAI model that others build on top of, you must give downstream deployers enough per-language information for them to meet their own obligations. This means structured, machine-readable per-language capability data — not a marketing PDF. Downstream deployers need to programmatically assess whether your model's quality in their target languages meets their application requirements. The providers who make this easy will have a competitive advantage. The providers who make it difficult will lose deployers to competitors who provide better transparency.

The fourth component is **incident tracking by language**. When your system produces a harmful, incorrect, or low-quality output, you need to track whether the incident was language-specific. A hallucination in Estonian has different root causes and different remediation paths than a hallucination in English. Language-segmented incident tracking lets you identify patterns — is Estonian quality degrading over time? Did a recent model update introduce regression in specific languages? — and respond before those patterns become systemic risks that trigger Commission scrutiny.

## What Compliance Looks Like in Practice

The difference between compliant and non-compliant multilingual AI operations is not a matter of legal sophistication. It is a matter of engineering discipline.

A non-compliant operation evaluates in English, documents in English, monitors aggregate quality, markets broadly as "multilingual," and discovers per-language failures when users complain or regulators inquire. The documentation exists on paper but does not reflect reality. Per-language quality varies wildly, but no one has measured the variance. The team assumes multilingual capability because the model accepts non-English input, confusing input acceptance with output quality.

A compliant operation evaluates in every supported language on a regular cadence. It documents per-language scores, updates documentation when scores change, discloses known limitations honestly, and implements safeguards for languages below quality thresholds. It tracks incidents by language, communicates per-language capabilities to downstream deployers, and treats every supported language as a first-class product surface that must meet defined quality standards. It does not claim capability it has not measured. It does not market quality it cannot demonstrate.

The compliant operation is more expensive to build and maintain. But the cost of non-compliance — fines that can reach 3% of global annual turnover for GPAI violations, reputational damage from public enforcement actions, and the remediation cost of retrofitting compliance into a system that was never designed for it — makes the investment in proper multilingual compliance infrastructure one of the most straightforward ROI calculations in AI engineering.

The EU AI Act is the most comprehensive AI regulation in force as of 2026, but it is not the only framework that creates language-specific obligations. The next subchapter examines how language rights legislation — laws that predate AI entirely — creates additional compliance requirements for multilingual systems in jurisdictions around the world.
