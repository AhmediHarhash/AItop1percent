# 5.11 â€” Quality Assurance for Translated AI Output

You test your model output. You test your prompts. You test your retrieval pipeline. You run regression suites before every deployment, track quality metrics on dashboards, and page on-call engineers when scores drop. But do you test your translations? For most AI teams, the answer is no. The model produces output. The translation layer converts it. The user receives it. Nobody verifies that the translated version says what the original version meant, uses the correct terminology, matches the right formality register, or avoids cultural landmines. The quality assurance infrastructure that your engineering team spent months building for the English pipeline simply does not exist for the multilingual pipeline. And the failures -- the mistranslated medical instruction, the casualized formal communication, the terminology that contradicts your own product UI -- reach users unchecked.

This is **The Translation QA Gap**: the asymmetry between how rigorously teams evaluate their model's English output and how casually they treat the translated versions of that same output. Closing it is not optional. It is the difference between a multilingual product that works and one that merely appears to.

## What Translation QA Actually Covers

Translation quality assurance is not a single check. It is five distinct quality dimensions, each with its own failure modes and its own evaluation methods.

**Accuracy** asks whether the meaning was preserved. Did the translation convey the same information as the source? This is the most fundamental dimension and the most dangerous when it fails. An inaccurate translation does not just confuse -- it misinforms. A chatbot that tells a German user their refund will be processed "within 3 business days" when the English source said "within 30 business days" has created a customer service crisis. Accuracy errors in medical, legal, or financial content create liability.

**Fluency** asks whether the translation reads naturally in the target language. A translation can be accurate -- every word correctly mapped -- and still sound stilted, mechanical, or unnatural to a native speaker. Fluency failures erode trust gradually. Users do not consciously think "this translation is disfluent." They think "this product feels cheap" or "this company does not take my market seriously." Fluency is harder to score automatically than accuracy because it requires native-speaker intuition about natural phrasing, word order, and rhythm.

**Terminology consistency** asks whether the translation uses the same terms your product uses everywhere else. If your UI calls the feature "Workspace" in Spanish, does the translated chatbot response also call it "Workspace" in Spanish, or does it use "Area de trabajo" or "Espacio de trabajo"? Terminology inconsistency confuses users who learned your product's vocabulary and now encounter different words for the same concept. This dimension is directly measurable by comparing translated terms against your glossary.

**Formatting correctness** asks whether locale-specific conventions are applied. Dates in the right format. Numbers with the right decimal and thousands separators. Currency symbols in the right position. Measurement units converted. Address formats matching local conventions. Formatting errors are often invisible to non-native reviewers because the format looks normal through their own cultural lens.

**Cultural appropriateness** asks whether the content is free of references, assumptions, or phrasings that are offensive, confusing, or irrelevant in the target culture. A product that suggests users "celebrate their progress with a Thanksgiving dinner" in a Japanese market has made a cultural appropriateness error. An AI-generated response that uses informal slang in a context where Korean honorifics are expected has made a register error that Korean users perceive as disrespectful. Cultural appropriateness is the hardest dimension to automate and the most damaging when it fails, because users interpret cultural errors as the company not caring about their market.

## Automated QA: Catching Failures at Scale

You cannot have a human reviewer read every translated word your product generates. At production volumes, the content is too vast, too dynamic, and too continuous. Automated QA is your first line of defense -- the system that scores every translation and flags the ones that need human attention.

**COMET and xCOMET scoring** are the current standard for automated translation quality evaluation in 2026. COMET is a neural evaluation model that scores translations on a scale from 0 to 1, where scores above 0.85 typically indicate high-quality translation and scores below 0.75 indicate translations that likely contain meaningful errors. xCOMET extends this with fine-grained error detection, identifying specific text spans that contain minor, major, or critical errors according to the MQM error typology. Running xCOMET on all translated output gives you both a quality score and a map of where the errors are. This is dramatically more useful than legacy metrics like BLEU, which correlate poorly with human judgment and tell you nothing about what went wrong.

The practical implementation works like this: every translated segment passes through xCOMET scoring before or immediately after reaching the user. Segments scoring above your quality threshold -- typically 0.85 to 0.90 for user-facing content -- pass without review. Segments scoring below the threshold are flagged for human review. Segments scoring below a critical threshold -- typically 0.70 to 0.75 -- are blocked from the user entirely and either retranslated or routed to a human translator.

**Pattern-based checks** catch mechanical failures that quality scoring models might miss. These include untranslated text -- source-language words or phrases left in the translation, which is one of the most common LLM translation failures for technical terms and brand names. Length anomalies -- a translation that is dramatically shorter or longer than expected for the language pair, which often indicates truncation or hallucination. Formatting errors -- wrong date formats, wrong number formats, wrong currency symbols. Missing placeholders -- dynamic content markers that were present in the source but absent in the translation. Each of these checks is a simple rule that catches a specific failure mode, and together they form a safety net beneath the neural quality scoring.

**Reference-free quality estimation** runs when you do not have a source-language reference to compare against -- for example, when the model generates directly in the target language rather than translating from English. Quality estimation models like COMETKiwi score the output based on fluency and coherence alone, without needing the source text. The scores are less precise than reference-based scoring but still useful for flagging outputs that are likely to contain errors.

## Human QA: The Sample-Based Safety Net

Automated scoring catches most quality failures, but it misses subtleties that only a native speaker can detect. Cultural appropriateness, register correctness, brand voice consistency, and the kind of fluency that separates "grammatically correct" from "sounds like a human wrote it" all require human judgment. The question is not whether to do human QA. It is how much.

The answer depends on the risk tier of the content.

For safety-critical content -- medical instructions, financial disclosures, legal terms -- 100 percent human review is non-negotiable. Every translated word is reviewed by a qualified native speaker before it reaches a user. The cost is high, but the volume in this tier is low, and the consequences of an unreviewed error are severe enough to justify the expense.

For user-facing product content -- help center articles, documentation, onboarding flows -- a 15 to 25 percent sample review is the industry standard in 2026. A native-speaking reviewer evaluates a random sample of translated content each week, scoring it on the five quality dimensions and flagging systematic issues. If the sample reveals error rates above your threshold, you expand the sample or trigger a full review of recent content in that language.

For dynamic AI-generated content -- chatbot responses, summaries, recommendations -- a 3 to 10 percent sample review is typical. The volume is too high for exhaustive review, and the content is ephemeral. The goal of sampling is not to catch every error but to detect systematic quality problems -- a model that consistently mistranslates a specific term, a language where formality drift is getting worse, a content type where accuracy scores are declining.

For internal content -- admin panels, debug logs, developer documentation -- no regular human review is needed. Periodic audits -- perhaps quarterly -- verify that the automated systems are working correctly.

The sampling methodology matters. Random sampling catches average quality, but it misses targeted failures. Supplement random samples with stratified sampling: pull extra samples from low-scoring segments, from recently added languages, from content types that have historically produced more errors, and from any language where automated quality scores have trended downward. The goal is to direct human attention where it is most likely to find problems.

## The Feedback Loop: QA Findings That Improve the System

QA is not a gate. It is a feedback loop. Every error a reviewer finds is data about how your translation system fails, and that data should flow back into the system to prevent the same failure from recurring.

Terminology errors feed back into the glossary. If reviewers consistently find that the model translates "Workspace" differently across outputs, the glossary entry for "Workspace" needs reinforcement -- a clearer definition, additional context, or explicit do-not-translate instructions. The updated glossary feeds into the system prompt for LLM translation, reducing future terminology drift.

Fluency and register errors feed back into prompt engineering. If Japanese output consistently uses casual register where formal register is expected, the system prompt needs stronger formality instructions for Japanese. If German output produces awkward compound nouns that a native speaker would never use, the prompt needs examples of natural German phrasing. These prompt improvements are informed directly by the patterns that human reviewers identify.

Accuracy errors feed back into model selection decisions. If a particular LLM consistently produces accuracy errors in Thai but performs well in Japanese, you may route Thai translation to a different model or add a verification step for Thai that other languages do not require. The per-language quality data from your QA system informs your translation routing logic.

Formatting errors feed back into post-processing rules. If dates are consistently formatted wrong for a locale, the post-processing layer needs a locale-specific date formatter. If currency symbols appear in the wrong position, the formatter needs adjustment. These are deterministic fixes -- once the post-processing rule is correct, the error category disappears entirely.

The feedback loop creates a quality flywheel. QA catches errors. Errors inform improvements. Improvements reduce future errors. QA catches the remaining errors. Each cycle produces a measurably better translation system. Teams that treat QA as a one-time audit miss this flywheel entirely. They find problems, fix them once, and watch the same problems recur because the system that generated them was never updated.

## Common Translation Failures in AI Systems

AI systems produce translation failures that traditional translation workflows rarely encounter. Understanding these failure modes helps you build QA checks specifically designed to catch them.

**Hallucinated translations** occur when the model generates content in the target language that was not present in the source. The English source says "Your order will arrive Tuesday." The Japanese translation says "Your order will arrive Tuesday. Please note that delivery times may vary during national holidays." The model added a sentence that does not exist in the source. This is not a translation error in the traditional sense -- the added content is plausible and even helpful. But it is fabricated, and in contexts where accuracy to the source is critical, hallucinated additions are dangerous. They are particularly hard to catch because the added content is fluent and relevant. xCOMET with source-reference comparison is the best automated defense against hallucinated translations.

**Partial translations** leave some text untranslated in the source language. This happens most frequently with technical terms, brand names, acronyms, and content in quotation marks. The model is uncertain whether these items should be translated or left as-is, and sometimes makes the wrong call. A partial translation might render a customer support response that is 90 percent in Spanish and 10 percent in English, which looks broken to the user. Pattern-based checks that scan for source-language script in the target-language output catch this failure mode reliably.

**Register drift** occurs when the model translates content at the wrong formality level. Formal business communication translated with casual vocabulary. Customer-facing support text translated with overly academic phrasing. The model does not have a stable sense of which register to use, and small changes in source phrasing can push it from formal to informal without warning. Register drift is most damaging in languages with grammaticalized formality systems -- Japanese, Korean, German, Hindi -- where the wrong register is not just a stylistic choice but a social signal. Human reviewers are better at catching register drift than automated metrics, which is why per-language sample review should specifically include register evaluation.

**Terminology inconsistency across outputs** means the model uses different translations for the same product term in different responses. The chatbot calls the feature "Panel de control" in one response and "Tablero de control" in the next. Both are valid Spanish translations of "Dashboard." Neither is the term in your glossary, which specifies "Panel de control." This inconsistency is an LLM-specific failure -- human translators with access to a TM and glossary produce consistent terminology because they reference the glossary explicitly. LLMs produce inconsistent terminology because each generation is independent unless the glossary is injected into the prompt. The fix is glossary enforcement in the system prompt combined with automated terminology checks that compare translated terms against the approved glossary.

**Meaning distortion under complexity** occurs when the model mishandles complex source sentences -- those with multiple clauses, negations, conditional logic, or technical precision. A source sentence like "This feature is not available for users who have not verified their email, unless they were added by an administrator before January 2025" contains multiple negations and a conditional exception. The translated version might simplify the logic, drop the exception, or invert a negation, producing a translation that sounds fluent but says something different from the source. This failure mode is hardest to catch automatically and most dangerous in legal, medical, and policy content. Source-reference quality scoring helps, but complex sentences often benefit from human review.

## Building QA Into Your Translation Pipeline

Translation QA should not be a separate process that runs after translation is complete. It should be embedded in the translation pipeline as a stage that runs automatically.

The pipeline flow for user-facing content looks like this: source content enters the pipeline. The routing logic classifies it by tier. Tier 1 content goes to human translation. Tier 2 and 3 content goes to LLM translation with glossary and TM augmentation. The translated output passes through automated QA -- xCOMET scoring, pattern checks, terminology verification, and formatting validation. Outputs that pass all checks are delivered. Outputs that fail any check are routed based on the severity: minor issues are logged for trend analysis, moderate issues are flagged for human review in the next QA cycle, and critical issues are blocked and either retranslated with adjusted parameters or sent to a human translator.

The infrastructure requirements are modest. xCOMET scoring can run on the same GPU infrastructure you use for model inference. The xCOMET-XL model at 3.5 billion parameters is small enough to run alongside your translation pipeline without dedicated hardware. Pattern checks and terminology verification are CPU-bound string operations. Formatting validation is rule-based. The entire automated QA pipeline adds 200 to 500 milliseconds of latency per segment -- negligible for batch translation, acceptable for most real-time use cases.

The reporting layer is where QA generates long-term value. Every scored segment, every flagged issue, every human correction should feed into a quality dashboard that tracks translation quality by language, by content type, by time period, and by translation method. This dashboard tells you where quality is improving, where it is degrading, and where your investment in prompt improvements, glossary updates, or model changes is paying off. Without the dashboard, QA is a gate. With the dashboard, QA is a management system.

## The QA Maturity Path

Not every team starts with a comprehensive QA system. The maturity path for translation QA follows a predictable progression.

**Stage 1: No QA.** Translations ship unreviewed. Quality issues are discovered by users, reported through support tickets (if at all), and fixed reactively. This is where most teams start when they first add multilingual support, and it is acceptable for a limited time if the content is low-risk. It is not acceptable for any team that considers multilingual support a production capability.

**Stage 2: Manual spot checks.** A team member who speaks the target language reviews translations periodically -- maybe weekly, maybe before major releases. The reviews are inconsistent, undocumented, and dependent on that individual's availability and judgment. Better than nothing. Not a system.

**Stage 3: Automated scoring.** COMET or xCOMET scoring runs on all translated output. A threshold flags low-quality translations. The scores are logged and tracked over time. This is the minimum viable QA system, and it catches the worst failures automatically. Most teams can reach this stage within one to two engineering sprints.

**Stage 4: Automated scoring plus structured human review.** Automated scoring handles the bulk of QA. A structured human review process evaluates samples based on the five quality dimensions, with documented scoring rubrics, per-language reviewers, and findings that feed back into the translation system. This is the standard for teams that take multilingual quality seriously.

**Stage 5: Integrated quality management.** Automated and human QA are embedded in the translation pipeline. Quality metrics appear on production dashboards alongside model quality metrics. QA findings automatically trigger glossary updates, prompt modifications, and routing changes. Translation quality is treated as a production metric with the same rigor as model accuracy or latency. This is the target state for any product where multilingual quality directly affects revenue or safety.

The progression from Stage 1 to Stage 3 can happen in weeks. From Stage 3 to Stage 5 takes months and requires organizational commitment -- dedicated reviewers, engineering investment in the feedback loop, and leadership that treats translation quality as a product metric rather than a localization afterthought.

The translation and localization stack is only half the multilingual challenge. The other half is retrieval: how you find the right information when your users search in one language and your documents exist in another. The next chapter opens that problem -- the multilingual retrieval challenge that sits at the heart of every multilingual RAG system.