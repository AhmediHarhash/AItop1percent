# 10.1 â€” Code-Switching Is Not an Edge Case: Why Mixed-Language Input Is the Default

If you test your system with single-language input, you are testing for a minority of your users. More than half the world's population functions in two or more languages on a daily basis. Approximately 43 percent of people globally speak two languages fluently, with an additional 17 percent speaking three or more. When these speakers interact with your AI system, they do not neatly separate their languages. They mix them. They borrow words, switch mid-sentence, blend grammars, and produce input that no single-language test set has ever captured. If your evaluation pipeline uses clean, monolingual text, you are measuring performance against a user who does not exist in most of your markets.

This is not a niche concern for a handful of multilingual countries. This is the default communication pattern for the majority of the world's population. And until your system handles it, you are building for a fiction.

## What Code-Switching Actually Is

**Code-switching** is the practice of alternating between two or more languages within a single conversation, sentence, or even word. It is not translation. It is not error. It is a natural, rule-governed communication strategy used by bilingual and multilingual speakers everywhere.

The simplest form is inter-sentential code-switching, where the speaker uses one language for one sentence and another language for the next. A customer support message might start in Hindi, switch to English for a technical question, and close in Hindi again. More complex is intra-sentential code-switching, where languages alternate within a single sentence. A user in Manila might write "Can you check yung order ko from last week" -- mixing English and Filipino in a grammatically coherent structure that follows specific syntactic rules. The most granular form is intra-word code-switching, where morphemes from two languages combine into a single word. "Chating" combines the English root "chat" with a Hindi-influenced suffix. "Logearse" combines English "log" with a Spanish reflexive verb ending.

None of these are mistakes. Each follows patterns that linguists have studied for decades. The Matrix Language Frame model, developed by Carol Myers-Scotton, describes how one language provides the grammatical frame -- the word order, function words, and morphological structure -- while the other language contributes content words that slot into that frame. The language providing the grammatical skeleton is called the **matrix language**. The language contributing inserted elements is the **embedded language**. Understanding this distinction matters for AI systems because it determines which language the speaker is primarily operating in, which affects everything from response language to retrieval strategy.

## Who Code-Switches and Where

Code-switching is not limited to a few bilingual regions. It is the norm in most of the world outside monolingual pockets of the United States, United Kingdom, Japan, and a handful of other largely monolingual nations.

In India, Hindi-English code-switching -- often called **Hinglish** -- is the default communication mode for hundreds of millions of urban and semi-urban speakers. It appears in text messages, social media, customer support interactions, and everyday speech. An Indian user typing a query into your chatbot is more likely to produce Hinglish than pure Hindi or pure English.

In Singapore, speakers routinely blend English, Mandarin, Malay, and Tamil in a practice sometimes called **Singlish** when it includes distinctive local grammar. In the Philippines, **Taglish** -- Tagalog-English code-switching -- is so pervasive that many Filipinos consider it their natural register. In North Africa, Arabic-French code-switching is standard in Morocco, Algeria, and Tunisia, where a sentence might use French technical vocabulary embedded in a Darija Arabic grammatical frame. In the United States, Spanish-English code-switching among Latino communities has been extensively documented and studied for decades. Across Sub-Saharan Africa, code-switching between local languages and colonial-era European languages is the daily reality for millions.

In Europe, code-switching is common in bilingual regions like Catalonia, the Basque Country, Belgium, Switzerland, and South Tyrol. Urban centers across the continent increasingly feature code-switching among immigrant communities. A Turkish-German speaker in Berlin, an Arabic-Swedish speaker in Malmo, or a Polish-English speaker in London code-switches as naturally as breathing.

The pattern is clear. If your product serves users outside a handful of monolingual markets, code-switching is not an edge case in your traffic. It is a significant portion of it. The question is whether you know how much, because most teams never measure.

## Why People Code-Switch

Understanding why users code-switch helps you build better systems, because the motivation determines the pattern.

**Precision.** Some concepts are more precisely expressed in one language than another. A Hindi speaker discussing a bank transaction might switch to English for "fixed deposit" or "recurring payment" because the English terms are the standard vocabulary for these financial products in India. A French-Arabic speaker might use Arabic for emotional expression but French for bureaucratic or technical concepts. The speaker is not confused. They are selecting the most precise tool for each part of the message.

**Identity and solidarity.** Code-switching signals belonging. A Filipino professional might use English in a formal email but switch to Taglish when chatting with colleagues, signaling that the conversation has shifted from formal to casual. A second-generation Turkish-German speaker might code-switch to signal cultural identity. When users code-switch with your system, they are often communicating in the register that feels most natural to them. Forcing them into a single language is like forcing a formal tone on a casual conversation -- it works, but it feels wrong, and users notice.

**Accommodation.** Speakers code-switch to accommodate the perceived abilities of their conversation partner. A user who suspects the AI system handles English better might embed English keywords into their Hindi message to increase the chances of being understood. This is a learned accommodation strategy -- users have been trained by years of poor multilingual AI to modify their natural language to help the system. When your system rewards this accommodation by performing better on English-heavy input and worse on language-heavy input, you are reinforcing a pattern that excludes less English-proficient users.

**Vocabulary gaps.** Sometimes a speaker simply does not know or cannot recall the term in the other language. A user typing quickly may use the first word that comes to mind, regardless of language. This produces code-switched input that is unplanned and unpredictable -- the hardest type for systems to handle because it follows no consistent pattern.

## How Code-Switching Breaks Your System

The damage is not theoretical. It is measurable and it compounds across every layer of your pipeline.

Language detection fails first. Standard language identification tools -- FastText-based classifiers, CLD3, even more recent models like GlotLID -- are designed to assign a single language label to a text. When a sentence is 60 percent Hindi and 40 percent English, the classifier must pick one. It picks whichever language has slightly more statistical weight, and in doing so tells your downstream pipeline that the entire input is in that language. The pipeline then processes a mixed-language input through a monolingual lens, and everything after that point degrades.

Embedding models struggle next. As we will explore in detail in subchapter 10.5, multilingual embedding models create partially separated language clusters. Code-switched text does not land cleanly in any cluster. A Hinglish query about "mujhe ek loan chahiye for home renovation" produces an embedding that partially overlaps with Hindi documents and partially with English documents but strongly matches neither. Your retrieval system returns worse results than it would for the same query expressed entirely in Hindi or entirely in English.

Safety classifiers miss harm expressed across languages. A user who cannot get a harmful response in English might rephrase the request with half the words in another language. Research on code-mixed attacks has demonstrated attack success rates above 95 percent in some configurations, because the code-mixing breaks the tokenization patterns that safety classifiers rely on. Your safety dashboard shows 97 percent harm blocked. That number describes English. The code-switched bypass route describes reality.

Generation quality degrades. When the model receives code-switched input, it must decide what language to generate in. Often it defaults to English regardless of the input mix, producing responses that feel foreign to the user. Sometimes it attempts to match the code-switching pattern but produces awkward, unnatural mixed output that no real speaker would produce. The result is a system that feels like it does not understand the user -- because, in a meaningful sense, it does not.

## The Measurement Gap

The most concerning aspect of code-switching failure is that most teams do not know it is happening. Their test sets are monolingual. Their eval suites use clean, standardized text. Their metrics reflect performance against an idealized user who writes in one language at a time.

The CS-FLEURS benchmark, presented at Interspeech 2025, revealed the scale of the problem. It covers 52 languages across 113 code-switched language pairs and provides the first large-scale test of how speech systems handle real code-switched input. Whisper Large v3, one of the strongest automatic speech recognition models available, showed character error rates more than double on code-switched input compared to monolingual input. For language pairs that use different scripts -- Hindi in Devanagari mixed with English in Latin script -- error rates tripled. These are not minor degradations. They represent a system that works well in the lab and fails in the field.

The same pattern holds for text systems. Teams that have measured code-switching performance in production consistently report quality drops of 15 to 30 percent compared to monolingual input of equivalent semantic content. But most teams have never run this measurement, because their evaluation infrastructure was designed around the assumption that input arrives in one language at a time.

## The Business Case You Cannot Ignore

If you serve India, the Philippines, Singapore, North Africa, Latin America, or multilingual communities in Europe and North America, code-switching users are not a segment. They are your core market.

A fintech app serving Indian users that cannot handle Hinglish input is not serving India. It is serving the subset of Indian users who happen to write in pure English or pure Hindi, and that subset is smaller than most product teams assume. A customer support chatbot for the Philippine market that breaks on Taglish is broken for most Filipino users. A content moderation system in North Africa that cannot parse Arabic-French code-switching is blind to how people actually communicate in Morocco and Algeria.

The cost of ignoring code-switching shows up in three places. User experience degrades -- users get worse answers, feel misunderstood, and churn. Safety weakens -- code-switched bypass attacks exploit the gap your monolingual testing missed. And market reach shrinks -- you are functionally excluding the majority of users in your most multilingual markets by testing only for a communication pattern they do not use.

## The Reframe

Code-switching is not a complication to be handled. It is a communication norm to be supported. The monolingual assumption -- that users speak one language at a time and your system can safely process one language at a time -- is the edge case. It describes users in a handful of monolingual markets. The rest of the world mixes languages, and your system either handles that or it does not.

Every evaluation pipeline, every embedding model, every safety classifier, and every generation strategy you build must be tested against mixed-language input. Not as a stretch goal. Not as a phase-two improvement. As a baseline requirement for serving the majority of the world's population.

The next subchapter examines exactly how code-switching breaks each layer of a multilingual AI system -- detection, embeddings, and generation -- and quantifies the damage at each stage.
