# 9.8 â€” Red-Teaming Across Languages: Finding the Failures English Testing Misses

Your red team found 47 jailbreaks in English and fixed them all. The safety classifiers were updated, the guardrails were reinforced, and the compliance report looked clean. Then a team of native Arabic speakers sat down for their first day of adversarial testing and found 31 new jailbreaks before lunch. None of the English fixes blocked the Arabic attacks. The Arabic prompts exploited religious framing, script-based obfuscation, and culturally coded requests that the English red team had no reason to think of and no ability to recognize. Your system was not safe. It was safe in English.

This is the English red-teaming gap, and it is the single most predictable safety failure in multilingual AI systems. Most red-teaming exercises are conducted in English, by English speakers, against English-language attack vectors. The resulting safety improvements protect English-speaking users and leave everyone else exposed. Research published in 2025 confirmed what practitioners already suspected: LLMs are measurably more vulnerable when prompted in low-resource languages than in English. Safety alignment mechanisms that are robust in high-resource languages degrade significantly in underrepresented ones. Models from major providers, including DeepSeek and Qwen, failed when tested with prompts in underrepresented languages, revealing blind spots that no amount of English-language testing would have caught.

## Why English Red-Teaming Is Not Enough

The assumption behind English-only red-teaming is that safety is language-agnostic. If the model refuses to generate harmful content in English, it should refuse in every language. This assumption is wrong for three reasons.

First, safety alignment is disproportionately trained on English data. The reinforcement learning from human feedback, the constitutional AI principles, the safety fine-tuning datasets -- all of these are overwhelmingly English. When a model learns that "how to make a weapon" should trigger a refusal, it learns the English pattern. The same request in Swahili, Urdu, or Thai may not match the learned refusal patterns because the model has seen far fewer safety examples in those languages. The alignment is real but shallow outside of English.

Second, harmful content is expressed differently across languages and cultures. An English jailbreak attempt typically uses direct phrasing, role-play scenarios, or encoding tricks. An Arabic jailbreak might frame a harmful request in religious scholarly language. A Japanese attack might exploit politeness conventions to gradually escalate from a benign request to a harmful one. A Hindi attack might use caste-coded language that carries violent implications but contains no keywords that a safety filter would catch. These are not translations of English attacks. They are language-native attack vectors that emerge from the linguistic and cultural structure of each language.

Third, script diversity creates obfuscation opportunities that do not exist in Latin-script languages. Arabic script is bidirectional, allowing mixed-direction text that confuses classifiers. Chinese characters can be substituted with visually similar but semantically different characters. Devanagari has conjunct characters that can be decomposed in ways that break tokenization. These script-level attacks are invisible to anyone who does not read the script.

## Language-Specific Attack Vectors

Multilingual red teams consistently discover attack categories that have no English equivalent. Understanding these categories is the first step toward building defenses.

**Culturally coded harmful requests** use cultural references to frame harmful content as legitimate. In Arabic, framing a request as religious scholarship -- asking how a historical battle was conducted, how a weapon described in historical texts was constructed -- bypasses safety filters because the surface framing is educational. In Hindi, referencing historical caste violence as "cultural tradition" or "historical practice" can extract content that the model would refuse if the request were made directly. In Japanese, using keigo (formal politeness registers) to make a request sound official and legitimate can bypass safety checks that are tuned for casual or aggressive phrasing.

**Script-based obfuscation** exploits the visual and structural properties of non-Latin scripts. Mixing Arabic and Persian characters that look identical but have different Unicode code points. Using full-width Latin characters mixed with CJK text. Inserting zero-width characters between words to break pattern matching. Using transliteration -- writing Hindi in Latin script (Romanized Hindi) or Arabic in Latin script (Franco-Arabic) -- to evade classifiers trained on the native script. Each of these techniques is trivial for a native speaker to produce and difficult for an automated system to catch without language-specific preprocessing.

**Code-switching to evade filters** is one of the most effective multilingual attack strategies. The attacker writes most of the prompt in a well-monitored language like English, then switches to a less-monitored language for the harmful portion. "Please explain in detail how to" followed by the harmful action described in Swahili, Tagalog, or Bengali. The English portion looks benign. The non-English portion contains the harmful request. Safety classifiers that process the full prompt as a single unit often miss the language switch, and classifiers that detect the language as English (based on the majority of the text) apply English-only safety rules and miss the non-English harm entirely. The Multi-lingual Multi-turn Automated Red Teaming framework published in 2025 specifically targets this gap, testing how models behave when attacks span multiple languages across conversation turns.

**Low-resource language bypass** exploits the simple fact that models have less safety training in languages with less training data. Requests in Yoruba, Khmer, Amharic, or Lao may bypass safety filters that work perfectly in English, Spanish, and Chinese simply because the model has not been taught to refuse in those languages. This is not a sophisticated attack. It requires nothing more than asking a harmful question in a language the model was barely trained on.

**Transliteration attacks** exploit the gap between a language's native script and its Romanized form. Hindi written in Devanagari passes through Hindi safety filters. The same Hindi words written in Latin script (Hinglish) may not be recognized as Hindi at all, falling through the cracks between English and Hindi safety systems. This applies to Arabic (Franco-Arabic), Russian (Romanized Russian), and virtually any language that has both a native script and a common Romanized form used in informal digital communication.

## Building Multilingual Red Teams

Effective multilingual red-teaming requires native speakers who are not translating English attacks but creating language-native ones. The distinction matters enormously. A translated attack tests whether the model's safety alignment transfers across languages. A language-native attack tests whether the model can handle threats that only exist in that language. Both are necessary, but the second is far more revealing.

Recruit native speakers for each language you support. These are not translators. They are adversarial testers who understand the cultural context, the idiomatic patterns, and the script-level tricks specific to their language. A native Arabic speaker on your red team knows that religious framing is an effective bypass. A native Japanese speaker knows that politeness escalation works. A native Hindi speaker knows which caste references carry implicit violence. No amount of briefing can give an English speaker this knowledge. It is tacit, cultural, and linguistic.

The ideal multilingual red team member has three qualities. They are a native speaker of the target language with cultural fluency in at least one major region where that language is spoken. They have enough technical understanding to think adversarially about how models process text. And they have domain knowledge in the types of harms most relevant to their language -- religious sensitivity, political sensitivity, social hierarchy, gender dynamics, or whatever the primary risk vectors are.

For each language, the red team should work in structured rounds. The first round tests translated English attacks to establish a baseline: do known English jailbreaks work when translated? The second round creates language-native attacks based on the cultural and linguistic patterns specific to that language. The third round combines both with code-switching, transliteration, and script obfuscation techniques. The first round typically finds gaps in safety alignment transfer. The second round finds the vulnerabilities that English testing could never have revealed. The third round finds the hybrid attacks that are most likely to appear in real-world abuse.

## Per-Language Findings: What Each Language Reveals

Multilingual red teams consistently find that each language surfaces unique vulnerability categories. Understanding these patterns helps you prioritize and prepare.

Arabic red teams typically uncover gaps in religious sensitivity handling. They find that models will generate content about religious practices, figures, or sectarian issues that would be deeply offensive or dangerous in Muslim-majority contexts. They also find script-based attacks using Arabic-Persian character substitution, bidirectional text tricks, and the mixing of Modern Standard Arabic with regional dialects to confuse language detection.

Japanese red teams exploit the politeness system. Japanese has multiple registers of formality, and a request made in the most formal register can sound like an official instruction. Models trained primarily on casual English safety data often fail to recognize when a formal Japanese prompt is actually a manipulation tactic. Japanese red teams also find encoding attacks using the multiple Japanese writing systems -- switching between hiragana, katakana, and kanji to break pattern matching.

Hindi red teams surface caste-related harms that are invisible to non-Indian testers. Caste-based discrimination, slurs, and coded references are a major category of harmful content in Hindi, and these terms have no direct English equivalents. A safety filter trained on English hate speech categories will not catch a caste slur because it was never trained to recognize one. Hindi red teams also exploit the Hinglish phenomenon -- mixing Hindi and English within a single prompt to fall between both language safety systems.

Chinese red teams find political sensitivity gaps. References to events, figures, or territories that are politically sensitive in mainland China, Taiwan, or Hong Kong trigger different harms depending on the audience, and models often handle these inconsistently. Chinese red teams also exploit character substitution -- using visually similar but different characters to bypass keyword filters, a technique with a long history in Chinese internet censorship evasion.

Spanish red teams reveal the gap between Latin American and European Spanish. Content that is benign in Spain may be offensive in Mexico, Colombia, or Argentina due to differences in slang, cultural norms, and historical sensitivities. A single "Spanish" safety filter misses these regional variations entirely.

## Automated Multilingual Red-Teaming

Human red teams are essential but expensive and slow. Automated red-teaming tools can scale adversarial testing across languages, but only if they are designed for multilingual operation.

Most automated red-teaming frameworks in 2025 and 2026 operate primarily in English. They generate adversarial prompts, test model responses, and iterate. Extending these to multilingual operation requires more than translation. The prompt generation must produce language-native attacks, not translated English ones. The response evaluation must understand cultural context in each language. The iteration strategy must be informed by language-specific attack patterns.

The emerging approach is hybrid: use automated tools to generate high-volume, broad-coverage adversarial prompts across languages, then use human red teamers to evaluate the results, identify false negatives, and create the culturally specific attacks that automated tools miss. The automated system handles scale. The human team handles nuance. Neither is sufficient alone.

For each language, maintain a growing adversarial prompt library that includes both human-crafted and machine-generated attacks. Tag each prompt by language, attack type (cultural, script-based, code-switching, low-resource bypass, transliteration), and severity. This library becomes the foundation for regression testing: every time you update the model or the safety classifiers, run the full library to ensure that previously discovered attacks remain blocked.

## Reporting and Remediation

Red team findings must be structured for action, not just documentation. Each finding should include the language, the attack type, the specific prompt or prompt pattern, the model response, the severity of the harm, and the recommended remediation.

Categorize findings along two axes. The first axis is language -- which languages are vulnerable and to what types of attacks. The second axis is attack type -- whether the vulnerability is in safety alignment transfer, cultural sensitivity, script handling, code-switching detection, or low-resource coverage. This dual categorization enables targeted fixes. A vulnerability in Arabic religious sensitivity framing requires different remediation (cultural sensitivity training data) than a vulnerability in Romanized Hindi detection (script-handling preprocessing).

Remediation must be tested per language. A fix that blocks a jailbreak in Arabic must be verified not to break legitimate Arabic queries. A fix that catches code-switching attacks must be verified not to flag legitimate multilingual conversations. And the remediation must be tested in the specific language where the vulnerability was found, not just in English. Safety teams that fix a vulnerability in English and assume the fix works in all languages are repeating the exact mistake that created the vulnerability in the first place.

Set a remediation SLA by severity. Critical findings -- attacks that produce content that could cause real-world harm -- should be blocked within 48 hours. High-severity findings should be remediated within two weeks. Medium and low findings enter the standard sprint cycle. Track these SLAs per language. If Arabic critical findings are remediated in 48 hours but Thai critical findings take three weeks because your team lacks Thai expertise, your safety program has a language-based inequality that must be addressed.

## Cross-Reference: Section 22

This subchapter covers the multilingual dimension of red-teaming. For the comprehensive methodology -- team composition, attack taxonomies, reporting frameworks, organizational integration, and the full lifecycle of adversarial testing -- see Section 22 on Red-Teaming and Adversarial Testing. The principles in Section 22 apply to every language. The principles in this subchapter apply specifically to the additional attack surface that multilingual operation creates.

## Building the Continuous Multilingual Red-Teaming Program

Red-teaming is not a one-time exercise. Models change. Safety classifiers are updated. Cultural contexts shift. New languages are added. A red team finding from six months ago may have been fixed -- or a fix from six months ago may have regressed. Continuous multilingual red-teaming means running adversarial testing on a regular cadence, updating the adversarial prompt library with new attack patterns, and expanding language coverage as your product grows.

Schedule red-teaming rounds per language on a quarterly cadence at minimum. Higher-risk languages -- those with the largest user bases, the most sensitive cultural contexts, or the most documented safety gaps -- should be tested monthly. Every model update, every safety classifier update, and every expansion to a new language should trigger an ad hoc red-teaming round for affected languages.

The cost of a multilingual red-teaming program is real. Native-speaker adversarial testers command higher rates than general annotators, typically 75 to 150 dollars per hour depending on language and expertise. A quarterly red-teaming round covering ten languages with two testers per language for two days each costs roughly 25,000 to 50,000 dollars per round. This is a fraction of the cost of a single culturally insensitive incident in a major market, which can easily run into millions in reputational damage, regulatory fines, and lost users.

Red-teaming tells you where your safety system fails. But even the best testing program only catches failures before they reach users. Once content is live, you need moderation at scale -- the subject of the next subchapter, which examines how to build content moderation systems that work across languages without creating safety inequalities between your highest-resource and lowest-resource markets.
