# 3.12 â€” The Tokenizer Selection Decision: API Models vs Open-Weight Models

When you choose a model, you also choose a tokenizer. Most teams evaluate the model -- benchmark scores, latency, pricing, context window -- and never once examine the tokenizer that determines how their text is represented. This is like choosing a car by its engine horsepower and ignoring the transmission. The engine might be powerful, but if the transmission loses half the power before it reaches the wheels, the car underperforms. The tokenizer is the transmission of your multilingual system. It determines how efficiently your text is converted into the units the model processes, and for multilingual products, that efficiency varies by a factor of two to five depending on the language. This subchapter gives you the decision framework for choosing tokenizers -- or more precisely, for choosing models with tokenizers that match your language portfolio.

## API Models: You Accept What You Get

When you use an API model from OpenAI, Anthropic, Google, or any hosted provider, you accept their tokenizer as-is. You have zero control over the vocabulary, zero control over the merge rules, and zero control over the fertility ratios for your target languages. The tokenizer is a fixed property of the model, inseparable from the weights, and it changes only when the provider ships a new model version.

GPT-5 and GPT-5-mini use the o200k_base tokenizer -- a vocabulary of approximately 200,000 tokens built with byte-pair encoding. The o200k_base vocabulary doubled the size of GPT-4's cl100k_base, adding significantly more coverage for non-Latin scripts. This improved token efficiency for Chinese, Arabic, Hindi, and many other languages, reducing fertility ratios by 15 to 30% compared to the previous generation. But o200k_base is still English-dominant. English text tokenizes at roughly 1.1 tokens per word. Japanese text still runs at 2.0 to 2.5 tokens per character depending on content. Arabic runs at 2.0 to 3.0 tokens per word. The improvement is real. The parity is not.

Claude Opus 4.6, Sonnet 4.5, and Haiku 4.5 use Anthropic's proprietary tokenizer. Anthropic has not publicly released their tokenizer's vocabulary or technical details, but empirical analysis from researchers in 2025 and early 2026 shows competitive multilingual coverage -- roughly comparable to o200k_base for major world languages, with some variations in specific script handling. You cannot inspect the tokenizer directly, which means you must measure fertility empirically by counting tokens on representative text samples through the API's token counting endpoint.

Gemini 3 Pro, Flash, and Deep Think use a SentencePiece tokenizer with approximately 262,000 tokens -- the largest vocabulary among major providers. Google's access to multilingual data through Search and Translate gives this tokenizer broad coverage across over 140 languages. The 262,000-token vocabulary allocates more space to non-Latin scripts than smaller vocabularies can afford, which translates to measurably better fertility ratios for languages like Arabic, Hindi, Thai, and Vietnamese. For teams serving a wide range of languages, Gemini's tokenizer is currently among the most balanced of the major API providers.

The constraint with all API models is that the tokenizer is not a separate decision. You get the model and the tokenizer as a package. If GPT-5 has the best quality for your task but the worst tokenizer for your primary language, you are stuck with the trade-off. You can mitigate it with prompt compression, preprocessing, and the strategies from subchapter 3.10, but you cannot change the tokenizer itself.

## Open-Weight Models: Control at a Cost

Open-weight models give you options that API models do not. You can inspect the tokenizer's vocabulary, measure fertility ratios precisely, and -- with significant engineering effort -- swap or adapt the tokenizer for your specific language needs. This control comes with costs that most teams underestimate.

Llama 4 Scout and Maverick use a tokenizer with approximately 128,000 tokens. Meta invested in multilingual support for about twelve languages, with improved coverage for Arabic, Hindi, Thai, Vietnamese, and Indonesian compared to Llama 3. But twelve languages out of the world's thousands still leaves many languages in the high-fertility zone. If your primary languages happen to be among Meta's twelve, Llama 4's tokenizer serves you well. If your primary language is Burmese, Georgian, or Amharic, you inherit a tokenizer that fragments your text into bytes.

Qwen 3, developed by Alibaba, takes a deliberately CJK-optimized approach. Its tokenizer allocates substantial vocabulary space to Chinese, Japanese, and Korean characters, achieving fertility ratios for Chinese that approach English efficiency -- roughly 1.5 tokens per character for common text. Qwen is the obvious choice when your primary market is Chinese-speaking. But that CJK optimization comes at a trade-off: Qwen's tokenizer offers less efficiency for Arabic, Hindi, and African languages than Gemma's broader approach.

Gemma 3, developed by Google, shares the same SentencePiece tokenizer and 262,000-token vocabulary as Gemini. This makes Gemma 3 the most tokenizer-balanced open-weight model for teams serving many languages. The breadth of coverage means no single language gets the extreme optimization that Qwen provides for CJK, but no language gets left behind as badly as it would with a 100,000-token English-centric vocabulary.

The key advantage of open-weight models is not that they have better tokenizers by default -- some do, some do not. The advantage is that you can choose the model whose tokenizer matches your language needs, and in extreme cases, you can modify the tokenizer itself.

## Tokenizer Replacement: Powerful but Expensive

Research from 2025 and early 2026 has made tokenizer replacement for open-weight models increasingly viable, though still far from trivial. Techniques like Zero-Shot Tokenizer Transfer and Model-Aware Tokenizer Transfer allow you to swap a model's tokenizer without retraining the entire model from scratch. The approach works by mapping token embeddings from the old vocabulary to the new vocabulary, preserving as much of the model's learned representations as possible.

The practical reality is that tokenizer replacement requires substantial engineering expertise, significant compute for the adaptation process, and rigorous evaluation to verify that the swap did not degrade quality. A team that replaces the tokenizer on a seven-billion-parameter model should budget two to four weeks of engineering time and several hundred dollars in compute for the adaptation runs. For a seventy-billion-parameter model, the compute cost rises to thousands of dollars and the engineering time extends proportionally.

Tokenizer replacement is justified in a narrow set of circumstances. If your product serves primarily one or two high-fertility languages, and no available model offers both acceptable quality and acceptable tokenization for those languages, building a custom tokenizer and adapting an open-weight model to use it can reduce your per-query cost by 40 to 60% for those languages. That cost reduction, compounded over millions of queries per month, can justify the upfront engineering investment within a single quarter.

For most teams, tokenizer replacement is overkill. Choosing a model whose existing tokenizer is well-suited to your language portfolio achieves 80% of the benefit at 10% of the effort.

## The Decision Factors

Four factors determine whether to use an API model with its fixed tokenizer or an open-weight model with tokenizer flexibility.

**Which languages you serve.** If your primary languages are English and a handful of well-resourced European languages, every major API model's tokenizer is adequate. The fertility ratios for Spanish, French, German, and Portuguese are close enough to English that the token tax is modest. Use whatever model gives you the best quality, and do not worry about the tokenizer. If your primary languages include CJK, Arabic, Thai, or any Tier 3 or Tier 4 language, the tokenizer becomes a material cost factor and should influence your model choice.

**Your cost sensitivity.** If inference cost is a minor line item -- perhaps you serve low volumes or your per-query revenue is high enough to absorb the token tax -- then tokenizer efficiency matters less than model quality. If inference cost is a major budget concern -- you serve high volumes, your margins are thin, or you are in a price-competitive market -- then per-language token efficiency directly affects your business viability. Teams spending more than $20,000 per month on inference for non-English languages should evaluate model options based on tokenizer efficiency alongside quality benchmarks.

**Your control requirements.** API models give you zero control over tokenizer changes. When a provider ships a new model version, the tokenizer may change, and your per-language costs may shift overnight. OpenAI's transition from cl100k_base to o200k_base improved multilingual efficiency, but a future change could go the other direction. Open-weight models give you version control: you deploy a specific model with a specific tokenizer, and it does not change until you choose to upgrade. For teams that need predictable, stable per-language costs, this control is worth the infrastructure overhead of self-hosting.

**Your latency constraints.** API models handle infrastructure for you. Open-weight models require you to manage inference servers, GPU allocation, and scaling. If your product requires sub-200-millisecond response times across global regions, the infrastructure complexity of self-hosting open-weight models may outweigh the tokenizer benefits. If latency is flexible -- batch processing, async workflows, or offline evaluation -- self-hosting an open-weight model with a superior tokenizer for your languages is more feasible.

## When Tokenizer Quality Should Override Model Quality

This is the contrarian claim that most teams resist: sometimes you should choose a model with lower benchmark scores because its tokenizer is better for your languages.

Consider a product where 70% of traffic is in Chinese, Japanese, and Korean. Model A scores 92% on your task-specific evaluation in CJK languages. Model B scores 89%. Model A uses a tokenizer with CJK fertility of 2.3 tokens per character. Model B uses a tokenizer with CJK fertility of 1.6 tokens per character. At a million CJK queries per month, the tokenizer difference translates to a 30% cost reduction on Model B. Over twelve months, that savings can reach six figures.

The three-point quality difference between 92% and 89% may or may not matter to your users. The six-figure cost difference definitely matters to your business. And the cost savings can be reinvested into quality improvement: more evaluation data, better prompts, human review on edge cases. In many cases, the team that chooses the cheaper model and invests the savings in quality engineering ends up with a better product at lower total cost than the team that chose the more expensive model and accepted the tokenizer tax.

This calculus does not apply to every situation. If the quality gap is large -- ten points or more -- the model quality usually wins. If your CJK traffic is 10% rather than 70%, the tokenizer savings are proportionally smaller. But for products where non-English traffic is the majority and cost is a real constraint, tokenizer quality deserves equal weight with model quality in your selection process.

## The Hybrid Approach

You do not have to choose one model for everything. A hybrid architecture uses API models for languages where their tokenizer is efficient and open-weight models for languages where specialized tokenizers provide a cost advantage.

The most common hybrid pattern in 2026 routes English and Tier 1 European languages to an API model like GPT-5 or Claude, which offer strong quality and acceptable tokenizer efficiency for these languages. CJK traffic routes to a Qwen-based open-weight deployment optimized for Chinese, Japanese, and Korean. Arabic and Southeast Asian traffic routes to a Gemma 3 deployment that benefits from its 262,000-token vocabulary.

The engineering cost of a hybrid architecture is real: multiple model integrations, language-based routing logic, separate prompt variants per model, and separate evaluation suites per model-language combination. But for products serving five or more languages at significant volume, the cost savings from tokenizer-optimized routing typically pay for the engineering overhead within three to six months.

## Future-Proofing Your Tokenizer Decision

Tokenizer technology is not static. Between 2024 and 2026, vocabulary sizes grew from 100,000 to 262,000, multilingual coverage improved significantly, and research on tokenizer transfer made swapping tokenizers increasingly practical. These trends will continue.

Budget for periodic reevaluation. Every six months, remeasure your fertility ratios on the latest model versions. A provider that shipped an English-heavy tokenizer in 2025 may release a significantly more multilingual tokenizer in their next version. An open-weight model that currently requires tokenizer replacement for your languages may release a variant with native support. The tokenizer landscape changes faster than the model quality landscape, because expanding a vocabulary is cheaper than retraining a model.

Keep your cost model current. When you reevaluate tokenizers, update your per-language cost projections and compare them against your current architecture. If a new model's tokenizer reduces your Japanese fertility from 2.5 to 1.8, the cost savings may justify migrating even if the model quality is similar. If a new open-weight release makes your hybrid architecture unnecessary by offering good tokenization across all your languages in a single model, simplifying to one model reduces operational complexity.

The tokenizer is the hidden variable in your multilingual cost equation. Most teams discover it after launch, when the invoice arrives. The teams that build durable multilingual products discover it before launch, model for it, and revisit it regularly.

This chapter has traced the token tax from its origin in byte-pair encoding through its impact on cost, quality, and context windows, and into the practical strategies for measuring, mitigating, and budgeting for it. The next chapter shifts from how your system processes multilingual text to how you evaluate whether it processes it well -- starting with the most common and most dangerous shortcut in multilingual evaluation: translating your English eval suite and trusting the results.