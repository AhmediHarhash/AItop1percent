# 2.3 — High-Resource vs Low-Resource: The Language Tier System

Not all languages are created equal in the eyes of a language model. This is not a political statement. It is a technical reality with direct consequences for what you can build, how much it will cost, and whether your product will work at all. A model that achieves 92% accuracy in English may hit 88% in Spanish, 76% in Japanese, 62% in Hindi, and 41% in Yoruba — on the same task, with the same prompt, through the same API. Those are not different quality levels. Those are different products. The Spanish user gets a professional tool. The Yoruba user gets something closer to a random number generator with plausible grammar.

**The Language Tier System** organizes this reality into a decision framework. It categorizes languages by the performance gap you should expect from current models, the investment required to reach production quality, and the strategies available for closing the gap. The tiers are not fixed attributes of languages themselves. They are descriptions of how current models treat languages, shaped by training data availability, tokenizer efficiency, script complexity, and the commercial incentives of model providers. A language's tier can shift as training data grows, models improve, and regional specialists emerge. But in early 2026, the tiers are remarkably stable, and ignoring them is the single fastest way to ship a multilingual product that fails.

## Tier 1 — Near-Parity Languages

**Performance gap from English: 3 to 5 percentage points.** Languages: French, German, Spanish, Portuguese, Italian, Dutch, Russian. Swedish, Norwegian, Danish, and Finnish often fall into this tier as well, though with slightly wider gaps approaching 6 to 7 points.

These languages share three advantages that models exploit heavily. First, massive internet representation. French, German, and Spanish each account for 3 to 7% of Common Crawl, providing models with billions of tokens of diverse, high-quality text. Second, structural similarity to English. Latin-script writing, shared vocabulary through French and Latin roots, and grammatical structures that overlap enough for positive transfer during pretraining. Third, robust evaluation coverage. Every major benchmark — MMMLU, MMLU-ProX, INCLUDE, BenchMAX — covers these languages with large, high-quality question sets, which means model developers actively optimize for them.

In production, Tier 1 languages behave almost like English. Generation quality is high. Instruction following transfers well from English system prompts. Domain-specific vocabulary is generally represented in the model's training data — French legal terminology, German engineering vocabulary, Spanish medical terminology. The gaps that remain are nuanced: idiomatic expressions that the model renders too literally, formality register errors in languages where register matters more than in English (French "tu" versus "vous," German "du" versus "Sie"), and cultural references that default to English-speaking contexts.

For most applications, Tier 1 languages require minimal additional investment beyond translation of prompts and localization of examples. You still need native-speaker evaluation — automated metrics miss the register and idiom issues — but you do not need language-specific models, specialized fine-tuning, or parallel infrastructure. A single frontier model serves Tier 1 languages well enough for production deployment.

The mistake teams make with Tier 1 languages is assuming they need no attention at all. A 4-point accuracy gap may sound small, but at production scale it means thousands of degraded interactions per day. And the gap widens on harder tasks. Simple Q and A may show a 3-point gap. Complex multi-step reasoning may show an 8-point gap. Summarization quality may degrade more than factual accuracy. The tier tells you the floor, not the ceiling, of the effort required.

## Tier 2 — Moderate Gap Languages

**Performance gap from English: 8 to 15 percentage points.** Languages: Chinese (Simplified and Traditional), Japanese, Korean, Turkish, Polish, Czech, Vietnamese, Ukrainian, Romanian, Hungarian.

Tier 2 languages have strong online presence — Chinese is the second-largest language on the internet by volume, and Japanese and Korean have vibrant digital ecosystems — but they differ from English in ways that create compounding technical challenges. Three factors drive the wider gap.

The first factor is script divergence. Chinese, Japanese, and Korean use non-Latin writing systems. Japanese uses three writing systems simultaneously. These scripts interact differently with byte-pair encoding tokenizers, which were designed primarily around Latin-script languages. The result is the **token tax** — the same semantic content requires far more tokens to represent in CJK languages than in English. Research on tokenizer efficiency shows that most BPE-based tokenizers assign approximately one token per character for CJK text, resulting in roughly 4 to 5 times more tokens per semantic unit compared to English. This means CJK text is 4 to 5 times more expensive to process, fits less content into context windows, and hits token limits faster. The token tax is not just a cost problem. It is a quality problem, because the model has fewer tokens of budget to reason through the same problem.

The second factor is grammatical distance. Turkish is agglutinative — single words carry meaning that English spreads across three or four words. Japanese sentence structure is subject-object-verb, the inverse of English subject-verb-object. Korean's honorific system encodes social relationships that English handles through word choice alone. These structural differences mean that positive transfer from English training is limited. The model cannot simply map English patterns onto these languages. It must learn distinct patterns, and the training data for those distinct patterns is smaller than the English data.

The third factor is cultural distance from English-centric training. The model's world knowledge skews heavily toward English-speaking contexts — American history, Western cultural references, common law legal systems. When a Japanese user asks about a topic that exists primarily in Japanese contexts — local tax regulations, Japanese employment law, regional cultural practices — the model may lack the knowledge entirely or confuse Japanese concepts with superficially similar Western ones.

For Tier 2 languages, model selection matters significantly. The gap between the best and worst frontier models on a Tier 2 language can be 8 to 12 percentage points. Qwen 2.5 72B and Qwen 3 are measurably stronger than GPT-5 for Chinese and Japanese tasks. Mistral Large 3 outperforms Llama 4 on Turkish and Polish. Choosing the wrong model for a Tier 2 language can push your effective quality from the Tier 2 range down into Tier 3 territory.

Production deployment for Tier 2 languages typically requires at least three additional investments beyond what Tier 1 languages need. You need language-native evaluation by native speakers who understand domain-specific usage, not just grammar. You need prompt engineering that accounts for the target language's structure — system prompts designed for Japanese should account for the language's indirectness, while Turkish prompts should anticipate the morphological complexity of the output. And you need cost modeling that accounts for the token tax. A product that is cost-viable in English at $0.003 per interaction may cost $0.012 to $0.015 per interaction in Chinese or Japanese simply because of tokenizer inefficiency.

## Tier 3 — Significant Gap Languages

**Performance gap from English: 15 to 22 percentage points.** Languages: Arabic (Modern Standard and dialects), Hindi, Bengali, Urdu, Thai, Indonesian, Farsi, Tamil, Telugu, Marathi.

Tier 3 languages sit in a painful middle zone. They represent large user populations — Hindi has over 600 million speakers, Arabic over 400 million, Bengali over 300 million — but the model infrastructure has not caught up with the demand. The gap is driven by three compounding factors.

Training data scarcity relative to speaker population is the primary cause. Hindi, despite its massive speaker base, accounts for less than 1% of Common Crawl. Arabic is better represented than Hindi but suffers from extreme dialectal variation — the Arabic that appears in training data is disproportionately Modern Standard Arabic from news sources, while real users speak Egyptian Arabic, Gulf Arabic, Levantine Arabic, Maghrebi Arabic, or dozens of other varieties. Models trained on Modern Standard Arabic produce text that sounds formal and artificial to users who expect colloquial communication. Bengali, Urdu, Tamil, and Telugu are even more underrepresented, with minimal high-quality web text beyond news and Wikipedia.

Script and tokenizer challenges compound the data problem. Arabic's right-to-left script, connected letterforms, and diacritical marks create tokenization challenges. Hindi's Devanagari script, Thai's lack of spaces between words, and Bengali's complex conjunct characters all interact poorly with tokenizers optimized for Latin and CJK scripts. The token tax for these languages is typically 2 to 3 times English, less severe than CJK but still significant.

The evaluation gap is perhaps the most dangerous factor. Most published benchmarks provide thin coverage of Tier 3 languages. INCLUDE covers some — Hindi, Arabic, Thai, Indonesian — but with fewer questions and narrower domain coverage than Tier 1 or Tier 2 languages. MMLU-ProX includes Arabic, Hindi, and Thai, but the translated-from-English format understates the real performance gap on native-language tasks. BenchMAX includes a subset. The practical consequence is that teams often cannot accurately assess model quality for Tier 3 languages using published benchmarks alone. The score looks acceptable on paper, but the model fails in production because the benchmark did not test what production demands.

For Tier 3 languages, model selection alone is not sufficient. You need a combination strategy: select the best available model, supplement with prompt engineering specifically designed for the target language, build native-speaker evaluation infrastructure, and often fine-tune on domain-specific data in the target language. Regional specialist models — Jais for Arabic, Aya Expanse for Hindi and Bengali — often outperform frontier models despite being smaller, because their training data is more relevant. A 13-billion-parameter model trained on diverse Arabic text may produce better Arabic output than a 200-billion-parameter model whose Arabic training consisted mostly of translated web pages and news articles.

The business case for Tier 3 languages requires honest math. If your quality requirement is 85% accuracy and the best available model achieves 68% in Hindi, you are facing a 17-point gap that no prompt engineering will close. You either invest in fine-tuning, accept lower quality, or do not launch in that language. Many teams choose the fourth option without realizing it: they launch, see the numbers, and quietly deprioritize the language while never formally deciding to support it poorly.

## Tier 4 — Barely Functional Languages

**Performance gap from English: 22 percentage points or more, or effectively untested.** Languages: Yoruba, Igbo, Amharic, Zulu, Xhosa, Hausa, Somali, most other African languages, most Indigenous languages worldwide, Burmese, Khmer, Lao, Pashto, Mongolian, and dozens of South and Southeast Asian languages with small digital footprints.

Tier 4 is not a tier in the same sense as the others. It is a warning label. For these languages, frontier models produce output that may look plausible to a non-speaker but is frequently nonsensical, factually wrong, grammatically broken, or culturally inappropriate to anyone who actually reads the language. On MMLU-ProX, the best-performing model scored 30 points below English on Swahili, and Swahili is one of the better-resourced African languages. For Yoruba, Igbo, and Amharic, the gaps are wider and the benchmarks are thinner.

The core problem is near-total training data absence. These languages have minimal web presence. Their written traditions may be primarily oral, their digital content may be concentrated in a few domains, and their writing systems may have limited support in standard text processing tools. When models encounter these languages during training, they see too little data to learn meaningful patterns. The result is a model that has memorized some surface statistics of the language — common word patterns, basic sentence structures — without understanding grammar, meaning, or pragmatics.

The danger of Tier 4 is not that the model produces obviously broken output. The danger is that the model produces plausible-looking output that is subtly wrong in ways that only a native speaker can detect. A model generating Yoruba text may produce sentences that look grammatically structured but use words incorrectly, confuse tonal distinctions that change meaning entirely, or mix registers in ways that a Yoruba speaker would immediately recognize as artificial. If your team does not include native speakers of the target language, you have no way to detect these failures, and your users will discover them before you do.

For Tier 4 languages, the honest assessment is that general-purpose models are not ready for production use. Your options are narrow. Aya Expanse provides the broadest coverage of low-resource languages but with quality that is still well below production standards for most applications. Fine-tuning on domain-specific native-language data can improve performance, but acquiring that data is itself a significant challenge for languages with limited digital presence. Machine translation from a Tier 1 or Tier 2 language, presented transparently as translation rather than native generation, may produce better results than asking the model to generate natively — and it sets honest expectations with your users.

Some organizations are taking a different approach: working with native-speaker communities to create training data, building language-specific models from scratch, and contributing to open datasets that benefit the entire ecosystem. These efforts are essential for long-term progress, but they operate on a timeline of years, not quarters. If you need to serve Tier 4 languages today, your strategy is mitigation, not solution.

## How Tiers Shift Over Time

Language tiers are not permanent. They reflect the current state of training data, model architecture, and commercial investment. The movement between tiers follows a predictable pattern: commercial demand drives training data investment, which drives model improvement, which improves benchmark scores, which generates more commercial confidence, which drives more deployment.

Chinese moved from roughly Tier 2 quality in 2023 to the upper end of Tier 2 or even Tier 1 quality by 2026, driven primarily by Qwen's and DeepSeek's massive investment in Chinese-language pretraining data. Turkish improved meaningfully as Mistral and Llama 4 invested in broader European and MENA language coverage. Indonesian benefited from SEA-LION's focused training. Swahili remains at the border of Tier 3 and Tier 4, improving slowly as organizations like Cohere For AI invest in East African language data through the Aya initiative.

The implication for your roadmap is that language tier assignments should be reevaluated every six months. A language that was Tier 3 when you last evaluated may have moved to Tier 2 through a new model release or a regional specialist emerging. Your mitigation strategy for that language — the fine-tuning investment, the specialized routing, the lower quality threshold you set — may no longer be necessary. Conversely, if a model provider that your Tier 2 language depends on changes their pricing or deprecates a model, your effective tier can move backward overnight.

## Using the Tier System for Product Decisions

The tier system is not just a technical classification. It is a product strategy tool that maps directly to launch decisions, quality targets, and investment budgets.

For Tier 1 languages, the default answer is yes — launch, target production-grade quality, and invest in native-speaker evaluation with modest additional effort. For Tier 2 languages, the answer is conditional — launch if you can invest in language-specific model selection, native-speaker evaluation, and prompt engineering, and if the revenue or strategic value justifies the token tax overhead. For Tier 3 languages, the answer requires serious cost-benefit analysis — the investment to reach production quality is substantial, and you should only commit if the market opportunity or regulatory requirement is clear and funded. For Tier 4 languages, the default answer is not yet — unless you are willing to build infrastructure that does not currently exist, or unless you can deploy with transparent limitations and a human-in-the-loop safety net.

This is uncomfortable. It means telling stakeholders that your product cannot serve certain language markets with the quality your brand requires. It means saying no, or saying not yet, or saying yes with caveats that the sales team finds inconvenient. But launching in a language where the model produces subtly broken output is worse than not launching at all. A user who encounters your product in their native language and finds it incompetent will not come back when it improves. They will tell everyone they know that it does not work.

The next subchapter examines the tier system in action for a specific language family — the CJK languages where specialized models have changed the competitive dynamics entirely.
