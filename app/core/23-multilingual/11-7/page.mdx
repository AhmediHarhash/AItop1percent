# 11.7 â€” Model Merging for Cross-Lingual Capability Transfer

In late 2025, a language technology company in Nairobi set out to build a single model that could handle financial customer support in English, Swahili, Amharic, and Somali. They had abundant English training data -- twelve thousand high-quality examples. They had a respectable Swahili dataset of three thousand examples, sourced from their own support transcripts. For Amharic, they scraped together eight hundred examples. For Somali, they had fewer than two hundred. The team's plan was straightforward: combine all the data into a single multilingual fine-tuning run and let cross-lingual transfer fill the gaps. Three weeks later they had a model that was mediocre in every language and excellent in none. English performance dropped from the base model's strong baseline because the mixed training data pulled it toward simpler sentence structures. Swahili performance improved modestly but the model kept inserting English financial terms where Swahili equivalents existed. Amharic performance barely moved. Somali performance was essentially random.

The model had learned to be average everywhere instead of strong anywhere. The team had run into the **blending trap** -- the assumption that throwing multilingual data into a single fine-tuning run produces a model that is good at all languages. In practice, languages compete for the model's capacity during training. High-resource languages dominate gradient updates. Low-resource languages get drowned out. The result is a model that has shifted slightly toward every language but arrived fully at none.

Model merging offers a fundamentally different approach. Instead of training one model on all languages simultaneously, you train separate models -- each specialized on a single language or task -- and then combine their weights after training. The merged model carries capabilities from each individual model without any of them having competed for capacity during training.

## What Model Merging Actually Is

Model merging takes two or more model checkpoints that share the same architecture and combines their parameters into a single model. The simplest form is a weighted average: for each parameter in the model, the merged value is a weighted combination of the corresponding parameters from each source model. The merged model is then used directly for inference without any additional training.

This sounds too simple to work. And for a long time, the machine learning community assumed it did not work well. But research from 2024 through 2025 repeatedly demonstrated that model merging produces surprisingly capable models, particularly when the source models are fine-tuned from the same base model on different but complementary tasks. The intuition is that fine-tuning from a shared base model means all the source models live in the same region of parameter space. Their fine-tuning moved them in different directions -- one toward English financial text, another toward Swahili financial text -- but they started from the same point. Combining their movements produces a model that has shifted partially in all directions simultaneously, landing in a region of parameter space that captures aspects of each specialization.

For multilingual systems, this is transformative. You can train a Swahili financial expert and an Amharic financial expert as separate models, each receiving the full benefit of dedicated training without cross-lingual interference, and then merge them into a single model that handles both. The merged model has never seen Amharic and Swahili data in the same training run, yet it carries capabilities from both.

## The Four Merging Methods That Matter

Four merging techniques have emerged as practical tools for multilingual systems, each with different strengths and different situations where they excel.

**Linear interpolation** is the simplest. You assign a weight to each source model and compute a weighted average of all parameters. If you have an English expert and a Swahili expert, you might use 0.5 for each, producing a model that is equally influenced by both. Or you might use 0.7 for Swahili and 0.3 for English if Swahili performance is your priority. The advantage is simplicity and predictability. The disadvantage is that parameter interference is uncontrolled -- parameters that moved in opposite directions during fine-tuning average toward zero, canceling out both adaptations instead of preserving either.

**SLERP** -- Spherical Linear Interpolation -- treats the model parameters as points on a high-dimensional sphere and interpolates along the surface of that sphere rather than through the interior. The practical effect is smoother transitions between the source models, with better preservation of parameter magnitudes. SLERP tends to produce merged models with more stable behavior than linear interpolation, particularly when the source models have diverged significantly from the base model. It works between exactly two models, which limits its use for multilingual merging where you might have five or ten source models.

**TIES-Merging** -- Trim, Elect Sign, and Merge -- addresses the parameter interference problem directly. It works in three steps. First, it trims small-magnitude parameter changes, removing deltas that are close to zero and unlikely to carry meaningful task information. Second, for parameters where the source models disagree on the direction of change, it resolves the conflict by electing the sign that the majority of source models agree on. Third, it merges only the parameters that survived trimming and sign election. The result is a merged model where parameter conflicts have been explicitly resolved rather than averaged into mush. TIES-Merging consistently outperforms linear interpolation for merges involving three or more source models.

**DARE** -- Drop And Rescale -- takes a different approach to reducing interference. Before merging, it randomly drops a fraction of the fine-tuning delta parameters from each source model, then rescales the remaining deltas to compensate. The intuition is that most fine-tuning parameters are redundant -- you can drop a significant fraction without losing the essential adaptation. By dropping different parameters from different source models, DARE reduces the chance that the same parameter receives conflicting updates from multiple sources. DARE is often combined with TIES-Merging in a method called DARE-TIES, which applies the dropout-and-rescale preprocessing before the TIES conflict resolution, producing cleaner merges than either method alone.

## Cross-Lingual Transfer Through Merging

The most striking finding from recent research is that model merging can transfer capabilities to languages that appeared in none of the source models' training data.

The paper "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs," presented at the MRL 2025 workshop at ACL, investigated this phenomenon directly. The researchers fine-tuned separate expert models on different tasks -- mathematical reasoning in English, and language capabilities in Bengali, Swahili, and Telugu -- and then merged them. The key finding was that the mathematical reasoning and multilingual capabilities occupy distinctly non-overlapping regions of parameter space. This means you can combine them without the destructive interference that plagued the Nairobi team's single-run approach.

The most consistently successful method was not traditional weight averaging but a technique called **layer swapping**. Instead of averaging all parameters, the researchers swapped entire transformer layers between the language expert and the task expert. The merged model used language-expert layers for the components that encode linguistic knowledge -- typically the lower layers of the transformer -- and task-expert layers for the components that encode reasoning patterns -- typically the upper layers. This produced models that outperformed both individual experts, improving performance by roughly 10 percent on average across the target languages on the MGSM multilingual mathematics benchmark. For languages where task-specific training data was scarce, the improvement was even larger.

The implication for practitioners is direct. If you have a strong English reasoning model and a strong Bengali language model, you can merge them to create a Bengali reasoning model -- even if you have zero Bengali reasoning training data. The merged model inherits reasoning patterns from one expert and linguistic competence from another.

## When Merging Works

Model merging is not magic. It succeeds under specific conditions, and understanding those conditions prevents you from wasting compute on merges that will fail.

Merging works best when source models are fine-tuned from the same base model. This ensures they start from the same point in parameter space, which means their fine-tuning deltas are compatible. Merging a Llama 4 model fine-tuned on Swahili with a Llama 4 model fine-tuned on Amharic works because both started from Llama 4. Merging a Llama 4 model with a Mistral model does not work because their parameter spaces are fundamentally different, even if the architecture is similar.

Merging works well when the capabilities being combined are complementary rather than conflicting. Language-specific adaptation and task-specific adaptation are highly complementary -- they modify different parts of the model's knowledge. Merging a Thai language expert with a medical reasoning expert produces a strong Thai medical model because the adaptations do not fight each other. Merging a formal-tone adapter with a casual-tone adapter does not work because they modify the same stylistic parameters in opposite directions.

Merging benefits from related language families. Languages that share script systems, grammatical structures, or vocabulary produce better merges because their fine-tuning deltas are more compatible. Merging a Spanish expert with a Portuguese expert works well because the languages are closely related. Merging a Finnish expert with a Mandarin expert typically produces worse results because the adaptations have little linguistic common ground to build on.

Merging works better when source models were fine-tuned with similar hyperparameters. If your Swahili expert was trained with a learning rate of 2e-4 for three epochs and your Amharic expert was trained with a learning rate of 1e-3 for ten epochs, the magnitude and distribution of their parameter deltas will be very different. This mismatch makes the merge less stable. Standardizing hyperparameters across your language-specific training runs -- same learning rate schedule, same number of epochs, same batch size -- produces cleaner merges.

## When Merging Fails

Recognizing failure modes early saves weeks of debugging.

Very distant language pairs produce poor merges. The fine-tuning deltas for Finnish and Mandarin do not share structural patterns because the languages are fundamentally different in grammar, morphology, and script. Merging them averages the deltas in ways that help neither language. The merged model performs worse than either individual expert on its respective language.

Merging fails when source models were fine-tuned on very different task types. A model fine-tuned for text classification and a model fine-tuned for open-ended generation have modified the base model in structurally different ways. Classification fine-tuning sharpens the model's decision boundaries. Generation fine-tuning expands the model's output distribution. Averaging these opposing modifications produces a model that is indecisive -- neither sharp nor expansive.

Merging degrades when too many models are combined at once. Merging two or three models produces reliable results. Merging ten models simultaneously accumulates parameter interference faster than conflict-resolution methods like TIES can manage. The practical limit depends on how similar the source models are, but most teams find that merging more than five models requires multiple rounds -- merge pairs first, then merge the merged models -- to control quality.

## The Merge-Then-Evaluate Workflow

Model merging is not a set-and-forget operation. It is an iterative process where you merge, evaluate, adjust, and repeat until you find the combination that maximizes performance across your target languages.

The workflow starts with training your individual language or task experts and evaluating each one independently to establish baselines. You know the Swahili expert scores 78 on your Swahili eval suite and the Amharic expert scores 71 on your Amharic eval suite. These baselines define what you are trying to preserve in the merge.

Next, you perform your initial merge using TIES-Merging or DARE-TIES with default parameters. You evaluate the merged model on every language's eval suite -- not just the languages represented by the source models. Sometimes merging improves performance on languages that were not in any training set, through the cross-lingual transfer effect. Sometimes it degrades performance on the base model's strong languages. You need visibility across the full language portfolio.

If the merged model underperforms expectations in a specific language, you adjust. The primary levers are interpolation weights and merging method. If the Amharic expert's contribution is being diluted, increase its weight in the merge. If TIES-Merging is over-trimming important parameters, reduce the trimming threshold. If two source models are interfering destructively, try layer swapping instead of parameter averaging. Each adjustment produces a new merged model that you evaluate against the same suite. The iteration cycle is fast -- merging takes minutes, not hours -- so you can explore dozens of configurations in a single day.

The workflow produces a merge configuration file that specifies exactly which models, which method, which weights, and which parameters were used. This file is versioned alongside your model artifacts, ensuring reproducibility. When you train an updated Swahili expert on new data next month, you re-run the same merge configuration and evaluate whether the updated expert improves the merged model.

## Practical Tools: mergekit

The open-source toolkit **mergekit**, maintained by Arcee AI, is the standard tool for model merging as of early 2026. It supports all four merging methods described above -- linear interpolation, SLERP, TIES-Merging, and DARE -- plus several additional methods including Task Arithmetic and Passthrough. It works with any model architecture supported by Hugging Face Transformers, which covers the vast majority of open-weight models.

mergekit's critical design feature is its out-of-core processing. It does not need to load all source models into memory simultaneously. It processes one layer at a time, loading the relevant parameters from each source model, computing the merge for that layer, and writing the result before moving to the next. This means you can merge models on machines with as little as 8 gigabytes of VRAM, or entirely on CPU if no GPU is available. For teams working on low-resource languages with limited compute budgets, this accessibility matters.

A typical merge configuration specifies the source models, the merge method, and the per-model weights. For a three-way merge of an English financial expert, a Swahili financial expert, and an Amharic financial expert using DARE-TIES, you define each model's path, assign weights based on your target language priorities, and set the DARE density parameter to control how aggressively redundant parameters are dropped. Running the merge produces a single model directory compatible with standard inference frameworks.

For advanced multilingual merging, mergekit supports per-layer weight specifications. You can apply different interpolation weights to different transformer layers, giving higher weight to the language expert in early layers (where linguistic features are primarily encoded) and higher weight to the task expert in later layers (where task-specific reasoning happens). This manual layer-swapping approach approximates the automated layer-swapping technique from the MRL 2025 research and can produce stronger merged models than uniform-weight merging.

## The Strategic Picture

Model merging fundamentally changes the economics of multilingual system development. Without merging, supporting N languages requires either one massive multilingual training run that produces mediocre quality across all languages, or N separate models that each require separate deployment infrastructure. With merging, you train N specialized experts and combine them into a single deployable model that carries the strengths of each.

The cost savings are substantial. Training a single-language expert takes a few hundred dollars in compute for a LoRA adapter or a few thousand dollars for full fine-tuning. Merging those experts into a single model costs essentially nothing -- minutes of CPU time. The alternative -- a multilingual training run with enough data in every language to avoid the blending trap -- costs tens of thousands of dollars and still produces worse per-language quality.

For the Nairobi team, the solution to their blending trap was exactly this approach. They trained separate LoRA adapters for each of their four languages, merged the adapters using TIES-Merging with mergekit, and evaluated the merged model across all four languages. English performance recovered to within two points of the base model. Swahili performance jumped twelve points above their original blended model. Amharic and Somali performance improved by eight and six points respectively. Total compute cost for the four adapters plus merging was under three hundred dollars. Their original blended fine-tuning run had cost over two thousand.

The next subchapter addresses a question that merging and adapter training both depend on: how to balance your training data across languages so that each language gets enough examples to learn without drowning out the others.
