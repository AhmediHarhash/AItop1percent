# 4.5 â€” LLM-as-Judge Across Languages: Bias, Calibration, and Failure Modes

The team had built everything right. Clean native-language eval sets in twelve languages. Carefully designed rubrics with per-language calibration notes. A production system serving millions of users across East Asia, Southeast Asia, and Latin America. The only missing piece was scale -- they could not afford human evaluators reviewing thousands of responses per language per week. So they did what every team in this position does in 2026: they deployed GPT-5 as an automated judge, scoring model responses on a one-to-ten scale across fluency, accuracy, and helpfulness.

The English scores looked plausible. The French and Spanish scores tracked closely with the human evaluators who had rated a calibration sample. Then the Korean team lead reviewed the Korean scores and raised an alarm. The automated judge had rated Korean responses an average of 12 points lower than English responses on a hundred-point normalized scale. A Korean linguist independently rated the same response pairs and found no meaningful quality gap. The responses were equally fluent, equally accurate, equally helpful. The judge was not detecting a quality problem in Korean. The judge had an English-preference bias -- a systematic tendency to rate English output higher than equivalent non-English output, baked into the judge model's training distribution.

This team lost three weeks recalibrating their pipeline. Other teams never catch the bias at all. They ship quality decisions, staffing plans, and language-priority roadmaps based on scores that reflect the judge's linguistic prejudice rather than actual output quality. LLM-as-judge for multilingual evaluation is one of the most powerful tools available in 2026 and one of the most dangerous when deployed without understanding its failure modes.

## Why Teams Want LLM-as-Judge for Multilingual

The appeal is straightforward math. If you serve users in fifteen languages and want to evaluate a thousand responses per language per week, you need fifteen thousand human evaluations per week. At an average of five minutes per evaluation, that is 1,250 hours of human labor -- roughly thirty full-time evaluators working forty-hour weeks. At native-speaker evaluation rates, you are looking at $20,000 to $60,000 per month depending on the languages involved.

An LLM judge can evaluate the same fifteen thousand responses in under an hour at a cost of a few hundred dollars. The cost reduction is two orders of magnitude. The speed improvement is even larger -- you can run evaluations continuously, not in weekly batches. You can evaluate every response in production, not a statistical sample. You can run evaluations at model-update speed, testing a new model version across all fifteen languages before the engineering team finishes lunch.

This cost and speed advantage explains why LLM-as-judge adoption has exploded. Research from 2025 found that the majority of teams running multilingual AI products use some form of automated LLM evaluation. The approach works well enough for English that many teams extend it to other languages with minimal modification -- the same judge prompt, the same scoring rubric, the same model. This is where the problems begin.

## The English-Preference Bias

Frontier models are trained on corpora where English dominates. GPT-5, Claude Opus 4.6, Gemini 3 Pro -- every major model in 2026 has seen vastly more English training data than any other language. This imbalance does not just affect the models' ability to generate text. It affects their ability to evaluate text.

When a frontier model acts as a judge, it brings its English-centric training distribution into the evaluation. The model has a deeper, more nuanced understanding of what "good English" looks like than what "good Korean" looks like. It has seen millions of examples of well-written English prose and can assess subtle quality differences -- the right word choice, the natural sentence rhythm, the appropriate register. For Korean, Thai, or Swahili, the model's internal representation of quality is sparser, less precise, and more likely to default to surface-level signals like whether the text resembles translated English.

Research presented at EMNLP 2025 measured this effect systematically across 25 languages and five evaluation tasks. The study found that multilingual LLM judges averaged a Fleiss' Kappa of approximately 0.3 for cross-language consistency -- meaning that the same judge, given equivalent-quality responses in different languages, produced dramatically different scores. Neither increasing model size nor multilingual fine-tuning reliably improved this consistency. The bias is not a bug that scales away. It is a structural consequence of training data distribution.

The practical impact looks like this: your automated judge rates English responses at an average of 7.8 out of 10, French responses at 7.5, Spanish at 7.4, Korean at 6.6, Thai at 6.3, and Swahili at 5.9. The team interprets these scores as quality differences between languages. They allocate more engineering resources to Thai and Swahili. They deprioritize Korean improvements because the score is "acceptable." But the scores do not reflect quality differences. They reflect the judge's familiarity with each language. The real quality gap between Korean and English might be two points, not twelve -- and the actual quality gap between Thai and English might be in the opposite direction from what the scores suggest.

## Calibration Varies by Language

Even if you are aware of the English-preference bias and attempt to correct for it, you face a second problem: calibration accuracy varies per language. A judge model might be well-calibrated for French -- meaning its scores correlate strongly with human judgments, even if the absolute values are slightly lower than English. The same judge might be poorly calibrated for Thai -- meaning its scores not only differ from human judgments in absolute terms but also fail to rank responses correctly.

Well-calibrated means the judge agrees with humans about which response is better, even if it assigns different absolute scores. Poorly calibrated means the judge disagrees with humans about relative quality -- it rates response A higher than response B when human evaluators consistently rate response B higher. The first problem you can fix with a linear correction. The second problem means the judge's scores in that language are unreliable for any decision-making purpose.

Calibration accuracy correlates with language resource availability, but not perfectly. Some mid-resource languages show surprisingly good calibration because the training data for those languages happened to include high-quality evaluation examples. Other languages with more total training data show poor calibration because the data was low-quality, repetitive, or domain-narrow. You cannot predict calibration accuracy from language category alone. You have to measure it.

Measuring calibration requires exactly what you were trying to avoid: human evaluation. You need a calibration set per language -- at minimum fifty response pairs rated by qualified native speakers -- against which you can benchmark the judge's scores. If the judge's rankings correlate with human rankings at a Spearman rho of 0.85 or higher, the judge is usable for that language with appropriate score normalization. If correlation drops below 0.7, the judge's scores in that language should not inform quality decisions. Between 0.7 and 0.85, the judge is useful for coarse screening but not for fine-grained quality assessment.

The uncomfortable truth is that building a reliable multilingual LLM judge requires per-language human calibration data -- which means you need native-speaker evaluators for every language anyway. The LLM judge does not replace human evaluation. It extends it. Human evaluation provides the calibration ground truth. The LLM judge provides the scale. Without the human ground truth, you have scale without accuracy.

## The Fluency-Accuracy Confusion

The third major failure mode is what practitioners call **the fluency-accuracy confusion**. When judging non-English output, LLM judges systematically overweight fluency relative to accuracy. A response that is beautifully written in Thai but contains a factual error will often score higher than a response that is awkwardly phrased in Thai but factually correct.

This happens because the judge model's ability to detect fluency problems is language-general -- unnatural phrasing, grammatical errors, and stilted vocabulary are relatively easy to spot across languages. But the judge's ability to detect factual errors requires deep knowledge in the target language, and the model's knowledge is thinner in non-English languages. The judge defaults to what it can assess confidently: surface fluency. The dimension it cannot assess confidently -- factual accuracy in a non-English context -- gets underweighted.

A healthcare company discovered this when their automated judge gave high scores to model responses in Arabic that contained medically incorrect dosage information. The responses were fluent, well-structured, grammatically perfect Arabic. A native-speaking pharmacist flagged the errors immediately. The judge had given a 9 out of 10 to a response that could have harmed a patient.

The fluency-accuracy confusion is especially dangerous in domains where accuracy matters more than fluency -- healthcare, legal, financial services, technical support. In these domains, a correct but awkward response is vastly better than a fluent but wrong one. The judge's scoring inverts this priority for non-English languages, and the inversion gets worse as the language moves further from English in the judge model's training distribution.

## Mitigation: Per-Language Calibration Curves

The most reliable mitigation is building per-language calibration curves. The process works like this. For each language you serve, collect a calibration dataset of at least fifty model responses rated by three or more qualified native-speaker evaluators. Calculate the average human score for each response. Run the same responses through your LLM judge. Plot the judge's scores against the human scores and fit a correction function -- linear correction works for well-calibrated languages, while poorly calibrated languages may need a nonlinear mapping or may need to be excluded from automated evaluation entirely.

Once you have the calibration curve, apply it to all automated scores in that language. If the judge systematically rates Korean responses 1.5 points lower than humans on average, the calibration curve corrects for this offset. If the judge compresses its scoring range for Thai -- using only the 5 to 7 range when humans use the full 3 to 9 range -- the calibration curve expands the range to match human distributions.

Refresh the calibration data quarterly. Judge model updates, production model updates, and shifts in the types of queries your system handles can all change the calibration relationship. A calibration curve built in January may be stale by April if you have updated either the judge model or the production model. Treat calibration like any other measurement instrument -- it requires regular recalibration.

The cost of calibration is modest compared to the cost of fully replacing automated evaluation with human evaluation. Fifty responses per language per quarter, rated by three evaluators, costs roughly $500 to $2,000 per language per quarter depending on evaluator rates. For a product serving twelve languages, that is $6,000 to $24,000 per year -- a fraction of the cost of running human evaluation at production scale, and the minimum investment required to trust your automated scores.

## Mitigation: Language-Matched Judges

A second mitigation is using language-matched judges -- models that were primarily trained on the target language rather than English. The M-Prometheus project, published in early 2025, demonstrated this approach by fine-tuning multilingual judge models using Qwen as the backbone. The key finding was that Qwen-based judges significantly outperformed English-centric judges when evaluating Chinese, Japanese, and Korean output.

The principle extends beyond Qwen. For Chinese evaluation, a judge model from the Qwen or Yi family -- trained on massive Chinese corpora -- will assess Chinese output quality more accurately than a GPT-5 or Claude model that treats Chinese as a secondary language. For Japanese, a model with strong Japanese training data produces more reliable judgments. For Arabic, models with extensive Arabic pretraining data show better calibration.

This approach trades one problem for another. Language-matched judges are better calibrated within their strong languages but worse calibrated for cross-lingual comparison. If you use Qwen to judge Chinese and GPT-5 to judge English, you cannot directly compare the scores between languages because the judges have different internal scoring standards. You need an additional normalization layer that maps both judges' scores to a common scale -- which brings you back to the per-language calibration problem, just with better raw calibration per language as a starting point.

The practical pattern for teams with the engineering capacity: use a frontier model as the primary judge for all languages, use language-matched judges as secondary evaluators for languages where the primary judge shows poor calibration, and use the agreement or disagreement between the two judges as a confidence signal. When both judges agree, trust the score. When they disagree, flag the response for human review. This ensemble approach costs more in inference but produces more reliable scores than either judge alone.

## Mitigation: Rubric Engineering for Multilingual Judges

The third mitigation operates at the prompt level. Most teams use the same judge prompt across all languages -- a generic rubric that says something like "rate the quality of this response on a scale of 1 to 10 considering fluency, accuracy, and helpfulness." This generic rubric amplifies all three problems: it lets the judge default to English-centric quality standards, it provides no language-specific calibration anchors, and it weights fluency and accuracy implicitly rather than explicitly.

Language-specific judge rubrics partially address this. Instead of a single generic rubric, you write a rubric for each language that specifies what quality means in that language. For Japanese, the rubric explicitly lists keigo (honorific language) appropriateness as a scoring criterion and provides examples of correct and incorrect usage. For Arabic, the rubric specifies that both Modern Standard Arabic and dialectal Arabic may be appropriate depending on context, and provides guidance on when each register is expected. For Korean, the rubric addresses the speech-level system and specifies which level is appropriate for the product's context.

The rubric should also include calibration anchors -- example responses at specific score levels for each language. A response that deserves a 3 in Korean should look different from a response that deserves a 3 in English, because the quality dimensions differ. By providing the judge with language-specific examples of what each score level looks like, you reduce the judge's tendency to default to English quality standards.

Research from 2025 on checklist-based evaluation frameworks showed that judges using structured, criteria-specific rubrics produced more reliable multilingual scores than judges using open-ended quality prompts. Breaking "quality" into explicit checkboxes -- grammar correctness, factual accuracy, cultural appropriateness, register correctness, completeness -- forces the judge to evaluate each dimension separately rather than collapsing them into a single gestalt judgment that favors English-like output.

## When LLM-as-Judge Works for Multilingual

Despite the biases, there are evaluation scenarios where LLM judges are genuinely useful across languages.

**Ranking.** Relative quality is more reliable than absolute quality. Even a poorly calibrated judge tends to correctly identify which of two Korean responses is better, even if it assigns both of them scores that are too low compared to equivalent English responses. Pairwise comparison -- "which response is better?" rather than "how good is this response on a scale of 1 to 10?" -- reduces the impact of per-language calibration errors. If you need to compare two model versions or two prompt strategies within a single language, pairwise LLM judging is significantly more reliable than absolute scoring.

**Detecting obvious failures.** A response in the wrong language, a response that is gibberish, a response that completely ignores the question -- these failure modes are detectable by LLM judges across all languages with high reliability. The judge does not need deep language-specific calibration to notice that a Spanish question received an English response or that a Thai response is incoherent. Using LLM judges as a first-pass filter to catch catastrophic failures is a high-value, low-risk application.

**High-volume screening.** When you need to evaluate ten thousand responses per day across twelve languages, LLM judges provide the only economically viable screening layer. The scores may not be perfectly calibrated per language, but they are calibrated enough to separate obviously good responses from obviously bad ones and to flag the uncertain middle range for human review. This triage function -- not final judgment, but intelligent routing of human attention -- is where LLM judges deliver the most value in multilingual settings.

## When LLM-as-Judge Fails for Multilingual

The failure modes cluster around subtlety. The more nuanced the quality distinction, the less reliable the cross-lingual judge.

**Cultural appropriateness.** Whether a response respects cultural norms, avoids culturally sensitive topics appropriately, and uses culturally appropriate examples requires cultural knowledge that English-centric judges lack. A judge might miss that a response to a Japanese user uses a level of directness that would be considered rude, or that a response to an Egyptian user uses religious references that are inappropriate for the context.

**Formality register.** Many languages have complex formality systems that English does not. Korean has seven speech levels. Japanese has elaborate honorific structures. Thai distinguishes formality through pronouns, particles, and vocabulary. German differentiates between formal and informal second-person address. LLM judges that were primarily trained on English have a shallow understanding of these systems and frequently rate register-inappropriate responses as acceptable.

**Nuanced correctness.** When the distinction between a correct and incorrect answer requires understanding local regulations, local business practices, or local medical standards, the judge's thinner knowledge in non-English languages leads to false positives. The judge rates a plausible-sounding but factually wrong answer as correct because it cannot detect the error.

**Idiomatic naturalness.** Whether a response sounds like it was written by a native speaker or translated from English is a quality dimension that native speakers detect instantly and that LLM judges detect poorly outside of high-resource languages. Translationese -- text that is grammatically correct but does not sound natural -- passes LLM judges in most non-English languages.

## The Hybrid Pattern

The architecture that most mature multilingual teams converge on is the hybrid evaluation pattern. The LLM judge evaluates one hundred percent of production output. Human evaluators review a calibrated sample per language per evaluation cycle.

The LLM judge serves three functions in this pattern. First, it provides continuous monitoring -- detecting regressions, tracking score trends, and flagging sudden quality drops across all languages in near-real-time. Second, it performs triage -- identifying the responses most likely to be problematic and routing them to human review, which concentrates expensive human attention where it matters most. Third, it generates comparative data -- enabling quick comparison of model versions, prompt strategies, and configuration changes across all languages simultaneously.

Human evaluators serve three complementary functions. First, they provide calibration ground truth -- the human scores that keep the LLM judge honest and enable per-language calibration curves. Second, they catch what the judge misses -- the cultural errors, register failures, and subtle inaccuracies that automated evaluation cannot detect. Third, they evolve the rubric -- native-speaker evaluators notice new quality patterns and failure modes that the existing rubric does not cover, feeding continuous improvement back into both the human and automated evaluation processes.

The sample size for human review depends on your confidence requirements and the calibration quality per language. Well-calibrated languages where judge and human scores correlate above 0.85 need smaller human samples -- perhaps fifty responses per week. Poorly calibrated languages where the correlation is below 0.75 need larger samples -- perhaps two hundred responses per week -- until either the calibration improves or you decide to rely primarily on human evaluation for that language.

## Building the Judge Evaluation Pipeline

Deploying a multilingual LLM judge is not a one-time setup. It is a pipeline that requires ongoing maintenance.

Start by establishing your calibration baseline. Before deploying the judge on any new language, collect calibration data from native-speaker evaluators. Do not deploy the judge on a language where you have no human calibration data. You will have no way to know whether the scores mean anything.

Monitor calibration drift. Track the correlation between judge scores and human scores over time for every language. When correlation drops below your threshold, either update the calibration curve or expand the human evaluation sample until you understand what changed. Common causes of calibration drift include judge model updates, production model updates, changes in user query patterns, and seasonal variation in the types of content your system processes.

Track per-language score distributions. The judge's scoring distribution should be roughly similar across languages after calibration correction. If one language shows a dramatically different distribution -- all scores compressed into a narrow range, or a bimodal distribution when other languages are unimodal -- investigate. Unusual distributions signal calibration problems or genuine quality problems, and you need to distinguish between the two before acting.

Log judge reasoning. Most LLM judges can provide natural-language explanations for their scores. Log these explanations for a sample of evaluations per language. Review them quarterly with native-speaker evaluators. The explanations reveal whether the judge is assessing quality for the right reasons or whether it is defaulting to surface-level signals. A judge that explains a low Korean score by citing "unnatural phrasing" might be detecting real fluency problems or might be flagging natural Korean constructions that differ from English syntax. The explanations help you diagnose which.

## The Economics of Getting This Right

Teams often frame multilingual LLM-as-judge as a cost question: "automated evaluation is cheaper than human evaluation, so we use it." This framing misses the downstream costs of bad evaluation data.

If your automated judge systematically underrates Korean quality by 15 points, your team will overinvest in Korean quality improvements that are not needed. If the judge overrates Thai quality by 10 points, your team will underinvest in Thai improvements while users experience quality problems that the dashboard does not show. If the judge cannot distinguish fluent-but-wrong from awkward-but-correct in Arabic, your quality decisions in Arabic will optimize for the wrong dimension.

The cost of miscalibrated evaluation compounds over time. Every sprint planned based on bad scores allocates engineering effort suboptimally. Every model update validated by a biased judge ships quality changes that might improve metrics without improving user experience. Every language priority decision made from uncalibrated data sends the product roadmap in a direction that does not match reality.

Investing in per-language calibration -- a few hundred evaluations per language per quarter, amounting to tens of thousands of dollars per year -- is not a cost center. It is insurance against hundreds of thousands of dollars in misallocated engineering effort. The LLM judge is a powerful instrument, but like any instrument, it needs regular calibration to produce measurements you can trust.

The next subchapter addresses the other side of the evaluation equation -- human evaluation for multilingual systems, where to find native-speaker evaluators, how to train them, and what it actually costs to build human evaluation capacity across a dozen languages.