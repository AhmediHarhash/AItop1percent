# 10.5 â€” Embedding and Retrieval Behavior Under Mixed-Language Queries

Where does a Hinglish query land in embedding space? Is it near the English cluster, the Hindi cluster, or lost in between? The answer determines whether your retrieval system returns relevant documents or irrelevant noise, and for code-switching users, the answer is almost always "lost in between."

Multilingual embedding models are one of the genuine achievements of modern NLP. Models like BGE-M3, Qwen3 Embedding, and Cohere Embed v3 create shared semantic spaces where "how do I open a bank account" in English and its Hindi equivalent cluster near each other. Cross-lingual retrieval works. But cross-lingual and code-switched are not the same problem. Cross-lingual retrieval asks: given a query in language A, find documents in language B. Code-switching asks: given a query that is simultaneously in languages A and B, find documents in any language. The second problem is harder, less studied, and less supported by existing models.

## The Training Data Gap

The root cause of embedding failure on code-switched input is simple: the training data is monolingual. Embedding models learn from pairs of texts that mean the same thing. The training set contains English-English pairs, Hindi-Hindi pairs, and English-Hindi cross-lingual pairs. It does not contain Hinglish-English pairs or Hinglish-Hindi pairs, because code-switched text is underrepresented in the curated corpora that embedding models train on.

This matters because the model learns to represent text in the patterns it has seen. It learns that Hindi sentences cluster together and English sentences cluster together, with cross-lingual bridges connecting equivalent meanings. It does not learn where to place text that is simultaneously Hindi and English, because it has never been asked to.

The Massive Multilingual Text Embedding Benchmark (MMTEB), introduced in early 2025, covers more than 500 evaluation tasks across 250 languages. It is the most comprehensive evaluation of multilingual embeddings to date. But even MMTEB evaluates monolingual and cross-lingual performance. It does not include code-switched evaluation tasks. The benchmark measures what the models were designed for, and code-switching was not part of the design.

## How Code-Switched Queries Behave in Embedding Space

When you embed a code-switched query, the resulting vector is pulled in two directions. The Hindi tokens activate features associated with the Hindi cluster. The English tokens activate features associated with the English cluster. The final embedding is a weighted average of these competing signals, landing in a region of embedding space that is between the language clusters but not aligned with either.

This between-cluster position has three consequences for retrieval.

**Weak matches to monolingual documents.** A Hinglish query about "mujhe home loan ki details chahiye" (I need home loan details) has moderate cosine similarity to English documents about home loans and moderate cosine similarity to Hindi documents about home loans, but high cosine similarity to neither. If your retrieval threshold requires a similarity score above 0.75, the Hinglish query may not cross the threshold for documents that would be top results for the equivalent monolingual query.

**Diluted relevance signal.** In a ranked retrieval system where you take the top K results, the Hinglish query's top results are a mix of partially relevant English documents and partially relevant Hindi documents. Neither set is as relevant as the top results for a clean monolingual query. The result set is noisier, the precision is lower, and the generation model downstream receives worse context.

**False matches across topics.** The between-cluster embedding sometimes lands near documents that share surface-level tokens with the query but are semantically unrelated. An English document that happens to contain Hindi loanwords, or a Hindi document that uses English technical vocabulary, may score higher than a document that is actually relevant but written in a single language. The embedding model has learned that shared tokens signal relevance, but in the code-switched case, shared tokens signal language overlap rather than semantic overlap.

## Quantifying the Retrieval Drop

Teams that have measured code-switched retrieval performance against monolingual baselines consistently find recall drops of 15 to 30 percent. The exact number depends on several factors.

**The language pair matters.** Hindi-English and Spanish-English code-switching have the most representation in multilingual training data. Embedding models have seen some code-switched text for these pairs, even if it was not explicitly labeled as such, through web crawls that include social media and forum posts. For these pairs, the recall drop is toward the lower end of the range -- around 15 to 20 percent. For less-represented pairs like Korean-English, Arabic-French, or Tamil-English, the drop can exceed 25 percent because the model has essentially zero code-switched training examples for these combinations.

**The mixing ratio matters.** Queries that are 80 percent in one language with a few borrowed words from another show minimal retrieval degradation. The embedding is dominated by the majority language and lands close to that language's cluster. Queries that are roughly 50-50 between two languages show the maximum degradation because the embedding is pulled equally in both directions and lands in the least populated region of embedding space.

**The script difference matters.** Language pairs that share a script -- Spanish-English, Turkish-German -- produce code-switched text where the tokenizer processes all characters consistently. Language pairs with different scripts -- Hindi (Devanagari) mixed with English (Latin), Arabic mixed with French (Latin), Thai mixed with English (Latin) -- force the tokenizer to handle two character systems in the same input. The tokenization is less predictable, the resulting token sequence is more unusual, and the embedding quality is lower.

**The embedding model matters.** More recent models with broader multilingual training data show smaller degradation than older models. BGE-M3, which explicitly supports multi-granularity retrieval across dense, sparse, and ColBERT representations, handles code-switching better than models that rely purely on dense retrieval. But no current embedding model was specifically designed for code-switched input, and all show measurable degradation.

## The Romanization Complication

Code-switching is complicated enough when each language uses its native script. But in many markets, speakers write non-Latin-script languages using Latin characters -- a practice called **romanization**. Hindi speakers on social media frequently write Hindi in Latin script rather than Devanagari. Arabic speakers write colloquial Arabic in Latin characters (sometimes called Arabizi). Thai speakers romanize Thai in casual contexts.

Romanized code-switched text is the worst case for embedding models. The model's tokenizer was trained on Hindi in Devanagari script. When it encounters Hindi words written in Latin characters -- "mujhe" instead of the Devanagari equivalent -- it tokenizes them as if they were English, producing subword tokens that bear no relationship to the Hindi vocabulary the model has learned. The Hindi meaning is lost entirely in the embedding. The query looks like broken English to the model.

A user searching for "mujhe ek accha restaurant chahiye near Connaught Place" has written a Hindi-English query entirely in Latin script. The embedding model treats "mujhe," "ek," "accha," and "chahiye" as unknown English words, breaking them into subword tokens that activate no meaningful features. The resulting embedding captures "restaurant" and "Connaught Place" and little else. The retrieval system finds restaurants near Connaught Place -- if the user is lucky -- but has completely lost the qualifier "accha" (good) and the personal framing "mujhe chahiye" (I need).

For markets where romanization is common, you may need a romanization-to-native-script preprocessing step before embedding. This is imperfect -- romanization is not standardized, and the same Hindi word may be romanized multiple ways -- but even approximate transliteration to Devanagari activates more useful features in the embedding model than leaving the text in Latin script.

## Mitigation: Query Expansion

The most effective mitigation for code-switched retrieval degradation is **query expansion** -- generating additional query variants that cover the monolingual equivalents of the code-switched query.

For a Hinglish query "mujhe home loan ki details chahiye," you generate three search variants. First, the original code-switched query as-is. Second, a pure Hindi translation: the full sentence in Hindi. Third, a pure English translation: "I need home loan details." You embed all three and search with each, then merge the result sets.

This approach triages the problem. The code-switched query may produce weak matches, but the monolingual variants produce strong matches against their respective language clusters. The merged result set captures documents that any single query alone might miss.

The cost is three times the embedding computation and three times the retrieval calls. For high-traffic systems, this cost is significant. You can reduce it by generating expanded queries only when code-switching is detected in the input, using the detection approaches from subchapter 10.3. Monolingual queries skip the expansion step and proceed normally. Code-switched queries pay the extra cost for better results.

The quality of the expansion depends on the quality of the translation. If you use the model itself to generate the monolingual variants, the translations may be imperfect. If you use a dedicated translation model, you add latency and another potential failure point. For high-resource language pairs, model-generated translations are usually good enough. For lower-resource pairs, a dedicated translation step may be worth the investment.

## Mitigation: Multi-Vector Retrieval

An alternative to query expansion is multi-vector retrieval, where the query is represented not as a single embedding but as multiple vectors that capture different aspects of the input.

ColBERT-style late interaction models represent each token in the query as a separate vector and compute relevance by finding the best token-level matches between query and document. For code-switched queries, this means Hindi tokens can match against Hindi tokens in documents and English tokens can match against English tokens, without the signal being diluted into a single averaged embedding.

BGE-M3 supports this approach natively, combining dense retrieval (single-vector), sparse retrieval (token-level weighted matching), and ColBERT-style multi-vector retrieval. For code-switched queries, the sparse and ColBERT components often outperform the dense component because they preserve per-token language signal rather than averaging it away.

The tradeoff is infrastructure complexity. Multi-vector retrieval requires storing multiple vectors per document, which increases index size and retrieval latency. Not every vector database supports ColBERT-style retrieval natively. The infrastructure investment is justified if code-switching is a significant portion of your traffic, but it may be overkill if code-switching affects a small percentage of queries.

## Mitigation: Code-Switched Training Data

The most principled solution is to include code-switched examples in the embedding model's training data. If the model has seen Hinglish paired with Hindi and English equivalents during contrastive training, it learns where to place Hinglish embeddings in the shared space.

This approach requires code-switched training data, which is the fundamental bottleneck. You can generate synthetic code-switched text by taking monolingual sentence pairs and algorithmically mixing them -- replacing some words in the Hindi sentence with their English equivalents -- but synthetic code-switching often does not match the patterns of real code-switching. The algorithmic mixing ignores the syntactic rules that govern real code-switching, producing unnatural combinations that may not help the model generalize to real input.

A better approach is to collect real code-switched text from your own production traffic, paired with monolingual equivalents. If you have bilingual annotators, they can provide the monolingual translations. You then include these triplets -- code-switched text, language-A equivalent, language-B equivalent -- in a contrastive fine-tuning step. Even a few thousand examples per language pair can meaningfully improve embedding quality for code-switched queries.

## Measuring Embedding Quality for Code-Switched Input

You cannot improve what you do not measure. Building a code-switching retrieval evaluation requires three components.

First, collect a set of real code-switched queries from production. One hundred to two hundred queries per language pair is sufficient for a meaningful signal. If you do not have production traffic yet, recruit bilingual speakers to write queries on representative topics.

Second, for each code-switched query, create monolingual equivalents in both languages. Have bilingual annotators write the "same question" in pure language A and pure language B. These are your baselines.

Third, run all three variants -- code-switched, monolingual A, monolingual B -- through your retrieval pipeline and compare the results. Measure recall at K, mean reciprocal rank, and the overlap between the code-switched result set and the monolingual result sets. The gap between code-switched recall and the better of the two monolingual recalls is the size of your code-switching retrieval penalty.

Track this penalty over time. As you add code-switched training data, implement query expansion, or upgrade your embedding model, the penalty should shrink. If it does not, your mitigations are not working and you need to investigate why.

## The Asymmetry Problem

One uncomfortable finding from embedding research is that the retrieval penalty is not symmetric. A Hinglish query searching against Hindi documents performs differently than the same Hinglish query searching against English documents. The direction of the asymmetry depends on the embedding model's training data distribution.

Models trained with more English data tend to embed code-switched text closer to the English cluster than to the other language's cluster. This means a Hinglish query retrieves English documents with moderate success but retrieves Hindi documents poorly. For a system with a Hindi document index, this asymmetry is devastating -- the users who code-switch the most are the users who get the worst retrieval from the Hindi index.

The mitigation is to measure the asymmetry explicitly and adjust your retrieval strategy accordingly. If Hinglish queries retrieve English documents better than Hindi documents, you may want to include English equivalents in your Hindi index, or run retrieval against both language indexes and merge results.

## Building for the Real World

The core lesson of this subchapter is that multilingual retrieval and code-switched retrieval are different problems. Multilingual retrieval is well-studied, well-benchmarked, and well-served by current embedding models. Code-switched retrieval is underserved, undermeasured, and structurally disadvantaged by the monolingual bias of training data.

If you serve markets where code-switching is common, you cannot trust your retrieval metrics. Those metrics were measured on monolingual queries. Your code-switching users experience a different retrieval quality -- measurably worse -- and your aggregate metrics hide the gap behind an average that describes nobody.

Measure the gap. Implement query expansion as a first-line mitigation. Explore multi-vector retrieval if the traffic justifies the infrastructure. And invest in code-switched training data, even small amounts, because the embedding model cannot learn to handle input it has never seen.

The next subchapter takes these retrieval findings into the full RAG pipeline, showing how code-switched input creates failure modes at every stage -- from retrieval through reranking through generation -- and what a code-switching-aware RAG architecture looks like.
