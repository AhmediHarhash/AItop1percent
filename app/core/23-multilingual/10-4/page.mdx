# 10.4 â€” Prompt Strategies for Code-Mixed Conversations

Do not try to force code-switching users into a single language. Meet them where they are.

This principle sounds obvious. In practice, it contradicts the default behavior of almost every deployed multilingual system. The standard pipeline detects the input language, selects a language-specific prompt template, and generates a response in that one language. For code-switching users, this means the system picks one of their languages -- usually the wrong one -- and responds entirely in it. The user wrote in Hinglish. The system responded in English. The user feels misunderstood, not because the answer was wrong, but because the system's language did not match their own. The content was correct. The communication was broken.

Prompt engineering for code-switched conversations requires a different design philosophy. Instead of detecting a language and enforcing it, you design prompts that observe the user's language pattern and match it. The technical challenge is smaller than most teams assume. The organizational challenge -- convincing stakeholders that mixed-language output is not a bug -- is larger.

## The Wrong Approach: Detect and Normalize

The most common strategy for handling code-switched input is to detect the dominant language and force the entire interaction into that language. The reasoning seems sound: pick a language, be consistent, avoid confusion. In practice, this approach creates three problems.

First, it alienates code-switching users. When a Hinglish speaker receives a pure English response, the tone shifts. The conversation feels formal when the user intended casual. It feels foreign when the user intended natural. The system's refusal to match the user's language pattern signals that the system was not built for them. Users in bilingual markets notice this immediately, even if they cannot articulate why the interaction feels off.

Second, it introduces errors. When the system normalizes the input into one language before processing, it often mistranslates the code-switched portions. A Hinglish sentence where the user deliberately chose English for a technical term gets that term translated back into Hindi, producing an unnatural phrasing that no Hindi speaker would use. The normalization step is supposed to simplify processing but instead corrupts the user's intent.

Third, it mishandles the cases where the user code-switched for a reason. As subchapter 10.1 explained, code-switching is often deliberate. The user chose English for "fixed deposit" because that is what the financial product is called in India. The user chose Hindi for the emotional framing because Hindi carries that register more naturally. Flattening everything to one language loses the information that the language choice itself carried.

## The Right Approach: Mirror the User's Pattern

The simplest and most effective prompt strategy for code-switched conversations is to instruct the model to match the user's language pattern. If the user mixes Hindi and English, respond in a similar mix. If the user writes entirely in Hindi, respond in Hindi. If the user switches from Taglish to English mid-conversation, follow the switch.

This works because modern language models can already produce code-switched text. They have seen it in training data -- social media posts, forum discussions, informal writing. The problem is not capability but instruction. The model's default behavior, shaped by RLHF training on predominantly monolingual data, is to produce clean single-language output. It needs explicit permission -- and explicit examples -- to code-switch in its responses.

A system prompt for a customer support bot serving the Indian market might include the following instruction in plain prose: "Respond in the same language mix that the user uses. If the user writes in Hindi with some English words, respond in Hindi with English words where natural. If the user writes in pure English, respond in pure English. If the user writes in pure Hindi, respond in pure Hindi. Match the user's language pattern. Do not default to English unless the user writes in English."

This instruction alone improves language alignment significantly. But instructions are brittle. The model may follow them in some turns and ignore them in others, especially when the response requires technical explanation where English dominates the training data. Few-shot examples are the more reliable mechanism.

## Few-Shot Examples That Teach the Pattern

The model learns language patterns from examples more reliably than from instructions. A few-shot set that includes code-switched examples teaches the model the specific register and mixing pattern you want.

For a Hinglish customer support bot, your few-shot examples should include exchanges where the user writes in Hinglish and the assistant responds in Hinglish. The assistant's Hinglish should sound natural -- not a mechanical alternation between languages but the fluid mix that a real bilingual speaker would produce. This means the few-shot examples should be written or reviewed by actual bilingual speakers, not generated by translating an English response and inserting Hindi words.

The number of code-switched examples matters. If your few-shot set contains five English examples and one Hinglish example, the model's prior toward English will dominate. The Hinglish example will be treated as an exception rather than a pattern. For markets where code-switching is the norm, at least half your few-shot examples should be code-switched. This signals to the model that code-switching is the expected register, not a deviation.

Vary the examples across the spectrum of code-switching intensity. Include one example with light code-switching -- mostly Hindi with a few English technical terms. Include one with heavy code-switching -- roughly equal Hindi and English in the same sentences. Include one where the user shifts from one language to another across turns. This variety teaches the model to match different intensities rather than producing a single fixed code-switching ratio.

## System Prompt Design Patterns

Three system prompt design patterns have proven effective for code-switched conversations in production.

**The mirror pattern** instructs the model to observe and match. "Match the language pattern of the user. If they mix languages, you mix languages. If they use one language, you use one language." This is the simplest and most generally applicable pattern. It works across language pairs without pair-specific tuning.

**The explicit pair pattern** specifies the exact language pair and the expected behavior for each. "This conversation may include Hindi-English code-switching (Hinglish). When the user writes in Hinglish, respond in Hinglish. Use Hindi for conversational framing and English for technical terms, product names, and numbers. Do not translate English technical terms into Hindi." This pattern is more specific and produces better results for a known language pair but requires separate templates for each pair you support.

**The matrix language pattern** uses the matrix language concept from linguistics. "Identify the grammatical frame language of the user's message. Respond using the same grammatical frame language, with embedded terms from the other language where natural." This pattern is the most linguistically informed and produces the most natural code-switching, but it requires that the model understand the matrix language concept, which not all models handle well.

In practice, the explicit pair pattern produces the best results for teams that know their primary language pairs. The mirror pattern is the best default for teams that serve many markets without pair-specific optimization. The matrix language pattern is the best choice for teams with strong linguistic expertise who can validate the output quality.

## When Not to Mirror: The Clean-Language Exception

Not every application should mirror the user's code-switching. Some domains require responses in a single, standard language for clarity, legal compliance, or safety.

**Medical and clinical applications** should respond in one clean language. When a patient asks about medication dosage in Hinglish, the response should be in clear, unambiguous Hindi or clear, unambiguous English. Code-switched medical advice risks misunderstanding in exactly the situations where misunderstanding is most dangerous. The dosage instruction must be unmistakable. Using a code-switched register that the patient may interpret differently than intended is an unnecessary risk.

**Legal and financial disclosures** have similar constraints. Regulatory language, terms of service, and contractual text should be in one language. If your system generates legal disclosures in response to code-switched questions, generate them in the language required by the applicable regulation, not in the mixed register the user used.

**Safety-critical refusals** should be in the user's strongest language, which may or may not be the language they code-switched in. When your system refuses a harmful request, the refusal should be maximally clear. A code-switched refusal risks being ambiguous. A clear, single-language refusal in the matrix language of the user's input is more effective.

For all other applications -- customer support, general conversation, product assistance, search, recommendations -- mirroring the user's code-switching is the right default. It produces more natural conversations, higher user satisfaction, and lower abandonment rates than forcing users into a single language.

## The Response Language Decision Framework

When your system encounters code-switched input, it must make a language decision. This framework structures that decision.

First, determine the matrix language. What language provides the grammatical frame of the user's message? This is the language the user is primarily operating in. If your detection is uncertain, default to the language the user started the conversation in.

Second, determine the application context. Is this a casual conversation, a customer support interaction, a medical consultation, a legal transaction? The application context determines whether code-switching in the response is appropriate.

Third, apply the appropriate strategy. For casual and general contexts, mirror the user's code-switching pattern. For domain-specific contexts where clarity is critical, respond in the matrix language only. For safety-critical contexts, respond in whichever single language the user is most likely to understand unambiguously.

Fourth, be consistent within a session. Once you establish a language pattern in the first few turns, maintain it unless the user explicitly shifts. A system that responds in Hinglish for three turns and then suddenly switches to English breaks conversational continuity and confuses the user.

## Testing Code-Switched Prompt Strategies

You cannot ship a code-switched prompt strategy without testing it. The evaluation requires bilingual judges, not monolingual metrics.

Build evaluation sets that include code-switched input for each language pair you support. For each test case, include the expected language behavior: should the system mirror, should it respond in one language, should it follow the matrix language? Then generate responses and have bilingual evaluators assess three dimensions.

**Language appropriateness.** Did the system respond in the right language or language mix? A Hinglish query that receives a pure English response fails this criterion. A medical query in Hinglish that receives a clear Hindi response passes.

**Naturalness.** If the system code-switched, does the code-switching sound natural? Would a real bilingual speaker produce this response? Awkward code-switching -- Hindi words inserted at unnatural points, English grammar imposed on Hindi sentence structure -- fails this criterion even if the language mix is roughly correct.

**Content quality.** Did the language strategy affect the accuracy of the response? Sometimes mirroring the user's code-switching causes the model to produce less accurate content because its knowledge is stronger in one language than the other. If mirroring degrades content quality, you may need to adjust the strategy -- perhaps mirroring the register but responding with technical details in the language where the model is most reliable.

Measure these dimensions separately. Aggregate scores hide the specific failure modes. A system that mirrors well but produces awkward code-switching looks fine in aggregate but feels wrong to every user.

## The Organizational Challenge

The hardest part of deploying code-switched prompt strategies is not technical. It is organizational. Product managers, QA teams, and stakeholders who are not bilingual will see code-switched output and flag it as a bug. "The bot is mixing languages -- that looks broken." You need to build understanding across the organization that code-switching is an intentional feature, not a defect.

This requires education. Share data showing what percentage of your traffic is code-switched. Show the user satisfaction difference between code-switched responses and forced-monolingual responses. Demonstrate that code-switching is the norm in your target markets, not an aberration. Build a brief internal reference document that explains what code-switching is, why your system does it, and when it should and should not do it.

Without this organizational alignment, your code-switched prompt strategy will be rolled back the first time a monolingual stakeholder reviews the bot's output. The technical work is wasted if the organization does not understand why the output looks the way it does.

The next subchapter shifts from generation back to retrieval, examining how code-switched queries behave in embedding space and what that means for the quality of your RAG pipeline.
