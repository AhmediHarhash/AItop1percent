# 10.7 â€” Dialectal Variation: When Standard Language Models Meet Real Users

Models learn standard languages. Users speak dialects. The gap between the two is where quality dies.

Every major language model in 2026 -- GPT-5, Claude Opus 4.6, Gemini 3, Llama 4 -- was trained primarily on written, formal, standardized text. Wikipedia articles. News corpora. Books. Government documents. Technical papers. This text represents how languages are written in official contexts. It does not represent how the majority of speakers actually use those languages. A user in Cairo writes Egyptian Arabic, not Modern Standard Arabic. A user in Guangzhou thinks in Cantonese, not Mandarin. A user in Mumbai writes in a Hindi inflected by Marathi, not the textbook Hindi of Doordarshan broadcasts. The model has been trained on the version of the language that appears in formal writing. The user communicates in the version that appears in daily life. These are often not the same language.

## The Dialect Gap

**The Dialect Gap** is the performance cliff between a model's accuracy on standard language variants and its accuracy on the dialectal variants that real users actually produce. It is not a subtle degradation. It is a cliff.

The DialectalArabicMMLU benchmark, released in 2025, tested 19 open-weight Arabic and multilingual language models across five major Arabic dialects: Syrian, Egyptian, Emirati, Saudi, and Moroccan. The results confirmed what Arabic-speaking engineers had long observed anecdotally. Model performance dropped substantially across all dialects compared to Modern Standard Arabic. Egyptian Arabic, the most widely represented dialect in training data thanks to Egypt's large media industry, showed the smallest gap. Moroccan Arabic -- Darija -- showed the largest. One evaluation found Egyptian dialect achieving an overall quality score of roughly 3.8 out of 5, while Moroccan Darija scored 3.3. The models frequently defaulted to generic Modern Standard Arabic when encountering Moroccan input, occasionally misusing regional vocabulary in ways that a Moroccan speaker would find jarring or even offensive.

This is not an Arabic-specific problem. It affects every language with significant dialectal variation -- which is most languages on earth. The gap exists because of a fundamental mismatch between how languages appear in training data and how languages appear in real communication.

## Why the Gap Exists

The training data pipeline creates the dialect gap through three mechanisms.

**The formality filter.** The web crawls that feed language model training are dominated by formal text. News sites, Wikipedia, government portals, academic papers, and corporate websites all use standardized language. Social media and messaging platforms -- where dialectal writing thrives -- are underrepresented because they are harder to crawl, harder to clean, and full of the short, noisy text that data pipelines tend to filter out. The cleaning steps that remove "low quality" text disproportionately remove dialectal text, because dialect looks like noise to quality classifiers trained on standard language.

**The annotation bias.** Reinforcement learning from human feedback relies on annotators, and annotators are typically selected for their ability to evaluate standard language. An annotator judging Arabic text quality will rate Modern Standard Arabic responses higher than Egyptian Arabic responses, even if the Egyptian Arabic response is more natural for the user's context. The model learns to produce standard language because standard language gets higher reward signals.

**The tokenizer mismatch.** Tokenizers are trained on the same standard-language-heavy corpora as the models themselves. A tokenizer that efficiently encodes Modern Standard Arabic may break dialectal Arabic words into many more subword tokens than necessary, because the dialectal vocabulary was rare in the tokenizer's training data. More tokens per word means more computation per dialectal input, lower effective context window for dialectal text, and worse generation quality because the model is working with a less efficient representation.

## Where the Gap Manifests

The dialect gap does not affect all capabilities equally. Some tasks are more dialect-sensitive than others.

**Vocabulary comprehension.** Dialects introduce words that do not exist in the standard language. Egyptian Arabic uses "izzayak" for "how are you," while Modern Standard Arabic uses "kayf haluk." Moroccan Darija uses "bzzaf" for "a lot," a word with no MSA equivalent. When a user writes a word the model has rarely or never seen, the model either guesses from context, misinterprets the word as a similar-looking word in the standard language, or ignores it. Each failure mode produces a different kind of error, and none of them is what the user intended.

**Grammar differences.** Dialects often have different grammatical structures than their standard counterparts. Negation in Egyptian Arabic works differently than in MSA -- it uses a circumfix construction that wraps around the verb. Cantonese sentence-final particles convey mood and attitude in ways that Mandarin does not encode. Swiss German word order diverges from Standard German in specific constructions. When the model encounters dialectal grammar, it may parse the sentence incorrectly, treating dialectal grammatical markers as errors rather than as features of a different grammar.

**Pragmatic and cultural references.** Dialects carry cultural knowledge that the standard language does not. An Egyptian Arabic speaker referencing "el bawab" (the building doorman) is invoking a cultural institution specific to Egyptian urban life. A Mexican Spanish speaker using "aguas" (literally "waters") as a warning has a meaning that Castilian Spanish speakers would not immediately recognize. Models trained on standard language lack these cultural anchors. They either miss the reference entirely or interpret it literally.

**Sentiment and tone.** Dialectal expressions carry emotional register that standard language equivalents do not. The same semantic content expressed in dialect versus standard language conveys different levels of formality, intimacy, and intensity. A sentiment classifier trained on standard Arabic text systematically misjudges the sentiment of dialectal input because the linguistic markers of positive and negative sentiment differ between the standard and dialectal registers. Research has documented cases where sarcasm in one dialect is interpreted as neutral or positive by models trained on another.

## The User Experience Consequence

When a user writes in their dialect and the system responds in the standard language, the user feels one of three things. If the system understood the query correctly despite the dialect, the response feels stiff and formal -- like talking to a bureaucrat when you wanted to talk to a friend. If the system partially misunderstood, the response is off-topic and the user must rephrase, often in the standard language or in English, adapting to the system instead of the system adapting to them. If the system completely failed to understand, the user gets no useful response and abandons the interaction.

None of these outcomes registers as a "failure" in standard metrics. The system generated a response. It was grammatically correct. It might even have been topically relevant in a general sense. But the user left dissatisfied. They did not feel understood. And they will not come back voluntarily.

A food delivery platform operating across the Middle East measured this effect directly. They A/B tested their customer support chatbot with two configurations: one that responded in Modern Standard Arabic regardless of input, and one that attempted to detect and match the user's dialect (with imperfect accuracy -- roughly 70 percent correct dialect detection). The dialect-matching configuration had 23 percent higher resolution rates and 31 percent fewer escalations to human agents, despite its imperfect detection. Users forgave occasional dialect misidentification. They did not forgive being spoken to in a register that felt foreign.

## Why Benchmarks Lie

Standard multilingual benchmarks evaluate models on standard language. MMLU, HellaSwag, and their multilingual variants use text that has been carefully translated into formal, standard variants of each language. A model that scores 85 percent on an Arabic benchmark is scoring 85 percent on Modern Standard Arabic. That number tells you nothing about the model's performance on Egyptian Arabic, Gulf Arabic, Levantine Arabic, or Darija.

The 2025 wave of dialectal benchmarks -- DialectalArabicMMLU, Absher for Saudi dialects, the AbjadNLP evaluation tasks -- began to close this measurement gap. But these benchmarks are still concentrated in Arabic, because Arabic has the most active research community around dialectal NLP. Dialectal benchmarks for Chinese varieties, Hindi regional variants, Spanish national varieties, and other major language families are far less developed.

This means that for most language-market combinations, you have no external benchmark for dialectal performance. You know how your model performs on standard Hindi. You do not know how it performs on the Hindi variant spoken in Hyderabad, which is heavily influenced by Telugu and Urdu. You know how it performs on standard Spanish. You do not know how it handles the voseo constructions used by 100 million speakers in Argentina, Uruguay, and Central America.

The only way to close this gap is to build your own dialectal evaluation sets using speakers of the specific dialects you serve. No off-the-shelf benchmark will tell you what you need to know.

## The "Standard Is Neutral" Fallacy

Teams often resist investing in dialectal support because they believe the standard language is a neutral, universal choice that all speakers can understand. This is technically true for some language pairs and dangerously false for others.

Modern Standard Arabic is understood, with effort, by educated speakers of most Arabic dialects. But it is nobody's native language. Nobody grows up speaking MSA. It is the language of formal education, news broadcasts, and government documents. Using it in a chatbot is like using Shakespearean English in a customer support email -- the user can parse it, but it signals distance, formality, and a lack of connection.

For Chinese varieties, the "standard is neutral" fallacy is even more dangerous. Standard Mandarin and Cantonese are not mutually intelligible in spoken form. In written form, Cantonese has its own characters and grammar that differ from standard Mandarin. A system that responds in standard Mandarin to a Cantonese-speaking user in Hong Kong is not being neutral. It is being incomprehensible in specific ways and alienating in general.

For Hindi-Urdu, the standard language itself is contested. Hindi and Urdu are the same spoken language -- Hindustani -- but they use different scripts (Devanagari for Hindi, Nastaliq for Urdu), draw formal vocabulary from different sources (Sanskrit for Hindi, Arabic and Persian for Urdu), and carry different political and religious associations in South Asia. Defaulting to "standard Hindi" is not neutral for Urdu speakers, and vice versa.

The point is that choosing to respond in standard language is itself a choice with consequences. It is not the absence of a dialectal strategy. It is a strategy that prioritizes formal-register speakers over dialectal speakers, and in most markets, the dialectal speakers are the larger group.

## Detecting Dialect

Before you can respond appropriately to dialectal input, you need to identify which dialect the user is writing in. This is a harder problem than language detection.

Language detection tools distinguish between languages: is this Hindi or Tamil? Dialect detection must distinguish within a language: is this Egyptian Arabic or Gulf Arabic? The signals are subtler. Two Arabic dialects may share 60 to 70 percent of their vocabulary. The distinguishing features are specific vocabulary choices, grammatical constructions, and even character-level patterns.

Off-the-shelf dialect detection tools exist but vary in accuracy. For Arabic, several classifiers can distinguish between major dialect groups -- Egyptian, Gulf, Levantine, Maghrebi, MSA -- with accuracy in the 75 to 85 percent range on clean text. Accuracy drops on short text (where there are fewer dialect markers) and on text that mixes dialectal and standard features (which much real-world text does).

For most other language families, dialect detection tools are limited or nonexistent in production-ready form. You may need to build your own classifier using labeled data from your users, or use proxy signals like user location, app language settings, or self-reported preferences to infer dialect.

The detection does not need to be perfect. Even coarse dialect identification -- "this is probably Gulf Arabic rather than Egyptian Arabic" -- allows you to make better processing decisions than treating all Arabic input identically. Perfect detection is impossible because dialects exist on a continuum, and many speakers mix dialectal features from multiple regions. Useful detection is achievable.

## The Dialect Continuum Problem

Dialects do not have sharp borders. They exist on a continuum where neighboring regions share more features than distant ones. A speaker from Amman, Jordan, uses Arabic that shares features with both Syrian Arabic and Gulf Arabic. A speaker from Lucknow, India, uses Hindi that carries Urdu influences more heavily than a speaker from Delhi. A speaker from Medellin, Colombia, uses Spanish that shares features with other Colombian varieties but also has distinct local vocabulary.

This continuum means that any dialect classification system involves arbitrary boundaries. You must decide how many dialect categories to support, and every boundary you draw will misclassify some speakers. A system with five Arabic dialect categories will lump Syrian and Jordanian speakers together despite their differences. A system with ten categories will still miss the variation within each category.

The practical approach is to start with broad dialect groups based on mutual intelligibility and market significance, then refine as your data and your business needs grow. You do not need to support every microdialect on day one. You need to support enough dialectal variation that the majority of your users in each market feel understood.

## The Fine-Tuning Temptation and Its Limits

When teams discover the dialect gap, their first instinct is to fine-tune the model on dialectal data. This can help, but it carries risks that are often underestimated.

Fine-tuning on Egyptian Arabic data improves the model's Egyptian Arabic performance. But it may degrade performance on Gulf Arabic, Levantine Arabic, and MSA. This is the catastrophic forgetting problem applied to dialects within a language. The model's capacity to handle one dialect may come at the expense of another, especially if the fine-tuning dataset is imbalanced toward one dialect.

A more careful approach uses parameter-efficient fine-tuning with dialect-specific adapters. You train a LoRA adapter for Egyptian Arabic, another for Gulf Arabic, another for Levantine Arabic. At inference time, you detect the dialect and load the appropriate adapter. This preserves the base model's general capabilities while adding dialect-specific knowledge. The infrastructure cost is higher -- you are maintaining multiple adapters and a dialect detection step -- but the risk of cross-dialect degradation is much lower.

We will explore multilingual fine-tuning strategies in depth in Chapter 11. For now, the key insight is that fine-tuning is a tool for closing the dialect gap, not a solution by itself. Without dialectal evaluation data to measure the gap, you cannot know whether your fine-tuning is helping. Without cross-dialect evaluation, you cannot know whether it is hurting other variants. The evaluation infrastructure must come before the fine-tuning investment.

The next subchapter takes the concept of language variation from the general to the specific, examining the particular challenges of Arabic dialects, Chinese variants, Hindi-Urdu, Brazilian Portuguese, and Spanish varieties -- the language families where the gap between "we support this language" and "we actually serve these users" is widest.
