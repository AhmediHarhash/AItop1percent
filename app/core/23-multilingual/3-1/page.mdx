# 3.1 — How Tokenizers Work and Why They Are Built for English

A tokenizer converts raw text into a sequence of integers that the model can process. Every word, every character, every punctuation mark must become a number before the model sees it. The model never reads text. It reads token IDs — integers that index into a fixed vocabulary of subword units. The quality of that conversion determines how much your text costs to process, how much context window it consumes, and how well the model understands what you wrote. For English, the conversion is efficient. For most other languages, it is not. Understanding why requires understanding how tokenizers are built, what biases the training process bakes in, and why the dominant algorithm in 2026 — Byte Pair Encoding — produces fundamentally unequal results across languages.

## What a Tokenizer Actually Does

The tokenizer sits between your text and the model's neural network. Its job is to break your input text into pieces — called tokens — that map to entries in the model's vocabulary. Each vocabulary entry has an integer ID. The model receives a sequence of these IDs, processes them through its transformer layers, and produces output IDs that the tokenizer converts back into text.

The vocabulary is fixed at training time. A model trained with a 100,000-token vocabulary will always use exactly those 100,000 tokens, no more and no fewer. Every possible input text must be expressible as a combination of those tokens. If a word appears in the vocabulary as a single token, it maps cleanly. If a word does not appear, the tokenizer splits it into smaller pieces — subwords, characters, or even individual bytes — until every piece matches something in the vocabulary.

This splitting is where the inequality begins. The word "the" in a typical English-trained vocabulary is a single token. The word "understanding" might be two tokens: "understand" and "ing." A common Chinese character like the one meaning "person" might also be a single token if the vocabulary was trained on enough Chinese data. But a less common Chinese character — or a word in Thai, Khmer, or Amharic — might be split into three, four, or five byte-level tokens, each representing a fragment that carries no meaning on its own. The model must reconstruct the meaning from these fragments, and it does so with less accuracy than when processing tokens that correspond to whole words or meaningful subwords.

## How Byte Pair Encoding Works

Byte Pair Encoding, or BPE, is the tokenization algorithm behind GPT-5, Claude Opus 4.6, Llama 4, Mistral Large 3, and most major models in production today. The algorithm is conceptually simple. Its consequences for multilingual AI are profound.

BPE starts with a base vocabulary of individual bytes — 256 entries that can represent any text in any language, because any Unicode text can be encoded as a sequence of bytes. Then it learns merge rules from a training corpus. The algorithm scans the corpus, finds the most frequently occurring pair of adjacent tokens, and merges that pair into a new token. This process repeats thousands of times until the vocabulary reaches a target size.

Consider a training corpus that is 60% English. The most frequent adjacent byte pairs will be English character combinations. The pair "t" and "h" merges early because "th" appears millions of times in English text. Then "th" and "e" merges to create "the." Then "in" and "g" merges to create "ing." Each merge creates a new vocabulary entry that represents a longer, more meaningful unit of English text. After tens of thousands of merges, common English words and subwords occupy dedicated slots in the vocabulary. "The," "and," "ing," "tion," "ment," "com," "pre" — all become single tokens.

Now consider what happens to Amharic during this same process. Amharic uses the Ge'ez script, which appears in perhaps 0.01% of the training corpus. Amharic character pairs almost never win the frequency competition against English pairs. They are rarely merged. The result: Amharic text remains fragmented into byte-level tokens long after English text has been compressed into efficient, meaningful units. A single Amharic syllable that a native reader processes as one unit may require three or four tokens in the model's vocabulary.

The bias is not intentional. Nobody designed BPE to penalize Amharic. The algorithm simply optimizes for frequency, and frequency in the training corpus is dominated by English. The bias is structural — a direct consequence of training data composition.

## The Scale of the English Advantage

The disparity is measurable and consistent across every major tokenizer in production. Research from 2024 and 2025 quantified the gap using a metric called **token fertility** — the average number of tokens per word in a given language.

For English, token fertility across major tokenizers hovers between 1.0 and 1.3. Most English words are a single token. Longer or rarer words split into two. Very few English words require three or more tokens. This is what efficient tokenization looks like.

For Chinese, token fertility ranges from 1.5 to 2.5 depending on the tokenizer and the text. Each Chinese character is one to two tokens in tokenizers with dedicated CJK vocabulary like Qwen's, but two to three tokens in tokenizers trained primarily on English. Since Chinese does not use spaces between words, and words can be one to four characters, a Chinese word might consume two to ten tokens for the same semantic content that English expresses in one to three.

For Arabic, token fertility ranges from 2.0 to 3.5. Arabic's rich morphology — a single word can encode subject, verb, object, and tense through prefixes, suffixes, and internal vowel changes — means that morphologically complex words get split into fragments that strip away the grammatical information encoded in the word's structure. The model must reconstruct that information from disconnected byte sequences.

For Thai, the situation is worse. Thai has no spaces between words, uses a complex script with vowels that can appear above, below, before, or after consonants, and is underrepresented in most training corpora. Token fertility for Thai ranges from 2.5 to 4.0 on English-centric tokenizers. A Thai sentence that conveys the same information as a ten-token English sentence might consume thirty or more tokens.

For languages written in scripts with very little representation in training data — Ge'ez for Amharic and Tigrinya, Myanmar script for Burmese, Khmer script for Cambodian — token fertility can exceed 5.0. Every character is split into raw bytes. The model processes a stream of meaningless fragments and attempts to extract meaning from their positions and patterns, a task far more difficult than processing semantically meaningful tokens.

## Vocabulary Design and Its Language Priorities

The vocabulary size and composition of a tokenizer reveal its language priorities as clearly as a budget reveals an organization's priorities.

GPT-4's cl100k_base tokenizer has approximately 100,000 tokens. Analysis of its vocabulary shows heavy English dominance — common English words, English subwords, and English-adjacent patterns occupy the majority of vocabulary slots. When OpenAI released GPT-4o, the tokenizer expanded to o200k_base with approximately 200,000 tokens. The expanded vocabulary added significantly more multilingual coverage, including dedicated tokens for non-Latin scripts. The result was meaningful improvement in token efficiency for languages like Chinese, Arabic, Hindi, and Thai — but the improvement was incremental, not transformative. English text still tokenizes 1.5 to 3 times more efficiently than most non-English text even with the larger vocabulary.

Qwen's tokenizer takes a different approach. With approximately 152,000 tokens and deliberate investment in CJK character coverage, Qwen achieves token fertility for Chinese that approaches English efficiency — roughly 1.5 characters per token for common Chinese text. This is not a coincidence. Alibaba built Qwen to serve Chinese-speaking users, and they allocated vocabulary space accordingly. The trade-off is that Qwen's tokenizer offers less efficiency for languages that are not CJK — its Arabic and Hindi fertility is comparable to or slightly worse than GPT-4o's.

Llama 4's tokenizer uses approximately 128,000 tokens with a broader multilingual allocation than earlier Llama versions. Meta's investment in multilingual instruction tuning for twelve languages extended to the tokenizer, with improved efficiency for Arabic, Hindi, Thai, Vietnamese, and Indonesian compared to Llama 3. But twelve languages out of the world's seven thousand still leaves the vast majority underserved.

Gemma 3's tokenizer, developed by Google, takes perhaps the most aggressive multilingual approach among major open-weight models, with dedicated support for over 35 languages and pretraining data spanning over 140 languages. Google's access to multilingual data through Search and Translate gives it a structural advantage in building tokenizers that allocate vocabulary more equitably across languages.

The pattern is clear: the tokenizer's language priorities are set by whoever builds it, based on their market priorities and their data. No tokenizer is neutral. Every tokenizer is a set of decisions about which languages deserve efficient representation and which do not.

## The Three Consequences of Tokenizer Bias

Tokenizer bias creates three concrete problems that compound across your entire multilingual system. Each one affects cost, quality, and user experience in ways that most teams never trace back to the tokenizer.

**The first consequence is cost inflation.** API providers charge per token. If the same semantic content costs 1.5 tokens in English and 4.5 tokens in Thai, you pay three times more per interaction for Thai users than for English users. At scale, this changes the unit economics of your multilingual product. A customer service bot that costs $0.02 per interaction in English costs $0.06 per interaction in Thai — not because Thai queries are more complex, but because the tokenizer fragments Thai text into more pieces. Multiply that by hundreds of thousands of interactions per month and the cost difference is material enough to affect pricing, margins, and the business case for supporting the language at all.

**The second consequence is context window starvation.** A model with an 8,000-token context window holds roughly 6,000 words of English text. The same context window holds only 2,000 to 3,000 words of Thai text, or 3,000 to 4,000 words of Arabic text. Your system prompt, conversation history, retrieved documents, and user query must all fit within this budget. For English, you have room to include rich context. For Thai, the same context window forces you to truncate conversation history, reduce the number of retrieved documents, or shorten the system prompt. The model receives less information and produces worse output — not because the model is less capable in Thai, but because the tokenizer left less room for context.

**The third consequence is generation quality degradation.** When a model processes fragmented tokens, it must do more work to reconstruct the meaning that would be immediately available if the tokens were whole words or meaningful subwords. Research has shown that models produce more grammatical errors, more semantic incoherence, and more repetitive outputs in languages with high token fertility. The fragmentation forces the model to maintain more internal state across more token positions, increasing the chance that attention mechanisms miss connections and that generated sequences lose coherence. This degradation is subtle — it does not produce error messages or obvious failures. It manifests as slightly worse grammar, slightly less natural phrasing, and slightly more frequent hallucinations. But "slightly worse" across millions of interactions is the difference between a product users trust and a product users abandon.

## The Named Concept: The Token Tax

These three consequences — cost inflation, context starvation, and quality degradation — combine into what this book calls **The Token Tax**. The token tax is the cost premium that non-English languages pay due to tokenizer design. It is not a bug in any specific model. It is a structural feature of how tokenizers are built when training data is dominated by one language.

The token tax is invisible unless you measure it. Most teams never compare their per-language token costs, their per-language context utilization, or their per-language generation quality. They see that Thai output is worse than English output and attribute it to the model's language capability. Sometimes that attribution is correct. But often, a significant portion of the quality gap comes not from the model's ability to reason in Thai but from the tokenizer's inability to represent Thai efficiently. The model may be perfectly capable of producing fluent Thai if it received the same amount of context and the same quality of token representation that English receives. The tokenizer prevents it from getting that chance.

Measuring the token tax requires a simple experiment. Take a representative sample of inputs in each of your supported languages. Count the tokens each input consumes. Divide by the number of words or characters to get token fertility per language. Compare the fertility numbers across languages. The ratio between your highest-fertility language and English is your token tax multiplier. If English fertility is 1.2 and Thai fertility is 3.6, your Thai token tax is three times — every interaction costs three times as many tokens, uses three times as much context window, and gives the model three times less semantic information per token position.

Once you know the tax, you can start managing it — through tokenizer selection, prompt compression, and cost model adjustment. That is the work of the subchapters that follow.

The next subchapter quantifies the token tax across specific languages and model families, giving you the exact numbers you need to budget, plan context window allocation, and make informed decisions about which tokenizers to use for which languages.
