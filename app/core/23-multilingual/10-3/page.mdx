# 10.3 â€” Code-Switch Detection and Language Identification in Mixed Input

In early 2025, a conversational AI team serving the Indian market analyzed their language detection logs and found a pattern they had not expected. Their chatbot received roughly 40 percent of all messages in what the classifier labeled "English." When a linguist reviewed a sample of those messages, more than half were not English at all. They were Hinglish -- sentences like "Mujhe ek appointment book karna hai for next Tuesday" where Hindi provided the grammatical frame and English contributed a handful of content words. The classifier, trained to assign one label per input, saw the English tokens, counted them, and picked English. The system then routed the message to the English pipeline, which did not understand the Hindi tokens. Users received garbled responses to perfectly clear questions. The fix was not a better monolingual classifier. It was a fundamentally different approach to language identification -- one that operates at the word level instead of the sentence level.

## The Problem with Sentence-Level Classification

Standard language identification assigns a single label to an entire text. This design makes a hard assumption: the text is in one language. For monolingual input, this assumption holds and the tools work beautifully. GlotLID covers more than 2,000 language labels. OpenLID provides broad coverage with strong accuracy. FastText-based models run in microseconds. For clean, single-language text in well-resourced languages, sentence-level identification is a solved problem.

Code-switching violates the core assumption. When a sentence contains tokens from two or more languages, the classifier must either pick one or produce a low-confidence "unknown" label. Both outcomes are wrong, and both send the wrong signal to the downstream pipeline. Picking the dominant language -- the language with slightly more tokens -- ignores the minority-language tokens entirely. Labeling the input as "unknown" tells the pipeline nothing useful and typically triggers a fallback that is no better than guessing.

The severity of the problem depends on the balance between languages. When a sentence is 90 percent Hindi with a single English word, the classifier picks Hindi and the pipeline handles it adequately. The single English word may cause a minor retrieval miss, but the overall processing is reasonable. When a sentence is 55 percent English and 45 percent Hindi, the classifier is essentially flipping a coin between two wrong answers, and the pipeline downstream receives actively misleading information about the input.

## Moving to Word-Level Identification

The solution is to stop asking "what language is this text in?" and start asking "what language is each word or span in?" Word-level language identification treats the problem as a token classification task. Each word receives a language tag, and the resulting annotation tells you not just that the text contains two languages but exactly where each language appears and in what proportion.

This is not a new idea. The LinCE benchmark, published in 2020, established standardized evaluation for word-level code-switching identification across four language pairs: Spanish-English, Nepali-English, Hindi-English, and Modern Standard Arabic-Egyptian Arabic. It defined the task, provided datasets, and set baselines. What has changed since then is the quality of the models and the breadth of language coverage.

Current approaches to word-level language identification fall into three categories.

**Transformer-based token classifiers** treat the problem as sequence labeling, similar to named entity recognition. A multilingual encoder like XLM-RoBERTa is fine-tuned on code-switched text with per-token language labels. For well-studied pairs -- Hindi-English, Spanish-English -- these models achieve 85 to 92 percent word-level accuracy. For less-studied pairs, accuracy drops to 70 to 80 percent, still useful for approximate language proportion estimation but unreliable for precise word-level routing.

**Iterative masking approaches** like MaskLID, presented at ACL 2024, take a different strategy. Instead of training a new model, they use existing sentence-level classifiers iteratively. The first pass identifies the dominant language. The second pass masks the features associated with that language and re-runs the classifier, which now identifies the secondary language. This approach requires no additional training data and works with any FastText-based classifier, making it practical for teams that need code-switching detection without the cost of training specialized models.

**Subword-level identification** extends the approach below the word boundary to handle intra-word code-switching. When a user writes "chating" -- combining English "chat" with Hindi morphology -- the word itself is code-switched. Segmental models split the word at the language boundary and tag each segment separately. This is the most granular approach and the most computationally expensive. It matters most for language pairs with frequent intra-word mixing, such as Hindi-English, Turkish-German, and Arabic-French.

## The Matrix Language: What Your System Actually Needs to Know

For most downstream tasks, you do not need perfect word-level identification. What you need is the answer to a more specific question: which language is the matrix language?

In code-switching linguistics, the **matrix language** provides the grammatical frame -- the word order, function words, morphological inflections, and syntactic structure. The **embedded language** provides content words -- nouns, verbs, and technical terms -- that slot into the matrix language's frame. In the sentence "Mujhe ek appointment book karna hai for next Tuesday," Hindi is the matrix language. It provides the grammar: "Mujhe" (to me), "ek" (one), "karna hai" (have to do). English provides embedded content words: "appointment," "book," "next Tuesday."

The matrix language is the language the user is operating in. Identifying it correctly tells your system more than a word count ever could. A sentence that is 40 percent English by word count but grammatically Hindi is a Hindi sentence with English borrowings. The user expects a Hindi-patterned response. A sentence that is 40 percent Hindi by word count but grammatically English is an English sentence with Hindi vocabulary. The user may expect an English response.

Detecting the matrix language is more reliable than word-level identification because it depends on grammatical structure -- function words, word order, inflectional patterns -- rather than on classifying every individual word correctly. Function words are typically short, frequent, and language-specific. If the function words are Hindi, the matrix language is Hindi, regardless of how many English nouns appear in the sentence. This heuristic is imperfect but powerful. It gives your system the most important piece of information -- what language the user is thinking in -- with relatively simple detection.

## Detection Accuracy: What to Expect

Setting realistic expectations is essential. Word-level language identification is not a solved problem for most language pairs.

For high-resource pairs with extensive code-switching corpora -- Hindi-English, Spanish-English, Modern Standard Arabic-Egyptian Arabic -- the best models achieve 85 to 92 percent word-level F1 scores. This means roughly one in ten words is misidentified, which is good enough for aggregate statistics like language proportion estimation but not good enough for precise per-word routing decisions.

For medium-resource pairs -- Turkish-German, Arabic-French, Filipino-English, Korean-English -- accuracy drops to 75 to 85 percent. The training data is smaller, the linguistic patterns are less well-studied, and the models generalize less reliably.

For low-resource pairs -- virtually any code-switching pair that does not involve English, such as Hindi-Marathi, Swahili-Arabic, or Malay-Tamil -- accuracy can drop below 70 percent. For these pairs, there may be no publicly available code-switched training data at all. You are working with zero-shot transfer from related language pairs, which is better than nothing but far from reliable.

The DIVERS-Bench evaluation in 2025 confirmed this hierarchy. It tested across diverse conditions and found that language identification models achieve high performance on curated datasets but degrade sharply on noisy, informal, and code-switched text. The gap between clean-text and real-world performance was consistent across all tested models and all tested conditions.

## The Code-Switching Continuum

Not all code-switching is the same, and your detection strategy should vary accordingly. Think of it as a continuum from lightest to heaviest mixing.

At one end is **lexical borrowing** -- a single word from another language inserted into an otherwise monolingual sentence. "Let's meet at the boulangerie" is English with a single French borrowing. This requires minimal special handling. Your monolingual pipeline can usually process it. The borrowed word might cause a minor embedding miss but rarely breaks the system.

Next is **tag-switching** -- switching at the boundary of a clause or sentence, often for discourse markers, fillers, or tags. "So, woh keh raha tha ki..." (So, he was saying that...). The switch happens at a natural boundary and does not disrupt the grammatical structure of either language. Detection is straightforward because the language boundary aligns with syntactic boundaries.

In the middle is **inter-sentential switching** -- alternating languages between sentences. One sentence in Hindi, the next in English. Each sentence is internally monolingual. Detection can be handled by applying sentence-level classification to each sentence individually.

Further along is **intra-sentential switching** -- switching languages within a single sentence with both languages contributing to the grammatical structure. This is the pattern that breaks standard classifiers and requires word-level or span-level detection.

At the far end is **intra-word switching** -- morphemes from two languages combined within a single word. This is the hardest to detect and requires subword-level analysis.

Your detection strategy should match the types of code-switching you actually encounter. For a customer support chatbot, inter-sentential and light intra-sentential switching account for most code-switched traffic. Investing in subword-level detection may not be worth the cost. For a speech recognition system in a heavily code-switching market, intra-word switching may be common enough to warrant the investment.

## When Detection Matters and When It Does Not

Not every application needs sophisticated code-switching detection. The question is what your downstream pipeline does with the language information.

**Detection matters for routing.** If your pipeline routes to language-specific models, indexes, or prompt templates based on detected language, getting the detection wrong means the entire pipeline processes the input in the wrong context. In this architecture, code-switching detection is critical infrastructure.

**Detection matters for safety.** If your safety classifier is language-specific, code-switched input that is misdetected as one language will be evaluated by a classifier that only understands half the message. Harmful intent distributed across two languages slips through.

**Detection matters less for universal models.** If your pipeline uses a single multilingual model for all languages -- no language-specific routing, no language-specific retrieval -- then language detection is less critical. The model receives the code-switched input directly and processes it as best it can. The quality may still degrade, but the degradation comes from the model's limitations, not from a routing error.

**Detection matters less when you can handle it natively.** The most robust approach to code-switching is not to detect and route but to build a system that handles mixed-language input without needing to know the language at all. This is the direction that frontier language models are moving, though as the OLA benchmark showed in January 2026, they are not there yet.

## Practical Implementation

For teams adding code-switching detection to an existing pipeline, start with the simplest approach that addresses your most common failure mode.

If your primary problem is sentence-level misclassification, use MaskLID on top of your existing classifier. It requires no training data, runs on existing infrastructure, and provides a secondary language label that your routing logic can use for ambiguous cases.

If you need word-level identification for a specific high-resource language pair, fine-tune XLM-RoBERTa on the LinCE dataset for that pair. The datasets are publicly available and the fine-tuning takes hours, not days. Deploy the model as a preprocessing step that annotates each token with a language label before routing.

If you need to handle many language pairs, invest in a general multilingual token classifier. Train on all available code-switching corpora and rely on cross-lingual transfer for unsupported pairs. Accept that accuracy for unsupported pairs will be lower and supplement with the MaskLID iterative approach as a fallback.

Regardless of approach, measure your detection performance on real user traffic, not on benchmark data. Collect a sample of code-switched messages from production, have bilingual annotators label the languages at the word level, and compare your detection output against these human annotations. This ground truth measurement tells you where your detection actually stands, not where the benchmark says it should stand.

## The Detection-Generation Loop

Code-switching detection feeds directly into generation strategy. When your detection tells you the user is operating in Hinglish with Hindi as the matrix language, your generation strategy can instruct the model to respond in a similar pattern. When detection tells you the user switched from Hindi to English mid-conversation, your generation strategy can decide whether to follow the switch or maintain the previous language.

This loop -- detect the user's language pattern, then generate in a matching pattern -- is the foundation of natural-feeling multilingual conversation. Getting the detection right does not guarantee good generation, but getting the detection wrong guarantees bad generation.

The next subchapter explores the generation side of this loop: how to design prompts that produce natural, appropriate responses to code-switched input, and when to mirror the user's code-switching versus when to respond in a single clean language.
