# 6.11 â€” Multilingual RAG Evaluation: Metrics That Capture Cross-Lingual Quality

Standard RAG metrics -- retrieval recall, faithfulness, answer relevance -- were designed for monolingual systems. They assume the query, the documents, and the answer are all in the same language. The moment your system serves multiple languages, those metrics still matter, but they are no longer sufficient. You need per-language breakdowns of every metric. You need cross-lingual retrieval quality scores for language pairs where the query and the source document differ. You need a language-match accuracy rate that tells you how often the system responds in the correct language. And you need all of these tracked over time, per language, so that a silent degradation in Thai retrieval quality does not hide behind a global average that still looks healthy.

Multilingual RAG evaluation is not harder because the metrics are different. It is harder because the evaluation matrix is larger, the failure modes are more varied, and the cost of comprehensive measurement scales with every language you add.

## The Three Evaluation Dimensions

Every multilingual RAG response can fail in three independent ways, and each dimension requires its own metrics.

**Retrieval quality** answers the question: did the system find the right documents? The query was in Korean, the knowledge base contains documents in Korean and English, and the system retrieved five chunks. Were they relevant? Were they the best five chunks available? Did the system correctly identify that the most relevant content was in an English document even though the query was in Korean? Retrieval quality is the foundation -- if the system retrieves the wrong content, everything downstream is wrong regardless of how good the generation is.

**Generation quality** answers the question: is the answer correct, complete, and faithful to the sources? The system retrieved three relevant chunks and generated a response. Does the response accurately represent the information in those chunks? Did it hallucinate additional facts? Did it omit critical details? Generation quality in multilingual systems has an extra failure mode: the meaning can shift during cross-lingual generation in ways that are invisible unless you evaluate in both languages simultaneously.

**Language quality** answers the question: is the response in the correct language, natural-sounding, and culturally appropriate? A technically correct answer delivered in the wrong language is useless. A response in the correct language that reads like machine translation erodes trust. A response that uses inappropriate formality for the target culture damages the user relationship. Language quality is the dimension most teams measure last, and it is the one users notice first.

These three dimensions are independent. A response can score perfectly on retrieval (correct documents found), perfectly on generation (answer is faithful and complete), and still fail on language quality (response is in English when the user asked in Japanese). Or it can be in the correct language and sound natural but be based on irrelevant documents. You must measure all three separately to diagnose where your system is breaking.

## Per-Language Retrieval Metrics

The standard retrieval metrics apply, but they must be computed per language and per language pair.

**Recall at k** measures what fraction of relevant documents appear in the top k results. For monolingual queries (Japanese query, Japanese documents), recall at k tells you how well your embedding model and index perform for that language. For cross-lingual queries (Japanese query, English documents), recall at k tells you how well your system bridges the language gap during retrieval. These are different capabilities and must be tracked separately.

**Normalized Discounted Cumulative Gain** -- nDCG -- measures not just whether relevant documents were retrieved, but whether they were ranked correctly. A system that consistently places the most relevant document at position four instead of position one has an nDCG problem even if its recall is high. In multilingual systems, nDCG often degrades for non-English queries because the embedding model ranks English-adjacent results higher by default. A Japanese query might retrieve the right English document, but rank it below a less-relevant Japanese document because the language match boosted the Japanese document's similarity score.

**Mean Reciprocal Rank** -- MRR -- measures the position of the first relevant result. For user-facing systems where the top result matters most, MRR per language reveals which languages get the best first-result experience. A system with an MRR of 0.85 in English but 0.62 in Arabic means Arabic users see their best result at position two or three on average, while English users see it at position one. The experience gap is real even if the downstream generation compensates.

Compute each metric in a matrix: query language on one axis, document language on the other. An eight-language system produces a sixty-four-cell retrieval matrix. Most cells will be empty (you rarely need Thai-to-Finnish cross-lingual retrieval), but the cells that matter -- your actual query-document language pairs in production -- should have solid numbers. If Japanese queries against English documents show recall at 10 of 0.72 while English queries against English documents show 0.91, you know cross-lingual retrieval is your bottleneck for Japanese users.

## Cross-Lingual Retrieval Metrics

Cross-lingual retrieval deserves its own evaluation because it tests a fundamentally different capability than same-language retrieval. When a Korean user asks a question and the best answer exists only in an English document, the system must bridge a linguistic gap at the embedding level. This is not just "retrieval in another language" -- it is retrieval across languages, and it fails differently.

The XRAG benchmark, introduced in 2025, formalizes this evaluation. It tests LLM generation when the retrieval results do not match the user's language. The findings are sobering: cross-lingual retrieval quality drops 30 to 50 points on Hits at 20 compared to same-language retrieval in balanced domain-specific corpora. The gap is not uniform -- it is smaller for languages closer to English and larger for languages with different scripts and structures.

To evaluate cross-lingual retrieval in your own system, build a test set with queries in each supported language and known relevant documents in each supported language. For each query, the evaluation should check whether the system retrieves the relevant document regardless of the document's language. A Japanese query about employee benefits should retrieve the relevant English-language HR policy document if that is the best source, even though a lower-quality Japanese document also exists. This test reveals whether your system prioritizes language match over content relevance -- a common failure where the retrieval layer returns same-language results even when better content exists in another language.

Track the cross-lingual retrieval gap: the difference in recall between same-language pairs and cross-language pairs, for each query language. If the gap is large, consider query translation, cross-lingual reranking, or hybrid strategies covered in earlier subchapters. The metric itself tells you whether those interventions are working.

## Generation Faithfulness Per Language

Faithfulness -- whether the generated answer accurately reflects the retrieved sources -- varies by language in ways you cannot predict without measuring. The previous subchapter covered cross-lingual faithfulness in depth. Here, the evaluation question is: how do you track faithfulness systematically across all your languages?

The practical approach is an LLM-as-judge framework adapted for multilingual evaluation. Use a frontier multilingual model (GPT-5, Claude Opus 4.6, Gemini 3 Pro) to evaluate whether a generated response in language X faithfully represents source documents in language Y. The judge evaluates along three axes: does the response contain information not present in the sources (hallucination), does the response omit critical information from the sources (omission), and does the response distort the meaning of information in the sources (distortion)?

Run this evaluation on a stratified sample per language pair. A sample of 100 to 200 responses per language pair per evaluation cycle is enough to produce statistically meaningful faithfulness rates. Track the rate per language pair over time. The numbers will vary: English-to-French faithfulness might run at 96 percent while English-to-Thai runs at 88 percent. Both numbers matter independently.

The MEMERAG benchmark from Amazon Science, published in 2025, provides a reference framework for calibrating LLM judges on multilingual RAG faithfulness. Their finding that even the best LLM judges disagree with human annotators on 10 to 15 percent of cases means you should calibrate your judge against human ratings for each language pair you care about. A judge that is well-calibrated for English-French may be poorly calibrated for English-Korean, and the miscalibration will produce misleading faithfulness scores.

## Language-Match Rate

The simplest multilingual metric is also one of the most important: what percentage of responses are in the language the user expects?

Language-match rate measures whether the system's output language matches the user's input language (or the language explicitly requested by the user, if your system supports language selection). A response in English to a Japanese query is a complete failure from the user's perspective, regardless of how accurate the content is. A response that starts in Japanese and switches to English mid-sentence is worse -- it signals a system that is unstable, not just misconfigured.

Measure language-match rate automatically using language detection on the output. FastText, the lingua library, and cloud-based language detection APIs all work for sentence-level detection. Run detection on every response and compute the match rate per expected language. A global language-match rate of 97 percent sounds acceptable until you discover that it is 99.5 percent for English, 98 percent for French, and 82 percent for Thai. Thai users experience a fundamentally broken product.

Track language-match rate separately from other quality metrics because it is a prerequisite, not a component. A response in the wrong language cannot be faithful, cannot be relevant, and cannot be useful. Fix language-match problems before investing in faithfulness or relevance improvements for that language.

Also track partial language mismatches: responses where the majority of the text is in the correct language but individual sentences, technical terms, or quoted passages revert to English. These partial mismatches are harder to detect automatically but are common in systems where the model's English training dominates. A Japanese response that uses English for all numerical expressions or technical terms may technically pass a document-level language check but feel unnatural to the reader.

## End-to-End Metrics Per Language

The metrics above measure individual components. End-to-end metrics measure the complete user experience: the user asked a question, and the system produced a response. Was it correct, complete, useful, and well-expressed?

**Answer correctness** measures whether the final response contains the right information. For factual questions with verifiable answers, this is binary: correct or incorrect. For open-ended questions, it requires a judgment about whether the key information is present. Evaluate answer correctness per language using a test set with known-correct answers in each language.

**Answer completeness** measures whether the response covers all relevant aspects of the question. A correct but incomplete response that answers half the question is a partial failure. Completeness is harder to evaluate automatically and typically requires LLM-as-judge or human evaluation. Per-language completeness scores reveal whether the system generates more thorough answers in some languages than others -- a pattern that often reflects the imbalance in the model's training data.

**Fluency** measures whether the response reads naturally in the target language. A grammatically correct but stilted response that reads like translated text damages user trust. Fluency evaluation is inherently subjective and varies by language. What counts as "natural" in German is different from what counts as "natural" in Japanese. Automated fluency scoring using a multilingual model provides directional guidance, but periodic human evaluation by native speakers is necessary for calibration.

**Response usefulness** is the ultimate metric: did the user get what they needed? This is best measured through implicit signals (did the user ask a follow-up question? Did they rephrase the same question? Did they abandon the session?) and explicit signals (thumbs up/down, satisfaction surveys). Track these per language. A language where users consistently rephrase their questions has a quality problem, even if your automated metrics look acceptable.

## The Multiplicative Evaluation Challenge

Here is the math that stops most teams from doing multilingual evaluation properly.

Suppose you support eight languages. You track six retrieval metrics (recall at 5, recall at 10, recall at 20, nDCG at 10, MRR, cross-lingual retrieval gap). You track four generation metrics (faithfulness, hallucination rate, omission rate, distortion rate). You track three language metrics (language-match rate, partial mismatch rate, fluency). You track three end-to-end metrics (correctness, completeness, usefulness). That is sixteen metrics across eight languages. If you also track cross-lingual pairs, the matrix grows further.

Eight languages times sixteen metrics is 128 individual numbers per evaluation cycle. Add cross-lingual pairs and you approach 300. Track weekly and you produce over 15,000 data points per year. No team reviews 15,000 numbers. The data becomes noise unless you have a system for making it manageable.

**The priority matrix.** Not all language-metric combinations matter equally. Rank your languages by business importance -- revenue, user count, regulatory exposure. Rank your metrics by impact -- language-match and faithfulness matter more than fluency for most applications. Focus detailed evaluation on the top three languages and the top five metrics. For lower-priority languages, track a smaller set of aggregate metrics and investigate only when they cross alert thresholds.

**The budget allocation principle.** Allocate evaluation budget proportional to risk, not proportional to volume. A low-volume language with regulatory requirements (medical AI serving a specific market) may need more evaluation investment than a high-volume language where the consequences of errors are lower. The evaluation budget for each language-metric combination should reflect the cost of failure, not just the frequency of use.

**Automated triage.** Set thresholds for every metric and every language. When a metric crosses the threshold, it generates an alert. When it stays within bounds, it generates a green light. Your weekly review examines only the alerts and the metrics that changed significantly. This reduces the 128-number review to a manageable set of items that need attention.

## Dashboarding for Multilingual RAG Quality

The evaluation data is only useful if it reaches the right people in the right format.

Build a dashboard with three views. The executive view shows one number per language: an aggregate quality score that combines the most important metrics (language-match rate times faithfulness times answer correctness). This single number per language tells leadership whether each market is healthy. When a number drops, they know to ask questions.

The engineering view shows the full metric breakdown per language, with trend lines over the past four to eight weeks. Engineers use this view to diagnose which component is degrading and in which language. Color-code cells by status: green for within threshold, yellow for approaching threshold, red for below threshold. The red cells are the priority list.

The operations view shows real-time language-match rates and error rates per language, with alerts for sudden changes. Operations uses this view to detect acute problems: a model update that breaks Japanese language control, a corpus update that degrades Thai retrieval, a traffic spike that overwhelms the cross-lingual reranker for a specific language.

Alert thresholds should be language-specific. A 2 percentage point drop in English faithfulness may be noise. A 2 percentage point drop in Thai faithfulness, where the baseline is already lower, may push the metric below the acceptable threshold. Set thresholds relative to each language's baseline, not as global constants.

## Evaluation Frequency and Sample Size

How often you evaluate and how many responses you sample per language determines whether your metrics are trustworthy or noisy.

For language-match rate, evaluate every response. This metric is cheap to compute (run language detection on the output) and critical enough to warrant 100 percent coverage. There is no reason to sample.

For retrieval metrics, evaluate on a fixed test set weekly. The test set should contain at least 50 queries per language, with known-relevant documents. Run the queries through your retrieval pipeline, compute the metrics, compare to the previous week. A test set smaller than 50 queries per language produces metrics with confidence intervals too wide to detect meaningful changes.

For faithfulness and generation quality, evaluate on a sampled set. The sample should be stratified by language and by domain (if your system covers multiple knowledge domains). A sample of 100 to 200 responses per language per week, evaluated by an LLM judge, produces faithfulness estimates with a margin of error of plus or minus 3 to 5 percentage points. For languages where faithfulness is close to your threshold, increase the sample size to narrow the confidence interval.

For fluency and usefulness, evaluate monthly with native speaker reviewers. These metrics are too subjective for weekly automated tracking and too expensive for large samples. A monthly review of 30 to 50 responses per language by native speakers provides the calibration data you need to validate your automated metrics.

## Common Evaluation Pitfalls

Teams that attempt multilingual RAG evaluation for the first time make predictable mistakes.

**Translated test sets instead of native test sets.** Taking your English test queries and translating them into Japanese does not test how Japanese users actually search. It tests how English search patterns perform when expressed in Japanese words. Real evaluation requires queries written by native speakers, exhibiting native query patterns (the short keyword queries, the compound nouns, the formality variations covered in subchapter 6.9). Translated test sets produce misleadingly high retrieval scores because the translated queries are structurally English.

**Global averages that hide per-language problems.** A faithfulness rate of 94 percent across all languages sounds solid. If English is 97 percent and Thai is 79 percent, 94 percent is not a meaningful number. It actively conceals a problem. Every metric must be reported per language, and global averages should be supplemented with the minimum per-language score and the variance across languages.

**Evaluating retrieval and generation together.** When end-to-end quality drops for a specific language, you need to know whether the problem is in retrieval, generation, or language control. If you only measure end-to-end metrics, you cannot diagnose. Measure each stage independently so that when a problem emerges, you know where to investigate.

**Using English judges for non-English content.** An LLM judge prompt written in English, asking the model to evaluate Japanese faithfulness, may perform differently than a judge prompt written in Japanese. The judge's language affects its evaluation accuracy. For each language, test whether the judge performs better when prompted in English or in the target language, and use whichever produces higher agreement with human annotators.

## Building Your Evaluation Roadmap

You do not need all of this on day one. Build multilingual RAG evaluation incrementally.

**Week one: language-match rate.** Implement automatic language detection on every response. Track per-language match rates. Fix any language that falls below 95 percent match rate before worrying about other metrics.

**Month one: per-language retrieval metrics.** Build a test set with native queries for each language. Compute recall at 10, nDCG, and MRR per language. Identify the languages with the worst retrieval quality and investigate the causes.

**Month two: faithfulness sampling.** Implement an LLM-as-judge pipeline for faithfulness evaluation. Sample 100 responses per language per week. Track faithfulness trends. Calibrate the judge against human ratings for your top three languages.

**Month three: end-to-end metrics and dashboarding.** Build the dashboard with executive, engineering, and operations views. Set alert thresholds. Establish the weekly review cadence. Begin tracking answer correctness and completeness per language.

**Ongoing: expand and refine.** Add cross-lingual retrieval metrics for language pairs that matter. Add fluency evaluation with native speakers. Increase sample sizes for languages approaching thresholds. Adjust thresholds as your system improves.

This incremental approach means you always have some visibility into multilingual quality, and you add depth as your system and team mature. The worst outcome is measuring nothing because the full evaluation matrix felt too large to start.

The next subchapter addresses the infrastructure side of multilingual RAG at scale: when query volume grows, when language count expands, and when latency and cost budgets tighten, what architectural patterns keep the system performing across every language you support?