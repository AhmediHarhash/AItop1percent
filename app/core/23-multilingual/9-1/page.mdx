# 9.1 â€” The English Safety Blind Spot: Harms That Filters Miss

Your safety classifiers were trained on English hate speech, English toxicity data, and English harmful content patterns. They are blind to the ways harm is expressed in other languages. Not partially blind. Not slightly weaker. Structurally blind -- because the mechanisms of harm in most languages have no English equivalent, and systems built to detect English-shaped harm will never recognize them no matter how many parameters you add to the model.

Subchapter 7.7 covered the technical mechanism: why safety refusal rates degrade across languages and what pipelines you can build to narrow the gap. This subchapter goes deeper into the cultural layer. The problem is not only that your classifier is weaker in Thai or Yoruba. The problem is that harm itself looks different in Thai and Yoruba -- and your classifier was never designed to see those shapes.

## The Training Data Tells One Story

Safety alignment in modern language models starts with human-annotated data. During RLHF and DPO training, annotators label outputs as harmful or helpful. During red-teaming, attackers try to elicit dangerous content and the model learns to refuse. During content moderation classifier training, labeled examples of toxic and non-toxic text teach the system what to flag.

Nearly all of this work happens in English. A 2025 systematic review of nearly 300 publications across major NLP conferences found a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. The red-team prompts are English. The toxicity datasets are English. The refusal examples are English. The annotators are overwhelmingly English-speaking. The result is a safety system that understands English harm with deep, granular nuance and understands non-English harm the way a tourist understands local customs -- superficially, incompletely, and with dangerous blind spots.

This is not a secondary concern. It is the foundational weakness of every multilingual safety system deployed in 2026. You can build translate-then-classify pipelines, layer output filters, and add language-specific classifiers -- and you should do all of those things. But beneath every technical mitigation sits the same root problem: the model's concept of harm was shaped by one culture's expression of it.

## What Harm Looks Like When English Cannot See It

English-language toxicity follows recognizable patterns. Slurs, threats, explicit instructions for violence, sexual exploitation, hate speech with identifiable targets. English safety classifiers are excellent at detecting these patterns because they were trained on millions of labeled examples of exactly this kind of content. The problem begins when harm takes forms that have no English analogue.

In Japanese, **keigo** -- the elaborate politeness system with honorific, humble, and polite registers -- can be weaponized in ways that no English speaker would recognize as aggressive. A sentence that translates to English as "I humbly request that you consider the matter at your convenience" can, through deliberate misuse of politeness levels, convey vicious contempt to a Japanese reader. The speaker uses an excessively humble register when addressing someone they consider inferior, or deploys honorific forms with cutting irony that signals deep disrespect. A classifier trained on English toxicity will see polite language. A Japanese reader will see an insult designed to humiliate.

In Arabic, harm can operate through religious reference in ways that carry enormous weight. Invoking specific Quranic verses or hadiths out of context to justify violence, declaring someone a kafir (unbeliever) as a form of social ostracism, or using religious authority to legitimize discriminatory content -- these carry a gravity that no English-language toxicity label captures. A classifier trained to detect explicit threats will miss the implicit threat embedded in a religious pronouncement that, in certain communities, can lead to real-world violence against the target.

In Hindi and Tamil, caste-based slurs operate in a system that has no English parallel. Terms that reference caste position or ritual purity function as tools of oppression against hundreds of millions of people. These terms are deeply embedded in everyday language, appear in contexts that look innocuous to an outsider, and carry a history of violence and discrimination that stretches back millennia. An English toxicity classifier has no concept of caste hierarchy and no training data that connects these terms to harm. It will pass them through without a second look.

In many African languages, ethnic slurs target specific communities in ways that reflect local histories of conflict. A term in Kinyarwanda that references the Rwandan genocide, a phrase in Amharic that targets a specific ethnic group, a coded term in Yoruba that invokes regional tensions -- each carries a charge that requires understanding the specific historical and political context. No amount of cross-lingual transfer from English safety training can teach a model to recognize these as dangerous. The model simply does not have the cultural schema.

## The Politeness Trap

One of the most insidious forms of harm that English classifiers miss is what you might call **The Politeness Trap** -- content that is simultaneously polite in surface form and harmful in cultural meaning.

English toxicity tends to be blunt. Slurs are recognizable as slurs. Threats are recognizable as threats. The relationship between form and intent is relatively direct, which makes classification comparatively straightforward. Many other languages have far more complex relationships between surface politeness and underlying intent.

Korean has a speech-level system where using the wrong register can constitute a deliberate insult. Speaking to a senior person in the informal register, or to a peer in the excessively formal register used for strangers, signals disrespect that Korean speakers immediately recognize. Chinese can use elaborate indirectness to deliver devastating criticism that reads as courtesy in translation. Thai has particles and pronouns that signal social hierarchy, and deliberate misuse of these markers is a form of aggression invisible to outsiders.

The safety classifier that evaluates the English translation of these exchanges sees nothing wrong. Polite words, neutral tone, no slurs, no threats. Pass. The cultural harm sails through undetected because the classifier was never trained to detect harm that wears the mask of courtesy.

## The Bypass Vector

The English Safety Blind Spot is not just a quality problem. It is a security vulnerability. Users who want to extract harmful content from your model can exploit the safety gap by switching languages.

This attack requires no technical sophistication. A user whose harmful request in English is refused can rephrase it in a language where the safety classifier is weaker and receive a compliant response. Research on multilingual jailbreak challenges has demonstrated this consistently. A 2024 study found that ChatGPT produced unsafe output on over 80 percent of multilingual harmful prompts that it correctly refused in English. More recent work on code-mixed attacks -- requests that blend two languages in a single sentence, such as romanized Hindi mixed with English -- achieved attack success rates above 95 percent by breaking the tokenization patterns that safety classifiers rely on.

The attack surface is enormous. Your model supports twenty languages. Each language has a different safety posture. The attacker only needs to find the weakest one. And unlike prompt injection attacks, which require understanding model internals, the language-switching attack is available to any bilingual user with a keyboard.

The implication for your safety dashboard is uncomfortable. Your English safety metrics may show 97 percent of harmful content blocked. That number measures your strongest posture. In your weakest supported language, the number might be 45 percent. The aggregate safety metric -- if you even have one that spans languages -- hides the catastrophe behind an average. A system that blocks 97 percent of English harm and 45 percent of Thai harm is not a system with 71 percent average safety. It is a system that is 45 percent safe, because attackers choose the path of least resistance.

## The Cultural Reference Problem

Beyond linguistic form, harm operates through cultural references that require deep contextual knowledge to evaluate. A reference to the Nanjing Massacre deployed to humiliate Chinese users. A reference to the Partition of 1947 used to incite hatred between Indian and Pakistani users. A reference to colonial atrocities used to dehumanize people from formerly colonized nations. A reference to the Troubles used to provoke sectarian hatred in Northern Ireland.

These references are not explicit hate speech. They are often factual statements -- dates, place names, historical events. What makes them harmful is the intent behind their use and the context in which they appear. An English safety classifier that has been trained to detect explicit slurs and threats has no mechanism to evaluate whether a historically accurate statement is being deployed as a weapon. It cannot assess whether mentioning a specific event in a specific conversational context is educational or inflammatory, because making that assessment requires cultural knowledge that was never part of the training data.

This is the hardest category of harm for any automated system to detect. It requires not just language understanding but cultural understanding -- knowledge of what hurts whom, in what context, and why. No classifier trained on English data alone can acquire this knowledge, regardless of its size or architecture.

## False Safety Confidence

The most dangerous consequence of The English Safety Blind Spot is not the harm that gets through. It is the confidence that harm is being caught.

When your safety dashboard shows high block rates, your team believes the system is safe. They present safety metrics to leadership, to regulators, to customers. They make deployment decisions based on those numbers. They do not realize that the numbers describe safety performance in one language and extrapolate it to all languages.

This is **False Safety Confidence** -- the belief that your safety system is working across your entire product when it is actually working only where you measured it. It is the same mistake as testing your application only on Chrome and concluding it works on all browsers. The test results are real. The conclusion is wrong.

False Safety Confidence has regulatory consequences. Under the EU AI Act, which began GPAI enforcement in August 2025, high-risk AI systems must demonstrate safety across all populations they serve. A system that is safe for English speakers but unsafe for Arabic speakers is not compliant, regardless of what the aggregate safety metric shows. The regulation does not accept "our English safety numbers are excellent" as a defense when harm occurs in another language.

It has reputational consequences too. When a safety failure disproportionately affects non-English speakers, the story writes itself: the company built a safe product for English speakers and an unsafe product for everyone else. Whether that narrative is fair or not, it is the narrative that will emerge, and it is extraordinarily difficult to walk back.

## What To Do: The Multilingual Safety Audit

Narrowing the English Safety Blind Spot starts with measuring it. You cannot fix what you cannot see, and right now most teams cannot see their multilingual safety posture because they have never measured it.

A **Multilingual Safety Audit** is a structured assessment of your safety system's performance across every language you support. Subchapter 7.7 covered the technical mechanisms -- translate-then-classify pipelines, language-specific classifiers, per-language safety testing. This section focuses on the cultural dimension that those mechanisms must be designed to catch.

First, build a per-language harm taxonomy. For each supported language, work with native speakers who have cultural expertise -- not just translation ability -- to identify the categories of harm that are specific to that language and culture. What are the slurs that target specific ethnic groups? What are the historical references that can be weaponized? What are the politeness patterns that can convey aggression? What are the religious terms that cross the line from discussion to incitement? This taxonomy will be different for every language. That is the point.

Second, create a culturally informed test suite. For each category in the per-language harm taxonomy, create test cases that represent realistic harmful content in that language. These test cases must be written by native speakers, not translated from English, because translated harmful content preserves English harm patterns and misses language-specific ones. A translated test suite tests whether your classifier catches English-style harm in another language. A natively written test suite tests whether your classifier catches harm as it actually appears in that language.

Third, measure and compare. Run every language's test suite against your safety system and compare the results. The gap between your English detection rate and each other language's detection rate is the size of your blind spot for that language. Track this gap over time. Set a maximum acceptable gap and treat any language that exceeds it as a safety incident requiring remediation.

Fourth, build feedback loops. Deploy native-speaker reviewers who can evaluate flagged and unflagged content in each supported language. These reviewers serve as both quality assurance for your classifiers and as a source of new training data for culturally specific harm patterns. The reviewers must have cultural expertise, not just language fluency. A fluent Hindi speaker who grew up in London may not recognize caste-based slurs that a reviewer from rural Maharashtra would catch instantly.

## The Maintenance Problem

Cultural harm is not static. New slurs emerge. Political events create new sensitivities. Historical events are reinterpreted. The caste-based slurs relevant in 2024 may evolve into coded variants by 2026 precisely because the original terms start getting caught by classifiers. The cultural references that carry weight shift as political contexts change.

This means your multilingual safety system requires continuous maintenance, not a one-time build. The per-language harm taxonomies must be updated at least quarterly. The test suites must incorporate new attack patterns as they emerge. The native-speaker reviewers must be retained as ongoing team members, not one-time consultants.

The maintenance cost scales with the number of supported languages. Every language you add is not just a new set of model capabilities to test -- it is a new set of cultural harm patterns to understand, a new taxonomy to build, a new set of native reviewers to hire, and a new set of test cases to maintain. This is the real cost of multilingual safety that most launch plans underestimate.

## The Asymmetry You Must Accept

There is an uncomfortable asymmetry at the heart of multilingual safety that no amount of engineering fully resolves. Your English safety system benefits from decades of research, millions of labeled examples, thousands of annotators, and billions of dollars of investment in English-language content moderation. Your safety system for Amharic, Yoruba, Khmer, or Tagalog benefits from almost none of that. The gap is not a bug in your implementation. It is a structural inequality in the global distribution of safety resources.

Accepting this asymmetry does not mean accepting unsafe systems for non-English users. It means being honest about where you stand so you can prioritize effectively. It means measuring the gap so you can close it incrementally. It means investing in the languages where your safety posture is weakest rather than continuing to refine your already-strong English safety. And it means being transparent with users about the limitations of your safety system rather than projecting a uniform level of safety that you cannot actually deliver.

The English Safety Blind Spot is not a problem you solve once. It is a condition you manage continuously, with increasing sophistication, across every language your product touches. The teams that acknowledge it build progressively better safety systems. The teams that deny it learn about it from their users, their regulators, or the press -- and by then the damage is already done.

What counts as harmful is not universal. The same content that is innocuous in one culture can be deeply offensive or even dangerous in another. The next subchapter maps the categories of harm that vary by culture, from religious sensitivity to political taboos to historical trauma, and shows you how to build safety systems that respect these differences without imposing any single culture's norms on the world.