# 6.3 â€” Cross-Lingual Retrieval: Finding English Documents with Japanese Queries

In late 2025, a global enterprise software company with 14,000 employees launched a unified knowledge management system. The knowledge base contained 120,000 documents: technical specifications, product manuals, internal policies, and customer-facing guides. Roughly 80 percent of the content was in English, written by engineering and product teams based in the United States and Europe. The remaining 20 percent was split across Japanese, German, French, and Mandarin, mostly localized user guides and regional compliance documents. Sales teams in Tokyo, Osaka, and Nagoya relied on this knowledge base to answer customer questions during demos and support calls. They queried in Japanese. The system used a multilingual embedding model and returned results. The results were poor. Japanese-to-English cross-lingual recall sat at 62 percent -- meaning that for every 10 relevant English documents, the retrieval system found only 6. The sales teams stopped trusting the system within weeks and reverted to asking their US colleagues to search for them manually. The company spent three months rebuilding the retrieval pipeline with query expansion, cross-lingual reranking, and a hybrid search strategy. After the rebuild, recall climbed to 81 percent and the Tokyo office started using the system again. The 19-point recall improvement was the difference between a tool people abandoned and a tool people depended on.

This is the operational reality of cross-lingual retrieval. The embedding model gets you partway there. The retrieval architecture gets you the rest.

## The Baseline Problem

Cross-lingual retrieval -- using a query in Language A to find relevant documents in Language B -- is consistently worse than monolingual retrieval. This is not a failure of any specific model. It is an inherent property of how cross-lingual embedding models work.

In monolingual retrieval, the query and the document share everything: vocabulary, grammar, script, idiomatic patterns, and the statistical regularities the embedding model learned during training. The model had billions of examples of English text paired with other English text. It learned the deep associations. "Terminate the contract" and "end the agreement" cluster tightly because the model saw them in similar contexts millions of times.

In cross-lingual retrieval, the model must bridge all of those gaps at once. The Japanese phrase for "terminate the contract" uses different characters, different grammar, different word order, and different idiomatic patterns than the English equivalent. The model learned to align them from parallel data -- bilingual sentence pairs where a human translator expressed the same meaning in both languages. But parallel data is finite, unevenly distributed, and concentrated in general domains. The model's cross-lingual alignment is an approximation, and approximations degrade under specificity.

The typical quality gap is 10 to 25 percent lower recall compared to monolingual retrieval on the same content. Research from 2025 measured even sharper drops in domain-specific cross-lingual retrieval, with recall on balanced, domain-specific corpora falling 30 to 50 points below same-language retrieval. The gap is widest for low-resource language pairs and specialized vocabularies. It is narrowest for high-resource pairs like English-French and general-purpose queries.

## Language Pair Asymmetry

Not all cross-lingual pairs are created equal. The quality of cross-lingual retrieval depends heavily on which two languages are involved, and the relationship is not symmetrical.

English-to-French cross-lingual retrieval performs well because the training data for every major multilingual model contains enormous quantities of English-French parallel text. United Nations documents, European Union proceedings, Canadian government publications, and decades of professional translation provide millions of high-quality aligned sentence pairs. The embedding model has seen extensive evidence of how English and French express the same concepts.

English-to-Thai cross-lingual retrieval performs significantly worse. Thai parallel data is scarcer. Thai uses a non-Latin script with no spaces between words, making tokenization less efficient. Thai-English parallel corpora are smaller and more concentrated in tourism, basic commerce, and government communication -- not in the technical domains that enterprise knowledge bases typically cover. The embedding model had fewer examples to learn from, and its cross-lingual alignment for Thai is weaker as a result.

The asymmetry also affects language direction. Japanese-to-English retrieval quality may differ from English-to-Japanese retrieval quality for the same model. The model's ability to map a Japanese query near relevant English documents is not the same as its ability to map an English query near relevant Japanese documents, because the embedding function is not perfectly bidirectional. In practice, the difference is small for high-resource pairs but can be meaningful for lower-resource ones.

The implication for system design: you cannot assume that cross-lingual retrieval quality is uniform. Measure it per language pair, in both directions, on your domain-specific content. The aggregate cross-lingual score on a benchmark tells you an average. Your users experience the specific pairs.

## Query Expansion: The Highest-Impact Technique

**Query expansion** is the single most effective technique for improving cross-lingual retrieval quality, and it is straightforward to implement. The idea: take the user's query in their language, translate it to the primary language of your document corpus, and search with both the original query and the translated version. Merge the results.

A Japanese user searches for the Japanese equivalent of "how to configure network timeout settings." The system sends two queries to the index: the original Japanese query and an English translation, "how to configure network timeout settings." The Japanese query finds relevant Japanese documents through monolingual retrieval. The English query finds relevant English documents through monolingual retrieval. The merged result set contains both Japanese and English documents ranked by relevance.

The quality improvement is substantial. The English translation of the query enters the same monolingual English embedding space as the English documents, which means it benefits from the high-quality within-language retrieval that monolingual systems enjoy. You are no longer relying solely on the cross-lingual alignment of the embedding model. You are also using its monolingual English retrieval capability, which is almost always stronger.

The cost is latency and complexity. You need a translation step in the query pipeline -- typically an LLM call or a dedicated NMT service. This adds 50 to 200 milliseconds depending on your translation provider and query length. You also need result merging logic: when the same document appears in both result sets (because the cross-lingual embedding found it from the Japanese query and the monolingual embedding found it from the English translation), you need a strategy for combining scores. Reciprocal rank fusion is the most common approach -- you take the reciprocal of each document's rank in each result set and sum them. Simple, effective, and well-tested.

Query expansion does not replace cross-lingual embeddings. It complements them. The cross-lingual embedding catches documents that the translated query might miss due to translation errors or paraphrasing differences. The translated query catches documents that the cross-lingual embedding ranks too low due to imperfect alignment. Together, they cover each other's blind spots.

## Document Translation: The Brute Force Approach

If cross-lingual retrieval introduces a quality gap, the most direct way to eliminate it is to remove the cross-lingual component entirely. **Pre-translate all documents** into every supported language. A Japanese user queries in Japanese and searches a Japanese index containing both originally-Japanese documents and Japanese translations of English documents. All retrieval is monolingual. The cross-lingual problem disappears.

This is the brute force approach, and it works. Monolingual retrieval quality is higher than cross-lingual retrieval quality, period. By translating your English documents into Japanese, you give the Japanese index the same content coverage as the English index, and the Japanese embedding model retrieves Japanese documents with its full monolingual capability.

The costs are real. Translating 100,000 English documents into Japanese is a significant investment -- even at LLM translation rates, the volume adds up quickly. You also multiply your storage requirements: a 100,000-document corpus serving five languages becomes a 500,000-document corpus. Every document update must be translated into all target languages, creating an ongoing maintenance burden. And translation errors in the documents propagate into the retrieval system. A poorly translated technical term in a Japanese version of an English document means that Japanese queries using the correct term will not find that document, even though the retrieval is monolingual.

The approach works best when your document corpus is relatively stable, when the languages you support are few, and when translation quality is high enough that translated documents do not introduce more errors than they prevent. For a knowledge base of 10,000 documents supporting three languages with monthly updates, document translation is feasible and often the right choice. For a corpus of 500,000 documents supporting twelve languages with daily updates, the ongoing translation cost and maintenance burden may outweigh the retrieval quality improvement.

A practical middle ground: translate only your most important documents. Identify the 20 percent of your corpus that receives 80 percent of the traffic. Translate those documents into all supported languages. Leave the long-tail content in its original language and rely on cross-lingual retrieval for those documents. This gives you monolingual retrieval quality for the most-searched content while controlling translation costs.

## Hybrid Retrieval: Dense Plus Sparse Across Languages

Cross-lingual retrieval benefits enormously from combining dense semantic search with sparse keyword-based search. The reason is that dense and sparse methods fail on different types of queries, and combining them covers both failure modes.

Dense cross-lingual retrieval excels at finding documents that express the same concept in different words. A Japanese query about "server response delays" finds English documents about "latency issues" and "timeout errors" because the embedding model has learned that these concepts are semantically related across languages.

Dense cross-lingual retrieval struggles with proper nouns, product names, version numbers, error codes, and other tokens that should be matched exactly rather than semantically. A query containing "ERR_CONNECTION_TIMEOUT" or "v2.3.1" relies on exact token matching. The semantic embedding treats these tokens as opaque character sequences and may not place them near the same tokens in English documents.

Sparse retrieval -- BM25 or learned sparse representations -- excels at these exact-match queries. But standard BM25 operates on tokens, which means it only works within a single language. A Japanese query and an English document share no tokens, so BM25 returns nothing.

The solution is to combine query expansion with sparse retrieval. Translate the query to the document language and run BM25 on the translated query. The translated query now shares tokens with the English documents, enabling keyword-based matching. Run dense cross-lingual retrieval in parallel using the original query and the multilingual embedding model. Merge the results from both methods.

BGE-M3 makes this pattern particularly clean because it produces both dense and sparse representations from a single forward pass. You embed the original query in Japanese and get dense, sparse, and multi-vector representations. The dense representation searches the cross-lingual index. The sparse representation of the translated query searches using keyword matching. The multi-vector representation enables fine-grained ColBERT-style scoring for the top candidates. Three retrieval signals from one model family, combined into a single ranked result set.

## Cross-Lingual Reranking

Initial retrieval -- whether dense, sparse, or hybrid -- produces a candidate set. Reranking takes that candidate set and re-scores each document for relevance to the query, using a more computationally expensive model that can examine the query and document together rather than comparing precomputed embeddings.

In cross-lingual retrieval, reranking is especially valuable because the initial retrieval may rank documents poorly due to the cross-lingual quality gap. A relevant English document might appear at position 15 in the initial results for a Japanese query. A strong cross-lingual reranker can identify it as the most relevant document and promote it to position 1.

The reranking landscape for multilingual systems has matured significantly by 2026. Qwen3-Reranker, available in 0.6B, 4B, and 8B parameter sizes, supports over 100 languages and handles context lengths up to 32,000 tokens. It is specifically designed for cross-lingual query-document scoring. Jina Reranker v2 Multilingual supports over 100 languages and includes specialized capabilities for code search alongside natural language search. Contextual AI's Reranker v2 has shown competitive results on multilingual benchmarks while maintaining strong English performance.

The operational cost of reranking is that it runs on every query, scoring each candidate document individually. If your initial retrieval returns 50 candidates and your reranker takes 10 milliseconds per document, reranking adds 500 milliseconds to your query latency. For latency-sensitive applications, limit the reranking candidate set to 10 or 20 documents and accept that some relevant documents in positions 21 through 50 may be missed. For quality-sensitive applications where latency is less constrained, rerank a larger candidate set and accept the latency trade-off.

The reranker should be evaluated specifically on your cross-lingual pairs. A reranker that excels at English-English relevance scoring may perform mediocrely on Japanese-English scoring. Test with your actual query-document pairs, in your actual language combinations, and measure MRR and nDCG at 5 to see whether the reranker improves upon your initial retrieval ranking.

## The Full Cross-Lingual Retrieval Pipeline

Putting the pieces together, a production cross-lingual retrieval pipeline has five stages.

**Stage 1: Language detection.** Identify the query language. This is typically fast and accurate for queries longer than a few words. Short queries or queries that mix languages require more robust detection, which subchapter 6.7 covers in detail.

**Stage 2: Query expansion.** Translate the query into the primary corpus language. If your corpus spans multiple languages, translate into each. Run the translation asynchronously with Stage 3 to minimize latency -- the cross-lingual embedding of the original query can proceed while the translation completes.

**Stage 3: Parallel retrieval.** Run dense cross-lingual retrieval using the original query against the full multilingual index. Run sparse retrieval using the translated query against the same index. If you use language-partitioned indexes, run monolingual retrieval against the query-language index and cross-lingual retrieval against other language indexes.

**Stage 4: Fusion.** Merge the result sets from parallel retrieval. Reciprocal rank fusion is the standard approach. Deduplicate documents that appear in multiple result sets. Produce a combined candidate set of the top 20 to 50 documents.

**Stage 5: Reranking.** Score each candidate document against the original query using a cross-lingual reranker. Reorder the results by reranker score. Return the top-k documents to the generation stage.

This five-stage pipeline adds latency compared to a simple monolingual retrieval system. The translation step adds 50 to 200 milliseconds. The parallel retrieval runs at roughly the same latency as a single retrieval since the searches happen concurrently. The reranking step adds 100 to 500 milliseconds depending on the candidate set size and model. Total end-to-end retrieval latency for a cross-lingual query is typically 200 to 700 milliseconds, compared to 50 to 150 milliseconds for monolingual retrieval.

For most knowledge base and enterprise search applications, this latency is acceptable. For real-time conversational applications, you may need to simplify the pipeline -- drop the reranking stage, reduce the candidate set, or pre-translate the query with a lightweight NMT model instead of an LLM.

## Measuring Cross-Lingual Retrieval Quality

You cannot improve what you do not measure, and cross-lingual retrieval demands more granular measurement than monolingual retrieval.

**Per-language-pair recall** is the most important metric. Measure recall at k equals 5 and k equals 10 for every combination of query language and document language that your system supports. If you support Japanese, English, French, and German, you have 12 directed language pairs to evaluate. Do not average them into a single score. The average hides the failures. A system with 90 percent recall for English-French and 45 percent recall for Japanese-English averages to 67.5 percent -- which sounds acceptable but means your Japanese users get a terrible experience.

**nDCG at 10** captures not just whether relevant documents are retrieved, but whether they are ranked highly. A system that retrieves all relevant documents but buries them at positions 7, 8, and 9 while placing irrelevant documents at positions 1, 2, and 3 has high recall but low nDCG. For RAG applications, where the generation model reads the top-k documents, ranking quality matters as much as recall.

**Mean reciprocal rank** tells you where the first relevant document appears. If MRR is 0.5, the first relevant document is typically at position 2. If MRR is 0.2, the first relevant document is typically at position 5. For single-answer queries where the user needs one specific document, MRR is the most actionable metric.

Run these metrics on a test set that reflects your actual query distribution. Include easy queries (high-frequency, general vocabulary) and hard queries (domain-specific terminology, rare language pairs, ambiguous intent). Weight the metrics by query frequency if your traffic is skewed -- a 5-percent recall improvement on the most common query type matters more than a 20-percent improvement on a rare one.

Update your test set quarterly. As your document corpus grows and your user base evolves, the queries and documents that matter change. A static test set from six months ago may not reflect the retrieval challenges your system faces today.

## When Cross-Lingual Retrieval Is Not Enough

Cross-lingual retrieval with all the techniques described in this subchapter -- query expansion, hybrid search, reranking -- still has a ceiling. That ceiling is determined by the embedding model's alignment quality for your specific language pairs and domain. If the alignment quality is fundamentally insufficient for your use case, no amount of pipeline engineering closes the gap.

Signs that you have hit the ceiling: recall plateaus despite adding more retrieval stages. The reranker frequently promotes documents that the initial retrieval ranked very low, indicating the initial retrieval is unreliable. Users in specific languages consistently report irrelevant results for domain-specific queries. Quality differences between languages remain large even after optimizing the pipeline.

When you hit this ceiling, the path forward is document translation for your highest-value content combined with cross-lingual retrieval for the long tail. Accept that some language pairs will have lower retrieval quality, build monitoring to detect when quality drops below acceptable thresholds, and use human feedback loops to identify and fix the specific queries and documents where the system fails most often.

The next subchapter examines one of the most underestimated components of the multilingual retrieval pipeline: chunking. The way you split documents into embeddable segments works one way for English and breaks in specific, predictable ways for CJK languages, Thai, Arabic, and other scripts -- and those chunking failures quietly degrade everything downstream.