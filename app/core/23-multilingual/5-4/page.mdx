# 5.4 â€” Machine Translation Post-Editing: The Cost-Quality Sweet Spot

The localization manager stares at a spreadsheet with three columns. Column one: full human translation for fourteen languages, $320,000 per quarter, six-week turnaround, publication quality. Column two: raw machine translation, nearly free, instant, littered with errors that would embarrass the company if shipped to customers. Column three is empty. She needs a middle option -- something that costs less than human translation, delivers higher quality than raw MT, and ships fast enough for a product that updates weekly. That empty column is where MTPE belongs.

**Machine translation post-editing** is the workflow where a machine translates first and a human editor fixes what the machine got wrong. The editor works from the MT output, not from scratch. This distinction changes everything about the economics and the skill set required. MTPE is not cheaper human translation. It is a fundamentally different process that produces different cost curves, requires different editor competencies, and creates different failure modes. Done well, it occupies the cost-quality sweet spot that makes multilingual products financially viable. Done poorly, it produces translations that are worse than either pure human or pure machine output, because the editor patched surface errors while leaving deeper problems intact.

## How MTPE Works

The MTPE workflow has three stages that most teams understand in theory and execute poorly in practice.

Stage one is machine translation. Your source content -- a knowledge base article, a product description, an AI-generated customer communication -- passes through a machine translation engine. In 2026, this engine might be a neural machine translation service like DeepL or Google Translate, or it might be a frontier LLM like GPT-5 or Claude Opus 4.6 used as a translator. The choice of engine matters enormously for what happens next, because the quality of the initial machine output determines how much editing the human needs to do.

Stage two is human post-editing. A trained editor reads the machine output alongside the source text and corrects errors. The editor fixes mistranslations, smooths unnatural phrasing, enforces terminology standards, and adjusts formality and register. The key word is "corrects." The editor is not translating from scratch. They are starting from a machine-generated draft and improving it. This changes the cognitive task from generation to evaluation and correction -- a faster process for most content types, but one that introduces its own psychological traps.

Stage three is quality assurance. The edited output is reviewed against the source, checked for consistency with terminology glossaries, and scored using metrics like COMET or CometKiwi. This stage catches errors that the editor missed or introduced. It also generates data that feeds back into stage one: if the same error type recurs across dozens of segments, the MT engine or its prompting strategy needs adjustment.

## Light Post-Editing vs Full Post-Editing

The industry distinguishes two tiers of post-editing, and choosing the wrong tier for your content type is one of the most common MTPE mistakes.

**Light post-editing** corrects only errors that affect comprehension. The editor ensures the meaning is accurate, fixes factual errors, corrects mistranslations, and removes anything offensive or dangerous. They do not polish style, fix awkward phrasing, or optimize for naturalness. The output reads like decent machine translation -- correct but sometimes stiff. Light post-editing typically costs 40 to 50 percent of full human translation rates. An editor doing light post-editing can process 5,000 to 8,000 words per hour, compared to 2,000 to 3,000 words per hour for full translation from scratch.

**Full post-editing** produces publication-quality text. The editor corrects everything that light PE covers, plus they improve fluency, adjust style and register, ensure natural phrasing, and produce output that reads as if a native speaker wrote it. Full post-editing costs 60 to 75 percent of human translation rates. Processing speed drops to 3,000 to 5,000 words per hour, because the editor is doing more than patching errors -- they are reshaping prose.

The mistake teams make is applying full post-editing to content that only needs light editing, or applying light editing to content that needs full treatment. Internal knowledge base articles, developer documentation, and analytics dashboards need light post-editing at most. The meaning must be correct. The style does not need to be elegant. Customer-facing product descriptions, marketing emails, and support communications need full post-editing because the user perceives quality through naturalness, not just accuracy. A technically correct but robotic-sounding product description damages brand perception in ways that no accuracy metric captures.

Legal contracts, medical instructions, and regulatory filings should not go through MTPE at all. They need full human translation or native-language creation, because the stakes of any residual error -- even one that a competent editor might miss -- are too high to accept the risk inherent in starting from machine output.

## The Anchoring Problem

The most dangerous failure mode in MTPE is not an editor missing an error. It is an editor accepting an error because the machine output looks plausible.

This is **anchoring bias** in post-editing, and it is well-documented in translation research. When an editor reads machine-generated text, their brain uses the MT output as a reference point. If the machine produces a sentence that is fluent and grammatically correct but contains a subtle meaning error -- a negation that was dropped, a number that was slightly changed, a conditional that became an unconditional -- the editor's cognitive load works against detection. The sentence looks right. It reads smoothly. The brain says "this is fine" and moves on. The same editor, translating from scratch without the MT anchor, would never make this error because they would construct the meaning directly from the source text.

Research on post-editing bias has found that editors working from MT output accept incorrect suggestions from the machine at surprisingly high rates. The errors are not random -- they cluster around specific types. Dropped negations are the classic example: the source says "do not store above 25 degrees" and the MT output says "store above 25 degrees." The sentence is fluent. The editor scans it, sees correct grammar, sees the temperature, and moves on. The missing "not" is invisible because the surrounding text provides no signal that something is wrong.

Anchoring bias is worse with higher-quality MT. When the machine output is mostly bad -- stilted, ungrammatical, obviously wrong -- the editor is in full correction mode. Their attention is high. Every sentence gets scrutinized because every sentence needs work. When the machine output is mostly good -- the LLM-based translations of 2026, which are fluent and natural for high-resource language pairs -- the editor relaxes. Their correction threshold rises. They scan rather than scrutinize. And the rare errors that remain in otherwise excellent output become harder to catch, not easier.

The mitigation is structural, not motivational. Telling editors to "be more careful" does not overcome a cognitive bias. Instead, build detection aids into the workflow. Flag segments where the MT engine's confidence score is lower than average. Highlight segments that contain numbers, negations, or conditionals -- the error-prone categories. Use xCOMET's error span detection to pre-identify segments likely to contain accuracy errors and route those segments for closer review. Give the editor tools that compensate for the bias the MT output creates.

## When MTPE Makes Sense -- and When It Does Not

MTPE is the right workflow for a specific band of content. Understanding the boundaries of that band prevents costly misapplication.

MTPE works best for user-facing content that needs quality but not perfection. Knowledge base articles, help documentation, product descriptions for mid-tier categories, FAQ pages, in-app guidance text, email notifications, and AI-generated summaries all fit this band. The content matters to users but does not carry legal or safety liability. Errors are embarrassing, not dangerous. The volume is high enough that full human translation is prohibitively expensive but low enough that quality matters too much for raw MT.

MTPE is the wrong workflow for three categories of content. First, legal and regulatory text -- contracts, terms of service, privacy policies, compliance disclosures, medical instructions. These documents carry liability. A post-editor who misses a subtle error in a translated medical instruction creates legal exposure that far exceeds the cost savings from using MTPE instead of full human translation. Second, brand-critical marketing copy -- taglines, campaign headlines, brand manifestos, product launch announcements. These need creative native writing, not edited machine output. The best MTPE in the world still reads like edited machine output to a native marketing professional. Third, ultra-low-stakes internal content -- log translations, internal analytics labels, developer comments. These do not need human editing at all. Raw MT is sufficient, and adding a human editing step wastes budget that should go to higher-value content.

The decision framework is simple. Ask two questions. First: does an error in this content create legal, financial, or safety risk? If yes, skip MTPE and use full human translation. Second: does the quality of this content affect user perception or brand trust? If yes, use full MTPE. If no, use raw MT or light MTPE. The answers to these two questions sort your content into the right workflow tier for more than 90% of cases.

## Measuring MTPE Quality

You need two types of measurement to manage an MTPE program: output quality and process efficiency.

Output quality measurement uses the same metrics described in the previous subchapter. Run COMET or CometKiwi on the post-edited output. Compare scores against your quality thresholds by content tier. Track scores over time by language, by editor, and by MT engine. A score that trends downward for a specific language means your MT engine has degraded for that language pair, your editor is fatiguing, or your content has shifted into a domain where the engine performs poorly.

Process efficiency measurement uses **Translation Edit Rate**, or TER. TER measures the number of edits -- insertions, deletions, substitutions, and word reorderings -- needed to transform the MT output into the final post-edited version, divided by the length of the final version. A TER of 0.15 means the editor changed 15% of the text. A TER of 0.60 means the editor rewrote most of it.

TER serves two purposes. First, it measures the quality of your MT engine. If TER is consistently above 0.40, the MT output requires so much editing that you are not saving meaningful time over translating from scratch. The engine is not producing useful first drafts -- it is producing raw material that the editor must largely rebuild. When TER reaches this level, either switch to a better engine for that language pair or route the content to human translation instead.

Second, TER helps you price MTPE work fairly. If you pay editors per word, you need to know how many words they actually edited, not how many words the document contains. Paying per-source-word for MTPE penalizes editors who work on poor-quality MT output -- they do more work for the same pay. Paying based on edit distance, calibrated by TER, aligns compensation with effort. Several major language service providers shifted to TER-based pricing between 2024 and 2025, and the practice has become standard for professional MTPE programs by 2026.

## The Editor Skill Set

MTPE editors need different skills than translators, and most teams underestimate this difference.

A translator starts from a blank page in the target language. They read the source, understand the meaning, and construct the target text from their own linguistic knowledge. The core skills are comprehension, native fluency, domain knowledge, and creative expression. The translator is a writer who works from a foreign-language brief.

An MTPE editor starts from a machine-generated draft. They read both the source and the MT output simultaneously, identify discrepancies, evaluate whether each discrepancy is an error or an acceptable variant, and correct the genuine errors while preserving the acceptable passages. The core skills are comparison, error detection, judgment about when to edit and when to leave alone, and the discipline to resist anchoring bias. The editor is a quality controller who also writes.

These are overlapping but distinct skill sets. Excellent translators sometimes make poor post-editors because they instinctively rewrite the entire output to match their preferred style, even when the MT output is already correct. This over-editing wastes time and budget without improving quality -- research has found that roughly one in three edits made by post-editors are unnecessary stylistic changes that do not improve the translation. On the other hand, editors with strong quality control instincts but weaker generative writing skills sometimes under-edit, accepting awkward phrasing that a stronger writer would improve.

The best MTPE editors have a specific profile: bilingual fluency, domain expertise, high tolerance for imperfect prose (they do not need to make every sentence beautiful, only correct and clear), strong error detection skills, and awareness of their own anchoring susceptibility. Train editors explicitly on anchoring bias. Show them examples of plausible-looking MT errors. Run calibration exercises where they post-edit segments that contain deliberately introduced errors and measure their detection rate. The editors who detect 90% or more of planted errors are your best post-editors. The editors who detect less than 70% need more training before they work on production content.

## The LLM-MTPE Hybrid

The shift from NMT to LLM-based translation changes the MTPE equation significantly. When your initial machine translation comes from a frontier LLM rather than a traditional NMT engine, the post-editing task transforms in several ways.

LLM translations are more fluent than NMT translations for most high-resource language pairs. They produce fewer grammatical errors, more natural phrasing, and better tone consistency. The TER for LLM-based output is typically 0.10 to 0.25 for high-resource pairs, compared to 0.20 to 0.40 for NMT. This means editors do less work per segment, which translates directly to lower cost and faster turnaround. Industry experience from 2024-2025 shows that switching the MT engine from NMT to LLM reduces post-editing time by 30 to 50 percent for language pairs like English-German, English-French, and English-Spanish.

But LLM translations introduce new error patterns that NMT does not produce. LLMs sometimes hallucinate content that does not appear in the source -- adding explanatory text, elaborating on a point, or inserting information the model considers helpful but that the source text does not contain. NMT engines never add content. They translate what is there, sometimes incorrectly, but they do not invent new content. An MTPE editor working with NMT output looks for translation errors. An MTPE editor working with LLM output must also look for additions and hallucinations -- a different cognitive task that requires comparing the source and target not just for accuracy but for completeness.

LLMs also produce more confident-sounding errors. An NMT engine that mistranslates a term often produces a slightly awkward sentence -- a signal to the editor that something is off. An LLM that mistranslates the same term produces a fluent, natural-sounding sentence with the wrong meaning -- a harder error to catch. This amplifies the anchoring problem described earlier. The better the MT output sounds, the more trust the editor places in it, and the more likely they are to miss the errors that remain.

The practical workflow for LLM-MTPE in 2026 is a three-pass system. First, the LLM translates the content with detailed instructions -- domain, glossary, formality level, tone. Second, an automated quality layer (xCOMET or CometKiwi) scores every segment and flags low-confidence ones. Third, the human editor reviews all segments but focuses attention on the flagged ones, with specific instructions to check for hallucinated content, dropped negations, and terminology consistency. This workflow produces higher quality at lower cost than either NMT-MTPE or full human translation, and it is the approach that the most sophisticated localization operations have converged on.

## Workflow Integration

MTPE does not work as an afterthought bolted onto an existing content pipeline. It works when it is designed into the pipeline from the start.

The content management system must support MTPE-specific states: source content ready, MT generated, assigned to editor, editing in progress, editing complete, QA in progress, QA approved, published. These states are different from a translation workflow because the MT generation step is automated and the editing step is a revision, not a creation. The CMS must track both the MT version and the post-edited version, because the difference between them is your TER data.

The assignment system must route content to the right editor. Not every editor handles every domain. A medical post-editor should not receive marketing copy. A marketing post-editor should not receive legal text. Domain routing is basic but frequently neglected -- many MTPE programs assign work by language only, ignoring domain expertise, which leads to editors spending extra time on unfamiliar content or producing lower-quality edits because they lack domain vocabulary.

The feedback loop must be continuous. When editors repeatedly correct the same error type -- a specific term that the MT engine consistently mistranslates, a formality level that the engine defaults to incorrectly, a number format that the engine gets wrong for a specific locale -- that pattern must feed back into the MT engine's configuration. If you are using an LLM for translation, this means updating the translation prompt or the glossary. If you are using NMT, this means updating the custom model or the terminology database. Without this feedback loop, editors correct the same errors every cycle, which is a waste of human effort and a sign that the system is not learning.

## The Cost-Quality Math

The economics of MTPE make it the only viable path to multilingual products at scale. The numbers tell the story clearly.

Full human translation for a mid-size AI product -- 200,000 words of source content, fourteen languages -- costs roughly $280,000 to $420,000 per translation cycle at rates of $0.10 to $0.15 per word. Turnaround is four to eight weeks. Quality is high but the cost scales linearly with content volume and language count. Doubling your content or adding seven languages doubles the cost.

Raw machine translation for the same content costs under $500 using NMT, or $2,000 to $5,000 using LLMs. The quality is insufficient for customer-facing content. You save money and lose users.

MTPE for the same content -- LLM-based MT with full post-editing -- costs $84,000 to $168,000 per cycle at rates of $0.03 to $0.06 per post-edited word. Turnaround is two to four weeks. Quality approaches publication level for most content types. The savings compared to full human translation are 50 to 70 percent, with quality that meets or exceeds user expectations for everything except the highest-stakes content tiers.

Light MTPE reduces costs further to $56,000 to $112,000 per cycle, suitable for knowledge bases, internal documentation, and content where correctness matters more than elegance. The math becomes even more favorable as your content volume grows, because MT cost scales near-linearly while post-editing effort scales sub-linearly -- editors get faster as they internalize your domain vocabulary and the MT engine's typical error patterns.

The cost-quality sweet spot is exactly this: LLM-based machine translation with human post-editing, tiered by content criticality, with automated quality scoring on every segment. It is not the cheapest option. It is not the highest-quality option. It is the option that makes multilingual products economically sustainable at the scale most teams actually operate.

The next subchapter moves from how you translate to how you adapt -- building translation systems that understand context, tone, and domain, so the machine output arrives closer to correct and the human editor has less to fix.