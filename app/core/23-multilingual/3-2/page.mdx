# 3.2 — The Token Tax by Language: Quantifying the Cost Disparity

The same sentence costs different amounts depending on which language it is written in. This is not a bug. It is the economics of tokenizer design. And until you measure the exact multiplier for every language you support, you are running a multilingual product with hidden subsidies for English users and hidden penalties for everyone else. The previous subchapter explained why tokenizers create this disparity. This one gives you the numbers.

## The Metric That Makes the Tax Visible

**Token fertility** is the ratio that quantifies the tax. You compute it by dividing the number of tokens a tokenizer produces for a given text by the number of words or characters in that text. For languages with space-delimited words like English, French, or German, fertility is tokens per word. For languages without word boundaries like Chinese, Thai, or Japanese, fertility is tokens per character or tokens per semantic unit, depending on which measurement your team adopts.

A fertility of 1.0 means every word maps to a single token. This is the theoretical ideal for efficiency. English on most major tokenizers achieves fertility between 1.0 and 1.3, meaning common words are single tokens and only uncommon words split into two or three subwords. A fertility of 3.0 means every word becomes three tokens on average — three times the cost, three times the context window consumption, three times the generation latency per semantic unit of meaning.

The number that matters for your business is not fertility in isolation but the **fertility ratio**: your target language's fertility divided by English fertility on the same tokenizer. If English fertility is 1.2 and Korean fertility is 2.83, the fertility ratio is 2.36. Korean text costs 2.36 times more tokens than semantically equivalent English text. That ratio is your token tax multiplier. It shows up in your API bill, your context budget, and your generation quality — every single request, every single day.

## The English Baseline

English occupies a privileged position across every tokenizer in production. On OpenAI's cl100k_base, English fertility for general text averages around 1.2 tokens per word. On the newer o200k_base, it drops to approximately 1.1. On Llama 4's tokenizer, it sits near 1.2. On Qwen's tokenizer, near 1.15. On Gemma 3's tokenizer, near 1.15.

These numbers are not accidental. They are the direct result of English dominating the training corpora that shaped these tokenizers. Common English words — "the," "is," "and," "have," "with," "from" — are single tokens everywhere. Even moderately complex words like "understanding," "production," or "infrastructure" split into at most two tokens. English-specific patterns like "-tion," "-ment," "-ing," "-ness" each occupy dedicated vocabulary slots, giving the tokenizer efficient merge paths for the most productive English suffixes.

This is the baseline against which you measure every other language. When someone says a model has a 128,000-token context window, they are describing a window that holds roughly 96,000 to 110,000 words of English. For any other language, divide that word count by the fertility ratio. The effective context window shrinks in direct proportion to the tax.

## Western European Languages: The Moderate Tax

Western European languages that share the Latin alphabet with English enjoy a relatively mild token tax, but it is not zero.

French averages roughly 1.2 to 1.4 times English fertility on most tokenizers, giving it a tax of around 1.1 to 1.2 times. French uses diacritical marks like accents and cedillas that English does not, and certain French words diverge enough from English patterns that they split into additional subwords. But the structural similarity to English — shared alphabet, shared word roots from Latin and Norman French, similar word lengths — keeps the penalty modest.

Spanish sits even closer to English, with fertility ratios around 1.05 to 1.15. Spanish has regular spelling, consistent syllable patterns, and heavy representation in training data as the world's fourth most spoken language. Spanish is arguably the cheapest non-English language to serve through token-based APIs.

German presents a more interesting case. Its fertility ratio reaches 1.2 to 1.4, higher than French or Spanish, largely because of compound nouns. German freely creates new words by combining existing ones — a legal term might be a single word of thirty or more characters. Tokenizers trained primarily on English have no single vocabulary entry for these compounds, so they split them into three, four, or five subwords. The compound word for "beef labeling supervision task transfer law" is a single word in German that a typical English-centric tokenizer fragments into eight or more tokens. Individual compound words create local spikes in fertility that average out across full documents but cause real problems for specific domains — German legal text, scientific text, and government correspondence are measurably more expensive to process than conversational German.

Russian and other Cyrillic-script languages face a fertility ratio of roughly 1.4 to 1.6. The Cyrillic alphabet shares no characters with Latin, so tokenizers must allocate separate vocabulary space for Cyrillic letter combinations. Most major tokenizers now include dedicated Cyrillic tokens for common Russian words, but the coverage is thinner than for Latin-script languages. Less common Cyrillic-script languages like Bulgarian, Serbian, or Macedonian pay a steeper tax than Russian because their specific word patterns receive fewer dedicated vocabulary slots.

## CJK Languages: The Structural Challenge

Chinese, Japanese, and Korean each present distinct tokenization challenges, and grouping them as "CJK" obscures important differences.

**Mandarin Chinese** uses characters where each character typically represents a morpheme — a unit of meaning. Common characters in a well-designed tokenizer like Qwen's map to single tokens, achieving fertility around 1.5 to 1.8 tokens per character. On English-centric tokenizers like cl100k_base, that same character might require two to three byte-level tokens, pushing fertility to 2.0 to 2.5 per character. But here is the nuance that raw fertility numbers miss: a single Chinese character often carries the semantic weight of an entire English word. The character meaning "electricity" is one character, one concept. In English, "electricity" is one word but two or three tokens. So the per-concept cost is closer to parity than the raw per-character fertility suggests — perhaps 1.3 to 1.8 times English on a semantic basis for Mandarin on a good tokenizer. On a bad one, it can reach 2.5 times.

**Cantonese**, written with traditional Chinese characters plus Cantonese-specific characters not found in standard Mandarin, faces a steeper penalty. Its unique characters are less represented in training data, and tokenizers optimized for Simplified Chinese do not cover them efficiently. Cantonese fertility ratios reach 2.0 to 2.1 times English on a semantic basis — measurably worse than Mandarin.

**Japanese** is the most complex case in the CJK group because it uses three writing systems simultaneously: kanji, hiragana, and katakana, plus Latin characters for borrowed words and numbers. A single Japanese sentence might switch between all four. Kanji characters share vocabulary entries with Chinese characters, but hiragana and katakana syllables often fragment because their frequency in training data is lower than kanji. Mixed-script sentences create token boundary chaos — the tokenizer must constantly shift between different parts of its vocabulary, and boundary tokens between scripts add overhead. Japanese fertility ratios range from 2.0 to 3.5 times English, with the high end appearing in conversational text heavy with hiragana particles and katakana loan words.

**Korean** uses Hangul, an alphabetic system where letters are combined into syllable blocks. Each syllable block is visually one unit but can be decomposed into two to three component letters. Tokenizers handle Korean syllable blocks inconsistently. Some tokenize at the syllable-block level, achieving reasonable fertility. Others decompose blocks into individual jamo letters, which then fragment further into byte-level tokens. On OpenAI's tokenizers, Korean achieves a fertility ratio of approximately 2.36 times English — meaning every Korean interaction costs roughly 2.4 times as many tokens. On Qwen's tokenizer, which has better CJK coverage, the ratio drops to around 1.8. On Gemma 3's tokenizer, it lands near 2.0. The choice of tokenizer changes the Korean token tax by thirty to forty percent.

## Arabic and Complex Morphology Languages

Arabic presents a unique challenge that goes beyond script unfamiliarity. Arabic is a **root-and-pattern morphological system** where a three-letter root combines with vowel patterns, prefixes, and suffixes to create an enormous range of word forms. A single root can generate dozens of words, each carrying different tense, person, number, and mood information. The word "and they wrote it" can be a single Arabic word of six or seven characters — encoding conjunction, subject, verb, object, and tense in one unit.

Tokenizers split these morphologically dense words into subwords that fragment the grammatical information. The prefix meaning "and" separates from the root, the suffix meaning "it" separates from the verb form. The model must reconstruct the grammatical relationships from disconnected fragments. Arabic fertility ratios range from 2.0 to 3.5 times English across major tokenizers. Formal Arabic — the register used in legal documents, news, and government communication — tends toward the higher end because it uses longer, more morphologically complex word forms than conversational Arabic.

Turkish, Finnish, and Hungarian belong to a category linguists call agglutinative languages, where grammatical information is expressed through chains of suffixes attached to a base word. A single Turkish word can express what English requires a full clause to say. These suffix chains produce long words that tokenizers fragment aggressively. Turkish fertility ratios reach 1.8 to 2.5 times English. Finnish, with its fifteen grammatical cases and complex compound word system, ranges from 2.0 to 3.0. Recent research in 2025 using Turkish as a benchmark demonstrated that tokenizer choice alone can swing accuracy on downstream tasks by ten to fifteen percentage points for agglutinative languages — a magnitude of impact that most teams attribute to model quality rather than tokenizer design.

## South and Southeast Asian Scripts

Hindi and other Devanagari-script languages face fertility ratios of 2.0 to 3.0 times English. Devanagari combines consonants and vowels into conjunct characters — visual units where multiple sounds merge into a single glyph. Tokenizers struggle with conjuncts because the byte-level representation of a conjunct is three or four bytes, and English-centric BPE merge rules rarely create dedicated tokens for Devanagari conjunct patterns. Hindi, the most represented Devanagari language, gets the best treatment. Marathi, Nepali, and Sanskrit pay a higher penalty because their specific conjunct patterns appear less frequently in training data.

Thai deserves special attention because it combines two problems: an unfamiliar script and no word boundaries. Thai is written as a continuous stream of characters without spaces between words. The tokenizer must simultaneously figure out where words begin and end and how to represent each word in its vocabulary. Most tokenizers solve this badly for Thai. They fall back to character-level or byte-level tokenization, producing fertility ratios of 2.5 to 4.0 times English. A Thai business email that communicates the same information as a 200-token English email might consume 600 to 800 tokens. The cost implications are staggering for any company serving Thai-speaking users at scale.

Vietnamese uses the Latin alphabet but with an extensive system of diacritical marks — six tones marked with diacritics, plus additional marks for vowel quality. Each Vietnamese word is short (typically one or two syllables), but the diacritics create byte sequences that English-centric tokenizers handle poorly. Fertility ratios range from 1.5 to 2.5, better than Thai but worse than Western European languages.

## The Low-Resource Cliff

Below the top thirty or forty languages, tokenizer efficiency falls off a cliff. Languages like Amharic, Tigrinya, Khmer, Myanmar, Lao, Sinhala, and Georgian use scripts that appear in vanishingly small proportions of tokenizer training data. Their characters never win the BPE frequency competition. They stay fragmented into raw bytes.

Amharic, spoken by over fifty million people, achieves fertility ratios of 5.0 to 8.0 times English on most major tokenizers. Each Ge'ez script syllable — a single visual unit that a literate Amharic speaker processes instantly — becomes four to six tokens. A model with a 128,000-token context window that holds 96,000 words of English holds fewer than 15,000 words of Amharic. The effective context window shrinks by more than 80%.

Myanmar script, used by over fifty million speakers of Burmese and related languages, faces similar penalties. Khmer, the language of Cambodia, and Lao, the language of Laos, are equally affected. These are not rare languages. They are the national languages of entire countries, spoken by tens of millions of people, yet they receive tokenizer treatment comparable to what English would receive if the entire tokenizer vocabulary had only 200 slots.

The implication is stark: for truly low-resource scripts, the token tax is not a cost multiplier. It is a viability question. If serving Amharic-speaking users costs five to eight times more per interaction than serving English-speaking users, and the quality is measurably worse because the model receives less context and processes less meaningful tokens, the business case for supporting the language collapses unless you take active countermeasures.

## Per-Model Tokenizer Comparison

Not all tokenizers impose the same tax. Your choice of model family changes the tax your users pay, and the differences are large enough to alter product strategy.

OpenAI's evolution from cl100k_base to o200k_base illustrates progress within a single vendor. The larger vocabulary added tens of thousands of multilingual tokens, reducing fertility for Chinese by roughly 15 to 20%, for Arabic by 10 to 15%, and for Hindi by 10 to 20%. The improvement for Thai and low-resource scripts was smaller — perhaps 5 to 10% — because the additional vocabulary slots still prioritized higher-frequency languages. GPT-5 and its variants use o200k_base, so any OpenAI deployment in 2026 benefits from the improved tokenizer.

Qwen's tokenizer, with its deliberate CJK optimization and approximately 152,000 vocabulary tokens, achieves the best fertility for Chinese and Korean among major model families. If your primary non-English market is East Asian, Qwen's tokenizer gives you a twenty to thirty percent token cost advantage over OpenAI or Llama for those languages. The trade-off is less competitive fertility for Arabic and South Asian scripts.

Llama 4's tokenizer expanded to roughly 128,000 tokens with improved coverage for the twelve languages Meta prioritized for multilingual instruction tuning. Arabic, Hindi, Thai, Vietnamese, and Indonesian all saw meaningful fertility improvements over Llama 3. But languages outside Meta's target twelve saw minimal change.

Gemma 3's tokenizer, with approximately 262,000 vocabulary tokens — the largest among major open-weight models in 2026 — takes the broadest multilingual approach. Its fertility ratios for Arabic, Hindi, and Thai are competitive with or better than any other model family. The larger vocabulary means more scripts receive dedicated tokens rather than falling back to byte-level representation. For teams serving linguistically diverse user bases, Gemma 3's tokenizer offers the most equitable token tax distribution across languages.

A February 2026 technique called LiteToken addresses the vocabulary efficiency problem from a different angle. Researchers found that roughly 10% of tokens in any BPE vocabulary are intermediate merge residues — tokens that were frequently created during the BPE merge process but are rarely used in the final tokenized output. Removing these residue tokens and reallocating their vocabulary slots can reduce fragmentation and improve fertility for underserved languages without expanding the total vocabulary size. LiteToken is a plug-and-play optimization that works with any existing tokenizer, and early results show it reduces token counts for non-English text by three to seven percent while preserving English efficiency.

## How to Measure Your Own Token Tax

You cannot rely on published fertility numbers. They are averages across general-purpose text. Your actual token tax depends on the specific language variety your users speak, the domain your product operates in, and the distribution of formal versus informal text in your pipeline.

To measure your token tax, collect a representative sample of real user inputs — at least a thousand per language. Run each input through the tokenizer of every model you are considering. Count the tokens. Compute fertility as tokens per word for space-delimited languages, or tokens per character for scripts without word boundaries. Calculate the fertility ratio relative to English for the same tokenizer.

Build a table with one row per language, one column per tokenizer, and fertility ratio as the cell value. This table is your token tax map. It tells you exactly how much more each language costs, how much context window each language consumes, and which tokenizer minimizes the penalty for your specific language mix. Post this table where your team can see it. Update it whenever you evaluate a new model or tokenizer. Make the tax visible, because invisible taxes are taxes nobody manages.

The fertility ratio also gives you a correction factor for cost projections. If your English cost per interaction is $0.02 and your Korean fertility ratio is 2.36, your Korean cost per interaction is approximately $0.047 — before any other multilingual overhead. If your Japanese fertility ratio is 2.8, Japanese interactions cost approximately $0.056. These are not estimates. They are direct multipliers. Every token in the fertility ratio shows up on your invoice.

## What the Numbers Mean for Product Decisions

The token tax is not just a cost line item. It is a product design constraint. When you know that Japanese text consumes 2.5 to 3.5 times more tokens than English, every product decision that involves token budgets must account for that multiplier.

Your system prompt budget shrinks. Your few-shot example budget shrinks. Your RAG retrieval budget shrinks. Your conversation history budget shrinks. All of them shrink by the fertility ratio, simultaneously, for every non-English language. The cumulative effect is that your non-English users receive a fundamentally different product — less context, fewer examples, shorter history, worse answers — unless you design your system to compensate.

This is not a theoretical concern. It is the daily reality of every multilingual AI product in 2026. And the first step to managing it is knowing the exact number. The next subchapter shows what happens when that number meets a fixed context window — a phenomenon this book calls Context Window Starvation.
