# 4.3 â€” The Culture-Specific Question Problem: What MMLU Gets Wrong

Twenty-eight percent of MMLU questions contain culture-specific content. Of the geography-related questions, 84.9 percent focus on North America and Europe. These numbers come from the Global MMLU study, which hired native-speaking annotators across 42 languages to audit every question for cultural sensitivity -- and the findings should change how you think about every benchmark score you have ever seen.

MMLU -- the Massive Multitask Language Understanding benchmark -- has been the default standard for measuring large language model capability since its introduction. Model comparison leaderboards use it. Company press releases cite it. Engineering teams use it to choose which model to deploy. When the benchmark was extended to other languages through translation, those translated versions became the default for multilingual evaluation too. But MMLU was designed by American academics for American academic contexts. Its questions assume a Western educational framework, Western cultural knowledge, and Western value systems. Translating those questions into Hindi, Arabic, or Swahili does not make them Hindi, Arabic, or Swahili evaluations. It makes them American evaluations in other languages.

This subchapter explains what culture-specific content means in the context of benchmarks, why it distorts multilingual evaluation, and what the research community has built to address it.

## The Anatomy of Cultural Bias in MMLU

The Global MMLU project, led by Cohere's research team and published at ACL 2025, conducted the most thorough audit of cultural bias in a major benchmark to date. Annotators classified each MMLU question into two categories: culturally agnostic -- meaning the question tests knowledge that is universal regardless of the respondent's culture -- and culturally sensitive -- meaning the question requires knowledge specific to a particular cultural or regional context.

The results were stark. Twenty-eight percent of all MMLU questions fell into the culturally sensitive category. For a benchmark that is treated as a universal measure of language understanding, nearly a third of its questions are not universal at all. They test whether the model has absorbed specific knowledge about specific cultures -- overwhelmingly Western cultures.

The cultural concentration becomes even more visible in specific subject areas. The majority of cultural tags, 86.5 percent, were classified as Western. Within that Western category, 73.9 percent of the questions required knowledge about the United States specifically, followed by the United Kingdom at 8 percent. The geography questions told the same story: 84.9 percent of questions requiring geographic knowledge focused on North American or European regions.

What does this look like in practice? A question about the structure of the US Senate appears in the political science section. A question about the Fourth Amendment appears in the law section. A question about the Emancipation Proclamation appears in the history section. A question about Thanksgiving traditions appears in the cultural studies section. Each of these is a perfectly valid knowledge question -- for an American student taking an American exam. None of them measures whether a model understands Hindi political structures, Japanese legal principles, Egyptian history, or Indonesian cultural practices.

## Why This Distortion Matters for Your Product

You might think that cultural bias in an academic benchmark is an academic problem. It is not. The distortion cascades directly into product decisions through three mechanisms.

The first mechanism is model selection. When you compare models on MMLU or its translated variants, you are comparing their ability to answer Western-centric questions. A model that was trained heavily on English Wikipedia and American educational materials will score well. A model that was trained on a more balanced multilingual corpus might score lower -- not because it is a worse model, but because it has allocated more of its capacity to non-Western knowledge. If you select your model based on MMLU scores, you are selecting for Western cultural knowledge and potentially selecting against the broad multilingual capability your non-English users need.

The Global MMLU study demonstrated this directly: model rankings changed when evaluated on the culturally sensitive subset versus the full benchmark. A model that ranked third overall might rank first on culturally agnostic questions and seventh on culturally sensitive ones. If your users are in Japan, India, or Brazil, the culturally sensitive ranking is the one that matters -- and it tells a different story than the headline number.

The second mechanism is training optimization. Model developers optimize for benchmarks. When MMLU and its variants are the primary evaluation targets, training data curation, fine-tuning objectives, and reinforcement learning reward signals all push toward Western-centric knowledge. This creates a feedback loop: Western-centric benchmarks reward Western-centric training, which produces Western-centric models, which perform well on Western-centric benchmarks. The loop is self-reinforcing and it has been running for years.

The third mechanism is false confidence. A team that evaluates their multilingual model on translated MMLU and sees strong scores in Japanese, Korean, and Arabic may conclude that the model is ready for those markets. But the scores reflect the model's ability to answer American questions in those languages, not its ability to serve users in those markets. The gap between benchmark confidence and real-world readiness is the culture-specific question problem in action.

## MMLU-ProX: Better Translation, Same Questions

MMLU-ProX, published at EMNLP 2025, represents the most rigorous attempt to create a high-quality multilingual version of MMLU. The benchmark covers 29 languages with 11,829 questions per language. The translations were produced by multiple large language models and then reviewed by human experts to ensure accurate expression, consistent terminology, and cultural relevance.

The translation quality is genuinely superior to previous MMLU translations. Expert reviewers verified that the translated questions were linguistically accurate, that technical terminology was correctly localized, and that the questions were comprehensible in the target language. The benchmark revealed important findings -- up to 24.3 percent accuracy gaps between the best and worst performing languages for individual models. Even the strongest model tested, Qwen2.5-72B, showed a 30.2 percentage point gap between English at 70.3 percent and Swahili at 40.1 percent.

But MMLU-ProX has a structural limitation that better translation cannot fix: the underlying questions are still the MMLU questions. They are still Western-centric in content. The question about the US Senate is now a beautifully translated question about the US Senate in Swahili -- but it is still testing whether a Swahili-speaking user's model knows about the US Senate. The cultural bias is not a translation quality problem. It is a question design problem. No amount of translation improvement addresses it, because the bias lives in what the questions ask, not in how the questions are phrased.

This is not a criticism of MMLU-ProX. The benchmark is valuable for what it measures: cross-lingual reasoning capability on a standardized question set. If a model can answer a complex biology question in English but cannot answer the same question in Thai, that is a real capability gap worth knowing about. MMLU-ProX measures this gap well. What it does not measure is whether the model has the knowledge and cultural competence that Thai users actually need.

## INCLUDE: What Native-Language Evaluation Looks Like at Scale

The INCLUDE benchmark, published as a spotlight paper at ICLR 2025, takes a fundamentally different approach. Instead of translating English questions, INCLUDE collected existing examination questions from 52 countries in their original languages. The dataset contains 197,243 questions across 44 languages and 15 scripts, drawn from 1,926 real examinations -- national university entrance exams, professional certification tests, secondary school finals, and government service examinations.

The difference is profound. INCLUDE's Hindi questions are questions that Indian students actually answer on Indian exams. They test knowledge of Indian history, Indian geography, Indian political systems, and Indian cultural contexts. The Arabic questions come from Arabic-language examinations in Middle Eastern and North African countries. The Japanese questions come from Japanese university entrance exams and professional certifications. Each language's questions reflect the educational and cultural framework of the communities where that language is spoken.

This is what native-language evaluation looks like at scale. The questions were not conceived by English speakers and then adapted. They were conceived by educators and test designers who think in those languages and understand what knowledge matters in those contexts. A Japanese student does not need to know about the US Senate to be well-educated. They need to know about the National Diet. INCLUDE tests for the knowledge that actually matters to the people who speak each language.

The benchmark also revealed uncomfortable truths about model capabilities. Even reportedly multilingual models showed dramatic performance variation across INCLUDE's languages. Models that scored impressively on translated MMLU variants performed significantly worse on INCLUDE's native-language questions in the same languages, because INCLUDE tests culturally grounded knowledge that translated benchmarks cannot reach.

## The Propagation Problem

Cultural bias in benchmarks does not stay in benchmarks. It propagates through the entire AI development pipeline.

Research labs use MMLU scores to claim their models are "state of the art in multilingual understanding." But that claim is conditional on a definition of understanding that privileges Western cultural knowledge. A model that scores 85 percent on MMLU in Japanese "understands" Japanese in the narrow sense that it can process Japanese text and answer Western questions. It may or may not understand Japanese in the sense that a Japanese user would recognize -- knowing about Japanese institutions, Japanese social conventions, Japanese business practices, and Japanese cultural references.

Product teams use MMLU-derived scores to set quality bars. "Our model must score at least 80 percent on MMLU in every supported language before launch." This bar rewards models that are good at Western knowledge across languages and does nothing to ensure models are good at local knowledge in any language. A model could clear the 80 percent bar in Arabic while being unable to answer basic questions about Sharia-compliant finance, Arabic poetry traditions, or the administrative structure of Gulf Cooperation Council states.

Marketing teams use benchmark scores to claim multilingual capability. "Our model supports 40 languages" sounds authoritative when accompanied by a chart showing high benchmark scores across those languages. But if the benchmark is culturally Western, "supports 40 languages" means "can discuss Western topics in 40 languages" -- which is not the same as being genuinely useful to speakers of those 40 languages in their own cultural contexts.

The propagation problem is why the culture-specific question problem matters for practitioners, not just researchers. Every decision in the chain -- model selection, quality gates, launch criteria, marketing claims -- is built on benchmark numbers. If those numbers are measuring the wrong thing, every downstream decision inherits the distortion.

## What This Means for Your Evaluation Strategy

The culture-specific question problem does not mean you should stop using benchmarks. It means you should stop using them as your only measure of multilingual quality.

For model selection, use MMLU-ProX and similar translated benchmarks as a first filter. They effectively measure cross-lingual reasoning capability -- whether the model can think in the target language at all. A model that scores poorly on MMLU-ProX in Korean probably has fundamental capability gaps in Korean that no amount of prompt engineering will fix. But a model that scores well on MMLU-ProX in Korean has only cleared the first bar. It can reason in Korean. Whether it can serve Korean users well is a different question that MMLU-ProX cannot answer.

For production quality measurement, supplement public benchmarks with native-language eval sets designed for your specific use case, market, and user population. These do not need to be massive. Fifty native-language cases per language, designed by native speakers with domain expertise, will tell you more about your model's real-world quality than a thousand translated MMLU questions.

For cultural competence assessment, use INCLUDE as a reference for what native-language evaluation should look like. You do not need to use INCLUDE directly unless you are building a general-knowledge product. But the methodology -- sourcing questions from local examinations, validating with native speakers, ensuring cultural grounding -- is the methodology your own eval development should follow.

For benchmark reporting, always separate results. Report MMLU-ProX scores for cross-lingual capability. Report native eval scores for production quality. Report INCLUDE-style scores for cultural competence. Never collapse these into a single number, because each measures a different dimension of multilingual quality. A single number hides exactly the gaps you need to see.

## The Deeper Lesson

The culture-specific question problem is a specific instance of a general principle: **evaluation is never neutral.** Every evaluation encodes assumptions about what matters, what "correct" looks like, and whose perspective counts. English-centric benchmarks encode the assumption that Western knowledge is universal knowledge. This assumption is invisible to teams that share the Western cultural frame and glaringly obvious to teams that do not.

Building genuinely multilingual AI systems requires confronting this assumption at the evaluation layer. Not with political arguments about representation, but with the pragmatic recognition that a benchmark which does not test what your users need does not tell you what you need to know. If your users are in Jakarta, your evaluation must reflect what a user in Jakarta considers a correct and helpful response. If your users are in Lagos, your evaluation must reflect Lagos. If your users are everywhere, your evaluation must be as diverse as your user base -- and no single translated benchmark, no matter how well translated, achieves that diversity.

The next subchapter surveys the multilingual benchmarks that have been purpose-built to address these gaps -- MMLU-ProX, BenchMAX, INCLUDE, and others -- and shows you how to combine them into a benchmark strategy that actually measures what matters across your supported languages.