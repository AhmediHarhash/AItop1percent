# 3.7 — Language-Optimized Tokenizers: Qwen, Gemma, and Custom BPE Training

The team had done everything by the book. Clean training data, solid evaluation suite, a well-tuned RAG pipeline, and a mid-tier model that hit their quality bar for English. Then they measured costs for their Korean market. Every Korean customer support query cost 2.4 times more tokens than the equivalent English query. At 800,000 Korean queries per month, the token tax added over $40,000 annually to their API bill — for one language, before accounting for the quality degradation that came with the fragmentation. The fix was not prompt engineering, not retrieval optimization, not conversation redesign. The fix was switching to a model whose tokenizer allocated more vocabulary space to Korean characters. After the switch, Korean fertility dropped from 2.4 to 1.6 times English. Costs fell by a third. Morphological error rates in Korean output dropped measurably. The team changed nothing about their system except the model that processed the text, and the improvement cascaded through every metric they tracked.

This is what language-optimized tokenizers do. They solve problems at the infrastructure layer that no amount of application-layer engineering can reach. The tokenizer is the floor. If the floor is cracked, everything above it wobbles.

## Qwen: The CJK Champion

Qwen's tokenizer is the benchmark for CJK efficiency in 2026. Built by Alibaba's Qwen team, it uses byte-level Byte Pair Encoding with a vocabulary of 151,669 tokens — 151,643 regular tokens and 208 control tokens. The design reflects a deliberate strategic decision: allocate vocabulary space generously to the languages that Alibaba's primary market uses most.

The Chinese coverage is unmatched. Every single one of the 8,105 characters in China's List of Commonly Used Standard Chinese Characters has its own dedicated token in the vocabulary. That means the most common Chinese characters never fragment. They are always a single token, always processed as a complete semantic unit. Beyond individual characters, the vocabulary contains over 25,000 tokens that begin with a CJK Unified Ideograph — multi-character tokens that represent common Chinese words and word fragments. When you tokenize a sentence of modern Chinese with Qwen's tokenizer, common words frequently compress into a single token. The result is that Chinese text achieves a remarkably low fertility of roughly 1.5 to 1.8 characters per token for everyday content.

For Japanese, the benefits are substantial but more complex. Kanji characters share vocabulary space with Chinese characters, so the 25,000-plus CJK tokens cover a large portion of Japanese kanji usage. But Japanese also relies heavily on hiragana for grammatical particles and verb conjugations, and katakana for loan words and emphasis. Qwen's vocabulary includes dedicated hiragana and katakana tokens, but the coverage is thinner than for kanji. Mixed-script Japanese sentences — which is to say, essentially all Japanese sentences — achieve fertility ratios around 1.8 to 2.2 times English on Qwen's tokenizer. This is meaningfully better than the 2.5 to 3.5 range that English-centric tokenizers produce for the same text.

Korean benefits from the CJK vocabulary overlap to a degree. Common Hangul syllable blocks have dedicated tokens, and high-frequency Korean words appear as multi-character tokens. Korean fertility on Qwen sits around 1.7 to 2.0 times English — roughly 20 to 30% better than on OpenAI's o200k_base tokenizer.

The trade-off is that Qwen's vocabulary investment in CJK comes at the expense of other scripts. Arabic, Hindi, Thai, and other non-CJK languages receive coverage comparable to, or in some cases slightly worse than, competing tokenizers with smaller vocabularies. If your primary non-English languages are not in the CJK family, Qwen's tokenizer advantage largely disappears.

## Gemma 3: The Broadest Vocabulary

Where Qwen optimizes deeply for a specific language family, Gemma 3 optimizes broadly across the world's languages. Google's Gemma 3 uses a SentencePiece-based tokenizer with approximately 262,000 vocabulary tokens — the largest vocabulary of any major open-weight model in 2026. This is the same tokenizer family that powers Gemini, giving it the benefit of Google's vast multilingual data from Search, Translate, and YouTube.

The strategy behind a 262,000-token vocabulary is straightforward: a larger vocabulary means more scripts can have dedicated tokens instead of falling back to byte-level encoding. Arabic characters, Devanagari conjuncts, Thai consonant clusters, Korean syllable blocks, Cyrillic combinations — all of these get more vocabulary slots than they would in a 100,000 or 130,000-token vocabulary. The result is that Gemma 3 achieves competitive or best-in-class fertility for a wider range of languages than any other single tokenizer.

The CJK improvements are explicitly documented. Google redesigned the tokenizer's vocabulary to significantly improve Chinese, Japanese, and Korean encoding efficiency, accepting a slight increase in token counts for English and code. That is a revealing trade-off. English and code get marginally worse so that CJK gets meaningfully better. For a team building an English-only product, this is a minor penalty. For a team building a multilingual product, it is a major advantage.

For Arabic, Gemma 3's vocabulary includes substantially more Arabic script tokens than most competitors. Arabic fertility on Gemma 3 sits around 2.0 to 2.5 times English — better than the 2.5 to 3.5 range common on English-centric tokenizers. For Hindi and other Devanagari languages, fertility drops to approximately 1.8 to 2.5, a meaningful improvement over models with smaller vocabularies. For Thai, which suffers some of the worst fertility on English-centric tokenizers, Gemma 3 achieves ratios around 2.5 to 3.0 — still high in absolute terms, but 15 to 25% better than the 3.0 to 4.0 range on smaller-vocabulary alternatives.

The trade-off with a 262,000-token vocabulary is resource cost. The embedding table — the matrix that maps each vocabulary entry to its dense vector representation — scales linearly with vocabulary size. A 262,000-token embedding table is roughly 2.6 times larger than a 100,000-token table, which increases memory usage during inference. For large models where the embedding table is a small fraction of total parameters, this overhead is negligible. For smaller models — particularly models under 3 billion parameters — the embedding table can become a significant fraction of the total model size, increasing load times and memory footprint. If you are deploying small models on edge devices or in memory-constrained environments, the 262,000-token vocabulary may impose a practical penalty.

## Llama 4: Expanded Multilingual in the Tiktoken Family

Meta's Llama 4 uses a tiktoken-based tokenizer with 202,048 vocabulary tokens, a substantial expansion from Llama 3's 128,256. The expansion was driven by Meta's investment in multilingual pretraining — Llama 4 was trained on approximately 40 trillion tokens of data encompassing 200 languages, with fine-tuning support for twelve priority languages: Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.

The twelve priority languages receive the most vocabulary investment. Arabic tokens expanded significantly, bringing Arabic fertility down to roughly 2.0 to 2.8 times English — an improvement of 15 to 25% over Llama 3. Hindi improved similarly, as did Thai, Vietnamese, and Indonesian. For the twelve supported languages, Llama 4's tokenizer is competitive with Gemma 3 and meaningfully better than OpenAI's o200k_base for non-Latin scripts.

For CJK languages, Llama 4's improvements are moderate. Chinese fertility on Llama 4 sits around 1.8 to 2.2 — better than Llama 3 but still behind Qwen's dedicated CJK optimization. Korean and Japanese follow similar patterns, improving over the previous generation but not reaching Qwen-level efficiency.

For languages outside the twelve priority list — the long tail of the world's languages — Llama 4's tokenizer offers incremental improvement from the larger vocabulary but not the transformative gains that the priority languages enjoy. If your target language is Swahili, Burmese, or Amharic, Llama 4's tokenizer is better than Llama 3's but not dramatically so.

## The Vocabulary Size Trade-Off

The trend toward larger vocabularies is clear. From GPT-4's 100,000 to OpenAI's 200,000 to Llama 4's 202,000 to Gemma 3's 262,000, vocabulary sizes have grown because the benefit — better multilingual efficiency — is obvious and the cost — larger embedding tables — is manageable at scale.

But larger vocabulary is not free. The embedding matrix that maps token IDs to dense vectors must store one vector per vocabulary entry. For a 4,096-dimensional embedding, a 100,000-token vocabulary requires approximately 400 million parameters in the embedding layer alone. A 262,000-token vocabulary requires over a billion. In a 70-billion-parameter model, this is less than 2% of total parameters and does not meaningfully affect inference speed. In a 1-billion-parameter model, the embedding layer could approach 50% of total parameters, fundamentally changing the model's architecture and memory profile.

The trade-off also affects tokenizer speed. A larger vocabulary means more potential merge operations during encoding. The impact on encoding latency is modest for single requests but measurable at scale. In benchmarks from mid-2025, a 262,000-token vocabulary added roughly 5 to 10% encoding latency compared to a 100,000-token vocabulary for the same text. For applications processing millions of requests per day, this additional latency accumulates.

The decision framework: if your model has more than 7 billion parameters, the embedding overhead of a large vocabulary is negligible and you should prefer the tokenizer with the best fertility for your target languages. If your model is under 3 billion parameters, weigh the embedding overhead against the fertility benefit. If you are deploying on edge devices with tight memory budgets, the embedding size may be the binding constraint.

## Custom BPE Training: When You Build Your Own Tokenizer

For most teams, selecting the best available tokenizer is the right strategy. Custom tokenizer training is a different category of effort entirely, appropriate only in specific circumstances.

You should consider training a custom tokenizer when all three of the following conditions are true. First, your target language has fertility above 4.0 on every available tokenizer — meaning no existing vocabulary provides adequate coverage. Second, you are training a model from scratch or doing significant continued pretraining — because changing the tokenizer requires retraining or substantially adapting the model's embeddings and prediction head. Third, you have a large, representative corpus in your target language — at least hundreds of millions of tokens, ideally billions — to train the tokenizer's vocabulary from.

If these conditions hold, the process is straightforward in concept and demanding in execution. You assemble a multilingual corpus that reflects your target language distribution. If 40% of your traffic is English, 30% is Hindi, and 30% is Amharic, your tokenizer training corpus should approximate that distribution. You then run the BPE algorithm on this corpus to learn merge rules. The resulting vocabulary will naturally allocate more entries to the scripts that dominate your corpus.

The tooling supports this. Hugging Face tokenizers provides a BPE trainer that accepts a corpus and a target vocabulary size, then produces a trained tokenizer you can use directly with transformer models. SentencePiece provides similar functionality with the added benefit of treating raw character streams without pre-tokenization, which is better for languages without word boundaries.

The execution challenges are significant. Training a tokenizer on a billion-token corpus takes hours to days, depending on vocabulary size and hardware. More importantly, a custom tokenizer is only useful if the model was trained with it. You cannot swap a tokenizer on a pretrained model and expect it to work. The model's embedding layer, its internal representations, and its output prediction head are all tied to the specific vocabulary it was trained with. Changing the vocabulary after training produces garbage output unless you retrain or adapt the model.

This means custom tokenizer training is primarily relevant for organizations training their own foundation models, fine-tuning open-weight models with continued pretraining, or building domain-specific models from scratch. For teams using API-based models, the tokenizer is fixed by the provider. For teams deploying pretrained open-weight models without modification, the tokenizer is fixed by the model release. Custom tokenizer training is a specialized tool for specialized circumstances.

## Tokenizer Adaptation: A Middle Path

A recent line of research from late 2025 and early 2026 explores a middle ground between accepting a pretrained tokenizer and training one from scratch. **Tokenizer adaptation** applies the BPE merge algorithm to your target domain's text, starting from the existing tokenizer's vocabulary, to learn additional merge rules specific to your language or domain. The new merges are added to the vocabulary, and the model's embedding layer is expanded to include the new tokens.

The advantage is that the base vocabulary remains intact — all of the model's existing knowledge is preserved — while the new entries compress your specific target language more efficiently. Early results show that tokenizer adaptation can reduce fertility for underserved languages by 10 to 20% without full retraining, at the cost of fine-tuning the expanded embedding layer on a modest amount of data.

This is not a production-proven technique in 2026. It is an active research area with promising results and unresolved questions about long-term stability. But for teams that have a specific underserved language and cannot wait for model providers to expand their vocabularies, tokenizer adaptation offers a path that is less expensive than training from scratch and more effective than accepting the status quo.

## SentencePiece vs Tiktoken vs Hugging Face Tokenizers: Choosing a Tooling Stack

The three major tokenizer implementations serve different purposes, and your choice affects both evaluation workflows and production deployment.

**SentencePiece** is the best choice for multilingual applications that include languages without clear word boundaries. It processes raw Unicode text as a character stream, which means it does not impose English-centric pre-tokenization rules that assume spaces between words. This matters for Thai, Chinese, Japanese, Khmer, Lao, and Myanmar, where inserting a pre-tokenization step based on spaces produces incorrect segmentation. SentencePiece supports both BPE and Unigram tokenization algorithms and is the native tokenizer for Google's Gemma and Gemini families. Its encoding speed is moderate — roughly 0.7 times the speed of tiktoken in head-to-head benchmarks — but adequate for most production workloads.

**tiktoken** is the fastest general-purpose tokenizer, achieving three to six times the throughput of alternatives in standard benchmarks. It is written in Rust with Python bindings. The speed advantage matters at scale: if you process tens of millions of requests per day, the encoding and decoding overhead of a slower tokenizer adds up. tiktoken is inference-only — it does not support training new tokenizers — so it is not suitable for custom vocabulary work. It is the native format for OpenAI models and has been adopted by Llama 4 and Mistral. For evaluation and production deployment with these model families, tiktoken is the default choice.

**Hugging Face tokenizers** is the most flexible option. It supports BPE, WordPiece, Unigram, and other algorithms. It can load tokenizers from any Hugging Face model repository, which makes cross-model evaluation seamless. It supports training new tokenizers, which makes it the right choice if you are exploring custom vocabulary. Its Rust backend makes performance competitive with tiktoken for most workloads, though not quite matching tiktoken's peak throughput. If your workflow involves evaluating multiple models across multiple languages and potentially training custom tokenizers, the Hugging Face library provides the broadest capability set.

For most multilingual teams in 2026, the practical approach is to use Hugging Face tokenizers for evaluation and comparison, tiktoken for production with OpenAI and Llama deployments, and SentencePiece for production with Gemma and Gemini deployments. The libraries are not mutually exclusive. You can evaluate with one and deploy with another.

## When to Choose a Model Based on Its Tokenizer

Most model selection decisions weight reasoning quality, speed, price, and context window length. For multilingual products, tokenizer efficiency should carry equal weight.

If your primary non-English languages are Chinese, Japanese, or Korean, Qwen's tokenizer gives you a 20 to 40% cost advantage over GPT-5's tokenizer for those languages. At scale — millions of CJK requests per month — that cost advantage is worth tens of thousands of dollars annually. The quality advantage from lower fragmentation is a bonus on top of the cost savings.

If your product serves a linguistically diverse user base across many scripts — Arabic, Hindi, Thai, CJK, European languages — Gemma 3's 262,000-token vocabulary provides the most equitable distribution of tokenizer efficiency. No single language wins as dramatically as Chinese wins on Qwen, but no language loses as badly either. The broad coverage minimizes the worst-case fertility across your language mix.

If your priority languages fall within Meta's twelve supported languages for Llama 4, and you need the flexibility of an open-weight model that you can fine-tune and deploy on your own infrastructure, Llama 4's tokenizer offers strong coverage for those specific languages with the operational advantages of the Llama ecosystem.

The decision is not always about which tokenizer is best in isolation. It is about which tokenizer is best for your specific language portfolio, at your specific scale, within your specific infrastructure constraints. The fertility table from the previous subchapter is the tool that makes this decision data-driven instead of instinctive.

## The Future Is Larger, More Balanced Vocabularies

The direction of the industry is clear. Every major model release in 2025 and early 2026 expanded its vocabulary relative to its predecessor. The 100,000-token vocabularies that defined the GPT-4 era are giving way to 200,000 and 260,000-token vocabularies that distribute coverage more equitably across the world's scripts. Techniques like LiteToken — which recovers wasted vocabulary space from intermediate merge residues and reallocates it to underserved scripts — offer further gains without increasing vocabulary size.

The convergence is toward tokenizers that impose a fertility ratio of 2.0 or less for the top fifty languages and 3.0 or less for the top hundred. That would transform the economics and quality of multilingual AI. But it has not happened yet. In 2026, the gap between English and the world's other languages remains wide enough that tokenizer selection is one of the highest-leverage decisions a multilingual team can make.

The next subchapter moves from how tokens are created to how the raw bytes that make up those tokens can silently corrupt your pipeline. Unicode normalization — the invisible layer between your text and your tokenizer — is where the most insidious multilingual bugs hide.
