# 3.6 — Tokenizer Evaluation: Measuring Fertility and Coverage Across Languages

Before you select a model for multilingual use, you must evaluate its tokenizer. Not its benchmark scores, not its parameter count, not its context window length. Its tokenizer. The tokenizer determines how much every request costs, how much context your prompt can carry, and how well the model handles your target languages at the most fundamental level. A model with state-of-the-art reasoning that fragments your primary language into byte-level confetti will cost you more and perform worse than a slightly less capable model whose tokenizer represents your language efficiently. Tokenizer evaluation is not a bonus step in model selection. It is the first step. Everything that follows — cost projections, context budgets, quality estimates — depends on the numbers you get from this evaluation.

## Token Fertility Ratio: The Core Metric

**Token fertility ratio** is the single most important number you will compute when evaluating a tokenizer for multilingual use. It tells you how many tokens a given text requires relative to an English baseline on the same tokenizer. The ratio is computed by dividing the number of tokens the tokenizer produces for a text sample in your target language by the number of tokens it produces for a semantically equivalent English text.

There are three granularities of fertility measurement, and each one serves a different purpose. **Tokens per word** works for languages that use spaces between words — English, French, German, Turkish, Hindi when written in Devanagari with spaces. You count the words, count the tokens, and divide. English typically scores between 1.0 and 1.3 tokens per word on major tokenizers. German scores 1.2 to 1.4 because of compound nouns. Turkish scores 1.8 to 2.5 because of agglutinative suffixes that inflate word length.

**Tokens per character** is the right metric for languages without word boundaries — Chinese, Japanese, Thai, Khmer, Lao, Myanmar. Since there are no spaces to define word boundaries, measuring tokens per word is meaningless. Instead, you count characters and tokens. Mandarin Chinese on Qwen's tokenizer achieves roughly 0.7 to 1.0 tokens per character for common text, meaning the tokenizer often compresses multiple characters into a single token. The same Chinese text on an English-centric tokenizer might score 1.5 to 2.0 tokens per character.

**Tokens per semantic unit** is the most meaningful metric for cross-language comparison but the hardest to compute. A semantic unit is a concept, roughly equivalent to a word in English. For Chinese, where a single character often conveys a full concept, one character is one semantic unit. For Japanese, where a concept might be expressed by one kanji, two hiragana, or a combination, the semantic unit count requires linguistic judgment. This metric answers the question that matters most for cost and quality: how many tokens does it cost to express the same idea in different languages? Teams that invest in computing semantic fertility get the most accurate cost projections and the most honest quality predictions.

## Coverage Analysis: What Does the Vocabulary Actually Contain

Fertility tells you how efficiently the tokenizer handles your language. **Coverage analysis** tells you why. Coverage measures what percentage of your target language's common characters and character combinations have dedicated vocabulary entries versus being decomposed into byte-level fallbacks.

To measure coverage, you need access to the tokenizer's vocabulary file. For most models in 2026, this is publicly available. Qwen publishes its vocabulary. Llama 4's vocabulary is accessible through the Hugging Face model repository. Gemma 3's vocabulary is distributed with the model weights. OpenAI's tokenizer vocabulary for o200k_base is available through the tiktoken library. You can inspect any of these programmatically.

The analysis proceeds in layers. First, count the number of vocabulary entries that begin with or consist of characters from your target script. For Qwen's tokenizer, this analysis reveals over 25,000 vocabulary entries that start with a CJK character, including dedicated single-character tokens for all 8,105 characters in China's List of Commonly Used Standard Chinese Characters. That is deep coverage. For comparison, count the Arabic script tokens in the same vocabulary. If you find 3,000 Arabic-starting tokens versus 25,000 CJK-starting tokens, you know immediately that Arabic will tokenize less efficiently on this tokenizer.

Second, check which of your language's most common characters are single tokens versus multi-byte sequences. Take the five hundred most frequent characters in your target language and run each one through the tokenizer individually. If a character maps to a single token, it has dedicated coverage. If it splits into two or three byte-level tokens, it is being handled by fallback encoding. A tokenizer where 95% of your top-500 characters are single tokens will produce dramatically better fertility than one where only 60% are single tokens. The remaining 40% each contribute one or two extra tokens per occurrence, and those extra tokens accumulate across every sentence.

Third, look for multi-character tokens — vocabulary entries that represent common words or word fragments in your target language. These are the high-value entries that compress your text most aggressively. A tokenizer that has dedicated tokens for common Korean syllable combinations, frequent Arabic word prefixes, or standard Thai consonant clusters will achieve much better fertility than one that represents each of these character by character. Count the multi-character tokens per script. More is better.

## The Fertility Test: A Practical Protocol

The previous subchapter encouraged you to build a fertility ratio table. This subchapter gives you the specific protocol for building one that you can trust.

Step one: assemble a representative corpus for each target language. The corpus must reflect the actual text your system will process, not generic benchmark text. If your product handles customer support queries in Japanese, your test corpus should be Japanese customer support queries — with the formality level, technical vocabulary, and sentence structures your users actually produce. If your product processes Arabic legal documents, your corpus should be Arabic legal text. A tokenizer that performs well on Arabic news articles may perform differently on Arabic legal prose, which uses longer morphologically complex words and more formal register.

The corpus should contain at least ten thousand tokens worth of text per language after tokenization. Smaller samples produce unreliable fertility estimates because short texts are sensitive to the specific words that happen to appear. Ten thousand tokens gives you a stable average that you can trust for cost projections.

Step two: obtain the same semantic content in English. This is the critical step that most evaluations skip. You cannot compare raw fertility numbers across languages meaningfully because different languages express the same idea with different numbers of words, entirely apart from tokenization. Japanese uses fewer words than English for many concepts because of the information density of kanji. Arabic uses more words for some formal constructions. To isolate the tokenizer's impact from the language's inherent verbosity, you need parallel corpora — texts that express the same content in each language.

Professional translation of your test corpus into English gives you the cleanest comparison. Alternatively, use established parallel corpora. The United Nations Parallel Corpus, the Europarl corpus for European languages, and the OPUS collection provide professionally translated text across dozens of languages. These are not perfect for domain-specific evaluation, but they are far better than comparing unrelated texts.

Step three: tokenize each language's corpus with every candidate tokenizer. Count the total tokens per language. Divide by the English token count for the same semantic content. The result is your fertility ratio per language per tokenizer.

Step four: build the comparison table. One row per language, one column per tokenizer, fertility ratio as the cell value. This table is the foundation of your model selection, cost projection, and context budget planning for multilingual deployment. It deserves the same rigor you would give to a performance benchmark or a capacity planning exercise.

## Benchmarking Methodology: Avoiding the Common Traps

Three common mistakes corrupt fertility benchmarks and lead to model selections that look good on paper and fail in production.

The first mistake is using generic text instead of domain-specific text. A tokenizer's fertility on Wikipedia text tells you almost nothing about its fertility on your product's actual text. Legal text uses longer words and more specialized terminology than conversational text. Medical text uses Latin and Greek roots that some tokenizers handle better than others. Financial text mixes numbers, currency symbols, and technical terms in patterns that stress tokenizers differently than prose. Always benchmark on text that resembles your production workload.

The second mistake is comparing raw token counts instead of semantic-equivalent token counts. If you tokenize a Japanese paragraph and an English paragraph that happen to be the same number of characters, you are measuring something, but not fertility. You are measuring the combined effect of language verbosity and tokenization efficiency. Only parallel corpora — the same content in both languages — let you isolate the tokenizer's contribution.

The third mistake is testing on a single text register. Most languages have formal and informal registers that tokenize differently. Arabic formal prose, with its complex morphology and long words, achieves higher fertility than Arabic conversational text, with its shorter words and simpler structures. If your product handles both — a customer support bot that processes formal complaints and casual questions — you need fertility measurements for both registers. Use the weighted average based on your expected register distribution.

## Tools for Tokenizer Evaluation

The tooling landscape for tokenizer evaluation in 2026 is mature enough that you do not need to build evaluation infrastructure from scratch.

**tiktoken** is OpenAI's tokenizer library. It is the fastest general-purpose tokenizer implementation, written in Rust with Python bindings. You use it to evaluate any tokenizer in the tiktoken format, which includes OpenAI's own tokenizers and any model family that adopted the tiktoken format — notably Llama 4 and Mistral. tiktoken does not support training custom tokenizers. It is inference-only. But for evaluation, its speed is unmatched: it can tokenize a million-token corpus in seconds, which makes large-scale fertility benchmarking practical.

**Hugging Face tokenizers** is the most flexible evaluation library. It supports BPE, WordPiece, Unigram, and other tokenization algorithms. It loads tokenizers from any Hugging Face model repository, which means you can evaluate every open-weight model's tokenizer through a single interface. It also supports training custom tokenizers, which matters if you are considering training your own vocabulary on domain-specific data. The library is Rust-backed, so performance is competitive with tiktoken for most workloads.

**SentencePiece** is Google's tokenizer library, used by Gemma and Gemini. It treats input as a raw character stream without pre-tokenization, which makes it particularly effective for languages without word boundaries. If you are evaluating Gemma models, you will use SentencePiece directly. It supports both BPE and Unigram algorithms and handles complex scripts — Thai, Khmer, Myanmar — better than libraries that assume space-delimited input.

For batch evaluation across multiple tokenizers, write a script that loads each candidate tokenizer, processes your test corpus in each language, counts tokens, computes fertility ratios, and outputs the comparison table. This script should be part of your model evaluation pipeline — run it every time you consider a new model, and update your fertility table accordingly. The script is typically twenty to forty lines per tokenizer and can be parameterized to run across your full language set in minutes.

## What Good Looks Like: Fertility Benchmarks by Language Tier

Not all languages can achieve the same fertility, and setting the wrong expectations leads to either disappointment or complacency. The right approach is to set fertility benchmarks by language tier, based on the script's structural complexity and its representation in tokenizer training data.

**Tier 1 languages** are those with Latin or Cyrillic scripts and heavy representation in training data: English, Spanish, French, German, Portuguese, Italian, Russian. For these languages, target a fertility ratio below 1.5 times English on your chosen tokenizer. Any major tokenizer in 2026 should achieve this. If it does not, the tokenizer has an unusual Latin-script weakness and you should investigate why.

**Tier 2 languages** are those with dedicated script support but moderate training data: Chinese, Japanese, Korean, Arabic, Hindi, Thai, Vietnamese, Turkish. These languages have scripts that are structurally different from Latin, which imposes a baseline tokenization cost. Target a fertility ratio below 2.5 times English for Tier 2 languages. Achieving this requires choosing a tokenizer with explicit support for these scripts. On a poorly chosen tokenizer, Tier 2 languages can easily exceed 3.0 or 4.0.

**Tier 3 languages** are those with limited script representation in most tokenizer training data: Amharic, Myanmar, Khmer, Lao, Sinhala, Georgian, Tibetan, various Indic scripts beyond Hindi. Target a fertility ratio below 4.0 times English. This is achievable on tokenizers with very large vocabularies like Gemma 3's 262,000-token vocabulary, but may not be achievable on tokenizers with 100,000 to 130,000 tokens. For Tier 3 languages, tokenizer choice is not a minor optimization. It is a viability decision.

**Tier 4 languages** are those with virtually no representation in standard tokenizer training data. For these — many Indigenous languages, minority scripts, historical writing systems — fertility ratios can exceed 6.0 or 8.0 times English. Standard tokenizers are not viable for production use with Tier 4 languages. You need either a custom-trained tokenizer or a specialized model built for these specific languages.

## Red Flags in Tokenizer Evaluation

Beyond fertility ratios, watch for specific failure modes that signal deeper problems.

**Fertility above 5.0 for any target language** is a red flag that the tokenizer has no meaningful support for that language's script. The text is being processed almost entirely at the byte level. Expect high costs, severe context window starvation, and measurable quality degradation.

**Characters splitting to individual bytes** is the strongest signal of missing coverage. If single characters in your target script consistently become three or four tokens, the tokenizer vocabulary contains no dedicated entries for those characters. Every word in that script will fragment heavily. Check this by tokenizing individual common characters and inspecting the token count.

**Diacritical marks separating from base characters** is a red flag for languages that rely on diacritics — Vietnamese, Arabic with tashkeel, French, Turkish. If the tokenizer places a diacritical mark in a different token than the character it modifies, the model is more likely to drop, misplace, or substitute diacritics during generation. Test this by tokenizing words with diacritics and checking whether the mark stays with its base character.

**Inconsistent tokenization of the same word** in different contexts signals unstable BPE merge boundaries. If the same Arabic word produces three tokens at the start of a sentence and four tokens in the middle, the tokenizer's pre-tokenization rules are interacting with context in ways that will produce inconsistent generation quality. Test by tokenizing the same word in multiple sentence positions.

**Extremely long token sequences for common words** — where a word that native speakers consider short and common requires five or more tokens — indicates that the vocabulary simply does not cover that word's character patterns. If common words fragment this badly, uncommon words will fragment catastrophically.

## From Evaluation to Decision

The fertility table, coverage analysis, and red flag checklist give you the data to make an informed tokenizer decision. The decision itself comes down to a weighted evaluation of three factors.

First, cost impact. Multiply the fertility ratio by your expected request volume per language and your API pricing to get the annual cost per language per tokenizer. The tokenizer that minimizes total weighted cost across your language mix wins on the cost dimension.

Second, quality impact. Languages with fertility above 2.5 on a given tokenizer will show measurable quality degradation — more morphological errors, more diacritical mark failures, more inconsistent terminology. If quality parity across languages is a product requirement, the tokenizer that keeps all target languages below 2.5 fertility may be worth the trade-off even if it is not the absolute cheapest for your highest-traffic language.

Third, operational simplicity. Using a single model with a single tokenizer is operationally simpler than routing different languages to different models. The cost and quality savings from multi-model routing must be weighed against the engineering complexity of maintaining multiple model integrations, multiple prompt templates, and multiple evaluation pipelines.

The best tokenizer evaluation does not produce a single winner. It produces a decision matrix: for each candidate model, the fertility ratio, estimated annual cost, expected quality tier, and operational complexity for your specific language mix. Your team reviews this matrix and makes a deliberate choice — not a default one.

The next subchapter examines what happens when the default tokenizer is not good enough and you need to go further: selecting models specifically for their tokenizer design, or in some cases, training your own vocabulary on domain-specific multilingual data.
