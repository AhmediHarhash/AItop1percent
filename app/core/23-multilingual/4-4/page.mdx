# 4.4 â€” Multilingual Benchmarks That Matter: MMLU-ProX, BenchMAX, INCLUDE

Which benchmark should you trust for multilingual model selection? The honest answer is none of them alone, and all of them together. Each major multilingual benchmark measures a different dimension of capability, has different strengths, carries different blind spots, and was designed for a different purpose. Using any single benchmark to evaluate your multilingual model is like using a thermometer to assess overall health -- the reading is real, but it captures one signal out of dozens you need.

This subchapter walks through the benchmarks that matter in 2026, explains what each one actually measures, identifies their limitations, and shows you how to combine them with your own evaluation data into a benchmark strategy that tells you the truth about your model's multilingual quality.

## MMLU-ProX: The Cross-Lingual Reasoning Standard

MMLU-ProX, published at EMNLP 2025, is the current gold standard for measuring whether a model can reason across languages on the same questions. The benchmark extends MMLU-Pro -- an enhanced version of MMLU with harder, reasoning-intensive questions -- to 29 typologically diverse languages. Each language version contains 11,829 identical questions, enabling direct cross-linguistic comparison. A lite version with 658 questions per language is available for faster evaluation cycles.

The translation quality distinguishes MMLU-ProX from earlier MMLU translations. Multiple large language models generated candidate translations, and human experts reviewed every question to verify linguistic accuracy, terminological consistency, and comprehensibility. The benchmark is not perfect -- no translated benchmark can be -- but it represents the most careful multilingual adaptation of MMLU to date.

What MMLU-ProX reveals is sobering. The benchmark evaluated 36 state-of-the-art models and documented massive cross-lingual performance gaps. Even the strongest model, Qwen2.5-72B, scored 70.3 percent in English and 40.1 percent in Swahili -- a 30.2 percentage point gap. Smaller models showed even wider disparities: DeepSeek-R1-7B demonstrated a 35.2 point difference between its best and worst languages. Across models, the gap between highest and lowest performing languages reached up to 24.3 percentage points.

The pattern is consistent: European languages cluster near English performance. Asian languages sit in the middle. African languages -- Swahili, Yoruba, Amharic -- trail significantly. This ranking correlates directly with the volume of training data available for each language, confirming that training data distribution, not model architecture, is the primary driver of cross-lingual capability gaps.

**When to use MMLU-ProX.** Use it as your first filter during model selection. If you are choosing between three candidate models for a product that serves Turkish users, run MMLU-ProX in Turkish for all three. The model that scores highest in Turkish is the one with the strongest reasoning foundation in Turkish. This does not guarantee it will serve your Turkish users well -- that depends on domain knowledge, cultural competence, and task-specific quality that MMLU-ProX does not measure -- but it guarantees the reasoning infrastructure is there.

**What MMLU-ProX does not tell you.** It does not tell you whether the model understands Turkish culture, Turkish business practices, or Turkish regulatory frameworks. As subchapter 4.3 explained, the underlying questions are Western-centric. A high MMLU-ProX score in Turkish means the model can reason in Turkish about Western academic topics. Whether it can reason in Turkish about Turkish topics is a different question.

## BenchMAX: The Parity Benchmark

BenchMAX, published in the Findings of EMNLP 2025, asks a different question than MMLU-ProX. Where MMLU-ProX measures absolute performance per language, BenchMAX measures whether a model performs equally well across languages. The distinction matters enormously.

The benchmark covers 17 languages: English, Spanish, French, German, Russian, Bengali, Japanese, Thai, Swahili, Chinese, Telugu, Arabic, Korean, Serbian, Czech, Hungarian, and Vietnamese. It evaluates ten diverse tasks spanning six core capabilities: instruction following, reasoning, long-context understanding, code generation, knowledge, and science reasoning. Each evaluation sample was machine-translated from English and then post-edited by three native annotators, providing higher-quality multilingual test cases than raw machine translation.

BenchMAX's signature contribution is the **GAP metric** -- Generation Accuracy Parity. Instead of reporting raw accuracy per language, GAP quantifies the delta between English performance and each other language. A model with perfect parity would show a GAP of zero across all languages: every language performs as well as English. In practice, no model achieves this. GAP scores reveal which models maintain the most consistent quality across their supported languages and which models have the widest chasms between their English capabilities and everything else.

The most important finding from BenchMAX is one that should shape how you think about multilingual strategy: **scaling model size does not close the parity gap.** Larger models perform better in every language, but the gap between English and other languages persists. A 70-billion parameter model scores higher than a 7-billion parameter model in both English and Bengali, but the gap between its English score and its Bengali score is roughly the same. Simply using a bigger model does not solve the multilingual quality problem. It raises the floor, but the relative disparity remains.

This finding has direct implications for architecture decisions. If you are counting on the next generation of larger models to automatically fix your multilingual quality issues, BenchMAX says that expectation is wrong. Closing the parity gap requires targeted interventions -- multilingual fine-tuning, balanced training data, language-specific optimization -- not just scale.

**When to use BenchMAX.** Use it when you need to understand how consistently a model performs across your supported languages relative to English. If your product serves users in eight languages and you need comparable quality in all of them, BenchMAX's GAP metric tells you which model gives you the most uniform experience. A model with a lower average score but tighter parity might be a better choice than a model with a higher average score but wild variation between languages.

**What BenchMAX does not tell you.** Like MMLU-ProX, BenchMAX's questions originate in English and are translated. The post-editing by native annotators improves quality significantly, but the underlying scenarios remain English-conceived. BenchMAX also covers a specific set of capabilities that may not align with your product's requirements. If your product is primarily a conversational agent, BenchMAX's code generation and long-context understanding tasks are less relevant than its instruction following and reasoning tasks.

## INCLUDE: The Native-Language Gold Standard

INCLUDE, published as a spotlight paper at ICLR 2025, is the benchmark closest to what native-language evaluation should look like. It contains 197,243 questions across 44 languages and 15 scripts, drawn from 1,926 real examinations in 52 countries. The questions were not translated from English. They are actual examination questions from national university entrance exams, professional certification tests, secondary school assessments, and government service examinations in each country.

The scope is ambitious. INCLUDE covers 58 knowledge domains, from STEM subjects to humanities, social sciences, professional fields, and practical knowledge. The questions span difficulty levels from middle school to professional certification. Each language's questions reflect the educational curriculum and cultural knowledge framework of the countries where that language is spoken. Hindi questions test knowledge relevant to Indian education. Arabic questions test knowledge relevant to the educational systems of Arabic-speaking countries. Japanese questions test knowledge from Japanese examinations.

This design makes INCLUDE the only major benchmark that genuinely tests culturally grounded knowledge across languages. When a model scores well on INCLUDE's Hindi questions, it demonstrates knowledge that Hindi-speaking users would consider real-world relevant -- not just the ability to process Hindi text about Western topics.

INCLUDE's evaluation revealed that even models claiming strong multilingual support showed dramatic performance variation across its languages. The best-performing model at the time of publication, GPT-4o, achieved approximately 77 percent accuracy averaged across all domains and languages. But that average obscured wide variation -- strong performance on high-resource languages with extensive training data and notably weaker performance on lower-resource languages, particularly those with non-Latin scripts.

**When to use INCLUDE.** Use it when you need to assess a model's cultural competence across languages. If your product requires the model to have real-world knowledge relevant to users in specific countries -- local regulations, local educational content, local professional knowledge -- INCLUDE is the benchmark that tests for this. It is also useful as a reference design for your own native-language eval sets: the methodology of sourcing from local examinations and validating with native speakers is a template you can follow at smaller scale.

**What INCLUDE does not tell you.** INCLUDE tests knowledge, not generation quality. It uses multiple-choice questions, which means it measures whether the model can identify the correct answer, not whether it can generate fluent, well-structured, culturally appropriate text. A model that scores well on INCLUDE might still produce awkward phrasing, incorrect formality levels, or unnatural sentence structures when generating open-ended responses. INCLUDE also does not cover every language equally -- some languages have thousands of questions while others have hundreds, reflecting the availability of source examinations.

## Global MMLU: The Bias-Aware Variant

Global MMLU, released by Cohere's research team and published at ACL 2025, takes a different approach to the cultural bias problem. Instead of replacing the questions, it annotates them. The benchmark covers 42 languages with professional and community annotators who classified questions as culturally sensitive or culturally agnostic and verified translation quality for each language version.

The value of Global MMLU is its transparency. By tagging which questions contain cultural bias, it allows you to evaluate models on the culturally agnostic subset and get a cleaner signal about cross-lingual capability without cultural confounds. You can also evaluate on the culturally sensitive subset specifically to see how well a model handles Western cultural knowledge -- a valid capability for some use cases.

Global MMLU's key finding: model rankings change depending on which subset you evaluate. A model that ranks first on the full benchmark might rank fifth on the culturally sensitive subset, or vice versa. This confirms that a single headline score conflates two different capabilities -- language proficiency and cultural knowledge -- that vary independently across models.

**When to use Global MMLU.** Use it when you want to separate language capability from cultural knowledge in your model evaluation. The culturally agnostic subset gives you a cleaner measure of whether the model can reason in the target language. The culturally sensitive subset tells you how much Western cultural knowledge the model has absorbed. Both signals are useful. Conflating them is not.

## CS-FLEURS: The Code-Switching Benchmark

CS-FLEURS, presented at Interspeech 2025, addresses a capability that the text-focused benchmarks above ignore entirely: code-switching. The benchmark contains 52 languages and 113 unique code-switched language pairs, testing whether models can handle the fluid mixing of languages that billions of multilingual speakers use every day.

The dataset includes multiple test sets: real voice recordings of code-switched speech between 14 languages and English, synthetically generated code-switched text-to-speech across 16 language pairs, and expanded sets covering Arabic, Mandarin, Hindi, and Spanish mixed with dozens of other languages. The benchmark revealed that even strong multilingual speech models like Whisper struggle significantly with code-switched input, particularly when the switched languages use different scripts.

CS-FLEURS is primarily a speech benchmark, but its implications extend to text-based evaluation. If your users code-switch in text -- typing a message that mixes Hindi and English, or Spanish and English, or Arabic and French -- you need to evaluate whether your model handles this gracefully. Chapter 10 of this section covers code-switching evaluation in detail, but CS-FLEURS provides the reference benchmark for understanding how far current models are from handling code-switched input reliably.

## Vision-Language Multilingual Benchmarks

If your product includes vision-language capabilities -- image captioning, visual question answering, multimodal reasoning -- you face an additional evaluation challenge. Most vision-language benchmarks are English-centric, and extending them to other languages introduces the same cultural bias problems that affect text benchmarks.

MaRVL, the Multicultural Reasoning over Vision and Language benchmark, was specifically designed to address this. Its images and captions were collected by native speakers across five languages -- Indonesian, Chinese, Swahili, Tamil, and Turkish -- and reflect visual concepts that are culturally relevant to each language community. An image of a traditional market in Indonesia tests different visual-linguistic associations than an image of an American supermarket, and MaRVL captures these differences.

More recent benchmarks, including MVL-SIB and MerCulture presented at major venues in 2025, have expanded multilingual vision-language evaluation to additional languages and cultural contexts. These benchmarks consistently find that vision-language models show even larger cross-lingual performance gaps than text-only models, because visual-linguistic alignment is harder to transfer across languages than text-only reasoning.

If your product involves multimodal capabilities across languages, you need dedicated multimodal multilingual evaluation. Text benchmarks will not catch failures in image understanding, visual question answering, or multimodal reasoning that occur only in non-English languages.

## How to Combine Benchmarks into a Strategy

No single benchmark gives you the complete picture. The strategy is layered.

**Layer one: cross-lingual capability screening.** Use MMLU-ProX to screen candidate models for basic reasoning capability in your target languages. This is a pass-fail filter. If a model cannot reason at a minimum threshold in a language, that language is not ready for production regardless of other qualities. MMLU-ProX's 29-language coverage and standardized question set make it ideal for this screening step.

**Layer two: parity assessment.** Use BenchMAX to evaluate how consistently the surviving candidates perform across your language set. A model that scores well in six of your eight languages but poorly in two may be a worse choice than a model that scores slightly lower overall but maintains tighter parity. Your users in the weakest languages will have the worst experience, and BenchMAX's GAP metric quantifies exactly how bad that experience gap is.

**Layer three: cultural competence.** Use INCLUDE to assess whether the model has real-world knowledge relevant to users in your target markets. This layer is especially important if your product requires domain knowledge that varies by country -- legal information, educational content, local business practices. INCLUDE's native-language questions test for this knowledge in a way that translated benchmarks cannot.

**Layer four: task-specific native evaluation.** Supplement public benchmarks with your own native-language eval sets designed for your specific product, domain, and user population. Public benchmarks tell you about general capability. Your private eval sets tell you about the specific quality dimensions that matter for your product in each language. This is the layer that catches the failures your users will actually encounter.

**Layer five: specialized evaluation.** Add CS-FLEURS for code-switching, MaRVL or MVL-SIB for vision-language, and any domain-specific benchmarks relevant to your product. These cover capability dimensions that general-purpose benchmarks miss.

The layers are not equally weighted. For most products, layer four -- your own native-language eval sets -- carries the most weight in production quality decisions. Public benchmarks inform model selection and architecture decisions. Private eval sets inform shipping decisions and quality gates. A model might clear every public benchmark with strong scores and still fail your private native eval because the benchmarks do not test the specific capabilities your users need.

## Building Your Benchmark Tracking System

Once you have selected your benchmark strategy, you need a system to track results consistently across evaluation cycles.

Track every benchmark score per language, per model version, per evaluation date. Store the raw scores, not just the aggregates. When a model update drops your MMLU-ProX score in Arabic by four points, you need to drill into which question categories drove the regression. When BenchMAX shows a widening GAP in Thai, you need to see whether the problem is in reasoning, instruction following, or another specific capability.

Maintain a benchmark comparison table that shows each candidate model's scores across all layers. This table becomes the primary artifact for model selection decisions. Include the public benchmark scores, your private native eval scores, and the parity metrics side by side. Decision-makers should never see a single number. They should see the full matrix of scores across languages and evaluation dimensions.

Set alerts for regressions. When a model update causes any language to drop below your minimum quality threshold on any benchmark layer, the update should be flagged before it reaches production. The alert threshold should be per-language, not averaged -- a two-point drop averaged across twelve languages might hide a fifteen-point drop in one language that serves a million users.

Review your benchmark strategy quarterly. New benchmarks are published regularly. Existing benchmarks are updated. Your product's language priorities change as you enter new markets. The benchmark strategy that was right when you launched with five languages may need adjustment when you expand to fifteen. Add new benchmarks when they cover capabilities your current suite misses. Remove benchmarks that no longer align with your product's requirements. The strategy should evolve with your product.

## The Benchmark is Not the Goal

Every benchmark in this subchapter measures something real and something limited. MMLU-ProX measures cross-lingual reasoning but not cultural knowledge. BenchMAX measures parity but starts from English-centric questions. INCLUDE measures cultural knowledge but only through multiple-choice questions. Global MMLU separates cultural bias but does not eliminate it. CS-FLEURS measures code-switching but only in speech.

The common mistake is treating benchmark scores as the goal rather than as signals. The goal is a product that serves every user well in their language. Benchmarks are instruments that provide partial visibility into whether you are achieving that goal. They are necessary because you cannot evaluate every dimension of every language manually. They are insufficient because no benchmark captures the full complexity of what "quality" means to a real user in a real context.

Use benchmarks to narrow the field, identify problems, and track trends. Use native-language evaluation to make shipping decisions. Use user feedback to validate both. The team that treats benchmarks as the final word on multilingual quality will be surprised by user complaints in the languages that benchmarks measured poorly. The team that uses benchmarks as one input among several will be prepared.

The next subchapter examines what happens when you try to automate multilingual evaluation using LLM-as-judge -- the biases that emerge, the calibration challenges, and the failure modes that make cross-lingual automated evaluation one of the hardest unsolved problems in multilingual AI.