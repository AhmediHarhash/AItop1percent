# 6.4 â€” Multilingual Chunking Strategies: Why English Chunking Rules Break for CJK

The standard chunking advice -- split on paragraphs, aim for 512 tokens, overlap by 20 percent -- assumes English. It assumes words are separated by spaces. It assumes sentence boundaries are unambiguous. It assumes that a fixed token count maps to a roughly consistent amount of semantic content. Apply these assumptions to Chinese, Japanese, Korean, Thai, or Arabic, and your retrieval quality drops by 15 to 30 percent before a single query ever reaches the embedding model. The degradation is silent. Your chunking pipeline runs without errors. Your embedding model produces vectors without complaints. Your index stores the chunks. But the chunks themselves are wrong -- split at meaningless boundaries, containing fractured ideas, sized in ways that waste the embedding model's capacity or overwhelm it. Every downstream component pays the price for a chunking pipeline that was designed for English and deployed globally.

## The English Chunking Assumption

Most chunking pipelines in production RAG systems were built for English text, and the assumptions baked into them are so natural to English speakers that they feel universal. They are not.

**Assumption one: words are separated by spaces.** English delimits words with whitespace. You split on spaces to count words, to identify boundaries, and to tokenize. This is true for most Latin-script languages, for Cyrillic-script languages, and for Arabic (which uses spaces between words despite its connected script). It is false for Chinese, Japanese, and Thai, where words are written without spaces. A Chinese sentence is a continuous stream of characters with no visual or structural delimiter between words. A Thai sentence looks the same: characters flow together without breaks. You cannot split on spaces because spaces do not exist.

**Assumption two: sentences end with periods.** In English, a period followed by a space and a capital letter signals a sentence boundary. Sentence-level chunking -- splitting documents at sentence boundaries -- is one of the most common strategies. But Chinese uses a different character for its full stop. Japanese uses yet another. Thai frequently omits sentence-ending punctuation in informal text. Arabic uses a mirrored comma. A chunking pipeline that looks for the English period character will fail to detect sentence boundaries in these scripts, producing chunks that split mid-sentence or combine multiple sentences in unpredictable ways.

**Assumption three: paragraphs are meaningful semantic units.** English-language documents typically organize ideas into paragraphs, each covering a coherent topic. Splitting on double newlines captures paragraph boundaries and produces semantically coherent chunks. But document formatting varies by language and culture. Japanese business documents often use different structural conventions than English ones. Chinese technical writing may use numbered sections without clear paragraph breaks. Arabic academic text follows different organizational conventions. A paragraph-based chunking strategy that works well for English technical documentation may produce incoherent chunks for documents in other languages.

**Assumption four: a fixed token count maps to consistent semantic content.** A 512-token chunk of English text contains roughly 350 to 400 words, which typically covers one to three coherent ideas. A 512-token chunk of Chinese text, due to the token tax (where non-Latin scripts consume more tokens per semantic unit), contains significantly less semantic content. The same fixed token budget buys different amounts of meaning depending on the language. This misalignment means that a chunking strategy optimized for English produces chunks that are either too small (for languages with efficient tokenization) or too large (for languages with inefficient tokenization) for other languages.

## CJK: No Spaces, No Simple Splits

Chinese, Japanese, and Korean present the most immediate challenge for standard chunking pipelines because they do not use spaces between words. This is not an edge case. CJK languages represent some of the largest language populations in the world, and any system claiming multilingual support will encounter them.

In Chinese, a sentence like "the natural language processing technology is developing rapidly" is written as a continuous string of characters without any visual word boundaries. The phrase for "natural language processing" is four characters that must be treated as a single unit. But without a word segmentation model, your chunking pipeline has no way to know where one word ends and the next begins. A naive character-count split might cut the four-character term for "natural language processing" in half, separating "natural language" from "processing." The resulting chunk contains a truncated concept, and the embedding of that chunk will not properly represent the original meaning.

Word segmentation is the solution. Libraries like jieba for Chinese, MeCab and Sudachi for Japanese, and KoNLPy for Korean perform statistical or dictionary-based word segmentation, inserting virtual word boundaries into the continuous character stream. After segmentation, you can apply word-count or semantic chunking strategies as you would for English text.

But segmentation introduces its own complications. Segmentation accuracy varies by domain. A general-purpose segmenter trained on news text performs well on news-style prose but stumbles on technical documentation, where domain-specific compound terms confuse the segmentation model. A medical document in Chinese contains terms that general-purpose segmenters split into meaningless fragments. You need domain-adapted segmentation for specialized corpora, which adds complexity and maintenance burden to your pipeline.

Japanese compounds the challenge because it uses three writing systems simultaneously: kanji (Chinese characters), hiragana, and katakana. A single sentence may contain all three. Katakana is used for foreign loanwords, hiragana for grammatical particles, and kanji for content words. The segmentation model must handle transitions between these scripts, identify compound words that span scripts, and correctly segment loanwords that may not appear in its dictionary.

Korean is more segmentation-friendly than Chinese or Japanese because it uses spaces between words, but Korean words are highly agglutinative -- they combine roots, prefixes, and suffixes into long compound forms. A single Korean word can encode meaning that requires an entire English phrase. This means that word-count-based chunking produces very different semantic coverage in Korean compared to English. A 100-word Korean chunk may contain as much meaning as a 200-word English chunk, because each Korean word is semantically denser.

## Thai, Lao, and Myanmar: The Segmentation Gap

Thai, Lao, and Myanmar present an even harder segmentation challenge than CJK. These scripts do not use spaces between words, similar to Chinese and Japanese. But unlike Chinese and Japanese, they have received far less attention from the NLP community. The segmentation tools are less mature, the training data for segmentation models is scarcer, and the error rates are higher.

Thai is the most practically relevant of the three for most multilingual systems. Thai text is a continuous stream of characters with no spaces between words and sometimes no clear sentence boundaries. The Thai writing system does not consistently use punctuation to mark sentence endings, especially in informal or conversational text. A standard chunking pipeline that looks for spaces and periods finds neither.

Thai word segmentation tools exist -- PyThaiNLP is the most widely used open-source library -- but their accuracy on domain-specific text is lower than equivalent tools for Chinese or Japanese. A segmentation error in Thai can cascade into a chunking error: if the segmenter misidentifies a word boundary, the chunk splits at the wrong location, and the resulting chunks contain fractured words or merged concepts.

The practical impact: Thai retrieval quality is often 10 to 20 percent lower than CJK retrieval quality even when the same embedding model and retrieval pipeline are used. The chunking quality contributes significantly to this gap. Teams that invest in Thai-specific segmentation -- training a domain-adapted segmenter on their own corpus, or manually correcting segmentation errors in their most important documents -- see measurable retrieval improvements.

For Lao and Myanmar, the tools are even less mature. If you need to support these languages, expect to invest engineering time in building or adapting segmentation models. Standard off-the-shelf chunking pipelines will produce poor results.

## Arabic and Hebrew: Right-to-Left and Morphological Complexity

Arabic and Hebrew present chunking challenges that are orthogonal to the CJK problem. Both languages use spaces between words, so segmentation is not the primary issue. The challenges are morphological complexity and right-to-left script handling.

Arabic is a morphologically rich language. A single Arabic word can encode the equivalent of an entire English clause by combining a root with prefixes, suffixes, and internal vowel patterns. The word for "and they will write it" is a single Arabic word. This morphological density means that word-count-based chunking strategies produce chunks with very different semantic coverage than in English. A 100-word Arabic chunk contains more semantic content than a 100-word English chunk, which means your chunks may be too large if you use English-calibrated word counts.

Arabic also exhibits extensive homography -- the same written form can have multiple meanings depending on context, because short vowels are typically omitted in standard written Arabic. The word for "book," "write," and "office" share the same consonantal root and are distinguished by vowel patterns that are usually not written. This does not directly affect chunking, but it affects the quality of embeddings generated from Arabic chunks, because the embedding model must disambiguate from context. Chunks that are too short provide insufficient context for disambiguation.

Hebrew shares many of these morphological properties. It is also right-to-left, uses consonantal roots with vowel patterns, and has high morphological density. The chunking implications are similar: word-count-based strategies need adjustment, and chunks should be long enough to provide disambiguation context.

Right-to-left script handling is primarily a rendering and storage concern rather than a chunking concern, but it can cause subtle bugs. If your chunking pipeline processes text as a byte string or uses string manipulation functions that assume left-to-right order, it may produce corrupted chunks for Arabic and Hebrew documents. Always use Unicode-aware text processing libraries and test your chunking pipeline with actual Arabic and Hebrew documents, not just English documents transliterated into Arabic script.

## Token-Based vs Character-Based vs Semantic Chunking

Three chunking approaches dominate production RAG systems, and each handles multilingual text differently.

**Token-based chunking** splits text by token count using the target embedding model's tokenizer. This approach has the advantage of producing chunks that align perfectly with the model's input capacity. A 512-token chunk uses exactly 512 tokens of the model's context window. The disadvantage for multilingual text is the token tax. The same semantic content requires different token counts in different languages. A 512-token English chunk might cover three paragraphs. A 512-token Japanese chunk, because each Japanese character may consume one or more tokens in a subword tokenizer, might cover only one paragraph. Fixed token counts produce chunks of wildly varying semantic density across languages.

**Character-based chunking** splits text by character count, which avoids the token tax problem. A 1,000-character chunk in any language contains approximately the same visual amount of text. But character count does not correspond to semantic content either, because characters carry different amounts of meaning in different scripts. A single Chinese character often represents an entire morpheme or word. A single Latin character is just a letter. A 1,000-character Chinese chunk contains far more semantic content than a 1,000-character English chunk. And character-based chunking ignores word and sentence boundaries entirely, which means it frequently splits mid-word in languages without spaces.

**Semantic chunking** uses the meaning of the text to determine chunk boundaries. Rather than splitting at a fixed count, it identifies topic shifts, section boundaries, or semantic coherence breakpoints and splits there. This approach produces the most coherent chunks regardless of language, because it respects the structure of the content rather than imposing an arbitrary length limit. The cost is computational: semantic chunking requires running the text through a model to identify breakpoints, which adds processing time to your indexing pipeline. Meta-Chunking, a technique from 2024 research that identifies boundaries by analyzing perplexity distribution, represents one approach to making semantic chunking practical at scale. It groups sentences into chunks based on the logical continuity of their content, producing chunks whose boundaries align with topic transitions rather than arbitrary character or token counts.

For multilingual RAG systems, semantic chunking is the most robust approach but also the most expensive. The practical compromise is language-aware token-based chunking with per-language size targets.

## Chunk Size by Language: The Per-Language Calibration

The optimal chunk size in tokens is not a universal constant. It varies by language, and calibrating it per language is one of the simplest ways to improve multilingual retrieval quality.

The calibration principle is straightforward: choose chunk sizes that produce roughly equivalent semantic content across languages. If your 512-token English chunk typically covers one to two coherent ideas, find the token count that covers one to two coherent ideas in each target language.

For Chinese, the token tax means that each character consumes roughly one to two tokens in most subword tokenizers. A Chinese sentence that would be 20 tokens in English might be 30 to 40 tokens in Chinese. To get the same semantic coverage as a 512-token English chunk, you might need 700 to 900 tokens for Chinese. If your embedding model supports 8,192 tokens (as BGE-M3 does), this is easily accommodated. If your model is limited to 512 tokens, you must accept that Chinese chunks contain less semantic content than English chunks, or you must reduce your English chunk size to match.

For Japanese, the situation is similar to Chinese, with the added complexity of the mixed writing system. Tokenization efficiency varies depending on the proportion of kanji, hiragana, and katakana in the text. Technical Japanese, which uses more kanji, tokenizes differently than conversational Japanese, which uses more hiragana. You may need different chunk size targets for different document types within the same language.

For Thai, the token tax is even more severe. Thai characters are smaller semantic units than Chinese characters, and most tokenizers handle Thai inefficiently. A sentence that takes 20 tokens in English may require 40 to 60 tokens in Thai. Chunk size targets for Thai should be 40 to 80 percent larger than English targets to achieve equivalent semantic coverage.

For Latin-script European languages -- French, German, Spanish, Portuguese -- the token tax is minimal. These languages tokenize nearly as efficiently as English, and the same chunk size targets work well. German compound words (which can be very long single words) occasionally cause tokenization surprises, but the impact on chunk sizing is small.

For Arabic, tokenization efficiency varies by tokenizer. Models trained with Arabic-aware tokenizers handle it more efficiently than models with English-centric tokenizers. Test with your actual model and representative Arabic text to determine the right chunk size.

## Language-Aware Chunking Pipelines

A production multilingual chunking pipeline has three stages: detect, segment, chunk.

**Stage 1: Language detection.** Before chunking, identify the language of each document. For monolingual documents, this is a single detection call. For mixed-language documents -- and these are more common than teams expect -- you may need to detect language at the paragraph or sentence level. A bilingual FAQ document with questions in Japanese and answers in English needs paragraph-level detection so that Japanese and English sections are chunked with their respective strategies.

**Stage 2: Language-specific segmentation.** For languages that require word segmentation -- Chinese, Japanese, Thai -- apply the appropriate segmentation tool. Use jieba or pkuseg for Chinese, MeCab or Sudachi for Japanese, PyThaiNLP for Thai. For languages that do not require explicit segmentation, skip this step. If your documents contain domain-specific terminology, use a domain-adapted segmenter or a custom dictionary that includes your domain terms.

**Stage 3: Language-calibrated chunking.** Apply chunking with per-language size targets. For English, your target might be 512 tokens with 20 percent overlap. For Chinese, 800 tokens with 15 percent overlap. For Thai, 900 tokens with 15 percent overlap. Respect sentence boundaries within the target language: never split mid-sentence if avoidable. Use the language-appropriate sentence boundary detection -- Chinese full stops, Japanese sentence-ending particles, Thai sentence-final markers where present.

The overlap percentage should also be language-aware. Overlap exists to ensure that ideas split across chunk boundaries are captured in at least one chunk. For languages with high morphological density (Arabic, Korean), where a single word can carry an entire clause's worth of meaning, splitting mid-word is more damaging than in English. Larger overlaps -- 25 to 30 percent -- reduce the risk of splitting critical terms across chunk boundaries.

## Testing Chunking Quality

Chunking quality is not directly measurable. There is no "chunking score." But chunking quality has a direct, measurable impact on retrieval quality, and that gives you a proxy for testing.

The test is simple: vary your chunking strategy per language and measure retrieval quality for each variant. Start with your baseline: a single chunking configuration applied uniformly across all languages. Measure recall at k equals 10 and nDCG at 10 for each language using your evaluation test set. This is your baseline.

Then apply language-aware chunking: per-language chunk sizes, language-specific segmentation, language-appropriate sentence boundary detection. Measure the same metrics. The difference is the retrieval quality impact of your chunking strategy.

In practice, teams that switch from uniform chunking to language-aware chunking see retrieval quality improvements of 8 to 20 percent for CJK languages, 5 to 15 percent for Thai and Arabic, and minimal change for Latin-script European languages. The improvements are largest for languages where the default chunking strategy was most misaligned -- typically CJK and Thai, where the absence of spaces and the token tax create the widest gap between English-optimized chunking and language-appropriate chunking.

Run this test for every language you support. The investment is modest -- a few hours of engineering time to implement per-language chunking configurations, plus the time to run your evaluation suite. The payoff is a measurable improvement in retrieval quality for your non-English users, achieved without changing your embedding model, your index, or your retrieval logic.

## Mixed-Language Documents

The hardest chunking scenario is a document that contains multiple languages. A bilingual product manual with English headings and Japanese body text. A customer support ticket that starts in French and switches to English midway through. A technical specification with English code comments embedded in a German document. An academic paper with an English abstract and an Arabic body.

Standard chunking strategies assume a document is in one language. Mixed-language documents violate this assumption and produce chunks that confuse embedding models. A chunk that contains half English and half Japanese produces an embedding that is neither a good English embedding nor a good Japanese embedding. It sits in a no-man's-land of the vector space where neither English queries nor Japanese queries find it reliably.

The solution is language-aware splitting before chunking. Detect language at the paragraph or sentence level. Group consecutive runs of the same language into segments. Chunk within each segment using that language's chunking strategy. Store metadata with each chunk indicating its language, so the retrieval system can filter by language if needed.

This creates an overhead. Language detection at the sentence level adds processing time. Some sentences are genuinely bilingual -- a Japanese sentence that includes an English technical term, or a French sentence that quotes an English document. For these cases, treat the sentence as belonging to its dominant language and accept that the embedded English term may reduce embedding quality slightly. The alternative -- splitting at the word level -- creates fragments too small to embed meaningfully.

## The Chunking Maintenance Tax

Chunking strategies are not set-and-forget. They require ongoing maintenance as your document corpus evolves, your embedding model changes, and your supported languages expand.

When you upgrade your embedding model, re-evaluate your chunk sizes. A new model with a different tokenizer will tokenize the same text differently, changing the token tax for each language. Chunk sizes calibrated for one tokenizer may be suboptimal for another.

When you add a new language, invest the time to calibrate chunking for that language specifically. Do not assume that a chunking strategy that works for an existing language in the same script family will work for the new one. Japanese and Chinese share some script elements but have very different segmentation requirements. Hindi and Bengali share the Devanagari-adjacent script tradition but have different morphological properties.

When your domain vocabulary changes -- new products, new technical terms, new regulatory concepts -- update your segmentation dictionaries. A new four-character Chinese product name that your segmenter does not recognize will be split into individual characters, producing poor-quality chunks for every document that mentions it. Maintaining segmentation dictionaries is an ongoing operational cost, not a one-time setup.

The chunking pipeline is invisible infrastructure. When it works, nobody notices. When it breaks, retrieval quality degrades across every language it touches, and the root cause is maddeningly difficult to diagnose because the symptoms appear in the retrieval metrics, not in the chunking pipeline itself. Treat it as production infrastructure with monitoring, testing, and regular maintenance -- not as a preprocessing script you wrote once and forgot.

The next subchapter moves from how you chunk your documents to where you store them -- the index design decisions that determine whether you use one unified multilingual index, separate indexes per language, or a hybrid approach that gives you the best of both at the cost of added complexity.