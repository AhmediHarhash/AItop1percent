# 9.9 â€” Content Moderation at Multilingual Scale

The trust and safety team stares at a dashboard showing 50,000 content moderation reports that arrived in the past 24 hours. The reports span twelve languages. English and Spanish are in good shape -- automated classifiers handle 89 percent of English reports and 81 percent of Spanish reports, with human reviewers clearing the rest within four hours. French and German are manageable, with classifiers handling about 70 percent and review queues running about eight hours behind. Then there is Thai. The Thai classifier catches only 38 percent of reported content. The manual review team for Thai is two people, both overwhelmed. The Thai review queue is six days behind. A user in Bangkok has been receiving AI-generated content that contains thinly veiled threats in Thai slang for nearly a week, and nobody on the trust and safety team can read the reports about it because nobody on the team speaks Thai.

This is not a hypothetical. This is what multilingual content moderation actually looks like at most companies. The moderation capabilities degrade language by language, from well-resourced to barely functional, and the users who are least protected are often the ones in markets where AI safety regulations are weakest and cultural harms are most acute.

## The Scale Problem

Multilingual content moderation does not scale linearly. Adding a new language is not like adding capacity to an existing system. Each language requires its own safety classifiers, its own training data, its own human reviewers, its own policy definitions, and its own cultural context. If you support twelve languages, you are not running one moderation system twelve times. You are running twelve distinct moderation operations, each with different maturity levels, different accuracy rates, and different resource constraints.

The content moderation market crossed 11.63 billion dollars in 2025 and is projected to reach 23 billion by 2030. That growth reflects the reality that moderation demand is exploding while the tools to meet it remain uneven. Automated moderation systems are up to 30 percent less accurate in non-English languages, according to industry analyses. Performance degrades further for regional dialects, slang, and culturally specific references. The gap between what your moderation system catches in English and what it catches in Bengali, Vietnamese, or Swahili is not a rounding error. It is a structural inequality baked into the system.

Why does it scale so poorly? Three reasons. First, training data scarcity. Building an accurate hate speech classifier requires thousands of labeled examples of hate speech in the target language, authored by native speakers who understand cultural context. For English, these datasets exist in abundance. For Thai, they barely exist. For Tigrinya or Lao, they do not exist at all. You cannot build what you cannot train, and you cannot train what you cannot label.

Second, linguistic complexity varies. A hate speech classifier for English must handle relatively straightforward keyword patterns, contextual slang, and coded language. A classifier for Arabic must handle Modern Standard Arabic, multiple dialects (Egyptian, Gulf, Levantine, Maghrebi), code-switching with French or English, script variations, and a morphological system where a single root can produce dozens of related words with very different meanings. A classifier for Japanese must handle three writing systems, politeness levels, implied meaning, and a cultural communication style where the most harmful content is often what is not said rather than what is.

Third, policy definitions are not portable. What constitutes hate speech in English-speaking Western contexts does not map cleanly onto other languages and cultures. A content moderation policy written in English by a team in San Francisco requires significant adaptation for every new language and cultural context. The adaptation is not translation -- it is re-authoring. Which slurs are harmful in this language? Which cultural references carry violent implications? Which topics are politically sensitive? The answers are different for every language and region.

## Automated Moderation: What It Can and Cannot Do

Automated classifiers are the first line of defense in any moderation system. At multilingual scale, they are necessary for survival. No human review team can manually process tens of thousands of reports per day across a dozen languages. Automation must carry the majority of the load.

For well-resourced languages, automated classifiers have reached usable accuracy. English hate speech classifiers routinely achieve above 90 percent precision and above 85 percent recall on standard benchmarks. Spanish, French, German, and Mandarin classifiers are somewhat behind but functional for the most common harm categories. These classifiers handle the high-volume, clear-cut cases -- obvious slurs, explicit threats, unambiguous toxic content -- and route the ambiguous cases to human reviewers.

For medium-resource languages -- Portuguese, Arabic, Hindi, Indonesian, Turkish, Korean -- automated classifiers exist but with significant accuracy gaps. Recall for culturally specific harms (caste-coded language in Hindi, sectarian references in Arabic, political sensitivity in Turkish) is often below 60 percent. The classifiers catch the obvious cases and miss the nuanced ones, which are often the most harmful because they are designed to evade detection.

For low-resource languages -- Thai, Vietnamese, Swahili, Amharic, Burmese, Tagalog, and dozens of others -- automated classifiers are either rudimentary or nonexistent. Teams in this situation face a choice: deploy an undertrained classifier that will produce high false positive rates and low recall, or route all moderation to human reviewers who may not be available.

The current best practice for medium and low-resource languages is a multilingual base classifier supplemented by language-specific fine-tuning. Start with a multilingual model that provides baseline coverage across languages, then fine-tune on language-specific labeled data as it becomes available. This approach provides coverage from day one (the base classifier catches the most extreme content in any language) while improving over time as language-specific training data accumulates.

## Human Review: The Necessary and Expensive Layer

Human reviewers handle what automated systems cannot: ambiguous content, culturally nuanced harm, novel attack patterns, and the constant stream of edge cases that define real-world moderation.

For multilingual operations, human review requires native speakers of each language. This is non-negotiable. A content moderator who does not speak Thai cannot moderate Thai content, regardless of how many translation tools they have access to. Machine translation loses the cultural context, the tone, the implied meaning, and the slang variations that are often the difference between harmful and benign content. A Thai sentence that translates to "I hope your family is well" in English might carry a threatening implication in its original phrasing that no translation captures.

Hiring native-speaker moderators for every supported language is expensive. Content moderators typically earn between 15 and 45 dollars per hour depending on language, region, and expertise level. Moderators for rare languages command premium rates because supply is scarce. A moderation team covering twelve languages with two moderators per language, working full shifts, costs between 500,000 and 1.5 million dollars per year in labor alone, before accounting for tooling, management, and the psychological support that moderation work requires.

The emotional toll of content moderation is real and well-documented. Moderators who review harmful content daily -- hate speech, threats, graphic descriptions, abuse -- experience elevated rates of PTSD, anxiety, and burnout. This is true for all moderators, but it is amplified for multilingual moderators reviewing content in their native language about their own culture, religion, or community. A Muslim moderator reviewing Islamophobic content in Arabic is not just processing harmful text. They are reading attacks on their own identity in their own language. Support structures -- counseling, rotation policies, exposure limits, wellness checks -- must be in place for every language team, not just for the English team.

## The Queue Inequality

This is the pattern that most multilingual moderation systems fall into, and it is the one you must actively fight against. English moderation is fast because you have good classifiers and large review teams. Low-resource language moderation is slow because you have poor classifiers and small review teams. The result is a safety inequality where non-English users are less protected, wait longer for harmful content to be addressed, and receive less responsive support when they report problems.

The queue inequality is measurable. Track your moderation response time per language: the elapsed time from when a user reports content to when a moderation decision is made. If English reports are resolved in a median of two hours and Thai reports are resolved in a median of six days, your Thai users are receiving 72 times less responsive moderation than your English users. That gap is not an engineering problem. It is an ethical problem and, increasingly, a regulatory one.

The EU's Digital Services Act requires platforms to disclose the linguistic expertise of their content moderation teams and to demonstrate adequate moderation capacity for all languages they serve. The EU AI Act's transparency requirements extend this to AI-generated content. A system that demonstrably provides slower moderation for certain languages is a system with a documented compliance gap.

## SLA Parity: The Operational Target

The antidote to queue inequality is SLA parity: establishing moderation response time targets that are equal across all supported languages and treating any language that falls below the target as an operational incident.

Set a target moderation response time and apply it uniformly. If your target is four hours for high-severity reports, that target applies to English, Thai, Swahili, and every other language you support. When a language consistently misses the target, the response is not to accept slower moderation as inevitable. The response is to invest: better classifiers, more training data, additional reviewers, or automated triage that escalates the highest-severity content in underserved languages.

SLA parity does not mean equal investment per language. English may need two reviewers because automation handles 89 percent of the volume. Thai may need four reviewers because automation only handles 38 percent. The investment is proportional to the gap, but the outcome target is the same.

In practice, achieving SLA parity for all languages simultaneously is unrealistic for most companies. A pragmatic approach is to set three tiers. Tier one: full SLA parity for your top languages by user volume, with the same response times as English. Tier two: near-parity for medium-volume languages, with response times no more than twice the English target. Tier three: best-effort for low-volume languages, with an explicit commitment to promote languages to higher tiers as user volume grows. The key is transparency: document which tier each language is in, and share that information with users so they know what to expect.

## Translation-Assisted Moderation: Useful but Dangerous

When you lack native-speaker moderators for a language, the temptation is to use machine translation to translate reported content into English and have English-speaking moderators review it. This approach is better than nothing and worse than it appears.

Translation-assisted moderation catches high-severity, unambiguous content reasonably well. A direct threat translates clearly enough for an English-speaking moderator to recognize it. Explicit hate speech with clear slurs translates recognizably. Instructions for harmful activities translate comprehensibly.

But translation-assisted moderation fails on exactly the content that most needs cultural expertise. Coded language does not translate. Sarcasm and irony do not translate reliably. Cultural references that carry harmful implications are stripped of their implications in translation. A caste slur in Hindi translated to English becomes a meaningless word. A Thai insult involving the monarchy translated to English loses the legal and cultural weight that makes it dangerous. The moderator sees a benign-looking English sentence and marks it as safe. The original Thai content continues harming users.

Use translation-assisted moderation as a stopgap, not a solution. It keeps the queue moving for languages where you have no native speakers. But flag every decision made through translation as lower-confidence, and prioritize hiring native-speaker reviewers for any language where translation-assisted moderation has been in use for more than six months. If you have been moderating Thai through English translation for more than half a year, you have been providing substandard moderation to Thai users for more than half a year, and you owe them a real solution.

## Shared Taxonomies and Per-Language Policies

A shared harm taxonomy provides the organizational backbone for multilingual moderation. Every language team classifies content using the same categories -- hate speech, threats, harassment, dangerous content, sexual content, misinformation -- so that reporting, metrics, and trend analysis work across languages. But the definitions within each category must be language-specific.

"Hate speech" in English includes racial slurs, homophobic language, and religious bigotry. "Hate speech" in Hindi includes all of those plus caste-based slurs and communal incitement. "Hate speech" in Arabic includes all of those plus sectarian content and blasphemy in jurisdictions where blasphemy is legally actionable. The category is shared. The examples, keywords, patterns, and severity assessments are per-language.

Maintaining per-language policy documents is an ongoing operational cost. Cultural contexts shift. New slurs emerge. Political events change what is sensitive. A moderation policy for Turkish written in 2024 may need significant updates by 2026 based on political developments. Each language policy needs a designated owner -- a native speaker with cultural expertise who reviews and updates the policy at least quarterly.

## Tooling for Multilingual Moderation

The moderation platform itself must support multilingual operation, and many do not. Requirements include native rendering of all supported scripts (right-to-left, CJK, Devanagari, Thai, and others) in the review interface. Moderators must see content as users see it, not as broken Unicode or garbled characters. The platform must support language-specific keyword lists, language-specific severity rules, and language-specific routing to the appropriate review team.

Automated classification should run per-language, with separate models or model configurations for each language. A single multilingual classifier is a reasonable starting point, but as your moderation volume grows, per-language classifiers trained on language-specific data consistently outperform multilingual models on culturally specific harm categories.

Queue management must be language-aware. Route content to reviewers who speak the language. Prioritize queues by SLA target, severity, and queue depth. Alert when a language queue exceeds its SLA target. None of this is novel engineering, but it requires deliberate design. A moderation platform built for English that has other languages bolted on as an afterthought will produce the queue inequality described above. A platform designed from the start for multilingual operation avoids it.

## Feedback Loops and Classifier Improvement

Every human moderation decision is a training signal. When a Thai moderator reviews a piece of content that the automated classifier missed, that decision should feed back into the classifier's training pipeline as a labeled example. When a moderator overrides a false positive -- content the classifier flagged as harmful that was actually benign -- that override should enter the training data as a negative example.

This feedback loop is the mechanism by which your moderation system improves over time, and it works best for the languages with the most human review activity. English classifiers improve fastest because they have the most human review data flowing back. Low-resource language classifiers improve slowest because they have less review data. This creates a compounding inequality: the languages with the worst classifiers get the least training data from human review, which means their classifiers improve the slowest, which means they continue to need more human review.

Break this cycle by prioritizing feedback ingestion for your weakest languages. If your Thai classifier is underperforming, route a higher proportion of Thai content through human review specifically to generate training data, even if the content is not flagged or reported. This is proactive data collection: investing human review hours specifically to improve automation for underserved languages. It is expensive in the short term and essential in the long term.

## Measuring Moderation Effectiveness

Track moderation effectiveness per language across four dimensions. First, coverage: what percentage of reported content in each language receives a moderation decision within the SLA target. Second, accuracy: when you audit moderation decisions, what percentage were correct, measured per language and per harm category. Third, user satisfaction: do users in each language feel that their reports are handled promptly and fairly, measured through post-moderation surveys. Fourth, harm reduction: is the volume of harmful content reaching users in each language declining over time, or is moderation merely treading water.

These metrics, broken down by language, tell you where your moderation system is working and where it is failing. If coverage is 98 percent in English and 61 percent in Vietnamese, you know exactly where to invest. If accuracy is high but user satisfaction is low for Arabic speakers, your policies may be correct but your communication about moderation decisions may be failing. The per-language breakdown is not optional. Aggregate metrics that average across languages hide the inequality that per-language metrics reveal.

Content moderation catches harm after it occurs. Red-teaming, covered in the previous subchapter, catches harm before it reaches users. But both are operational -- they react to content and test for failures. The governance layer that guides these operations, sets policies, and makes the cultural judgments that automated systems cannot make, is the subject of the final subchapter in this chapter: the Cultural Review Board.
