# Chapter 4 — Multilingual Evaluation and Quality Measurement

Translating your English eval suite into twelve languages and calling it multilingual evaluation is one of the most common and most dangerous shortcuts in AI engineering. Translation preserves the words but destroys the meaning. A multiple-choice question about American tax law does not become a valid Hindi evaluation just because the sentences are in Hindi. A fluency rubric designed for English prose misjudges formality in Japanese, politeness levels in Korean, and grammatical correctness in Arabic. Benchmarks like MMLU-ProX, BenchMAX, and INCLUDE exist precisely because the research community learned -- through years of misleading results -- that machine-translated evaluation sets inflate scores for high-resource languages and mask catastrophic failures in low-resource ones. This chapter teaches you how to build evaluation systems that tell the truth about your model's quality in every language you serve, using native-language test sets, culturally grounded rubrics, calibrated multilingual judges, and per-language dashboards that expose the gaps your translated evals are hiding.

---

- **4.1** — Why Translated Evals Produce Misleading Results
- **4.2** — Designing Native-Language Evaluation Sets: The Golden Rule
- **4.3** — The Culture-Specific Question Problem: What MMLU Gets Wrong
- **4.4** — Multilingual Benchmarks That Matter: MMLU-ProX, BenchMAX, INCLUDE
- **4.5** — LLM-as-Judge Across Languages: Bias, Calibration, and Failure Modes
- **4.6** — Human Evaluation for Multilingual: Recruiting Native-Speaker Evaluators
- **4.7** — Dimension-Specific Multilingual Metrics: Fluency, Accuracy, Cultural Fit
- **4.8** — Cross-Lingual Consistency: Ensuring the Same Question Gets the Same Answer
- **4.9** — Regression Testing Across Languages: The Multiplicative Scale Problem
- **4.10** — Building the Multilingual Eval Dashboard: Per-Language, Per-Dimension Tracking
- **4.11** — Eval Coverage Gaps: Detecting Which Languages Are Under-Tested

---

*An eval system that lies to you in one language will eventually lie to you in all of them. The chapters ahead build on this foundation -- translation pipelines, prompt engineering, cultural safety -- but none of them work if your measurement is broken.*
