# 9.3 â€” Bias in Multilingual Models: How Uneven Training Data Creates Uneven Outcomes

Multilingual models inherit the biases of their training data. Since training data is not evenly distributed across languages, the biases are not evenly distributed either. A model that associates "nurse" with women and "engineer" with men in English may make the same association in French and German -- or it may make entirely different, culturally specific associations that reflect the stereotypes present in the French and German training data. The biases are not uniform. They are not predictable from English alone. And they shape every output the model produces in every language, whether your team knows it or not.

This is the bias problem that most teams never measure. They test for bias in English because the benchmarks exist in English, the tooling works in English, and the evaluators are comfortable in English. They find English biases, mitigate English biases, and report English bias metrics. Meanwhile, the same model running in Arabic, Hindi, or Swahili carries biases that nobody has tested, nobody has measured, and nobody monitors. The biases are not absent. They are invisible -- which is worse.

## The Data Imbalance

The root cause of uneven bias is the uneven distribution of training data. English represents an estimated 50 to 70 percent of the text data used to train large language models, depending on the model and the training corpus. The remaining 30 to 50 percent is split among hundreds of other languages, but the split is not remotely even. German, French, Spanish, Chinese, Japanese, and Korean account for the majority of non-English data. Hindi, Arabic, and Portuguese get a meaningful but smaller share. Turkish, Thai, Vietnamese, Swahili, Yoruba, and hundreds of other languages share what is left -- often less than 0.1 percent of the training corpus each.

This distribution is not random. It reflects the structure of the internet, which reflects the structure of global economic power, education systems, and digital infrastructure. Countries with higher internet penetration, higher literacy rates, more publishing infrastructure, and more digital content creation produce more text data. Countries with lower internet penetration, oral rather than written knowledge traditions, or scripts that are poorly supported by digital tools produce less. The training data is a mirror of digital inequality, and every model trained on this data reproduces that inequality in its outputs.

The consequence for bias is direct. The model's representations of the world are shaped by who wrote the text it was trained on. English text was written disproportionately by Western, educated, relatively affluent people. The model's default worldview reflects their perspectives, their values, their assumptions about how the world works. When the model generates text in other languages, it does not suddenly adopt the worldview of speakers of those languages. It projects its English-shaped worldview through a different linguistic surface, producing text that reads as if it were written by someone who speaks Hindi but thinks in English.

## How Bias Differs by Language

The occupational gender stereotypes that are well-documented in English -- nurse as female, engineer as male, teacher as female, CEO as male -- are not universal. They vary by culture, and the model's biases in each language reflect the stereotypes present in that language's training data, not a uniform global stereotype.

In a study on cultural moral frameworks across multilingual models, researchers found that the moral judgments expressed by models varied significantly when prompted in different languages. The same model, asked the same ethical question in English and in Arabic, produced meaningfully different answers that reflected different cultural value systems. This is not the model being culturally sensitive. It is the model reproducing whatever values were most prevalent in the training data for each language.

Occupational stereotypes shift in revealing ways. In Scandinavian-language training data, where gender equality in the workplace is more advanced, the model's occupational gender associations may be weaker than in English. In training data from cultures with more traditional gender roles, those associations may be stronger. The model does not have a single bias profile. It has a different bias profile for every language, shaped by the cultural norms encoded in that language's training data.

Racial and ethnic stereotypes differ even more dramatically. The racial categories that are salient in the United States -- Black, white, Hispanic, Asian -- are not the categories that matter in most other countries. In India, caste is more salient than race. In Malaysia, the Malay-Chinese-Indian ethnic dynamic shapes social stereotypes. In Nigeria, ethnic identity between Hausa, Yoruba, and Igbo groups drives intergroup perception. In Japan, discrimination against Zainichi Koreans and Burakumin communities follows entirely different axes. A model that has been debiased for US racial stereotypes has not been debiased for any of these other systems of discrimination. And because bias evaluation tools were built for English and US racial categories, teams that run standard bias benchmarks in other languages may get passing scores that hide culturally specific biases the benchmarks were never designed to detect.

## The Western Default

When a model is asked about a topic where cultural perspectives diverge, it defaults to the perspective that dominates its training data. Since training data is overwhelmingly English and Western, the default is a Western perspective -- specifically, the perspective of the countries that produce the most English-language internet content: the United States, the United Kingdom, Canada, and Australia.

Researchers have documented this pattern extensively. A 2025 study found that 79.8 percent of unprompted entity recommendations from large language models referenced Western countries -- countries that fit the WEIRD profile: Western, Educated, Industrialized, Rich, and Democratic. When models were asked to recommend books, historical figures, cities to visit, or cultural practices, they overwhelmingly drew from Western sources, even when the prompt did not specify a cultural context.

The Western default manifests in ways that are subtle but pervasive. Ask the model about "traditional family values" and it will likely describe a nuclear family -- a Western concept. Many cultures worldwide operate with extended family structures, multigenerational households, or community-based child-rearing that the model's default framing ignores or treats as unusual. Ask the model about "professional success" and it will likely describe individual achievement, career advancement, and financial growth -- Western measures of success that do not map onto cultures where collective achievement, community standing, or spiritual fulfillment define success.

In hiring and professional contexts, the Western default creates measurable harm. Research has shown that Western communication styles yield significantly higher model-assigned "hireability" scores than communication styles from other cultures, even after anonymizing names and institutions. A candidate from India who communicates in a style typical of Indian professional culture -- which may include more deference, more indirect communication, more emphasis on collective achievement -- receives a lower score from the model than a candidate who communicates in a Western style. The model has not been programmed to prefer Western candidates. It has been trained on data that treats Western communication norms as the standard of professionalism, and it reproduces that standard in its evaluations.

## Representation Bias: Shallow Knowledge of Most Cultures

The data imbalance creates a representation problem that goes beyond stereotypes. For well-represented languages and cultures, the model has access to a rich, diverse set of perspectives. It has seen formal and informal writing, fiction and non-fiction, academic and popular text, multiple political viewpoints, regional variation, generational variation. Its representation of English-speaking cultures is three-dimensional -- imperfect, but deep enough to capture complexity.

For underrepresented languages and cultures, the model's knowledge is shallow. It may have seen primarily formal or educational text, because informal text in low-resource languages is less likely to be digitized. It may have seen primarily the perspectives of the educated elite, because those are the people most likely to produce text that ends up on the internet. It may have seen primarily externally produced descriptions of the culture -- tourism websites, anthropological studies, news articles written by outsiders -- rather than internally produced cultural expression.

The result is a model that knows about Yoruba culture the way a tourist knows about a country they visited for a week. It can produce surface-level accurate statements -- major cities, well-known cultural practices, famous historical figures. But it lacks the depth to capture the nuances that members of the culture take for granted. It produces text about Yoruba culture that reads as if it were written by someone who researched Yoruba culture rather than someone who lives it. And when that shallow knowledge intersects with stereotypes in the limited training data, the model's output can reduce an entire culture to a handful of tropes.

This representation asymmetry means that a user in a well-represented culture receives nuanced, contextualized, multifaceted responses, while a user in a poorly represented culture receives responses that feel generic, superficial, or subtly wrong in ways they can identify but may struggle to articulate. The model is not equally useful for all users, and the users who receive the worst service are the ones whose languages and cultures are least represented in the global digital landscape.

## Measuring Bias Across Languages

Detecting bias in a multilingual model requires benchmarks that are culturally adapted, not merely translated. This distinction is critical. If you take an English bias benchmark and translate it into Hindi, you test whether the model exhibits English-style biases when prompted in Hindi. You do not test whether the model exhibits Hindi-specific biases, because the benchmark was designed for English categories of bias and Hindi has its own.

The Bias Benchmark for Question Answering, known as BBQ, has been adapted for multiple cultural contexts since its original English release. The Multilingual Bias Benchmark (MBBQ) extended BBQ to Dutch, Spanish, and Turkish, while BharatBBQ adapted it for eight Indian languages. These adaptations did not simply translate the English questions. They replaced English bias categories with culturally relevant ones, rewrote scenarios to reflect local contexts, and validated that the stereotypes being tested actually exist in the target culture. Evaluations using MBBQ found that the extent to which models exhibit stereotypical behavior differs significantly across languages, that biases differ across bias categories, and that translation alone is insufficient for culturally valid bias measurement.

Building a bias evaluation for a new language requires three steps. First, identify the bias axes that are culturally relevant in that language and culture. For India, this includes caste, religion, and regional identity. For Japan, this includes ethnicity, class, and immigrant status. For Nigeria, this includes ethnic group, religion, and region. These axes may overlap with English bias categories, but they are not identical, and forcing English categories onto a non-English culture produces meaningless measurements.

Second, create evaluation scenarios that are natural in the target language. The scenarios should describe situations that a native speaker would recognize as realistic, using contexts, names, locations, and social dynamics that are culturally appropriate. A bias evaluation for Hindi that uses American names and American social contexts is not measuring Hindi bias. It is measuring how the model handles American cultural references in Hindi, which is a different and much less useful measurement.

Third, validate the evaluation with native speakers who have expertise in the cultural biases being measured. Domain experts -- sociologists, anthropologists, civil rights advocates, or community leaders from the target culture -- can identify whether the evaluation scenarios actually probe the biases that matter in that culture and whether the expected "unbiased" responses reflect a reasonable standard.

## Mitigation: What You Can Actually Do

Eliminating bias from a multilingual model is not currently possible. The biases are baked into the training data, which reflects real-world inequalities, and no post-training technique can fully undo what the pre-training data encoded. What you can do is measure, monitor, and reduce the most harmful biases through a combination of training-time and inference-time interventions.

**Balanced training data curation** is the most impactful intervention, if you control your training pipeline. This does not mean using equal amounts of data from every language -- that would mean degrading performance in high-resource languages without meaningfully improving low-resource ones. It means curating the data you include to ensure diverse perspectives within each language. For Hindi training data, include text from authors of different castes, different regions, different religious backgrounds, and different socioeconomic positions. For Arabic training data, include text from across the Arabic-speaking world -- not just Gulf States or not just Egypt, but the full geographic and cultural range. This curation is expensive and labor-intensive. It is also the single most effective way to reduce representation bias.

**Per-language bias testing** should be a standard part of your evaluation pipeline. Before any model deployment, run culturally adapted bias benchmarks for every language you support. Track bias metrics per language and per bias category. Set maximum acceptable bias levels and treat any language that exceeds them as a release blocker, the same way you would treat a language whose quality metrics fall below threshold.

**RLHF with diverse cultural perspectives** means including annotators from the cultures your model serves, not just English-speaking annotators. When human evaluators rate model outputs for helpfulness and harmlessness, their cultural perspectives shape what counts as harmful and what counts as helpful. Annotators from India may flag caste-based bias that American annotators would never notice. Annotators from Japan may identify social hierarchy violations that European annotators would miss. Diverse annotation teams produce alignment training that reflects a broader range of cultural values.

**Output monitoring for bias patterns** catches biases that your pre-deployment testing missed. Monitor the model's outputs in production, segmented by language, for patterns that indicate bias: systematic differences in how the model describes people of different genders, ethnicities, or social groups; differences in the quality or tone of responses about different cultures; patterns in which topics the model refuses to engage with in which languages. Automated monitoring can flag statistical anomalies. Human review of flagged outputs determines whether the anomaly represents a real bias that needs correction.

## The No-Clean-Answer Reality

You cannot create a culturally neutral model. Every model reflects some cultural perspective. A model trained mostly on English data reflects Western values. A model fine-tuned with Chinese alignment data reflects certain Chinese cultural values. A model aligned by annotators from the Global South reflects those perspectives. There is no view from nowhere. There is no set of values that all cultures share. The model must have some default, and that default will be more aligned with some cultures than others.

Accepting this reality is not defeatism. It is the prerequisite for making honest, informed decisions about whose values your model reflects and how to manage the consequences.

The practical approach is to be explicit about your model's cultural perspective rather than pretending it has none. Document which cultural values your alignment data reflects. Test for biases against the cultures your model serves. Build mechanisms for users and cultural experts to flag biases you missed. Update your bias mitigations as you learn. And be transparent with users about the model's limitations -- a model that says "my responses may reflect Western cultural perspectives on this topic; the situation may look different from other cultural viewpoints" is more honest and ultimately more trustworthy than a model that presents its culturally shaped perspective as universal truth.

The deepest form of bias is not a wrong answer to a specific question. It is a worldview so pervasive that neither the model nor its creators can see it. The cure is not a better algorithm. It is a broader team, a wider set of evaluators, a more honest acknowledgment of what the training data contains and what it excludes. The teams that do this work produce models that are not bias-free -- no model is -- but bias-aware, which in practice is far more valuable.

Bias does not remain static in the model's outputs. It amplifies. When a model's biased outputs become training data for the next generation of models, or when biased outputs shape user behavior that generates biased feedback data, the original bias grows stronger with each cycle. The next subchapter examines how stereotype amplification works across languages and what it takes to break the cycle before the bias becomes self-reinforcing.