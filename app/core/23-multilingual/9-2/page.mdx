# 9.2 â€” Culture-Specific Harm Categories: What Is Offensive Where

A customer support chatbot deployed across South and Southeast Asia casually suggests a leather goods store during a shopping assistance conversation with a user in Gujarat. In the United States, this is a normal product recommendation. In parts of India, where cow protection is a matter of deep religious conviction and active legislation -- where people have faced mob violence over suspicions of cattle-related activity -- this recommendation lands differently. The model has no concept of this distinction. It has no map of which topics carry lethal weight in which regions. It recommends leather goods the same way it recommends headphones: based on user preferences, purchase history, and relevance scores. The cultural weight of the recommendation is invisible to the system.

This is not a fringe scenario. It is the default behavior of every model that was not explicitly designed to account for cultural harm variation. And it reveals a problem more fundamental than weak safety classifiers: the assumption that harm is universal. That what is offensive in English-speaking markets is offensive everywhere, and what is innocuous in English-speaking markets is innocuous everywhere. That assumption is wrong, and products built on it cause real damage in real markets.

## The Principle: Harm Is Culturally Defined

The previous subchapter covered how safety classifiers miss non-English expressions of harm. This subchapter addresses a deeper issue: even if your classifier worked perfectly in every language, it would still need to know what to classify as harmful in each culture. The categories themselves are not the same.

English-language safety systems are built around categories that reflect primarily Western concerns: hate speech targeting race and ethnicity, explicit violence, sexual content, self-harm, and illegal activities. These categories are real and important. But they represent one culture's taxonomy of harm, shaped by one culture's history, laws, and social norms. Deploying this taxonomy globally is like deploying a single legal code worldwide and expecting it to match every country's laws. Some categories overlap. Many do not. And the gaps are where your users get hurt.

Building a culturally aware safety system requires understanding six categories of harm that vary significantly across cultures. Each one requires different detection methods, different policy decisions, and different trade-offs between cultural sensitivity and global consistency.

## Category One: Religious Sensitivity

Religion is the domain where cultural harm variation is most extreme and most consequential. Content that is routine in one culture can constitute blasphemy in another, and the consequences of blasphemy range from social disapproval to criminal prosecution to physical danger.

In Islamic cultures across the Middle East, North Africa, and Southeast Asia, depictions or descriptions of the Prophet Muhammad are not merely offensive -- they are considered gravely blasphemous by most Muslims. A chatbot that generates a physical description of the Prophet in response to a historical query is not making a cultural faux pas. It is producing content that could, in some jurisdictions, violate blasphemy laws and put users in legal jeopardy. Pakistan's blasphemy laws carry penalties up to death. Indonesia, Malaysia, and several Gulf states have blasphemy statutes with serious criminal consequences. Your model does not need to hold a religious opinion. It needs to understand that generating this content in these contexts creates real-world risk for real users.

In Hindu cultural contexts, cow-related content operates in a charged space. India's many states have varying laws regarding cattle slaughter, with some imposing penalties of up to ten years in prison. Cow vigilantism has led to dozens of killings in recent years. A product that casually recommends beef recipes, suggests steakhouses, or discusses cattle slaughter in a matter-of-fact tone to users in regions where these topics carry this weight is not being neutral. It is being culturally unaware in a way that can cause real harm -- from offending users deeply to, in extreme scenarios, contributing to the kind of content that inflames communal tensions.

Dietary practices tied to religious identity create a broader pattern. Kosher dietary laws in Jewish communities, halal requirements in Muslim communities, vegetarianism tied to Buddhist practice, and Jain dietary restrictions that extend to root vegetables and fermented foods -- each represents a system where food is not just nutrition but identity. A model that treats dietary preferences as interchangeable lifestyle choices rather than deeply held religious practices misunderstands its users at a fundamental level.

## Category Two: Political Sensitivity

Political topics that are matters of ordinary debate in one country are existential questions in another -- questions where the wrong answer is not just controversial but potentially criminal.

The political status of Taiwan is the most prominent example. In mainland China, describing Taiwan as a country or an independent nation contradicts the official One China Policy. Content that treats Taiwan as independent can trigger censorship, legal consequences for platform operators, or loss of market access. In Taiwan itself, asserting sovereignty is mainstream political discourse. A model that serves users in both markets cannot simply pick one framing. Either choice offends a large portion of its user base and potentially violates laws in one jurisdiction.

Kashmir generates similar tension between Indian and Pakistani perspectives. Indian law treats the entirety of Jammu and Kashmir as Indian territory. Pakistani law treats parts of it as Pakistani or disputed. A model that uses the phrase "Indian-administered Kashmir" in a response to an Indian user is making a political statement that the Indian government and many Indian users consider incorrect. Using "Pakistan-occupied Kashmir" in a response to a Pakistani user does the same in reverse. There is no phrasing that is neutral to both audiences.

Kurdish identity and statehood, the political status of Palestine, Catalan independence, Tibet's relationship with China, Crimea's status between Ukraine and Russia -- each of these creates a context where a factual statement in one country is propaganda in another. The model's training data contains all perspectives, and without explicit guidance it will produce whichever framing appeared most often in its training corpus, which typically reflects the perspective of English-language Western media. This default is not neutral. It is one perspective presented as fact.

## Category Three: Historical Trauma

Every culture carries wounds from its history. References to those wounds, in the wrong context or with the wrong tone, can cause profound distress, reopen communal divisions, or incite violence.

The Partition of India and Pakistan in 1947 displaced over 15 million people and killed between one and two million. In families across South Asia, Partition is not history in the abstract. It is the story of how grandparents lost their homes, their relatives, their communities. A model that casually references Partition in a conversation about South Asian geography, without any awareness of the emotional gravity of the topic, is not being informative. It is being careless with content that carries multigenerational trauma.

The Rwandan genocide of 1994, in which approximately 800,000 Tutsi people were killed in 100 days, makes any reference to ethnic identity in Kinyarwanda extremely sensitive. Language that categorizes people by ethnic group, references the historical ethnic hierarchy, or uses terminology associated with the genocide can be deeply harmful even when the speaker intends no malice. Rwanda has strict laws against genocide denial and ethnic divisionism, and content that a model generates in this context can have criminal implications.

Slavery and colonialism create a different kind of sensitivity. In the United States, references to slavery that minimize its severity, glorify its perpetrators, or frame it as economically beneficial cause harm to Black Americans for whom slavery is not ancient history but the foundation of ongoing systemic inequality. In formerly colonized nations across Africa, Asia, and Latin America, references to colonial powers that romanticize the colonial period, credit colonizers with "civilizing" influence, or erase the violence of colonial rule cause similar harm. A model trained predominantly on English-language data -- much of which was produced in former colonial powers -- may absorb and reproduce colonial perspectives without recognizing them as perspectives rather than facts.

## Category Four: Social Hierarchy

Many cultures have systems of social stratification that create categories of sensitive content with no English equivalent.

The caste system in South Asia is the most extensive. Despite legal protections and decades of reform, caste identity profoundly shapes social interactions, economic opportunities, and personal relationships across India, Nepal, Sri Lanka, and diaspora communities worldwide. Caste-based slurs target people from historically marginalized castes -- Dalits and Adivasis in particular -- and these slurs appear in everyday language in ways that a non-South-Asian speaker would not recognize as harmful. A model that reproduces these terms in its output, or that generates content reflecting caste hierarchies as natural or acceptable, causes real harm to the hundreds of millions of people who face caste-based discrimination.

Class systems in the United Kingdom carry their own sensitivities. Accents, word choices, educational backgrounds, and neighborhood references function as class markers that can be used to demean or exclude. A model that generates content about "chavs" or uses class-coded language to characterize people is reproducing a system of social hierarchy, even if the surface-level toxicity score is zero.

Ethnic hierarchies exist in many nations where one ethnic group holds political or economic dominance over others. In Malaysia, the relationship between Malay, Chinese, and Indian communities is shaped by constitutional provisions and affirmative action policies that treat ethnicity as a defining legal category. In Nigeria, the dynamics between Hausa, Yoruba, and Igbo communities shape politics and daily life. In each case, language that references these dynamics carelessly can inflame communal tensions.

## Category Five: Gender, Sexuality, and Family Structure

Perhaps no category of harm varies more dramatically across cultures than content related to gender roles, sexual orientation, and family structure.

As of 2025, at least 65 countries and territories still criminalize same-sex relationships. In several countries, the penalty is death. A model that generates content normalizing LGBTQ relationships is producing content that is affirming and appropriate in many Western contexts, potentially illegal in dozens of others, and physically dangerous in some. This is not a matter of corporate values or progressive politics. It is a matter of user safety. If your model generates content that could be used as evidence of criminal activity in the user's jurisdiction, you have created a safety risk that no toxicity classifier will catch because the content is not toxic -- it is simply illegal in that context.

The reverse is also true. A model that avoids LGBTQ content entirely to satisfy the most restrictive cultural norms fails users in countries where such content is legal, normal, and important. LGBTQ youth seeking support, individuals looking for community resources, people exploring their identity -- a model that refuses to engage with these topics because some jurisdictions criminalize them is providing worse service to users in accepting cultures. There is no single setting that is correct for all users worldwide.

Gender roles themselves vary. In some cultures, a woman asking for career advice is a completely normal query. In others, the same query might be considered inappropriate, or the model's response might inadvertently reproduce cultural expectations about what women should and should not do professionally. A model trained on English-language data absorbs Western gender norms and projects them onto users worldwide, sometimes in ways that are liberating (encouraging women in restrictive cultures) and sometimes in ways that are tone-deaf (assuming Western family structures are universal).

## Category Six: Humor, Satire, and Protected Expression

What is considered acceptable humor varies dramatically across cultures, and what is protected speech in one country is criminal speech in another.

Political satire of heads of state is protected expression in most Western democracies. In Thailand, lese-majeste laws make criticism of the monarchy punishable by up to fifteen years in prison. In Turkey, insulting the president is a criminal offense that has led to thousands of prosecutions. A model that generates political satire in response to a Thai or Turkish user's prompt is potentially generating content that constitutes a criminal offense in the user's country.

Religious humor faces similar variation. Jokes about religious figures or practices are common in secular Western cultures and deeply offensive in many others. A model that generates lighthearted content about religious topics may be producing content that is appropriate in one culture, mildly offensive in another, and blasphemous in a third.

Dark humor, self-deprecating humor, and humor that references cultural stereotypes all carry different weights in different cultures. Japanese humor often relies on wordplay and indirectness. German humor -- despite stereotypes -- has a strong tradition of political cabaret. British humor depends heavily on irony and understatement. American humor tends toward the explicit and the absurd. A model trained primarily on English-language humor defaults to English-language norms about what is funny and what crosses the line, norms that do not generalize.

## The Challenge: Aware Without Biased

The six categories above create a design problem with no clean solution. Your model must be culturally aware -- it must understand that certain content carries different weight in different cultures. But it must not be culturally biased -- it must not impose one culture's norms on users from another culture.

This tension is real and irreducible. If your model refuses to discuss beef in any context because it might offend Hindu users, it is imposing Hindu dietary norms on non-Hindu users. If your model discusses beef freely in all contexts, it is ignoring the sensitivity of the topic for Hindu users. If your model normalizes LGBTQ content globally, it may endanger users in criminalizing jurisdictions. If it avoids the topic globally, it fails users who need support.

The resolution is not a single policy but a framework for making and documenting these decisions. For each culture-specific harm category, you need a policy that specifies: what the model should and should not generate, under what conditions (which languages, which regions, which user contexts), and why. These policies must be explicit, written down, reviewed by cultural experts, and updated as cultural contexts change.

The framework must also accept that some decisions will be wrong in some contexts. A globally deployed model cannot be perfectly calibrated for every culture simultaneously. The goal is not perfection. It is informed, documented, transparent decision-making about cultural trade-offs, with monitoring to detect when those decisions cause harm you did not anticipate.

## Building Per-Region Harm Taxonomies

Operationalizing cultural sensitivity requires building harm taxonomies for each region you serve. A harm taxonomy is a structured catalog of content categories that are sensitive or harmful in a specific cultural context, along with severity levels, detection criteria, and policy responses.

Start with your highest-traffic regions. For each region, assemble a cultural advisory panel of three to five people who are native to the culture, fluent in the dominant languages, and knowledgeable about the political, religious, and social dynamics. These advisors are not translators. They are cultural experts who can tell you what your English-speaking team will miss.

The advisory panel reviews your existing global harm taxonomy and identifies two things: categories that are missing entirely (caste-based harm for India, lese-majeste for Thailand, sectarian content for Northern Ireland) and categories where severity levels differ (political satire that is low-severity in the US but high-severity in Turkey). They also identify false positives -- content that your global taxonomy flags as harmful but that is normal and appropriate in their culture.

Document each region's taxonomy as a structured reference that your safety team, your content policy team, and your classifier training team can use. The taxonomy should include specific examples of harmful content in the local language, written by native speakers rather than translated from English. These examples become training data for your safety classifiers and test cases for your safety evaluation suite.

Update the taxonomies at least twice a year, and more frequently when political events shift the sensitivity landscape. An election, a conflict, a social movement, a legislative change -- any of these can create new categories of sensitive content or change the severity of existing ones. Your cultural advisory panels should flag these shifts proactively, and your safety team should respond with updated classifiers and policies within weeks, not quarters.

## The Documentation Imperative

Cultural safety decisions must be documented explicitly. Every choice your team makes about how the model handles culturally sensitive content should be recorded: what the decision was, who made it, what cultural expertise informed it, what trade-offs were accepted, and when it should be reviewed.

This documentation serves three purposes. First, it creates accountability. When a cultural safety failure occurs -- and it will -- the documentation shows whether the team made a reasonable, informed decision or failed to consider the risk at all. The first is defensible. The second is not. Second, it enables consistency. A global team with dozens of engineers and content moderators needs a shared reference for how cultural sensitivity decisions are made, or each person will apply their own intuitions, which will be inconsistent and often wrong. Third, it satisfies regulatory requirements. The EU AI Act requires documentation of risk mitigation measures for high-risk systems. Per-region harm taxonomies and cultural safety policies are exactly the kind of documentation that demonstrates compliance.

Without documentation, cultural safety decisions live in the heads of individual team members. When those team members leave, the knowledge leaves with them. When a new market launches, the team rediscovers the same cultural pitfalls that were already identified for previous markets. When a regulator asks how you handle culturally sensitive content, you have no answer. Documentation is not bureaucracy. It is the infrastructure that makes cultural safety sustainable at scale.

Harm categories are only part of the picture. Even within the categories your safety system knows about, multilingual models amplify some biases and suppress others based on the uneven distribution of their training data. The next subchapter examines how bias manifests differently across languages and why the same model can be progressive in English and regressive in Arabic on the exact same topic.