# 11.5 â€” Synthetic Data Generation for Multilingual Training

Synthetic data is the primary path to multilingual fine-tuning at scale, and the sooner you accept that, the sooner you stop waiting for organic datasets that will never arrive. For the roughly two hundred languages that frontier models handle at production quality, you can collect real-world training examples through conventional means. For the remaining thousands -- the languages your users actually speak in Southeast Asia, Sub-Saharan Africa, South Asia, and Indigenous communities worldwide -- organic data at fine-tuning scale does not exist and is not coming. Synthetic generation is not a shortcut. It is the only viable route.

The economics make the case before the linguistics do. Human-written, domain-specific training examples cost between two and ten dollars per example depending on the language, the domain complexity, and how hard the annotators are to find. A single fine-tuning run for a customer support model might require three thousand examples per language. If you support fifteen languages, that is forty-five thousand examples, and at five dollars each you are looking at two hundred twenty-five thousand dollars before you have trained a single epoch. Synthetic generation through a frontier model like GPT-5 or Claude Opus 4.6 costs roughly a tenth of a cent per example. The same forty-five thousand examples cost under fifty dollars in API calls. The price difference is not incremental. It is three orders of magnitude.

But price only matters if the output is usable. And for multilingual synthetic data, "usable" is a much harder bar to clear than most teams realize.

## The Three Strategies for Multilingual Synthetic Data

Three distinct approaches have emerged for generating synthetic training data across languages, and each carries different quality profiles, cost structures, and failure modes. Understanding when to use which strategy -- and when to combine them -- is the difference between a fine-tuning dataset that teaches your model to serve real users and one that teaches it to produce fluent nonsense.

The first strategy is **translate-then-verify**. You generate high-quality training examples in English, which frontier models handle best, then translate each example into your target languages using either a translation API or the same frontier model. The advantage is that you start with the highest-quality source material. English is where these models are strongest, where your prompt engineering is most reliable, and where you can most easily validate that the generated examples match your task requirements. The translation step preserves the task structure -- the question-answer pairs, the classification labels, the instruction-response format -- while converting the language. The disadvantage is that translated text sounds translated. A customer complaint originally written in English and translated to Thai reads like a translated complaint, not like a Thai customer actually complaining. The phrasing is correct but the register is wrong. The cultural references are American. The emotional tone follows English patterns. For tasks where linguistic naturalness matters -- customer-facing applications, creative content, culturally sensitive domains -- translated synthetic data trains your model to sound foreign in every language except English.

The second strategy is **generate-in-language**. Instead of generating in English and translating, you prompt the frontier model to generate examples directly in the target language. You write your generation prompt in the target language, or you write it in English with explicit instructions to produce output in Thai, or Vietnamese, or Swahili. The advantage is that the output can be more natural than translated text because the model generates freely in the target language rather than mapping from English structures. The disadvantage is that frontier models are dramatically weaker in low-resource languages. GPT-5 generating Thai customer complaints produces passable Thai. GPT-5 generating Lao customer complaints produces text that a native speaker would immediately identify as machine-generated -- wrong particles, unnatural sentence boundaries, vocabulary that is technically correct but that no Lao speaker would choose. The lower the resource level of the language, the worse the generate-in-language strategy performs, which is precisely the situation where you need synthetic data most.

The third strategy is **back-translation and paraphrase augmentation**. You start with a small seed set of real examples in the target language -- even fifty to a hundred examples -- and use them to generate variations. Back-translation takes each example, translates it to an intermediate language, then translates it back. The round-trip introduces natural variation in phrasing while preserving meaning. Paraphrase augmentation asks the frontier model to rewrite each example in different ways: more formal, more casual, longer, shorter, with different vocabulary. The advantage is that you anchor your synthetic data in real language patterns from your seed set, which prevents the "translated feel" problem. The disadvantage is that you need a seed set, which brings you back to the human-written data problem, and the variation you generate is bounded by the diversity of your seed. If your fifty seed examples all follow the same structure, your five hundred augmented examples will follow the same structure too, just with different words.

## The Quality Cliff in Low-Resource Languages

The quality of synthetic data degrades along a predictable curve as you move from high-resource to low-resource languages, and most teams underestimate how steep that curve is.

For languages where the generating model has seen billions of tokens during pretraining -- English, Chinese, Spanish, French, German, Japanese, Korean -- synthetic data quality is high enough that a native speaker might need to read carefully to distinguish it from human-written text. The grammar is correct, the vocabulary is natural, and the cultural context is plausible. Error rates on careful human review typically run between two and five percent.

For languages in the middle tier -- Thai, Vietnamese, Indonesian, Turkish, Arabic, Hindi, Polish -- quality drops noticeably. Grammar is mostly correct but sentence structures show English influence. Vocabulary is accurate but sometimes stilted. Cultural references are occasionally off. A Vietnamese native speaker reviewing synthetic customer support data would flag perhaps ten to fifteen percent of examples as unnatural, though most would be usable with minor edits. The errors are subtle enough that they will not destroy your model, but they will train it to sound slightly foreign.

For low-resource languages -- Lao, Khmer, Burmese, Amharic, Yoruba, Quechua, most Indigenous languages -- the quality cliff is steep. Frontier models have seen relatively little text in these languages, and their generation capabilities reflect that scarcity. Grammar errors appear in twenty to thirty percent of examples. Vocabulary choices are frequently wrong -- the model picks a word that is technically a translation of the English concept but that no native speaker would use in that context. Cultural framing is often inappropriate. Tone and register are inconsistent. A native Yoruba speaker reviewing synthetic data might reject forty to sixty percent of examples as unusable without significant revision. At that rejection rate, the cost advantage of synthetic data narrows dramatically because you are paying for human review of every example and rewriting half of them.

## The Verification Bottleneck

Every synthetic data strategy for multilingual training eventually hits the same wall: you need native speakers to validate the output, and native speakers for low-resource languages are hard to find, expensive to engage, and difficult to retain.

For high-resource languages, verification is straightforward. You can hire Vietnamese speakers or Turkish speakers through standard annotation platforms. The labor market is deep enough that you can find annotators with domain expertise -- medical Vietnamese, legal Turkish, financial Arabic. Verification costs add perhaps 10 to 20 cents per example on top of the generation cost, and the total is still far cheaper than human-written data from scratch.

For low-resource languages, verification becomes the bottleneck that determines your entire timeline. Finding qualified Lao annotators who understand your domain is not a task you solve with a job posting. It requires reaching into specific communities, universities, diaspora networks, or NGOs. You might find three qualified reviewers for Khmer medical text. If one leaves, you have lost a third of your review capacity with no replacement pipeline. Payment expectations vary wildly -- a Yoruba linguistics graduate student in Lagos has different rate expectations than a Yoruba heritage speaker working at a tech company in London, and both have different expectations than a monolingual Yoruba speaker in a rural community. Managing these relationships is operational work that most engineering teams are not staffed for.

The verification itself is harder for low-resource languages because there is often no standardized written form. Yoruba has multiple orthographic conventions. Burmese script has regional variations. Quechua has competing standardizations across Peru, Bolivia, and Ecuador. Your annotator might flag an example as wrong not because the language is incorrect but because it follows a different regional standard than the one they use. Without clear guidelines on which orthographic standard to follow, your verification process produces inconsistent results.

## The Style Transfer Problem

Even when synthetic data is grammatically correct and factually appropriate, it carries a stylistic fingerprint from the generating model that leaks into your fine-tuned system.

GPT-5 and Claude Opus 4.6 have distinctive writing patterns in English -- preferences for certain sentence structures, transitional phrases, levels of formality. These patterns transfer when the model generates in other languages. A frontier model generating Japanese customer support responses produces text that is technically correct Japanese but that reads like an English speaker's idea of polite Japanese -- overly formal in some places, oddly casual in others, using keigo honorific patterns in ways that a native Japanese customer support agent would not.

This is the **synthetic register problem**. The generated text occupies a register -- a level of formality and a set of stylistic conventions -- that does not exist in natural communication in the target language. It is not wrong enough to be caught by grammar checks. It is not unnatural enough to be flagged by most human reviewers on a first pass. But when you fine-tune your model on thousands of examples that all share this synthetic register, the model learns to produce that register as its default. Your Thai customer support bot sounds like no Thai customer support agent who has ever existed. Your Arabic content generator produces text that native readers describe as "technically correct but weird." Users cannot articulate what is wrong, but they sense it, and it erodes trust.

The mitigation is style anchoring. Before generating synthetic data for a target language, collect a small reference corpus of natural text in that language and domain -- real customer support transcripts, real medical notes, real legal filings. Use this reference corpus in your generation prompt, instructing the model to match the style, register, and vocabulary patterns of the reference examples. This does not eliminate the synthetic register problem, but it constrains it. The model generates within the stylistic bounds of real text rather than inventing its own register.

## The Diminishing Returns Curve

Synthetic data quality does not degrade linearly as you push into harder languages. It follows a curve with a sharp knee.

For languages where the generating model is strong, you can produce ten thousand synthetic examples and expect ninety-five percent to be usable. For languages where the model is moderate, the same ten thousand examples might yield seventy percent usable output. For languages where the model is weak, you might get forty percent usable output from the same effort. But the relationship between usable examples and model improvement is not linear either. The first five hundred high-quality examples in Lao produce a dramatic improvement in your fine-tuned model's Lao performance. The next five hundred produce a noticeable improvement. The next five hundred produce a marginal improvement. Beyond fifteen hundred to two thousand examples, additional synthetic data in a low-resource language often produces no measurable quality gain because the errors in the synthetic data start to dominate -- your model is learning from noisy signal, and the noise outweighs the signal.

This creates a practical ceiling on synthetic data for low-resource languages. You can generate unlimited volume, but quality-adjusted volume has a hard cap determined by the generating model's capabilities in that language. Pushing past that cap does not help. It actively hurts, because the model starts memorizing the error patterns in your synthetic data and reproducing them at inference time.

The implication is that synthetic data alone is never sufficient for low-resource languages. It gets you from zero capability to basic capability. Getting from basic to production quality requires either genuine human-written data to supplement the synthetic foundation, or model merging and cross-lingual transfer techniques that leverage the model's strength in related languages. Synthetic generation is the starting line, not the finish line.

## Combining Strategies for Maximum Coverage

The teams that get multilingual synthetic data right do not pick one strategy. They layer all three and adjust the mix per language based on the generating model's strength in that language.

For high-resource target languages, generate-in-language is the primary strategy, supplemented with paraphrase augmentation to increase diversity. The model is strong enough in these languages that direct generation produces natural output, and you can validate with readily available native speakers at modest cost.

For mid-resource languages, translate-then-verify is the primary strategy, supplemented with a small seed set of human-written examples and generate-in-language examples for diversity. The translation approach gives you reliable task structure while the human-written seeds anchor the style. Verification costs are moderate, and you can find annotators through standard channels.

For low-resource languages, back-translation and paraphrase augmentation from a carefully curated seed set is the primary strategy. The seed set is small -- fifty to a hundred examples -- but it must be written by genuine native speakers, not translated from English. You augment this seed set with paraphrasing and back-translation to reach five hundred to fifteen hundred examples, then validate the augmented set with native speakers. Generate-in-language is used only as a supplement, and every generated example goes through human review because the error rate is too high to trust at scale.

This tiered approach matches effort to impact. You spend the least on languages where synthetic data is easiest, and the most on languages where it is hardest but most needed. The total cost is higher than a naive "generate everything synthetically" approach, but the quality is dramatically better, and quality is what determines whether your fine-tuned model actually serves users or just passes internal tests.

## The Audit Trail That Saves You

Every synthetic example in your training set should carry metadata recording how it was generated: which strategy, which generating model, which prompt version, which human reviewer approved it, and what quality score the reviewer assigned. This audit trail is not bureaucratic overhead. It is the diagnostic tool that saves your next training run.

When your fine-tuned model produces awkward output in Vietnamese, the audit trail lets you trace back to the Vietnamese training examples, filter by generation strategy, and discover that the ninety examples generated via translate-then-verify all share the same unnatural sentence-final particle usage because the translation step systematically converted English period placement into Vietnamese particle placement. Without the audit trail, you are debugging blind.

When your model performance in Amharic degrades after a data refresh, the audit trail lets you compare the current Amharic training set to the previous one, identify which examples changed, and determine whether the quality regression is in the generation or the verification step. Audit trails turn mysterious model failures into tractable data engineering problems.

The next subchapter examines how parameter-efficient fine-tuning methods -- LoRA, QLoRA, and language-specific adapters -- allow you to adapt a single base model for multiple languages without destroying its cross-lingual capabilities.
