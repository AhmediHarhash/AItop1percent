# 12.4 â€” Multilingual Cost Modeling: Per-Language Unit Economics

Every language your AI system supports has a different cost profile. English is cheap. Japanese costs roughly 1.8 times more per request. Korean costs around 2.4 times more. Arabic, depending on your model provider, can cost anywhere from 1.7 to 3.4 times more than English for semantically equivalent content. If your pricing model assumes uniform cost per request, you are losing money on every non-English interaction and do not know it.

Most teams discover this gap the hard way. They budget for AI costs using a single "average cost per request" figure derived from their English-dominated beta traffic, then launch in new markets and watch their unit economics collapse. The collapse is silent because the monitoring dashboards show total spend, not per-language spend. You see the monthly API bill increase. You attribute it to growth. You do not realize that your Japanese users cost nearly twice what your English users cost, and that your Korean users cost more than double, until someone finally runs the per-language breakdown and realizes the "profitable" product loses money in eight of its twelve supported languages.

This subchapter teaches you how to build the per-language cost model your finance team has been asking for and your engineering team has been avoiding.

## The Token Tax: Why Some Languages Cost More at the API Layer

The root cause of per-language cost disparity is tokenization. Large language models do not process characters or words. They process tokens -- subword units derived from byte-pair encoding or similar algorithms. These tokenizers are trained predominantly on English text, which means English receives the most efficient encoding. Common English words like "the," "and," or "customer" are each a single token. The same concepts in other languages often fragment into multiple tokens.

Research published at EMNLP 2023, in a paper titled "Do All Languages Cost the Same?", demonstrated that the tokenization tax is both significant and systematic. Languages using Latin scripts closely related to English -- French, Spanish, German -- pay a modest premium of 1.1 to 1.4 times the English token count for equivalent semantic content. Languages using non-Latin scripts pay substantially more. Mandarin Chinese averages roughly 1.8 times the English token count. Japanese, which mixes three writing systems, averages around 2.1 times. Korean averages 2.3 to 2.4 times. Arabic ranges from 1.7 to 3.4 times depending on the model's tokenizer, because Arabic's rich morphology means a single word can encode complex grammatical information that English requires several words to express, but the tokenizer breaks that single word into many subword pieces.

The token tax translates directly into API cost. If you pay per token -- and in 2026, whether you use OpenAI, Anthropic, Google, or any other provider, you pay per token -- a Japanese request that is semantically identical to an English request costs 1.8 to 2.1 times more. Not because the model works harder. Not because the output is longer. Simply because the same meaning requires more tokens to represent.

Newer tokenizers have narrowed this gap somewhat. GPT-5's tokenizer is more multilingual than GPT-4o's was. Claude Opus 4.6 handles CJK characters more efficiently than Claude 3.5 did. But no tokenizer has eliminated the tax. The fundamental constraint is that tokenizer vocabulary is finite -- typically 100,000 to 200,000 tokens -- and that vocabulary must be shared across all supported languages. English, having dominated the training data, claims the most efficient tokens. Every other language competes for the remainder.

For your cost model, you need a concrete multiplier per language. Do not trust the averages published in research papers, because your specific use case may deviate significantly. Measure your own token ratios. Take a sample of one thousand requests from your English production traffic, translate them into each target language using professional translators (not machine translation, which may introduce artifacts), and count the tokens. The ratio of target-language tokens to English tokens is your empirical token multiplier for that language. This number goes into your cost model.

## The Embedding Cost Multiplier

Token cost is the most visible per-language expense, but it is not the only one. If your system uses retrieval-augmented generation, you are also paying for embeddings -- and multilingual embedding models are more expensive to run than English-only ones.

English-only embedding models are smaller and faster. A high-quality English-only model like GTE-large runs at roughly 350 dimensions and processes text quickly because it was optimized for a single language. Multilingual embedding models like BGE-M3 or Cohere Embed v3, which must represent semantic meaning across dozens of languages in a shared vector space, use larger architectures and produce higher-dimensional embeddings. BGE-M3 produces 1,024-dimensional vectors. The larger dimensionality means larger vector indexes, slower similarity search, and higher storage costs.

The embedding cost multiplier compounds with the token tax. A Japanese query produces more tokens than an English query. Those tokens are processed by a larger, slower embedding model. The resulting vector is stored in a larger index. Each of these steps adds cost. A retrieval call for a Japanese query in a multilingual system can cost 2.5 to 3 times what the same retrieval call costs in an English-only system -- and that is before the generation step, which carries its own token tax.

If you currently run an English-only RAG system and plan to expand to multilingual, do not assume your retrieval costs will simply scale proportionally with traffic. They will scale proportionally with traffic times the embedding multiplier times the token multiplier. A four-language expansion that doubles your traffic may quadruple your retrieval costs.

## The Eval Cost Multiplier

Every evaluation dimension you measure must be measured in every language. This creates a cost multiplier that grows as the product of your language count and your eval dimension count, not as the sum.

Suppose you measure five quality dimensions: factual accuracy, relevance, fluency, safety, and tone. In an English-only system, you run five eval checks per response. Add Japanese and you run ten. Add Korean, Arabic, and Spanish and you run twenty-five. But the cost per eval check is not uniform across languages because of the same token tax -- the eval prompt and the response being evaluated both consume more tokens in non-English languages.

The true eval cost for a five-language, five-dimension system is not twenty-five times the cost of a single English eval. It is the sum of each language's token multiplier times the number of eval dimensions. If your English eval costs one dollar per thousand evaluations, Japanese costs $1.80, Korean costs $2.40, Arabic costs $2.20, and Spanish costs $1.20, then a five-dimension eval suite across all five languages costs $1.00 plus $1.80 plus $2.40 plus $2.20 plus $1.20 times five dimensions. That is $43.00 per thousand responses evaluated across all languages -- more than eight times the English-only cost of five dollars, even though you "only" added four languages.

Most teams do not calculate this number until they see their first monthly eval bill after a multilingual launch. The shock is predictable. The number is always larger than anyone expected because the multiplication is unintuitive. People think linearly. They expect adding four languages to add roughly four times the cost. The actual multiplier is much higher because each language has a different cost weight.

## The Human Review Cost

Automated eval catches many quality problems, but human review remains essential for nuanced language quality -- fluency, cultural appropriateness, idiomatic correctness, and the kind of subtle errors that automated metrics miss entirely. The cost of human review per language varies dramatically, and the variation does not follow the pattern most teams expect.

English-language reviewers are abundant and inexpensive relative to other languages. You can hire qualified English reviewers in global markets for $15 to $25 per hour. Reviewers for major European languages -- French, German, Spanish -- cost $20 to $35 per hour and are readily available. But as you move into less commonly served languages, costs escalate. Native Japanese reviewers with the technical knowledge to evaluate AI output quality cost $40 to $60 per hour. Korean reviewers are similarly priced. Arabic reviewers who can evaluate Modern Standard Arabic and specific dialectal variants are $35 to $55 per hour. For genuinely low-resource languages -- Amharic, Khmer, Lao, Burmese -- finding qualified reviewers at any price is difficult, and those you find command $50 to $80 per hour because the supply is so constrained.

The cost per review session is only part of the equation. Review throughput also varies by language. English reviewers can evaluate a conversational AI response in 45 to 90 seconds because they are reading in their native language and the response patterns are familiar. Japanese reviewers evaluating Japanese output take longer -- 90 to 150 seconds per response -- because Japanese text is denser and evaluating honorific levels, particle usage, and formality requires more cognitive effort. Arabic reviewers evaluating dialectal appropriateness take 120 to 180 seconds because they are simultaneously assessing linguistic accuracy and regional cultural fit.

When you multiply higher hourly rates by lower throughput, the per-review cost for low-resource languages can reach four to five times the per-review cost for English. A review that costs $0.50 in English costs $2.00 to $2.50 in Japanese and $3.00 or more in Khmer. Across thousands of reviews per month, this difference dominates your human review budget.

## The Infrastructure Cost: Regional Deployments

Data residency requirements add another layer to per-language cost modeling that is invisible when you operate exclusively in one region. GDPR requires that personal data of EU residents be processed within the EU or in countries with adequacy decisions. China's Personal Information Protection Law mandates that personal data of Chinese citizens be stored and processed within China unless specific cross-border transfer conditions are met. India's Digital Personal Data Protection Act of 2023 imposes similar localization requirements. Brazil's LGPD, while less strict on data localization, still imposes processing constraints that influence deployment architecture.

Each regional deployment carries fixed infrastructure costs. Running a GPU cluster in the EU, another in East Asia, and another in South Asia means three sets of inference servers, three sets of vector databases, three sets of monitoring infrastructure, and three sets of operational overhead. The model weights may be identical, but the infrastructure is triplicated.

For many AI systems, these regional deployments are not optional. If your Japanese customers' data must be processed in the Asia-Pacific region, you cannot route their requests to your US-based inference cluster no matter how much cheaper it would be. You pay the regional premium -- which varies from 10 to 40 percent more than US pricing depending on the cloud provider and region -- and you accept it as the cost of operating in that market.

The per-language cost model must capture these regional costs and allocate them to the languages they serve. If your East Asia deployment serves Japanese, Korean, and Traditional Chinese users, the fixed cost of that regional infrastructure divides across those three languages. If 70 percent of your East Asia traffic is Japanese, Japanese absorbs 70 percent of the regional infrastructure cost. This allocation is imprecise but essential because without it, your per-language cost model understates the true cost of serving each market.

## The Support Cost Multiplier

Multilingual customer support is more expensive per ticket than English-only support, and the cost difference is often larger than teams expect. Support for an AI product includes responding to user complaints about output quality, handling escalations when the model produces harmful or inappropriate content, and debugging user-reported issues that require reproducing the problem in the user's language.

English-speaking support agents can be hired globally and trained quickly. Multilingual support agents who can read and write fluently in the target language, understand the cultural context of complaints, and evaluate whether the AI's output was genuinely wrong or merely unfamiliar to the user -- these agents are more expensive and harder to find. A support team that handles English at $12 per ticket may handle Japanese at $22 per ticket and Korean at $25 per ticket. For languages where support is outsourced to specialized agencies, the per-ticket cost can reach $30 to $40.

Support volume also varies by language. Markets where AI adoption is newer tend to generate more support tickets per user because users are less familiar with AI limitations and more likely to report expected behavior as bugs. Markets where AI quality is lower -- typically your newer, less-tuned language deployments -- generate more legitimate quality complaints. Both effects increase per-language support costs for your non-English markets during the first six to twelve months after launch.

## Building the Per-Language Cost Model

The cost model is a spreadsheet. Not an algorithm. Not a dashboard. A spreadsheet, because the people who need to understand these numbers -- your finance team, your product leadership, your market expansion strategists -- think in rows and columns, not in code.

The rows are your cost components. For a typical multilingual AI system, these include: API token cost per request, embedding cost per retrieval call, eval cost per evaluated response, human review cost per reviewed response, regional infrastructure cost allocated per request, support cost per ticket, and operational overhead per language. Operational overhead captures the engineering time spent maintaining per-language prompt templates, monitoring per-language quality dashboards, updating per-language eval suites, and responding to per-language incidents.

The columns are your languages. Start with your current languages and add planned expansion languages. For each cell, fill in the actual cost based on your measured token multipliers, your contracted reviewer rates, your regional infrastructure bills, and your support ticket data.

The bottom row is the one that matters: **true cost per request by language**. This is the sum of all cost components for a single request in that language. For a mature multilingual system, you might find that an English request costs $0.012, a Spanish request costs $0.016, a Japanese request costs $0.024, a Korean request costs $0.029, and an Arabic request costs $0.027. These numbers look small individually, but at scale -- a million requests per month per language -- the differences compound into tens of thousands of dollars.

The true cost per request is the number your company has probably never calculated. It is the number that reveals whether your multilingual expansion is profitable or subsidized. And it is the number that should drive every decision about language prioritization, pricing, and quality investment.

## Using Per-Language Costs to Inform Decisions

Once you have the true cost per request by language, three strategic decisions become much clearer.

**Pricing decisions.** If your product charges a flat per-request fee regardless of language, you are cross-subsidizing expensive languages with revenue from cheap ones. English users are effectively subsidizing Japanese and Korean users. Whether this is acceptable depends on your market strategy. If you are trying to grow in Japan, the subsidy might be a deliberate investment. If you did not know it was happening, it is a leak. Section 30 covers pricing strategy in depth, but the per-language cost model is the input that makes those pricing decisions rational rather than arbitrary.

**Language prioritization.** When your team debates whether to add Thai or Polish next, the cost model changes the conversation. It shifts the question from "which market is bigger?" to "which market can we serve profitably given the cost structure?" A market of 20 million potential users is attractive, but if the per-request cost is three times your English cost and the willingness to pay in that market is lower than in your English markets, the unit economics may not work. The cost model does not make the decision for you, but it ensures the decision is informed.

**Quality investment.** Per-language costs reveal where quality investment has the highest return. If your Japanese eval suite costs 1.8 times your English eval suite, you need Japanese quality to be high enough that you are not wasting those eval dollars on a language that is barely passing. Conversely, if your Arabic human review costs are five times your English review costs, you have a strong incentive to invest in better automated Arabic eval to reduce the volume of responses that need human review. The cost model tells you where every dollar of quality investment delivers the most savings.

## The Number Most Teams Have Never Calculated

The true cost per request by language is the foundation of every financial decision about your multilingual AI system. It determines whether your expansion into a new market is an investment or a loss. It reveals which languages are subsidizing which. It tells you where quality automation will save the most money and where human review costs are unsustainable.

Calculate it. Put it in a spreadsheet. Share it with your finance team and your product leadership. Update it quarterly as token prices change, as your traffic mix shifts, as you add or remove languages. This single artifact -- a table of per-language costs -- will change more decisions about your multilingual strategy than any architectural diagram or quality report ever could.

The per-language cost model treats each language as an independent line item. But languages do not scale independently. The next subchapter examines the multiplicative scaling problem -- why adding your seventh language costs far more than adding your third, and why the operational overhead of managing many languages grows faster than linearly.
