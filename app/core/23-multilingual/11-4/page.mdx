# 11.4 â€” Data Collection for Low-Resource Languages: Strategies and Pitfalls

Everyone talks about multilingual AI. Conference keynotes celebrate models that "support 100 languages." Product pages list flags in neat grids. Almost nobody talks about where the training data comes from for languages spoken by fifty million people but barely represented on the internet. Yoruba has over forty-seven million speakers across Nigeria and the West African diaspora. Khmer has sixteen million speakers in Cambodia and beyond. Amharic is the working language of the Ethiopian government, spoken by over thirty million people. Yet these languages have less digital text available for model training than a single year of English Wikipedia edits. The gap between speaker count and digital representation is the defining problem of multilingual AI, and if you do not understand it, every technical decision you make downstream -- your fine-tuning recipe, your eval strategy, your quality targets -- is built on a foundation that does not exist.

## What "Low-Resource" Actually Means

The term **low-resource language** is misleading because it sounds like a statement about the language itself. It is not. It is a statement about the language's digital footprint -- the volume of written text available in formats that machine learning pipelines can ingest. A language can have hundreds of millions of speakers and still be low-resource if those speakers communicate primarily through speech, if the language lacks a standardized written form, or if the written form exists but has not been digitized at scale.

The factors that determine resource level have nothing to do with linguistic complexity or cultural importance. They are entirely about historical accidents of technology adoption and economic incentive. English dominates web text not because it is a better language but because the internet was invented in English-speaking countries, early computing was built around the Latin alphabet, and the economic incentives for digitizing English content arrived decades before equivalent incentives for Yoruba or Khmer. By the time these languages started appearing online in significant volumes, English had a multi-billion-document head start that no amount of catching up can close through organic growth alone.

The practical measure is straightforward: how many tokens of clean, native-speaker-written text can you collect for this language? For English, the answer is trillions. For Chinese, Spanish, and French, hundreds of billions. For Thai, Vietnamese, and Arabic, tens of billions. For Yoruba, Khmer, and Amharic, the answer drops to millions or low billions -- and much of that is noisy, poorly formatted, or of questionable authorship. The ratio between the top and bottom of this scale is not ten to one or a hundred to one. It is a thousand to one or more. That ratio is the core challenge everything else in this subchapter addresses.

## The Four-Tier Framework

Not all languages fall neatly into "high-resource" or "low-resource." The landscape is a gradient, and understanding where your target language sits on that gradient determines which collection strategies are viable.

**Tier 1** languages have abundant digital text -- trillions of tokens across every domain. English, Mandarin Chinese, and Spanish are the anchor languages. Models trained on web-scale data perform well in these languages without any special collection effort. Your challenge here is not finding data but filtering it for quality and domain relevance.

**Tier 2** languages have sufficient digital text -- hundreds of billions of tokens, enough for a foundation model to develop strong capabilities. French, German, Japanese, Korean, Portuguese, Russian, and Italian fall here. Fine-tuning data is available through standard web scraping and corpus aggregation. You can find domain-specific text for most verticals. The main challenge is balancing representation across subdomains to avoid skewing the model toward the topics that dominate that language's web presence -- news and politics for Russian, technology and pop culture for Korean.

**Tier 3** languages have limited but growing digital text -- tens of billions of tokens, enough for a model to develop basic capabilities but not enough for reliable performance across all domains. Thai, Vietnamese, Indonesian, Turkish, Arabic, Hindi, Polish, and Swahili sit in this tier. Web scraping yields usable data but with significant quality variation. Domain-specific text is sparse for specialized verticals. Fine-tuning for Tier 3 languages requires active collection beyond web scraping -- partnerships, community engagement, or synthetic augmentation.

**Tier 4** languages have scarce digital text -- millions of tokens or fewer, often concentrated in narrow domains like news, religion, or government. Yoruba, Khmer, Burmese, Amharic, many Indigenous Australian languages, Quechua, Guarani, and hundreds of others fall here. Standard web scraping produces almost nothing usable. The text that exists is often of poor quality -- machine-translated, written by non-native speakers, or concentrated in a single domain that creates massive distribution skew. Fine-tuning for Tier 4 languages requires dedicated investment in collection programs that are closer to linguistic fieldwork than data engineering.

Your language tier determines your entire data strategy. Treating a Tier 4 language like a Tier 2 language -- assuming you can web-scrape your way to a training set -- is the most common mistake teams make, and it leads to models that produce grammatically broken output because they were trained on grammatically broken data.

## Strategy One: Web Scraping with Quality Filters

Web scraping is the default starting point for any language, and for Tier 1 and Tier 2 languages it is often the ending point as well. The Common Crawl dataset alone contains text in hundreds of languages, and targeted scraping of language-specific websites, news archives, government portals, and forums can supplement it.

The problem is not the scraping. It is the filtering. Raw web text in any language is noisy -- duplicated pages, boilerplate navigation text, advertisements, spam, machine-translated content, and text in the wrong language entirely. For high-resource languages, you can afford aggressive filtering because there is enough volume to survive large rejection rates. If you discard 70 percent of your scraped English data, you still have more than you need. If you discard 70 percent of your scraped Khmer data, you might have nothing left.

The quality-volume trade-off is the defining tension for Tier 3 and Tier 4 languages. You can set strict quality filters -- requiring consistent script usage, minimum sentence length, language identification confidence above 0.95, perplexity scores within expected ranges -- and end up with a clean dataset so small it is useless for fine-tuning. Or you can relax your filters to preserve volume and end up with a noisy dataset that teaches your model errors as if they were correct language. Neither extreme works. The practical approach is graduated filtering: strict filters for the core of your training set, relaxed filters for a supplementary set that gets lower sampling weight during training.

Language identification is itself a major obstacle for Tier 4 languages. The standard language ID tools -- fastText, CLD3, OpenLID -- were trained primarily on Tier 1 and Tier 2 text. Their accuracy on Tier 4 languages is poor, often confusing closely related languages or failing to identify the target language entirely. Swahili text might be classified as generic "African language" or misidentified as a different Bantu language. Yoruba text without diacritical marks might be classified as English. If your language ID tool cannot reliably identify your target language, your entire scraping pipeline is producing mislabeled data.

## Strategy Two: University and Institute Partnerships

For Tier 3 and Tier 4 languages, the highest-quality data often sits in university linguistics departments, language institutes, and cultural preservation organizations -- places that have been collecting and digitizing text for decades as part of academic or cultural missions, not for AI training.

The Masakhane research community, a grassroots organization of African NLP researchers, has been building datasets and tools for African languages since 2019. By 2025, Masakhane had produced benchmark datasets for machine translation, named entity recognition, and sentiment analysis across dozens of African languages including Yoruba, Igbo, Hausa, Amharic, and Swahili. Their AfricaNLP workshop series, now in its sixth year, has become the primary venue for African language NLP research. For teams building AI systems targeting African languages, Masakhane's datasets are often the highest-quality starting point available.

Similar initiatives exist in other regions. The Southeast Asian Languages in One Network project covers languages like Lao, Khmer, and Burmese. University of Tokyo's language resource programs cover underserved Asian languages. The Indigenous Language Institute maintains archives for dozens of Native American languages. These organizations have data that no web scraping pipeline will ever find -- carefully transcribed speech, annotated text corpora, parallel translations by qualified linguists.

The partnership model requires patience and cultural sensitivity. These organizations are not data vendors. They are communities of researchers and language advocates who have been working on these languages for years or decades before AI companies showed interest. Approaching them with a "we need your data for our model" pitch is the fastest way to get rejected. The successful approach treats the partnership as bidirectional: you fund their research, share model capabilities that advance their linguistic goals, provide technical infrastructure they lack, and give them meaningful control over how their data is used. The partnerships that work are the ones where the language community benefits as much as your AI system does.

## Strategy Three: Community-Based Collection

When institutional data does not exist or is insufficient, the remaining option is to generate new data through community engagement -- paying native speakers to write, translate, review, and curate text in their language.

Community-based collection is the most labor-intensive strategy, but for Tier 4 languages it is often the only one that produces genuinely native text at meaningful scale. The approach works like annotation projects, but with a critical difference: you are not labeling existing data, you are creating original text from scratch. Native speakers write customer support dialogues, compose medical intake summaries, draft legal correspondence, or produce whatever domain-specific text your model needs for fine-tuning.

The quality of community-collected data depends almost entirely on how you design the collection protocol. Open-ended prompts like "write a customer complaint in Yoruba" produce wildly inconsistent results -- some writers produce formal Yoruba, others produce Yoruba heavily mixed with English, others produce text that is more accurately described as pidgin. Structured prompts with specific scenarios, register guidance, and example outputs produce much more consistent data. The tighter your specification, the more usable your output. But over-specifying the prompt risks producing data that is structurally uniform, which reduces the diversity your model needs to learn natural variation.

Payment structures matter enormously and carry ethical weight. Community members who speak low-resource languages are often in economically disadvantaged positions. Paying below-market rates to extract linguistic labor from marginalized communities is extractive in a way that paying English speakers market rate for English annotations is not. The asymmetry of economic power between a well-funded AI company and a Yoruba-speaking community in rural Nigeria demands conscious attention. Industry norms that emerged in 2024-2025 suggest that community-based collection should pay at minimum the local professional rate for translation or content creation work, not the global minimum for microtask annotation. Some organizations go further, establishing revenue-sharing agreements where the language community receives ongoing compensation as the data continues to be used in production models.

## Strategy Four: Government and Institutional Archives

Many languages have extensive written records in government, legal, and religious contexts -- even when they have almost no web presence.

Court records, parliamentary proceedings, government regulations, and official correspondence exist in the national and regional languages of most countries. Cambodia publishes legal documents in Khmer. Ethiopia publishes government proceedings in Amharic. Nigeria's federal system produces documents in Yoruba, Hausa, and Igbo alongside English. These documents are often high-quality -- written by educated native speakers, reviewed through institutional processes, and covering domains that are directly useful for AI applications like legal AI, government services, and civic technology.

Religious texts represent another substantial source. Bible translations exist in over 700 languages. The Quran has been translated, with scholarly commentary, into hundreds of languages. Buddhist texts exist in Pali, Burmese, Thai, and Khmer. Hindu scriptures exist in Sanskrit, Hindi, Tamil, and dozens of other languages. These texts provide parallel corpora -- the same content in multiple languages -- which is enormously valuable for cross-lingual alignment and translation quality. The Bible in particular has been used as a parallel corpus for low-resource languages since the earliest days of statistical machine translation, and it remains relevant in the neural era.

The limitations of institutional archives are domain skew and register skew. Legal text sounds like legal text. Religious text sounds like religious text. If your fine-tuning dataset is predominantly legal and religious sources because those are the only available high-quality Khmer texts, your model learns to produce output with a legalistic or scriptural tone that is inappropriate for casual customer support or social media interactions. You need to actively counterbalance institutional sources with conversational or informal text, which brings you back to community collection or synthetic augmentation.

## The Dialect Contamination Problem

This is the pitfall that causes more silent quality failures than any other: collecting data labeled as a single language when it actually contains a mixture of dialects, registers, or related languages that the model should treat differently.

Arabic is the canonical example. Text labeled "Arabic" on the web is a mixture of at least five distinct language varieties: Modern Standard Arabic, which is the formal written standard used in news and government; Egyptian Arabic, the most widely understood dialect due to Egyptian media dominance; Gulf Arabic, spoken across the UAE, Saudi Arabia, Kuwait, and neighboring states; Levantine Arabic, spoken in Syria, Lebanon, Jordan, and Palestine; and Maghrebi Arabic, spoken across Morocco, Algeria, and Tunisia. These are not minor accent differences. The dialects differ in vocabulary, grammar, sentence structure, and even script conventions. A model trained on "Arabic" data that is actually 60 percent Egyptian dialect and 30 percent MSA and 10 percent Gulf will perform well for Egyptian users, poorly for Gulf users, and unpredictably for Moroccan users -- even though all of them expect "Arabic" support.

The same problem appears in Chinese (Simplified vs Traditional, plus regional written conventions), Hindi (Devanagari vs Romanized, plus Urdu overlap), Spanish (Latin American vs Iberian, with significant vocabulary differences), and Portuguese (Brazilian vs European). For Tier 4 languages, the dialect problem is often worse because there is less standardization to begin with. Yoruba written in Lagos follows different conventions than Yoruba written in Ibadan, and both differ from Yoruba written by the diaspora community in London.

Detection requires metadata at the collection stage. Every scraped document should be tagged not just with "Arabic" but with the best available estimate of which variety of Arabic it represents -- based on the source URL's geographic metadata, the vocabulary patterns in the text, or explicit markers like country-specific news sites. If you cannot tag at the dialect level, at minimum separate formal written text from informal text, because the formal-informal distinction often maps loosely to the MSA-dialect distinction.

The consequence of ignoring dialect contamination is a model that speaks a language variety that does not exist -- a statistical average of five dialects that no native speaker actually uses. Users perceive this as the model "speaking Arabic but sounding weird," which is more damaging to trust than the model admitting it does not support their specific dialect.

## The Wikipedia Trap

Wikipedia is the single most commonly used source for low-resource language text in NLP research. It is also one of the most dangerous.

For Tier 1 and Tier 2 languages, Wikipedia is a reasonable component of a larger data pipeline. English Wikipedia is genuinely written by millions of native speakers, covers enormous topical breadth, and is actively maintained for quality. German, French, and Japanese Wikipedias are similarly robust.

For Tier 3 and Tier 4 languages, Wikipedia quality collapses. A 2025 MIT Technology Review investigation documented what researchers call a "doom spiral" in low-resource language Wikipedias: articles are often written by non-native speakers, or machine-translated from English using tools like Content Translation, which has produced over two million translated Wikipedia articles since its launch. These translated articles contain systematic errors -- unnatural sentence structures, vocabulary choices that are technically correct translations of English words but that no native speaker would use, missing diacritical marks that change meaning. The Yoruba Wikipedia, for example, was found to contain large volumes of text that lacked the tonal diacritics essential to Yoruba, rendering words ambiguous or meaningless to actual Yoruba readers.

Here is the trap: AI models trained on this Wikipedia text learn its errors. They reproduce the unnatural sentence structures, the missing diacritics, the vocabulary choices that native speakers would never make. Then some of that model output ends up back on the web, gets scraped in the next round of data collection, and reinforces the same errors. A 2024 audit of low-resource Wikipedia data quality found that the Twi Wikipedia contained articles that were incomprehensible to native Twi speakers and required manual correction before they could serve any NLP purpose. Creole language Wikipedias showed similar patterns -- sentences that were structurally English with Creole vocabulary substituted in.

The solution is not to avoid Wikipedia entirely but to treat it as unverified data that requires native speaker validation before inclusion in any training set. For Tier 4 languages, assume that Wikipedia text is guilty until proven innocent. Run it through native speaker review with explicit instructions to flag unnatural phrasing, missing diacritics, and non-native vocabulary choices. The clean subset that survives review is genuinely useful. The rest is poison.

## The Consent and Ethics Challenge

Data collection for low-resource languages raises ethical questions that high-resource language collection rarely confronts, because the power dynamics are fundamentally different.

When you scrape English web text, you are collecting data from a diffuse, global community of hundreds of millions of writers. No single community bears a disproportionate burden. When you collect Yoruba text, you are extracting value from a specific, identifiable community -- often one that is economically marginalized and has limited bargaining power in the global AI economy. The Yoruba-speaking community did not create their language resources so that Silicon Valley companies could train commercial models. Using those resources without meaningful consent or reciprocal benefit is extractive in a way that the AI industry has been slow to reckon with.

The concrete risks are not hypothetical. Community-generated content on forums, social media, and blogs in low-resource languages is often created by small, tight-knit communities where pseudonymity is thin. Scraping this content for training data can expose individuals in ways that scraping English Reddit does not, because the community is small enough that writing style alone can identify the author. Language preservation projects -- recordings, transcriptions, oral histories -- carry cultural significance that goes beyond the linguistic content. Using a community's oral history transcriptions to train a commercial chatbot without the community's explicit consent is not just an ethical gray area. In many Indigenous contexts, it violates community governance norms around who has the right to share, use, and profit from cultural knowledge.

The emerging standard, developed through initiatives like Masakhane and the First Peoples' Cultural Council, is community-controlled data governance. The language community sets the terms of use for data collected from or about their language. They define which uses are permitted, which are restricted, and what compensation is required. This slows down data collection -- you cannot simply scrape and go. But it produces data that is legally defensible, ethically sound, and practically superior because the community's involvement improves quality at every stage.

## Building a Collection Pipeline That Scales

A functioning low-resource language data pipeline combines multiple strategies layered on top of each other, with quality checks between each layer.

Start with whatever institutional data exists -- government archives, university corpora, religious parallel texts. This forms your high-confidence base layer. Tag it with source metadata and domain labels. Run it through deduplication and basic quality filtering.

Layer on web-scraped data with aggressive quality filters. Use the institutional base layer to calibrate your language ID model for the target language -- fine-tune a language identifier on known-good examples so it can more accurately identify your target language in web crawls. Accept that the volume from web scraping will be small for Tier 4 languages and plan accordingly.

Layer on community-collected data to fill domain gaps. If your institutional data is all legal and religious text, commission community writers to produce conversational, medical, commercial, and educational text. Pay fairly. Establish clear data governance agreements.

Finally, augment with synthetic data using the techniques covered in the next subchapter. Use your organic data as the style anchor for synthetic generation. Validate every synthetic example against native speaker judgment.

The total volume for a Tier 4 language might be modest -- perhaps ten thousand to fifty thousand clean, validated examples across all domains. That is enough for meaningful fine-tuning when combined with cross-lingual transfer from related Tier 2 or Tier 3 languages. It is not enough for training from scratch. Accepting this limitation and designing your fine-tuning strategy around it -- rather than pretending you can collect your way to abundance -- is what separates teams that ship real multilingual products from teams that announce multilingual support and silently deliver English with a translation layer.

The next subchapter explores how synthetic data generation can extend these organic collections, the specific failure modes of synthetic text in low-resource languages, and why synthetic data is a supplement to real data rather than a replacement for it.
