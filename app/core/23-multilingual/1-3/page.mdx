# 1.3 — The Performance Gap: Why Models That Excel in English Fail Everywhere Else

Large language models do not perform equally across languages. This is not a nuance. It is the central technical fact of multilingual AI, and it determines everything you can and cannot build for non-English users. On the MMLU-ProX benchmark, which covers 29 languages with 11,829 identical questions per language, the performance gap between English and the lowest-performing language exceeds 24 percentage points for state-of-the-art models. The best-performing model in that evaluation, Qwen 2.5 72B, scored 70.3% in English and 40.1% in Swahili — a 30-point collapse. These are not fringe models tested on obscure tasks. These are frontier-class systems evaluated on standard academic knowledge. If a 30-point accuracy drop existed between two features of your product, you would call it a critical defect. When it exists between two languages, most teams do not even measure it.

The gap is not a temporary limitation that the next model release will fix. It is a structural consequence of how language models are trained, what data they learn from, and how evaluation has been designed. Understanding why the gap exists — and how it varies across languages — is the foundation of every multilingual decision you will make.

## The Training Data Imbalance

The root cause of the performance gap is training data distribution. Language models learn from text. The amount of text available online varies enormously by language, and models learn the patterns of well-represented languages more deeply than those of underrepresented ones.

English dominates the internet. Estimates vary, but English accounts for roughly 55 to 60% of all web content, despite representing only about 25% of internet users. This means the training corpora for large language models are overwhelmingly English. Common Crawl, the largest publicly available web corpus and the foundation of most pretraining datasets, is approximately 46% English. The next largest languages — Chinese, German, Japanese, French, and Spanish — each represent between 3% and 7%. Languages like Hindi, Arabic, Swahili, Yoruba, Bengali, and Thai each represent less than 1%. Some languages spoken by tens of millions of people have almost no web presence at all.

When a model trains on this distribution, it develops deep, nuanced representations of English. It learns English grammar, idioms, cultural references, domain-specific vocabulary, and stylistic variations across registers. For German or French, it learns a reasonably good but less complete representation. For Hindi or Arabic, the representation is shallower — the model has seen fewer examples of complex reasoning, fewer examples of domain-specific text, fewer examples of natural conversational patterns. For Swahili or Yoruba, the model has seen so little training data that its outputs are often grammatically plausible but semantically hollow.

The imbalance is not purely about volume. It is about diversity within each language. English training data includes academic papers, legal documents, medical records, software documentation, creative fiction, social media posts, news articles, product reviews, and government communications. This diversity means the model has seen English used in virtually every context. For many non-English languages, the available training data is concentrated in a few domains — news articles and Wikipedia, primarily — which means the model has seen the language used in formal, informational contexts but has limited exposure to conversational, technical, legal, or creative use.

This diversity gap explains why models can produce grammatically correct text in a low-resource language while still failing badly on real tasks. The model knows how Hindi sentences are structured. It does not know how a Hindi-speaking doctor discusses a diagnosis with a patient, how a Hindi-speaking lawyer drafts a contract clause, or how a Hindi-speaking customer service agent de-escalates a complaint. The grammatical competence is there. The domain competence is not.

## The Gap Is Not Uniform

Not all non-English languages suffer equally. The performance gap follows a predictable pattern tied to language family, script, morphological complexity, and training data availability. Understanding this pattern is essential for prioritizing your multilingual investments.

European languages that share Latin script and significant structural similarity with English see the smallest drops. French, German, Spanish, Italian, Portuguese, and Dutch typically show performance declines of 3 to 8 percentage points compared to English on standard benchmarks. These languages benefit from large training corpora, shared vocabulary with English through Latin and French roots, and structural similarities that allow positive transfer during training. A model that excels in English will be reasonably competent in French without any French-specific optimization.

East Asian languages — Chinese, Japanese, and Korean — see moderate to significant drops, typically 8 to 15 percentage points. These languages use non-Latin scripts, have fundamentally different grammatical structures from English, and require different tokenization strategies. Chinese is better represented in training data than Japanese or Korean, which means Chinese performance is often closer to European languages while Korean can show gaps closer to 12 to 15 points. Japanese presents a unique challenge because it uses three writing systems simultaneously — hiragana, katakana, and kanji — and the model must handle all three correctly in context.

Arabic, Hindi, Bengali, and other South Asian and Middle Eastern languages see larger gaps, typically 10 to 18 percentage points. Arabic's right-to-left script, morphological richness, and significant dialectal variation create compounding challenges. Hindi and Bengali are underrepresented in training data relative to their speaker populations. These languages also lack the positive transfer effects that European languages enjoy — their structures are different enough from English that the model cannot easily generalize.

African languages — Swahili, Yoruba, Igbo, Amharic, Zulu — see the most severe gaps, often 20 to 30 percentage points or more. These languages have minimal representation in web training data. The MMLU-ProX results showed Swahili trailing English by 30 points even for the best-performing model. For less capable models, the gap is even larger. In practical terms, a model that achieves expert-level performance in English may function at barely-passing or failing levels in these languages.

## What The Language Tier System Looks Like

The performance distribution creates what practitioners informally call **The Language Tier System**. This framework, which Chapter 2 covers in full technical detail, segments languages into four tiers based on the quality you can realistically expect from current frontier models.

**Tier 1** languages are those where model performance is near-English quality. This tier includes French, German, Spanish, Italian, Portuguese, Dutch, and Mandarin Chinese. For these languages, frontier models like GPT-5, Claude Opus 4.5, and Gemini 3 Pro produce outputs that are fluent, contextually appropriate, and usable for production applications with standard quality assurance. The gaps are real but manageable — you might see slightly lower accuracy on domain-specific tasks, occasional awkward phrasing, and some cultural-reference failures, but the baseline quality is high enough that your multilingual product can launch without fundamental rearchitecting.

**Tier 2** languages show a moderate performance gap. Japanese, Korean, Russian, Polish, Turkish, and Thai fall into this tier. The model produces grammatically correct output most of the time, but domain-specific accuracy, idiomatic expression, and register control are noticeably weaker than in English. Production deployment is feasible but requires language-specific evaluation, prompt tuning, and quality monitoring that you do not need for Tier 1 languages.

**Tier 3** languages have a significant performance gap. Arabic (particularly dialectal Arabic), Hindi, Bengali, Vietnamese, Indonesian, and Farsi sit in this tier. The model can generate text in these languages, but accuracy drops enough that domain-specific tasks become unreliable without significant quality investment. Safety filters, factual accuracy, and cultural appropriateness degrade noticeably. You need native-language evaluation, native-language annotators, and language-specific prompt architecture to achieve acceptable quality.

**Tier 4** languages are those where current models are barely functional. Swahili, Yoruba, Igbo, Amharic, Hausa, Zulu, Khmer, Lao, and most indigenous languages fall here. The model may produce text that looks like the target language but contains fundamental errors in grammar, meaning, or cultural context. Production deployment in these languages requires either significant fine-tuning, specialized multilingual models, or hybrid approaches that combine model output with human review.

The tier system is not static. It shifts with every new model generation. Languages that were Tier 3 in 2024 may be Tier 2 in 2026 as training data improves and multilingual-focused models like Aya Expanse and BLOOMZ expand coverage. But the shift is slow, and the tier your target language sits in today determines the investment you need to make today.

## What Failure Actually Looks Like

Benchmark numbers describe the gap abstractly. What does the gap look like when a real user encounters it?

The most visible failure is wrong answers presented with high confidence. A model that scores 95% on English medical questions and 68% on Hindi medical questions is not just "less accurate" in Hindi. It is producing incorrect medical information in Hindi with the same confident tone it uses for correct English answers. The user has no signal that the answer is wrong. The model does not hedge more in Hindi than in English. It does not say "I am less certain about this because my Hindi training data was limited." It states incorrect facts with the same fluency and authority it uses for correct ones. In high-stakes domains — healthcare, legal, financial — this is not a quality problem. It is a safety problem.

The second failure mode is wrong cultural context. A model asked to write a business email in Japanese might produce text that is grammatically correct but uses casual register where formal register is required. A model generating a wedding invitation in Arabic might mix dialects inappropriately. A model writing marketing copy in Brazilian Portuguese might use vocabulary and phrasing that are natural in European Portuguese but sound foreign to Brazilian readers. These failures do not show up in accuracy benchmarks. They show up in user trust, which erodes silently and permanently.

The third failure mode is hallucinated facts that are plausible in English but nonsensical in the target language. A model might reference a legal precedent that exists in English common law but has no equivalent in Japanese civil law. It might cite a medical guideline published by a US institution that contradicts the standard of care in India. It might suggest a business practice that is normal in the United States but illegal or culturally offensive in Saudi Arabia. The model's English-centric training makes it confident about English-world facts, and it projects that confidence onto other cultural contexts where the facts do not apply.

The fourth failure mode is format and structure errors. Arabic and Hebrew require right-to-left text rendering. Some models handle this correctly in simple outputs but break down when mixing right-to-left text with left-to-right elements like numbers, URLs, or English brand names. Chinese and Japanese do not use spaces between words, which affects how the model handles tasks like keyword extraction or text summarization. Thai lacks clear word boundaries, which creates challenges for tokenization and downstream tasks. These are not exotic edge cases. They are fundamental properties of the languages, and models that do not handle them correctly produce outputs that are immediately recognizable as machine-generated.

## The Benchmark Problem

The performance gap you measure depends on how you measure it. Most widely used benchmarks are English-first: designed in English, written by English speakers, reflecting English-world knowledge. When these benchmarks are translated into other languages, the translation introduces artifacts that inflate performance estimates.

Translated benchmarks overestimate multilingual quality for several reasons. First, the questions assume English-world context. A history question about the American Civil War, translated into Thai, tests whether the model knows American history in Thai — not whether the model knows Thai history. The model might score well because it learned the English answer during training and can pattern-match to the Thai translation. This tells you nothing about the model's ability to handle Thai-native knowledge tasks.

Second, translated questions often have simpler linguistic structure than native questions. Translation tends to flatten complex sentence structures, reduce ambiguity, and normalize vocabulary. A question that is challenging in native Thai because of a subtle wordplay or cultural reference becomes straightforward in the translated version because the translation stripped out the difficulty. The model scores well on the translated version and poorly on a natively written equivalent.

The INCLUDE benchmark, presented at ICLR 2025, was designed to address exactly this problem. INCLUDE contains over 197,000 multiple-choice questions in 44 languages and 15 scripts, all written natively by speakers of each language. The questions are drawn from local academic exams, professional licensing tests, and occupational certifications — not translated from English sources. This means the questions test knowledge that is native to each language and culture. A Thai question tests Thai legal knowledge. A Nigerian question tests Nigerian medical practice. A Japanese question tests Japanese business conventions.

The results from INCLUDE and similar natively constructed benchmarks consistently show larger performance gaps than translated benchmarks. GPT-4o, one of the best-performing models on INCLUDE at the time of its evaluation, achieved an average accuracy of approximately 77% across all domains — strong, but with significant variation by language. Low-resource languages and questions requiring culturally specific knowledge showed the steepest drops. The lesson is clear: if you are evaluating your model's multilingual capabilities using translated benchmarks, you are overestimating its real-world performance. You are measuring the model's ability to answer English questions in other languages, not its ability to handle native tasks.

## Safety Filters Degrade Across Languages

The performance gap extends beyond accuracy into safety. This is the finding that should alarm you most.

Safety training — the RLHF, Constitutional AI, and red-teaming processes that teach models to refuse harmful requests — is conducted primarily in English. The refusal examples, the harmful content definitions, the adversarial test cases that shape the model's safety behavior are overwhelmingly in English. The result is that safety filters are significantly less effective in non-English languages.

Research published in 2025 documented the scale of this failure. Non-English prompts bypassed safety measures 60 to 80% more often than equivalent English prompts. Low-resource languages showed approximately three times the likelihood of encountering harmful content compared to high-resource languages. Researchers demonstrated that simply translating a harmful English prompt into a low-resource language was often sufficient to bypass safety filters entirely. More sophisticated attacks using code-mixed language — blending English words with Hindi phonetics, for example — achieved attack success rates as high as 99% for text generation.

The practical implication is severe. If your AI product includes safety filters — content moderation, harmful content detection, prompt injection defense — those filters are likely 90% effective in English and as low as 40 to 50% effective in Arabic, Hindi, or Swahili. Your users in those languages are exposed to harmful outputs at rates that would be unacceptable in English. And because your safety monitoring is likely English-centric, you may not detect the exposure until a public incident forces the issue.

This is not a theoretical risk. It is an active area of exploitation. Adversaries have discovered that multilingual prompts are a reliable jailbreak vector. If your product is deployed in multiple languages, your attack surface for prompt injection and safety bypass is proportional to the number of languages you serve — and your defense is weakest in exactly the languages where your users are most vulnerable.

## Practical Implications for Your System

The performance gap creates a cascading set of practical implications that most teams discover only after deployment.

Your 95% accuracy in English might be 71% in Thai. This is not a hypothetical. It is a direct extrapolation from benchmark data. If your English eval suite shows 95% accuracy and you have not measured Thai performance, you should assume a 15 to 25 percentage point drop for Tier 3 languages. The gap might be smaller for your specific task. It might be larger. You do not know until you measure, and measuring requires Thai-native test cases, not translated English ones.

Your latency model changes across languages. Languages with complex morphology — Finnish, Turkish, Hungarian, Arabic — require more tokens per semantic unit than English. A sentence that takes 20 tokens in English might take 35 tokens in Finnish because Finnish expresses grammatical relationships through suffixes that the tokenizer splits into multiple tokens. More tokens means more compute, higher latency, and higher cost. Your per-query cost model built on English token counts underestimates the cost of serving morphologically rich languages by 30 to 75%.

Your error handling breaks. Error messages, fallback responses, clarification prompts — all the scaffolding around your model's primary output — were probably written in English. When the model fails in Japanese, does the error message appear in Japanese or English? When the model asks for clarification in Arabic, is the clarification request grammatically correct and culturally appropriate? Most teams have not tested these paths in non-English languages because they tested the happy path in English and assumed the error paths would generalize.

Your monitoring is blind. If your quality monitoring uses English-language heuristics — keyword detection, sentiment analysis, toxicity scoring — those heuristics do not transfer reliably to other languages. A toxicity detector trained on English text will miss toxic content in Arabic because the linguistic patterns of toxicity are different. Your monitoring dashboards will show green for Arabic quality while Arabic users encounter toxic outputs.

## What Teams Should Do

The performance gap is real, it is significant, and it is not going away in the next model generation. Here is what it means for your practice.

First, measure every language you serve with language-native evaluations. Do not translate your English eval suite and call it multilingual evaluation. Build or acquire test sets that were written by native speakers in each target language, covering tasks that are native to that language's domain and cultural context. The INCLUDE benchmark is one resource. Language-specific professional exam datasets are another. Custom eval sets built by native-speaking annotators for your specific task are the gold standard.

Second, establish per-language quality baselines before you launch. Run your system against native eval sets for every language you intend to serve. Document the performance gap. Make a deliberate decision about whether that gap is acceptable for your use case. A 5-point drop from English for a casual content suggestion tool might be acceptable. A 15-point drop for a medical information system is not. The decision must be explicit and documented, not discovered in production.

Third, invest in language-specific prompt optimization. The prompt that maximizes English quality is not the prompt that maximizes Japanese quality. Different languages respond to different instruction structures, different example formats, different levels of explicitness. Treat prompt optimization as a per-language activity, not a one-size-fits-all activity.

Fourth, test your safety filters in every language you serve. Run adversarial evaluations in each language. Test for harmful content generation, safety bypass via multilingual prompts, and culturally inappropriate outputs. If your safety filters are significantly weaker in a language, either strengthen them before launch or do not launch in that language until they are adequate.

Fifth, build your monitoring to disaggregate by language. Aggregate quality metrics hide language-specific degradation. If your overall accuracy is 90% but Thai accuracy is 68%, the aggregate number gives you false confidence. Disaggregate every quality metric, every safety metric, and every latency metric by language. Set per-language alerts. Treat a quality drop in Korean with the same urgency as a quality drop in English.

The performance gap is the technical foundation of every challenge in multilingual AI. Understanding where your target languages sit, what the gap means for your use case, and how to measure it accurately is the prerequisite for everything that follows. But the gap is not purely about model capability. The next subchapter examines the dimension that benchmarks cannot capture: the cultural gap, where technically correct output is culturally wrong, and why that distinction matters more than accuracy scores.
