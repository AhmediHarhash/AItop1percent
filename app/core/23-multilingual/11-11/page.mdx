# 11.11 — The Multilingual Fine-Tuning Decision Tree

Every multilingual fine-tuning project starts with the same question and most teams answer it too quickly. The question is whether you need to change the model at all. The answer is often no — but discovering that requires a systematic process, not a gut feeling. This subchapter synthesizes every technique, trade-off, and failure mode covered in Chapter 11 into a single decision framework. Follow the steps in order. Each step either resolves the problem or advances you to the next decision point. The teams that succeed at multilingual fine-tuning are the ones that spend more time in the early steps — ruling out simpler approaches — and less time jumping to the expensive ones.

## Step 1 — Is Prompting Sufficient?

This is the first gate, and it eliminates the majority of fine-tuning projects that should never have started.

Run your full eval suite with optimized prompts in every target language. Not a quick spot check. Not a demo to stakeholders. A rigorous evaluation using the four dimensions described in subchapter 11.9 — task accuracy, fluency, safety, and regression baseline — across every language you intend to support. The prompts should be fully optimized: explicit language and register instructions, few-shot examples in the target language, retrieval of language-matched context, and chain-of-thought reasoning in the target language where applicable.

If all languages meet your quality threshold across all four dimensions, stop. You do not need fine-tuning. This is not a consolation prize. This is the best outcome. You have achieved your quality goals with the lowest-cost, lowest-maintenance, most flexible approach available. When the base model updates in three months, your optimized prompts will likely work as well or better with the new model. You have no forgetting risk, no training pipeline to maintain, no per-language adapters to manage. Ship it.

The mistake teams make at this step is not being thorough enough. They test with two or three example inputs per language, see decent results, and move on — or they test only their primary language and assume the others are similar. Invest the time here. Two days of careful prompting and evaluation across all languages can save eight weeks of fine-tuning effort. The prompt ceiling measurement described in subchapter 11.1 — testing progressively more sophisticated prompting strategies and measuring when the quality curve flattens — is the tool that makes this step rigorous.

## Step 2 — Which Languages Fail and by How Much?

If step 1 reveals that some languages do not meet your quality threshold, the next question is precision: which languages, on which dimensions, and by how many points?

Create a gap table. Each row is a language. Each column is a dimension. Each cell contains the difference between your quality threshold and the model's performance with optimized prompts. A cell showing negative five means the model falls five points below your threshold on that dimension in that language. A cell showing positive three means the model exceeds your threshold by three points.

The gap table reveals the shape of your problem. If only two languages fall short and only on task accuracy, you have a narrow problem. If seven languages fall short across multiple dimensions, you have a broad problem. If the gaps are small — two to five points below threshold — they might be closable with data-efficient techniques or further prompt optimization. If the gaps are large — fifteen or twenty points — you need more aggressive intervention.

The gap table also reveals which languages are close enough to potentially fix with minimal effort and which require significant investment. A language sitting two points below threshold on fluency might be fixable with ten hours of native-speaker prompt refinement. A language sitting eighteen points below threshold on task accuracy needs thousands of training examples and weeks of work. Knowing the difference before you commit resources prevents the most common waste in multilingual projects: treating every failing language as the same size problem.

## Step 3 — Is the Gap Closable with Fine-Tuning?

Not every quality gap can be closed by fine-tuning. Some gaps are architectural, baked into the model at a level that task-specific training cannot fix. Identifying these unclosable gaps before you invest in fine-tuning saves enormous effort.

The first architectural check is **tokenizer fertility**. Tokenizer fertility measures how many tokens the model's tokenizer needs to represent text in a given language. A fertility of 1.0 means each word maps to roughly one token. English typically has fertility between 1.1 and 1.3 for most modern tokenizers. High-resource European languages range from 1.2 to 1.8. Languages with complex morphology or scripts underrepresented in the tokenizer's training data can have fertility of 3.0 to 5.0 or higher — meaning the model needs three to five tokens to represent what English represents in one.

High tokenizer fertility has cascading consequences. The model's effective context window shrinks, because each word consumes more tokens. Inference cost per word increases proportionally. Most importantly, the model's ability to process and generate the language degrades, because it is operating on subword fragments rather than meaningful units. Research from 2025 showed that applying English-centric tokenizers to multilingual models results in performance degradation of up to 68 percent on downstream tasks for the most affected languages.

If your target language has fertility above 3.0 on the model you intend to fine-tune, fine-tuning will have limited impact. You are trying to teach a model to work with a language whose text it cannot even tokenize efficiently. In this case, consider switching to a model with better tokenizer coverage for that language. Qwen models, for instance, use a tokenizer with strong CJK and Southeast Asian coverage. Gemma and Gemini models have improved multilingual tokenizer design that reduces fertility gaps. The right answer for some languages is not "fine-tune this model" but "use a different model that was designed for this language."

The second architectural check is pretraining data coverage. If the base model saw almost no text in your target language during pretraining, fine-tuning with a few thousand examples cannot compensate for what the model never learned. Fine-tuning adjusts existing knowledge — it does not create knowledge from nothing. For languages where the base model has minimal pretraining coverage, the realistic path is either a much larger fine-tuning dataset (tens of thousands of examples rather than a few thousand), a different base model with better coverage, or routing to a specialized model for that language.

## Step 4 — Do You Have Data?

Data availability determines which fine-tuning techniques are available to you.

For each language where fine-tuning is warranted, assess your data situation honestly. You need task-specific examples in the target language — inputs paired with the outputs you want the model to produce. Where does that data come from?

**Organic data** is the gold standard. If your product is already in production in that language, you may have logs of user inputs and human-generated or human-reviewed outputs that can be cleaned and formatted as training data. A customer service platform with six months of Thai conversation logs, reviewed by Thai-speaking agents, has a valuable training corpus. The challenge is volume — you typically need at minimum 1,000 to 5,000 high-quality examples per language for LoRA fine-tuning to show reliable improvement, and 10,000 or more for full fine-tuning.

**Translated data** is the most common shortcut and the most common source of quality problems. Taking your English training data and translating it into target languages gives you volume quickly but introduces translation artifacts — unnatural phrasing, incorrect register, terminology that is technically correct but not what a native speaker would use. Translated training data produces models that generate translated-sounding output, which users perceive as lower quality even when the factual content is correct. If you use translated data, budget for native-speaker review and correction of at least 20 to 30 percent of the examples, and expect that fluency eval scores will be lower than if you had used native-authored data.

**Synthetic data** — generated by a frontier model — offers a middle path, as covered in subchapter 11.5. The quality depends on the frontier model's capability in the target language, which brings us back to the teacher quality gap discussed in the distillation subchapter. Synthetic data is viable for languages where the frontier model is strong and risky for languages where it is weak. Verify before you synthesize.

If organic data is insufficient and neither translation nor synthesis can produce quality training data for a specific language, that language is not a good candidate for fine-tuning. Return to step 1 and determine whether further prompt optimization, a different base model, or a routing strategy can serve that language instead.

## Step 5 — Which Technique?

The gap size, data availability, and your infrastructure constraints determine the right fine-tuning technique.

**LoRA adapters** are the right choice for surgical improvements — closing a five-to-fifteen-point gap in task accuracy for a specific language, adapting register or formality, or adding domain terminology. LoRA trains a small number of additional parameters (typically 0.5 to 2 percent of the model's total) and leaves the base model's weights frozen. This minimizes forgetting risk and allows per-language adapters that can be swapped at inference time. LoRA is also the fastest technique to iterate on — training runs take hours instead of days, and you can experiment with different configurations at low cost.

**Full fine-tuning** is warranted when the gap is large — more than fifteen points — and the model needs deep adaptation to a language or domain. Full fine-tuning updates all model weights, which means it has the highest capacity to learn new patterns but also the highest risk of catastrophic forgetting. For multilingual systems, full fine-tuning requires interleaving data from all supported languages to prevent the model from losing capability in languages not included in the training set. This is the most expensive and highest-risk technique, reserved for cases where LoRA's constrained parameter updates are insufficient to close the quality gap.

**Distillation** is the right choice when the quality gap is not in the frontier model but in the production model. If GPT-5 handles your language perfectly but your production model is a 7B parameter model that cannot match GPT-5's quality through prompting alone, distillation transfers the frontier model's behavior to your smaller model. The previous subchapter covers this technique in detail.

**Model merging** is worth considering when you have multiple language-specialized models and want to combine their capabilities into one. If you fine-tuned one LoRA adapter for Japanese legal text and another for Korean legal text, merging techniques like TIES or DARE can combine them into a single model or adapter that handles both, as discussed in subchapter 11.7.

**Routing to a specialized model** is not a fine-tuning technique, but it belongs in this decision step because it is sometimes the right answer. If Thai performance requires a Thai-specialized model and Korean performance requires a Korean-specialized model, you might be better served by routing Thai requests to one model and Korean requests to another rather than trying to build one model that does both. This trades training complexity for serving complexity, and the trade-off is often favorable when languages have very different requirements.

## Step 6 — What Is Your Forgetting Budget?

Before you begin training, define how much regression you can tolerate in languages you are not fine-tuning for. This is your **forgetting budget**, and it must be a number, not a vague commitment to "minimize forgetting."

A forgetting budget of two percentage points means you will accept up to a two-point decline in any non-target language on any evaluation dimension. A budget of zero means no regression whatsoever — the fine-tuned model must match or exceed the base model on every language and every dimension. A budget of five points means you are willing to trade moderate quality in some languages for strong improvement in your target languages.

The forgetting budget determines your training configuration. A zero-point budget requires careful techniques: low learning rates, heavy regularization, LoRA with small rank, interleaved multilingual data, and extensive per-language evaluation at every checkpoint. It also means more training experiments and more evaluation cycles, because the constraint is tight. A five-point budget allows more aggressive training — higher learning rates, more target-language data, fewer constraints — because you have accepted that some regression is the price of stronger target-language performance.

Most production multilingual systems operate with a forgetting budget between two and three points. This provides enough room for the model to learn new target-language patterns without requiring the extreme conservatism of a zero-point budget, while keeping regression below the level where users would notice degradation. Your specific budget should be set by the product team based on user impact analysis: which languages have users who are most sensitive to quality changes, and what is the cost of a three-point decline in those languages?

## Step 7 — What Is Your Eval Plan?

Define your per-language gating criteria before you start training. Not after. Not during. Before.

The eval plan specifies, for each language: the evaluation sets, the metrics, the absolute quality floors, the regression thresholds, the LLM-as-judge configuration and its calibration status, and the human evaluation protocol for dimensions where automated evaluation is insufficient. All of this was covered in subchapter 11.9, but the decision tree places it here because the eval plan must be ready before training begins. Teams that define their eval plan after training has started are biased toward finding reasons to ship whatever the training produced, rather than holding the model to standards set before they had a result to justify.

The eval plan also specifies the abort criteria — under what conditions do you stop the fine-tuning project entirely rather than continuing to iterate? If three rounds of training fail to close the gap for a language, is the fourth round justified? If the model consistently trades quality between languages — improving Japanese while degrading Korean, then improving Korean while degrading Japanese — is that a sign that your approach is wrong? Define these abort criteria in advance, because in the middle of a project, momentum and sunk cost make it hard to walk away.

## Common Paths Through the Tree

Most multilingual teams land on one of a small number of paths through this decision framework.

**The prompting path.** The team evaluates their target languages with optimized prompts and discovers that the frontier model — GPT-5, Claude Opus 4.6, Gemini 3 Pro — meets quality thresholds across all languages. They ship with prompting only, invest in monitoring, and revisit the decision when they add new languages or change quality requirements. This is the most common path for teams supporting high-resource languages with a frontier model budget.

**The selective LoRA path.** Prompting works for six of eight languages but falls short in two. The team fine-tunes LoRA adapters for the two failing languages, evaluates all eight languages against the gating criteria, and deploys with per-language adapter routing. This is the most common path for teams that include one or two lower-resource languages alongside primarily high-resource ones.

**The distillation path.** The frontier model meets quality thresholds across all languages, but the cost or latency makes frontier inference unsustainable. The team distills to a smaller model, verifies per-language quality, and deploys the student. This is the standard path for high-volume, latency-sensitive applications like search, chat, and real-time translation.

**The routing path.** The team discovers that no single model can serve all their languages at acceptable quality within their budget. They segment languages into groups — each served by the model best suited for that group — and build a routing layer that directs requests to the appropriate model. This is common for teams supporting ten or more languages that span high-resource European languages, CJK languages, and lower-resource Southeast Asian or African languages.

**The hybrid path.** The team uses prompting for their best-served languages, LoRA fine-tuning for languages that need a small boost, distillation for cost optimization on high-volume languages, and routing to specialized models for languages where no general model performs well. This is the path taken by teams with the most mature multilingual infrastructure, and it is the path most teams should aspire to as they scale.

## The Abort Branches

Sometimes the right decision is to stop and choose a fundamentally different approach.

**Abort: tokenizer fertility too high.** If the target language has tokenizer fertility above 3.5 on your chosen base model, fine-tuning is unlikely to produce acceptable results. Switch to a model with better tokenizer coverage for that language before investing in fine-tuning.

**Abort: no teacher produces quality output.** If no available frontier model produces output that native speakers rate above 70 percent quality in the target language, distillation will not work and fine-tuning from synthetic data will not work. You need either a dedicated model trained on that language from pretraining, or you need to accept that the language is not yet supportable at your quality standard and communicate that honestly to stakeholders.

**Abort: forgetting is intractable.** If every training configuration you try improves the target languages while degrading others beyond your forgetting budget, and you have exhausted techniques like LoRA, low learning rates, and interleaved multilingual data, the model may not have enough capacity to serve all your languages simultaneously. The solution is a routing architecture that uses separate models for separate language groups rather than forcing one model to do everything.

**Abort: data quality is unfixable.** If your training data in the target language is too noisy, too translated-sounding, or too small to improve after multiple rounds of cleaning and augmentation, the fine-tuning project will not produce a deployable model. Invest in data collection before investing in training. As covered in subchapter 11.4, better data beats more training every time.

## The Principle That Holds It All Together

After working through this decision tree, the most experienced multilingual teams converge on the same insight: the best multilingual fine-tuning decision is often to fine-tune less than you think you need.

The instinct when quality is poor in a language is to reach for the heaviest tool available. Full fine-tuning, thousands of examples, weeks of compute. But the heaviest tool carries the most risk — forgetting, maintenance burden, training pipeline complexity, per-language evaluation overhead. The practitioners who get the best results start with prompting, prove it is insufficient through measurement, apply the lightest effective technique, and evaluate every language before shipping. They fine-tune surgically, they distill strategically, and they route pragmatically.

They also know when to wait. A language that requires painful fine-tuning on the current model generation might be well-served by prompting on the next generation. The model ecosystem is moving fast. Every six months, frontier models improve their multilingual coverage. A language that was Tier 3 in 2024 might be Tier 2 by 2026. The team that waited six months and re-evaluated before fine-tuning sometimes finds that the problem solved itself through model improvement — and that patience, backed by measurement, was the cheapest solution of all.

The next chapter moves from model-level decisions to the operational reality of running multilingual AI at scale — compliance across jurisdictions, cost modeling per language, and the organizational structures that determine whether your multilingual system succeeds or slowly collapses under its own complexity.
