# 9.6 â€” Gender and Pronoun Handling Across Grammatically Gendered Languages

English has a relatively simple gender system. He, she, they. A noun is a noun -- "doctor," "teacher," "friend" -- without inherent gender. Most of the world's major languages are far more complex. French assigns gender to every noun: a table is feminine, a desk is masculine. Arabic conjugates verbs differently depending on the gender of the subject, the object, and the person being addressed. Hindi changes verb endings based on the gender of the subject and sometimes the object. Spanish adjectives shift form: "cansado" for a tired man, "cansada" for a tired woman. Japanese has multiple first-person pronouns -- watashi, boku, ore, atashi -- each carrying different gender and formality signals. These are not edge cases. They are how most human languages work. And they create a set of design problems that English-speaking AI teams rarely encounter until a gendered language exposes them.

Every sentence your model generates in a grammatically gendered language forces a gender decision. Not sometimes. Every sentence. In English, "The doctor will see you now" is gender-neutral. In French, the model must choose between "Le medecin" (masculine, the traditional default) and "La medecine" (feminine). In Spanish, between "El doctor" and "La doctora." In Arabic, the verb conjugation itself reveals the assumed gender. In Hindi, the verb ending changes. The model cannot produce grammatically correct output in these languages without either knowing the gender of the person being discussed or choosing a default. And every default encodes an assumption.

## The Default Masculine Problem

When a model does not have gender information about a person being discussed, it must still generate grammatically correct output in gendered languages. Across virtually all gendered languages, the default is masculine. "A doctor" becomes the masculine form. "Someone" takes masculine agreement. "The user" is treated as male until proven otherwise.

This is not a bug in AI systems. It reflects a grammatical convention that has existed in these languages for centuries: the masculine form serves as the "unmarked" or generic form, used when gender is unknown or when referring to mixed groups. But in AI systems, the convention has consequences that go beyond grammar.

Research published through 2025 confirmed the pattern at scale. Cross-linguistic evaluations of machine translation and large language models across Greek, German, Spanish, and Dutch found continued default to male-gendered translations when the source language provides no gender signal. The Finnish example is notorious: "Han on laakari. Han on sairaanhoitaja" -- two sentences using the gender-neutral Finnish pronoun "han" meaning "that person" -- is routinely translated as "He is a doctor. She is a nurse" by major translation systems. The model has no gender information. The Finnish source is perfectly neutral. But the model's statistical associations override the linguistic evidence, producing a stereotyped output that reinforces exactly the occupational gender biases the previous subchapter described.

A decade of research on gender bias in machine translation, surveyed in a comprehensive 2025 review covering over 100 studies, found that despite growing research efforts and improved recognition of the problem, there is no simple technical solution. The bias is deeply embedded in the statistical patterns the models learn, and surface-level corrections often introduce new problems or fail to generalize.

The default masculine is not just a translation problem. It affects any generative task. When your model writes a story in French and introduces a character whose gender has not been specified, it defaults to masculine forms. When it generates a job description in Spanish, it uses masculine role titles. When it produces a summary in Arabic, it conjugates verbs in the masculine form for unnamed professionals. Across millions of interactions per day, this default reinforces the association between professional roles and maleness, between authority and maleness, between generic personhood and maleness. The model is not expressing an opinion. It is following a grammatical convention that happens to carry social weight.

## The Translation Gender Gap

Translation between English and gendered languages creates a specific class of gender failures that deserve focused attention, because translation is one of the highest-volume use cases for multilingual AI.

When the source language is gender-neutral and the target language requires gender, the model faces what researchers call the gender gap: information that is absent in the source must be present in the target. "The doctor said they will call you back" in English provides no gender signal for the doctor. Translating this sentence into Spanish requires choosing "El doctor dijo que le devolvera la llamada" (masculine) or "La doctora dijo que le devolvera la llamada" (feminine). Into Arabic, the verb "said" itself must be conjugated for gender. Into Hindi, the verb ending changes. The model must make a choice, and whatever choice it makes reveals its biases.

The reverse direction creates a different problem. Translating from a gendered language to English can erase gender information that was explicitly present. A French sentence that clearly refers to "une ingenieure" (a female engineer) may be translated to English simply as "an engineer," losing the gender marking. This erasure is sometimes appropriate -- English does not mark engineer by gender -- but in contexts where the person's gender is relevant, the erasure changes the meaning of the text.

Code-switched and multilingual conversations create even more complex scenarios. A user who writes in English but includes a name in Hindi script, or who writes in French but discusses a Japanese institution, creates inputs where the model must infer gender from cultural and linguistic cues that span multiple systems. A Hindi name may carry strong gender signals that the model should use. A Japanese institutional title may imply gender in ways the model should not assume.

## Gender-Neutral Language: The Moving Target

Some languages are actively developing gender-neutral forms, but adoption is uneven and often politically charged. Your model's handling of these emerging forms is itself a cultural statement.

In Spanish, the traditional masculine-as-default has been challenged by multiple alternative forms. The "-x" suffix (as in "Latinx") gained traction in English-language discourse about Spanish-speaking communities but has been criticized as unpronounceable in Spanish and primarily used by English speakers. The "-e" suffix (as in "Latine," "todes," "amigues") has gained more traction within Spanish-speaking communities as a pronounceable alternative. But both forms remain controversial, and many Spanish speakers -- including the Real Academia Espanola, the language's de facto authority -- reject them as unnecessary grammatical innovations.

In German, the "Genderstern" (gender star) and "Gendergap" (gender gap) notations write both masculine and feminine forms with a typographic separator: "Lehrer*innen" or "Lehrer_innen" to include all teachers regardless of gender. Some publications and institutions have adopted these forms. Others reject them. The Rat fur deutsche Rechtschreibung (Council for German Orthography) has declined to include gender-inclusive typographic forms in official spelling rules. A model that uses the Genderstern in German output is making a political statement about gender inclusivity. A model that avoids it is also making a political statement.

In French, "ecriture inclusive" (inclusive writing) uses medial points to combine masculine and feminine forms: "les etudiant.e.s" for all students. The Academie francaise has explicitly condemned inclusive writing as a threat to the French language. Researchers at multiple French universities have developed tools like GeNRe for gender-neutral rewriting in French, but the practice remains contentious. The French government has banned inclusive writing in official documents.

For your model, the question is not which form is linguistically correct. The question is what your model's default should be, and whether it should adapt based on user preference. A model that produces gender-neutral Spanish for all users will annoy users who consider these forms grammatically incorrect. A model that never produces gender-neutral forms will alienate users who consider them important for inclusion. The practical solution is to make gender-neutral language a configurable option -- available when requested, not imposed by default -- and to document the decision so users know what to expect.

## Pronoun Systems Beyond the Binary

English-speaking teams often think of pronouns as a simple three-way choice: he, she, they. Most languages have pronoun systems that are far more complex, and the complexity creates design challenges that English does not prepare you for.

Japanese has no fewer than a dozen common first-person pronouns, each carrying gender, formality, and personality connotations. "Watashi" is formal and gender-neutral. "Boku" is casual and typically masculine. "Ore" is rough and masculine. "Atashi" is casual and typically feminine. "Uchi" is informal and often used by women in the Kansai dialect. When the model generates first-person text in Japanese -- when it "speaks" as itself -- its pronoun choice communicates gender identity, social status, and personality. A model that uses "watashi" sounds formal and distant. One that uses "boku" sounds casually masculine. The choice is not trivial, and different users will have different expectations about which pronoun is appropriate for an AI system.

Thai has a similarly complex pronoun system where first-person pronouns vary by gender, social status, and the relationship between speaker and listener. "Phom" is used by male speakers. "Dichan" is formal feminine. "Chan" is informal feminine. "Rao" can be used by any gender but carries specific connotations. The model's pronoun choice in Thai is a cultural act.

Korean and Vietnamese use age-based and relationship-based speech levels rather than pronouns per se, but the choice of formality level communicates assumptions about the relationship between the model and the user. A model that defaults to the highest formality level sounds stiff and distant. One that defaults to a casual level can sound presumptuous or disrespectful, especially to older users.

For non-binary users, the pronoun challenge is compounded in languages where the grammar itself resists gender neutrality. Swedish has adopted "hen" as a gender-neutral pronoun alongside "han" (he) and "hon" (she), and it has been included in the Swedish Academy Dictionary since 2015. But most gendered languages have no widely accepted gender-neutral pronoun. Non-binary users of French, Spanish, Arabic, and Hindi often have to choose between a pronoun that misrepresents their identity and a neologism that many speakers consider grammatically incorrect.

## User Gender Handling: When to Ask, When to Infer, When to Default

Your model will frequently need to know a user's gender -- or specifically, not to assume it -- in order to generate grammatically correct output in gendered languages. How you handle this creates both UX and ethical considerations.

**Never infer gender from names.** Names carry statistical gender associations, but acting on those associations is a form of stereotyping. A user named "Andrea" is female in Italian and male in German. A user named "Kim" could be any gender in English but is typically male in Korean. Even within a single language, names do not determine gender identity. Building a name-to-gender inference layer is building a stereotype amplification engine.

**Ask explicitly when gender affects output quality.** If the model must generate text about the user in a gendered language -- addressing them, referring to them, conjugating verbs for them -- the most respectful approach is to ask. "How would you like me to refer to you?" or "Which grammatical form should I use?" This works well in interfaces where there is a setup step or user profile. It works poorly in one-shot interactions where asking adds friction.

**Default to the most neutral available form.** When gender information is unavailable and the interaction does not justify asking, default to whatever form is most neutral in the target language. In languages with a gender-neutral option, use it. In languages where the masculine form traditionally serves as the default, consider whether the context makes the feminine or a circumlocution more appropriate. There is no universal right answer, but there is a wrong answer: assuming masculine in every case without reflection.

**Respect stated preferences absolutely.** If a user specifies their gender or pronoun preference, the model must honor it without exception and without commentary. A non-binary user who requests gender-neutral forms in Spanish should receive them, even if the model's grammar checker considers them nonstandard. User identity takes precedence over grammatical convention.

## Per-Language Gender Guidelines

Operationalizing gender handling across languages requires explicit, documented guidelines for each language your model supports. These guidelines are not optional. Without them, individual engineers and annotators will apply their own intuitions, which will be inconsistent across languages and across team members.

Each language's gender guideline should specify the default gender treatment when no user information is available, the available gender-neutral alternatives and when to use them, how to handle translation between languages with different gender systems, how to handle non-binary users and identities, and how to handle culturally specific gender concepts that do not map to the Western binary.

The guidelines should be developed with native speakers who have expertise in both linguistics and gender issues in their culture. A linguist can tell you what is grammatically possible. A gender-studies expert can tell you what is socially appropriate. You need both perspectives to create guidelines that are linguistically correct, culturally sensitive, and practically implementable.

## The Deeper Challenge

Gender handling in multilingual AI is not ultimately a technical problem. It is a problem about values. A model that defaults to masculine forms everywhere is expressing a value: that masculine is normal and everything else is a deviation. A model that aggressively uses gender-neutral forms everywhere is expressing a different value: that gender neutrality is important enough to override traditional grammar. A model that asks every user their gender is expressing yet another value: that individual identity should always be respected, even at the cost of conversational efficiency.

None of these values is wrong. But each of them will be wrong for some users in some contexts. The job of the engineering team is not to find the one correct value but to build a system flexible enough to respect different values in different contexts, and to document the default values so that users and stakeholders understand the choices that have been made on their behalf.

The gender and pronoun challenges discussed in this subchapter are just one dimension of the broader cultural safety problem. Detecting and mitigating all of these cultural harms -- stereotypes, religious sensitivity, political sensitivity, gender bias -- requires safety classifiers that understand cultural context, not just linguistic content. The next subchapter examines how to build culture-aware safety classifiers that can distinguish universal harms from culturally variable ones.