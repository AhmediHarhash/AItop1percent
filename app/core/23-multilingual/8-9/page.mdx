# 8.9 â€” Input Methods, Virtual Keyboards, and Text Entry Across Scripts

Your text input field works perfectly for 26 letters. It accepts keystrokes, displays characters, and sends the result to your API. But for 1.5 billion Chinese speakers, text entry means typing a phonetic approximation, reviewing a list of candidate characters, and selecting the right one -- because thousands of distinct characters share the same pronunciation. For 125 million Japanese speakers, typing a single sentence requires switching between three different scripts mid-word. For 80 million Korean speakers, each visible syllable is assembled from two to four individual strokes that combine in real time as they type. The text input experience that English speakers take for granted -- press key, see letter, done -- is the exception, not the rule. And the gap between how English input works and how CJK input works has direct consequences for any AI product that offers auto-suggest, auto-complete, real-time validation, or streaming responses.

Most AI teams build their input handling for Latin keyboards and discover the problem when their first CJK users report that auto-complete fires mid-composition, suggestions interrupt the candidate selection process, or real-time validation rejects partially composed characters as invalid input. These are not edge cases. They affect over two billion users. If your product does not handle composition-based input correctly, it is broken for a third of the world's population.

## How Latin Input Works: The Baseline

Understanding why CJK input is different requires understanding what Latin input takes for granted. When an English-speaking user presses the "a" key, the following happens: the keydown event fires, the character "a" is inserted into the text field, the input event fires, and the application can immediately read the field's value. Each keystroke produces exactly one character. The character is final. No further user action is needed to confirm it. The application can safely process the current value of the field at any point because what the user sees is what the field contains.

This one-keystroke-one-character model is the assumption baked into virtually every web framework, every mobile text field, and every real-time processing pipeline. Auto-suggest reads the field value after each keystroke and queries the server. Auto-complete offers a suggestion that the user can accept with Tab or Enter. Real-time validation checks the field value on every input event. Streaming AI responses begin generating as soon as the user pauses typing. All of these features assume that the field's value at any given moment represents the user's intended input.

For CJK input, that assumption is false.

## Chinese Input: Pinyin to Character Conversion

Chinese speakers type using an **Input Method Editor**, or IME. The most common input method for Simplified Chinese is pinyin -- the romanization system that maps Chinese pronunciations to Latin letters. A user typing the Chinese word for "you" presses the keys "n," "i." The IME displays "ni" as a provisional composition -- typically underlined or highlighted to indicate that it is not yet finalized. The IME then presents a candidate list: the character meaning "you," the character meaning "mud," the character meaning "near," and several others, all pronounced "ni" but written with different characters and carrying different meanings.

The user selects the correct character -- by pressing a number key, clicking the candidate, or using arrow keys -- and the provisional "ni" is replaced by the selected character. Only at this point is the input finalized. Only now does the text field contain the character the user intended.

The composition phase -- the period between when the user starts typing pinyin and when they select a character -- is the critical window that most AI products mishandle. During composition, the text field contains pinyin letters, not Chinese characters. The value of the field is "ni," not the Chinese character. If your auto-suggest feature reads the field value and sends "ni" to the server, the server will try to suggest completions for the Latin string "ni," which is meaningless in a Chinese context. If your real-time validation checks whether the field contains valid Chinese characters during composition, it will reject the input. If your character counter counts the pinyin letters, it will show a misleading count.

The problem scales with sentence-level input. Many Chinese users type full sentences in pinyin before converting. A user might type "jin tian tian qi hen hao" (today's weather is very nice) as a continuous pinyin string, then convert the entire string to Chinese characters in one step. During the composition of that string, the field contains 22 Latin characters. After conversion, it contains 6 Chinese characters. Any feature that processes the field value during composition is working with the wrong data.

## Japanese Input: Three Scripts, One Sentence

Japanese input is more complex than Chinese because Japanese uses three scripts interchangeably, often within a single sentence. **Hiragana** is the phonetic script for native Japanese words. **Katakana** is the phonetic script for foreign loanwords and emphasis. **Kanji** are Chinese-derived characters used for most content words.

A typical Japanese input flow works like this. The user types Roman letters on their keyboard: "k," "y," "o," "u." The IME converts these to hiragana in real time, displaying the hiragana characters as a provisional composition. The user then presses the space bar to convert the hiragana to kanji. The IME presents candidate kanji characters that match the reading. The user selects the correct one and presses Enter to finalize.

But Japanese composition is more fluid than Chinese. The user might accept the hiragana form without converting to kanji -- some words are conventionally written in hiragana. They might convert to katakana instead, for a loanword or stylistic effect. They might type part of a sentence, convert it, then continue typing the next segment. The composition and conversion happen in segments, not all at once, and the user can go back and reconvert a segment they already finalized.

This means that at any given moment during Japanese input, the field may contain a mix of finalized text (characters the user has confirmed) and provisional composition text (characters the user is still working on). An AI auto-suggest feature must distinguish between the two. Suggesting completions based on the finalized text is helpful. Suggesting completions based on the provisional hiragana is premature because the user has not yet decided which kanji to select.

## Korean Input: Real-Time Syllable Composition

Korean uses a unique composition model that is different from both Chinese and Japanese. The Korean writing system, **Hangul**, is composed of individual letter components called jamo. Each Hangul syllable block is assembled from two to four jamo: an initial consonant, a vowel, and optionally one or two final consonants.

When a Korean user types, the composition happens character by character in real time. Pressing "g" produces the consonant jamo. Pressing "a" combines it with the vowel to form the syllable "ga." Pressing "n" changes the display: the "n" might be the final consonant of the current syllable ("gan") or the initial consonant of the next syllable. The IME does not know until the user types the next character. If the next character is a vowel, the "n" becomes the initial consonant of a new syllable. If the next character is a consonant or the input ends, the "n" is the final consonant of the current syllable.

This means Korean composition is perpetually provisional. Each character the user types can retroactively change the structure of the previous syllable. The field's value shifts with every keystroke as the IME reinterprets the jamo sequence. An auto-suggest feature that fires on every input event will see the text change shape -- not just grow -- with each keystroke. The syllable that was "ga" becomes "gan" becomes "gang" or splits into "ga" plus "na" depending on what the user types next.

For AI products, this means that Korean input cannot be processed keystroke-by-keystroke with the assumption that earlier characters are stable. The composition must be treated as a unit that resolves when the user moves to the next syllable or finalizes the input.

## The Composition Event API: How to Handle IME Input Correctly

Web browsers provide a set of events specifically designed to handle IME composition: compositionstart, compositionupdate, and compositionend. These events signal the beginning, progression, and conclusion of a composition session.

**compositionstart** fires when the IME begins a new composition. From this point until compositionend fires, the text in the field is provisional. Your application should suppress any behavior that depends on the field's value being final: auto-suggest queries, real-time validation, character counting, and streaming input to the server.

**compositionupdate** fires on every change during composition -- each new pinyin letter, each hiragana conversion, each jamo combination. The field's value changes, but the change is provisional. Reading the value during compositionupdate and acting on it is the source of nearly every CJK input bug in AI products.

**compositionend** fires when the user finalizes the composition -- by selecting a candidate, pressing Enter, or moving to the next segment. At this point, the field contains the user's intended text. Your application can safely read the value, trigger auto-suggest, run validation, and send input to the server.

The implementation pattern is straightforward. Maintain a boolean flag -- call it isComposing -- that is set to true on compositionstart and false on compositionend. Guard all input-processing logic behind a check of this flag. If isComposing is true, skip the processing. If false, proceed normally. This single flag prevents the vast majority of CJK input issues.

The mistake most teams make is processing input on the input event without checking for composition state. The input event fires during composition, and in many browsers the field's value during composition includes the provisional text. Processing this value produces nonsensical suggestions, false validation errors, and incorrect character counts.

## Auto-Suggest and Auto-Complete During Composition

AI products that offer real-time suggestions face a specific challenge with IME input: the user needs the IME's candidate list to select characters, and your product's suggestion list competes for the same visual space and the same keyboard shortcuts.

Consider a Chinese user typing in an AI chatbot that offers auto-complete. The user types "ni hao" in pinyin. The IME displays a candidate list of possible character combinations. Simultaneously, the chatbot's auto-suggest feature detects the input and offers a completion: "ni hao ma" (how are you). Both the IME candidate list and the auto-suggest dropdown appear at the same time, overlapping each other. The user presses "1" to select the first IME candidate, but the auto-suggest feature intercepts the keystroke and accepts its own suggestion instead.

This collision is not hypothetical. It is reported by CJK users of AI products regularly, and it is one of the most common reasons CJK users disable auto-suggest entirely -- losing the feature because the implementation did not account for their input method.

The solution has two parts. First, suppress your auto-suggest UI during composition. When isComposing is true, do not display suggestions. Let the IME's candidate list have exclusive control of the user's attention and keyboard. Second, trigger your auto-suggest after compositionend, when the user has finalized their input and the IME is no longer active. The user completes their Chinese character selection, the composition ends, and then your suggestion appears. This sequential approach avoids the collision entirely.

For products that want to offer suggestions during the composition phase -- predicting what the user is trying to type based on partial pinyin input -- the implementation becomes significantly more complex. You need to integrate with the IME's candidate list rather than replacing it, which requires platform-specific APIs that vary across operating systems and browsers. In 2026, most AI products choose the simpler approach: wait for composition to end before suggesting. The user experience is slightly less responsive but dramatically more reliable.

## Mobile Keyboards: A Different Challenge

Mobile text entry adds another layer of complexity because the keyboard itself changes based on the user's language.

**Latin-script languages** on mobile typically use a QWERTY layout with swipe typing (tracing a path through the keys to form a word). Swipe typing works because Latin words are sequences of characters where each character corresponds to a distinct key position. The swipe algorithm predicts the word based on the path geometry.

**Chinese on mobile** uses a specialized keyboard layout. The pinyin keyboard resembles QWERTY but includes a candidate bar above the keys that displays character suggestions based on the pinyin sequence. Some users prefer the stroke-based keyboard, which offers five basic stroke types (horizontal, vertical, left-falling, right-falling, and turning) that the user taps in sequence to identify a character. Stroke input is slower but does not require knowing pinyin, which is important for users who speak a dialect where standard pinyin does not accurately represent their pronunciation.

**Japanese on mobile** commonly uses the "flick" input method. The keyboard displays the kana syllabary in a grid, and the user taps a key then flicks in one of four directions to select the specific kana in that group. This is faster than cycling through options with repeated taps. Japanese mobile keyboards also support romaji input, which works like a small QWERTY layout with IME composition.

**Korean on mobile** uses a layout based on the standard Korean keyboard (dubeolsik), with jamo keys arranged for thumb typing. The composition behavior is the same as desktop -- jamo combine into syllable blocks in real time -- but the smaller screen and touch input make the composition more error-prone.

**Arabic on mobile** uses a right-to-left keyboard layout where letters change form based on position (initial, medial, final, or isolated). The visual display of the keyboard shows isolated letter forms, but the text field shows the contextually connected forms. Users accustomed to this behavior navigate it fluently, but products that attempt to render Arabic input character-by-character without applying contextual shaping produce disconnected letters that are illegible.

**Indic scripts on mobile** -- Hindi, Bengali, Tamil, and others -- use transliteration keyboards (type in Latin letters, get Devanagari output) or native script keyboards with multi-tap or swipe input. The transliteration approach creates a composition flow similar to Chinese pinyin, with the same implications for auto-suggest and validation timing.

For AI products, the key takeaway is that mobile text entry is not a simplified version of desktop entry. It is a parallel input ecosystem with its own conventions, its own composition models, and its own interaction patterns. Auto-suggest, auto-complete, and real-time features must be tested on mobile keyboards in every supported language, not just on desktop browsers with Latin keyboard layouts.

## Voice Input as the Equalizer

For scripts where keyboard input is complex, voice input becomes disproportionately valuable. A Chinese user who must navigate pinyin composition and candidate selection for every character can instead speak naturally and have the speech-to-text system produce the correct characters directly. Japanese users can avoid the romaji-to-hiragana-to-kanji conversion chain by speaking. Users of scripts with complex keyboard layouts -- Thai, Arabic, Hindi -- can bypass the keyboard entirely.

This makes voice input not just a convenience feature but an accessibility necessity for many multilingual users. AI products that support voice input in multiple languages reduce the input friction that composition-based keyboards create. The challenge, of course, is that speech recognition quality varies dramatically by language -- a topic the next subchapter covers in depth.

For products that do not support voice input, the quality of the keyboard text entry experience becomes the bottleneck for user engagement. A product where Chinese input works smoothly -- where auto-suggest waits for composition to end, where the candidate list is not obscured by product UI, where character count reflects final characters rather than pinyin length -- will see higher engagement from Chinese users than a product where the input experience feels like it was designed for English and grudgingly adapted for everything else.

## Accessibility Across Input Methods

Users with disabilities rely on alternative input methods that vary by locale and script. Screen readers interact with IME composition differently across platforms. Switch access and eye-tracking input methods must work with composition-based entry. Voice control systems that translate spoken commands into text input must handle the composition lifecycle correctly.

In Japan, the accessibility landscape includes input methods designed for users who cannot use standard keyboards at all -- scanning interfaces that present character groups sequentially, allowing the user to select with a single switch input. These methods produce text through the same IME composition pipeline, which means your product's handling of composition events affects not just convenience but accessibility compliance.

The Web Content Accessibility Guidelines (WCAG) require that all functionality be operable through a keyboard interface, but the definition of "keyboard interface" implicitly assumes Latin input. For CJK users, the keyboard interface includes the IME, and any product feature that interferes with IME operation -- by capturing keystrokes during composition, by overlaying UI on the candidate list, or by processing provisional input -- is an accessibility barrier.

Test your product with screen readers in each supported language. Test with each major IME (Microsoft IME, Google Input Tools, Apple's native input methods, and third-party IMEs like Sogou and Baidu for Chinese). Test on both desktop and mobile. The intersection of accessibility and IME input is undertested across the industry, and the products that get it right earn fierce loyalty from users who have been ignored by everyone else.

## The Implementation Checklist

Getting composition-based input right in your AI product requires attention at multiple levels.

At the browser event level, listen for compositionstart, compositionupdate, and compositionend. Maintain a composition state flag and suppress input processing during composition. After compositionend, process the finalized input normally.

At the UI level, do not display auto-suggest, auto-complete, or inline validation during composition. Do not overlay any product UI on the area where the IME candidate list appears. Ensure that your product's keyboard shortcuts do not conflict with IME keyboard shortcuts (arrow keys, number keys, space bar, and Enter are all used by IMEs during composition).

At the data level, count characters after composition, not during. Validate input after composition, not during. Send input to the server after composition, not during. Measure input field length based on the final text, not the provisional pinyin or romaji.

At the testing level, test every input-related feature with Chinese (pinyin), Japanese (romaji-to-hiragana-to-kanji), and Korean (jamo composition) input. Test on desktop and mobile. Test with the most popular IMEs for each language. Test with auto-suggest, auto-complete, real-time validation, and streaming features active simultaneously.

The cost of not doing this is not subtle. CJK users who encounter broken composition behavior do not file bug reports. They disable your features, work around your product, or leave for a competitor that handles their input method correctly.

Text entry is the first half of multilingual interaction. The second half is voice. For users across all scripts, voice interaction introduces a new set of multilingual challenges -- accents, dialects, transliteration, code-switching in speech, and real-time language detection in audio. The next subchapter covers multilingual voice interaction and bridges to Section 21, where voice and real-time system architecture are covered comprehensively.