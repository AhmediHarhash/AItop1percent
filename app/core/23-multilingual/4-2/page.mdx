# 4.2 â€” Designing Native-Language Evaluation Sets: The Golden Rule

If you did not write the evaluation question in the language you are testing, you are not testing the language. You are testing translation. This is the golden rule of multilingual evaluation, and it separates teams that understand their multilingual quality from teams that think they do. A native-language evaluation set is one where every question, every scenario, every expected answer was conceived, written, and validated by a native speaker of that language -- someone who thinks in that language, who understands the cultural frame of that language's users, and who would never phrase a question the way a translated English question phrases it.

The distinction sounds subtle. It is not. A translated evaluation question about customer service might ask a model to handle a complaint about a late delivery. The scenario works in English. Translated into Japanese, it retains the complaint structure, the direct tone, the expectation of a refund. But a native Japanese customer support scenario would involve different expectations entirely -- the customer might express frustration indirectly, the expected model response would require layered honorifics, and the resolution path would involve an apology structure that has no parallel in English. A translated question tests whether the model can respond to an American complaint written in Japanese words. A native question tests whether the model can navigate a Japanese customer interaction. These are fundamentally different capabilities.

## What Native-Language Actually Means

The term "native-language evaluation" gets misused. Teams sometimes claim their eval sets are native because a native speaker reviewed the translations. That is translation review, not native evaluation design. The distinction rests on where the question originates.

A native-language evaluation case starts from the target language. The evaluator sits down with a blank page, thinks about what a real user in that language and culture would actually ask or need the model to do, and writes a scenario that reflects that reality. The vocabulary comes naturally from the language -- not mapped from English equivalents. The cultural context is the evaluator's own culture, not an imported one. The expected answer reflects what a native speaker would consider correct, helpful, or appropriate in that context.

Consider a factual accuracy test. A translated English question might ask "What is the capital of Australia?" A native Hindi evaluation for Indian users might ask about the structure of the Lok Sabha or the significance of the Panchayati Raj system. Both test factual knowledge. But the Hindi question tests whether the model has factual knowledge that Hindi-speaking users actually care about, using terminology and framing that Hindi-speaking users would naturally use. The translated question tests whether the model knows Australian geography in Hindi -- technically valid but practically irrelevant for most Hindi-speaking users.

The same principle applies to every quality dimension. Fluency evaluation in English checks for natural word order, appropriate vocabulary, and grammatical correctness. Fluency evaluation in Arabic must additionally check for correct use of case endings, appropriate integration of loanwords versus native vocabulary, and natural dialectal variation where relevant. A translated English fluency rubric misses all of these Arabic-specific quality signals because the rubric was designed for English-specific quality dimensions.

## The People Who Write Native Evals

Building native-language evaluation sets requires a specific kind of contributor: someone who is both a fluent native speaker and a competent evaluator. These two skills rarely come from the same training.

Native fluency means more than speaking the language. It means cultural immersion -- understanding the humor, the formal registers, the regional variations, the idiomatic expressions that mark text as natural versus awkward. A heritage speaker who grew up in the United States speaking Korean at home but was educated in English may have conversational fluency but miss the professional register expectations that native-educated Korean speakers take for granted. A translator who learned the language to professional proficiency but is not a native speaker may produce grammatically correct evaluation cases that lack the cultural intuition a native speaker would bring.

Evaluation competence means understanding what makes a good test case. The evaluator needs to design cases that are specific enough to have clear correct and incorrect outputs, diverse enough to cover the quality dimensions that matter, and calibrated to the right difficulty level. An expert native speaker who has never designed evaluation cases will produce scenarios that are culturally authentic but may be too open-ended to evaluate, too easy to discriminate between good and bad models, or too narrow in what they test.

The ideal contributor has three qualities: native-level language fluency with cultural immersion, domain expertise in whatever your product does, and experience with evaluation design or quality assessment. Finding people who have all three is difficult. The practical solution is to pair native speakers who have domain expertise with evaluation specialists who provide the design framework. The native speaker creates the scenario and validates cultural authenticity. The evaluation specialist ensures the test case is well-constructed, appropriately scoped, and aligned with your quality rubric.

## Recruiting and Managing Native Evaluators

Where you find native evaluators depends on your budget, your timeline, and how many languages you support.

For high-priority languages where your product has significant user volume, invest in contracted evaluators who work with your team over time. These people learn your product, understand your quality dimensions, and produce increasingly refined eval cases as they internalize what matters. A contracted evaluator who has worked with your product for three months writes dramatically better eval cases than a freelancer who receives a brief and a deadline. The contracted evaluator costs more per case, but the quality of the evaluation signal makes the investment worthwhile.

For medium-priority languages, use specialized annotation platforms that maintain pools of native speakers with domain expertise. Companies like Appen, Toloka, and Scale AI maintain annotator pools across dozens of languages. The quality varies. You need to establish clear guidelines, review the first batch of submitted cases carefully, provide feedback, and reject cases that do not meet your cultural authenticity standard. Expect to reject 20 to 40 percent of the first batch from a new annotator pool. By the third batch, rejection rates typically drop below 10 percent as annotators learn your expectations.

For lower-priority languages where you need basic coverage, academic partnerships and community contributions can bootstrap an initial eval set. University language departments often have graduate students who can create evaluation cases as part of research projects. The INCLUDE benchmark, published at ICLR 2025, demonstrated this approach at scale -- its 197,243 questions across 44 languages were collected from local educational and professional examinations in each country, then validated by native speakers. You do not need to operate at that scale, but the methodology of sourcing from existing native-language materials and having native speakers validate appropriateness is one you can adopt.

Regardless of the sourcing model, every evaluator needs a clear brief. The brief should specify the evaluation dimensions you are testing, the format of the evaluation cases, the difficulty distribution you want, and examples of good and bad cases in their language. Never provide the English originals and ask for "equivalents." Provide the quality dimensions in the evaluator's language and let them create scenarios that naturally test those dimensions.

## How Many Cases You Actually Need

The question teams ask most often is how many native-language eval cases they need per language. The honest answer is fewer than you think, as long as they are well-designed.

A translated eval set of two hundred cases might cover ten quality dimensions with twenty cases each. That sounds like solid coverage. But if the cultural assumptions, the phrasing patterns, and the reference answers are all wrong in the ways described in subchapter 4.1, those two hundred cases are producing noise, not signal. Fifty well-designed native cases that genuinely test what matters for your users in that language produce a cleaner, more actionable quality signal than two hundred translated cases that test the wrong things.

The minimum viable native eval set for a language contains twenty to thirty cases. This is enough to cover your core quality dimensions -- typically fluency, factual accuracy, instruction following, tone appropriateness, and one or two domain-specific dimensions -- with four to six cases each. At this volume, you cannot measure fine-grained score changes, but you can detect meaningful quality shifts. If a model update drops your native eval score from 85 percent to 70 percent across twenty cases, you have a real regression, even without the statistical power to declare the exact magnitude.

The working target for production-grade coverage is fifty to two hundred cases per language. At fifty cases, you have enough to track quality trends, detect regressions, and compare model versions with reasonable confidence. At one hundred, you can break down scores by quality dimension and identify which aspects of the language are strong or weak. At two hundred, you approach the statistical power needed for fine-grained A/B testing and per-dimension regression tracking.

Above two hundred native cases, you get diminishing returns unless your product has highly specialized domains that each require dedicated evaluation coverage. A general-purpose customer support model might need sixty native cases for Korean. A medical information model serving Korean users might need two hundred, because the medical domain alone requires dedicated test cases for terminology accuracy, regulatory compliance, and clinical appropriateness.

## The Minimum Viable Eval Set

If budget or timeline forces you to start small, here is what the minimum viable native eval set looks like for a single language.

Twenty cases total. Four cases testing fluency -- whether the model produces natural, grammatically correct output that a native speaker would recognize as well-written. Four cases testing accuracy -- whether the model's factual claims are correct for that language's cultural and regional context. Four cases testing instruction following -- whether the model follows directions expressed in the target language, including directions that use language-specific conventions. Four cases testing tone -- whether the model selects the appropriate formality level, politeness register, and cultural communication style. Four cases testing your most critical domain-specific capability -- whatever makes or breaks your product for users of that language.

Each case must include the prompt or scenario, the expected output characteristics, and a scoring rubric written by a native speaker. The rubric is crucial. A generic rubric that says "output should be fluent" does not help. A native rubric that says "output should use standard written register, avoid mixing dialectal forms unless the prompt context calls for informal speech, and use appropriate verb conjugation including subjunctive where the context demands it" gives the evaluator a clear quality target.

Every case must pass the nativeness test: could this question have been conceived by someone who only speaks the target language? If the scenario requires knowledge of American culture, English-language conventions, or Western educational frameworks to make sense, it fails the test. If a monolingual speaker of the target language would find the scenario perfectly natural and relevant, it passes.

## Maintaining Eval Sets Over Time

Native-language eval sets are not static artifacts. They need maintenance for three reasons.

First, languages evolve. New slang enters common usage. Formal registers shift. Terminology in technical domains updates as industries adopt new vocabulary. An eval set written in 2024 for Brazilian Portuguese might use terminology that has already been supplanted by 2026. Annual reviews by native speakers catch these drifts and keep the eval set current with how the language is actually used.

Second, your product changes. New features, new domains, new user segments all create new evaluation needs. When you add a medical information feature to a product that previously handled only customer service, your existing native eval cases do not cover the medical domain. You need new native cases for medical scenarios in each language.

Third, cultural context shifts. Regulatory changes, political events, social movements, and shifting business practices change what constitutes a correct or appropriate response. A model serving Turkish users needs to reflect current Turkish regulatory frameworks, not the ones that applied two years ago. A model serving users in Southeast Asia needs to account for regional trade agreements and business practices that change year to year.

The maintenance cadence depends on your product's rate of change. A stable product with slow-evolving use cases might review native eval sets annually. A fast-moving product with frequent feature launches should review quarterly, with targeted additions whenever a major feature ships. The review itself does not require recreating the entire eval set. A native speaker spends two to four hours reviewing fifty existing cases, flagging any that feel outdated, and proposing five to ten new cases that reflect current usage patterns.

## The Cost Equation

Native-language eval sets cost more than translated ones. There is no escaping this. Translation of two hundred English eval cases into a target language costs between 500 and 2,000 dollars depending on the language pair and the translation quality you demand. Building fifty native-language eval cases from scratch costs between 1,500 and 5,000 dollars, depending on the domain complexity, the language, and the evaluator compensation rates in that market.

Budget two to three times the cost of translation for native eval development. For a product supporting ten languages, this means an initial investment of 15,000 to 50,000 dollars for native eval sets across all languages, compared to 5,000 to 20,000 for translated sets. The gap is real.

But consider what the translated sets are costing you in hidden ways. The B2B team from subchapter 4.1 spent three months responding to Spanish-speaking customer complaints that their translated evals did not detect. The engineering time alone -- triaging complaints, debugging quality issues, building patches -- cost more than a native Spanish eval set would have. The customer relationship damage is harder to quantify but more expensive still. One enterprise customer explicitly cited "poor Spanish quality" as a factor in their decision not to renew a 120,000-dollar annual contract.

The native eval set is an insurance policy. Its cost is concrete and upfront. The cost of not having it is diffuse and delayed, but almost always larger. Every team that has invested in native eval sets and then compared the results to their translated eval sets reports the same experience: the native sets revealed problems the translated sets were hiding, and fixing those problems prevented user-facing quality failures that would have been far more expensive to address after launch.

## From Eval Set to Eval Culture

The deeper shift that native eval sets enable is cultural. When your team builds evaluation cases with native speakers, the team starts to internalize what quality means in each language. The product manager who reviews native Japanese eval cases discovers that Japanese users expect a level of politeness that the English product never considered. The engineer who sees native Arabic eval results learns that right-to-left formatting issues cause quality failures that English testing never surfaces. The designer who reads native German eval cases realizes that German compound words create display issues that break the interface in ways that English strings never would.

Native eval sets do not just measure quality. They teach your team what quality looks like in languages they do not speak. That education is worth the investment even if you never run the eval suite a second time, because it changes how the team thinks about multilingual product decisions from that point forward.

The next subchapter examines one of the deepest challenges in multilingual evaluation: culture-specific content that makes standard benchmarks unreliable for non-Western languages, and what the research community's analysis of MMLU's cultural biases reveals about the benchmarks you may be trusting today.