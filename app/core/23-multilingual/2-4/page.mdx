# 2.4 — CJK Specialists: Qwen, DeepSeek, and Models Built for East Asian Languages

In late 2024, a mid-sized SaaS company based in Singapore launched a Japanese customer support chatbot powered by GPT-4o. The English version had tested at 96% accuracy on their internal quality rubric. The Japanese version scored 71%. When native Japanese speakers reviewed the failing responses, they found a pattern that no automated metric had caught: roughly one in four replies contained grammar errors that sounded like a foreigner who had studied Japanese from textbooks but never lived in the country. Particles were misplaced. Keigo honorific forms were mixed with casual speech in the same sentence. Compound nouns were split in places where no Japanese speaker would break them. The team spent three months trying to fix the problem with prompt engineering, adjusting system prompts, adding few-shot examples in Japanese, and rewriting instructions to specify formality levels. The error rate dropped from 29% to 19%. Then they switched to Qwen 2.5 72B, changed nothing else, and the error rate fell to 4%. The model was not smarter. It was trained on better data for the task. That is the entire argument for CJK specialists in one story.

CJK — Chinese, Japanese, Korean — represents over 1.5 billion native speakers and three of the world's ten largest digital economies. These are not niche languages. They are not emerging markets that can wait for general-purpose models to catch up. They are markets where users have high expectations, where competitors ship native-quality products, and where the gap between a model trained for CJK and a model that happens to include CJK in its training data is the gap between a viable product and an embarrassment.

## Why CJK Languages Break English-Centric Models

The challenges that CJK languages pose to general-purpose models are not surface-level translation problems. They are structural incompatibilities baked into every layer of how the model processes language, from tokenization to generation.

The first and most quantifiable problem is the **token tax**. Most byte-pair encoding tokenizers were designed around Latin-script languages, where a common English word like "information" is typically one or two tokens. CJK characters interact differently with these tokenizers. In OpenAI's cl100k_base encoding, out of roughly 100,000 tokens, fewer than 900 are CJK tokens — less than 1% of the vocabulary dedicated to writing systems used by over a billion people. The result is that most CJK characters require two or three tokens each. A Japanese sentence that would be 15 tokens in English may consume 40 to 60 tokens in Japanese. This is not just a cost multiplier. It is a quality multiplier. The model has less room in its context window to reason, less budget for complex outputs, and higher latency per semantic unit.

Qwen's tokenizer takes a fundamentally different approach. With a vocabulary of approximately 151,643 tokens, Qwen allocates substantial space to CJK characters and common CJK word fragments. The practical result is that one Qwen token covers roughly 1.5 to 1.8 Chinese characters, compared to approximately one token per character in English-centric tokenizers. This means a Chinese paragraph that costs 200 tokens in GPT-5 might cost 120 tokens in Qwen. Over millions of API calls, that 40% reduction compounds into enormous savings. More importantly, the model can fit more context into the same window size, which directly improves response quality for complex tasks.

The second structural challenge is grammatical divergence. Japanese and Korean follow subject-object-verb word order, the reverse of English subject-verb-object. Chinese has no inflection — no verb conjugation, no plural markers, no tense — relying instead on word order and context particles that English speakers have no analog for. Japanese mixes three writing systems in a single sentence: kanji for content words, hiragana for grammatical markers, and katakana for foreign loanwords. Korean is agglutinative, stacking meaning into single words through suffixes that English would spread across three or four separate words. These structural differences mean that models cannot simply map English syntax patterns onto CJK output. They need to learn fundamentally different generation patterns, and models trained predominantly on English learn those patterns from a smaller, less diverse training set.

The third challenge is cultural and pragmatic. Japanese keigo, the honorific system, has five distinct levels of formality that must be consistent within a single response. Using the wrong level when addressing a customer is not a grammar error in the way English speakers understand grammar errors — it is a social error that immediately marks the speaker as either incompetent or rude. Korean has seven speech levels, and the choice of level signals the relationship between speaker and listener. Chinese requires sensitivity to simplified versus traditional characters, which is not merely a font choice but a political and cultural marker that distinguishes mainland Chinese, Taiwanese, and Hong Kong audiences. A model that defaults to simplified characters when addressing a Taiwanese user has made a cultural mistake that no amount of factual accuracy can overcome.

## The Qwen Family: The CJK Benchmark

Qwen 2.5, released by Alibaba Cloud in September 2024, and its successor Qwen 3, released in mid-2025, have established themselves as the default choice for CJK workloads. The reasons are concrete and measurable.

Qwen 2.5 72B was trained on 18 trillion tokens with heavy representation of Chinese web text, Chinese books, Chinese technical documentation, and Chinese social media — the kind of diverse, colloquial, domain-spanning data that makes a model sound natural rather than translated. The model's performance on Chinese benchmarks like C-Eval and CLUEWSC consistently matched or exceeded GPT-5, and on Japanese and Korean tasks it outperformed every open-weight alternative by significant margins. Qwen 3 expanded language coverage to 119 languages while pushing CJK quality even further, particularly for Japanese reasoning tasks and Korean conversational fluency.

The Qwen family spans a wide parameter range — 0.5B, 1.5B, 7B, 14B, 32B, and 72B — which gives teams deployment flexibility that no other CJK-optimized model family offers. The 7B model delivers Japanese quality that rivals GPT-4o at a fraction of the cost when self-hosted, making it viable for edge deployment or on-device applications in markets like Japan where data residency concerns are common. The 14B model is the sweet spot for most production workloads: strong enough for complex CJK tasks, small enough to run on a single high-end GPU. The 72B model is the quality ceiling, competitive with frontier API models on CJK tasks and often surpassing them.

For teams building CJK-first products, the decision tree is straightforward. If you need the absolute best Chinese, Japanese, or Korean quality and have the compute budget, Qwen 2.5 72B or Qwen 3 is your starting point. If you need cost efficiency at moderate quality, the 7B or 14B models deliver remarkably well. If you need CJK quality plus strong performance in five or more additional languages, Qwen 3's expanded language support makes it a genuine multi-language contender rather than a CJK-only specialist.

## DeepSeek: The Chinese Reasoning Specialist

DeepSeek occupies a different niche than Qwen. Where Qwen aims for broad CJK coverage with strong general capabilities, DeepSeek focuses intensely on Chinese reasoning — mathematical, scientific, legal, and technical problem-solving in Chinese.

DeepSeek V3.2 and DeepSeek R1 are trained on massive Chinese-language corpora with particular emphasis on structured reasoning tasks. On the Chinese National Medical Licensing Examination, DeepSeek R1 outperformed GPT-5 and Claude Opus 4.5 by meaningful margins. On Chinese legal reasoning benchmarks, DeepSeek's chain-of-thought capabilities produce step-by-step analyses that Chinese legal professionals describe as genuinely useful rather than superficially plausible. The model excels at tasks where the reasoning itself must happen in Chinese — where the domain concepts, the logical connectives, and the expected output format are all Chinese-native.

The tradeoff is clear. DeepSeek's non-Chinese performance drops off more steeply than Qwen's. Japanese and Korean support is functional but not exceptional. European languages are serviceable at best. If your product serves Chinese-speaking users working on technical, professional, or analytical tasks, DeepSeek is a serious contender that may outperform Qwen on those specific workloads. If you need Chinese plus Japanese plus Korean plus any other language, Qwen is the better foundation.

DeepSeek R1's reasoning capabilities are particularly valuable for a category of CJK applications that general-purpose models handle poorly: tasks that require understanding Chinese domain-specific conventions. Chinese accounting standards differ from International Financial Reporting Standards. Chinese contract law has structural conventions that no English-trained model understands natively. Chinese academic citation formats, Chinese patent filing structures, Chinese regulatory compliance frameworks — all of these have conventions that DeepSeek has learned from Chinese-native training data and that frontier API models approximate from translated or parallel text.

## Japanese-Specific Challenges

Japanese deserves special attention because it concentrates every CJK challenge into a single language and adds several unique ones.

The three-script system means that a single Japanese sentence may contain kanji, hiragana, and katakana, each serving a different linguistic function. A model generating Japanese must choose the correct script for each word and switch seamlessly between them. Writing "coffee" in hiragana instead of katakana is not technically wrong, but it looks bizarre to a native reader — like writing "kaw-fee" instead of "coffee" in English. Models trained primarily on English data frequently make these script-choice errors because they have not seen enough Japanese text to internalize the conventions.

Japanese compound nouns are another minefield. Japanese freely creates compound nouns by stacking kanji characters, and these compounds can grow to extraordinary lengths. A term like "environmental impact assessment report review committee" becomes a single unbroken string of kanji in Japanese. Models must understand where semantic boundaries fall within these compounds to generate, summarize, and reason about them correctly. English-centric models often split these compounds incorrectly or fail to recognize them as single semantic units.

Keigo — the honorific system — is the challenge that most directly impacts customer-facing applications. There is no English equivalent. English has "please" and "sir" and a few formal constructions, but Japanese has an entire parallel grammar for polite speech, humble speech, and respectful speech, with different verb conjugations, different nouns, and different sentence structures for each level. A customer support bot that mixes casual and formal speech in the same response signals either a broken product or profound disrespect for the customer. Qwen's models handle keigo significantly better than English-centric alternatives because Qwen's training data includes substantial Japanese customer service, business communication, and formal correspondence — the domains where keigo matters most.

## Korean-Specific Challenges

Korean presents a different set of obstacles. Hangul, the Korean writing system, is highly systematic — each block represents a syllable composed of consonant and vowel components — but the language itself is agglutinative, meaning that words are built by stacking suffixes onto roots. A single Korean word can carry the meaning that English would express in an entire clause: subject, object, tense, aspect, mood, and speech level all encoded in one morphological unit.

This agglutinative structure creates tokenization challenges even for CJK-optimized tokenizers. The boundaries between meaningful morphemes within a Korean word are not always obvious from the surface form, and tokenizers that split incorrectly can fragment meaningful units in ways that degrade the model's understanding. Spacing rules compound the problem. Korean uses spaces between words, unlike Chinese and Japanese, but the rules for where spaces fall are complex and inconsistently applied even by native speakers. Models frequently produce spacing errors that are immediately noticeable to Korean readers.

Korean's seven speech levels function similarly to Japanese keigo but with different social dynamics. The wrong speech level in Korean does not just sound awkward — it can sound aggressive or dismissive. A chatbot that addresses a customer using the panmal casual register instead of the jondaenmal polite register has essentially insulted them. Most frontier API models default to a safe middle register in Korean, which is better than getting it wrong but still sounds noticeably robotic to native speakers who expect natural register shifts.

## When to Use CJK Specialists vs Frontier API Models

The decision between a CJK specialist like Qwen and a frontier API model like GPT-5.1 or Claude Opus 4.6 comes down to four factors.

Quality is the first. For tasks where CJK-native fluency matters — customer service, content generation, formal correspondence, any task where the output is read by native speakers who will judge its naturalness — CJK specialists consistently outperform frontier API models. The gap is largest in Japanese and Korean, where honorific and formality systems expose the limitations of English-centric training. For tasks where CJK is the input language but the core task is language-agnostic — classification, entity extraction, sentiment analysis — the gap narrows, and frontier API models may be competitive or even superior on certain reasoning-heavy tasks.

Cost is the second factor. The token tax means that CJK workloads are inherently more expensive on English-centric tokenizers. A product that costs $0.003 per interaction in English may cost $0.010 to $0.015 in Japanese on GPT-5, simply because of token count inflation. Self-hosting a Qwen 7B or 14B model eliminates the per-token cost entirely and replaces it with a fixed infrastructure cost that becomes more favorable as volume increases. For high-volume CJK applications — millions of interactions per month — the cost argument for self-hosted specialists is overwhelming.

Latency is the third. More tokens per semantic unit means longer generation times and higher time-to-first-token for CJK on English-centric models. Self-hosted CJK specialists with efficient tokenizers produce the same semantic content in fewer tokens, which translates directly to lower latency. For real-time applications like customer chat or voice interfaces in CJK markets, this latency difference is user-visible and business-critical.

Ecosystem integration is the fourth. If your product already uses GPT-5 or Claude for other features, adding a CJK specialist means maintaining a multi-model architecture — separate API integrations, separate prompt engineering, separate evaluation pipelines. For teams serving CJK markets as one part of a global product, the complexity cost of adding a specialist model must be weighed against the quality gain. For teams where CJK markets are the primary or sole focus, there is no tradeoff. The specialist model is the obvious choice.

## The Evolving CJK Landscape

The CJK model landscape is not static. Chinese-language model quality has improved faster than any other language family between 2023 and 2026, driven by massive investment from Alibaba, DeepSeek, Baidu, and others. Chinese may now be approaching Tier 1 quality — near-parity with English — on the best specialist models, a position that no other non-English language has reached. Japanese and Korean are improving but lag behind Chinese, partly because the Chinese model ecosystem dwarfs the Japanese and Korean ones in investment and training data scale.

For teams planning CJK deployments, the practical implication is to reevaluate model choices every six months. A model that was the best Japanese option in early 2025 may have been surpassed by mid-2026. The Qwen release cadence — roughly every six to nine months — means that meaningful quality improvements arrive regularly. DeepSeek's focus on reasoning capabilities means that each release brings step-function improvements for Chinese professional and technical tasks. Locking into a single model and ignoring the landscape is a guaranteed way to fall behind competitors who are paying attention.

The CJK specialist ecosystem represents the most mature example of regional models outperforming global ones. The same dynamic is beginning to play out in European languages, where a French company is building models that understand European languages better than any American competitor. That is where we turn next.
