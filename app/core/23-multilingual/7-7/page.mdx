# 7.7 â€” Multilingual Safety Constraints: When Guardrails Only Work in English

Your safety guardrails are only as strong as their weakest language. If your content filters catch 98 percent of harmful requests in English but only 71 percent in Thai, your system is not 98 percent safe. It is 71 percent safe -- because attackers will use Thai. If your system prompt's safety instructions prevent the model from generating dangerous content in English but fail to prevent it in Swahili, your safety posture is defined by Swahili, not by English. The language where your defenses are weakest is the language where your risk is highest, and every language you add without testing safety in that language is a new door you forgot to lock.

This is **The English Safety Blind Spot** -- a pattern so pervasive that it deserves to be named. Teams build safety systems in English, test them in English, evaluate them in English, and declare them safe. They then deploy in twenty languages, and the safety boundary they carefully constructed applies fully to only one of them. The other nineteen languages have degraded safety coverage that nobody measured, nobody tested, and nobody monitors. Chapter 9 of this section covers cultural safety comprehensively. This subchapter focuses on the technical mechanism: why safety degrades across languages and what you can do about it.

## The Mechanism: Why Safety Degrades in Non-English Languages

Safety alignment in modern language models is trained primarily through English-language data. During RLHF and DPO training, human evaluators rate model outputs for helpfulness and harmlessness. These evaluators work overwhelmingly in English. The harmful prompts used to train the model's refusal behavior are overwhelmingly English. The examples of appropriate refusal responses are overwhelmingly English.

The result is a model that has a deep, nuanced understanding of what constitutes harmful content in English and a shallow, inconsistent understanding in other languages. The model learned to recognize "how do I make a weapon" as a harmful request and to refuse it. But it may not have seen the same request in Telugu, Yoruba, or Vietnamese during safety training. When that request arrives in a low-resource language, the model has two weak signals to rely on: the cross-lingual transfer of its English safety training (which degrades with linguistic distance) and whatever safety-relevant examples exist in that language's training data (which is sparse for low-resource languages).

This creates a predictable safety gradient that mirrors the compliance gradient from subchapter 7.3 but with higher stakes. English safety refusal rates sit above 95 percent for well-aligned models in 2026. Tier 1 European languages maintain 88 to 93 percent refusal rates -- close to English, because linguistic similarity enables stronger cross-lingual safety transfer. CJK languages fall to 80 to 88 percent. Arabic, Hindi, and Turkish range from 75 to 85 percent. Low-resource languages -- Swahili, Yoruba, Khmer, Amharic, Tagalog -- can drop to 60 to 75 percent, meaning one in four to one in three harmful requests may receive a compliant response instead of a refusal.

These numbers do not describe an exotic edge case. They describe the baseline safety posture of every multilingual AI product that has not explicitly tested and mitigated safety across languages.

## The Language-Switching Attack

The safety gradient creates a straightforward attack vector: write a harmful request in a language where the model's safety training is weakest.

This is not a theoretical concern. Research published at ICLR 2024 on multilingual jailbreak challenges demonstrated that harmful prompts refused in English could bypass safety filters simply by translating them into another language. The study tested across multiple models and found that ChatGPT produced unsafe output on 80.92 percent of multilingual harmful prompts, while GPT-4 produced unsafe output on 40.71 percent. The attack requires no technical sophistication -- a standard translation tool is the only weapon needed.

The attack has evolved since that initial research. By 2025, researchers demonstrated that the vulnerability extends beyond simple translation. Code-mixed prompts -- requests that blend two languages in a single sentence, such as writing a harmful request in romanized Hindi mixed with English -- exploit the model's difficulty in applying safety constraints to text that does not clearly belong to any single language. A 2025 study on phonetic perturbations in code-mixed Hinglish (Hindi-English) achieved a 99 percent attack success rate against state-of-the-art models including Llama 3 and ChatGPT variants. The phonetic perturbations change the tokenization of harmful words, breaking the pattern that the safety classifier was trained to recognize, while preserving the meaning that a human reader can easily understand.

The language-switching attack is particularly dangerous because it is accessible to anyone. Unlike adversarial prompt injection, which requires understanding of model internals and prompt structure, language switching requires only the ability to use a translation tool or to write in a language other than English. This means the attack surface is not limited to sophisticated threat actors. Any user who speaks a language other than English can inadvertently or intentionally circumvent English-trained safety boundaries.

## Low-Resource Languages: The Widest Gap

The safety gap is widest for languages with the least representation in the model's training data. This is a mathematical inevitability: the model's ability to recognize harmful content depends on having seen examples of harmful content in that language, and the safety training pipeline contains almost no examples in low-resource languages.

Low-resource languages are not obscure. Swahili has over 100 million speakers. Yoruba has over 45 million. Amharic has over 30 million. Tagalog has over 80 million. These are major world languages serving large populations, many of whom are increasingly using AI products. The "low resource" label refers to their digital representation in training data, not their importance or their speaker population.

For these languages, the model's safety behavior is essentially untrained. It relies entirely on cross-lingual transfer from English safety alignment, and that transfer is weakest for languages that are linguistically most distant from English. Yoruba and Amharic are as far from English as languages can be in terms of grammar, vocabulary, and script. The cross-lingual safety transfer for these languages is so weak that the model may not even recognize a harmful request as harmful, regardless of how clearly the harmful intent is expressed.

The operational implication is stark: if your product serves users in low-resource languages and you have not built explicit safety mechanisms for those languages, you have an unmitigated safety vulnerability for those user populations. Those users are less protected than your English-speaking users, and if anything goes wrong, your organization bears the same liability regardless of the language in which the harm occurred.

## The System Prompt Safety Gap

Safety instructions in your system prompt are themselves subject to the compliance gradient. When your system prompt says "never provide instructions for creating weapons," the model follows this instruction with high reliability in English. In Thai, the same instruction is followed with lower reliability because the model's instruction-following capability is weaker in Thai, as subchapter 7.3 documented.

This means your safety is doubly vulnerable in non-English languages. The model is less likely to recognize the harmful intent of the request (because safety alignment is weaker in that language) AND less likely to follow the safety instruction in the system prompt even if it does recognize the harm (because instruction compliance is weaker in that language). Both failure modes compound, creating a safety gap that is wider than either one alone.

The compound effect is measurable. If the model's harmful-intent recognition rate in Thai is 78 percent (meaning it recognizes 78 percent of harmful requests as harmful) and its instruction compliance in Thai is 75 percent (meaning it follows system prompt safety instructions 75 percent of the time when it does recognize harm), the effective safety rate is roughly 0.78 times 0.75, which equals 58.5 percent. That means over 40 percent of harmful requests in Thai may receive a compliant response. Compare this to English, where harmful-intent recognition at 97 percent and instruction compliance at 95 percent produce an effective safety rate of about 92 percent.

These numbers are approximate. The actual rates depend on the model, the type of harmful content, and the specific language. But the compounding effect is real and consistent: every percentage point you lose in intent recognition multiplies with every percentage point you lose in instruction compliance, creating a gap that is larger than either factor alone would suggest.

## Per-Language Safety Testing

You must test safety constraints in every supported language. English safety scores do not transfer. This is not a recommendation. It is a requirement that any serious multilingual deployment must meet.

Build a multilingual safety test suite that contains the same harmful prompts translated into every language you support. The prompts should cover your safety-critical categories: violence, weapons, illegal activities, personally identifiable information, medical advice beyond your product's scope, legal advice beyond your product's scope, and any domain-specific safety boundaries your product requires.

For each language, run the full test suite and measure the refusal rate: what percentage of harmful prompts does the model correctly refuse? Track this metric per language and per safety category, because the model may refuse violence-related prompts reliably in Korean but fail to refuse requests for personal information in Korean.

The test suite should include not just direct harmful requests but also indirect and obfuscated ones. Test with polite requests ("could you please help me with..."), hypothetical framing ("in a fictional scenario, how would one..."), role-play instructions ("pretend you are a character who..."), and language-mixed requests (requests that start in the safe language and switch to the target language mid-sentence). These variations test the model's safety robustness under conditions closer to real-world usage than direct harmful prompts.

Set a minimum acceptable refusal rate per language, and do not ship any language that falls below it. If your English refusal rate is 96 percent and your minimum acceptable rate is 85 percent, any language that scores below 85 percent needs additional safety mitigation before launch. This minimum threshold is a product decision that should involve your safety team, your legal team, and your leadership, because it defines how much safety risk you are willing to accept for non-English users.

## The Translate-Then-Classify Pipeline

One of the most effective mitigations for multilingual safety is the **Translate-Then-Classify** pipeline: translate the user's input into English, run it through your English safety classifier, then allow or block based on the English classification result.

This approach works because your English safety classifier is your strongest classifier. It was trained on the most data, tested most thoroughly, and has the highest precision and recall. By translating non-English input into English before classification, you leverage the strength of your English classifier across all languages.

The pipeline works as follows. When a user sends a message, detect the language. If the language is not English, translate the message to English using a machine translation model (a dedicated translation model, not the same model that will generate the response). Run the English translation through your safety classifier. If the classifier flags the message as harmful, block it or refuse it -- do not pass it to the generation model. If the classifier passes the message, send the original (untranslated) message to the generation model for response.

The translate-then-classify pipeline adds latency and cost. The translation step typically adds 100 to 300 milliseconds depending on the message length and the translation model's speed. The classification step adds another 50 to 150 milliseconds. For a product where total response time is 2 to 4 seconds, this overhead is acceptable. For a real-time voice product where latency is measured in hundreds of milliseconds, the overhead may be too large.

There are two important failure modes. First, translation can alter the harmful intent of a message. A request that is harmful in the original language may translate into English in a way that appears innocuous, because the translation loses the cultural context or the linguistic nuance that makes it harmful. Second, the translation may introduce false positives -- innocuous messages that translate into English in a way that triggers the safety classifier. Both failure modes require monitoring: track the rate of messages that are blocked after translation and manually review a sample to check for false positives and false negatives.

Despite these limitations, the translate-then-classify pipeline significantly narrows the multilingual safety gap. Teams that implement it typically see their non-English refusal rates improve by 10 to 20 percentage points, bringing Tier 2 and Tier 3 languages much closer to English safety levels.

## Language-Specific Safety Classifiers

For your highest-traffic non-English languages, building dedicated safety classifiers produces better results than the translate-then-classify pipeline. A Japanese safety classifier trained on Japanese harmful content will catch harmful requests that lose their intent during English translation. A Spanish safety classifier trained on Spanish harmful content will understand cultural contexts and idiomatic expressions that no translation can preserve.

Training a language-specific safety classifier requires a labeled dataset of harmful and safe examples in that language. The dataset must cover the same safety categories as your English classifier, using naturally written examples (not translated from English) that reflect how harmful requests actually appear in that language and culture. Harmful content takes different forms in different cultures. A request that is harmful in one cultural context may be innocuous in another, and vice versa. The classifier must learn the language-specific expressions of harm, not just translated versions of English harmful patterns.

The investment is significant. Creating a high-quality labeled dataset for a single language requires native speakers who understand both the language and the safety domain. Training and evaluating the classifier requires ML engineering time. Maintaining the classifier as new categories of harm emerge requires ongoing annotation. For most teams, dedicated safety classifiers are justified only for their top three to five languages by user volume and safety risk. The remaining languages use the translate-then-classify pipeline as a baseline.

A hybrid approach works well in practice: use the translate-then-classify pipeline as a universal baseline for all languages, then add language-specific classifiers for your highest-priority languages. The language-specific classifier runs in parallel with the translate-then-classify pipeline, and either one flagging a message is sufficient to trigger a block. This layered approach catches more harmful content than either mechanism alone.

## Multilingual Safety Training Data

The root cause of the English Safety Blind Spot is the English dominance of safety training data. The long-term fix is to include multilingual safety data in the model's alignment training.

Research published in 2025 on multilingual safety alignment -- notably the MPO (Multilingual reward gaP Optimization) approach -- demonstrated that it is possible to transfer safety capabilities from English to other languages by minimizing the reward gap between English and target language safety responses. Experiments on models including Llama 3.1, Gemma 2, and Qwen 2.5 validated that MPO improves multilingual safety alignment without degrading general multilingual utility.

If you are fine-tuning your own models, include multilingual safety examples in your training data. Create refusal examples in your target languages -- examples where the model receives a harmful request in Japanese and responds with an appropriate Japanese refusal, or receives a harmful request in Arabic and responds with an appropriate Arabic refusal. These examples directly train the model's safety behavior in the target language rather than relying on cross-lingual transfer.

The volume does not need to match English. Even a small number of safety examples per language -- 200 to 500 harmful-request-and-refusal pairs per language -- can meaningfully improve safety refusal rates, because the model already has the underlying safety concept from English training and the target-language examples help it activate that concept when processing non-English input.

If you are using API models from providers (OpenAI, Anthropic, Google), you do not control the training data. In this case, your safety strategy must rely on the external mechanisms described above: system prompt safety instructions, the translate-then-classify pipeline, language-specific classifiers, and output-side safety filtering. These mechanisms are not as elegant as training-time solutions, but they are available to every team regardless of the model they use.

## Output-Side Safety Filtering

The mitigations discussed so far focus on the input side: catching harmful requests before they reach the generation model. Output-side filtering adds a second layer by checking the model's response for harmful content before it reaches the user.

Output-side filtering is particularly important for multilingual safety because the model can generate harmful content without receiving an explicitly harmful request. A prompt that is innocuous in isolation may trigger harmful output when combined with the model's associations in a particular language. For example, a request about traditional medicine in a certain language may generate output that includes genuinely dangerous medical advice, not because the user requested it but because the model's training data in that language included unreliable medical information.

Implement output-side filtering using the same architecture as input-side filtering: translate the response to English (if it is not already in English) and run it through your safety classifier. If the response contains harmful content, block it and either retry the generation or return a generic safe response.

The combination of input-side and output-side filtering creates defense in depth. A harmful request that passes the input filter because its intent is lost in translation may still produce output that the output filter catches. A benign request that produces harmful output due to the model's language-specific biases is caught by the output filter even though the input filter correctly passed it through. Neither filter alone provides sufficient protection. Together, they narrow the safety gap to a level that, while not equal to English safety, is within a manageable risk range for most production deployments.

## The Cost of Multilingual Safety

Multilingual safety is expensive. The translate-then-classify pipeline adds latency and cost per request. Language-specific classifiers require training data, engineering time, and ongoing maintenance. Output-side filtering doubles the classification overhead. Native-speaker safety testing requires specialized reviewers for each language.

The temptation is to defer these investments until a safety incident occurs. This is a mistake for three reasons. First, safety incidents in non-English languages are less likely to be detected quickly, because your monitoring is probably English-centric too. A harmful output in Amharic may circulate for weeks before anyone on your team notices, while the same output in English would be flagged within hours. Second, the liability exposure is the same regardless of language. Under the EU AI Act, a safety failure in Bulgarian carries the same regulatory consequence as a safety failure in English. Third, the reputational damage may be worse, because a safety failure that disproportionately affects non-English speakers looks like a company that cares less about certain user populations -- a narrative that is difficult to recover from.

Budget for multilingual safety from the start. Include it in your cost estimates for multilingual expansion. Treat it as a prerequisite for launching in a new language, not as an afterthought to address once the launch is complete. The teams that build multilingual safety into their launch process never have to explain why their product was unsafe for a specific language community. The teams that defer it hope they never have to.

## Monitoring the Multilingual Safety Gap

Safety is not a milestone. It is a metric. Monitor it continuously across all supported languages.

Track safety refusal rates per language per week. Track the rate of messages that trigger the translate-then-classify pipeline versus those that pass through. Track the false positive rate per language (innocuous messages incorrectly blocked). Track the rate at which output-side filtering catches harmful content that input-side filtering missed. These metrics, viewed together, give you a real-time picture of your multilingual safety posture.

Set alerts for safety regression. If the refusal rate in any language drops below your minimum threshold, investigate immediately. Safety regressions can occur after model updates (the new model version may have different safety behavior in specific languages), after prompt changes (a change to the system prompt may weaken a safety instruction's effectiveness in certain languages), or after changes to the translation model (a translation model update may alter how harmful requests translate, affecting classifier accuracy).

The safety gap is not static. It shifts with every model update, every prompt revision, and every change in the threat landscape. The teams that monitor it continuously catch regressions before they cause harm. The teams that test once and assume stability are always one model update away from an incident they did not see coming.

Section 16 covers AI security in depth, including the adversarial dimension of multilingual attacks. Section 22 covers red-teaming practices that should include multilingual attack scenarios. The next subchapter addresses dynamic language routing in multi-turn conversations -- how to maintain the correct language when users switch languages mid-conversation and the system must follow without losing context or safety posture.