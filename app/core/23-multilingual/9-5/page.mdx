# 9.5 â€” Religious, Political, and Historical Sensitivity by Region

Your model will be asked about God. About war. About who was right and who was wrong. About whether a territory is a country or a province, whether a historical figure was a liberator or a tyrant, whether a religious practice is sacred or superstitious. And the answer it gives will depend on which language the question is asked in -- because the training data for each language encodes different narratives, different heroes, different villains, and different definitions of truth. Users in one language will find the answer obvious and correct. Users in another language will find the same answer offensive, dangerous, or illegal. There is no response that satisfies everyone. There is only the question of how thoughtfully you navigate the impossibility.

This is not a fringe concern. Religious, political, and historical questions are among the most common queries that multilingual models receive. Users ask about their faith, their country's history, their government's policies. Students write essays about contested events. Journalists research sensitive topics. Citizens seek information about rights and laws that differ by jurisdiction. Every one of these queries puts the model in a position where its response is a political act -- whether it intends to be or not.

## Religious Content: The Blasphemy Minefield

Religious sensitivity in AI is not about respecting feelings. It is about legal liability, physical safety, and the recognition that billions of people structure their lives around beliefs that the model's training data may treat as one perspective among many.

The depiction of the Prophet Muhammad is the most acute example. In mainstream Islamic theology, visual or physical descriptions of the Prophet are forbidden. This is not a preference. It is a deeply held religious conviction shared by the overwhelming majority of Muslims worldwide. A model that generates a physical description of the Prophet -- his appearance, his clothing, his facial features -- in response to a historical or educational query is not being informative. It is producing content that constitutes blasphemy for approximately 1.9 billion people. In countries with blasphemy laws -- Pakistan, Indonesia, Saudi Arabia, Iran, Malaysia, and others -- generating this content can expose users, platform operators, and the company itself to criminal prosecution. Pakistan's blasphemy laws carry penalties up to death. These are not hypothetical risks.

Hindu religious sensitivities operate along different axes. Cow-related content -- beef recipes, leather recommendations, cattle slaughter descriptions -- is deeply offensive to many Hindus and legally restricted in multiple Indian states. But the sensitivity extends further. Depictions of Hindu deities in inappropriate contexts, casual or disrespectful references to religious texts, and content that treats Hinduism as mythology rather than living faith all carry potential for offense. The model's Western-trained default often treats all religions as equivalent subjects for academic analysis, which works for users who share that secular-academic framing but not for users whose relationship to their faith is devotional rather than analytical.

Buddhist sensitivities vary by country. In Thailand and Myanmar, criticism of Buddhist monks or the sangha (monastic community) is deeply taboo and, in Thailand, potentially criminal under lese-majeste-adjacent religious protections. In Sri Lanka, Buddhist nationalism has created an environment where any content perceived as disrespectful to Buddhism can trigger communal violence. In Japan, where Buddhism coexists with Shinto in a syncretic tradition, the sensitivities are different -- less about blasphemy and more about the appropriate register for discussing spiritual matters.

Catholic and Christian sensitivities are often underestimated by secular Western teams who may consider Christianity a "safe" topic for critical analysis. In Latin America, the Philippines, Poland, and much of sub-Saharan Africa, Catholic faith is central to identity in ways that Western European secularism does not reflect. Content that treats Catholic doctrine dismissively, that jokes about saints or sacraments, or that presents historical criticisms of the Church without context can alienate millions of users. The model must recognize that its training data's predominantly secular-analytical treatment of Christianity is itself a cultural perspective, not a neutral default.

## Political Content: Every Answer Is a Position

Political sensitivity in multilingual AI is defined by a simple, uncomfortable fact: on the most contested political questions, there is no neutral answer. Every response takes a position, including the response that tries not to.

Taiwan's political status is the defining case. The People's Republic of China considers Taiwan a province. Taiwan governs itself as an independent democracy. The United States maintains a policy of strategic ambiguity. A model asked "Is Taiwan a country?" cannot answer "yes" without contradicting China's position and potentially violating laws or content regulations in mainland China. It cannot answer "no" without contradicting Taiwan's self-governance and offending Taiwanese users. It cannot answer "it's complicated" without appearing evasive. And it cannot refuse to answer without frustrating users who have a legitimate informational need.

This pattern repeats across dozens of territorial and sovereignty disputes. Kashmir is Indian territory according to India and disputed territory according to Pakistan and international observers. Crimea is Ukrainian territory according to most of the world and Russian territory according to Russia. Palestine is a state according to over 130 countries and not a state according to Israel, the United States, and others. Kurdistan is a cultural and geographic region that spans four countries, none of which recognize Kurdish statehood. The Western Sahara, the Golan Heights, Northern Cyprus -- each has multiple legitimate framings that cannot be simultaneously correct.

The model's default framing -- the one it produces when no system prompt guidance overrides its training -- typically reflects the perspective of English-language Western media, which is the dominant source of its training data. This means it defaults to framings that align with US and Western European foreign policy perspectives: Taiwan's status is ambiguous, Kashmir is disputed, Palestine's statehood is contested. These framings are not neutral. They are the perspective of one set of actors in each dispute, presented as the objective middle ground.

## Historical Content: Whose Story Gets Told

Historical events are not neutral facts. They are narratives constructed by the people who write history, and who writes history is determined by who holds power. A model trained on English-language text inherits the historical narratives of English-speaking countries, which means it inherits the perspectives of former colonial powers on colonialism, of victors on wars, and of dominant groups on the struggles of marginalized peoples.

The question of whether colonialism was beneficial is instructive. In English-language academic and popular text, this question is debated -- a position exists that argues colonialism brought infrastructure, education, and governance to colonized peoples. In the countries that were colonized, this position is not a debate topic. It is an insult. It erases the violence, extraction, economic devastation, and cultural destruction that colonialism inflicted. When a model, prompted about the effects of British colonialism in India, produces a "balanced" response that lists both "positive" and "negative" effects, it is not being neutral. It is reproducing a colonial perspective that treats the subjugation of hundreds of millions of people as a matter where "both sides" have valid points.

The Partition of India in 1947 is another flashpoint. For Indian users, Partition is often framed as a tragic consequence of British colonial policy and Muslim League politics. For Pakistani users, Partition was the necessary creation of a homeland for South Asian Muslims. For Bangladeshi users, the story includes a second partition in 1971 and the violence of the Bangladesh Liberation War. A model asked "Was Partition necessary?" will produce different answers depending on which language it receives the question in -- not because it has been programmed to give different answers, but because the training data in Hindi, Urdu, and Bengali contains different dominant narratives.

The Armenian Genocide is recognized as genocide by most countries and by the vast majority of historians. Turkey officially denies that the events constitute genocide. A model asked about the Armenian Genocide in Turkish may reproduce Turkish state narratives from its Turkish training data, while the same model asked in Armenian or English will describe it as a genocide. The model is not making a historical judgment. It is reflecting the dominant narrative in each language's training data.

The Transatlantic slave trade presents a different challenge. In English, particularly American English, the dominant narrative has shifted over the past several decades toward a more honest reckoning with slavery's horrors and lasting effects. In Portuguese, Spanish, and Dutch -- the languages of other major slave-trading nations -- the narrative shift has been less complete. A model generating content about slavery in Portuguese may produce a version that minimizes Portugal's role or treats abolition as the primary narrative, because that is the dominant framing in Portuguese-language text.

## The Neutrality Trap

Many teams respond to these challenges by trying to make the model neutral. Do not take sides. Present all perspectives. Let the user decide. This approach sounds reasonable and is deeply flawed.

Neutrality is itself a position. When a model presents the question of whether the Rwandan genocide happened as a matter with "multiple perspectives," it is giving airtime to genocide denial. When a model presents the flat Earth theory alongside scientific consensus as two perspectives worthy of equal consideration, it is undermining scientific literacy. And when a model presents the question of whether colonial powers benefited colonized peoples as a balanced debate, it is legitimizing a narrative that the victims of colonialism find deeply offensive.

The harder truth is that some questions have answers. The Armenian Genocide happened. The Holocaust happened. Slavery was wrong. Colonialism was violent and extractive. These are not matters of perspective. A model that hedges on them in pursuit of neutrality is not being fair. It is being cowardly in a way that causes real harm to the descendants of victims.

But the line between "questions that have answers" and "questions that are genuinely contested" is itself contested. Whether Kashmir should be Indian or independent is a genuinely contested political question. Whether Taiwan is a country is a genuinely contested geopolitical question. Whether capitalism or socialism is a better economic system is a genuinely contested ideological question. The model must distinguish between questions where neutrality is appropriate and questions where neutrality is harmful, and that distinction is not always clear.

## Content Policies: Per-Market vs. Global

Companies deploying multilingual AI systems face a binary choice that is actually a spectrum. At one end: a single global content policy that treats all topics the same way regardless of the user's location or language. At the other end: per-market content policies that adapt the model's behavior to each region's legal requirements, cultural norms, and political sensitivities.

The global approach has the advantage of consistency. The model says the same thing about Taiwan in English, Mandarin, and Japanese. Users know what to expect. The company can defend its policies as principled rather than expedient. The disadvantage is that a single policy will be too restrictive in some markets and too permissive in others. A policy that avoids all discussion of Taiwan's status frustrates users seeking factual information. A policy that describes Taiwan as an independent democracy may violate Chinese law.

The per-market approach has the advantage of cultural relevance and legal compliance. The model's responses about Kashmir are calibrated for Indian users and separately calibrated for Pakistani users. The model's handling of LGBTQ content respects the legal realities of each jurisdiction. The disadvantage is that per-market policies create the appearance -- and sometimes the reality -- of hypocrisy. A company that censors LGBTQ content in Saudi Arabia while promoting LGBTQ inclusion in the United States is making a value judgment that commercial access to the Saudi market is worth compromising its stated values. Users and advocacy groups notice this inconsistency.

Most companies in practice end up somewhere in between. They maintain a global baseline -- universal prohibitions on content that incites violence, promotes child exploitation, or constitutes clear illegal activity in virtually all jurisdictions. Above this baseline, they create per-market layers that handle culturally variable topics: political sovereignty, religious content, historical narratives, LGBTQ content. These layers are implemented through per-region system prompts, content filters, or routing rules that adjust the model's behavior based on user locale, language, or deployment region.

## Implementation: Per-Region Content Guardrails

Operationalizing religious, political, and historical sensitivity requires infrastructure that goes beyond a single safety classifier.

**Topic sensitivity classifiers** identify when a query touches a sensitive domain for a specific region. These classifiers are trained per region on examples of sensitive queries in the local language. A query about beef recipes is flagged as sensitive in Hindi and several other Indian languages but not in English or French. A query about Taiwan's political status is flagged as sensitive in Mandarin but treated as a standard factual question in most other languages. The classifier does not block the query. It routes it to a handling pathway that applies the appropriate content policy for that region.

**Tiered response policies** define how the model handles each sensitivity tier. Tier one is universal: the model refuses to generate content that incites violence or promotes clearly illegal activity regardless of language or region. Tier two is regionally restricted: the model applies region-specific guardrails to topics like political sovereignty, blasphemy, and historical narratives, producing responses calibrated for the user's cultural and legal context. Tier three is monitored: the model generates responses freely but logs them for review, allowing the safety team to detect emerging sensitivity patterns before they become incidents.

**Escalation paths** handle queries that fall outside the classifier's confidence range. When the model encounters a query about a topic that might be sensitive but the classifier is not confident, the query is escalated -- either to a human reviewer or to a more conservative response pathway that acknowledges the topic's sensitivity without taking a position. This is preferable to both blocking the query entirely (which frustrates users) and generating a response without sensitivity awareness (which risks harm).

**Regular policy review cycles** ensure that content policies stay current as political and cultural contexts shift. An election in Taiwan, a military action in Kashmir, a legislative change in a country with blasphemy laws -- any of these can change the sensitivity landscape overnight. Content policies that were calibrated six months ago may be dangerously outdated. Quarterly reviews with cultural advisory panels are the minimum cadence, with ad hoc reviews triggered by major events.

## The Ethical Bedrock

Beneath the policy layers and classifier architectures, there is a set of ethical decisions that no technology can make for you. Will your model comply with blasphemy laws that restrict free expression? Will it comply with laws that criminalize LGBTQ identity? Will it present colonial history through the lens of the colonizer when the user speaks the colonizer's language? Will it tell users in one country a different version of history than users in another?

These are not engineering decisions. They are moral decisions that your leadership team must make, document, and defend. The engineering team implements whatever the leadership decides, but the decision itself requires ethical reasoning, legal counsel, and genuine engagement with the people who will be affected by the choice.

The one thing you cannot do is avoid the decision. Deploying a multilingual model without explicit policies for religious, political, and historical content is not neutrality. It is delegation of moral decisions to the training data -- to whatever narratives happened to be most common on the internet in the languages your model was trained on. That is not a principled position. It is an abdication of responsibility dressed up as objectivity.

Religious, political, and historical sensitivity shapes what the model says. But how the model says it -- the grammatical structures, the pronouns, the gendered forms -- creates a different and equally complex set of cultural challenges. The next subchapter examines how gender and pronoun handling across grammatically gendered languages forces design decisions that most English-speaking teams have never considered.