# 5.3 â€” Translation Quality: BLEU Is Dead, What Replaces It

BLEU score measures word overlap with a reference translation. It cannot detect meaning errors, tone mismatches, or cultural inappropriateness. A translation can score 0.45 on BLEU -- a respectable number by historical standards -- while containing a factual error that reverses the meaning of a medical instruction. Another translation can score 0.22 on BLEU -- a number that would alarm any dashboard -- while being a perfectly natural, accurate rendering that simply uses different vocabulary than the reference. For two decades, BLEU was the default metric for machine translation quality because it was cheap, fast, and reproducible. In 2026, it is a legacy metric that measures the wrong thing, and teams that still rely on it are making quality decisions based on noise.

This subchapter traces the evolution from BLEU to the neural metrics that replaced it, explains what each metric actually measures, and shows you how to build a translation quality pipeline that gives you real signal instead of comforting numbers.

## Why BLEU Fails

**BLEU** -- Bilingual Evaluation Understudy -- was introduced in 2002 by IBM researchers. It works by counting how many n-grams (sequences of one, two, three, and four consecutive words) in the candidate translation also appear in one or more reference translations. The more overlap, the higher the score. A perfect overlap produces a score near 1.0. Random text produces a score near 0.0.

The metric has three fundamental problems that make it unreliable for modern translation quality assessment.

First, BLEU penalizes valid paraphrases. If the reference says "the patient should take this medication twice daily" and the candidate says "this medicine should be taken by the patient two times each day," the meaning is identical but the word overlap is low. BLEU scores the second translation poorly because it used different words, even though both translations are equally correct. This problem is severe for morphologically rich languages like Arabic, Turkish, and Finnish, where the same meaning can be expressed with radically different word forms. It is equally severe for languages like Japanese and Korean, where word order flexibility means that valid translations can rearrange sentence components without changing meaning.

Second, BLEU cannot detect meaning errors. If the reference says "do not take this medication with alcohol" and the candidate says "do take this medication with alcohol," the word overlap is extremely high -- every word except "not" is shared. BLEU scores this translation well. A human immediately sees that the missing negation is a dangerous error. BLEU does not, because BLEU does not understand meaning. It counts words.

Third, BLEU requires reference translations, which introduces a bottleneck. For every segment you want to evaluate, you need at least one -- ideally multiple -- human-quality reference translations. Creating references is expensive and slow. For dynamic AI-generated content that changes with every request, producing reference translations is impractical. You cannot pre-write a reference for text that does not exist yet.

Despite these flaws, BLEU persisted for over twenty years because it was simple, fast, deterministic, and universally understood. Researchers cited BLEU scores in papers. Teams tracked BLEU on dashboards. Everyone knew the number was noisy, but it was the number everyone had, so it became the standard. That era is over. The neural metrics that replaced BLEU are now mature, fast enough for production use, and vastly more aligned with human quality judgments.

## COMET: The New Standard

**COMET** -- Crosslingual Optimized Metric for Evaluation of Translation -- is a neural metric developed by Unbabel that computes translation quality by comparing the source text, the candidate translation, and a reference translation using a multilingual language model. Instead of counting word overlaps, COMET encodes all three texts into a shared embedding space and predicts a quality score based on learned patterns from hundreds of thousands of human quality judgments.

The difference in practice is dramatic. COMET recognizes that two sentences can use completely different words and still convey the same meaning, because its underlying language model understands semantic similarity. It detects meaning errors that BLEU misses, because meaning shifts produce different embeddings even when the surface words are similar. It correlates with human quality assessments far more strongly than BLEU -- research presented at WMT (the Workshop on Machine Translation) has consistently shown COMET achieving segment-level correlations with human judgment above 0.70, while BLEU typically falls between 0.30 and 0.45.

The current production-ready version, based on the wmt22-comet-da model, produces scores on a scale where higher values indicate better translations. A score above 0.85 typically indicates high-quality translation. A score between 0.70 and 0.85 indicates acceptable quality with some issues. Below 0.70 suggests significant quality problems. These thresholds are approximate and vary by language pair, but they provide a far more reliable signal than BLEU scores for making quality decisions.

COMET has been validated across more than 100 languages and 230 translation directions, making it practical for genuinely multilingual evaluation. It runs in seconds per segment on modern hardware, which makes it fast enough for batch evaluation of translation pipelines. It is open source, available through the Unbabel COMET library, and integrates with standard Python ML workflows.

The limitation of standard COMET is that it requires a reference translation. For offline evaluation of translation systems -- comparing two NMT engines, evaluating a new model, auditing translation quality on a test set -- this is fine. You prepare reference translations once and reuse them. For real-time quality monitoring of production translations, where you need to assess quality without a reference, you need the reference-free variants.

## Reference-Free Metrics: CometKiwi and xCOMET

The reference-free variants of COMET are what make neural translation metrics practical for production monitoring. **CometKiwi** takes only the source text and the candidate translation -- no reference needed -- and predicts a quality score. It was trained on quality estimation data from WMT shared tasks, where human annotators rated translations without seeing a reference. The model learned to predict translation quality from the source-candidate pair alone.

CometKiwi's accuracy is lower than reference-based COMET -- without a reference, there is less information to make the judgment. But its correlation with human judgment still substantially exceeds BLEU's, and critically, it can run on every translation your system produces without requiring a pre-written reference. This makes it suitable for real-time quality monitoring: score every translated output, flag items below a threshold, and route flagged items to human review.

**xCOMET** takes quality estimation further by providing not just a segment-level score but also error span detection. When xCOMET evaluates a translation, it identifies the specific words or phrases that contain errors and categorizes those errors -- accuracy, fluency, terminology, style. This is closer to what a human translator does when post-editing: not just "this translation is 0.72 quality" but "the third sentence has an accuracy error where the negation is missing, and the sixth sentence has a fluency error where the word order is unnatural."

xCOMET comes in two sizes. xCOMET-XL at 3.5 billion parameters runs efficiently on a single GPU and is suitable for batch evaluation. xCOMET-XXL at 10.7 billion parameters provides higher accuracy but requires more computational resources. For most production quality monitoring, xCOMET-XL provides sufficient accuracy at practical cost.

The combination of segment-level scores and error span detection makes xCOMET particularly valuable for MTPE workflows. Instead of sending all translations to human post-editors, you send only the ones where xCOMET detected errors, along with the specific spans that need attention. The post-editor knows exactly where to look, which reduces editing time and cost.

## BLEURT: The Google Alternative

**BLEURT** -- Bilingual Evaluation Understudy with Representations from Transformers -- is Google's neural translation metric. Like COMET, it uses a pre-trained language model to assess translation quality beyond surface-level word overlap. BLEURT is pre-trained on synthetic data -- millions of automatically generated translation pairs with known quality labels -- and then fine-tuned on human quality judgments.

BLEURT produces scores roughly between 0 and 1, where 0 indicates random output and 1 indicates a perfect translation. Its current recommended version, BLEURT-20, provides solid correlation with human judgment and is widely used in research.

In practice, BLEURT occupies a narrower niche than COMET in 2026. It requires a reference translation, which limits its use in production monitoring. Its correlation with human judgment is competitive with but generally slightly below COMET's on recent WMT benchmarks. It is also reference-only -- there is no widely adopted reference-free BLEURT variant equivalent to CometKiwi. Teams that are choosing a primary neural translation metric in 2026 typically choose COMET for its broader ecosystem, reference-free variants, and error span detection capabilities. BLEURT remains a valid secondary metric and is worth including in evaluation pipelines for robustness, but it is not the primary metric most teams build around.

Researchers have also identified a vulnerability in BLEURT: certain adversarial translations can score artificially high by exploiting patterns in the metric's training data. This is not a practical concern for honest evaluation of your own translation system, but it is worth noting that no single metric is immune to gaming. Multiple metrics, combined with human evaluation, provide the most robust quality signal.

## MQM: The Gold Standard for Human Evaluation

Automated metrics, no matter how sophisticated, are proxies for human judgment. To calibrate those proxies, and to evaluate quality in dimensions that no automated metric captures well -- cultural appropriateness, formality correctness, brand voice adherence -- you need human evaluation. **MQM** -- Multidimensional Quality Metrics -- is the framework that the translation industry uses for structured human evaluation.

MQM works by having expert annotators read a translation and mark every error they find. Each error is categorized along two dimensions: type and severity. Error types include accuracy (wrong meaning, additions, omissions), fluency (grammar, spelling, punctuation, natural flow), terminology (wrong term, inconsistent term use), and style (register mismatch, awkward phrasing, cultural inappropriateness). Severities include minor (noticeable but not confusing), major (causes confusion or misunderstanding), and critical (causes harm or completely wrong meaning).

The scored output is a structured error report, not a single number. A translation might have zero accuracy errors, two minor fluency errors, and one major terminology error. This granularity tells you exactly what is wrong and where to invest in improvement. If your Japanese translations consistently show major terminology errors in financial vocabulary, you know to invest in a financial glossary. If your Spanish translations show minor fluency issues with verb conjugation, you know the machine is close but needs post-editing for verb forms. MQM turns "this translation is 72% quality" into an actionable diagnosis.

MQM evaluation is expensive. A trained annotator evaluating a single document takes 30 minutes to two hours depending on length and complexity. Annotator rates for qualified bilingual experts range from $30 to $80 per hour depending on the language pair and domain. Evaluating a 5,000-word document may cost $50 to $150. At scale, evaluating thousands of translations with MQM is impractical. This is why MQM is used for sampling, calibration, and high-stakes evaluation, not for monitoring every translation in production.

The MQM Council released updated scoring models in recent years, including a non-linear scoring model that accounts for how human tolerance for errors decreases as readers encounter more problems in a document. A single minor error in a 500-word text barely registers. Three minor errors begin to erode trust. Ten minor errors in the same text produce a perception of low quality even if no single error is significant. The non-linear model captures this cumulative effect, which linear error-counting approaches miss.

## LLM-as-Judge for Translation Quality

A newer approach gaining traction in 2026 is using frontier LLMs to evaluate translation quality. You provide the source text, the candidate translation, and a detailed evaluation rubric, and the LLM returns a quality assessment -- often including error identification, severity classification, and an overall score.

The appeal is speed and cost. An LLM can evaluate a translation in seconds at a cost of a few cents, compared to minutes or hours at tens of dollars for a human MQM evaluation. For high-volume monitoring where human evaluation is impractical, LLM-as-judge fills the gap between automated metrics and human assessment.

The limitations are real. LLMs carry biases in translation evaluation, just as they carry biases in translation itself. They tend to favor translations that match their own generation patterns, which can penalize valid translations that use different but equally correct phrasing. They sometimes miss subtle errors that human experts catch -- a formality mismatch in Japanese, a gender agreement error in Arabic, a cultural reference that is technically accurate but contextually inappropriate. Research from Translated in 2025 demonstrated that COMET can fall short on LLM-based machine translation evaluation specifically because the neural metric was trained on older NMT outputs and may not fully generalize to the distribution of LLM-generated translations.

The practical guidance is to use LLM-as-judge as a middle tier between automated metrics and human evaluation, not as a replacement for either. Automated metrics like COMET and xCOMET provide fast, cheap, consistent scoring across all translations. LLM-as-judge provides deeper analysis on flagged items -- translations where automated scores are low or uncertain. Human MQM evaluation provides the definitive assessment on a sample, calibrates both automated layers, and catches the errors that neither automated approach reliably detects.

## Building Your Translation Quality Pipeline

A production-grade translation quality pipeline combines all four layers -- automated metrics, reference-free monitoring, LLM-as-judge, and human evaluation -- into a continuous quality system.

**Layer 1: Score everything.** Every translation your system produces -- whether by NMT, LLM, or MTPE -- gets scored by a reference-free metric like CometKiwi or xCOMET. This scoring runs automatically, in batch or near-real-time, with no human involvement. The scores feed a dashboard that shows translation quality by language, by content type, by translation method, and over time.

**Layer 2: Flag and triage.** Translations scoring below your quality threshold -- say, below 0.75 on CometKiwi -- get flagged automatically. The flagged items go to a triage queue where an LLM-as-judge provides a more detailed assessment: what type of error, where in the text, how severe. Items that the LLM-as-judge confirms as problematic go to human review. Items that the LLM-as-judge rates as acceptable despite the low CometKiwi score are logged for later analysis but not escalated.

**Layer 3: Human review on a sample.** A random sample of translations -- not just the flagged ones -- goes to human MQM evaluation on a regular cadence. This sample calibrates your automated metrics. If human evaluators consistently find errors that CometKiwi misses, your threshold is too permissive. If human evaluators consistently approve translations that CometKiwi flags, your threshold is too strict. The calibration loop keeps your automated quality signal honest.

**Layer 4: Reference evaluation on benchmarks.** For periodic deep evaluation -- quarterly, or after major changes to your translation pipeline -- run reference-based COMET on a held-out test set with professional reference translations. This gives you the most accurate automated quality measurement and lets you track quality trends over time against a stable benchmark. Compare reference-based COMET scores to your production CometKiwi scores to measure the gap between your benchmark quality and your deployed quality.

## Setting Quality Thresholds by Content Type

Not all translations need the same quality bar. A chatbot response to a casual user query can tolerate minor fluency issues that would be unacceptable in a translated legal document. Your quality thresholds should reflect this.

**Critical content** -- legal terms, medical instructions, financial disclosures, regulatory filings -- should target a CometKiwi score above 0.88 with mandatory human review on 100% of outputs. Any error in this content carries liability risk. Machine translation of critical content should always go through full MTPE.

**User-facing content** -- product descriptions, customer communications, support articles, AI-generated responses to user queries -- should target a CometKiwi score above 0.78. Translations below this threshold get flagged for LLM-as-judge review and potential human correction. This tier balances quality with throughput.

**Internal content** -- log translations, internal reports, analytics dashboards, developer documentation -- can accept a CometKiwi threshold of 0.65 or even lower. The content serves its purpose if the meaning is correct, even if the phrasing is awkward. Human review is unnecessary for this tier.

These thresholds are starting points, not absolutes. Calibrate them against your own human evaluation data. If your human MQM reviewers find that translations scoring 0.80 on CometKiwi are consistently acceptable, you can lower the user-facing threshold to free up human review capacity for other tasks. If they find errors below 0.85, tighten the threshold. The numbers must come from your data, your languages, and your quality standards -- not from generic benchmarks.

## The Metric Transition

If your team currently tracks BLEU, the transition to neural metrics is straightforward but requires resetting organizational expectations. BLEU scores are deeply embedded in team vocabulary -- "we need a BLEU of 30," "the new model gained two BLEU points." These numbers carry years of intuition that does not transfer to COMET scores.

Run both metrics in parallel for at least one evaluation cycle. Show stakeholders the cases where BLEU and COMET disagree -- the translations that score well on BLEU but poorly on COMET, and the translations that score poorly on BLEU but well on COMET. Manually review those disagreement cases with native speakers. In nearly every case, the native speaker's judgment aligns with COMET, not BLEU. That alignment is the argument. Let the evidence do the convincing.

Once the team trusts the new metric, deprecate BLEU. Do not keep it as a secondary metric "for comparison." Keeping both metrics creates confusion, invites metric shopping (citing whichever metric tells the better story), and delays the organizational transition to quality signals that actually mean something. BLEU served its purpose for twenty years. Thank it for its service and move on.

The next subchapter examines machine translation post-editing in depth -- the workflow, the economics, the quality tiers, and the specific practices that make MTPE the cost-quality sweet spot for most AI products operating across languages.