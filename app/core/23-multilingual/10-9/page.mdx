# 10.9 â€” Evaluating Code-Switching and Dialectal Performance

Your multilingual eval suite tests standard languages. Your users speak non-standard varieties. You are measuring the wrong thing.

This is not hyperbole. It is the state of multilingual evaluation in most organizations in 2026. The test sets are clean. The user input is messy. The gap between them is the gap between your dashboard and reality. A model that scores 88 percent on your Hindi evaluation set might score 71 percent on the Hinglish queries that constitute half your actual traffic. A model that scores 82 percent on your Arabic evaluation set might score 58 percent on the Egyptian Arabic that your Cairo users actually write. Your evaluation says the model is good. Your users say the model does not understand them. Both are telling the truth about different things.

Closing this gap requires building evaluation infrastructure that measures what users actually experience -- code-switched input, dialectal variation, informal registers, and the full spectrum of linguistic reality that standard test sets exclude.

## The Eval Gap

**The Eval Gap** is the difference between your evaluation score on standard-language test sets and your true performance on the linguistic varieties your users actually produce. It exists in every multilingual system that evaluates on formal, standard-language data while serving users who communicate in code-switched or dialectal forms.

The gap is invisible until you measure it. No automated metric will flag it. No production monitoring dashboard will surface it. The only way to discover the eval gap is to deliberately build evaluation sets that reflect your users' actual linguistic behavior and compare performance against those sets to your standard evaluation.

The size of the gap varies by market. For markets where code-switching is heavy -- India, the Philippines, Singapore, North Africa -- the gap can be 15 to 25 percentage points on comprehension and response quality metrics. For markets where dialectal variation is wide -- the Arabic-speaking world, Chinese-speaking regions, the German-speaking world (Standard German versus Swiss German versus Austrian German) -- the gap can be 10 to 30 percentage points depending on the specific dialect.

These are not small differences. A 20-point quality gap means one in five code-switching or dialectal interactions produces a noticeably degraded experience. At the scale of millions of users, that gap represents hundreds of thousands of people who are not being served by a product they are paying for.

## Building Code-Switching Evaluation Sets

Code-switching evaluation requires test sets that contain actual code-switched text, not synthetic approximations.

**Source from production.** The best code-switching evaluation data comes from your own users. If your system logs user queries (with appropriate consent and privacy protections), sample queries that contain code-switching. Bilingual annotators can identify code-switched queries quickly -- the language mixing is visually obvious. Sample 200 to 500 code-switched queries per language pair for a statistically meaningful evaluation set.

**Stratify by mixing intensity.** Not all code-switching is the same. Some queries are 90 percent in one language with a few borrowed words. Others are a roughly even mix of two languages. Others switch languages at sentence boundaries. Your evaluation set should include all three patterns, because each creates different challenges for your system. Weight the strata based on their prevalence in your production traffic.

**Include romanized variants.** For language pairs where romanization is common -- Hindi-English, Arabic-French, Thai-English -- include romanized code-switched queries alongside native-script queries. As subchapter 10.5 discussed, romanized code-switched text is the worst case for most models. If you do not evaluate it, you are hiding your worst performance.

**Create monolingual baselines.** For each code-switched query in your evaluation set, create a monolingual equivalent in each component language. A Hinglish query about loan details should have a pure Hindi version and a pure English version. The monolingual versions serve as the performance ceiling -- the best your system could achieve if the code-switching penalty were zero. The gap between the code-switched score and the better monolingual score is the precise measurement of your code-switching penalty.

**Evaluate end-to-end, not component-by-component.** Your users experience the full pipeline: from query to retrieval to generation to response. Evaluate the final response quality for code-switched input, not just retrieval recall or generation fluency in isolation. A system might retrieve well and generate poorly for code-switched input, or retrieve poorly and compensate with good generation. Only the end-to-end evaluation captures the user's actual experience.

## Building Dialect Evaluation Sets

Dialect evaluation sets require speakers of the specific dialects you serve. There is no shortcut around this requirement.

**Recruit dialect-speaking evaluators.** For each dialect you need to evaluate, recruit evaluators who are native speakers of that dialect. Not just Arabic speakers -- Egyptian Arabic speakers, Gulf Arabic speakers, Moroccan Arabic speakers. Not just Chinese speakers -- Cantonese speakers, Mandarin speakers from specific regions. The evaluators must be able to both write in their dialect (for creating test inputs) and judge whether a model's output is appropriate for their dialect (for scoring responses).

**Create dialect-specific test inputs.** Have your dialect-speaking evaluators write queries in their natural dialect on topics relevant to your product. A customer support system needs dialect queries about orders, accounts, and products. A healthcare system needs dialect queries about symptoms and medications. The queries should reflect how a real user in that dialect community would phrase the question, not how a linguist would construct a "representative dialect sample."

**Score separately per dialect.** One aggregate "Arabic quality score" is useless. You need an Egyptian Arabic quality score, a Gulf Arabic quality score, a Levantine Arabic quality score, and so on for every dialect you serve. These scores will differ, often dramatically. The differences tell you where to invest.

**Include dialect comprehension and dialect generation.** Your evaluation must cover both directions. Can the model understand dialectal input? And can the model generate appropriate dialectal output? These are different capabilities. A model might understand Egyptian Arabic queries well but always respond in MSA. The comprehension score would be high while the generation score would be low. Both matter for user experience.

## The CS-FLEURS Benchmark and Beyond

The CS-FLEURS benchmark, presented at Interspeech 2025, provides the first large-scale standardized evaluation for code-switched speech, covering 52 languages across 113 code-switched language pairs. It demonstrated that even top-performing speech models show error rates two to three times higher on code-switched input compared to monolingual input.

For text systems, standardized code-switching benchmarks are less mature. The GLUECoS benchmark covers Hindi-English and Spanish-English code-switching for NLP tasks, but it was designed for the pre-LLM era and does not evaluate the end-to-end RAG or conversation quality that matters for production systems. The HiKE benchmark, introduced in late 2025, provides hierarchical evaluation of code-switched text generation quality, but its language pair coverage is limited.

The DIVERS-Bench benchmark evaluates language identification across domain shifts and code-switching contexts, measuring how well systems identify languages when the text is code-switched or domain-shifted. This is valuable because language identification is the first step in most multilingual pipelines, and a failure here cascades downstream.

For Arabic dialects, the DialectalArabicMMLU benchmark extends the MMLU evaluation framework across five major Arabic dialects with 3,000 manually translated and adapted multiple-choice questions. It provides the most rigorous dialectal evaluation available for Arabic, but it covers knowledge comprehension tasks rather than the conversational, support, or RAG tasks that most production systems handle.

These benchmarks are useful starting points, but they are insufficient for production evaluation. Your evaluation must reflect your product's specific tasks, your users' specific dialects, and your pipeline's specific architecture. No off-the-shelf benchmark covers all three.

## The "Which Dialect" Decision

You cannot evaluate every dialect and every code-switching pair. You must choose, and the choice should be data-driven.

Start with your traffic. If you can identify the dialect or code-switching pattern of incoming queries (even roughly, using location data or language detection), you can estimate the distribution. You might discover that 55 percent of your Arabic traffic is Egyptian, 25 percent is Gulf, 12 percent is Levantine, and 8 percent is Maghrebi. This distribution tells you where to invest first: Egyptian and Gulf evaluation sets are your priority. Levantine follows. Maghrebi can wait until you have the resources.

If you cannot identify dialect from traffic -- because your system does not log enough information or your dialect detection is too unreliable -- use market data. If your product is launched in Saudi Arabia and UAE, Gulf Arabic is your priority. If your growth target is Egypt, Egyptian Arabic is your priority. If you are entering the Moroccan market, Darija evaluation is not optional.

For code-switching, the prioritization is simpler: which language pairs produce the most code-switched traffic? Hindi-English is almost always the top pair for India-serving products. Tagalog-English for the Philippines. Arabic-French for North Africa. Spanish-English for the US Latino market. Build evaluation sets for the pairs that represent the most users.

Be explicit about what you do not evaluate. If your evaluation covers Egyptian and Gulf Arabic but not Maghrebi, document that limitation. Share it with product teams so they can make informed decisions about market expansion. An eval gap you know about is a risk you can manage. An eval gap you do not know about is a quality failure waiting to happen.

## Metrics for Code-Switched and Dialectal Evaluation

Standard quality metrics -- accuracy, relevance, fluency -- apply to code-switched and dialectal evaluation but must be supplemented with language-specific metrics.

**Comprehension accuracy.** Did the model correctly understand the user's intent, regardless of the language variety used? This is the most important metric. A model that understands a Hinglish query and responds accurately in English has high comprehension accuracy even if its language match is poor. Measure this by having bilingual evaluators judge whether the response addresses the actual question asked.

**Response naturalness.** Does the response sound natural to a speaker of the user's language variety? This is distinct from grammatical correctness. A response in formal MSA to an Egyptian Arabic speaker is grammatically perfect but socially unnatural. A response that attempts Egyptian Arabic but uses vocabulary from another dialect is linguistically wrong in a different way. Naturalness requires evaluators who are native speakers of the target variety.

**Language-match appropriateness.** Did the model respond in the appropriate language or language mix? For code-switched input, was the response in a matching code-switched register, or did the model default to a single language? For dialectal input, did the model match the dialect, switch to the standard language, or switch to a different dialect? This metric captures whether the model's language behavior is socially appropriate for the context.

**Cross-variant consistency.** Does the model give the same quality of answer regardless of which dialect or code-switching pattern the question uses? A query about refund policies should get an equally accurate answer whether asked in MSA, Egyptian Arabic, or Gulf Arabic. If the Egyptian Arabic query gets a better answer than the Gulf Arabic query, you have a quality equity problem.

Score these metrics separately. An overall quality score of 4.0 that averages a 4.5 for standard language and a 3.2 for dialectal input tells you nothing useful. The segmented scores tell you everything: where your system works, where it fails, and where to invest next.

## Evaluator Selection and Training

The quality of your dialect and code-switching evaluation depends entirely on the quality of your evaluators. This is not a task you can outsource to general-purpose annotation platforms without careful selection.

For dialect evaluation, each evaluator must be a native speaker of the specific dialect being evaluated. A Syrian Arabic speaker is not qualified to evaluate Egyptian Arabic quality, and vice versa. The dialects differ enough that cross-dialect evaluation produces unreliable scores. An Egyptian evaluator might rate a Gulf Arabic response as unnatural simply because it uses Gulf vocabulary the evaluator does not recognize.

For code-switching evaluation, each evaluator must be a natural code-switcher in the relevant language pair. A bilingual speaker who does not code-switch in daily life will judge code-switched output differently than a speaker for whom code-switching is the default register. Recruit evaluators from the actual user population or from communities where the relevant code-switching pattern is standard.

Train evaluators on the specific metrics you are measuring. Comprehension accuracy, response naturalness, and language-match appropriateness require different judgment criteria. Provide examples of each score level for each metric. Calibrate evaluators against each other using a shared sample set before they begin evaluating independently.

Pay evaluators fairly. Dialect-speaking and code-switching evaluators are harder to recruit than standard-language evaluators because the pool is smaller and the skill set is more specific. Underpaying leads to evaluator attrition, which leads to inconsistent evaluation, which makes your metrics unreliable.

## Automating What You Can

Human evaluation is the gold standard for dialect and code-switching quality, but it does not scale to every query in production. You need automated proxies that approximate human judgment for ongoing monitoring.

**Language identification as a proxy.** Run a language identifier on both the user's input and the model's response. If the user wrote in Egyptian Arabic and the model responded in MSA, the language identifier should flag the mismatch. This does not measure quality, but it measures language-match at scale. Track the mismatch rate over time.

**Response language distribution.** For each dialect or code-switching pattern in your traffic, measure what percentage of responses match the user's language variety. If 45 percent of your Hindi traffic is Hinglish but only 12 percent of your responses contain code-switching, you have a systematic language-match failure.

**Embedding-based quality estimation.** Embed both the user's query and the model's response and measure their semantic similarity in a multilingual embedding space. Low similarity suggests the model may have misunderstood the query. This is a crude proxy, but it catches catastrophic comprehension failures and can be run at scale on every query.

**A/B testing by language variety.** When you deploy improvements -- new prompt strategies, new models, new retrieval configurations -- measure the impact separately for code-switched and dialectal traffic. An improvement that helps monolingual queries but hurts code-switched queries is not an improvement. Segment your A/B test results by language variety to catch this.

## The Evaluation Cadence

Code-switching and dialectal evaluation is not a one-time audit. It is an ongoing measurement program.

Run your full human evaluation set quarterly. This is the gold-standard measurement that tells you the true size of the eval gap for each language variety you serve. Four times a year is frequent enough to catch regressions from model updates or pipeline changes while being manageable for evaluator scheduling and budget.

Run your automated proxy metrics continuously. Language-match rates, response language distributions, and embedding-based quality estimates should be part of your production monitoring dashboard. These metrics do not replace human evaluation, but they catch acute regressions between human evaluation cycles.

Update your evaluation sets annually or whenever you expand to a new market. Code-switching patterns evolve as languages evolve. Dialectal norms shift as populations migrate and media influence changes. An evaluation set built in 2025 may not capture the code-switching patterns that emerge in 2026. Refresh your test data with new samples from production traffic to keep the evaluation representative.

The next subchapter closes this chapter by synthesizing everything -- code-switching, dialectal variation, evaluation -- into a practical framework for building systems that handle language variation gracefully, even when you cannot handle every variant perfectly.
