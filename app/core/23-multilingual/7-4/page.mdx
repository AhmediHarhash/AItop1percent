# 7.4 â€” Few-Shot Examples in Multilingual Systems: Selection and Placement

Should your few-shot examples be in the user's language, or in English? The question seems straightforward. It is not. The answer depends on what you want the examples to teach the model, which language the model learns patterns from most effectively, and how much token budget you can afford to spend -- a budget that varies dramatically by language.

Every few-shot example teaches the model two things simultaneously: the task pattern and the language pattern. An English example of a customer support response teaches the model both the response structure (greeting, acknowledgment, solution, closing) and the English style (word choice, sentence length, tone). A Japanese example of the same response teaches the same structure but also teaches Japanese style -- keigo politeness levels, sentence-ending particles, and the different rhythm of Japanese business prose. When you choose the language of your examples, you are choosing which of these two lessons to prioritize.

## The English Example Advantage

English few-shot examples are more effective at teaching task patterns. This is not a controversial claim. It follows directly from the same training data distribution that drives the compliance gradient described in subchapter 7.3. The model learned to extract patterns from English examples during instruction tuning, and it performs this extraction most reliably when the examples are in English.

When you provide three English examples of the desired output format -- say, a structured response with a summary paragraph, three action items, and a closing sentence -- the model's ability to reproduce that structure in its output is highest when the examples are in English. This holds even when the output is in a different language. The model can learn the structural pattern from English examples and apply it to Japanese output, French output, or Arabic output. The cross-lingual pattern transfer is imperfect, as the previous subchapter documented, but it is stronger when the pattern source is English.

The practical implication is that if your primary goal is format compliance -- the model must produce a numbered list, or include specific sections, or follow a particular structure -- English examples will give you the highest compliance rate across languages. This is especially true for complex formats that require the model to hold multiple constraints simultaneously.

## The Target-Language Example Advantage

Target-language examples are more effective at teaching the language pattern. When the model sees a Japanese few-shot example, it does not just learn the task structure. It learns the natural phrasing, the appropriate formality level, the sentence-ending conventions, and the cultural context of a response in Japanese. These are things an English example cannot teach, no matter how well it demonstrates the task structure.

The output quality difference is audible to native speakers. A system that uses English examples to generate Japanese output produces Japanese that sounds translated -- grammatically correct, structurally appropriate, but subtly foreign in its word choices and rhythm. A system that uses Japanese examples produces Japanese that sounds native -- the kind of response a Japanese professional would actually write. For products where user experience and perceived naturalness matter, this difference is not cosmetic. It is the difference between a product that users tolerate and one they trust.

Target-language examples also teach cultural conventions that no English example can convey. Japanese business communication uses specific keigo (honorific language) patterns that vary by the relationship between speaker and listener. Korean has distinct speech levels that affect every verb ending. German business prose has conventions around compound noun formation and subordinate clause depth that English examples do not demonstrate. Arabic prose flows in ways that reflect rhetorical traditions fundamentally different from English expository writing. When your examples are in the target language, the model picks up these conventions through pattern matching, producing output that respects the cultural context.

## The Mixed-Language Strategy

Research published in 2025 on multilingual prompt engineering across NLP tasks found that mixed-language examples -- combining English and target-language examples in the same prompt -- often outperform either pure strategy. The finding makes intuitive sense once you understand what each language contributes.

English examples anchor the task structure. They tell the model, with maximum clarity, what the output format should look like. Target-language examples anchor the language quality. They tell the model how the output should sound in the specific language. When both are present, the model receives strong signals on both dimensions simultaneously.

The effective pattern is to lead with one or two English examples that demonstrate the task structure clearly, then follow with one or two target-language examples that demonstrate the same structure realized naturally in that language. The English examples establish the "what." The target-language examples establish the "how." Together, they produce output that is both structurally compliant and linguistically natural.

Teams that have tested this approach report compliance rates comparable to English-only examples (within 2 to 4 percentage points) combined with naturalness ratings comparable to target-language-only examples. The mixed strategy is not a compromise between two inferior options. It is a synthesis that captures the best of both.

## Token Budget Reality: The Fertility Tax

Every few-shot example costs tokens. In English, where tokenization is efficient, a single example might cost 80 to 120 tokens. The same example in a different language costs more -- sometimes dramatically more -- because of **token fertility**, the number of tokens required to represent a given amount of meaning.

Token fertility varies by language and tokenizer. For the tokenizers used by GPT-5 and Claude Opus 4.6 in 2026, the fertility ratios relative to English are roughly: French and Spanish at 1.1 to 1.3 times English, German at 1.2 to 1.4 times, Russian and Arabic at 1.5 to 2.5 times, Japanese and Chinese at 1.3 to 1.8 times (though this has improved significantly with recent tokenizer updates), Korean at 1.4 to 2.0 times, Thai at 2.0 to 3.0 times, and low-resource African and Southeast Asian languages at 2.5 to 5.0 times. These ratios mean that a few-shot example which costs 100 tokens in English could cost 250 to 500 tokens in Thai or Amharic.

The fertility tax has direct implications for how many examples you can afford. If your prompt budget allows 500 tokens for few-shot examples, you can fit four to five English examples or two to three Thai examples. Since example quality matters more than quantity (a point the next section addresses), the fertility tax forces a design decision: fewer, higher-quality examples in the target language, or more examples in English.

For most production systems, the optimal allocation is two to three examples total in a mixed-language configuration: one English example for structural clarity and one or two target-language examples for linguistic naturalness. This keeps the token cost manageable across languages while delivering both structural compliance and natural output.

## Quality Over Quantity

Two excellent examples beat five mediocre ones. This is true in English, and it is even more true in multilingual systems where every additional example consumes a larger share of the token budget.

An excellent few-shot example has four properties. First, it demonstrates the exact task the model will perform, not a related task. If your system generates customer support responses about billing inquiries, your example should be a billing support response, not a general customer support response. Second, it demonstrates the complete output format -- every section, every constraint, every formatting rule -- so the model can see the full pattern in a single example. Third, it is written by a native speaker (not machine-translated from English) and reflects the natural conventions of that language. Fourth, it is representative of the typical query complexity your system handles, not a trivially simple case that fails to demonstrate how the model should handle real-world nuance.

A mediocre example -- one that demonstrates a slightly different task, shows only partial formatting, was machine-translated, or represents an unrealistically simple case -- teaches the model imprecise patterns. The model may pick up the general idea but miss the specific constraints, producing output that is close but not quite right. Five mediocre examples create five slightly different imprecise signals, which the model averages into output that does not match any of them precisely.

The practical guidance is to invest the time to create high-quality examples for your top languages. Commission native speakers to write examples that demonstrate the exact output you want, in the natural style of their language. Treat these examples as product assets, not throwaway prompt components. They are the most direct lever you have for controlling output quality in non-English languages.

## Dynamic Example Selection

Static examples -- the same few-shot examples used for every query regardless of the user's language -- are simple to implement but leave performance on the table. Dynamic example selection, where the system chooses examples based on the detected user language, produces consistently better results.

The architecture is straightforward. Maintain an example library organized by language. Each entry in the library contains one or more high-quality examples for a specific language, written by native speakers, demonstrating the target output format. When a user query arrives, the language detection layer identifies the user's language. The prompt assembly layer selects the appropriate examples from the library and inserts them into the prompt.

The example library does not need examples for every language you support. Start with your top five to ten languages by user volume. For languages without dedicated examples, fall back to English examples or, better, to examples in a linguistically related language (see the next section on cross-lingual few-shot).

The maintenance cost is real but manageable. Each time you update your prompt's output format, you need to update the examples for each language in the library. This scales linearly with the number of languages you maintain examples for. Most teams find that maintaining examples for five to eight languages covers 80 to 90 percent of their user base, and English fallback handles the rest.

The performance gain is measurable. Teams that switch from static English examples to dynamic per-language examples typically see a 8 to 15 percentage point improvement in format compliance and a significant improvement in native-speaker naturalness ratings for non-English output. The improvement is largest for languages that are most distant from English, where English examples provide the weakest signal for language-specific conventions.

## Cross-Lingual Few-Shot: The Related Language Fallback

You will not have native-speaker examples for every language you support. When target-language examples are unavailable, the next best option is examples in a linguistically related language. This is **Cross-Lingual Few-Shot** -- using examples from a donor language to improve output in a recipient language.

The effectiveness of cross-lingual few-shot depends on the degree of linguistic similarity between the donor and recipient languages. The closer the languages are, the more the examples help. Spanish examples improve Portuguese output substantially, because the two languages share vocabulary, grammatical structures, and cultural conventions for business communication. Hindi examples improve Urdu output because the spoken languages are mutually intelligible despite their different scripts. Malay examples improve Indonesian output because the languages are closely related variants.

The similarity threshold below which cross-lingual few-shot stops helping is roughly at the language-family level. Romance language examples help other Romance languages. Slavic language examples help other Slavic languages. But Japanese examples do not meaningfully improve Korean output (despite geographic proximity, the languages are not linguistically related), and Arabic examples do not help Turkish output (despite shared cultural context in some regions, the languages belong to completely different families).

When selecting a donor language, prioritize linguistic similarity over cultural similarity. Ukrainian and Polish are linguistically close enough that Polish examples meaningfully improve Ukrainian output quality. Turkish and Arabic share cultural context in many domains but are linguistically distant enough that Arabic examples offer little benefit for Turkish output. The model's ability to transfer patterns between languages is driven by structural similarity -- shared grammar, shared vocabulary roots, similar word order -- not by geographic or cultural proximity.

For languages with no closely related donor available, English examples remain the best fallback. The model's cross-lingual transfer from English to any language is typically stronger than its transfer from an unrelated non-English language, because English has the deepest representation in the model's training data and the strongest instruction-following pathways.

## Example Placement: Where in the Prompt

The position of few-shot examples within the prompt affects their influence on the output. This effect exists in monolingual systems but is amplified in multilingual ones.

Examples placed near the end of the prompt -- closer to the user's message and the generation boundary -- have more influence on the output's style and language characteristics. The model's attention mechanism weights recent context more heavily during generation, so examples at the end of the prompt serve as stronger style anchors than examples at the beginning.

For multilingual systems, this positional effect has a specific implication: target-language examples should be placed after English examples, not before. If you use a mixed-language strategy with one English structural example and one Japanese style example, the Japanese example should come second, closer to the generation point. This arrangement lets the English example establish the structural pattern early in the context, then lets the Japanese example reset the model's language and style orientation immediately before generation.

The effect size is moderate -- placement alone typically shifts naturalness ratings by 5 to 10 percent -- but it is consistent and costs nothing to implement. You are rearranging the same examples, not adding new ones. There is no token cost, no maintenance overhead, and no reason not to capture the gain.

One caution: if your examples are placed too close to the user's message, the model may treat them as part of the conversation rather than as formatting demonstrations. Leave a clear separator between the examples section and the user's message. A brief instruction line like "Now respond to the user's message following the format shown above" serves as a boundary marker that prevents the model from confusing example text with conversational context.

## Maintaining an Example Library at Scale

As your system grows to support more languages, the example library becomes a product asset that requires its own quality process.

Each example should be versioned alongside the prompt. When the prompt changes -- a new constraint is added, the output format is modified, the tone is adjusted -- every example in the library must be updated to reflect the change. Stale examples that demonstrate an outdated format are worse than no examples, because they actively teach the model a pattern you no longer want.

Quality control for examples requires native-speaker review. Machine-translated examples are detectable by native speakers and produce output that inherits the translation's awkwardness. Every example in the library should be written or reviewed by a native speaker who understands both the target language and the intended output quality. This is an investment, but it is a one-time investment per language per prompt version, and the return in output quality is substantial.

Store examples in a structured format that your prompt assembly pipeline can query by language code. Tag each example with the prompt version it corresponds to, the date it was last reviewed, and the native speaker who authored or approved it. This metadata makes it possible to identify stale examples when the prompt changes and to track which languages have up-to-date examples and which are using fallback English or cross-lingual examples.

The library grows over time. As you optimize for specific use cases -- billing inquiries versus technical support versus onboarding -- you may need domain-specific examples per language. The library structure should accommodate this growth without requiring architectural changes. A flat key-value store indexed by language code and use-case tag is sufficient for most teams. The complexity is not in the storage. It is in the human process of creating, reviewing, and updating high-quality examples across languages.

## The Example Selection Decision Tree

When assembling a prompt for a specific user language, the selection logic follows a clear priority order.

First, check if native-language examples exist for this language and use case. If they do, use them. Place them after any English structural examples, near the end of the prompt.

Second, if native examples are not available, check if examples exist in a closely related language. If they do, use them as cross-lingual few-shot examples. The model will transfer patterns imperfectly but substantially better than from English alone.

Third, if neither native nor related-language examples exist, use English examples. English is the universal fallback because the model's cross-lingual transfer from English is the strongest baseline for any target language.

Fourth, regardless of which language the examples are in, always verify that the examples demonstrate the current output format. An outdated example in the target language is less useful than a current example in English.

This decision tree can be implemented as a runtime lookup in your prompt assembly pipeline. The cost is a language-to-example mapping maintained alongside your example library. The benefit is that every user, in every language, gets the best available examples, with graceful fallback when the ideal examples do not exist.

The next subchapter addresses a related challenge that few-shot examples alone cannot fully solve: controlling output format across writing systems, where structural conventions for lists, headers, emphasis, and paragraph organization vary by language and script.