# 11.1 — When Multilingual Fine-Tuning Is Necessary vs When Prompting Suffices

The cheapest multilingual improvement is the one you never fine-tune for. Most teams reach for fine-tuning the moment they see poor output in a non-English language, treating it as the obvious fix for any quality gap. It is not. Fine-tuning is the most expensive, most maintenance-heavy, and most risk-laden tool in your multilingual toolkit. The right starting point is always prompting, and the right question is always: have I exhausted what prompting can do before I commit to changing the model itself?

This is not a philosophical preference. It is an engineering discipline. Prompting costs hours and iterates in minutes. Fine-tuning costs weeks, requires data pipelines, introduces forgetting risk, and creates a model you must maintain, evaluate, and redeploy every time the base model updates. When prompting closes the gap, you win on every dimension: speed, cost, flexibility, and future-proofing. When prompting does not close the gap, you need to know exactly why, because that diagnosis determines whether fine-tuning will actually help.

## The Prompting Toolkit for Multilingual Quality

Before you consider fine-tuning for any language, you should have tried and measured the impact of at least four prompting strategies.

The first is explicit language and register instruction. Your system prompt should specify not just the target language but the register, formality level, and regional variant. "Respond in Brazilian Portuguese using a formal but approachable tone" produces measurably different output from "Respond in Portuguese." For many languages, the gap between poor output and acceptable output is nothing more than an underspecified system prompt. A travel company serving Latin America discovered that adding regional variant instructions to their prompts — specifying Mexican Spanish versus Argentine Spanish versus Colombian Spanish — improved user satisfaction scores by 22 percent across those markets without touching the model.

The second is few-shot examples in the target language. When you include three to five examples of the desired output in the target language within the prompt, you give the model a concrete template for vocabulary, sentence structure, and domain terminology. This is especially powerful for languages where the model has adequate but not fluent capability. The examples act as an anchor, pulling the model's generation toward the quality level demonstrated in the shots. For a legal technology startup serving Japanese enterprise clients, adding five examples of properly formatted Japanese legal summaries to their prompt eliminated 80 percent of the terminology errors that had initially seemed to require fine-tuning.

The third is retrieval of language-specific context. If your system uses retrieval-augmented generation, ensuring that your retrieval pipeline returns documents in the user's language gives the model language-appropriate vocabulary and phrasing to draw from. Many multilingual quality issues trace not to the model's generation capability but to the retrieval system feeding it English-only context and expecting non-English output. We covered this retrieval challenge in depth in Section 7, and the fix is often architectural rather than model-level.

The fourth is chain-of-thought prompting in the target language. Research from 2024-2025 consistently shows that when you instruct the model to reason in the target language rather than defaulting to English-language reasoning with a final translation step, output quality improves for medium and high-resource languages. The model that reasons in Korean before generating Korean produces more natural, idiomatic output than the model that reasons in English and translates its conclusion.

## The Prompt Ceiling

Here is the concept that should govern your fine-tuning decision: **the prompt ceiling**. For any given language, task, and model combination, there is a quality level beyond which no amount of prompt engineering will improve performance. You add more examples, you refine instructions, you optimize retrieval — and the numbers stop moving. The model has given you everything it can give through the prompt interface. What remains is a gap between what the model knows and what your task requires, and that gap lives in the weights.

The prompt ceiling is not theoretical. You can measure it. Run your multilingual eval suite against progressively more sophisticated prompting strategies: zero-shot, then few-shot with three examples, then few-shot with five examples, then few-shot with ten examples plus detailed system instructions, then the same with language-specific retrieval context. Plot the quality curve. When the curve flattens — when adding the sixth example produces less than a one-point improvement on your quality metric — you have found the ceiling.

The height of the ceiling varies enormously by language. For Spanish, French, German, and other high-resource languages served by models like GPT-5, Claude Opus 4.6, and Gemini 3 Pro, the prompt ceiling is high. These models have seen billions of tokens in these languages during pretraining, and prompting can usually close the remaining gap for most tasks. For Thai, Vietnamese, or Swahili, the ceiling is lower. The model has less linguistic knowledge to draw from, and prompting can only extract what the weights already contain. For truly low-resource languages — Yoruba, Khmer, many indigenous languages — the ceiling can be so low that even optimized prompting produces output that native speakers find unacceptable.

The mistake teams make is not measuring the ceiling before committing to fine-tuning. They see poor Japanese output, assume fine-tuning is the answer, and spend eight weeks building a training pipeline. Had they spent two days on systematic prompt optimization and measurement, they might have discovered that few-shot examples plus retrieval lifted Japanese quality from 62 to 84 on their eval metric — good enough for production — and that the remaining gap to 90 would require fine-tuning on 15,000 curated examples. That cost-benefit analysis looks very different from "Japanese is bad, let's fine-tune."

## The Four Conditions That Demand Fine-Tuning

Fine-tuning becomes the right tool when one or more of these conditions holds, and you have confirmed that prompting cannot address it.

First, the base model consistently fails a specific language despite prompt optimization. You have tried every prompting strategy, you have measured the ceiling, and the model simply cannot produce acceptable output in that language. This is common for Tier 3 and Tier 4 languages where pretraining data was sparse. No prompt can teach the model vocabulary it never learned. If your eval shows that optimized prompting in Burmese still produces output that native speakers rate below your quality threshold, the gap is in the weights, and fine-tuning is how you close it.

Second, you need domain terminology that does not exist in the model's training data. A pharmaceutical company operating in South Korea needs the model to use precise Korean medical terminology — not approximate translations, not English loanwords, but the specific terms that Korean clinicians expect. The model might handle conversational Korean at 85 percent quality but drop to 55 percent on specialized medical Korean because those terms appeared rarely or never in pretraining. Few-shot examples help, but if the terminology set is large and the expected usage patterns are complex, fine-tuning on a curated corpus of Korean medical text is the more reliable path.

Third, latency or cost requires a smaller model that cannot handle the language via prompting. A frontier model like GPT-5 or Claude Opus 4.6 might produce excellent Vietnamese output with careful prompting, but if your latency budget is 200 milliseconds and your cost target is less than 0.001 dollars per request, you need a smaller model. Smaller models have worse multilingual capabilities, and their prompt ceilings are lower. Fine-tuning a GPT-5-nano or Llama 4 Scout on Vietnamese task-specific data can lift the small model's Vietnamese performance to a level that prompting alone could never reach, at a fraction of the inference cost.

Fourth, style and register requirements are too nuanced for prompt-level control. A financial services company needs formal Bahasa Indonesia for regulatory communications and informal Bahasa Indonesia for customer chat. The difference is not just vocabulary — it is sentence structure, honorifics, discourse markers, and cultural tone. Prompting can approximate this distinction, but fine-tuning on examples of each register teaches the model the difference at a level that prompting cannot reliably sustain across thousands of production requests.

## The Decision Framework in Practice

The **Prompt-First Decision Framework** works in four steps.

Step one: baseline measurement. Run your eval suite on the target language using your current production prompt. Record the quality score. This is your starting point.

Step two: prompt optimization. Apply the four prompting strategies described above, one at a time, measuring after each. Track which strategies move the needle and by how much. Continue until you hit the prompt ceiling — the point where additional prompt sophistication produces less than a one-point improvement.

Step three: gap analysis. Compare the prompt ceiling to your production quality threshold. If the ceiling meets or exceeds your threshold, you are done. Ship the optimized prompt. If the ceiling falls short, calculate the gap. A gap of five points might be acceptable with other compensating controls. A gap of twenty points is a clear signal for fine-tuning.

Step four: fine-tuning cost-benefit. If fine-tuning is warranted, estimate the cost. You need training data — at minimum a few thousand high-quality examples in the target language and domain. You need compute for training. You need an expanded eval suite that covers not just the target language but all other languages the model supports, because fine-tuning risks degrading them. You need a plan for ongoing maintenance, because every time the base model updates, your fine-tune may need to be re-applied or re-evaluated. Compare this total cost against the business value of closing the quality gap.

This framework is not a one-time exercise. You revisit it every time the base model updates, because a new model version might raise the prompt ceiling high enough to eliminate the fine-tuning need entirely. Teams that fine-tuned for Thai in early 2025 found that GPT-5's improved multilingual pretraining raised the prompt ceiling for Thai by fifteen points, making their fine-tuning investment unnecessary for the new model.

## The Cost Arithmetic

The numbers tell the story clearly. Prompt engineering for a new language typically requires one to three days of an engineer's time, plus eval runs. Call it 5,000 to 15,000 dollars in loaded cost. The result is immediately portable — when the base model updates, the prompt usually works as well or better with the new model.

Fine-tuning for a new language requires data collection (two to eight weeks depending on domain and language), data cleaning and validation (one to two weeks), training experimentation (one to two weeks), comprehensive multilingual evaluation (one week), and deployment with monitoring (ongoing). The upfront cost ranges from 40,000 to 200,000 dollars depending on data sourcing complexity, and the ongoing maintenance cost — re-evaluating and potentially retraining when base models update — adds 10,000 to 30,000 dollars per model generation.

The gap between these costs means that fine-tuning for a language where prompting could have sufficed is not just wasteful — it is actively harmful. You have traded a flexible, low-maintenance solution for a rigid, high-maintenance one, and you have introduced forgetting risk for every other language your model supports.

## The Risk of Unnecessary Fine-Tuning

Fine-tuning does not simply add capability. It trades generality for specificity. When you fine-tune a model on Japanese legal text, you are updating weights that also encode the model's knowledge of Korean, Chinese, and every other language. The update might improve Japanese legal performance by thirty points while degrading Korean conversational ability by eight points. If you did not need fine-tuning — if prompting could have closed the Japanese gap — you took on this risk for nothing.

There is also the maintenance trap. Once you have a fine-tuned model in production, you own it. Every base model update requires you to decide: do you re-fine-tune on the new base? Do you keep the old fine-tuned model while other languages move to the new base? Do you run parallel models? Each decision has cost and complexity implications. The team that fine-tuned when prompting would have sufficed now maintains a bespoke model for one language while the rest of their system runs on the foundation model. That divergence compounds over time. By the third model generation, they are managing a zoo of specialized models with different capabilities, different eval suites, and different deployment pipelines.

The principle is straightforward: fine-tune only when the prompt ceiling is measurably below your quality threshold, the business value of closing the gap justifies the cost, and you have budgeted for the ongoing maintenance that fine-tuning demands. Every other case is a prompting problem dressed up as a fine-tuning problem.

## When the Answer Changes Over Time

The prompt-versus-fine-tune boundary is not static. It shifts every time a new model generation launches, every time your product requirements change, and every time your user base evolves.

A language that required fine-tuning in 2024 might be well-served by prompting in 2026, because the new base model was trained on more data in that language. Conversely, a language that was fine through prompting might need fine-tuning as your product adds more specialized domains or higher quality requirements. A healthcare platform that was satisfied with general Thai output in 2024 might need fine-tuned Thai medical terminology in 2026 as they expand their Thai clinical features.

The teams that succeed at multilingual AI revisit this decision quarterly. They run their eval suite against the current base model with optimized prompts, compare the results to their production quality thresholds, and only trigger a fine-tuning project when the gap is real, measured, and worth closing. Everyone else either over-invests in fine-tuning they don't need or under-invests in prompting they haven't tried.

The next subchapter examines what happens when you do fine-tune — specifically, how training on one language creates ripple effects across every other language your model supports, through the phenomenon of cross-lingual transfer.
