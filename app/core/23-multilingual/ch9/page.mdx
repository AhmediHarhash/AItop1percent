# Chapter 9 — Cultural Safety, Bias, and Harm Across Languages

Safety filters trained on English data miss culture-specific harms entirely. What counts as offensive, dangerous, or inappropriate varies dramatically across cultures, religions, and regions — and the classifiers most teams deploy were built by English-speaking teams testing against English-speaking norms. Cultural safety is a design constraint, not a feature to add after launch. Teams that rely on English-trained classifiers deploy systems that are simultaneously over-censoring in some cultures and dangerously permissive in others, creating legal exposure and user harm in every market they enter.

---

- **9.1** — The English Safety Blind Spot: Harms That Filters Miss
- **9.2** — Culture-Specific Harm Categories: What Is Offensive Where
- **9.3** — Bias in Multilingual Models: How Uneven Training Data Creates Uneven Outcomes
- **9.4** — Stereotype Amplification Across Languages and Cultures
- **9.5** — Religious, Political, and Historical Sensitivity by Region
- **9.6** — Gender and Pronoun Handling Across Grammatically Gendered Languages
- **9.7** — Building Culture-Aware Safety Classifiers
- **9.8** — Red-Teaming Across Languages: Finding the Failures English Testing Misses
- **9.9** — Content Moderation at Multilingual Scale
- **9.10** — The Cultural Review Board: Governance for Multilingual Safety

---

*Your safety system is only as good as its weakest language — and right now, you probably do not know which language that is.*
