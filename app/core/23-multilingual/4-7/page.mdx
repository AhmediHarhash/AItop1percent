# 4.7 â€” Dimension-Specific Multilingual Metrics: Fluency, Accuracy, Cultural Fit

Measuring multilingual quality as a single number hides the most important information. A response can be fluent but inaccurate, accurate but culturally inappropriate, or culturally perfect but in the wrong register. Collapsing these dimensions into a single "quality score" creates a number that looks actionable and is not. Your Korean quality score is 74 percent. What does that mean? Is the model producing grammatically awkward Korean? Is it producing beautiful Korean that contains factual errors? Is it producing fluent, accurate Korean that uses an inappropriately casual tone for the context? The single number cannot tell you, and without knowing which dimension is failing, your team cannot fix the problem.

This is not an abstract concern. A fintech company serving Southeast Asian markets spent four months trying to improve their Thai quality score from 71 to 80 percent. They rewrote prompts, adjusted system instructions, and fine-tuned on Thai data. The score barely moved. When they finally decomposed the score into dimensions, they discovered that Thai fluency was already at 88 percent and accuracy was at 83 percent. The dimension dragging the score down was cultural fit -- the model was giving financial advice using examples and assumptions appropriate for American users, not Thai users. Four months of prompt engineering aimed at general quality had been treating the wrong symptom. One week of targeted work on culturally appropriate financial examples moved the cultural fit dimension from 52 percent to 74 percent, and the overall score jumped to 81.

Dimension-specific measurement is not overhead. It is the difference between knowing what is wrong and guessing.

## Fluency: Does the Output Sound Native

**Fluency** measures whether the output reads naturally in the target language. Not whether it is grammatically correct -- grammar is a subset of fluency. Fluency encompasses word choice, sentence structure, idiom use, rhythm, and the overall impression of naturalness that a native speaker perceives in the first few seconds of reading.

The distinction between grammar and fluency matters because a response can be grammatically perfect and still sound unnatural. Machine-generated text in non-English languages frequently exhibits a pattern called translationese -- sentences that follow the syntactic patterns of English rather than the target language. The grammar is technically correct in the target language, but the sentence order, the clause structure, the word choices feel imported. A Japanese native speaker reading translationese can tell immediately that the text was not conceived in Japanese, even if every grammatical rule is followed. A Korean reader detects that the sentence places emphasis in the wrong position. An Arabic reader notices that the connective structure follows English logical patterns rather than Arabic rhetorical conventions.

Measuring fluency requires native-speaker judgment. As of 2026, no automated metric reliably captures the difference between "grammatically correct" and "sounds native" across languages. Perplexity-based metrics can flag obviously disfluent text, but they cannot distinguish between awkward-but-natural and fluent-but-imported. LLM judges can approximate fluency assessment in high-resource languages -- French, Spanish, German, Japanese, Mandarin -- where the judge model has enough training data to have developed internal fluency norms. For mid-resource and low-resource languages, LLM judges' fluency assessments correlate poorly with native-speaker judgments, as discussed in subchapter 4.5.

The standard measurement approach is a five-point native-speaker rating scale. Level one represents incomprehensible output -- garbled text that a native speaker cannot parse. Level two represents disfluent output that is understandable but requires effort to read, with frequent unnatural constructions. Level three represents adequate fluency -- understandable and mostly natural, with occasional awkward phrasing. Level four represents good fluency -- reads naturally with only minor imperfections that most readers would not notice. Level five represents native-quality fluency -- indistinguishable from text written by a skilled native speaker.

Anchor these levels with language-specific examples. A level-three response in Korean looks different from a level-three response in Arabic, because the markers of "adequate but imperfect" fluency differ between languages. Korean level three might exhibit occasional subject-marker errors or unnatural topic-comment ordering. Arabic level three might show correct grammar but inappropriate use of Modern Standard Arabic when a dialectal register would be more natural. Provide three to five example responses at each level for each language so evaluators have concrete reference points.

## Accuracy: Is the Output Correct

**Accuracy** measures whether the output is factually correct and responsive to the question asked. In theory, accuracy is language-independent -- a fact is either true or false regardless of what language it is expressed in. In practice, accuracy has a cultural dimension that makes it more complex than it first appears.

The straightforward cases are genuinely language-independent. If a user asks "what is the boiling point of water at sea level?" the correct answer is 100 degrees Celsius in every language. If a user asks for the square root of 144, the answer is 12 in every language. Mathematical, physical, and logical facts do not change with language.

But many real-world accuracy questions are culturally and jurisdictionally dependent. A user asking about tax filing deadlines needs the answer for their country, not for the United States. A user asking about recommended daily calorie intake may expect an answer based on their country's nutritional guidelines, which differ from American guidelines. A user asking about the legal drinking age needs the answer for their jurisdiction. A user asking about a medical dosage needs the answer appropriate for the drug formulations available in their market, which may differ from US formulations.

This jurisdictional dimension means accuracy evaluation must include a cultural-correctness component for any domain where "correct" varies by geography. Your accuracy rubric for a healthcare product should specify: is the dosage information correct for the user's market? Your accuracy rubric for a legal product should specify: does the response cite the relevant jurisdiction's laws? Your accuracy rubric for a financial product should specify: are the tax rates, contribution limits, and regulatory requirements correct for the user's country?

Measure accuracy on a binary or three-point scale per claim. Binary works for domains where claims are clearly right or wrong: correct or incorrect. A three-point scale adds a "partially correct" category for domains where responses contain multiple claims of varying accuracy -- the response got the main fact right but included an incorrect supporting detail, or the response is correct for one jurisdiction but was given to a user in a different jurisdiction.

Calculate per-response accuracy as the percentage of claims that are correct. Calculate per-language accuracy as the average across a representative sample of responses. Track this separately from fluency. A language where fluency is 4.2 out of 5 and accuracy is 62 percent tells a very different story from a language where fluency is 3.1 and accuracy is 91 percent. The first language needs fact-checking and knowledge improvements. The second language needs generation-quality improvements. The intervention is completely different, and a single quality number would obscure this distinction.

## Cultural Fit: Does the Output Respect the User's World

**Cultural fit** is the dimension that most evaluation suites skip, and it is the dimension that most directly determines whether users in a given market trust and return to your product. Cultural fit measures whether the output respects cultural norms, uses appropriate examples and references, avoids culturally sensitive topics correctly, and demonstrates an understanding of the user's cultural context.

Cultural fit failures are invisible to evaluation systems designed by monocultural teams. An American team reviewing Arabic output may not notice that the response references alcohol in a context where alcohol references are culturally inappropriate. A European team reviewing Japanese output may not notice that the response uses a level of directness that would be perceived as rude. A Chinese team reviewing Indian output may not notice that a dietary recommendation includes ingredients that violate common dietary restrictions in parts of India.

The cultural fit dimension breaks into several sub-dimensions.

**Appropriateness of examples and references.** Does the output use examples that are relevant to the user's cultural context? A response about savings strategies that uses the American 401(k) system as an example is culturally misfit for a user in Germany, where the retirement system works entirely differently. A response about cooking that assumes access to a full oven is culturally misfit for users in cultures where stovetop cooking is the norm. Evaluators should flag every instance where an example, analogy, or reference assumes a cultural context that does not match the user's likely context.

**Formality and register.** Many languages encode social relationships through linguistic register in ways that English does not. Korean has seven distinct speech levels, and using the wrong one signals disrespect, over-familiarity, or social ignorance. Japanese honorifics carry meaning about the speaker's relationship to the listener and the subject. German distinguishes between formal "Sie" and informal "du" in ways that affect how users perceive a product's tone. Thai uses pronouns and particles that encode age, social status, and intimacy. Evaluating register correctness requires native speakers who understand not just the linguistic rules but the social contexts in which each register is appropriate. Your customer service bot using the casual Korean speech level with a user who expects formal address is not a fluency error. It is a cultural fit failure that will cost you the customer's trust.

**Avoidance of sensitive topics.** What is sensitive varies by culture. Discussions of religion, politics, alcohol, sexuality, dietary practices, historical events, and social hierarchies carry different levels of sensitivity across cultures. A response that casually references pork in a context where the user is likely Muslim, or that discusses astrology dismissively in a culture where astrology is taken seriously, or that references a contested historical event using one country's framing -- these are cultural fit failures that automated evaluation cannot detect. Native-speaker evaluators from the target culture are the only reliable detection mechanism.

**Tone and emotional register.** Beyond linguistic formality, cultures differ in how professional communication should feel. American professional tone is direct, optimistic, and action-oriented. Japanese professional tone is indirect, measured, and relationship-oriented. German professional tone is precise, thorough, and hierarchically respectful. If your model produces American-toned output in Japanese -- enthusiastic, direct, casually encouraging -- it will read as unprofessional to Japanese users even if the language is fluent and the content is accurate.

Measure cultural fit on a three-point or five-point scale. Three points works for most use cases: culturally appropriate (no issues detected), minor cultural issues (small mismatches that most users would tolerate), culturally inappropriate (clear violations of cultural norms that would alienate users or cause offense). Five points provides more granularity for products where cultural fit is a primary differentiator.

## Completeness: Does the Output Answer the Full Question

**Completeness** measures whether the output addresses all parts of the user's question. This dimension shows consistent cross-lingual variation that most teams do not track. Non-English responses tend to be shorter and less detailed than English responses to the same question. A model that provides a thorough, multi-paragraph answer in English may give a two-sentence answer to the same question in Thai. Both answers may be fluent and accurate, but the Thai answer is incomplete.

The completeness gap has two causes. First, the model's generation behavior is influenced by its training data, and training data in non-English languages tends toward shorter texts. The model has learned that responses in Thai are typically shorter than responses in English, so it generates shorter responses. Second, the model's knowledge depth varies by language -- it has more material to draw on for English responses, which naturally produces more detailed output.

Measure completeness by comparing the information content of non-English responses against the information content of equivalent English responses. If the English response to a question includes five key points and the Korean response includes three of the same five points, the Korean completeness is 60 percent relative to English. This relative measure is more useful than an absolute completeness score because it directly quantifies the information gap your non-English users experience.

Track completeness separately from accuracy. A response can be 100 percent accurate but only 40 percent complete -- every fact it includes is correct, but it is missing the majority of the information the user needs. The intervention for low completeness is different from the intervention for low accuracy: completeness problems often respond to prompt engineering that explicitly requests detailed answers, while accuracy problems require knowledge improvements.

## Format Compliance: Does the Output Follow Structure Requirements

**Format compliance** measures whether the output follows the requested format -- lists when asked for lists, numbered steps when asked for numbered steps, specific field structures when the output feeds downstream systems. This dimension breaks more frequently in non-English languages than any other quality dimension, and it breaks silently.

Structured output -- JSON-like formats, tables, numbered lists, specific field labels -- relies on the model's ability to generate text that follows a syntactic template. This ability is strongly correlated with the volume of structured examples in the model's training data for each language. English has by far the most structured text examples in training corpora. The model has seen millions of English numbered lists, English tables, English structured templates. It has seen far fewer in Thai, Arabic, or Hindi.

The result is that format compliance degrades along a predictable gradient. English format compliance might be 97 percent. French and German might be 94 percent. Japanese and Korean might be 88 percent. Arabic and Hindi might be 80 percent. Thai and Vietnamese might be 72 percent. For products that depend on structured output -- anything that feeds into a downstream parser, a form-fill system, or a template rendering engine -- this degradation can break the product entirely in some languages while working fine in others.

Measure format compliance as a binary pass or fail per response: did the output match the required format or not? Calculate the pass rate per language. Track format-specific failure types: did the model skip the numbering, use the wrong list delimiters, mix up field labels, or generate unstructured prose when structured output was requested? Each failure type points to a different intervention -- some respond to prompt engineering, some require few-shot examples in the target language, and some require fine-tuning on structured output examples.

## Consistency: Same Question, Same Answer, Any Language

**Cross-lingual consistency** measures whether the model provides the same substantive answer to the same question asked in different languages. This is a distinct quality dimension from any of the above, and it is one of the most important for products serving multilingual user bases.

Research from 2025 documented that LLMs frequently give different answers to the same factual question depending on the language of the query. A user asking "what are the side effects of metformin?" in English might get a comprehensive list of five side effects. The same question in Hindi might produce a list of three side effects that partially overlaps with the English list. The same question in Thai might produce two side effects and a general disclaimer. The user's experience of the product depends on which language they happen to use, and the disparity is invisible unless you measure it explicitly.

Cross-lingual consistency matters most in domains where users should receive equivalent information regardless of language: healthcare, legal, financial, safety-critical applications. A patient asking about drug interactions should not receive less complete safety information because they asked in Bengali instead of English. A user asking about their rights under a return policy should not receive different terms because they asked in Vietnamese.

Measure consistency by maintaining a set of canonical questions with known correct answers. Periodically run these questions through the model in every supported language. Compare the substantive content of the responses -- not the exact wording, which will differ by language, but the factual claims, the completeness of the information, and the correctness of the answer. Calculate a consistency score per language pair: the percentage of canonical questions where language A and language B produce substantively equivalent answers.

Track consistency trends over time. Model updates, prompt changes, and knowledge base updates can all affect cross-lingual consistency in unpredictable ways. An update that improves accuracy in English might simultaneously degrade consistency if the non-English responses do not receive the same improvement. Your monitoring should detect these divergences before they reach users.

## Weighting Dimensions by Use Case

Not every dimension carries equal weight for every product. The weighting should reflect what your users care about most.

**Customer support chatbot.** Weight cultural fit and fluency highest. Users in customer support contexts are emotionally engaged -- they are frustrated, confused, or seeking help. A culturally inappropriate response or an awkwardly phrased one damages trust immediately. Accuracy is important but secondary to tone and naturalness for the most common support queries. Weight suggestion: fluency 25 percent, cultural fit 30 percent, accuracy 20 percent, completeness 15 percent, format compliance 5 percent, consistency 5 percent.

**Medical information system.** Weight accuracy and consistency highest. The stakes of inaccurate medical information are severe. Cross-lingual consistency is critical because patients in different languages must receive equivalent safety information. Fluency matters but is secondary -- an awkward response that is medically correct is vastly preferable to a fluent response that is medically wrong. Weight suggestion: accuracy 35 percent, consistency 25 percent, completeness 20 percent, fluency 10 percent, cultural fit 5 percent, format compliance 5 percent.

**E-commerce product descriptions.** Weight fluency and cultural fit highest. Product descriptions must sound natural and appealing to buyers in each market. Cultural fit includes appropriate marketing tone, relevant product comparisons, and locally meaningful value propositions. Accuracy of product specifications matters but is typically handled by structured data rather than free-text generation. Weight suggestion: fluency 30 percent, cultural fit 30 percent, accuracy 15 percent, completeness 15 percent, format compliance 10 percent, consistency 0 percent.

**Legal document analysis.** Weight accuracy and completeness highest. Missing a relevant legal point or mischaracterizing a regulation has real consequences. Fluency is important for readability but the analysis must be correct above all else. Consistency across languages is critical for multinational clients who need equivalent legal analysis regardless of the language of their query. Weight suggestion: accuracy 35 percent, completeness 25 percent, consistency 20 percent, fluency 10 percent, cultural fit 5 percent, format compliance 5 percent.

Document your dimension weights explicitly and review them quarterly. As your product evolves, user feedback accumulates, and market conditions change, the relative importance of dimensions may shift. A product that initially weighted accuracy highest might discover through user research that cultural fit is driving churn more than accuracy is.

## The Multilingual Quality Scorecard

The operational instrument that ties all of this together is the **multilingual quality scorecard** -- a per-language, per-dimension tracking system that gives your team a complete picture of quality across your entire language portfolio.

The scorecard has one row per language and one column per quality dimension. Each cell contains the current score for that language on that dimension, the trend (improving, stable, or declining over the last four evaluation cycles), and the distance from your target threshold. Color-coding makes the data scannable: green for scores above threshold, yellow for scores within 10 percent of threshold, red for scores below threshold.

The scorecard should also include a weighted composite score per language, using your use-case-specific dimension weights. This composite is the number that goes on executive dashboards and informs language-priority decisions. But the composite should always be expandable to its component dimensions. When the composite score for Turkish drops from 78 to 71, the scorecard should immediately show that the drop was driven by a cultural fit decline from 72 to 54 while fluency and accuracy held steady. The diagnosis is in the dimensions, not the composite.

Update the scorecard weekly for languages in active development and monthly for stable languages. Store historical scorecard snapshots so you can track quality trajectories over months. A language that has been declining 2 points per month for four months straight is a different problem from a language that dropped 8 points in a single week. The trajectory matters as much as the current score.

Share the scorecard broadly. Product managers need it to make market-entry and feature-priority decisions. Engineering leads need it to allocate improvement effort. Executives need it to understand the quality story across the language portfolio. The scorecard is not just a measurement tool. It is a communication tool that makes multilingual quality visible to everyone who needs to see it.

## The Interaction Between Dimensions

Quality dimensions are not independent. They interact in ways that matter for diagnosis and intervention.

A fluency improvement can mask an accuracy decline. If prompt engineering makes the model generate more natural-sounding output in Korean, the fluency score rises. But if the same prompt change causes the model to be less precise -- choosing natural-sounding approximations over precise-but-awkward phrasings -- accuracy may silently decline. Tracking dimensions separately catches this interaction. Tracking only a composite score hides it.

Cultural fit and accuracy conflict in some domains. The culturally appropriate answer to a dietary question in India might differ from the globally accepted medical answer. The model must navigate this tension, and your evaluation must assess whether it navigates it well. A single quality number cannot capture whether the model is correctly prioritizing cultural relevance over global generality or vice versa.

Completeness and fluency trade off in many languages. The model can produce a longer, more complete response that is slightly less fluent, or a shorter, more fluent response that omits information. Your dimension weights determine which trade-off is acceptable for your use case. But you can only make this determination if you are measuring both dimensions independently.

The discipline of dimension-specific measurement transforms multilingual quality management from an art into an engineering practice. Instead of "Korean quality feels low," you have "Korean fluency is 4.1, accuracy is 78 percent, cultural fit is 3.2, completeness is 64 percent relative to English." Instead of "improve Thai," you have "Thai cultural fit is below threshold -- the model uses American financial examples when discussing savings with Thai users." Every diagnosis is specific. Every intervention is targeted. Every improvement is measurable.

The next subchapter takes one of these dimensions -- cross-lingual consistency -- and examines it in depth, showing you how to build evaluation infrastructure that ensures users asking the same question in different languages get substantively equivalent answers.