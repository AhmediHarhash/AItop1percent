# 27.38 — The Latency Geography: How Physics Constrains AI Response Times

The engineer stares at the latency dashboard during a routine Monday review. Users in Singapore hit the Tokyo inference endpoint at 35 milliseconds of network round-trip latency. Users in Berlin reach the Frankfurt endpoint at 8 milliseconds. Users in Sao Paulo hit the US-East endpoint at 140 milliseconds. Users in Johannesburg have no nearby inference endpoint — their traffic travels to EU-West in Ireland, 180 milliseconds of network round-trip before a single token is processed. The engineer is building a voice AI product with a 500-millisecond total response time target. For the Singapore user, 35 milliseconds of network latency leaves 465 milliseconds for model inference and post-processing — comfortable. For the Johannesburg user, 180 milliseconds of network latency leaves 320 milliseconds for everything else — tight. And that 180 milliseconds is not a configuration problem, not a network optimization opportunity, not a caching strategy. It is physics. Light in fiber optic cable travels at roughly two-thirds the speed of light in vacuum. No amount of engineering changes that.

This is the latency geography — the physical reality that the distance between your user and your inference endpoint imposes a minimum latency floor that no software can reduce. Every AI system operates under this constraint, but most teams do not reason about it explicitly until they deploy globally and discover that users in certain regions cannot be served within their latency budget regardless of how fast the model runs.

## The Speed of Light Budget

Light in a vacuum travels at approximately 300,000 kilometers per second. Light in fiber optic cable travels at roughly 200,000 kilometers per second — about two-thirds of vacuum speed due to the refractive index of glass. For a round trip (request out, response back), you double the distance. This gives a useful rule of thumb: every 100 kilometers of fiber distance adds approximately one millisecond of round-trip latency, assuming a straight-line fiber path.

Fiber paths are never straight lines. Submarine cables follow ocean floors, curving around continents and seabeds. Terrestrial fiber follows highways, rail corridors, and rights-of-way that rarely take the shortest geographic path. The actual fiber distance between two cities is typically 1.3 to 2.0 times the great-circle distance, depending on terrain and available cable routes. Add router processing at each hop — typically 0.5 to 2 milliseconds per hop, with 10 to 20 hops on a long-haul path — and the practical latency is 1.5 to 3 times the theoretical minimum.

Here are the practical round-trip latencies between major cloud regions as measured in 2025 and 2026, representing the network latency floor before any server processing begins. New York to London: 65 to 75 milliseconds. New York to Frankfurt: 75 to 85 milliseconds. New York to Tokyo: 160 to 180 milliseconds. New York to Singapore: 220 to 240 milliseconds. London to Singapore: 160 to 175 milliseconds. London to Sydney: 260 to 280 milliseconds. Frankfurt to Mumbai: 110 to 130 milliseconds. Tokyo to Sydney: 100 to 120 milliseconds. Sao Paulo to New York: 120 to 140 milliseconds. Johannesburg to London: 160 to 180 milliseconds.

These numbers are not negotiable. You can optimize server processing time. You can compress payloads. You can cache responses. You cannot make light travel faster through glass. When you design a multi-region AI system, the latency geography is the first constraint you write down, and every other decision accommodates it.

## The Latency Budget Decomposition

Total response time as perceived by the user is the sum of several components, and understanding how they add up reveals where optimization is possible and where it is not.

**Network latency** is the round-trip time from the user's device to your inference endpoint and back. This is the physics-constrained component. For a user 5,000 kilometers from the nearest endpoint, network latency is roughly 60 to 80 milliseconds. For a user 15,000 kilometers away, it is 180 to 250 milliseconds.

**TLS handshake overhead** adds one to two round trips on the first connection. With TLS 1.3, the handshake adds one round trip. If your network latency is 80 milliseconds, the first request from a new connection incurs an additional 80 milliseconds for the TLS handshake. Subsequent requests on the same connection avoid this cost. Connection pooling and keep-alive reduce the impact, but mobile users on intermittent connections frequently establish new connections.

**Server processing time** includes request parsing, prompt assembly, any retrieval steps for RAG, and queue wait time if the inference server is busy. For a well-tuned system under moderate load, this adds 5 to 50 milliseconds. Under heavy load with request queuing, it can add hundreds of milliseconds.

**Model inference time** is the time the GPU spends generating the response. For a small model generating a short response (a classification, a short answer), inference takes 50 to 200 milliseconds. For a large model generating a long response (a multi-paragraph explanation), first-token latency is 100 to 500 milliseconds, with subsequent tokens streaming at 20 to 80 milliseconds each. The total generation time for a 200-token response from a 70-billion-parameter model can exceed 5 seconds.

**Response transmission time** adds latency proportional to response size and available bandwidth. A short API response of a few kilobytes is negligible. A streaming response where tokens are sent as they are generated amortizes this cost but does not eliminate the per-token network round trip in non-streaming configurations.

For a concrete example: a voice AI product targeting 500 milliseconds from user speech end to first audio output. The user is in Mumbai, the nearest inference endpoint is in Singapore at 80 milliseconds network latency. The latency budget is: 80 milliseconds network to Singapore, plus 50 milliseconds for speech-to-text processing, plus 150 milliseconds for LLM inference to first token, plus 40 milliseconds for text-to-speech processing of the first chunk, plus 80 milliseconds network return to Mumbai. Total: 400 milliseconds. That leaves 100 milliseconds of headroom — tight but viable. Move the endpoint to US-East (200 milliseconds network to Mumbai) and the budget is immediately blown: 200 plus 50 plus 150 plus 40 plus 200 equals 640 milliseconds, exceeding the 500-millisecond target by 28 percent with no room for optimization.

## Region Placement Strategy

Where you place inference endpoints is the single highest-leverage decision for global latency. Every other optimization — faster models, better hardware, smarter caching — operates within the latency budget that region placement establishes.

The standard approach is to start with three regions that cover the majority of the global population within 100 milliseconds of network latency: US-East (covering North America and much of Latin America), EU-West or EU-Central (covering Europe and Africa), and AP-Southeast or AP-Northeast (covering Asia-Pacific). This three-region footprint puts roughly 80 percent of the world's internet-connected population within 100 milliseconds of an inference endpoint. The remaining 20 percent — users in Sub-Saharan Africa, Central Asia, the Pacific Islands, and parts of South America — face 120 to 250 milliseconds of network latency.

Expanding beyond three regions follows a diminishing-returns curve. A fourth region — typically AP-South (Mumbai) or South America (Sao Paulo) — brings another 10 percent of users below 100 milliseconds. A fifth region might cover the Middle East or Australia, capturing another 5 percent. Each additional region adds full infrastructure cost (GPU procurement, model replication, operational overhead) for a progressively smaller user population improvement.

The decision framework for adding a region is not "do we have users there?" but "do we have enough users there with latency-sensitive workloads to justify the infrastructure cost?" A batch processing API that returns results in minutes does not need a region in every geography. A real-time chatbot with a 500-millisecond response target does. A voice AI product with a 200-millisecond first-response target might need six or seven regions to serve a global user base. The latency requirement drives the region count, not the other way around.

## The Long-Tail Latency Problem

Optimizing for median latency is a trap. If your three-region deployment gives 90 percent of users sub-100-millisecond network latency, the system feels fast in aggregate. But the 10 percent of users who face 150 to 250 millisecond latency — often located in emerging markets that represent your fastest-growing user segments — experience a meaningfully worse product. They see the loading spinner longer. Their voice AI conversations feel sluggish. Their interactive agent takes a beat too long to respond. These users do not know they are in the latency long tail. They just think the product is slow.

The long tail is not uniformly distributed. It clusters in specific geographies: Sub-Saharan Africa, Southeast Asia outside the major hubs, South America outside Brazil and Argentina, Eastern Europe, and the Middle East. These are often the markets where AI products have the largest greenfield opportunity and where competitors with better regional infrastructure will win the user.

Three approaches address the long tail without deploying full inference infrastructure in every geography. First, **edge points of presence** that terminate TLS and maintain persistent connections to the nearest inference region. This eliminates the TLS handshake penalty for new connections and keeps the TCP connection warm, reducing effective latency by 30 to 80 milliseconds for users on high-latency paths. CDN providers like Cloudflare, Akamai, and Fastly operate PoPs in over 200 cities worldwide, including many in the latency long-tail geographies. Second, **response prediction and prefetching** for conversational AI — if the system can predict the most likely next user intent (based on conversation history and common patterns), it can pre-generate responses and serve them from cache with zero inference latency. This works for the 20 to 30 percent of interactions that follow predictable patterns. Third, **streaming responses** that send the first token as soon as it is generated, rather than waiting for the complete response. For a user at 200 milliseconds of network latency, streaming means they see the first word 200 milliseconds after it is generated, and subsequent words arrive at generation speed. The perceived latency is first-token latency plus network latency, rather than full-generation latency plus network latency. Streaming converts a 5-second wait into a 200-millisecond wait followed by real-time text appearing — a dramatically better user experience.

## Edge Inference: When Centralized Latency Cannot Work

For some applications, even the nearest cloud region is too far. Voice AI, real-time translation, AR/VR overlays, and in-vehicle assistants demand response times measured in tens of milliseconds — budgets that leave no room for a network round trip to a cloud data center hundreds of kilometers away.

**Edge inference** deploys models directly to locations close to users — CDN edge nodes, telecom PoPs, on-premises servers, or the user's own device. In 2026, edge inference is moving from experimental to production for specific model categories. Small models — those under 3 billion parameters — run efficiently on edge hardware including NVIDIA Jetson, Apple Neural Engine, and Qualcomm AI accelerators built into smartphones and laptops. Quantized versions of 7-billion-parameter models can run on high-end consumer GPUs at the edge with acceptable throughput for single-user serving.

The tradeoff is model capability. The models that fit on edge hardware are smaller and less capable than the frontier models running on H100 clusters in cloud data centers. A 3-billion-parameter model deployed at the edge produces faster responses but lower-quality outputs than a 70-billion-parameter model in the cloud. The architectural pattern that balances this tradeoff is **tiered inference**: the edge model handles the first response or a quick classification, and if the query requires deeper reasoning, it escalates to the cloud model. The user sees an instant initial response from the edge model followed by a refined response from the cloud model seconds later. This pattern is becoming standard for voice assistants and real-time search in 2026, where the perceived responsiveness of the edge model masks the latency of the cloud model working in the background.

For organizations that need cloud-grade models closer to users but cannot deploy their own edge hardware, providers like Akamai, Cloudflare, and several telecom companies offer inference-at-the-edge services — GPU-equipped PoPs in metropolitan areas that are 5 to 20 milliseconds from end users instead of 50 to 200 milliseconds. These edge GPU clusters are smaller than centralized cloud regions and support a narrower range of models, but for latency-critical workloads they close the gap between what physics allows and what the product requires.

## When Latency Tolerance Defines Architecture

Not every AI workload fights the same physics battle. The latency tolerance of your application — not the ambition of your engineering team — should dictate your architectural choices.

**Sub-200-millisecond applications** include voice AI, real-time translation, and interactive gaming. At this budget, network latency consumes most of the available time. These applications require edge inference for the most latency-sensitive path, with cloud fallback for complex queries. Region count is high — six to eight or more — or edge deployment is mandatory. Every millisecond of inference optimization matters because the margin between meeting and missing the target is razor thin.

**200-to-500-millisecond applications** include interactive chatbots, real-time search augmentation, and live content moderation. A three-to-five-region deployment with careful region placement serves most global users within budget. Streaming responses reduce perceived latency. Model optimization (smaller models, speculative decoding, quantization) directly improves the user experience because inference time is the dominant variable after network latency is controlled by region placement.

**500-millisecond-to-5-second applications** include document analysis, complex reasoning tasks, and multi-step agent workflows. Network latency is a minor factor. A two-to-three-region deployment is sufficient. Investment in faster inference hardware or model optimization has diminishing returns because the user already expects a wait. Investment in response quality — using larger models, more retrieval steps, longer reasoning chains — is more valuable than investment in speed.

**Above-5-second applications** include batch processing, overnight report generation, and large-scale data analysis. Latency is irrelevant. A single-region deployment is often sufficient. Route batch workloads to the region with the cheapest GPU capacity, regardless of user location. The physics of light in fiber optic cable has no impact on a job that runs for hours.

Matching your architecture to your latency tier prevents two common mistakes: over-engineering low-latency infrastructure for applications that do not need it (wasting money on edge deployment for a batch pipeline) and under-engineering for applications that do (deploying a voice AI product from a single US region and wondering why users in Asia complain about delays). The latency geography does not change. Your architecture must accommodate it, not ignore it.

---

Physics sets the latency floor. Traffic steering chooses the region. But neither tells you whether the region has enough GPU capacity to handle the traffic it receives — and in a world where GPU supply changes quarterly, capacity planning for multi-region AI is its own discipline. The next subchapter covers regional capacity planning in a GPU-scarce world: how to forecast demand, secure supply, and handle the reality that the GPUs you need may not be available in the region where you need them.
