# 27.28 — High-Performance Storage for AI: Object Stores, Parallel File Systems, and Model Weight Distribution

In late 2025, a computer vision company deployed its new object detection model to production. The model was a fine-tuned 70-billion-parameter architecture — roughly 140 gigabytes of weights in FP16 precision — stored in their standard S3-compatible object store. The deployment pipeline was simple: a new inference pod starts, downloads the model weights from S3, loads them into GPU memory, and begins serving requests. In testing, with a single pod pulling weights over an uncongested network, the download took about four minutes. Acceptable. Then the team scaled to twelve pods across three availability zones for a peak-traffic product launch. All twelve pods started simultaneously. All twelve began downloading 140 gigabytes from the same S3 bucket at the same time. Total bandwidth demand: 1.68 terabytes, all hitting the same storage endpoint within the same minute. S3 throttled the requests. Download times climbed from four minutes to forty-five. Pods failed their readiness probes and were killed by Kubernetes. Restarted pods tried again, adding to the congestion. The product launch was delayed by over an hour, and the root cause was not the model, not the code, not the orchestration. It was the storage layer — designed for web-scale object access patterns, not for the brute-force bandwidth demands of loading multi-gigabyte AI artifacts into GPU memory under concurrency.

AI workloads have storage requirements that break every assumption built into standard cloud storage. Training reads terabytes of data in sequential streams that must keep GPUs fed without stalls. Inference loads model weights that range from a few gigabytes for small models to hundreds of gigabytes for frontier architectures. Checkpointing writes tens of gigabytes every few minutes and must complete faster than the next training step begins, or the entire pipeline stalls. Understanding the storage hierarchy — and matching each workload to the right tier — is the difference between an AI platform that performs and one that spends half its GPU budget waiting for bytes.

## The Storage Hierarchy for AI Workloads

AI storage operates across three tiers, and each tier serves a fundamentally different purpose. Mixing them up — using cold storage where you need hot, or hot storage where cold would suffice — either cripples performance or wastes money.

The **hot tier** is local NVMe SSD attached directly to the compute node. NVMe drives in 2026 deliver 3 to 7 gigabytes per second of sequential read throughput per drive, with sub-100-microsecond latency. A node with four NVMe drives can sustain 12 to 28 gigabytes per second of local storage bandwidth. This tier is critical for two use cases: model weight caching and training checkpoints. When an inference pod starts, it should find model weights already cached on local NVMe rather than pulling them from the network. When a training job writes a checkpoint, the write goes to local NVMe first, then asynchronously replicates to durable storage. The local NVMe tier is ephemeral — data on it disappears when the node is recycled — but its speed makes it the foundation of fast cold starts and reliable checkpointing.

The **warm tier** is a high-performance shared file system accessible by all nodes in a cluster. This is where parallel file systems live — Lustre, IBM Spectrum Scale (formerly GPFS), and WekaFS. These systems provide shared, high-throughput access at bandwidths of hundreds of gigabytes per second across the cluster. The warm tier stores shared datasets that multiple training jobs read simultaneously, model weights that multiple inference pods load concurrently, and intermediate artifacts that need to persist beyond a single node's lifetime. The warm tier is the workhorse of AI storage.

The **cold tier** is object storage — S3, Google Cloud Storage, Azure Blob Storage. Object stores are cheap, durable, and practically infinite in capacity. They are also high-latency and single-stream. A single S3 GetObject request delivers roughly 100 to 700 megabytes per second depending on the object size and concurrency. Fine for archiving training datasets, storing model version history, and serving as the system of record for artifacts that change infrequently. Not fine for anything that sits in the critical path of model loading or training data ingestion.

## Parallel File Systems: The Warm Tier That Changes Everything

The single most impactful storage investment for an AI platform is a parallel file system. Standard file systems — NFS, EFS, even high-performance block storage — were designed for workloads where a single client reads a single file. AI workloads need hundreds of clients reading different chunks of the same dataset simultaneously, at multi-gigabyte-per-second throughput per client.

**Lustre** is the most widely deployed parallel file system for AI in 2026. Originally built for high-performance computing, Lustre stripes files across multiple storage servers (called Object Storage Targets, or OSTs), allowing parallel reads that scale linearly with the number of servers. A Lustre deployment with 20 OSTs, each backed by NVMe storage, can deliver over 200 gigabytes per second of aggregate read throughput. Google Cloud launched its managed Lustre CSI driver for GKE in mid-2025, and AWS offers FSx for Lustre with direct S3 integration. Azure offers Azure Managed Lustre. All three allow Kubernetes pods to mount Lustre volumes through CSI drivers, making the parallel file system appear as a standard POSIX directory inside the container.

**WekaFS** has emerged as the performance leader for mixed AI workloads. WekaFS uses an NVMe-native architecture that eliminates the legacy storage controller bottleneck, delivering throughput that benchmarks at roughly three times faster than FSx for Lustre in mixed read-write patterns. WekaFS supports both POSIX and S3 API access to the same data, which means training jobs can read datasets through the POSIX mount while data pipelines ingest new data through S3-compatible API calls. Its Kubernetes CSI plugin provides persistent volume mounts with low-latency NVMe performance across a shared file system.

**IBM Spectrum Scale** (GPFS) remains dominant in on-premises AI infrastructure, particularly at organizations that run both HPC and AI workloads. Its strength is protocol flexibility — the same file system serves NFS, SMB, S3, and POSIX clients simultaneously — and its mature data lifecycle management, which automatically tiers data between flash and spinning disk based on access patterns.

The choice between these systems depends on your deployment model. If you run on a single cloud provider, their managed parallel file system offering (FSx for Lustre on AWS, Managed Lustre on GCP, Managed Lustre on Azure) minimizes operational overhead. If you run on-premises or multi-cloud, WekaFS or Spectrum Scale give you portability. If throughput per dollar is the primary concern and your workload is read-heavy sequential access, Lustre wins. If your workload mixes random reads, writes, and metadata operations, WekaFS handles the mixed pattern more efficiently.

## The Model Weight Loading Problem

Loading model weights into GPU memory is the dominant cold-start cost for inference workloads, and it is getting worse as models grow. A 7-billion-parameter model in FP16 is about 14 gigabytes. A 70-billion-parameter model is 140 gigabytes. A mixture-of-experts model with hundreds of billions of total parameters can exceed 300 gigabytes even after quantization. Loading these weights from storage into GPU memory is a sequential bottleneck: the weights must be read from storage, transferred across the network to the host, copied from host memory to GPU memory, and then the model is ready to serve.

Over standard S3 with a single connection, loading 140 gigabytes takes 3 to 7 minutes depending on network conditions. Over a parallel file system like Lustre or WekaFS, the same load completes in 20 to 40 seconds because the file is striped across multiple servers and read in parallel. Over local NVMe cache, the load takes 20 to 35 seconds limited by the PCIe bandwidth between the drive and host memory. The gap between S3 and parallel file system is the gap between a 7-minute cold start and a 30-second cold start. For inference workloads that autoscale in response to traffic spikes, this is the difference between meeting your latency SLA and dropping requests for minutes while new pods load weights.

**GPUDirect Storage**, introduced by NVIDIA, eliminates one hop in the data path. Instead of reading from storage into host memory and then copying from host memory to GPU memory, GPUDirect Storage allows the storage device to DMA data directly into GPU memory over PCIe or NVMe-over-Fabrics. This eliminates the host memory copy and reduces the CPU overhead of the transfer. In practice, GPUDirect Storage with a high-performance parallel file system can reduce model loading time by 30 to 50 percent compared to the standard two-hop path. WekaFS and Lustre both support GPUDirect Storage integration, and it is becoming the standard path for model loading on NVIDIA hardware in 2026.

The most sophisticated model loading systems in 2026 go further. **Model streaming** — where inference begins before the entire model is loaded — allows a pod to start serving requests as soon as the first layers are in GPU memory, while background threads continue loading the remaining layers. This cuts the effective cold-start time from the full model load time to the time required to load just the first few layers. Benchmarks show effective cold-start reductions of up to 6x with model streaming compared to full-load-then-serve approaches.

## CSI Drivers: Bridging Storage and Kubernetes

**Container Storage Interface** drivers are the bridge between high-performance storage systems and the Kubernetes pod lifecycle. Without CSI drivers, you would need to manually mount file systems on every node, manage credentials outside Kubernetes, and handle storage provisioning through scripts disconnected from your workload definitions.

A CSI driver for a parallel file system allows pods to declare a PersistentVolumeClaim that references a Lustre or WekaFS volume. When the pod is scheduled to a node, the CSI driver mounts the file system automatically. When the pod is deleted, the mount is cleaned up. This lifecycle integration means your training jobs and inference deployments can reference shared storage in their pod specs just like they reference any other Kubernetes volume.

Google's GKE Managed Lustre CSI driver, launched in 2025, provisions and manages the Lustre file system entirely through Kubernetes APIs — you create a StorageClass referencing Managed Lustre, create a PersistentVolumeClaim, and GKE handles the rest. AWS's FSx for Lustre CSI driver provides similar integration for EKS. WekaFS publishes its own CSI driver that supports dynamic provisioning, snapshots, and volume expansion. For on-premises deployments, the open-source Lustre CSI driver maintained by the community provides basic mount integration, though it requires you to manage the Lustre cluster yourself.

The critical configuration detail that trips up most teams is read-write mode. AI training jobs typically need ReadWriteMany access — multiple pods reading and writing to the same volume simultaneously. Standard Kubernetes block storage supports ReadWriteOnce (a single node). Parallel file systems natively support ReadWriteMany because that is their entire purpose. But you must specify the access mode correctly in the PersistentVolumeClaim, and the CSI driver must support it. Mismatched access modes result in pods stuck in Pending with cryptic "volume already attached" errors that waste hours of debugging time.

## Checkpointing: Where Write Performance Becomes Critical

Training checkpoints are the insurance policy against lost work. A 70-billion-parameter model with optimizer state produces checkpoints of 280 to 560 gigabytes — the model weights plus the optimizer's momentum and variance buffers, often in full FP32 precision. If checkpoint writes are slower than the checkpoint interval, checkpoints pile up and compete with training for storage bandwidth. If checkpoint writes fail, the training run has no rollback point. A hardware failure after twelve hours of training with no valid checkpoint means twelve hours of wasted GPU time.

Local NVMe is the first checkpoint target because it is the fastest write path. A node with four NVMe drives can absorb a 280-gigabyte checkpoint in 10 to 20 seconds. After the local write completes, an asynchronous process copies the checkpoint to durable shared storage — the parallel file system or object store. This two-phase checkpoint pattern decouples the training pipeline from the durability pipeline. The training step is blocked only for the fast local write, not for the slower network transfer.

Distributed checkpointing, where each node writes its own shard of the checkpoint simultaneously, multiplies the aggregate write throughput by the number of nodes. A 32-node training job with four NVMe drives per node has an aggregate local write bandwidth of over 400 gigabytes per second. Even a 560-gigabyte checkpoint completes in under two seconds at that bandwidth. The challenge shifts to the parallel file system, which must absorb all 32 shards arriving simultaneously. This is exactly the access pattern that parallel file systems are designed for, and it is exactly the pattern that object stores handle poorly. Writing 32 concurrent multi-gigabyte objects to S3 triggers throttling, retry storms, and write latencies that stretch checkpoint completion from seconds to minutes.

## Designing for the Storage Mix

No single storage technology serves every AI workload optimally. The design pattern that mature AI platforms follow in 2026 is a tiered storage architecture where data moves automatically between tiers based on access frequency and latency requirements.

Model weights start in the cold tier (object store) when first published by the training pipeline. When a deployment targets a model for serving, a pre-staging job copies the weights to the warm tier (parallel file system) in the target region. When an inference pod is scheduled to a node, the node's local cache agent pulls the weights from the warm tier to the hot tier (local NVMe). When the pod starts, it loads from local NVMe at full drive speed. This three-tier pipeline turns a 7-minute S3 cold start into a sub-30-second local load, provided the pre-staging runs ahead of the deployment.

Training datasets follow a similar pattern. Raw data lives in the cold tier. The data pipeline processes and writes to the warm tier. Training jobs read from the warm tier's parallel file system at aggregate throughputs that would overwhelm any object store. Checkpoints write to the hot tier first, then replicate to the warm tier, then archive to the cold tier for long-term retention.

The storage budget reflects this hierarchy. Expect the hot tier (local NVMe) to cost 3 to 5 times more per gigabyte than the warm tier (parallel file system), and the warm tier to cost 5 to 10 times more than the cold tier (object store). The key is putting the minimum necessary data on the most expensive tier and the maximum on the cheapest. Model weights that will be served in the next hour belong on NVMe. Model weights from six months ago belong on S3. Treating all storage as equal — either putting everything on S3 and accepting slow loads, or putting everything on NVMe and accepting massive cost — is the storage anti-pattern that most teams adopt by default before they learn the tiered model the hard way.

---

Storage speed determines how fast your pods start. But before the storage layer even comes into play, the container image must arrive on the node — and AI container images are an order of magnitude larger than anything traditional infrastructure was designed to handle. The next subchapter examines why multi-gigabyte container images break standard image management and what to do about it.
