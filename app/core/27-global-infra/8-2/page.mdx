# 27.57 â€” Distributed Training Infrastructure: FSDP, DeepSpeed, and Multi-Node Coordination

In late 2025, a machine learning platform team at a mid-sized AI company attempted to fine-tune a 70-billion-parameter model across 16 nodes, each with eight H100 GPUs. Their first attempt used standard data parallelism -- replicating the full model on every GPU. The job crashed immediately with out-of-memory errors. The model alone consumed 140 gigabytes in 16-bit precision, far more than a single GPU's 80 gigabytes. Their second attempt switched to Fully Sharded Data Parallel, distributing the model across all GPUs, but the default configuration generated so much inter-node communication that effective throughput was worse than training on a single node with a smaller batch size. Their third attempt -- after two weeks of configuration work -- combined FSDP with the correct sharding strategy, overlapped communication with computation, applied gradient checkpointing to reduce peak memory, and placed tensor-parallel groups within nodes where NVLink provided high bandwidth. That third configuration achieved 87 percent scaling efficiency across eight nodes. The lesson was expensive but universal: distributed training is not something you turn on. It is something you engineer.

## Why Distributed Training Exists

The reason is simple arithmetic. Models have grown faster than GPU memory. A 7-billion-parameter model in 16-bit precision fits on a single 80-gigabyte GPU with room for activations and optimizer state. A 70-billion-parameter model does not fit on a single GPU even in 16-bit, let alone with the 4x to 8x memory multiplier that training requires for optimizer state and gradients. A 400-billion-parameter model requires dozens of GPUs just to hold the parameters. The only path forward is to split the work across multiple GPUs, and the way you split it determines whether you get near-linear speedup or near-total waste.

There is a second motivation beyond memory: time. Even when a model fits on a single GPU, training on a large dataset may take weeks. Distributing the work across 8, 32, or 128 GPUs can compress that timeline to days or hours. But distributed training only saves time when the communication overhead of coordinating GPUs does not consume the time saved by parallelizing computation. The infrastructure decisions you make -- which framework, which parallelism strategy, which hardware configuration -- determine where you land on that spectrum.

## The Parallelism Taxonomy

Distributed training is not one technique. It is a family of strategies, each splitting the work differently, each with different infrastructure requirements.

**Data parallelism** is the simplest. Every GPU holds a complete copy of the model. Each GPU processes a different mini-batch of training data. After the forward and backward passes, all GPUs synchronize their gradients through an all-reduce operation -- each GPU sends its gradients to every other GPU, and all GPUs end up with the same averaged gradient. Then each GPU updates its local copy of the model with the averaged gradient. The infrastructure requirement is straightforward: enough memory per GPU to hold the full model plus optimizer state, and a network fast enough to complete the all-reduce before it becomes the bottleneck. For models that fit on a single GPU, data parallelism with 8 to 64 GPUs is the default approach and often the only one you need.

**Model parallelism**, also called tensor parallelism, splits the model itself across GPUs. Instead of each GPU holding the full model, each GPU holds a fraction of each layer's weight matrices. A single matrix multiplication is split so that each GPU computes a portion and the partial results are combined. This reduces the per-GPU memory requirement proportionally -- splitting across four GPUs means each GPU holds roughly one quarter of each layer. The infrastructure requirement is severe: tensor parallelism generates communication within every forward and backward pass, not just at the gradient synchronization point. GPUs performing tensor parallelism must exchange activations and partial results continuously. This demands the highest bandwidth interconnect available -- NVLink within a node, not InfiniBand between nodes. In practice, tensor parallelism is almost always confined to GPUs within a single node, where NVLink provides the necessary bandwidth.

**Pipeline parallelism** splits the model by layers rather than within layers. GPU 0 holds layers 1 through 10, GPU 1 holds layers 11 through 20, and so on. Data flows through the pipeline like an assembly line -- GPU 0 processes the first micro-batch's early layers and passes the activations to GPU 1, then immediately starts on the second micro-batch while GPU 1 processes the first. The infrastructure requirement is more relaxed than tensor parallelism because the communication is point-to-point between adjacent stages, not all-to-all. But pipeline parallelism introduces **pipeline bubbles** -- time when some GPUs sit idle because they are waiting for the previous stage to finish. Techniques like interleaved scheduling and micro-batching reduce but do not eliminate these bubbles.

**Expert parallelism** applies specifically to Mixture of Experts models, where each input token is routed to a subset of specialized "expert" sub-networks. Each GPU hosts a different set of experts, and tokens are communicated to the GPU holding the relevant expert via all-to-all communication. The infrastructure requirement is high-bandwidth all-to-all networking, similar to data parallelism but with more irregular communication patterns because different tokens route to different experts.

Most large-scale training in 2026 uses a combination of these strategies simultaneously. A common configuration for a 70B model on 64 GPUs across 8 nodes: tensor parallelism across 4 GPUs within each node using NVLink, pipeline parallelism across 2 stages spanning node pairs, and data parallelism across 8 replicas of the pipeline. This 3D parallelism approach matches each strategy to the interconnect tier it requires -- tensor parallel within high-bandwidth NVLink, pipeline parallel across moderate-bandwidth InfiniBand, data parallel across the full cluster.

## FSDP: PyTorch's Native Sharding

**Fully Sharded Data Parallel** is PyTorch's built-in approach to distributed training for models that are too large for standard data parallelism. Instead of every GPU holding a complete copy of the model, FSDP shards the model parameters, gradients, and optimizer states across all participating GPUs. Each GPU holds only a fraction of the total model state. When a GPU needs to compute a forward or backward pass for a particular layer, it gathers the full parameters for that layer from all other GPUs, performs the computation, and then discards the gathered parameters to free memory.

The first version of FSDP, now referred to as FSDP1, concatenated and flattened groups of parameters into large buckets before sharding. This worked but created awkward constraints around frozen parameters and made debugging difficult because the relationship between the original model parameters and the sharded buckets was opaque. FSDP2, which became the recommended path in PyTorch starting in 2025, uses per-parameter sharding instead. Each parameter is chunked independently along its first dimension and distributed across GPUs. This makes the sharding intuitive -- you can inspect any parameter's shard directly -- and eliminates the constraints that FSDP1 imposed on mixing frozen and trainable parameters, which is critical for parameter-efficient fine-tuning techniques like LoRA.

The infrastructure implication of FSDP is that it converts memory pressure into communication pressure. Instead of needing enough per-GPU memory for the full model, you need enough network bandwidth to gather parameter shards before each layer's computation. FSDP overlaps these gather operations with computation -- while one layer is being computed, the parameters for the next layer are being gathered in the background. This overlap works only when the network is fast enough that the gather completes before the computation finishes. On InfiniBand with 400 gigabits per second, this overlap is typically effective. On slower networks, the gather becomes a blocking operation and training throughput drops.

## DeepSpeed: ZeRO and Beyond

Microsoft's **DeepSpeed** library provides a complementary set of distributed training capabilities, with its ZeRO optimizer being the most widely used. ZeRO comes in three stages of increasing aggressiveness. ZeRO Stage 1 shards only the optimizer state across GPUs. ZeRO Stage 2 shards optimizer state and gradients. ZeRO Stage 3 shards optimizer state, gradients, and model parameters -- functionally equivalent to FSDP's full sharding.

The choice between FSDP and DeepSpeed ZeRO-3 is less about capability and more about ecosystem. FSDP integrates natively with PyTorch's distributed primitives, making it the natural choice for teams already deep in the PyTorch ecosystem. DeepSpeed provides a self-contained runtime with its own configuration system, which some teams find easier to manage but others find opaque when debugging. In 2026, the Hugging Face Accelerate library supports both frameworks through a unified configuration interface, making it possible to switch between them without rewriting training code -- which is valuable because the optimal choice can depend on the specific model architecture, hardware configuration, and parallelism strategy.

DeepSpeed also offers capabilities beyond ZeRO. DeepSpeed-MoE optimizes all-to-all communication for Mixture of Experts models, significantly reducing the overhead of expert parallelism. DeepSpeed Inference provides optimized kernels for serving, though that is outside the training infrastructure scope. The DeepSpeed ecosystem's breadth makes it particularly valuable for teams training MoE architectures or needing pipeline parallelism integrated with ZeRO sharding.

## Communication Patterns and Network Requirements

Every parallelism strategy generates a specific communication pattern, and the infrastructure must be designed to handle the pattern your training configuration produces.

Data parallelism and FSDP generate **all-reduce** and **all-gather** operations. All-reduce aggregates gradients across all GPUs -- every GPU contributes its local gradient and receives the averaged result. All-gather collects parameter shards from all GPUs so each GPU temporarily holds the full parameter for a layer. Both operations involve all GPUs communicating simultaneously, and the total data volume scales with the model size. For a 70B model, a single all-reduce of gradients moves approximately 140 gigabytes of data across the network. With training steps happening every few seconds, this sustained bandwidth requirement is enormous.

Tensor parallelism generates **all-reduce** operations within each tensor parallel group, but at a much higher frequency -- after nearly every matrix multiplication, not just at the end of the training step. The volume per operation is smaller, but the sensitivity to latency is much higher because computation blocks on each communication step. This is why tensor parallelism demands NVLink-class interconnect with sub-microsecond latency, not InfiniBand with multi-microsecond latency.

Pipeline parallelism generates **point-to-point** sends between adjacent stages. The volume is moderate -- it is the size of the activation tensors at the boundary between pipeline stages. The latency sensitivity is moderate because micro-batch scheduling provides some tolerance for communication delay.

The infrastructure decision tree follows directly. Place tensor parallel groups on GPUs connected by NVLink within a single node. Place pipeline stages on nodes connected by the fastest available inter-node interconnect. Place data parallel replicas across the broader cluster. If you have only one interconnect tier -- for example, a cloud cluster where all GPUs are connected by the same network fabric -- you are limited to strategies that tolerate higher-latency communication, which typically means FSDP or DeepSpeed ZeRO without tensor parallelism.

## The Communication-Computation Overlap

The single most important optimization in distributed training infrastructure is not faster hardware. It is hiding communication behind computation. Modern distributed training frameworks schedule gradient communication to overlap with the next layer's computation. While GPU 0 is computing the backward pass for layer N, it is simultaneously sending the gradients from layer N+1, which already completed, to all other GPUs.

This overlap turns a sequential process -- compute, then communicate, then compute, then communicate -- into a pipelined process where communication and computation happen in parallel. When the overlap is effective, the communication cost nearly disappears from the wall-clock training time. When it fails -- because the network is too slow, or the computation per layer is too fast for communication to hide behind, or the framework configuration does not enable prefetching -- communication appears as dead time in the GPU utilization profile. You see the GPUs at 100 percent utilization for a burst, then dropping to near zero while they wait for communication, then spiking again. That sawtooth pattern in GPU utilization is the signature of failed communication overlap.

Diagnosing this requires profiling tools like PyTorch Profiler or NVIDIA Nsight Systems, which show the timeline of computation and communication operations on each GPU. Look for gaps between computation kernels. If those gaps correlate with communication operations on the network interface, your overlap is not working. The fix depends on the cause -- it might be increasing the number of FSDP sharding groups so that smaller communications can complete faster, enabling prefetching so gather operations start earlier, or adding gradient checkpointing so that computation per layer takes longer relative to communication, giving the overlap more time to hide the transfer.

---

A training cluster with the right hardware and the right distributed training configuration can run a job efficiently. But in any organization with more than one team, the question immediately becomes: who gets to run their job, and when? The next subchapter tackles training job scheduling -- the system that decides which team's job starts first, how to handle preemption, and how to prevent one team from monopolizing the cluster while others wait.
