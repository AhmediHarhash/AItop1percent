# 27.9 — Why Kubernetes Won the AI Orchestration War

Every major AI company in 2026 runs on Kubernetes. This was not inevitable. Kubernetes was designed for stateless web microservices, not for GPU-intensive training jobs that need gang scheduling, topology-aware placement, and terabytes of checkpoint storage. The fact that the container orchestrator built for autoscaling PHP frontends became the substrate for training frontier language models is one of the most unlikely infrastructure stories of the decade — and understanding how it happened is essential for anyone building AI platforms today, because it explains both the strengths you can rely on and the gaps you will need to fill.

## The Alternatives That Lost

To understand why Kubernetes won, you need to understand what it beat.

**Slurm** was the incumbent. For twenty years, Slurm was the standard job scheduler in high-performance computing. It was built for exactly the kind of workloads AI teams run — distributed jobs across dozens of GPUs, gang scheduling that ensures all nodes start simultaneously, and queue management that fairly distributes a shared cluster across research teams. Slurm's scheduling language is terse and expressive. A distributed training job that takes forty lines of Kubernetes YAML can be described in a ten-line Slurm script. For pure training on dedicated bare-metal GPU clusters, Slurm is arguably still the better scheduler in 2026.

But Slurm lost the war for three reasons. First, it has no concept of cloud-native operations. Slurm assumes a static cluster with fixed hardware that an operations team manages manually. When the cloud became the default compute environment — elastic, API-driven, on-demand — Slurm had no answer. Adding or removing nodes required manual intervention or fragile scripts bolted onto a system that was never designed for elasticity. Second, Slurm has almost no ecosystem for inference serving, monitoring, service discovery, or rolling deployments. If you run training on Slurm, you still need a separate platform for everything else. Third, the talent pool shifted. Engineers entering the workforce in 2024 and 2025 learned Kubernetes. They learned Helm charts and container images and declarative YAML. Slurm expertise concentrated in academic HPC centers and national labs. Hiring Slurm operators became harder every year while Kubernetes engineers became abundant.

**Apache Mesos** was briefly a contender. In 2015 and 2016, Mesos could claim superior scheduling capabilities and better resource isolation. Twitter, Apple, and Airbnb ran massive Mesos deployments. But Mesos lost developer mindshare to Kubernetes between 2017 and 2019, and without mindshare, the ecosystem stopped growing. Cloud providers chose to build managed Kubernetes services, not managed Mesos services. The Apache Software Foundation moved Mesos to the attic in 2024. It is effectively dead.

**Custom schedulers** — internal systems built by individual companies for their specific workloads — work for exactly one organization at exactly one point in time. Several large AI labs built custom orchestration layers between 2020 and 2023, and most of them have since migrated to Kubernetes with custom extensions. The reason is straightforward: maintaining a bespoke orchestrator requires a dedicated infrastructure team of five to fifteen engineers who must solve every problem that the Kubernetes community solves collectively. When an engineer leaves, their institutional knowledge of the custom system leaves with them. When a new GPU architecture arrives, the custom system needs custom integration work. The maintenance tax compounds until the organization either collapses under it or migrates to the community-supported option.

## What Kubernetes Got Right

Kubernetes won not because it was good at running AI workloads. It won because it was good at being a platform that other people could extend to run AI workloads. The distinction matters enormously.

The **declarative API model** is the foundation. You describe the desired state of your workload — how many replicas, what resources it needs, how it should be scheduled — and the control plane continuously reconciles actual state with desired state. This model is powerful for any workload type because it separates intent from implementation. When NVIDIA needed to integrate GPU scheduling into Kubernetes, they did not need to modify the core scheduler. They wrote a device plugin that registers GPUs as a schedulable resource type. When the community needed workload queuing, they did not need to fork Kubernetes. They built Kueue as an external controller that manages admission to the scheduler. The API model made Kubernetes a platform for building platforms, which is exactly what AI infrastructure requires.

**Custom Resource Definitions** are the second decisive advantage. CRDs let any team extend the Kubernetes API with domain-specific objects. PyTorch training jobs, inference endpoints, model registries, evaluation pipelines — all of these can be expressed as Kubernetes-native resources with their own schemas, controllers, and lifecycle management. This means the tooling ecosystem — kubectl, Helm, ArgoCD, Terraform — works with AI-specific resources without modification. An inference endpoint is managed the same way as a deployment, with the same GitOps workflows, the same RBAC, and the same audit logging.

**Cloud provider integration** is the third advantage. Every major cloud provider offers a managed Kubernetes service — GKE, EKS, AKS — with first-class GPU node pool support, automated driver installation, and integration with cloud-native storage and networking. This means that organizations can run AI workloads on Kubernetes without managing the control plane themselves. The operational burden that made early Kubernetes adoption painful — etcd maintenance, API server scaling, certificate rotation — is handled by the cloud provider. Your platform team focuses on the AI-specific layer, not on keeping the orchestrator itself alive.

## What Kubernetes Got Wrong Initially

The list of what Kubernetes lacked for AI workloads as recently as 2023 is sobering. No GPU memory awareness — the scheduler knew a node had four GPUs but not whether each had 40 or 80 gigabytes of memory. No topology-aware scheduling — the scheduler placed pods on GPUs without knowing which GPUs were connected via NVLink and which communicated over the comparatively glacial PCIe bus, creating an eight-times performance penalty for distributed training when pods landed on the wrong topology. No workload queuing — when a GPU cluster was full, pods simply failed to schedule rather than entering a queue, which meant teams had to build retry logic or watch for capacity themselves. No gang scheduling — a distributed training job requesting thirty-two GPUs would get twenty-four immediately and wait indefinitely for the remaining eight, while those twenty-four GPUs sat idle waiting for the gang to complete. No admission control — two hundred teams could submit jobs simultaneously with no mechanism to enforce fair-share or prevent one team from monopolizing the cluster.

Any one of these gaps would be an inconvenience. Together, they meant that vanilla Kubernetes was essentially unusable for production AI workloads beyond the simplest single-GPU inference scenario. The platform teams that adopted Kubernetes for AI in 2021 and 2022 did so with the understanding that they would need to build, integrate, and maintain a substantial stack of extensions on top of the base platform.

## How the Ecosystem Filled the Gaps

The ecosystem response has been one of the most productive periods of open-source infrastructure development in the cloud-native era. Each gap in Kubernetes attracted focused projects that solved specific problems without requiring changes to the core platform.

The **NVIDIA device plugin** and **GPU Operator** addressed the most basic need — making GPUs visible to the Kubernetes scheduler. The device plugin exposes GPUs as an extended resource type, allowing pods to request a specific number of GPUs. The GPU Operator automates the full lifecycle of GPU software management — driver installation, container toolkit configuration, device plugin deployment, and GPU feature discovery — so that adding a new GPU node to a cluster requires no manual configuration. By 2026, the GPU Operator has reached its twenty-fifth major release and supports every NVIDIA GPU architecture from Ampere through Blackwell, along with both CUDA and the newer Kubernetes Dynamic Resource Allocation interface.

**Kueue** became the standard for workload queuing and fair-share scheduling. Built by the Kubernetes community with heavy investment from Google, Kueue sits between job submission and the Kubernetes scheduler, holding workloads in a queue until resources are available. It provides cluster-wide quotas, per-team allocation policies, resource borrowing between teams, and priority-based preemption. By 2026, Kueue has become the default recommendation for any organization running more than a handful of GPU workloads on a shared cluster.

**Volcano** solved the gang scheduling problem for distributed training. It ensures that all pods in a training job are scheduled simultaneously or not at all, preventing the deadlock scenario where partial allocations waste expensive GPU resources while waiting for the full gang to assemble.

NVIDIA's **KAI Scheduler** — open-sourced in early 2025 from their Run:ai acquisition — brought enterprise GPU scheduling features including topology-aware placement, fractional GPU sharing with memory isolation, hierarchical queue management, and time-based fair-share policies. KAI Scheduler combined capabilities that previously required assembling three or four separate projects.

**Dynamic Resource Allocation**, which reached general availability in Kubernetes 1.34 in late 2025, replaced the simple counter-based device plugin model with a structured claim-and-allocation system. Instead of requesting "two GPUs," a workload can now request "two GPUs with at least 80 gigabytes of memory each, CUDA 12 compatibility, and NVLink connectivity between them." This is a fundamental improvement that makes the scheduler aware of hardware characteristics that previously required manual node labeling and affinity rules.

## The CNCF AI Conformance Signal

In November 2025, the Cloud Native Computing Foundation launched the Certified Kubernetes AI Conformance Program. This was not a marketing exercise. It was a formal recognition that AI workloads require specific platform capabilities beyond standard Kubernetes conformance — GPU scheduling, device management, networking for distributed training, and gang scheduling support.

The initial certification included conformance tests for GPU device discovery and allocation, multi-GPU scheduling across pod boundaries, and basic distributed training support. Broadcom, Google, Microsoft, Oracle, and Red Hat were among the first certified platforms. The v1.0 certification established a baseline. The v2.0 roadmap, expected in 2026, expands to include inference-specific patterns, enhanced monitoring requirements, and stricter security controls for GPU workloads.

The conformance program matters because it signals portability. Before conformance certification, every managed Kubernetes service implemented GPU support slightly differently — different device plugin versions, different driver management approaches, different scheduling behaviors. A workload tested on GKE might behave differently on EKS because of device plugin version differences or driver lifecycle management. The conformance program establishes a minimum capability set that any certified platform must provide, reducing the vendor-specific debugging that platform teams face when running multi-cloud or migrating between providers.

## Proof of Scale

The most compelling evidence that Kubernetes works for AI at scale comes from production deployments that push it to its limits. OpenAI has publicly discussed orchestrating over 25,000 GPUs on Kubernetes, achieving 97 percent utilization despite hardware failures occurring on average every two and a half hours across the fleet. Their infrastructure team built custom operators that detect GPU failures, checkpoint training state, reallocate workloads, and resume training automatically — all within the Kubernetes operator framework.

OpenAI's experience also reveals the limits. Vanilla Kubernetes begins to degrade around 5,000 nodes without significant modifications. Beyond that threshold, the team needed hierarchical cluster federation, custom scheduling algorithms, and GPU-aware autoscaling that tracks each individual H100 as a distinct asset. This is not vanilla Kubernetes. This is Kubernetes plus a substantial layer of custom extensions — which is precisely the point. The API model and extensibility architecture made those extensions possible without forking the project.

CoreWeave, which operates one of the largest GPU-focused Kubernetes platforms globally, runs Kueue as its primary scheduler for AI training workloads across tens of thousands of GPUs for some of the most demanding AI labs in the world. Their deployment validates that the ecosystem tools — Kueue, the GPU Operator, DRA — work at production scale, not just in test clusters.

## The Remaining Gaps in 2026

Kubernetes is far more capable for AI workloads in 2026 than it was even two years ago, but significant gaps remain that your platform team should understand.

**Checkpointing and migration** are still primitive. When a GPU fails during a training job, the standard recovery path is to restart the job from the last checkpoint. But Kubernetes has no native understanding of AI checkpoints — when to take them, where to store them, how to resume from them. Every organization builds its own checkpointing logic. The emerging cooperative preemption feature in Kueue, which allows workloads that implement checkpointing to be gracefully preempted and resumed, is a step toward solving this, but it requires application-level integration that is far from standardized.

**Multi-cluster scheduling** for AI workloads remains immature. Kueue's MultiKueue feature, which dispatches jobs across multiple clusters, is a 2026 development priority but not yet production-stable for all scenarios. Organizations that need to federate GPU resources across clusters still rely on custom tooling or manual coordination.

**Cost visibility** at the GPU-workload level is better than it was in 2024 but still requires significant integration work. Kubernetes native resource tracking knows how many GPUs a pod requested but not how efficiently those GPUs were used. Correlating GPU utilization from DCGM metrics with scheduling data from Kueue quotas and cloud billing from your provider remains a custom data pipeline that every organization builds differently.

**Networking for large-scale distributed training** — specifically RDMA over Converged Ethernet and InfiniBand integration — works but requires specialized configuration that sits outside the standard Kubernetes networking model. CNI plugins for high-performance networking exist, but they add operational complexity that your team must understand and maintain.

## The Ecosystem Is the Product

The core insight for any platform team in 2026 is this: you are not adopting Kubernetes. You are adopting Kubernetes plus the GPU Operator plus DRA plus Kueue or Volcano or KAI Scheduler plus custom operators for your specific workload patterns. Vanilla Kubernetes is a necessary foundation but an insufficient platform for AI. The real product is the ecosystem stack, and your job as a platform engineer is to assemble, configure, and operate that stack for your organization's specific scale, hardware mix, and workload profile.

This means your team needs expertise not just in Kubernetes fundamentals — pods, services, deployments, RBAC — but in the AI-specific extension layer. Your engineers need to understand how DRA ResourceClaims interact with the scheduler, how Kueue ClusterQueues enforce fair-share policies, how the GPU Operator manages driver lifecycle across heterogeneous hardware, and how topology-aware scheduling prevents the eight-times performance penalty of placing a distributed training job on GPUs that cannot communicate efficiently.

The organizations that struggle are the ones that treat Kubernetes as a solved problem and the AI layer as an afterthought. The organizations that succeed are the ones that recognize the AI orchestration stack as a first-class engineering discipline — staffed, maintained, and iterated with the same rigor as the models themselves.

---

The next subchapter examines the lowest layer of the GPU-Kubernetes integration — how Kubernetes discovers, allocates, and manages individual GPU devices, from the original device plugin model through the Dynamic Resource Allocation system that became generally available in Kubernetes 1.34.
