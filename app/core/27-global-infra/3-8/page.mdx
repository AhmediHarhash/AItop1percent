# 27.24 — Topology-Aware Scheduling: Placement as a First-Class Performance Lever

Where a workload runs matters as much as what hardware it runs on. Two teams can use the same GPU model, the same model architecture, the same batch size, and the same optimizer configuration — and one team finishes training in twelve hours while the other takes twenty. The difference is not the code. It is the placement. The fast team's eight GPUs share an NVLink domain on a single server, communicating at 900 gigabytes per second. The slow team's eight GPUs are spread across two servers, communicating at 25 to 100 gigabytes per second over the network. Same hardware, same code, 40 percent less throughput — because the scheduler did not consider topology when placing the workload.

In traditional Kubernetes scheduling, placement is a resource-matching problem. The scheduler finds a node with enough CPU, memory, and GPUs to satisfy the pod's request, and it places the pod there. It does not care whether those GPUs are connected by NVLink or PCIe. It does not care whether the two nodes in a distributed training job share a top-of-rack switch or sit in different network pods separated by three switch hops. For CPU workloads, this indifference is fine — the performance difference between "same rack" and "different rack" for a web service is a few microseconds of network latency, irrelevant at application timescales. For GPU training workloads, this indifference is a performance catastrophe.

## The Topology Hierarchy

GPU infrastructure has a physical hierarchy that determines communication bandwidth between any two points. Understanding this hierarchy is essential for making placement decisions that do not waste half your compute budget on communication overhead.

At the narrowest level, within a single GPU die, cores communicate through on-chip interconnects at terabytes per second. This is not schedulable — it is fixed by the chip design — but it sets the ceiling for what intra-GPU parallelism can achieve.

The first schedulable boundary is the **NVLink domain**. Within a server, GPUs are connected by NVLink — NVIDIA's high-bandwidth interconnect that provides 600 to 900 gigabytes per second of bidirectional bandwidth in current-generation hardware. An eight-GPU DGX H100 server connects all eight GPUs through NVLink in a fully connected mesh. Any GPU can communicate with any other GPU at full NVLink bandwidth. This domain is the natural unit of parallel training for most model sizes in 2026.

The second boundary is **PCIe**. Some server configurations have GPUs that share a PCIe switch but are not connected by NVLink. The PCIe bus provides 32 to 64 gigabytes per second per link — roughly ten times slower than NVLink. GPUs on the same server but outside the NVLink domain communicate through PCIe, and the performance difference is immediately measurable. A data-parallel training step that takes 50 milliseconds over NVLink takes 200 to 400 milliseconds over PCIe, because the gradient synchronization that happens at every training step is bandwidth-bound.

The third boundary is **NUMA** — Non-Uniform Memory Access. In multi-socket servers, each CPU socket has its own memory controller and its own PCIe root complex. GPUs attached to one CPU socket can access memory controlled by the other socket, but at higher latency and lower bandwidth. A GPU attached to socket zero that needs to DMA data from system memory on socket one pays a NUMA penalty that adds 20 to 40 percent latency to host-to-device transfers. This matters for workloads that frequently move data between CPU and GPU — data loading pipelines, preprocessing stages, and inference workloads that marshal inputs and outputs through host memory.

Beyond the server boundary, the hierarchy continues through the **network fabric**. Two servers on the same top-of-rack switch communicate at 100 to 400 gigabits per second with sub-microsecond latency. Two servers in the same pod but different racks add a switch hop, maintaining bandwidth but adding latency. Two servers in different pods add another hop, and in some data center designs, the cross-pod bandwidth is oversubscribed — less total bandwidth is available than the sum of individual server uplinks. For GPU-to-GPU communication using RDMA over InfiniBand or RoCE, each hop adds measurable latency and reduces effective throughput for the collective operations that distributed training depends on.

## Why Topology Matters for AI Specifically

The reason topology dominates AI workload performance — and barely registers for web services — is the communication pattern of distributed training.

Distributed data-parallel training divides a batch across multiple GPUs. Each GPU computes gradients on its portion of the batch, and then all GPUs must synchronize their gradients before the next step. This synchronization — the **all-reduce** operation — requires every GPU to send data to every other GPU. The total data volume per step equals the model size (the number of parameters times the bytes per parameter). For a seven-billion-parameter model in 16-bit precision, that is 14 gigabytes per all-reduce step. For a 70-billion-parameter model, it is 140 gigabytes.

Over NVLink at 900 gigabytes per second, a 14-gigabyte all-reduce completes in roughly 16 milliseconds. Over the network at 50 gigabytes per second effective throughput, the same operation takes 280 milliseconds. If the forward and backward pass take 200 milliseconds, the NVLink version spends 7 percent of step time on communication. The network version spends 58 percent. The NVLink-connected GPUs are compute-bound — they spend most of their time doing useful math. The network-connected GPUs are communication-bound — they spend most of their time waiting for data transfer.

This 3x to 10x bandwidth difference at each topology boundary is why placement is a first-class performance variable. A training job that the scheduler places within a single NVLink domain might achieve 85 percent hardware utilization. The same job spread across two servers might achieve 45 percent. The same job spread across servers in different racks might achieve 25 percent. You are paying the same price per GPU-hour in all three cases.

## Kubernetes Topology Labels and Constraints

Kubernetes provides mechanisms for topology-aware placement, but they require explicit configuration. The scheduler does not understand GPU topology by default.

The foundation is **node labels** that encode topology information. The NVIDIA GPU Feature Discovery plugin, deployed as part of the GPU Operator, automatically labels nodes with GPU metadata — the GPU model, driver version, memory size, and MIG configuration. For topology information, you need additional labels that encode the node's position in the physical hierarchy: which rack it sits in, which network pod, which availability zone. These labels are typically set during node provisioning by the infrastructure automation and follow a convention like topology.kubernetes.io/zone for the zone, topology.kubernetes.io/rack for the rack, and nvidia.com/gpu.topology for the NVLink domain configuration.

**TopologySpreadConstraints** tell the scheduler how to distribute pods relative to these labels. A constraint can say "spread pods evenly across racks" or "place all pods in the same zone." For training workloads, the constraint you typically want is the opposite of what Kubernetes defaults suggest. Kubernetes defaults prefer spreading workloads for high availability. Training workloads prefer concentrating workloads for high bandwidth. A TopologySpreadConstraint with maxSkew of zero and whenUnsatisfiable set to DoNotSchedule, keyed on the rack label, ensures that all pods in a training job land on nodes in the same rack — maximizing the chance that GPU-to-GPU communication stays within a single top-of-rack switch.

The limitation of TopologySpreadConstraints is that they operate at the pod level, not the job level. If a training job has four pods and the constraint says "same rack," the scheduler tries to place all four pods in the same rack but may split them across racks if no single rack has enough capacity. Gang scheduling through Kueue or Volcano addresses this by treating the job's topology requirements as an all-or-nothing admission decision.

## Volcano and Advanced Topology-Aware Scheduling

**Volcano**, the CNCF batch scheduling project, provides the most sophisticated topology-aware scheduling for GPU workloads in Kubernetes as of 2026. Its network topology-aware scheduling plugin, currently in alpha, constructs a hierarchical model of the cluster's physical topology from node labels. It understands that two nodes in the same rack are "closer" than two nodes in different racks, and that two nodes in the same pod are "closer" than two nodes in different pods.

When a training job is submitted, Volcano evaluates placement options against this hierarchy. It prefers placing all pods on the same server (NVLink bandwidth). If that is not possible, it prefers the same rack (single-hop networking). If that is not possible, it prefers the same network pod (minimum cross-pod traffic). The scheduling decision is not just "can these pods fit" but "where should these pods go to minimize communication overhead."

Volcano uses a concept of **hypernodes** — logical groupings that represent physical topology levels. The bottommost hypernode level is the individual server. The next level up is the top-of-rack group. Above that is the pod or spine group. Each level has a label — kubernetes.io/hostname at the server level, volcano.sh/tor at the rack level — and Volcano's scheduler walks the hierarchy from bottom to top, trying to pack all pods into the tightest topology group that has sufficient capacity.

NVIDIA's own **KAI Scheduler**, open-sourced in early 2025, takes a similar approach with deeper integration into NVIDIA's hardware discovery stack. KAI uses the detailed GPU topology information exposed by NVIDIA's device plugin — not just which node has GPUs, but how those GPUs are interconnected within the node — to make scheduling decisions that respect NVLink domain boundaries. For servers with partial NVLink connectivity (some GPUs connected by NVLink, others only by PCIe), KAI ensures that a four-GPU training job receives four NVLink-connected GPUs rather than two NVLink-connected and two PCIe-connected.

## Performance Impact: The Numbers

The performance difference between topology-aware and topology-ignorant scheduling is not subtle. It is the difference between a cost-effective training pipeline and one that wastes half its budget on communication overhead.

Benchmarks across multiple organizations running distributed training on H100 clusters in 2025 consistently showed 40 to 60 percent training throughput improvement when topology-aware scheduling replaced default Kubernetes scheduling. A 70-billion-parameter model fine-tuning job on 32 GPUs completed in 8 hours with topology-aware placement (all GPUs within two servers in the same rack) versus 14 hours with default placement (GPUs scattered across four racks). The compute cost was identical — 32 GPU-hours times 14 hours versus 32 GPU-hours times 8 hours. The topology-aware version cost 43 percent less in wall-clock GPU-hours because it spent less time waiting for communication.

For inference workloads, the impact is smaller but still meaningful. A large language model serving with tensor parallelism across four GPUs shows 15 to 25 percent latency improvement when those GPUs share an NVLink domain versus communicating over PCIe. At the tail — the 99th percentile latency that determines user experience — the improvement can reach 40 percent because NVLink communication has more consistent latency than PCIe, which is susceptible to bus contention from other devices on the same PCIe root complex.

The smallest model sizes — those that fit on a single GPU — see no topology benefit because there is no inter-GPU communication. Topology-aware scheduling becomes important at two GPUs and critical at eight or more. If your workload mix is entirely single-GPU inference, topology is irrelevant. If you run any multi-GPU training or multi-GPU inference, topology is a performance lever you cannot afford to ignore.

## The Scheduling Flexibility Tradeoff

Topology-aware scheduling improves performance but reduces scheduling flexibility. This tradeoff is real and must be managed explicitly.

A strict topology constraint — "all eight pods must be on the same server" — means the job can only run on servers with eight free GPUs. If no server has eight free GPUs, the job waits, even though the cluster has 50 free GPUs scattered across many servers. The constraint turns a schedulable job into an unschedulable one. Relax the constraint to "all pods in the same rack," and the job becomes schedulable across any combination of servers in a single rack. Relax further to "any placement," and the job is schedulable anywhere — but runs at half the throughput.

The practical approach is tiered constraints. Production training jobs that run for days get strict topology constraints because the cumulative performance impact justifies waiting for optimal placement. Development training jobs that run for hours get relaxed constraints — same rack preferred, cross-rack tolerated — because fast iteration matters more than peak throughput. Inference workloads get per-server constraints for the tensor-parallel portion (GPUs serving a single model must share NVLink) but relaxed constraints across replicas (different model replicas can be on different servers).

Kueue's admission logic interacts with topology constraints in an important way. When Kueue evaluates whether to admit a job, it considers not just total GPU availability but schedulable availability given the topology constraint. A job requesting eight GPUs with a same-server constraint is admitted only if a server with eight free GPUs exists. This prevents the job from entering the scheduling loop and consuming scheduler cycles on a placement it can never achieve. The job waits in the queue, visible to the team and the capacity dashboard, until the constraint can be satisfied.

The scheduling flexibility tradeoff is ultimately a conversation between the platform team and the workload teams. Strict constraints deliver better performance. Relaxed constraints deliver faster scheduling. The right balance depends on the workload's time sensitivity, the cluster's current fragmentation, and whether the team would rather wait two hours for a fast training run or start immediately on a slow one. Making this choice explicit — rather than letting the scheduler make it implicitly through default placement — is what separates a mature AI platform from one that wastes compute through ignorance of the physical world underneath the Kubernetes abstraction.

---

Topology, preemption, fragmentation, and multi-tenancy all operate within the boundaries of a single cluster or a fleet of clusters. But the workloads running on those clusters generate and consume vast quantities of data that must flow across the network and persist on storage systems designed for AI-scale throughput. The next chapter examines the networking and storage infrastructure that makes high-performance GPU compute possible — and the failure modes that emerge when data movement becomes the bottleneck.
