# Chapter 7 — Edge AI and Distributed Inference

The center of gravity for AI inference is shifting. By 2026, industry analysts project that the majority of inference workloads run outside traditional cloud data centers -- on phones, laptops, IoT gateways, industrial sensors, and vehicles. Small language models in the one-to-nine billion parameter range now deliver useful quality on consumer hardware, and on-device inference eliminates the round-trip latency and data-transfer costs that make cloud-only architectures impractical for real-time, privacy-sensitive, or connectivity-constrained use cases. But edge deployment trades one set of problems for another. You no longer worry about cloud scaling -- you worry about pushing model updates to tens of thousands of heterogeneous devices, maintaining reliability when connectivity is intermittent or absent, observing a fleet you do not physically control, and enforcing privacy constraints that require data to never leave the device. The infrastructure patterns that work in a centralized data center break completely at the edge.

This chapter covers the full stack of edge AI infrastructure: the model formats and serving runtimes that make on-device inference viable, the hybrid routing architectures that split workloads between cloud and edge, the update and rollback pipelines that keep a distributed fleet consistent, and the observability and privacy patterns that let you operate at the edge without flying blind.

---

- **7.1** — The Edge Inference Shift
- **7.2** — Small Language Models for Edge Deployment
- **7.3** — Edge Serving Frameworks: OpenVINO, LiteRT, and On-Device Runtimes
- **7.4** — Hybrid Cloud-Edge Routing
- **7.5** — Model Updates at the Edge
- **7.6** — Edge Reliability Without Guaranteed Connectivity
- **7.7** — Device Fleet Observability and OTA Rollback
- **7.8** — Privacy and Data Sovereignty at the Edge

---

*The cloud will always matter for training and for the heaviest inference workloads. But the edge is where your users actually are -- and the next chapter shifts from running models to training them, where the infrastructure requirements change from milliseconds of latency to days of sustained GPU compute.*
