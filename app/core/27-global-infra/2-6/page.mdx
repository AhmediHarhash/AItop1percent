# 27.14 — Autoscaling for AI: When Horizontal Pod Autoscaling Is Not Enough

In late 2024, a computer vision startup serving a retail analytics platform ran their entire inference fleet behind Kubernetes' standard Horizontal Pod Autoscaler and Cluster Autoscaler. The system worked well for months. Traffic was steady, scaling events were rare, and the team moved on to other priorities. Then a major retail client launched a holiday campaign that tripled request volume in under ten minutes. The HPA detected the traffic increase and requested additional pods. Kubernetes attempted to schedule those pods, found no GPU nodes with available capacity, and triggered the Cluster Autoscaler. The cloud provider began provisioning new GPU instances. Eight minutes later, the instances were ready. The NVIDIA driver loaded. The container image — eleven gigabytes, containing the model weights — pulled from the registry. The model loaded into GPU memory. Thirteen minutes after the traffic spike began, the new capacity started serving requests. By that time, the spike had already peaked and was subsiding. During those thirteen minutes, the existing pods were overwhelmed. Latency climbed from 120 milliseconds to over four seconds. The client's dashboard froze. Their operations team opened an urgent incident ticket. The startup had plenty of infrastructure automation. What they lacked was infrastructure that could respond at the speed their traffic demanded.

This failure is not a story about a team that ignored autoscaling. It is a story about a team that used the autoscaling tools Kubernetes provides and discovered that those tools were designed for a fundamentally different class of workload.

## Why Standard Autoscaling Fails for AI

Kubernetes' Horizontal Pod Autoscaler was built for stateless web services. The assumptions baked into its design reflect that origin. A new pod starts in seconds because the container image is small and the application initializes by binding to a network socket. Scaling is horizontal — add more identical replicas behind a load balancer and throughput increases linearly. The scaling metric is typically CPU utilization, which correlates directly with the workload's bottleneck. Nodes are fungible — any node in the cluster can run any pod, so the Cluster Autoscaler just needs to add generic compute capacity.

AI inference workloads violate every one of these assumptions. Pod startup is measured in minutes, not seconds. Container images are enormous because they include model weights, tokenizers, and GPU runtime libraries — five to thirty gigabytes is typical for a large language model deployment. Even after the image is pulled, the model must be loaded into GPU memory, a process that takes thirty seconds to five minutes depending on model size, GPU memory bandwidth, and whether the weights are being loaded from local storage, network storage, or a remote registry. Only after the model is fully loaded can the pod begin serving requests.

Node startup compounds the delay. GPU instances from cloud providers take two to fifteen minutes to provision, depending on instance type and availability zone capacity. Once the instance is running, the NVIDIA driver or AMD ROCm stack must initialize, the device plugin must register the GPU with Kubernetes, and any node-level health checks must pass before the scheduler considers the node ready. The combined cold-start time — from autoscale trigger to first served request — ranges from five to twenty minutes. For a web service, this might be tolerable. For real-time inference serving user-facing applications, it is a failure.

The metrics problem is equally fundamental. The HPA's default scaling metrics — CPU utilization and memory consumption — do not reflect the bottleneck in GPU inference workloads. Your pods might show thirty percent CPU utilization while the GPU is saturated at ninety-five percent, or the inference queue might be growing while both CPU and GPU utilization appear moderate because the bottleneck is memory bandwidth rather than compute throughput. Scaling decisions based on CPU utilization will either scale too late, too aggressively, or not at all.

## Custom Metrics: Scaling on What Actually Matters

The first fix is to give the HPA metrics that actually reflect GPU workload pressure. This requires a custom metrics pipeline — typically Prometheus collecting GPU metrics from the NVIDIA DCGM exporter, a Prometheus adapter exposing those metrics through the Kubernetes custom metrics API, and HPA configurations that reference those custom metrics instead of CPU utilization.

The metrics that drive intelligent autoscaling for AI inference are specific. **Inference queue depth** — the number of requests waiting to be processed — is the most responsive leading indicator of capacity pressure. When queue depth exceeds your target, you need more replicas. When it drops to zero consistently, you might have too many. **GPU utilization percentage** reflects compute saturation. **GPU memory utilization** reveals whether you are approaching out-of-memory conditions. **Tokens per second** for language models and **images per second** for vision models measure actual throughput against your targets. **Request latency at the ninety-fifth and ninety-ninth percentile** captures the user experience directly.

Configure the HPA to scale on a combination of these metrics, using the most aggressive signal. If queue depth says scale up but GPU utilization says do not, queue depth wins — it means requests are arriving faster than the current replicas can process them, even if the GPU is not saturated, possibly because the bottleneck is elsewhere in the pipeline. Set the scaling target conservatively. An average queue depth target of two to five requests per replica, rather than zero, prevents the HPA from oscillating between scaling up and scaling down on every traffic fluctuation.

## Karpenter and KEDA: Smarter Provisioning

The Cluster Autoscaler reacts to unschedulable pods — pods that the scheduler cannot place because no node has the required resources. It then provisions a new node from a predefined node group. This reactive model adds minutes of delay on top of the HPA's already-delayed scaling decision. **Karpenter**, originally developed by AWS and now an open-source project used across providers, takes a fundamentally different approach. Instead of reacting to unschedulable pods after the HPA has already decided to scale, Karpenter evaluates pod requirements directly and provisions the optimal instance type within seconds of the scheduling request.

Karpenter's advantage for GPU workloads is its instance-type flexibility. Rather than scaling a fixed node group that uses a single instance type, Karpenter can choose from a range of instance types based on the pod's resource requirements, the current spot pricing, and the availability of each type. If the pod needs a single GPU with sixteen gigabytes of memory, Karpenter might provision an L4 instance, an A10G instance, or a T4 instance — whichever is cheapest and available right now. This flexibility reduces provisioning time because it avoids the "specific instance type unavailable" delays that plague fixed node groups. On AWS, Karpenter provisions nodes in fifty to seventy seconds on average, compared to three to five minutes for the Cluster Autoscaler.

**KEDA** — the Kubernetes Event-Driven Autoscaler — complements Karpenter by handling the pod-level scaling that the HPA does poorly. KEDA scales based on external event sources: message queue depth, HTTP request rate, Prometheus metrics, or custom signals from your inference gateway. Unlike the HPA, which polls metrics every fifteen to thirty seconds and scales gradually, KEDA can react to events with sub-second granularity and scale aggressively — adding five or ten replicas in a single scaling event rather than the HPA's conservative one-at-a-time approach. For inference workloads with spiky traffic patterns, KEDA's event-driven model matches the workload profile far better than the HPA's periodic polling.

The combination of KEDA for pod scaling and Karpenter for node provisioning reduces the total scaling response time from minutes to under two minutes for many configurations — still not instant, but fast enough to handle most traffic spikes without severe degradation.

## Warm Pools: Pre-Loaded Capacity for Instant Response

Even with Karpenter and KEDA, a two-minute scaling response is too slow for some use cases. Real-time inference for voice applications, financial trading systems, or interactive AI assistants needs sub-second scaling. The only way to achieve this is to have capacity already running and ready to accept traffic before the demand arrives.

**Warm pools** are pre-provisioned GPU nodes with models already loaded into GPU memory, sitting idle and waiting for traffic. When demand increases, the load balancer routes requests to the warm replicas immediately — there is no image pull, no model loading, no driver initialization. The response is instant because the capacity was prepared in advance.

The obvious objection is cost. An idle GPU node running an H100 at four dollars per hour costs nearly three thousand dollars per month and serves zero requests until traffic arrives. For most organizations, maintaining a large warm pool is prohibitively expensive. The practice is to right-size the warm pool to handle expected traffic bursts for the first two to three minutes — long enough for Karpenter or the Cluster Autoscaler to provision additional capacity that handles the sustained increase. A warm pool of two or three replicas covers the gap between burst arrival and autoscaler response, preventing the latency spike that damages user experience.

GKE's Buffer API, introduced in 2025, formalizes this pattern at the cloud provider level. You declare a buffer of ready-to-use nodes, and GKE maintains them in a provisioned-but-unscheduled state, available for near-instant use when pods need scheduling. This reduces the cost of warm pools by allowing the nodes to be provisioned without running idle workloads, while still eliminating the provisioning delay when demand arrives.

## Predictive Autoscaling: Scaling Before Demand Arrives

The most sophisticated teams do not wait for traffic to arrive before scaling. They predict it. Many inference workloads have highly predictable traffic patterns. A customer support chatbot peaks at nine in the morning and four in the afternoon every weekday. A content moderation service spikes when the marketing team sends email campaigns. A retail analytics platform surges during known promotional events. These patterns are visible in historical traffic data, and a system that learns from that history can pre-scale capacity before the demand arrives.

**Predictive autoscaling** uses historical traffic patterns — typically analyzed through time-series models running on the trailing seven to thirty days of traffic data — to forecast demand for the next one to four hours. When the forecast predicts a traffic increase, the system scales up capacity in advance, so that when the demand actually arrives, the pods are running and the models are loaded. The user experiences no cold-start latency because the capacity was never cold.

Implementation varies by maturity. The simplest version is a cron-based scaler that increases replica counts at known peak times and decreases them during known troughs. More sophisticated versions use KEDA's cron and metrics scalers together — cron handles the predictable daily pattern, and event-driven scaling handles the unpredictable deviations. The most advanced implementations use purpose-built forecasting that feeds predictions into Kubernetes scheduling decisions, a pattern that cloud providers increasingly support through their managed Kubernetes offerings.

The risk of predictive scaling is over-provisioning when the forecast is wrong. A holiday that shifts traffic patterns, a product launch that changes the daily curve, a competitor's outage that sends unexpected traffic your way — any of these can make historical patterns unreliable. The mitigation is layered autoscaling: predictive scaling provides the baseline capacity, event-driven scaling through KEDA adjusts for real-time deviations, and the HPA serves as the fallback when both miss. Each layer compensates for the others' blind spots.

## Scale-to-Zero and Fast Restart

Not every inference endpoint needs to run continuously. Development and staging environments, internal tools used only during business hours, and low-traffic endpoints that serve a few hundred requests per day are all candidates for **scale-to-zero** — completely removing all replicas when traffic stops and restarting them when the first request arrives.

Scale-to-zero for GPU workloads requires solving the cold-start problem differently than for CPU workloads. A CPU web service can start in seconds, making the first request's wait time barely noticeable. A GPU inference service needs minutes to restart, making the first request's wait time unacceptable if the user is waiting. The solutions include storing model weights on high-speed local NVMe storage instead of pulling them from a remote registry, pre-baking model weights into the container image so they are cached on the node's image layer cache, and using warm-start containers that keep the model in system RAM and transfer it to GPU memory on wake-up — a process that takes seconds rather than minutes.

KEDA natively supports scale-to-zero, and when paired with Karpenter's ability to deprovision empty nodes, the entire stack scales to zero — no pods, no nodes, no cost. When the first request arrives, KEDA triggers a scale-up from zero to one replica, Karpenter provisions a node if needed, and the pod starts loading the model. The total restart time depends on your cold-start optimizations but typically ranges from thirty seconds to three minutes with pre-cached images and local model storage.

For development environments, this pattern alone can reduce GPU costs by sixty to eighty percent. A team of ten data scientists, each with a personal inference endpoint that runs eight hours a day and scales to zero overnight and on weekends, uses roughly a third of the GPU-hours compared to always-on endpoints. At three to four dollars per hour per GPU, the savings are substantial.

## Vertical Pod Autoscaling for GPU Memory

Not every scaling challenge is horizontal. Sometimes you do not need more replicas — you need the existing replicas to use GPU memory more efficiently. **Vertical Pod Autoscaling** for GPU resources is less mature than horizontal scaling, but it addresses a real problem: models change size as they are updated, quantized, or replaced, and the GPU memory allocated to their pods should change accordingly.

A model that was initially deployed with a thirty-gigabyte GPU memory request might be quantized to sixteen-bit or eight-bit precision, reducing its actual memory footprint to fifteen gigabytes. Without vertical scaling, the pod continues reserving thirty gigabytes, preventing other workloads from using the remaining capacity. With VPA for GPU resources, the system monitors actual GPU memory usage over time and recommends — or automatically applies — adjusted resource requests that match real consumption.

VPA for GPU is still emerging in 2026. The standard Kubernetes VPA does not natively understand GPU memory as a resource. Custom implementations use DCGM metrics to track per-pod GPU memory consumption, feed those metrics into a recommendation engine, and surface resize recommendations through the platform's deployment tooling. Fully automated GPU VPA — where the system resizes pod GPU allocations without human intervention — requires careful safeguards because reducing GPU memory below the model's actual requirement causes an out-of-memory crash, and the consequences are more severe than the equivalent CPU or system memory resize.

---

Autoscaling determines how your cluster responds to changing demand. But even perfect autoscaling cannot save you if the fundamental software layer between Kubernetes and the GPU hardware is broken. The next subchapter examines the CUDA compatibility matrix — the driver, toolkit, and runtime version dependencies that determine whether your workloads can actually run on the hardware you have provisioned.