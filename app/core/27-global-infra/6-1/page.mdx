# 27.41 — The Multi-Cloud Reality: Why Single-Provider Lock-In Is a Strategic Risk

Multi-cloud is not a best practice. It is a survival strategy. The conventional pitch for multi-cloud focuses on negotiation leverage — play AWS against Azure, extract better pricing, keep your options open. That framing is superficial and, for AI workloads specifically, dangerously incomplete. The real case for multi-cloud is not about getting a better rate on your next committed-use agreement. It is about the fact that a single cloud provider going through a GPU capacity crunch, a pricing restructure, or a regional outage can take your entire AI capability offline with no alternative, no fallback, and no negotiating position. In 2025, multiple organizations discovered this firsthand when their sole cloud provider ran out of H100 capacity in their region for months at a time, right when they needed to scale inference for a product launch. They could not move. They could not scale. They waited.

The instinct to go single-cloud is understandable. One provider means one billing console, one set of APIs, one identity system, one support contract, one way of doing things. Engineers learn one platform deeply. Operations become muscle memory. But for AI infrastructure, the things that make single-cloud convenient are the same things that make it fragile.

## GPU Availability Is Not Uniform Across Providers

The most immediate risk of single-provider lock-in for AI is GPU supply. Unlike commodity compute — where all three major clouds offer essentially identical x86 virtual machines in abundance — GPU capacity is scarce, unevenly distributed, and volatile. A provider that had surplus H100 capacity in January may have a six-month waitlist by July because a single large customer signed a multi-billion-dollar reservation. AWS, Azure, and Google Cloud each have different GPU procurement pipelines, different relationships with NVIDIA, and different regional rollout strategies. When you lock into one provider, you lock into one supply chain.

The problem extends beyond NVIDIA GPUs. Each major provider has invested heavily in proprietary accelerators that only work on their platform. AWS has Trainium and Inferentia, custom chips designed for training and inference respectively. Trainium3, announced in late 2025, delivers 2.5 petaflops of FP8 compute per chip and costs significantly less than equivalent H100 instances for supported model architectures. Google Cloud has Tensor Processing Units, now in their seventh generation with Ironwood, offering over 4,600 teraflops per chip at FP8 and among the best performance-per-watt ratios in the industry. Microsoft has developed Maia for internal Azure and Copilot workloads, though external availability remains limited. Each of these accelerators offers genuine price-performance advantages for specific workloads. But every hour of engineering you invest in optimizing for Trainium's NeuronX compiler or Google TPU's JAX integration is an hour that ties you deeper to that provider.

This is the core tension. Proprietary accelerators are often 30 to 50 percent cheaper than NVIDIA GPUs for the workloads they support well. Ignoring them means leaving real money on the table. Embracing them means building dependencies that take months to unwind.

## The Lock-In Vectors Beyond Compute

GPU availability is the most visible lock-in risk, but it is not the only one. Single-provider commitment creates dependencies across at least five layers of your AI stack, each of which increases switching cost independently.

The first is managed Kubernetes. Amazon EKS, Google GKE, and Azure AKS all run Kubernetes, but they differ in networking models, identity integration, GPU device plugins, node autoscaler behavior, storage class implementations, and ingress controllers. A Kubernetes manifest that works on GKE may fail on EKS because of different assumptions about service account binding, different behavior in how node pools handle GPU taints, or different default container network interfaces. Teams that use platform-specific features — GKE Autopilot, EKS Fargate profiles, AKS virtual nodes — face a rewrite, not a migration.

The second is storage. Model weights, training datasets, evaluation results, and checkpoint files live in object storage. AWS S3, Google Cloud Storage, and Azure Blob Storage have functionally similar APIs but subtly different consistency models, different lifecycle management capabilities, and wildly different egress pricing. Moving a 10-terabyte model repository from S3 to Google Cloud Storage incurs hundreds of dollars in egress fees and takes hours even with high-bandwidth connections. More importantly, every script, pipeline, and automation that references S3 URIs or uses the S3 SDK must be updated.

The third is networking. VPC peering, private endpoints for GPU clusters, load balancer configurations for inference traffic, and DNS-based routing all work differently on each provider. A networking architecture designed for AWS — using VPC endpoints, PrivateLink, and Application Load Balancers — does not translate to Azure's Private Link and Application Gateway without redesign.

The fourth is identity and access management. IAM policies on AWS, Google Cloud IAM roles, and Azure RBAC follow different permission models, different inheritance hierarchies, and different audit logging formats. Your security team's compliance controls, audit processes, and access review workflows are all provider-specific.

The fifth is model serving infrastructure. If you deploy inference on SageMaker, Vertex AI, or Azure AI Studio, your model packaging format, endpoint configuration, autoscaling rules, and monitoring integration are all specific to that managed service. These are not thin wrappers — they are deep integrations that touch model loading, hardware allocation, traffic routing, and billing.

Each vector independently increases the cost and time of moving to another provider. Together, they can make a migration take six months to a year for a production AI system of moderate complexity.

## The Two-Body Problem

There is a named tension at the heart of every multi-cloud strategy that teams must acknowledge honestly: **The Two-Body Problem**. You need portability to avoid lock-in. You need deep provider integration for performance. These two goals pull in opposite directions, and no architecture resolves them fully.

Deep integration means using SageMaker's optimized serving containers, Google TPU's distributed training primitives, Azure's tightly-coupled Cosmos DB and AI Search stack. These integrations exist because the providers spent years optimizing the interaction between their services. An inference endpoint running on SageMaker with a custom NVIDIA Triton container, backed by S3 model storage with VPC-internal networking, will outperform the same model running on a generic Kubernetes cluster with a compatibility shim for object storage. The gap is typically 10 to 20 percent in latency and 15 to 25 percent in throughput. That gap matters when you are serving millions of requests per day.

Portability means writing against abstractions, not implementations. Kubernetes instead of managed node pools. S3-compatible APIs instead of provider-native storage SDKs. Generic inference serving instead of managed model endpoints. Every abstraction introduces overhead — an extra network hop, a compatibility translation, a feature that is available on one provider but becomes a no-op on another. The portability layer does not make providers identical. It makes them similar enough that switching is months of work instead of years.

The Two-Body Problem has no clean solution. It has managed tradeoffs. The organizations that navigate it successfully are the ones that make explicit, documented decisions about where to integrate deeply and where to insist on portability. They integrate deeply with the GPU compute layer — using provider-specific instance types, taking advantage of proprietary accelerators when the savings justify it — because this is where performance gains are largest and where switching costs are most manageable (you are swapping hardware, not rewriting business logic). They insist on portability for orchestration, storage interfaces, model packaging, and monitoring — because this is where switching costs are highest and where provider differences create the deepest lock-in.

## When Multi-Cloud Is Worth the Complexity

Multi-cloud is not universally correct. It adds operational overhead, increases the surface area for misconfiguration, requires engineers who understand multiple platforms, and demands abstraction layers that must be built and maintained. For some organizations, the complexity is not justified.

Single-cloud is a defensible strategy when your GPU needs are small — fewer than 50 GPUs — and well-served by one provider's available inventory. When your workloads do not require proprietary accelerators. When you operate in a single regulatory jurisdiction that your provider covers. When your revenue impact from a provider outage is lower than the annual cost of maintaining a second cloud. When your engineering team is small enough that spreading Kubernetes expertise across two platforms thins it beyond usefulness.

Multi-cloud becomes necessary when GPU supply constraints on your primary provider have already delayed a product launch or capacity expansion. When regulatory requirements demand infrastructure in regions where your primary provider has limited presence. When your annual cloud spend exceeds a threshold — typically one to two million dollars — where negotiation leverage from credible alternatives produces savings that exceed the cost of maintaining portability. When your AI system is revenue-critical and a provider outage that lasts more than four hours would cost more than the annual investment in a secondary deployment.

The decision is not ideological. It is arithmetic. Calculate the cost of lock-in — supply risk, outage risk, pricing risk, regulatory risk. Calculate the cost of multi-cloud — engineering time, operational overhead, abstraction layer maintenance, split expertise. When the first number exceeds the second, multi-cloud is correct. When it does not, single-cloud with deliberate portability investments at the orchestration and storage layers is the pragmatic choice.

## Building Toward Multi-Cloud Without Building Multi-Cloud

Even if you decide that full multi-cloud is premature, you can make decisions today that preserve your ability to go multi-cloud later without paying the full complexity cost now. This is not fence-sitting. It is engineering prudence.

Run your inference workloads on Kubernetes from day one, even if you use a single provider's managed Kubernetes service. The Kubernetes API is the closest thing to a universal orchestration interface that exists. The CNCF's Certified Kubernetes AI Conformance Program, launched in November 2025, is establishing standards for GPU management, gang scheduling, and networking that make AI workloads more portable across certified providers. A workload that runs on EKS today can run on GKE or AKS with configuration changes rather than architecture changes, provided you avoid deep dependencies on provider-specific extensions.

Use S3-compatible APIs for model storage. Every major cloud and most private storage systems support the S3 API as a compatibility layer. MinIO provides an S3-compatible interface for on-premises storage. Writing your model loading code against S3 APIs rather than provider-native SDKs means your inference containers work on any platform without code changes.

Containerize your model serving with open-source inference engines — vLLM, NVIDIA Triton, or TensorRT-LLM. These run on any NVIDIA GPU regardless of which cloud it sits in. The container image is your portable artifact. The runtime environment is the variable.

These three investments — Kubernetes orchestration, S3-compatible storage, containerized inference — cost almost nothing if adopted from the start. They cost months if adopted after your stack is built on provider-specific managed services. The team that uses SageMaker endpoints on day one and discovers they need to move to Google Cloud a year later faces a migration project. The team that uses vLLM on EKS on day one faces a redeployment.

## The Strategic Calculation

Lock-in risk is not theoretical for AI infrastructure. It is material. GPU supply crunches happen every year. Cloud pricing changes arrive quarterly. Provider outages affect every major cloud multiple times per year. The EU AI Act's compliance requirements create geographic constraints that may not align with your current provider's regional footprint. Any of these can force a migration under pressure, and migration under pressure is the most expensive kind.

The goal is not to eliminate lock-in. That is impossible — every technical choice creates some dependency. The goal is to choose your lock-in deliberately. Lock into the layers where deep integration delivers measurable performance gains and where the dependency is on commodity hardware (NVIDIA GPUs) rather than proprietary services. Stay portable in the layers where switching costs compound — orchestration, storage, identity, monitoring — and where provider differences create the deepest traps.

Multi-cloud is not about running on every cloud simultaneously. It is about being able to run on a different cloud within weeks instead of months, and about having the negotiating position and the operational flexibility that comes from that capability.

---

The Two-Body Problem is real, but it is manageable when you choose the right abstraction boundaries. The next subchapter examines the specific abstraction layers that enable model portability across providers — where they pay for themselves, where they cost more than they save, and how to build them without sacrificing the performance your users depend on.
