# Chapter 1 — The AI Infrastructure Problem: Why Platform Engineering Changes Everything

Every assumption your infrastructure team built its expertise on is wrong for AI workloads. CPUs are fungible, schedulable, and overcommittable — GPUs are none of those things. Web services are stateless, horizontally scalable, and tolerant of cold starts — inference endpoints are latency-sensitive, model-loading beasts that punish you for every unnecessary restart. Batch processing is embarrassingly parallel and failure-tolerant — distributed training is tightly coupled, communication-bound, and a single node failure kills the entire job. The teams that approach AI infrastructure as "just more cloud" follow a predictable arc: they provision general-purpose instances, discover GPU scheduling doesn't work like CPU scheduling, burn through six figures in idle capacity, and finally realize that the platform itself — not just the workloads — needs to be redesigned from the ground up.

This is not a scaling problem. It is a category problem. AI workloads demand a different relationship between compute, memory, networking, and storage than anything your traditional infrastructure was designed to handle. A single training job can saturate your entire network fabric. A single inference deployment can consume more GPU memory than your previous largest application consumed in total RAM. The power draw of a modern GPU rack exceeds what most data centers allocated per row five years ago. And the pace of hardware change — new GPU architectures every twelve to eighteen months, each with different driver requirements, memory capacities, and interconnect topologies — means your platform is never "done." It is permanently under construction.

This chapter establishes why a dedicated AI platform engineering discipline exists, why it cannot be bolted onto your existing infrastructure team's processes, and what the foundational design decisions are that separate teams burning money from teams building durable AI platforms. Whether you are running ten inference endpoints or ten thousand training jobs, the principles are the same. Get the platform right, and everything above it — models, applications, evaluation pipelines — becomes dramatically easier. Get it wrong, and every team building on top of you will spend half their time fighting infrastructure instead of building product.

---

## What This Chapter Covers

- **1.1** — Why AI infrastructure is not just more compute
- **1.2** — The GPU scheduling problem
- **1.3** — The inference dominance shift
- **1.4** — The power and capacity crisis
- **1.5** — The platform-application split
- **1.6** — The AI infrastructure stack
- **1.7** — Infrastructure maturity levels
- **1.8** — Designing for the infrastructure you will need

---

*The first subchapter begins with the question every infrastructure team gets wrong: why treating GPUs like expensive CPUs is the most costly mistake in AI platform engineering.*
