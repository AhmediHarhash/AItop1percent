# Chapter 2 — Kubernetes for AI Workloads

Kubernetes won the orchestration war for AI, but the victory came with a massive asterisk. The core abstractions that made Kubernetes dominant — pods, deployments, services, horizontal pod autoscalers — were designed for stateless web applications that scale by adding identical replicas behind a load balancer. AI workloads violate nearly every assumption baked into that model. A training job is not a deployment. An inference server is not a web service. A GPU is not a CPU with more cores. The default Kubernetes experience will let you schedule a pod onto a GPU node, and that is roughly where its helpfulness ends. Everything beyond that — device plugin configuration, workload queuing, accelerator sharing, topology-aware placement, driver compatibility — is an engineering discipline unto itself.

The gap between "my pod can see a GPU" and "my platform efficiently manages GPU resources across dozens of teams" is where most organizations stall. You discover that Kubernetes has no native concept of GPU memory. You learn that the default scheduler treats all GPUs as identical, even when your cluster contains three different generations of NVIDIA hardware with wildly different performance characteristics. You find that autoscaling based on CPU utilization is meaningless when your bottleneck is GPU memory, and that scaling from zero requires loading a multi-gigabyte model into VRAM before the first request can be served. Each of these problems has solutions — GPU device plugins, Kueue for workload queuing, Multi-Instance GPU for sharing, custom metrics for autoscaling — but they form a stack of operational complexity that your team must own, understand, and maintain.

This chapter walks through the mechanics of making Kubernetes actually work for AI. Not the marketing version where everything is declarative and self-healing, but the real version where you are debugging CUDA driver mismatches at two in the morning and writing custom scheduling policies because the default bin-packing algorithm wastes forty percent of your GPU capacity.

---

## What This Chapter Covers

- **2.1** — Why Kubernetes won the AI orchestration war
- **2.2** — GPU device management and dynamic resource allocation
- **2.3** — Workload queuing and priority with Kueue and Volcano
- **2.4** — Multi-Instance GPU and time-slicing
- **2.5** — Node pool design for heterogeneous hardware
- **2.6** — Autoscaling for AI workloads
- **2.7** — The CUDA compatibility matrix
- **2.8** — The utilization ceiling

---

*The first subchapter examines how Kubernetes outpaced every alternative orchestrator for AI workloads — and why that dominance created a new class of infrastructure problems that the ecosystem is still solving.*
