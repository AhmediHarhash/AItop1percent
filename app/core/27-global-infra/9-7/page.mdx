# 27.70 â€” On-Call for AI Infrastructure: What Changes When the Platform Has GPUs

The most dangerous pattern in AI infrastructure operations has a name: **the web-era runbook**. It looks like this. A GPU node goes unhealthy at 2 AM. The on-call engineer opens the standard Kubernetes troubleshooting guide. They check pod status, examine node conditions, look for OOMKilled containers, and try the standard remediation sequence: cordon the node, drain the pods, restart the kubelet. The node comes back, the pods reschedule, and the engineer marks the incident resolved. Two hours later, the same node goes unhealthy again. The engineer repeats the procedure. This cycle continues until the morning shift arrives and someone with GPU-specific experience looks at the actual hardware metrics and discovers that the node has been reporting correctable ECC memory errors at fifty times the normal rate for the past week -- a clear precursor to uncorrectable errors that will crash any workload running on those GPUs. The node should have been removed from the pool days ago. Instead, it was recycled back into service repeatedly because the runbook did not include GPU-specific diagnostic steps and the on-call engineer did not know to look for hardware-layer signals that have no equivalent in CPU infrastructure.

GPU infrastructure does not fail the way CPU infrastructure fails. The failure modes are different, the recovery procedures are different, and the blast radius of a single failure can be orders of magnitude larger. On-call for AI infrastructure requires a different approach to monitoring, different runbooks, different escalation paths, and a different set of skills on the rotation.

## GPU Failure Modes That Do Not Exist in CPU Infrastructure

CPU servers fail in ways that Kubernetes was designed to handle. A node goes down, the pods on that node get rescheduled to other nodes, and the service continues with slightly less capacity. The failure is binary -- the node works or it does not -- and the recovery is automated.

GPU servers fail in ways that Kubernetes was not designed to handle, because GPUs introduce an entire layer of hardware complexity that sits between the operating system and the workload.

**ECC memory errors** are the most common GPU-specific failure. GPU memory uses Error Correcting Code to detect and correct bit flips. Single-bit errors are correctable -- the hardware fixes them transparently and logs the event. But correctable errors accumulate. An accelerating rate of correctable errors is a leading indicator that the memory module is degrading and will eventually produce a double-bit uncorrectable error that crashes any workload using that memory region. Meta's public disclosure about training Llama 3 on a cluster of 16,384 H100 GPUs documented 419 unexpected failures over 54 days, with GPU hardware failures including memory errors accounting for roughly 30 percent of all interruptions. At that scale, the annualized GPU failure rate approached 9 percent. Your cluster is smaller, but the failure modes are identical.

**Thermal events** occur when GPU temperatures exceed safe operating limits. GPUs under sustained training workloads generate enormous heat -- 700 watts per GPU for H100s, higher for newer Blackwell hardware. When cooling is insufficient, the GPU throttles its clock speed to reduce heat generation. This throttling is silent from Kubernetes' perspective. The pod is still running, the health check still passes, but the workload runs at 60 to 70 percent of its expected speed. For inference, this means latency increases that degrade user experience without triggering a pod restart. For training, this means one node in a distributed job runs slower than all others, creating the straggler effect that drags down the entire cluster's throughput.

**NVLink failures** affect multi-GPU communication within a node. NVLink connects the GPUs inside a single server at bandwidths of hundreds of gigabytes per second. When an NVLink connection degrades, communication between the affected GPUs slows dramatically. For training workloads that rely on tensor parallelism or pipeline parallelism across GPUs within a node, a degraded NVLink connection can reduce throughput by 50 percent or more without producing any error visible to the application layer. Research published in 2025 documented cases where a single NVLink error on one GPU cascaded into MPI failures that brought down jobs spanning four separate nodes.

**Driver crashes and CUDA errors** are the GPU equivalent of a kernel panic. The NVIDIA kernel driver or the CUDA runtime encounters an unrecoverable error and the GPU becomes unusable until the driver is reset. Unlike a CPU kernel panic that reboots the entire node, a GPU driver crash can leave the node itself healthy while rendering all GPUs on that node unusable. Kubernetes sees the node as ready, but the device plugin reports zero available GPUs. Pods requesting GPUs cannot schedule, but the node is not cordoned because it is technically healthy.

**Model weight corruption during loading** is a failure mode that exists only in AI workloads. When a model server starts, it downloads weights from object storage and loads them into GPU memory. If the download is interrupted, if the storage client retries a failed read and gets a different version, or if a GPU memory error occurs during the transfer from CPU memory to GPU memory, the loaded weights may be silently corrupted. The model serves responses, but the responses are subtly or dramatically wrong, and no infrastructure-level health check catches this because the pod is running and responding to requests.

## The Blast Radius Difference

When a CPU node with 32 pods fails, the blast radius is 32 pods. Each pod is independent. Kubernetes reschedules them across the remaining healthy nodes within minutes. Service capacity drops temporarily but recovers automatically.

When a GPU node in a distributed training job fails, the blast radius is the entire training job -- potentially spanning 8, 16, 64, or more nodes. Distributed training uses synchronous gradient aggregation, meaning every GPU must complete its computation and exchange results before any GPU can proceed to the next step. If one GPU becomes unavailable, every other GPU in the job halts. For a 64-GPU training job running on 8 nodes, a single GPU failure on one node stops all 64 GPUs. If the job does not have checkpointing configured, or if the last checkpoint was hours ago, all training progress since the last checkpoint is lost.

This blast radius asymmetry fundamentally changes the urgency calculus. A CPU pod failure at 3 AM can often wait until morning because the self-healing mechanisms handle it. A GPU node failure affecting a multi-day training job at 3 AM is a genuine emergency because every hour of delayed response is an hour of lost training progress multiplied across dozens of GPUs. The financial impact of that delay can easily reach thousands of dollars per hour for large training runs.

For inference workloads, the blast radius is smaller but still different from CPU infrastructure. A single GPU node failure removes one or more model replicas from the serving pool. If the model is large and requires an entire node per replica, losing one node means losing a significant fraction of total serving capacity. Autoscaling can replace the lost capacity, but GPU node provisioning takes five to fifteen minutes depending on the cloud provider, during which the remaining replicas absorb the full traffic load. If the remaining capacity is insufficient, requests queue, latency increases, and timeouts propagate to upstream services.

## Runbooks for GPU-Specific Failures

Every GPU failure mode needs its own runbook entry. The runbook must cover four things: how to identify the failure, how to verify it is the actual root cause, how to mitigate the immediate impact, and when to escalate to GPU-specific support.

The **ECC error runbook** starts with checking the DCGM metrics for the affected node. Look at the correctable and uncorrectable ECC error counts. If uncorrectable errors are present, the GPU is unreliable and must be removed from service immediately -- cordon the node and drain all GPU workloads. If only correctable errors are present but the rate exceeds your threshold (industry practice is to flag nodes with more than 10 correctable errors per hour as at-risk), cordon the node, drain workloads during the next maintenance window, and open a hardware replacement ticket. Do not simply reboot and return to service. ECC errors indicate physical memory degradation that does not improve with restarts.

The **thermal throttling runbook** requires checking GPU temperature and clock speed through nvidia-smi or DCGM metrics. If temperatures exceed 80 degrees Celsius under sustained load and clock speeds have dropped below the base frequency, the GPU is thermally throttled. Check the cooling infrastructure: is the ambient temperature in the data center within specifications? Are fans operating at expected RPM? For liquid-cooled GPU systems, which are increasingly common in 2026, check coolant flow rate and temperature. Thermal issues may require physical intervention that the on-call engineer cannot perform remotely, so the escalation path leads to data center operations or the cloud provider's hardware team.

The **NVLink failure runbook** starts with checking NVLink status through nvidia-smi's NVLink topology output. Degraded or inactive links appear as failed connections between specific GPU pairs. Any NVLink failure on a node that is part of a multi-GPU training job requires immediate cordon and drain because the training job cannot maintain full throughput with degraded intra-node communication. For inference workloads that use only a single GPU per model replica, NVLink failures may not affect current workloads but should still trigger node replacement because the failure can cascade.

The **training job failure runbook** is different from all the infrastructure runbooks because it starts with the application state, not the infrastructure state. When a training job fails, the first question is: what is the checkpoint status? Is the most recent checkpoint intact and verified? How many training steps were completed since the last checkpoint? Can the job resume from the checkpoint on the remaining healthy nodes, or does it need the full original GPU count? The answer to these questions determines whether the response is "resume the job" -- which may take minutes -- or "wait for replacement capacity and restart from checkpoint" -- which may take hours.

## Monitoring and Alerting for GPU Infrastructure

Standard Kubernetes monitoring misses the signals that matter for GPU failures because those signals come from the hardware layer below Kubernetes' awareness.

Deploy **NVIDIA DCGM** (Data Center GPU Manager) or equivalent hardware monitoring on every GPU node. DCGM exposes per-GPU metrics for temperature, power draw, clock speed, memory utilization, compute utilization, ECC error counts, NVLink bandwidth, and PCIe throughput. Export these metrics to your monitoring system -- Prometheus is the standard -- and build alerting rules at thresholds lower than you would use for traditional infrastructure.

Alert on GPU temperature above 82 degrees Celsius. Alert on correctable ECC errors exceeding 5 per hour. Alert on NVLink bandwidth dropping below 80 percent of the expected rate. Alert on GPU clock speed falling below 90 percent of the base frequency, which indicates throttling. Alert on GPU memory utilization above 95 percent, which leaves insufficient headroom for allocation spikes and can lead to CUDA out-of-memory errors. Each alert should include the node name, the specific GPU index, the current metric value, and the threshold that was crossed, so the on-call engineer can determine severity without needing to log into the node.

Build a dedicated GPU health dashboard that shows all GPU nodes in the cluster with a color-coded status: green for healthy, yellow for degraded (correctable errors above baseline, temperature elevated, clock throttled), and red for failed (uncorrectable errors, driver crash, GPU not detected). The on-call engineer should be able to see the cluster's GPU health status in a single glance, which is not possible with standard Kubernetes dashboards that focus on pod and node status.

## Escalation Paths for GPU Issues

On-call for CPU infrastructure has a simple escalation path: the on-call engineer tries standard remediation, and if that fails, escalates to a senior engineer on the team. GPU infrastructure adds external escalation paths that do not exist for standard Kubernetes operations.

**Cloud provider GPU support** is the first external escalation for hardware issues in cloud environments. GPU instance types have dedicated support tiers that are separate from general compute support. When you open a GPU-related support case, specify the GPU model, the error codes from DCGM or nvidia-smi, and the instance ID. Cloud providers can inspect host-level hardware diagnostics that are invisible to the tenant and can replace faulty GPU instances or migrate workloads to healthy hardware.

**NVIDIA Enterprise Support** provides a second escalation path for driver and CUDA-level issues. Driver crashes, CUDA errors that do not match known failure patterns, and NCCL communication failures during distributed training are best diagnosed with vendor expertise. NVIDIA's support team has access to internal diagnostic tools and firmware-level telemetry that can identify root causes invisible from the application layer.

These external escalation paths mean the on-call engineer needs to know not just what to do, but who to call and what information they will need. The runbook should include the exact steps for filing a cloud provider GPU support ticket and the NVIDIA support case, including which metrics and log files to attach.

## On-Call Rotation Design for AI Infrastructure

AI infrastructure on-call cannot be staffed from the general Kubernetes on-call pool. The failure modes require GPU-specific knowledge that most Kubernetes engineers do not have. An engineer who is excellent at debugging pod scheduling, network policies, and Ingress configuration may have never seen an ECC error, may not know that NVLink exists, and may not understand why a training job cannot simply reschedule its failed pod the way a stateless web service does.

This creates a staffing challenge. The pool of engineers with both Kubernetes expertise and GPU infrastructure expertise is smaller than the pool with either skill alone. Smaller rotation pools mean more frequent on-call shifts, which leads to burnout. The sustainable approach is to invest in cross-training: take your strongest Kubernetes engineers and train them on GPU hardware, DCGM monitoring, CUDA debugging, and distributed training fundamentals. Build the GPU-specific runbooks detailed enough that an engineer with solid Kubernetes skills but limited GPU experience can handle the most common failure modes by following the documented procedures. Reserve the truly novel failures -- the ones the runbook does not cover -- for escalation to the GPU specialists on the team.

On-call shifts for AI infrastructure should be backed by a secondary on-call specifically for training workload incidents. The primary on-call handles infrastructure -- node failures, scheduling issues, autoscaling, networking. The secondary on-call handles training-specific incidents -- checkpoint corruption, training divergence, NCCL timeouts, distributed training job failures. This separation prevents the primary on-call from being overwhelmed by training incidents they are not equipped to handle, and it ensures that training incidents get attention from engineers who understand the application-layer implications.

## Post-Incident Reviews for AI Infrastructure

AI infrastructure incidents often reveal failure modes that have no traditional analogue, and the post-incident review process must adapt to capture these novel patterns.

Standard post-incident reviews focus on timeline, root cause, impact, and remediation. For GPU infrastructure incidents, add two additional sections. First, **hardware-layer analysis**: what was the physical hardware state before, during, and after the incident? Were there precursor signals in the DCGM metrics that should have triggered earlier action? Is the hardware failure a one-off or part of a trend across the fleet? Second, **workload impact analysis**: for training incidents, how much training progress was lost? Were checkpoints intact? Could the job have resumed faster with a different checkpointing strategy? For inference incidents, how did the loss of GPU capacity affect latency, throughput, and error rates? Did the autoscaler respond within the platform SLO?

Track GPU-specific incident categories separately from general infrastructure incidents. Over time, this data reveals patterns: which GPU models have higher failure rates, which node configurations are most prone to thermal issues, which failure modes are most common and which are most expensive. This data drives fleet management decisions -- replacing aging GPU nodes proactively rather than waiting for failures, configuring more aggressive monitoring thresholds for known-problematic hardware generations, and informing procurement decisions about which GPU hardware to buy next.

---

Runbooks, SLOs, and on-call rotations keep the platform running day to day. But AI infrastructure also operates under regulatory and compliance requirements that demand specific technical controls at the platform layer. The next subchapter examines compliance infrastructure -- the audit logging, data residency enforcement, and access controls that the platform must provide for the organization to meet its regulatory obligations.
