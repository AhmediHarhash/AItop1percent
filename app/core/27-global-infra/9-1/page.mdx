# 27.64 â€” AI Infrastructure Observability: Beyond Traditional Monitoring

In late 2025, an insurance company running production AI inference on Kubernetes experienced a slow, puzzling degradation. Their monitoring stack -- Prometheus for metrics, Grafana for dashboards, PagerDuty for alerts -- showed everything green. CPU utilization hovered at 40 percent. Memory usage was stable. All pods reported healthy. Uptime was 99.97 percent. Yet customer support tickets about poor response quality tripled over two weeks. Summaries were truncated. Complex policy questions received shallow answers. The team spent nine days investigating application code before a GPU engineer, brought in as a last resort, identified the root cause in four hours: GPU memory fragmentation was silently forcing the inference engine to truncate context windows. Long prompts that should have been processed in full were being cut to fit available contiguous memory blocks. The model was not failing -- it was operating on incomplete inputs. Every traditional metric said "healthy." Every user said "broken."

This incident illustrates a fundamental gap that most teams discover only after it costs them. Traditional infrastructure monitoring was built for CPU-based workloads where the relationship between resource utilization and application behavior is well understood. High CPU means the application is working hard. High memory means data is being processed. Network throughput correlates with request volume. These relationships break down completely for AI workloads running on GPUs, where the most important signals live in dimensions that traditional monitoring does not even measure.

## What Traditional Monitoring Misses

Standard Kubernetes monitoring collects node-level CPU, memory, disk, and network metrics. Container-level metrics track the same signals per pod. Health checks verify that processes are running and responding to probes. This is necessary but nowhere near sufficient for AI inference infrastructure.

GPU utilization is the first blind spot. A GPU is not a CPU. It has its own memory hierarchy, its own compute units called Streaming Multiprocessors, its own interconnects, and its own thermal characteristics. The `nvidia-smi` command that engineers run manually reports a single "GPU-Util" percentage, but this number tells you only whether the GPU is doing something -- not whether it is doing something efficiently. A GPU reporting 90 percent utilization might be running a poorly optimized kernel that uses only a fraction of its available compute units. A GPU reporting 50 percent utilization might be perfectly efficient for a memory-bandwidth-bound workload. Without GPU-specific metrics -- SM occupancy, memory bandwidth utilization, thermal state, error rates -- you are flying blind.

Model-level metrics are the second blind spot. Traditional monitoring does not know what a token is. It does not track time to first token, tokens per second, KV cache hit rates, batch utilization percentages, or request queue depths at the model server level. These metrics are the pulse of an AI inference service. A model server can report healthy to Kubernetes while its KV cache is thrashing, its batch utilization is at 12 percent, and its time to first token has crept from 80 milliseconds to 1.4 seconds. None of this appears in standard dashboards.

The third blind spot is the relationship between infrastructure state and output quality. In a traditional web service, if the server is up and responding within its latency budget, the output is almost certainly correct. AI inference breaks this assumption. A model can respond quickly with confident, fluent text that is completely wrong because it was working with a truncated context, a corrupted KV cache, or a quantized weight that drifted during a thermal throttling event. Observability for AI must bridge the gap between "the system is running" and "the system is producing good outputs."

## The Three Pillars for AI Infrastructure

The observability model for AI workloads extends the traditional three pillars -- metrics, logs, and traces -- with AI-specific dimensions in each pillar.

**Metrics** for AI infrastructure operate at three levels. Infrastructure metrics cover GPU utilization, GPU memory allocation, GPU temperature, power draw, network throughput, storage IOPS, and scheduling latency. Model metrics cover tokens per second, time to first token, time per output token, KV cache utilization and hit rate, batch size distribution, request queue depth, and model-specific error rates. Business metrics cover requests per second, successful completions, timeouts, user-facing latency percentiles, and quality scores from any online evaluation system. The infrastructure metrics tell you whether hardware is healthy. The model metrics tell you whether the model server is performing well. The business metrics tell you whether users are getting value. You need all three layers because problems can originate at any level and propagate upward silently.

**Logs** for AI infrastructure must capture more than error messages. Every inference request should generate a structured log entry that includes the request ID, the model version serving the request, the input token count, the output token count, the time to first token, the total latency, the GPU that served the request, the batch position, and any truncation or fallback that occurred. Training workloads should log step-level progress, gradient norms, learning rate, loss values, and any hardware events like ECC corrections or NVLink errors. These logs become the forensic record when something goes wrong -- and in AI infrastructure, something always goes wrong eventually.

**Traces** for AI infrastructure follow a request across every service it touches. An inference request in a production AI system does not simply hit a model and return. It passes through a load balancer, a request router, possibly a model selector or routing layer, a retrieval system if RAG is involved, a vector database, the inference engine itself, post-processing or guardrail services, and response formatting. Each hop adds latency and introduces a failure point. Without distributed tracing, diagnosing why a request took eight seconds instead of 200 milliseconds requires guessing. With tracing, you can see exactly which component consumed the time.

## GPU-Specific Observability

GPU monitoring deserves dedicated treatment because GPU failure modes are unlike anything in CPU-based infrastructure. A GPU does not simply work or not work. It degrades along multiple dimensions simultaneously, and the degradation is often invisible to standard health checks.

NVIDIA's Data Center GPU Manager, known as DCGM, is the foundational layer for GPU observability. DCGM runs as a system daemon on each GPU node, communicates with the GPU driver, and exposes hundreds of metrics about GPU state. The **DCGM Exporter** packages these metrics in Prometheus format, making them scrapeable by standard Prometheus installations. In a Kubernetes environment, the DCGM Exporter typically deploys as a DaemonSet -- one pod per GPU node -- and is included automatically when you deploy the NVIDIA GPU Operator.

The metrics that DCGM exposes fall into several categories. Utilization metrics include SM Active percentage, memory utilization percentage, and encoder and decoder utilization for video workloads. Memory metrics include framebuffer memory used, framebuffer memory free, and memory bandwidth utilization. Thermal metrics include GPU temperature, memory temperature, and thermal violation flags. Error metrics include single-bit and double-bit ECC error counts, and XID error codes that indicate specific hardware or driver faults. Interconnect metrics include NVLink bandwidth utilization and PCIe replay error counts. Power metrics include current power draw, power limit, and throttling reasons.

The gap between having these metrics available and actually using them for operational decisions is where most teams stall. Collecting GPU metrics into Prometheus is the easy part. Knowing which metrics to alert on, what thresholds to set, and what the metrics mean in the context of AI workloads -- that is the hard part, and it is the subject of the next subchapter.

## Model Server Observability

The inference engine -- vLLM, TensorRT-LLM, NVIDIA Triton, or whatever serving framework your team uses -- exposes its own layer of metrics that sit between the GPU hardware metrics and the business-level metrics. These model server metrics are critical because they translate hardware state into application behavior.

**Time to first token** measures how long a user waits before seeing any response. For interactive applications, this is the most important latency metric. TTFT includes prompt processing time (the prefill phase), scheduling time if the request waited in a queue, and any time spent loading or swapping model weights. Monitor TTFT at p50, p95, and p99 percentiles. A healthy p50 with a degraded p99 usually indicates queuing or memory pressure affecting a subset of requests.

**Tokens per second** measures throughput -- how fast the model generates output. This metric depends on batch size, model architecture, quantization level, and GPU capability. Track it per GPU and per model deployment. A sudden drop in tokens per second with stable request volume often indicates thermal throttling, memory pressure, or a degraded GPU.

**KV cache utilization** tracks how much of the allocated key-value cache memory is in use. When KV cache utilization hits 100 percent, the inference engine must evict entries or reject requests. Rising KV cache utilization with stable request volume means average context lengths are growing -- perhaps because a new feature sends longer prompts, or because multi-turn conversations are accumulating history. This metric predicts capacity problems before they become user-visible.

**Batch utilization** measures how effectively the inference engine fills its processing batches. A batch utilization of 80 percent means the GPU is processing near its optimal throughput. A batch utilization of 15 percent means the GPU is doing short bursts of work with idle time between them -- you are paying for GPU capacity you are not using. Low batch utilization at low traffic volume is expected. Low batch utilization at high traffic volume indicates a scheduling or routing problem.

## Building the Observability Stack

The standard observability stack for AI infrastructure in 2026 layers several components. NVIDIA DCGM Exporter provides GPU hardware metrics. The inference engine's built-in metrics endpoint provides model server metrics. Application-level instrumentation provides business metrics. OpenTelemetry provides the collection and export framework for traces and structured logs. Prometheus provides metric storage and alerting. Grafana provides visualization with dedicated dashboards for each layer.

The operational key is building dashboards that correlate across layers. A GPU temperature dashboard alone does not help. A GPU temperature dashboard linked to the same GPU's SM utilization, the model server's tokens-per-second on that GPU, and the business-level latency percentiles for requests served by that GPU -- that dashboard tells a story. When temperature rises, does throughput drop? When throughput drops, does user latency increase? These correlations are where operational insight lives.

Do not underestimate the volume of data. A cluster with 100 GPUs, each exposing 50 DCGM metrics at 10-second intervals, generates 30,000 metric data points per minute from GPU hardware alone. Add model server metrics, application metrics, structured logs, and distributed traces, and the observability system itself becomes a significant infrastructure component. Plan for metric retention policies, log rotation, trace sampling, and storage costs from the beginning.

## The Observability Gap Is the Operations Gap

The insurance company from the opening story did not have a monitoring problem. They had an observability gap. Their monitoring system answered "is the infrastructure running?" but could not answer "is the infrastructure performing well for AI workloads?" The gap between those two questions is where silent degradation lives. Models that produce worse output because hardware is subtly misbehaving. Inference engines that slow down because a resource they depend on is constrained. Quality that erodes over days or weeks because no metric captures the drift.

Closing this gap requires treating AI observability as a first-class infrastructure concern, not an afterthought bolted onto existing dashboards. It requires GPU-specific metrics, model-specific metrics, and the discipline to correlate them with user-facing outcomes. The tooling exists -- DCGM, OpenTelemetry, Prometheus, Grafana. What most teams lack is the operational knowledge to use it effectively.

---

With the observability foundation in place, the next question is what to do with the GPU metrics you are now collecting. The next subchapter dives deep into the specific GPU metrics that matter for AI operations -- what each metric actually measures, what it tells you about your workload, and what thresholds should trigger action.
