# 27.42 — Abstraction Layers for Model Portability Across Providers

In early 2025, a healthcare analytics company built its entire inference stack on Amazon SageMaker. Model packaging used SageMaker's custom container format. Autoscaling relied on SageMaker's built-in endpoint scaling policies. Model artifacts lived in S3 buckets referenced by SageMaker-specific ARNs. Monitoring flowed through SageMaker Model Monitor into CloudWatch dashboards. The system worked well for eighteen months. Then two things happened simultaneously. AWS experienced a sustained GPU capacity shortage in their primary region that delayed a critical capacity expansion by four months. And a competitor launched a nearly identical product running on Google Cloud's Vertex AI with 40 percent lower inference costs, enabled by committed-use TPU pricing that AWS could not match. The company's leadership decided to evaluate Google Cloud as a secondary provider. The infrastructure team's estimate for migration: six months of engineering work and a near-complete rewrite of the serving stack. Every SageMaker-specific integration — model packaging, autoscaling, monitoring, A/B testing, endpoint routing — had to be rebuilt for a different platform. They had not built a serving stack. They had built a SageMaker integration, and SageMaker does not run on Google Cloud.

That story plays out in variations across the industry. The lesson is not that managed services are bad. The lesson is that the boundary between "using a platform" and "being used by a platform" is invisible until you try to cross it. Abstraction layers exist to make that boundary visible and keep it in a place you chose deliberately.

## The Abstraction Tax

Every abstraction layer has a cost, and pretending otherwise leads to architectures that are portable on paper and slow in production. This cost is **the abstraction tax** — the latency, complexity, and maintenance burden that portability adds to every request.

The tax shows up in three forms. The first is latency. A direct call from your inference server to a provider-native storage API is one network hop. A call through an S3-compatibility shim that translates between a generic interface and a provider-specific backend adds translation time, potentially an extra network hop, and occasionally surprising corner cases where the compatibility layer's behavior diverges from the native API. For model loading — where you transfer tens of gigabytes from storage to GPU memory — this overhead is negligible. For real-time feature retrieval during inference, where every millisecond matters, even small per-request overhead compounds across thousands of calls.

The second is engineering complexity. An abstraction layer is code you write, test, and maintain. It has bugs. It has edge cases. When the underlying provider changes behavior, the abstraction must adapt. When a new provider feature offers genuine performance improvement, you must decide whether to surface it through the abstraction (adding complexity) or bypass the abstraction (creating a leak). Every abstraction accrues maintenance cost over its lifetime.

The third is capability ceiling. An abstraction can only expose the features that all underlying providers support. If AWS offers a model caching feature that reduces cold-start latency by 60 percent but Google Cloud has no equivalent, your abstraction layer either ignores this feature (sacrificing performance on AWS) or exposes it as a provider-specific extension (weakening portability). In practice, the abstraction gravitates toward the lowest common denominator of provider capabilities, which means you systematically underuse the most powerful features of every provider you support.

Acknowledging the abstraction tax does not mean avoiding abstraction. It means being strategic about where to pay the tax and where to refuse. The goal is not uniform portability across your entire stack. It is surgical portability at the layers where lock-in creates the most risk.

## Kubernetes as the Orchestration Portability Layer

If there is one technology that has earned its place as the standard portability layer for AI infrastructure, it is Kubernetes. Not because Kubernetes is perfect — its complexity is legendary and its GPU support required years of maturation — but because it provides a common API for container orchestration that runs on every major cloud, on premises, and at the edge. A Kubernetes Deployment, a Service, a ConfigMap, and a PersistentVolumeClaim describe your workload in terms that are meaningful on EKS, GKE, AKS, and bare-metal clusters alike.

The CNCF launched the Certified Kubernetes AI Conformance Program in November 2025 specifically to standardize how AI workloads run across Kubernetes distributions. This program defines a technical baseline for GPU device plugin behavior, network interface expectations for distributed training, and gang scheduling semantics — the features where provider-specific divergence causes the most portability pain. By mid-2026, conformance-certified distributions must pass a validation suite that ensures AI workloads produce consistent behavior regardless of the underlying cloud.

Kubernetes portability is real but not automatic. The Kubernetes API is consistent. Everything around it is not. Node pools on GKE behave differently from managed node groups on EKS. GPU taints and tolerations work the same way in theory but interact with each provider's autoscaler differently. The Container Storage Interface drivers for each provider implement different subsets of the CSI spec. Ingress controllers, load balancers, DNS integration, and certificate management all have provider-specific implementations beneath the Kubernetes abstraction.

The practical approach is to treat Kubernetes as the portable core and accept provider-specific configuration at the edges. Your Deployment specs, your inference server configurations, your health checks, and your resource requests are portable. Your StorageClass definitions, your ingress annotations, your node affinity rules, and your autoscaler configurations are provider-specific and live in separate configuration files that vary per environment. This separation — portable workload definitions with parameterized infrastructure configuration — is what makes Kubernetes-based portability work in practice rather than in slide decks.

## Container Images as Portable Artifacts

The container image is the most underappreciated portability tool in the AI stack. A Docker image containing your model server, your inference engine, your model weights (or a reference to where they live), and your runtime dependencies is a self-contained artifact that runs identically on any container runtime. Build once, push to a registry, pull from any cluster. The same image that runs on an H100 in AWS us-east-1 runs on an H100 in Google Cloud europe-west4 or on a bare-metal server in your own data center.

This portability holds as long as you follow three discipline rules. First, never bake provider-specific SDKs or credentials into the image. Storage access, secrets, and service discovery should be injected through environment variables, mounted volumes, or Kubernetes service accounts — not compiled into the container. Second, pin your CUDA toolkit, cuDNN, and driver versions explicitly. A container image that implicitly depends on whatever NVIDIA driver the host provides will behave differently across providers that run different driver versions. Third, use multi-stage builds to keep the image lean. An inference container does not need the training framework, the data preprocessing tools, or the evaluation harness. A bloated image slows pull times across regions, and on a slow network link during a burst scaling event, image pull time can be the difference between meeting and missing your autoscaling SLA.

For model weights specifically, the portability decision is whether to include weights in the image or load them at startup from external storage. Including weights in the image makes the image large — 15 to 140 gigabytes for a modern LLM — but guarantees that the model is available the instant the container starts. Loading from external storage keeps images small but adds cold-start latency and creates a dependency on storage availability. The standard pattern in 2026 is to keep the inference engine and runtime in the image while loading model weights from S3-compatible storage at startup. This balances image portability with startup flexibility and avoids the nightmare of pushing a 100-gigabyte container image to five registries every time you update model weights.

## Inference Engine Standardization

The inference engine — the software that actually loads model weights into GPU memory, runs the forward pass, and returns results — is where most of the computational work happens. Standardizing on open-source inference engines that run on any NVIDIA GPU is one of the highest-leverage portability decisions you can make.

**vLLM** has emerged as the dominant open-source inference engine for large language models. Its PagedAttention mechanism for KV cache management, continuous batching, and support for a wide range of model architectures make it the default choice for teams that need high-throughput LLM serving without provider lock-in. vLLM runs on any NVIDIA GPU with CUDA support, on AMD GPUs via ROCm, and increasingly on other accelerators. A vLLM deployment on AWS is functionally identical to a vLLM deployment on Google Cloud or on bare metal, because vLLM talks to the GPU through standard CUDA APIs and does not depend on any cloud-specific service.

**NVIDIA Triton Inference Server** provides a broader abstraction. Triton supports multiple inference backends — TensorRT-LLM for optimized LLM inference, ONNX Runtime for cross-framework model support, PyTorch for dynamic models, and vLLM as a backend for LLM workloads. Triton's model repository format, its HTTP and gRPC serving APIs, and its ensemble pipeline feature provide a consistent interface regardless of the underlying hardware or cloud. The Triton container image is available on NVIDIA's NGC registry and runs on any platform with NVIDIA GPU drivers.

**TensorRT-LLM** is NVIDIA's proprietary optimization layer that compiles models into highly optimized execution graphs for specific GPU architectures. TensorRT-LLM delivers the best raw performance on NVIDIA hardware — typically 20 to 40 percent higher throughput than vLLM for the same model on the same GPU — but requires a compilation step that produces GPU-architecture-specific artifacts. A model compiled for H100 must be recompiled for H200 or Blackwell. This compilation is a per-hardware operation, not a per-cloud operation, so TensorRT-LLM is portable across clouds as long as the target GPU architecture is the same.

The inference engine choice is not about picking one engine for everything. It is about avoiding engines that only run on one provider. SageMaker's built-in inference containers, Vertex AI's custom prediction routines, and Azure AI's managed online endpoints all add provider-specific wrapping around the inference engine. That wrapping is what creates lock-in. The engine underneath — vLLM, Triton, TensorRT-LLM — is portable. Keep the portable engine. Replace the provider-specific wrapping with Kubernetes services and standard load balancers.

## The Storage Abstraction: S3 as Lingua Franca

Model weights, adapter files, evaluation datasets, and inference logs all live in object storage. The question is which object storage API your code speaks.

The S3 API has become the de facto standard for object storage interoperability, not because it is technically superior to alternatives, but because virtually every storage system supports it. AWS S3 is the native implementation. Google Cloud Storage offers S3-compatible access through its interoperability API. Azure Blob Storage can be accessed through S3-compatible gateways. MinIO provides a fully S3-compatible object store that runs on premises, in any cloud, or on a laptop. Ceph, the open-source distributed storage system used in many private clouds, exposes an S3-compatible gateway.

Writing your model loading code, your checkpoint management scripts, and your data pipelines against the S3 API — using a library like boto3 with configurable endpoints — means the same code works against any S3-compatible backend. When you migrate from AWS to Google Cloud, you change an endpoint URL and a credential. When you add an on-premises storage tier, you point to a MinIO cluster. The code does not change.

The caveat is performance. Native cloud storage APIs sometimes offer features that the S3 compatibility layer does not fully replicate. Google Cloud Storage's resumable uploads, Azure Blob Storage's page blob semantics, and AWS S3 Express One Zone's single-digit-millisecond latency are all provider-specific capabilities that an S3 compatibility layer may not expose. For bulk model weight transfers — where you are reading tens of gigabytes sequentially — these differences rarely matter. For latency-sensitive metadata operations or high-frequency small-object access patterns, they can be significant.

## What Not to Abstract

The mistake that turns multi-cloud portability from a strategic advantage into an engineering quagmire is trying to abstract everything. Some things should not be abstracted. They should be parameterized.

GPU-specific optimizations are the clearest example. CUDA kernel configurations, Multi-Instance GPU (MIG) partitioning, GPU memory management tuning, and tensor core utilization settings are all hardware-specific. Abstracting them into a generic "GPU configuration" layer that pretends all GPUs are the same defeats the purpose of having specialized hardware. Instead, parameterize them: define a configuration profile per GPU type (H100, H200, Blackwell, TPU v5, Trainium) and select the appropriate profile at deployment time based on the target environment. The inference engine knows it is running on an H100 and applies H100-specific optimizations. If you move to H200, you apply the H200 profile. The abstraction is in the selection mechanism, not in the optimization itself.

Provider-specific networking optimizations — placement groups for low-latency GPU communication, enhanced networking for inter-node bandwidth, RDMA over Converged Ethernet for distributed training — should also be parameterized rather than abstracted. These features deliver genuine performance improvements that a generic networking abstraction would erase.

The rule is simple: abstract the things that are the same across providers (container orchestration, storage APIs, health check interfaces, metric collection). Parameterize the things that are different but configurable (GPU tuning, network optimization, autoscaler behavior). Integrate deeply only where the performance gain is too large to leave on the table and the switching cost is manageable.

## The 80/20 Rule of Portability

The practical truth of multi-cloud portability is that 80 percent of your AI infrastructure stack is the same across providers. Containers run the same. Kubernetes APIs work the same. vLLM serves models the same. Prometheus scrapes metrics the same. S3 APIs read model weights the same. This 80 percent is where abstraction earns its keep — build once, deploy anywhere, change configuration.

The remaining 20 percent is where providers genuinely differ: GPU instance types and pricing, proprietary accelerator support, networking fabric behavior, autoscaler tuning, storage performance characteristics, regional availability. This 20 percent is where you should optimize for the specific provider you are running on in each environment. Do not abstract the 20 percent. Embrace the differences. Write provider-specific configuration for it. Take advantage of whatever each provider does best.

Teams that try to abstract 100 percent end up with a least-common-denominator platform that runs poorly everywhere. Teams that abstract 0 percent end up locked into a single provider. Teams that abstract 80 percent and optimize 20 percent get portability where it matters and performance where it counts.

---

Abstraction layers make it possible to move between cloud providers. But for some organizations, the most important destination is not a different cloud — it is their own hardware. The next subchapter examines when on-premises and private cloud GPU infrastructure makes financial sense, where the breakeven point actually falls, and what the total cost of ownership looks like when you account for everything cloud pricing conveniently hides.
