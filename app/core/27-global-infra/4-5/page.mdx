# 27.29 — Container Image Strategy for AI: Multi-Gigabyte Images, Pre-Pulling, and Registry Architecture

The container image that works perfectly for web services is an anti-pattern for AI workloads. A typical microservice image — a Go binary on Alpine Linux, a Python Flask app with a few pip packages — weighs 100 to 500 megabytes. It pulls in seconds. It stores cheaply. Registries handle thousands of them without breaking a sweat. Then you build your first AI inference image. The base CUDA runtime layer is 4 to 6 gigabytes. Add cuDNN and NCCL: another 1 to 2 gigabytes. Add PyTorch or TensorFlow with GPU support: 3 to 5 gigabytes. Add your application code, configuration, and dependencies: 1 to 3 gigabytes. The total image, before you even consider baking model weights into it, sits at 15 to 25 gigabytes. Some teams, especially those who embed model weights in the image for deployment simplicity, ship images exceeding 100 gigabytes. At this scale, every assumption about container image management — pull time, registry bandwidth, layer caching, node disk space — collapses.

This is not a minor inconvenience. It is an infrastructure-level problem that affects cold-start latency, autoscaling responsiveness, deployment velocity, and cluster cost. Teams that ignore it discover the consequences during their first real traffic spike, when autoscaler adds ten new pods and every one of them spends twelve minutes pulling images before serving a single request.

## The Pull Time Problem

Image pull time is dead time. From the moment a pod is scheduled to a node until the container runtime finishes pulling and extracting the image, the pod cannot serve traffic, cannot process training data, cannot do anything useful. For a 200-megabyte web service image on a node with 10 gigabits per second of network bandwidth, pull time is under two seconds. For a 20-gigabyte AI image on the same network, pull time is at minimum 16 seconds under ideal conditions — and ideal conditions almost never exist.

In practice, the pull time for large AI images is far worse than the theoretical minimum. Container registries throttle concurrent connections. Network bandwidth is shared with other traffic — training data streams, checkpoint uploads, monitoring telemetry. The container runtime must not only download the layers but also decompress and extract them, which adds CPU load and I/O pressure on the node's root disk. A 20-gigabyte compressed image that decompresses to 35 gigabytes requires both the network bandwidth to download and the disk I/O to write 35 gigabytes to the node's filesystem. On nodes where the root disk is a standard SSD rather than NVMe, the decompression stage can take longer than the download.

The compounding problem is concurrency. When an autoscaler adds five nodes simultaneously, all five begin pulling the same 20-gigabyte image from the registry at the same time. That is 100 gigabytes of bandwidth demand hitting a single registry endpoint. If the registry is a managed service like Amazon ECR, Google Artifact Registry, or Azure Container Registry, it may throttle to protect itself. If it is a self-hosted registry, it may simply run out of network or disk I/O bandwidth. Pull times that were tolerable for a single pod become unacceptable when multiplied by the concurrency of a scale-up event.

## Layer Optimization: Share Everything Possible

Container images are built from layers, and the container runtime only downloads layers that are not already present on the node. This layering system is the primary tool for reducing pull times in AI workloads, but only if you design your Dockerfiles to maximize layer sharing.

The principle is simple: put things that change infrequently at the bottom of the Dockerfile (early layers) and things that change frequently at the top (late layers). For AI images, this means the CUDA base layer, cuDNN, and NCCL go first. They change only when you upgrade the CUDA toolkit, which might happen once or twice a year. PyTorch or your inference framework goes next — updated perhaps monthly. Your application code, model configuration, and custom dependencies go last — updated with every deployment.

When you follow this ordering, a deployment that changes only application code requires pulling only the top few layers — perhaps 50 to 200 megabytes — while the 15-gigabyte GPU framework stack remains cached on the node from the previous deployment. A deployment that changes only a configuration file pulls even less. The initial pull of a fresh node still takes minutes, but subsequent deployments to that node take seconds.

The anti-pattern that destroys layer caching is the monolithic build. Teams that combine CUDA installation, dependency installation, and application code into a single Dockerfile RUN command create a single massive layer that must be re-downloaded every time anything changes. A one-line code fix triggers a 15-gigabyte pull because the entire layer is invalidated. Splitting the Dockerfile into granular, logically grouped layers is not premature optimization for AI images. It is a deployment-speed requirement.

Standardization across teams amplifies the benefit. If every AI team in your organization uses the same base image — the same CUDA version, the same PyTorch version, the same OS layer — then those shared base layers are pulled once per node and cached for every team's workloads. A node that serves inference for three different models from three different teams only needs to download the model-specific layers for each, not three copies of the full PyTorch stack. Platform teams that publish official base images and require all AI workloads to inherit from them can reduce aggregate registry bandwidth by 60 to 80 percent compared to organizations where every team builds from scratch.

## Pre-Pulling: Put Images on Nodes Before You Need Them

The fastest image pull is the one that does not happen at all. **Pre-pulling** — downloading images to nodes in advance of when they will be needed — eliminates pull-time latency from the critical path of pod startup.

The standard Kubernetes approach to pre-pulling uses a DaemonSet. You deploy a DaemonSet that references the AI image in its pod spec, and Kubernetes ensures that a pod with that image exists on every node (or every node matching a node selector, such as all GPU nodes). When the DaemonSet pod is created, the kubelet pulls the image. Because the DaemonSet pod does nothing useful — it exists only to trigger the pull — it can use minimal resources: request zero CPU and minimal memory. The image now sits in the node's container image cache, ready for any pod that references it.

The timing of the pre-pull matters. If you deploy a new model version and update the DaemonSet image tag simultaneously with the inference deployment, the DaemonSet and the inference pods compete for pull bandwidth — defeating the purpose. The correct pattern is to update the DaemonSet first, wait for all nodes to confirm the pull is complete, and then roll out the inference deployment. This staging can be automated through a deployment pipeline that monitors DaemonSet rollout status before proceeding to the workload rollout.

A more sophisticated approach uses a dedicated pre-pull controller. Tools like the Kubernetes Image Puller or custom operators watch for model deployment events and proactively pull images to the nodes where the deployment will land, based on scheduling constraints and node affinity rules. The controller knows which nodes have GPUs, which nodes are in the target node pool, and which nodes already have the image cached. It issues pulls only to nodes that need them and reports completion status to the deployment pipeline.

The limitation of pre-pulling is node disk space. If your cluster runs ten different AI models, each with a 20-gigabyte image, the cache requires 200 gigabytes of disk per node. Add previous image versions that have not been garbage-collected, and the requirement grows quickly. Kubernetes garbage-collects unused images when disk pressure exceeds a threshold (controlled by the imageGCHighThresholdPercent and imageGCLowThresholdPercent kubelet flags), but its garbage collector does not understand which images are "important." It may evict a pre-pulled AI image that no pod is currently using, only for a new pod to need it minutes later. Tuning garbage collection thresholds and using separate disks for image storage versus pod ephemeral storage prevents this thrashing.

## Registry Architecture: Local Mirrors and P2P Distribution

When every AI image pull moves 15 to 25 gigabytes, the registry becomes a bandwidth bottleneck. The architectural response is to bring the registry closer to the nodes and distribute the load across peers.

**Local registry mirrors** in each cluster or region cache images from the central registry. When a node pulls an image, it first checks the local mirror. If the image exists, the pull completes at cluster-local network speed — typically 10 to 25 gigabits per second — without touching the internet or the central registry. If the image is not cached, the mirror pulls it from the central registry, caches it, and serves it to the node. Subsequent nodes in the same cluster get the image from the mirror. For a 20-gigabyte image pulled by 50 nodes, the mirror reduces external bandwidth from 1 terabyte (50 nodes times 20 gigabytes each) to 20 gigabytes (one mirror pull). The cost of running a mirror — a stateless caching proxy with sufficient disk and network — is trivial compared to the bandwidth savings.

**Peer-to-peer image distribution** goes further by eliminating even the mirror as a single point of bandwidth. **Spegel**, now embedded in the SUSE K3s and RKE2 Kubernetes distributions, turns every node into a peer registry. When one node pulls an image, other nodes can fetch that image directly from the first node using a distributed hash table based on the Kademlia protocol. There is no central server, no persistent state, no additional infrastructure to manage. Benchmarks show 82 percent improvement in image retrieval time compared to pulling from a central registry, because the bandwidth load is spread across all nodes rather than concentrated on a single endpoint.

**Dragonfly**, a CNCF project, takes a more structured approach to P2P distribution. It uses a scheduler component to coordinate chunk-level distribution of image layers across peers, optimizing for bandwidth and locality. Dragonfly requires running Redis and a database component, which adds operational complexity compared to Spegel's stateless design. The trade-off is more control over distribution policies and better visibility into transfer progress. For large clusters with hundreds of GPU nodes, Dragonfly's coordination layer can be worth the overhead.

## Lazy Pulling: Start Before the Download Finishes

Traditional container image pulling is all-or-nothing. The container runtime downloads every layer, decompresses every layer, and extracts every file before the container starts. For a 20-gigabyte image, this means the pod waits for all 20 gigabytes even if the container's entrypoint only needs a fraction of the files to begin executing.

**Lazy pulling** inverts this model. Instead of downloading the entire image upfront, the container runtime downloads file contents on demand as the application accesses them. The container starts immediately with a virtual filesystem that knows where every file lives but has not yet fetched the data. When the application reads a file, the runtime fetches just that file's data from the registry. Files that are never accessed are never downloaded.

Three technologies enable lazy pulling in Kubernetes as of 2026. **eStargz**, maintained as a containerd sub-project, reformats the image's tar layers into a seekable format that allows fetching individual files without downloading the entire layer. It is backward-compatible with standard OCI registries — an eStargz image can be pulled normally by runtimes that do not support lazy pulling. **SOCI** (Seekable OCI), developed by AWS, takes a different approach by creating an external index artifact that maps file offsets within compressed layers, allowing the runtime to decompress from any checkpoint rather than from the beginning. **Nydus**, a CNCF sandbox project, uses a chunk-based content-addressable filesystem that supports both FUSE and in-kernel EROFS backends for high-performance lazy access.

For AI workloads, lazy pulling is most valuable during the initial boot sequence. An AI inference container may have gigabytes of Python packages, CUDA libraries, and framework code, but only a fraction of those files are needed before the model loading phase begins. Lazy pulling lets the container start and begin loading model weights (from the separate high-performance storage tier) while background fetches fill in the remaining filesystem. In practice, lazy pulling reduces effective container startup time by 50 to 70 percent for AI images, because the critical-path files are a small fraction of the total image size.

## Separating Model Weights from Container Images

The most impactful architectural decision for AI image management is also the simplest: do not put model weights in the container image.

Teams often start by baking model weights into the Docker image for convenience. One image, one artifact, one thing to deploy. But a 70-billion-parameter model adds 140 gigabytes to the image. Every deployment — even a one-line configuration change — requires pushing 140 gigabytes to the registry and pulling it to every node. Image builds take hours. Registry storage costs explode. Layer caching provides no benefit because the weight layer changes with every model update.

The correct pattern is to mount model weights from external storage. The container image contains the runtime environment — CUDA, PyTorch, your serving framework, your application code — and nothing else. Model weights live on the parallel file system or local NVMe cache, mounted as a volume. The pod spec references a PersistentVolumeClaim for the weights and a container image for the runtime. Updating the model means writing new weights to the shared storage and restarting pods (or using live weight reloading if your framework supports it). Updating the code means building a new 15-gigabyte image without any weight data. The two concerns are decoupled, and each can iterate at its own pace.

This separation also enables multi-model serving. A single container image — your inference framework with all dependencies — can serve any model by mounting different weight volumes. A node can cache weights for ten different models on local NVMe while running a single container image version. The image is shared. Only the weights differ. This pattern reduces total disk usage per node from hundreds of gigabytes of duplicate container layers to a single cached image plus the model weights themselves.

---

Getting images and weights onto nodes is only half the challenge. When your AI platform spans multiple regions, the problem scales from cluster-level to planetary: how do you distribute multi-gigabyte artifacts across the globe without creating stampedes that overwhelm your registries and storage systems? The next subchapter addresses global artifact distribution and the cold-start stampede problem that emerges at scale.
