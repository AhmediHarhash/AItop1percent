# 27.43 — On-Premises and Private Cloud: When Self-Hosting Makes Economic Sense

At sufficient scale, owning GPUs is cheaper than renting them. The breakeven point is lower than most teams think. A Lenovo TCO analysis published in early 2026 showed that an eight-GPU H100 system running sustained inference workloads reached cost parity with on-demand cloud pricing in under four months. Even against committed-use cloud pricing with one-year reservations, the breakeven arrived at roughly twelve months. Over a five-year lifecycle, the same analysis estimated savings of over five million dollars per server compared to hourly cloud rates. These are not marginal differences. They are the kind of numbers that turn infrastructure decisions into board-level conversations.

But cost is not the only variable, and the teams that rush into self-hosting because of a compelling spreadsheet often discover that the spreadsheet left out the hardest parts. Power, cooling, networking, staffing, maintenance, depreciation, and the opportunity cost of capital all live in the gap between "the GPU is cheaper" and "the total system is cheaper." This subchapter lays out the full calculation honestly — where self-hosting wins, where cloud always wins, and where the answer depends on factors that no generic analysis can resolve for you.

## The Crossover Point: When Owning Beats Renting

The fundamental economics of cloud computing have always been a tradeoff between flexibility and unit cost. The cloud provider buys hardware in bulk, amortizes it across thousands of customers, adds a margin, and sells compute by the hour. For workloads that are variable, short-lived, or unpredictable, the cloud's per-hour pricing is efficient because you pay only for what you use. For workloads that are sustained, predictable, and high-utilization, you are paying the cloud provider's margin on hardware that you would have kept busy anyway.

GPU workloads for AI inference tend to be sustained and predictable once a product reaches production scale. A customer-facing chatbot that handles thousands of requests per hour does not turn off at night. A fraud detection model scoring every transaction runs around the clock. An embedding pipeline processing document uploads runs whenever users are active, which for a global product is always. These workloads push GPU utilization to 60, 70, 80 percent or higher — levels where the cloud provider's margin becomes a significant fraction of your total cost.

The crossover calculation is straightforward in principle. An NVIDIA H100 SXM GPU costs roughly $25,000 to $35,000 to purchase, depending on configuration and volume. An eight-GPU H100 server — the DGX H100 or equivalent — costs $250,000 to $350,000 fully configured. Cloud pricing for the same eight-GPU H100 instance runs approximately $25 to $55 per hour on-demand from the major providers, or $10 to $20 per hour with one-year committed-use agreements. At $15 per hour committed pricing, you are paying roughly $130,000 per year for that eight-GPU instance. A $300,000 server reaches the raw hardware breakeven against committed cloud pricing in about 28 months — just over two years — before accounting for any additional on-premises costs.

The real breakeven is later than the raw hardware breakeven, because you must add the on-premises costs that cloud pricing bundles. But it is earlier than the five-year amortization schedule that cautious finance teams prefer, because GPU utilization at production scale makes every month of ownership progressively cheaper on a per-compute-hour basis.

## Total Cost of Ownership: The Full Picture

**Total cost of ownership** for an on-premises GPU cluster includes at least eight categories, and teams that forget even one of them produce dangerously optimistic ROI projections.

Hardware is the most visible cost and the easiest to estimate. Server purchase price, network switches, NVMe storage for model weights and checkpoints, rack infrastructure, cabling, and spare components. For a meaningful production cluster — say 64 GPUs across eight servers — hardware costs typically range from $2 million to $3 million.

Power is the second-largest cost and the one that surprises teams most. A single DGX H100 draws 10.2 kilowatts at peak load. Eight servers draw roughly 80 kilowatts before networking and cooling. At the US average commercial electricity rate of around $0.12 per kilowatt-hour, power for 80 kilowatts running continuously costs approximately $84,000 per year. In regions with higher electricity costs — parts of Europe, Japan, urban data centers — power can cost two to three times that amount. Power cost is non-negotiable and scales linearly with hardware.

Cooling is tightly coupled to power. Every watt of electricity consumed by GPUs becomes heat that must be removed. Traditional air cooling handles standard server racks drawing 5 to 10 kilowatts. GPU racks drawing 40 to 80 kilowatts per rack require enhanced cooling — rear-door heat exchangers, in-row cooling units, or direct liquid cooling. Liquid cooling systems add $50,000 to $150,000 in capital cost per rack and reduce power consumption for cooling by 30 to 40 percent compared to air cooling, but they require plumbing infrastructure and specialized maintenance. The cooling cost of a GPU cluster typically adds 30 to 50 percent on top of the raw power cost, depending on the cooling technology and the facility's Power Usage Effectiveness rating.

Networking connects the GPUs to each other and to the outside world. InfiniBand fabric for inter-server GPU communication — essential for distributed training and high-performance inference — costs $15,000 to $30,000 per server in switch ports and cables for HDR InfiniBand at 200 Gbps. NDR InfiniBand at 400 Gbps costs roughly double. Ethernet networking for management, storage, and external traffic adds another $5,000 to $10,000 per server. A 64-GPU cluster with full InfiniBand fabric and Ethernet networking typically requires $200,000 to $400,000 in network infrastructure.

Real estate is the cost of physical space. If you own a data center, the marginal cost of additional rack space may be low. If you are leasing space in a colocation facility, the cost depends on power density and location. High-density colocation in a major US market — the kind that supports 40 to 80 kilowatts per rack — runs $200 to $500 per kilowatt per month. For 80 kilowatts of GPU power, that is $16,000 to $40,000 per month, or $192,000 to $480,000 per year.

Staffing is the cost that separates a theoretical spreadsheet from an operational reality. Running on-premises GPU infrastructure requires dedicated people. At minimum, you need systems administrators who understand GPU hardware, drivers, and firmware. You need network engineers who can manage InfiniBand fabric and diagnose connectivity issues. You need a on-call rotation for hardware failures — GPUs fail, power supplies fail, NVLink connections develop intermittent errors, storage drives degrade. A minimal team for a 64-GPU cluster is three to five infrastructure engineers, depending on whether they also manage other infrastructure. At fully-loaded costs of $180,000 to $250,000 per engineer, staffing adds $540,000 to $1.25 million per year.

Maintenance covers hardware support contracts, spare parts inventory, firmware updates, and the inevitable component replacements. NVIDIA's DGX enterprise support contract costs roughly $90,000 per year per server. Third-party maintenance contracts are cheaper but provide slower response times. Budget 5 to 10 percent of hardware cost annually for maintenance and support, or $100,000 to $300,000 per year for a 64-GPU cluster.

Depreciation is the accounting reality that your $2.5 million hardware investment loses value every year. Standard depreciation schedules for GPU hardware are three to five years. A three-year depreciation schedule writes off $833,000 per year. A five-year schedule writes off $500,000 per year. The choice affects your balance sheet, your tax treatment, and your capital planning, but the underlying reality is the same: the hardware becomes less valuable over time as newer generations offer better performance.

## The GPU Depreciation Curve

GPU depreciation is not merely an accounting exercise. It reflects a real economic reality driven by NVIDIA's aggressive generational improvement cadence. Each new GPU generation — Ampere to Hopper to Blackwell — delivers roughly 2x performance improvement for AI inference workloads. This means that a GPU bought today will be capable of half the work-per-dollar of a GPU available two years from now.

H100 GPUs that sold for $40,000 at peak scarcity in 2023 trade in the $25,000 to $30,000 range in early 2026, a decline of 25 to 38 percent. Cloud rental rates for H100 instances have dropped even more sharply — from $8 to $10 per GPU hour at peak to $2.75 to $3.50 per GPU hour as Blackwell capacity comes online and older hardware becomes abundant. By mid-2026, industry analysts project H100 cloud rates will fall below $2 per hour as they shift from the premium tier to the mid-tier in provider offerings.

This depreciation curve has two implications for your self-hosting decision. First, your ROI calculation must factor in the declining value of the hardware over its useful life, not just the purchase price amortized over years. A server that cost $300,000 today may be worth $150,000 in resale value two years from now and $60,000 four years from now. Second, the cascade effect means that older GPUs do not become useless — they become cheaper compute for less demanding workloads. An H100 that can no longer justify its rack space for cutting-edge inference still delivers excellent performance for embedding generation, batch classification, and smaller model serving. The smart operators do not retire old GPUs. They cascade them down to less latency-sensitive workloads and extend their economic life.

## Colocation: The Middle Ground

**Colocation** offers a middle path between full cloud and full on-premises. You own the hardware. The colocation provider owns the building, the power infrastructure, the cooling, and the physical security. You ship your servers to their facility, they rack them, and you get dedicated rack space with guaranteed power, cooling, and network connectivity.

The financial advantage is clear. You pay hardware cost (which you control) plus colocation fees (which are predictable) instead of cloud instance pricing (which includes the provider's margin on all of the above). For a 64-GPU cluster, colocation eliminates the need to build or lease your own data center, install your own power distribution, or maintain your own cooling plant. These are the highest-capital-cost and longest-lead-time components of self-hosting.

The operational advantage is partial. Colocation providers handle physical security, fire suppression, power redundancy, and building maintenance. But you still own the hardware. When a GPU fails at 2 AM, someone from your team — or a contracted remote-hands service — must diagnose and replace it. When firmware needs updating, your team schedules the downtime. When you need to expand from eight servers to sixteen, you procure, configure, and ship the new hardware.

Colocation costs for high-density GPU racks vary significantly by market. Tier 1 markets — Northern Virginia, Dallas, Chicago, Amsterdam, Frankfurt, Singapore — offer competitive pricing because of dense provider competition. Secondary markets may have fewer options but lower power costs. The typical colocation agreement is one to three years, with early termination penalties. Before committing, verify that the facility can support your power density (40 kilowatts per rack minimum for modern GPU servers), that they have available capacity in the timeframe you need, and that their network connectivity includes low-latency paths to the cloud providers you use for burst capacity.

## The Hybrid Model: Own the Base, Burst to Cloud

The most economically rational approach for organizations with predictable, sustained AI workloads is not pure cloud and not pure on-premises. It is hybrid: own the hardware that handles your baseline load, burst to cloud for peaks.

The pattern works because most AI workloads have a predictable baseline with occasional spikes. Your inference fleet handles 10,000 requests per minute during normal hours. During product launches, marketing campaigns, or seasonal peaks, demand spikes to 25,000 requests per minute. Sizing your owned infrastructure for the peak means 60 percent of your GPUs sit idle during normal hours. Sizing for the baseline means you cannot handle the peak without degradation.

The hybrid answer: own enough GPU capacity to handle 10,000 requests per minute — your steady-state baseline. Configure cloud-based burst capacity that automatically activates when demand exceeds on-premises capacity. The cloud GPUs are expensive per hour, but you only use them during peaks — maybe 15 to 20 percent of total hours in a month. Your owned hardware handles the remaining 80 to 85 percent at a fraction of the cloud cost.

Implementing this requires your inference stack to be portable enough to run on both owned hardware and cloud GPU instances. This is where the abstraction layers from the previous subchapter pay for themselves directly. A containerized inference server running vLLM on Kubernetes deploys identically to your on-premises cluster and to a cloud-based Kubernetes cluster. The load balancer routes overflow traffic from on-premises to cloud. When the peak subsides, cloud instances scale down and you stop paying.

## When Cloud Always Wins

Self-hosting is not universally correct. There are scenarios where cloud is the better choice regardless of scale.

Early-stage companies and teams with unpredictable workloads benefit from cloud's flexibility. If you do not know whether your inference workload will be 100 requests per hour or 100,000 requests per hour in six months, committing $2 million to hardware is a capital risk that cloud eliminates. Cloud lets you scale incrementally and pay proportionally.

Teams with variable or experimental workloads — frequent model retraining, A/B testing across multiple model architectures, rapid iteration on serving configurations — benefit from cloud's ability to provision different GPU types on demand. One week you need H100s for a large model. The next week you need a fleet of smaller GPUs for a distilled model evaluation. Cloud provides this flexibility without hardware inventory management.

Organizations with strict compliance requirements may find that managed cloud services handle security certifications, audit controls, and compliance documentation that would require significant investment to replicate on-premises. A healthcare company that needs HIPAA-compliant infrastructure can use AWS or Azure's HIPAA-eligible services out of the box. Building equivalent compliance controls on owned hardware requires dedicated security engineering and regular third-party audits.

Small GPU needs — fewer than 16 to 32 GPUs — rarely justify self-hosting. The fixed costs of staffing, colocation, and networking do not amortize well across a small hardware footprint. Three infrastructure engineers managing eight servers is expensive per GPU. The same three engineers managing 80 servers is efficient. Self-hosting has a minimum scale below which cloud is simply more practical.

## Making the Decision

The decision framework reduces to four numbers. First, your sustained GPU utilization rate. If your inference fleet averages below 50 percent utilization, cloud's pay-per-use model may already be cost-efficient. If you consistently run above 60 percent, self-hosting deserves serious evaluation. Second, your predictable baseline load. The higher the fraction of your total workload that is steady-state rather than bursty, the stronger the case for owned hardware. Third, your time horizon. Self-hosting requires capital investment that pays back over two to five years. If your planning horizon is shorter — because the product is experimental, the company is early-stage, or the technology landscape is shifting — cloud's operational-expense model is less risky. Fourth, your team capacity. If you can staff and retain three to five infrastructure engineers dedicated to GPU operations, self-hosting is operationally feasible. If not, you are buying hardware that nobody can maintain.

Run the numbers honestly, including every cost category. Hardware, power, cooling, networking, real estate, staffing, maintenance, depreciation. Compare the five-year total to five years of cloud spend at committed-use pricing for your actual workload — not list pricing, not on-demand pricing, but the negotiated rate you would realistically get at your volume. Add the cost of burst cloud capacity for peaks that on-premises cannot handle. The answer is usually one of three outcomes: cloud is clearly cheaper (workloads are too variable or too small), self-hosting is clearly cheaper (workloads are large and sustained), or the difference is close enough that non-financial factors — team capability, compliance requirements, strategic flexibility — should break the tie.

---

Whether you run on one cloud, three clouds, or your own hardware, GPU availability drives every capacity decision. The next subchapter examines how GPU capacity varies across providers and regions, how to plan for allocation delays, and how to build capacity strategies that survive the supply volatility that has defined the AI hardware market since 2023.
