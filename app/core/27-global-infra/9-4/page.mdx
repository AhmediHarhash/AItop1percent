# 27.67 â€” The Power Equation: Energy, Cooling, and Sustainability as Infrastructure Constraints

How much electricity does your AI inference consume per query? If you cannot answer that question, you cannot answer how much your AI actually costs, whether your data center can support the GPU expansion you planned for next quarter, or whether you will hit a power ceiling before you hit a compute ceiling. Most engineering teams treat electricity as someone else's problem -- a line item in the facilities budget, invisible to the people building and running the AI platform. That invisibility is a liability. In 2026, power is the binding constraint for AI infrastructure more often than compute, more often than network bandwidth, and more often than engineering headcount. The teams that treat energy as an infrastructure variable they can measure, optimize, and plan around gain a material advantage over those who discover their power limits only when the data center operator refuses to provision another rack.

## The Scale of AI Power Consumption

The numbers are stark and worth internalizing. A single NVIDIA H100 SXM GPU consumes up to 700 watts at peak load. An H200 operates at the same 700-watt thermal design power. A Blackwell B200 in the GB200 configuration is rated for up to 1,000 watts, though real-world inference workloads typically draw 500 to 700 watts per chip depending on utilization. These are per-GPU numbers. A single server with eight H100 GPUs draws 5,600 watts from the GPUs alone, plus another 500 to 1,000 watts for CPUs, memory, storage, and networking. A fully loaded eight-GPU server consumes 6 to 7 kilowatts.

Scale that to a rack. A standard GPU rack with four eight-GPU servers consumes 25 to 30 kilowatts. Compare that to a typical enterprise server rack at 5 to 8 kilowatts. GPU racks consume three to five times more power per unit of floor space. NVIDIA's GB200 NVL72 rack-scale configuration, which packs 72 Blackwell GPUs into a single rack with liquid cooling, consumes approximately 120 to 132 kilowatts per rack -- roughly fifteen times the power of a traditional server rack.

Scale further to a cluster. A 256-GPU inference cluster using H100s draws approximately 200 kilowatts from GPUs alone. With servers, networking, storage, and cooling overhead, the total facility power for that cluster approaches 300 kilowatts. A 1,000-GPU training cluster exceeds 1 megawatt. These are not numbers that fit invisibly into an existing data center's power budget. They require dedicated electrical infrastructure -- transformers, switchgear, distribution panels, backup generators -- that takes months to provision and millions of dollars to install.

## Power as the Binding Constraint

In traditional data centers, the binding constraint was usually floor space or cooling capacity. You could always buy more servers if you had room for them and could keep them cool. For AI infrastructure in 2026, the binding constraint has shifted to electrical power. Many colocation facilities and hyperscale data centers are power-limited: they have physical space for more racks but cannot deliver more electricity to the floor.

The International Energy Agency projected that global data center electricity consumption would reach 650 to 1,050 terawatt-hours by 2026, with AI workloads representing a rapidly growing share. In practical terms, this means that organizations planning GPU expansions increasingly find themselves in multi-month queues for electrical capacity. The data center has space. It has cooling capacity. It does not have another 500 kilowatts of electrical distribution available until the next transformer installation, which is eight months away.

This constraint changes how you plan infrastructure. Traditional capacity planning works in units of servers: how many servers do you need, and where will you put them? AI capacity planning must work in units of power: how many kilowatts do you need, where can you get them, and when will they be available? A team that secures GPU inventory but neglects to secure power capacity ends up with expensive hardware sitting in a warehouse.

The power constraint also creates geographic implications. Regions with abundant, cheap electricity -- the Nordics, Quebec, parts of the American Midwest -- become more attractive for AI infrastructure not because of their proximity to users but because of their ability to power dense GPU deployments. This creates a tension between latency optimization (deploy close to users) and power optimization (deploy where electricity is available and affordable) that shapes multi-region architecture decisions.

## Cooling: The Hidden Multiplier

Every watt consumed by a GPU must be dissipated as heat. Cooling is not an optional complement to power -- it is a mandatory multiplier. The cooling system consumes its own electricity, and the ratio between the total power consumed by the facility and the power consumed by the IT equipment is captured by **Power Usage Effectiveness**, or PUE. A PUE of 1.0 would mean zero cooling overhead, which is physically impossible. A PUE of 2.0 means the cooling system consumes as much electricity as the IT equipment itself. In 2026, well-run hyperscale data centers achieve a PUE of 1.08 to 1.2. Google reports a fleet-wide PUE of approximately 1.09. Typical enterprise data centers operate at 1.4 to 1.8. Every 0.1 improvement in PUE at megawatt scale saves hundreds of thousands of dollars per year in electricity costs.

Traditional air cooling works by blowing cold air past servers and exhausting the heated air. This approach has physical limits. Air cooling can effectively remove approximately 15 to 20 kilowatts per rack. A standard GPU rack at 25 to 30 kilowatts already exceeds this limit. A GB200 NVL72 rack at 120 to 132 kilowatts is six to eight times beyond what air cooling can handle. The math is simple: dense GPU infrastructure requires liquid cooling.

**Direct-to-chip liquid cooling** runs coolant through cold plates mounted directly on the GPU packages. The liquid absorbs heat far more efficiently than air -- water has roughly 3,500 times the heat capacity of air per unit volume. Direct-to-chip cooling can handle the thermal loads of current and next-generation GPUs while maintaining temperatures well below throttling thresholds. NVIDIA's GB200 NVL72 is designed exclusively for liquid cooling, with 115 kilowatts of its 132-kilowatt load dissipated through the liquid cooling loop.

**Immersion cooling** submerges entire servers in a dielectric fluid that absorbs heat directly from all components. This approach achieves even higher cooling density than direct-to-chip but introduces complexity in hardware maintenance -- you cannot simply slide a server out of a rack when it is submerged in fluid. Immersion cooling is gaining adoption for dedicated AI clusters where the density benefits justify the operational complexity.

The cooling infrastructure decision must be made before GPU procurement, not after. You cannot install a 120-kilowatt liquid-cooled rack in a facility designed for 8-kilowatt air-cooled racks without significant plumbing, electrical, and structural modifications. Teams that plan GPU deployments without coordinating with facilities engineering discover this constraint painfully when the hardware arrives and the data center cannot accept it.

## Per-Query Energy Cost

Understanding power consumption at the query level transforms energy from an opaque facilities cost into a variable that engineers can optimize. The calculation is straightforward.

Measure the average GPU power draw during inference. For an H100 running a typical LLM inference workload, this is typically 300 to 500 watts depending on model size, batch utilization, and quantization level. Measure the average number of requests the GPU processes per minute. Divide the power by the request rate to get watt-minutes per request, then convert to kilowatt-hours. Multiply by your electricity rate to get a dollar cost per query for the GPU electricity alone.

A concrete example: an H100 drawing 400 watts and processing 80 requests per minute consumes 5 watt-minutes per request, or 0.000083 kilowatt-hours. At an electricity rate of 0.10 dollars per kilowatt-hour, the GPU electricity cost is 0.0000083 dollars per request -- a fraction of a cent. But multiply by the PUE to capture cooling overhead (multiply by 1.2 for a well-cooled facility), add the non-GPU server power (another 20 to 30 percent), and multiply by millions of daily requests. A service handling 10 million requests per day at this rate spends roughly 120 dollars per day on electricity -- 44,000 dollars per year -- just for inference compute on a single GPU. A fleet of 100 GPUs pushes this to 4.4 million dollars per year in electricity alone. These numbers become line items that engineering and finance need to manage jointly.

Per-query energy cost also enables comparison across models and serving configurations. A smaller model at INT8 quantization might draw 250 watts and process 200 requests per minute -- dramatically lower per-query energy cost than the full-precision large model. Quantifying this difference in energy terms adds a dimension to the model selection decision that complements the quality and latency dimensions covered in Section 9.

## Idle Power: The Expensive Nothing

One of the most counterintuitive facts about GPU power consumption is that idle GPUs are not free. An H100 GPU at idle -- powered on but processing no work -- still draws 40 to 60 percent of its peak power consumption. An idle eight-GPU server consumes 2,500 to 3,500 watts of electricity doing absolutely nothing useful. Over a year, an idle eight-GPU server costs 2,000 to 3,000 dollars in electricity alone, not counting the amortized hardware cost.

This idle power cost creates a strong economic incentive for consolidation and intelligent scheduling. Instead of provisioning dedicated GPU pools for each team or application -- where some pools sit idle during off-peak hours while others are overloaded -- consolidate into shared pools with workload scheduling that maximizes GPU utilization. Shut down or power-gate GPUs that will be idle for extended periods. Schedule batch workloads like evaluation runs, fine-tuning jobs, and data processing during periods when interactive inference demand is low, filling the valleys in utilization that would otherwise waste power.

The operational discipline here connects directly to the FinOps practices covered in the next subchapter. Teams that do not measure idle GPU time do not know how much they are wasting. Teams that measure it but cannot schedule across workload types cannot act on the knowledge. Effective power management requires both observability and flexible scheduling infrastructure.

## Sustainability, Carbon, and Regulatory Pressure

Energy consumption at AI scale creates carbon footprint obligations that are becoming both a regulatory requirement and a customer expectation. The EU AI Act's provisions for high-risk AI systems include transparency requirements that will increasingly encompass energy consumption and environmental impact reporting. Large enterprise customers, particularly in Europe, are asking AI vendors for carbon footprint data per model, per query, and per deployment.

**Carbon intensity** varies dramatically by electricity source. A GPU cluster running in a region powered primarily by hydroelectric or nuclear generation produces a fraction of the carbon emissions per query compared to the same cluster in a region powered by coal or natural gas. In Norway, the grid carbon intensity is approximately 20 grams of CO2 per kilowatt-hour. In Poland, it exceeds 600 grams. The same AI inference request generates 30 times more carbon in one country than the other.

This creates an optimization opportunity: schedule carbon-intensive batch workloads during periods when the electrical grid is running on cleaner energy. Real-time grid carbon intensity data is available from organizations like Electricity Maps and WattTime. A workload scheduler that factors in carbon intensity can reduce the carbon footprint of batch AI workloads by 30 to 50 percent without any reduction in total compute -- simply by shifting when the work happens. Interactive inference workloads cannot be shifted this way because they must respond in real time, but training runs, evaluation suites, and offline processing can be scheduled around grid carbon intensity without affecting users.

For teams reporting sustainability metrics, the calculation framework involves measuring total electricity consumed by the AI platform (including cooling via PUE), multiplying by the carbon intensity of the local grid, and attributing the resulting emissions to specific models, teams, or products based on their GPU utilization share. This per-model carbon attribution enables both internal accountability and external reporting.

## Optimization Levers

Power optimization for AI infrastructure operates at multiple levels. At the model level, smaller models and more aggressively quantized models consume less energy per query. Choosing a 7-billion-parameter model quantized to INT8 over a 70-billion-parameter model at BF16 can reduce per-query energy consumption by ten times or more, with quality trade-offs that may or may not be acceptable depending on the use case.

At the infrastructure level, maximizing GPU utilization reduces the energy wasted on idle power draw. Liquid cooling reduces the PUE multiplier. Efficient power distribution reduces losses between the utility feed and the GPU. Modern data center designs use 48-volt DC power distribution to reduce conversion losses compared to traditional AC distribution.

At the scheduling level, bin-packing workloads onto fewer GPUs and powering down unused capacity eliminates idle power waste. Running batch jobs during off-peak hours and low-carbon grid periods reduces both cost and emissions. Preemptible workload scheduling that can pause and resume training or evaluation jobs enables tighter packing across the available power envelope.

None of these optimizations matter if you cannot measure the starting point. Power observability -- tracking watts per GPU, per server, per rack, and per query -- is the foundation. Without measurement, optimization is guesswork.

---

Power determines what your infrastructure can physically support. Cost determines what your organization will pay for. The next subchapter shifts from physical constraints to financial ones, examining the FinOps practices that make AI infrastructure spend visible, attributable, and optimizable across teams and projects.
