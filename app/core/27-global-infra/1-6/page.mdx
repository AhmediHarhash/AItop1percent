# 27.6 — The AI Infrastructure Stack: Layers, Components, and Responsibilities

In early 2025, a mid-sized healthcare AI company spent four months and $1.4 million optimizing its inference serving layer. The team profiled their vLLM deployment, tuned batch sizes, experimented with speculative decoding, and squeezed an additional fifteen percent throughput from their model serving pipeline. They were proud of the work. Then their on-call engineer received a page at two in the morning: inference latency had spiked from 200 milliseconds to 3.8 seconds across their entire production fleet. The serving layer was healthy. The model was running correctly. The batch tuning was working as designed. The problem was two layers below — a firmware update on their InfiniBand switches had silently degraded cross-node bandwidth by eighty percent, and the inference pods that were spread across multiple nodes for redundancy were now paying a massive penalty on every cross-node tensor transfer. The team spent forty-two hours debugging what they assumed was a model or application issue before a network engineer identified the root cause in the switch firmware.

This story illustrates the single most important lesson about AI infrastructure: problems at lower layers manifest as symptoms at higher layers. A GPU driver incompatibility looks like random model crashes. A storage throughput bottleneck looks like slow training that nobody can explain. A power distribution failure looks like nodes disappearing. If you do not understand the full stack — every layer, what it does, what owns it, and how it interacts with the layers above and below — you will misdiagnose problems, optimize the wrong components, and waste time and money fixing symptoms instead of causes.

## The Seven-Layer Model

The AI infrastructure stack has seven distinct layers, each with its own components, failure modes, and ownership boundaries. Understanding this stack is not academic — it is the difference between debugging a latency issue in forty-two hours and debugging it in forty-two minutes. Every component in your system lives at one of these layers. Every incident you will ever investigate crosses at least two.

The layers, from bottom to top, are: physical, hardware, driver and runtime, orchestration, platform services, serving, and observability. Each layer depends on every layer below it and provides abstractions to every layer above it. The higher you go, the more the layer looks like traditional software engineering. The lower you go, the more the layer looks like electrical engineering and physics. Most software engineers spend their entire careers in the top three layers. AI infrastructure demands fluency across all seven.

## Layer One: The Physical Foundation

The physical layer is everything that exists before a single piece of computing equipment is installed. Power delivery from the utility grid to the rack. Cooling systems — chillers, cooling distribution units, liquid cooling manifolds, and air handling for the portions of the facility that still use air cooling. Rack space with the structural capacity to support the weight of modern GPU servers, which can exceed one hundred pounds per server unit. Network cabling — both copper and fiber — connecting racks to top-of-rack switches, switches to spine switches, and the facility to the external internet and private interconnects.

This layer is almost always owned by the data center facility operator, whether that is a colocation provider, a cloud provider, or your own facilities team. Most AI engineers will never interact with this layer directly, but its constraints propagate upward in ways that affect every other layer. If the facility cannot deliver enough power to your racks, no amount of software optimization will increase your compute capacity. If the cooling capacity is insufficient, your GPUs will thermal-throttle and your training jobs will slow down without any error message explaining why. If the cabling between racks cannot support the bandwidth your training jobs require, your distributed training will bottleneck on communication and you will see unexplained slowdowns that look like software problems.

The most common failure mode at the physical layer is not a dramatic failure — not a power outage or a cooling system shutdown. It is a degradation. A partial reduction in cooling capacity that causes some GPUs to throttle during peak utilization. A utility feed that experiences voltage fluctuations, causing occasional node reboots attributed to hardware faults. A fiber cable with a marginal bend radius that produces intermittent packet loss at high throughput. These subtle physical failures are the hardest to diagnose because they produce symptoms that look random from the perspective of higher layers.

## Layer Two: Hardware Accelerators and Interconnects

The hardware layer is where the compute actually happens. This includes the GPUs or other accelerators — NVIDIA's H100, B100, and B200 series, AMD's Instinct MI300 and MI350 series, Google's TPU v5 and v6 — along with the CPUs, system memory, and local storage in each server. It also includes the high-speed interconnects that connect GPUs within a server and across servers. NVLink connects GPUs within a single node at bandwidths of 900 gigabytes per second or more per GPU. NVSwitch extends this connectivity across multiple GPUs in a rack. InfiniBand or high-speed Ethernet connects nodes across racks at 400 or 800 gigabits per second.

The hardware layer is where topology becomes a performance variable. Two GPUs that are NVLink-connected within the same node can exchange data orders of magnitude faster than two GPUs communicating across an InfiniBand network between different racks. A training job that places its workers on GPUs within the same NVLink domain will train faster than the same job spread across the data center, even if both configurations use the same number of GPUs. This topology awareness — understanding which GPUs are "close" and which are "far" in network distance — is critical for scheduling and will be explored in depth in later chapters.

The hardware layer is typically owned by the cloud provider or the organization's hardware operations team. For cloud deployments, you interact with hardware through instance types and machine families. For on-premises deployments, someone on your team is responsible for firmware updates, hardware failure detection and replacement, and lifecycle management of equipment that depreciates over three to five years. The most common failure mode is component failure — GPUs that develop memory errors, InfiniBand cards that produce correctable but performance-degrading errors, fans that fail in air-cooled nodes. Modern GPU servers include telemetry that reports these issues, but only if someone is monitoring that telemetry and acting on it.

## Layer Three: Drivers, Runtimes, and the Software-Hardware Interface

The driver and runtime layer is the bridge between hardware and software. It includes GPU drivers from NVIDIA or AMD, the CUDA toolkit and its associated libraries like cuDNN, cuBLAS, and NCCL for collective communication, the container runtime that enables workloads to access GPU devices from inside containers, and the Kubernetes device plugin that advertises GPU resources to the scheduler.

This layer is deceptively thin — it is a small amount of software — but it is the single most common source of "inexplicable" failures in AI infrastructure. A CUDA version mismatch between the driver and the PyTorch build will cause cryptic error messages or silent numerical errors. A container runtime that does not correctly expose GPU devices will make GPUs invisible to workloads. An NCCL version incompatibility will cause distributed training to hang without any error. A device plugin crash will cause Kubernetes to stop scheduling GPU workloads, and the failure may not be obvious because the rest of the cluster appears healthy.

The ownership of this layer is usually shared. The platform team manages the GPU drivers, container runtimes, and device plugins as part of the node image or configuration management. Application teams specify the CUDA version and libraries they need in their container images. The interface between these two — ensuring that the driver on the node is compatible with the CUDA version in the container — is where most problems occur. The **CUDA compatibility matrix**, the set of version constraints that determine which driver versions work with which CUDA toolkit versions and which PyTorch or TensorFlow versions, is one of the most important reference documents in AI infrastructure. Teams that do not manage this matrix explicitly will encounter version conflicts on a recurring basis.

## Layer Four: Orchestration and Resource Management

The orchestration layer is Kubernetes and the ecosystem of controllers, operators, and scheduling extensions that manage AI workloads. This includes the core Kubernetes components — the API server, the scheduler, etcd, the controller manager — as well as AI-specific extensions like Kueue for workload queuing, the GPU device plugin for resource advertisement, custom schedulers for topology-aware placement, and operators that manage the lifecycle of training jobs and inference deployments.

This layer is where the platform team has the most direct control and where the most consequential design decisions are made. How many clusters to run. How to partition GPU node pools. How to configure resource quotas and priority classes. How to handle autoscaling for inference workloads versus batch scheduling for training jobs. How to manage the Kubernetes control plane itself — etcd performance, API server rate limiting, and the operational challenges of running a control plane that manages thousands of GPU nodes.

The orchestration layer's most common failure mode is resource exhaustion — not of GPUs, but of the control plane itself. A cluster with thousands of pods, thousands of GPU resource requests, and frequent job churn can overwhelm etcd or hit API server rate limits. These failures are invisible to application teams because the cluster still appears to be running. Pods just stop being scheduled, or scheduling latency increases from seconds to minutes. The platform team must monitor control plane health with the same rigor it applies to the workloads the control plane manages.

## Layer Five: Platform Services

The platform services layer includes everything that application teams use to build and deploy AI products but that is not part of the serving path itself. Model registries that store and version model weights. Artifact caches that speed up container image pulls. Experiment tracking systems that record training runs and their results. CI/CD pipelines that test, build, and deploy model serving configurations. Job scheduling systems that manage the lifecycle of training jobs — launch, monitoring, checkpointing, restart on failure.

This layer is entirely owned by the platform team and represents the "product surface" of the internal platform — the tools and services that application teams interact with daily. The quality of this layer determines developer productivity. A model registry that is slow, unreliable, or hard to use will cause application teams to work around it, storing model weights in ad hoc locations and losing track of which version is deployed where. A CI/CD pipeline that takes forty-five minutes to validate and deploy a model configuration will slow down iteration cycles and encourage teams to skip validation.

The most overlooked component in this layer is the training infrastructure. Managing distributed training jobs — launching them across multiple GPU nodes, monitoring their progress, detecting failures and restarting from checkpoints, managing the job queue when the cluster is contended — is a complex operational problem that requires dedicated tooling. Teams that treat training as "just run a script on some GPUs" discover that at scale, training jobs fail frequently, failures are hard to diagnose, and restarts without proper checkpointing waste thousands of dollars of GPU time.

## Layer Six: Serving and Inference

The serving layer is where models turn into products. This includes inference engines like vLLM for large language model serving, NVIDIA Triton Inference Server for multi-model and multi-framework deployments, and TensorRT for optimized GPU inference. It includes load balancers that distribute requests across inference replicas, traffic management systems that route requests based on model version, input characteristics, or A/B test assignments, and autoscaling logic that adds or removes inference replicas based on demand.

The serving layer is where platform and application ownership overlap most. The platform team typically provides the inference engine infrastructure, the load balancing, and the autoscaling primitives. The application team configures them — selecting the model, setting batch sizes and timeout thresholds, defining routing rules, and specifying scaling parameters. This shared ownership requires a clear interface: the platform team provides a serving template or deployment abstraction, and the application team fills in the parameters.

The most common failure mode at the serving layer is silent quality degradation. The inference engine is running, responses are being returned, latency is within bounds — but the model is producing lower-quality outputs because a weight update was partially applied, a configuration change altered the decoding parameters, or the serving engine's batching strategy is pairing incompatible request types. Serving-layer monitoring must include not just operational metrics like latency and throughput but quality signals like evaluation scores on sampled outputs. Many teams monitor the first category rigorously and ignore the second entirely.

## Layer Seven: Observability

The observability layer wraps around all other layers. It includes metrics collection systems that gather GPU utilization, memory consumption, network throughput, and application-level performance data. Logging infrastructure that captures structured logs from every component. Distributed tracing that follows a single inference request from API gateway through load balancer through inference engine to GPU execution and back. Alerting systems that detect anomalies and notify the right team. And cost attribution systems that tag every resource-hour to the team and workload that consumed it.

Observability is not a layer that sits passively on top. It is the mechanism through which you understand every other layer. Without observability, every incident is a mystery. With it, you have the telemetry to trace a production latency spike from the application layer through the serving engine, through the orchestration layer, through the driver layer, to the physical layer where a cooling degradation caused GPU thermal throttling. The healthcare company from the opening of this subchapter lacked cross-layer observability. They had application metrics and serving metrics, but no visibility into the network fabric two layers below. Forty-two hours of debugging time was the cost of that gap.

The ownership of observability is entirely platform, but the consumption is shared. The platform team builds and operates the collection, storage, querying, and alerting infrastructure. Application teams define their own dashboards, their own alerts, and their own SLOs using the infrastructure the platform provides. The platform team also maintains cross-layer dashboards that no single application team would build — views that correlate GPU hardware health, driver status, network performance, and application latency in a single pane that makes cross-layer debugging possible.

## The Cross-Layer Debugging Principle

The most expensive lesson teams learn about the AI infrastructure stack is that symptoms lie about their origin. A model that produces garbled output might have a CUDA memory corruption issue in the driver layer. An inference endpoint that is slow might have a saturated network link in the physical layer. A training job that fails to converge might have a topology mismatch in the orchestration layer where workers that should be NVLink-connected are placed on separate racks communicating over InfiniBand.

The reflex of most engineers is to debug at the layer where the symptom appears. If inference is slow, tune the inference engine. If training fails, check the training code. If the model produces bad output, examine the model. This reflex is correct for traditional software, where layers are well-isolated and failures rarely cross boundaries. It is disastrously wrong for AI infrastructure, where every layer depends on every layer below it and failures regularly propagate upward.

The correct debugging approach for AI infrastructure is top-down symptom identification followed by bottom-up root cause investigation. Identify the symptom at whatever layer it appears. Then systematically check each layer below it, starting with the one immediately underneath and working down. Is the serving engine healthy? Check the orchestration layer — are pods scheduled correctly, are resources allocated, is the scheduler responding? Check the driver layer — are GPUs reporting healthy, are CUDA versions consistent, are device plugins running? Check the hardware layer — are GPUs reporting memory errors, are interconnects at full bandwidth? Check the physical layer — is power delivery stable, is cooling adequate?

This requires cross-layer visibility, which requires the observability layer to collect data from all other layers. It requires the platform team to understand the full stack, not just the layers they directly manage. And it requires incident response runbooks that include cross-layer investigation steps, not just application-layer troubleshooting. Teams that build this capability resolve incidents in minutes. Teams that lack it spend days chasing symptoms through the wrong layers.

The AI infrastructure stack is your mental model for every technical decision in this section. When later chapters discuss GPU scheduling, they are operating at layer four. When they discuss inference optimization, they are at layer six. When they discuss network topology, they are at layer two. Every problem you encounter in production will make more sense when you can point to the exact layer where it originates and trace its impact through the layers above. The next subchapter introduces infrastructure maturity levels — a framework for assessing where your organization falls on the spectrum from manual, ad hoc GPU management to fully automated, self-healing AI platforms.
