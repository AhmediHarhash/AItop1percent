# 27.25 — Network Architecture for AI: InfiniBand, RDMA, and the Bandwidth Wall

The network that runs your web application has almost nothing in common with the network that trains your AI model. Web traffic moves small payloads — kilobytes per request, milliseconds of tolerance, stateless connections that can retry on failure. AI training traffic moves massive payloads — gigabytes of gradient data every few seconds, microseconds of tolerance, tightly synchronized connections where a single slow link degrades every GPU in the job. The protocols, hardware, topologies, and cost structures are fundamentally different. Teams that try to run distributed training over standard enterprise networking infrastructure discover this the hard way: their GPUs spend more time waiting for data than computing it, their training runs take three to five times longer than expected, and their cost per training step makes the entire project economically unviable.

Understanding AI networking starts with understanding why the communication pattern of distributed training breaks conventional networking assumptions — and what the industry built to replace them.

## Why AI Needs Different Networking

Distributed training splits a model across multiple GPUs that must stay synchronized. In the most common pattern — data parallelism — each GPU processes a different batch of training data, computes gradients independently, and then every GPU must exchange those gradients with every other GPU before the next training step can begin. This exchange is an **all-reduce** operation, and it is the heartbeat of distributed training. It happens at every step, it involves every GPU, and the data volume is proportional to the model size.

For a seven-billion-parameter model in 16-bit precision, each all-reduce moves 14 gigabytes of gradient data. For a 70-billion-parameter model, it moves 140 gigabytes. A training step that takes 200 milliseconds of compute generates this data volume at every step. If the network cannot absorb 14 gigabytes in a fraction of that 200 milliseconds, communication becomes the bottleneck and GPUs sit idle waiting for gradient synchronization. This is the **bandwidth wall** — the point where network throughput, not GPU compute, determines how fast your model trains.

Standard enterprise Ethernet runs at 10 to 25 gigabits per second per port. At 25 Gbps, moving 14 gigabytes takes roughly 4.5 seconds — longer than many training steps themselves. Even modern 100 Gbps Ethernet takes over a second. When the communication phase takes longer than the computation phase, adding more GPUs makes training slower, not faster. You are paying for expensive hardware that spends most of its time idle.

## InfiniBand: The Training Network

**InfiniBand** is the networking technology purpose-built for high-performance computing, and it dominates large-scale AI training infrastructure in 2026. Where Ethernet was designed for general-purpose networking — flexible, widely compatible, tolerant of packet loss and retransmission — InfiniBand was designed for a single objective: moving data between compute nodes as fast as physics allows.

Current-generation InfiniBand runs at 400 gigabits per second per port with NVIDIA's NDR (Next Data Rate) specification, and the newest XDR (eXtreme Data Rate) specification doubles that to 800 Gbps per port. At 400 Gbps, a 14-gigabyte all-reduce completes in roughly 280 milliseconds across a well-configured cluster — still meaningful overhead, but within the range where compute and communication can overlap effectively. At 800 Gbps, the same transfer drops to 140 milliseconds, pushing the bandwidth wall back far enough that training runs on hundreds of GPUs remain compute-bound rather than communication-bound.

InfiniBand achieves this performance through several design choices that differentiate it from Ethernet. The protocol is lossless by design — credit-based flow control prevents packet drops, eliminating the retransmission overhead that plagues Ethernet under heavy load. The switching hardware is purpose-built for low latency, achieving port-to-port latency of 90 to 130 nanoseconds in current-generation switches, compared to microseconds for Ethernet switches. And the network interface cards — NVIDIA's ConnectX series — offload protocol processing from the CPU to dedicated hardware on the NIC, freeing the host processor for other work.

The tradeoff is ecosystem lock-in and cost. InfiniBand is overwhelmingly an NVIDIA ecosystem. The switches are NVIDIA Quantum series. The NICs are NVIDIA ConnectX. The software stack — NCCL for collective communication, GPUDirect RDMA for bypassing the CPU — is NVIDIA-optimized. This vertical integration delivers performance but creates a single-vendor dependency that many organizations find uncomfortable.

## RDMA: Bypassing the Bottleneck

The performance advantage of InfiniBand comes not just from raw bandwidth but from how data moves through the system. Traditional networking requires data to flow through several layers: the application copies data to a kernel buffer, the kernel passes it to the network stack, the network stack processes protocol headers, the NIC transmits the data, and the reverse happens on the receiving side. Each copy and each context switch between user space and kernel space adds latency and consumes CPU cycles.

**RDMA** — Remote Direct Memory Access — eliminates this entire path. With RDMA, the network interface card reads data directly from GPU memory on the sending machine and writes it directly to GPU memory on the receiving machine. No CPU involvement. No kernel context switches. No intermediate copies. The data path is GPU memory to NIC to wire to NIC to GPU memory, and the CPU on both sides is free to do other work. For AI training, where GPUs constantly exchange gradient data, RDMA means the CPU is never a bottleneck in the communication path.

RDMA comes in three flavors, and the choice between them is one of the most consequential infrastructure decisions you will make. **InfiniBand native RDMA** operates over InfiniBand networks and delivers the lowest latency and highest throughput — 1 to 2 microseconds of latency for small messages, full line-rate bandwidth for large transfers. This is the gold standard for training clusters. **RoCE v2** — RDMA over Converged Ethernet, version 2 — brings RDMA capabilities to standard Ethernet infrastructure. RoCE v2 runs over regular Ethernet switches and cables but requires lossless Ethernet configuration using Priority Flow Control and Explicit Congestion Notification. Latency is 3 to 5 microseconds, higher than native InfiniBand but far lower than TCP. **iWARP** — Internet Wide Area RDMA Protocol — provides RDMA over standard TCP/IP and is the most network-friendly option but delivers the lowest performance of the three. iWARP is rarely used for AI training in 2026 because its TCP dependency reintroduces the overhead that RDMA is meant to eliminate.

For most organizations that are not hyperscalers, RoCE v2 is the practical choice. It runs over existing Ethernet infrastructure with configuration changes rather than wholesale hardware replacement. Meta demonstrated publicly that properly tuned RoCE v2 can match InfiniBand performance for distributed training workloads. The catch is "properly tuned." Lossless Ethernet configuration is notoriously finicky — a single misconfigured switch can cause packet drops that cascade into NCCL timeouts across the entire training job. InfiniBand's credit-based flow control handles congestion natively, while RoCE v2 requires careful network engineering to achieve the same reliability.

## The Ultra Ethernet Alternative

The networking landscape is shifting. The **Ultra Ethernet Consortium**, backed by AMD, Broadcom, Cisco, Google, Meta, Microsoft, and others, released the UEC 1.0 specification in mid-2025. Ultra Ethernet is not an incremental improvement over RoCE v2 — it is a ground-up redesign of Ethernet for AI workloads. It provides native RDMA without requiring lossless network configuration, built-in adaptive congestion control tailored for collective communication patterns, and packet-level multipathing that maintains message ordering without head-of-line blocking.

Ultra Ethernet products are beginning to ship in early 2026, and their long-term trajectory threatens InfiniBand's dominance of the training network market. For infrastructure teams making purchasing decisions today, the calculus is straightforward: if you are building a training cluster that must be operational in the next six months, InfiniBand NDR or XDR remains the safe choice with proven performance. If you are planning infrastructure for 2027 and beyond, Ultra Ethernet deserves serious evaluation because it promises InfiniBand-class performance on open Ethernet standards with broader vendor support and lower total cost of ownership.

## Network Topology for AI Clusters

Raw bandwidth per port means nothing if the topology connecting those ports creates bottlenecks. The topology of your network fabric determines the aggregate bandwidth available for collective operations and the latency of GPU-to-GPU communication paths.

**Fat-tree** topology is the traditional standard for high-performance computing. Every leaf switch connects to every spine switch, creating a non-blocking fabric where any server can communicate with any other server at full line rate. NVIDIA's DGX SuperPOD reference architecture uses a fat-tree InfiniBand topology with Quantum switches. Fat-tree delivers predictable, uniform bandwidth — the performance between any two servers is identical regardless of their physical location. The cost is high: a full fat-tree requires a large number of spine switches, and at 400 Gbps InfiniBand, each spine switch costs upward of $200,000.

**Rail-optimized** topology is a more cost-effective alternative designed specifically for AI training communication patterns. In a rail-optimized design, each GPU in a server connects to a different leaf switch, and GPUs of the same rank across different servers — all GPU-zero devices, all GPU-one devices — form a "rail" connected through a dedicated switch group. This topology exploits the fact that NCCL's all-reduce algorithm communicates primarily between same-rank GPUs across nodes, not between different-rank GPUs. By aligning network topology with communication pattern, rail-optimized designs achieve 90 percent or more of fat-tree performance at significantly lower cost — some analyses show networking costs below 10 percent of an equivalent fat-tree for all-reduce-dominated workloads.

**Spine-leaf** topology is the most common design in modern data centers and serves as the foundation for both fat-tree and rail-optimized variants. In a spine-leaf design, leaf switches connect to servers and uplink to spine switches that provide cross-rack connectivity. For AI clusters, the key parameter is the oversubscription ratio — the ratio of total leaf-to-server bandwidth versus leaf-to-spine bandwidth. A 1:1 ratio (non-blocking) provides full bisection bandwidth but requires many spine switches. A 2:1 or 3:1 ratio reduces spine switch count but creates potential bottlenecks during all-reduce operations when every server is transmitting simultaneously. For training clusters, oversubscription ratios above 2:1 are generally unacceptable because collective operations create all-to-all traffic patterns that expose any bandwidth shortfall.

## The Cost of AI Networking

InfiniBand networking adds 20 to 35 percent to the cost of a GPU cluster, and this cost is not always visible in initial project budgets. A ConnectX-7 NIC for NDR InfiniBand costs approximately $1,800. A 64-port NDR Quantum-2 switch costs roughly $200,000. The optical cables connecting them add hundreds of dollars per link. For a 512-GPU cluster, the networking infrastructure — NICs, switches, cabling, and the specialized expertise to configure and maintain it — typically costs $1.2 to $2 million more than an equivalent Ethernet deployment.

The three-year total cost of ownership tells a starker story. Analysis from multiple infrastructure providers in 2025 showed InfiniBand TCO at roughly $4.6 million versus Ethernet at roughly $2.4 million for comparable cluster sizes — a $2.2 million difference. The InfiniBand cluster, however, completed training jobs 40 to 60 percent faster, meaning the cost per completed training run was often lower despite the higher infrastructure cost. This is the economic calculation that justifies InfiniBand for organizations with sustained, large-scale training workloads. If your cluster runs training jobs continuously, the faster completion time pays for the networking premium through higher GPU utilization. If your cluster runs training jobs occasionally, the premium never pays back.

For inference-only infrastructure, the calculus is different. Inference workloads rarely use multi-node all-reduce operations. Tensor-parallel inference across a small number of GPUs benefits from NVLink within a server but places minimal demands on the inter-server network. Standard 100 to 400 Gbps Ethernet is more than sufficient for inference serving, model weight distribution, and client request handling. Spending InfiniBand budgets on inference clusters is, in most cases, waste.

## When the Network Becomes the Constraint

The bandwidth wall is not theoretical. It is something you can measure in your own infrastructure. The signature metric is **communication-to-computation ratio** — the fraction of each training step spent on gradient synchronization versus the fraction spent on forward and backward pass computation. When this ratio exceeds 30 percent, your training is communication-bound. The GPUs are idle for nearly a third of every step, waiting for the network.

You measure this by profiling a training step with NVIDIA Nsight Systems or PyTorch Profiler. The profile shows distinct bands: forward pass, backward pass, and all-reduce. If the all-reduce band is wide relative to the compute bands, the network is the bottleneck. The fix depends on where the bottleneck sits. If total bandwidth is insufficient, you need faster networking — upgrading from 100 Gbps to 400 Gbps, or from Ethernet to InfiniBand. If the topology introduces unnecessary hops, you need better placement — moving communicating nodes closer together in the network fabric. If a single slow link is dragging down the collective, you need hardware diagnostics — a degraded cable or a misconfigured switch can reduce effective bandwidth by 50 percent while the monitoring dashboard shows all ports as "up."

The bandwidth wall is also moving. As models grow larger and training runs scale to thousands of GPUs, the data volume per all-reduce step increases proportionally. A training job that was compute-bound at 64 GPUs may become communication-bound at 256 GPUs on the same network because the per-step data volume scales while the per-link bandwidth stays constant. Planning for the bandwidth wall means planning not for today's workloads but for the workloads you will run in twelve months.

---

The network between servers moves data at gigabits per second. But within a server, the GPUs need to communicate at terabytes per second — orders of magnitude faster than any network can provide. The next subchapter covers NVLink and NVSwitch, the interconnect technology that turns multiple GPUs into a single computing unit and determines the performance ceiling for every multi-GPU workload you run.
