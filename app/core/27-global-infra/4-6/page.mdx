# 27.30 — Global Artifact Distribution: Weight Caches, Regional Mirrors, and Cold-Start Stampedes

The team had tested the rollout procedure three times in staging. Everything worked. The new model — a 70-gigabyte quantized weights file for their recommendation engine — deployed cleanly to a single test cluster in under two minutes. Confident, they scheduled the production rollout: three regions, forty inference pods per region, simultaneous deployment at 2:00 AM UTC to minimize user impact. At 2:01 AM, 120 pods across three continents began downloading the 70-gigabyte weight file from the central model registry hosted in us-east-1. Total simultaneous demand: 8.4 terabytes. The registry's egress bandwidth saturated within thirty seconds. Pods in us-east-1, closest to the registry, managed to pull weights in twelve minutes. Pods in eu-west-1 and ap-southeast-1, fighting for the remaining bandwidth over cross-region links, saw download times climb past forty minutes. Kubernetes health checks, configured with a five-minute timeout, killed the stalled pods at 2:06 AM. The scheduler restarted them. They began downloading again, adding to the congestion. By 2:30 AM, the system had entered a cycle of downloads, timeouts, kills, and restarts that consumed registry bandwidth without making forward progress. The incident lasted ninety minutes. The root cause was not the model, the registry, or the network — each worked correctly in isolation. The root cause was deploying a 70-gigabyte artifact to 120 endpoints simultaneously without a distribution strategy.

This is **the Cold-Start Stampede** — the cascade failure that occurs when a model deployment triggers simultaneous multi-gigabyte downloads across many pods or many regions. It is one of the most predictable and most commonly ignored failure modes in global AI infrastructure. Every team encounters it eventually. The only question is whether you design around it proactively or discover it during an outage.

## Anatomy of the Stampede

The stampede follows a predictable three-phase pattern that amplifies initial load into cascading failure.

Phase one is the **thundering herd**. A deployment event — a new model version, a scaling event, a cluster recovery — triggers many pods to start simultaneously. Each pod needs the same multi-gigabyte artifact: model weights, a container image, or both. All pods issue download requests to the same source at the same time. If you have 40 pods each downloading 70 gigabytes, you are demanding 2.8 terabytes of bandwidth from a single endpoint within a narrow time window.

Phase two is **source saturation**. The artifact source — a model registry, an object store bucket, a container registry — has finite bandwidth. S3 can deliver high aggregate throughput, but a single bucket serving concurrent multi-gigabyte requests to hundreds of clients hits throughput limits, especially if the requests are not distributed across partition key prefixes. Container registries like ECR, Artifact Registry, and Docker Hub have explicit rate limits on pull bandwidth per account or per IP range. When demand exceeds capacity, the source throttles or drops connections. Download speeds for each client drop from gigabytes per second to megabytes per second.

Phase three is the **amplification loop**. Slow downloads cause pods to miss their health check deadlines. Kubernetes kills the pods and restarts them. Restarted pods begin downloading from scratch, adding to the demand on the already-saturated source. Each restart cycle increases total bandwidth demand without making progress. If the health check timeout is shorter than the degraded download time — which it almost always is during a stampede — the system enters a stable failure loop where no pod ever finishes downloading, every pod is continuously killed and restarted, and the registry is permanently saturated. The system does not recover until an operator intervenes to either stop the rollout, extend health check timeouts, or reduce concurrency.

## Weight Caching Tiers: Pre-Position Before You Need

The fundamental defense against stampedes is ensuring that artifacts are already present where they are needed before the deployment triggers pod creation. This requires a multi-tier caching architecture that mirrors the storage hierarchy discussed in the previous subchapters, extended to the global scale.

The **regional object store mirror** is the outermost cache layer. Each region where you run inference maintains its own object store bucket containing copies of all model weights that region's pods might need. When a new model version is published to the central model registry, a replication job copies the weights to every regional mirror. This replication happens asynchronously, decoupled from the deployment event. By the time the deployment rolls out to a region, the weights are already sitting in that region's local object store, accessible at intra-region bandwidth rather than cross-region bandwidth. Downloading 70 gigabytes within a single AWS region takes roughly 60 to 90 seconds from S3. Downloading the same 70 gigabytes cross-region, competing with production traffic, can take 10 to 30 minutes.

The **in-cluster shared cache** is the middle tier. A parallel file system or a high-performance cache service (such as a distributed NVMe-backed cache) within each cluster holds the model weights that are actively being served. When the first pod in a cluster needs a model, the weight file is pulled from the regional object store to the in-cluster cache. Subsequent pods mount the same cached copy. The parallel file system, covered in the previous subchapter, serves this role naturally — it provides high-throughput shared access to the same file from hundreds of concurrent readers. The cache absorbs the read amplification: 40 pods reading the same 70-gigabyte file create 2.8 terabytes of read demand, but only 70 gigabytes of actual data transfer from the regional mirror.

The **node-local NVMe cache** is the innermost tier. Once model weights are on a node's local SSD, any pod scheduled to that node loads from local disk at 3 to 7 gigabytes per second — independent of network conditions, registry health, or concurrent demands from other nodes. A node-level cache agent manages the local NVMe, pre-fetching weights for models that are scheduled to arrive and evicting weights for models that have been retired. This cache turns every model restart — whether from a crash, a rolling update, or an autoscaler scale-up — into a local disk read rather than a network download.

The three tiers work together to ensure that the common case — a pod starting to serve a model — never touches the network at all. The uncommon case — a brand new model version arriving in a new region — touches only the intra-region network to the local object store mirror. The stampede scenario — simultaneous downloads from a central source — is eliminated by design.

## CDN-Style Model Distribution

The pre-staging pattern — replicating weights to regional mirrors before deployment — is conceptually identical to how content delivery networks distribute web assets. The model weights are the content. The regional mirrors are the edge caches. The deployment event is the cache-warming trigger.

The critical difference from web CDNs is that you cannot tolerate a cache miss. If a web CDN misses, the user waits 200 extra milliseconds for the origin to respond. If a model weight cache misses in the middle of a scale-up, pods sit idle for minutes or trigger a stampede. This means model distribution must be push-based, not pull-based. You do not wait for the first pod in a region to request the weights and then cache them. You push the weights to every region before any pod needs them.

The implementation is a **model distribution pipeline** — a series of automated steps triggered when a new model version is approved for deployment. Step one: the training pipeline publishes weights to the central model registry with a version tag. Step two: a replication controller detects the new version and initiates parallel copies to all regional object store mirrors. Step three: the controller monitors copy progress and reports completion per region. Step four: only after a region's copy is confirmed complete does the deployment controller allow pods in that region to be updated to the new model version. If the copy to a region fails, that region is held back from the rollout until the copy succeeds or an operator intervenes.

This pipeline adds latency to the total deployment cycle — replicating 70 gigabytes to five regions takes 5 to 15 minutes depending on cross-region bandwidth. But it converts an unpredictable, failure-prone deployment (where download time depends on concurrent load, network conditions, and registry health) into a deterministic one (where download happens once, in advance, under controlled conditions). The 15-minute replication cost is paid once per model version, not once per pod.

## Staged Rollouts: One Region at a Time

Even with pre-staged weights, deploying to all regions simultaneously carries risk. A bad model version — one that passes validation but degrades quality on real traffic — hits all regions at once, affecting all users before you have time to detect and react. Staged rollouts, introduced in Section 19 on deployment patterns, apply directly to model weight distribution.

The pattern is sequential regional deployment with validation gates. Deploy to the canary region first — typically your smallest or lowest-risk region. Monitor inference quality metrics, latency, error rates, and business metrics for a validation window of 15 to 60 minutes. If metrics remain within acceptable bounds, proceed to the next region. If metrics degrade, halt the rollout and roll back the canary region to the previous model version.

Staged rollouts also eliminate residual stampede risk. Even if you have not fully pre-staged weights to all regions, deploying one region at a time means only that region's pods download simultaneously. Forty pods downloading 70 gigabytes from a regional mirror is 2.8 terabytes — manageable for a local object store. Deploying all five regions simultaneously would demand 14 terabytes, potentially overwhelming even pre-staged mirrors. Staging converts a 14-terabyte spike into five sequential 2.8-terabyte loads, each manageable on its own.

## P2P Weight Distribution Within a Cluster

Within a single cluster, peer-to-peer distribution applies the same principle that Spegel and Dragonfly apply to container images, but for model weight files. When the first node in a cluster downloads the 70-gigabyte weight file from the regional object store, that node becomes a seed. The second node downloads from both the regional store and the first node. The third node downloads from the store, the first node, and the second node. By the tenth node, there are nine local seeds, and the download is bandwidth-limited by the cluster's internal network rather than by the connection to the object store.

BitTorrent-style protocols are the natural fit here. The weight file is split into chunks — typically 4 to 16 megabytes each — and each chunk can be fetched from any peer that has it. A coordination layer tracks which nodes have which chunks and directs new downloaders to the nearest peer with the needed chunk. The aggregate download bandwidth scales linearly with the number of nodes that have completed the download, which means the last node to finish downloads far faster than the first.

Several teams in the AI infrastructure community have built custom P2P weight distribution systems using libraries like the Go-based BitTorrent implementation or the CNCF Dragonfly project configured for arbitrary file distribution rather than just container images. The common architecture uses a lightweight agent on each node that participates in both the download (as a leecher) and the upload (as a seeder), with a central tracker that maintains the chunk map. The tracker adds minimal overhead — it handles metadata, not data — and the data transfer is distributed across all participating nodes.

For a 40-node cluster downloading a 70-gigabyte weight file, P2P distribution can reduce the total time from (70 gigabytes divided by the single-connection bandwidth to the object store) to roughly (70 gigabytes divided by the aggregate intra-cluster bandwidth available from all seeds). In practice, P2P weight distribution reduces cluster-wide model loading from 10 to 15 minutes to 2 to 3 minutes for large models, with the benefit growing as cluster size increases.

## The Rollout Coordinator

Bringing all of these components together — pre-staging, caching tiers, staged rollouts, P2P distribution — requires a coordination layer that the standard Kubernetes deployment controller does not provide. The standard controller knows how to roll out pods, but it does not know whether model weights are cached in a region, whether the regional mirror has finished replication, or whether nodes have pre-pulled the container image.

A **rollout coordinator** is a custom controller that gates each stage of a model deployment on infrastructure readiness. Before updating pods in a region, it checks: are the model weights present in the regional object store mirror? Are the weights cached on the in-cluster parallel file system? Are the container images pre-pulled on the target nodes? Only when all preconditions are met does the coordinator allow the Kubernetes deployment to proceed.

The coordinator also manages rollback. If the deployment fails health checks in the canary region, the coordinator not only rolls back the pod spec to the previous model version but also ensures that the previous version's weights are still cached on all nodes. Without this check, a rollback can trigger its own stampede if the previous weights have been evicted from local caches to make room for the new version.

Building a rollout coordinator is not a trivial engineering effort — it requires integration with your model registry, your storage replication system, your container image pre-pull mechanism, and your Kubernetes deployment API. But the alternative is manual coordination across these systems for every model deployment, with human error as the primary stampede trigger. Organizations that deploy model updates more than once a week find the investment pays for itself within the first quarter.

## Monitoring Artifact Distribution Health

You cannot manage what you do not measure. Artifact distribution requires its own monitoring, separate from application metrics and Kubernetes cluster health.

Track **replication lag** — the time between a model version being published to the central registry and that version being available in every regional mirror. If replication lag exceeds your deployment cadence, you will deploy to regions before their weights are staged, triggering the download patterns you designed the entire system to avoid. Track **cache hit rate** — the percentage of pod startups that find model weights already present on local NVMe versus needing a network download. A cache hit rate below 90 percent indicates that your pre-staging pipeline is not running ahead of deployments, or your cache eviction policy is too aggressive. Track **concurrent download count** — the number of simultaneous weight downloads happening across the cluster or region at any moment. A spike in concurrent downloads is the early warning signal of a stampede forming.

Alert on any of these signals degrading. Replication lag exceeding five minutes, cache hit rates dropping below 85 percent, concurrent downloads exceeding a threshold appropriate for your cluster size — any of these deserves an alert that fires before the stampede fully develops, giving the platform team time to pause the rollout and investigate.

---

Artifacts move across the network. Data accumulates in storage. But when the data is massive and the compute must be close to it, you face a force that no amount of clever networking can fully overcome. The next subchapter examines data gravity — the principle that large datasets pull compute toward them — and how it reshapes placement decisions for AI workloads across a global infrastructure.
