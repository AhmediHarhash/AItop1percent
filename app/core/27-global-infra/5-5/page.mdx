# 27.37 — Global Traffic Steering: GeoDNS, Anycast, Health Models, and Regional Brownouts

Traffic steering for AI inference is the mechanism that decides which region serves each request. Get it wrong and users experience unnecessary latency, overloaded regions buckle while underutilized regions sit idle, and a single regional failure cascades into a global outage. Get it right and the user never thinks about infrastructure — their request travels to the nearest, healthiest, most capable region and returns a response before they notice the round trip happened. Traffic steering is the invisible hand of multi-region AI. Every decision it makes is invisible to the user, and every mistake it makes is immediately felt.

Traditional web applications solved traffic steering years ago. The patterns are well-established: DNS-based routing, load balancers, health checks, failover. AI inference inherits all of those patterns and adds a constraint that web applications rarely face: GPU capacity is not elastic in the way CPU capacity is. A web server can scale horizontally in seconds. A GPU inference pod takes minutes to start, requires model weights to be loaded into GPU memory, and depends on specialized hardware that may not be available in the target region at all. Traffic steering for AI must be capacity-aware in a way that web traffic steering never needed to be.

## GeoDNS: The Foundation Layer

**GeoDNS** is the simplest and most widely deployed traffic steering mechanism. When a user's device resolves your API hostname, the DNS server examines the source IP of the request (or the IP of the recursive resolver), estimates the user's geographic location, and returns the IP address of the nearest regional endpoint. A user in Frankfurt gets the EU-West endpoint. A user in Tokyo gets the AP-Northeast endpoint. A user in Sao Paulo gets the nearest available endpoint — which, depending on your region footprint, might be US-East or a dedicated South America region.

The implementation is straightforward. Cloud DNS services — AWS Route 53, Google Cloud DNS, Azure Traffic Manager, and third-party providers like ClouDNS and NS1 — all support geolocation-based routing natively. You configure a DNS record for your inference endpoint, define geographic routing policies (EU traffic goes to this IP, Asia-Pacific traffic goes to that IP), and the DNS layer handles the rest. Setup takes hours, not days, and requires no changes to your application code.

GeoDNS has three limitations that matter for AI workloads. First, geographic proximity does not guarantee lowest latency. A user in Istanbul might be geographically closer to your EU-West region in Ireland, but network topology might route their traffic faster through your EU-Central region in Frankfurt. GeoDNS routes by geography, not by measured latency, and the two do not always align. Second, DNS responses are cached by recursive resolvers, CDNs, and the user's operating system. When you change a routing policy — removing a failed region, shifting traffic during a capacity event — the change propagates only as fast as DNS caches expire. With typical TTL values of 60 to 300 seconds, it can take minutes for traffic to shift after a routing change. During those minutes, users continue hitting the old region. Third, GeoDNS has no awareness of backend health or capacity. It routes based on geography even if the target region is down, overloaded, or out of GPU capacity. You must layer health checks on top to remove unhealthy regions from the DNS response.

Despite these limitations, GeoDNS is the correct starting point for most multi-region AI deployments. It handles 80 to 90 percent of routing decisions correctly — most users are best served by the geographically nearest region — and its simplicity means fewer failure modes in the routing layer itself. Add sophistication on top of GeoDNS only when the 10 to 20 percent of cases it gets wrong start causing measurable user impact.

## Anycast: Single IP, Multiple Regions

**Anycast** takes a fundamentally different approach. Instead of returning different IP addresses to different users via DNS, Anycast advertises the same IP address from multiple locations. BGP (Border Gateway Protocol) routing at the network layer automatically directs each user's packets to the nearest advertising location based on network topology, not geography. A user's packets travel to whichever region is topologically closest in the BGP routing table, which usually — but not always — corresponds to the geographically nearest region.

The advantage of Anycast over GeoDNS is routing accuracy and speed. BGP routing decisions are made by the network itself, at the packet level, with no DNS caching delay. When a region stops advertising its Anycast address (because the region is down or its health check failed), BGP convergence shifts traffic to the next-nearest region within seconds to minutes — much faster than DNS TTL-based failover.

The disadvantage is less control. With GeoDNS, you can make fine-grained routing decisions: send French users to the Paris region, German users to the Frankfurt region, and everyone else in Europe to the London region. With Anycast, you hand routing decisions to BGP, and BGP optimizes for network topology, not business logic. You cannot easily say "send this customer's traffic to a specific region" or "avoid this region during a maintenance window" because BGP routing operates below the application layer. Anycast is excellent for getting traffic to the nearest region fast. It is less suitable when you need compliance-aware routing (EU data must stay in the EU) or capacity-aware routing (this region's GPUs are full, send traffic elsewhere).

In practice, most large-scale AI deployments use Anycast at the edge layer — for the initial packet routing to the nearest point of presence — and then apply application-level routing logic at that point of presence to make the final region selection. The user's traffic arrives at the nearest Anycast PoP in milliseconds. The PoP's load balancer then inspects the request, checks regional health and capacity, applies compliance rules, and forwards the request to the appropriate inference region. This two-layer approach combines Anycast's speed with application-level intelligence.

## Health-Based Routing: Removing Broken Regions

Neither GeoDNS nor Anycast inherently knows whether a region is healthy. Without health checks, a failed region continues receiving traffic until an operator manually removes it from the routing configuration. For AI inference, "unhealthy" encompasses more failure modes than for traditional services. A region can be unhealthy because the inference service is down. It can be unhealthy because the model failed to load and is returning errors. It can be unhealthy because GPU memory is exhausted and requests are queuing for minutes. It can be unhealthy because the wrong model version is loaded and responses are incorrect.

**Active health checks** are probes sent by the routing layer to each regional endpoint on a regular cadence — every 10 to 30 seconds. A health check for an AI inference endpoint should not just verify that the HTTP server responds. It should send a lightweight inference request and validate the response. A health check that sends "What is 2 plus 2?" and verifies the response contains "4" catches model loading failures, GPU memory errors, and configuration problems that a simple TCP or HTTP health check misses. If three consecutive health checks fail, the region is removed from the routing pool. If three consecutive checks succeed after a failure, the region is re-added.

**Passive health checks** complement active checks by monitoring real traffic. If the error rate from a region exceeds a threshold — say, more than five percent of requests returning 500 errors over a one-minute window — the routing layer reduces or eliminates traffic to that region without waiting for active health checks to detect the problem. Passive checks respond faster to emerging failures because they observe every request, not just periodic probes.

The re-addition policy is as important as the removal policy. A region that was removed for health reasons should not receive 100 percent of its normal traffic the moment it passes health checks. A **gradual ramp-up** — sending 10 percent of traffic initially, increasing to 25, then 50, then 100 over a period of 5 to 15 minutes — prevents a region that is technically healthy but still warming up from being overwhelmed by a sudden traffic flood. Inference pods that just loaded model weights have empty KV caches. Their first few hundred requests will be slower than steady-state as the cache fills. Ramping traffic gradually gives the region time to reach operating temperature.

## Capacity-Aware Routing: Avoiding GPU Saturation

This is where AI traffic steering diverges most sharply from traditional web routing. A web application behind an autoscaler can absorb a traffic spike by adding containers in seconds. An AI inference cluster behind the same autoscaler cannot, because adding GPU inference capacity requires loading multi-gigabyte model weights — a process that takes 30 seconds to several minutes depending on model size and storage tier. During that loading window, new pods are not serving traffic. If traffic spikes faster than pods can load, the existing pods saturate and latency climbs.

**Capacity-aware routing** adds a real-time signal from each region indicating its current GPU utilization and available inference headroom. Each region publishes a capacity score — a normalized metric reflecting how much additional traffic it can absorb without degrading latency. The routing layer uses this score alongside geography and health to make decisions. If a user's nearest region is at 95 percent GPU utilization, the routing layer sends their request to the next-nearest region that has available capacity, accepting a latency penalty to avoid a timeout.

The capacity signal must be fresh. GPU utilization can change dramatically within seconds as batch requests arrive or long-running generation tasks complete. A stale capacity signal — even 60 seconds old — can route traffic to a region that was healthy a minute ago but is now saturated. The practical approach is for each region's load balancer to publish a capacity metric to a lightweight coordination service (a distributed key-value store or a gossip protocol) that the global routing layer reads on every request. Update latency of one to two seconds is sufficient for most workloads. Sub-second updates add complexity without proportional benefit.

## The Brownout Pattern

A regional **brownout** is more insidious than a full outage. During a brownout, the region is not down. Health checks pass. The inference service responds. But something is degraded: GPU utilization is at 90 percent and climbing, latency has doubled from the baseline, error rates are elevated but below the threshold that triggers automatic removal, or a subset of GPUs have failed silently and the remaining GPUs are handling the load but at reduced throughput.

Brownouts are hard to detect automatically because every individual metric might be within acceptable bounds. Latency is high but not breaching the alert threshold. Error rate is elevated but below five percent. GPU utilization is high but the autoscaler has not triggered because it is already at maximum capacity. The signals are yellow, not red, and automated systems designed around binary healthy-or-not logic continue routing traffic to the brownout region.

The defense is **composite health scoring**. Instead of evaluating individual metrics against independent thresholds, combine them into a single regional health score that degrades when multiple metrics are simultaneously elevated. A region where latency is at the 80th percentile of its normal range AND error rate is at the 70th percentile AND GPU utilization is above 85 percent receives a health score that reduces its traffic share proportionally. No single metric triggers removal, but the combination triggers proportional reduction. The traffic shed from the brownout region is distributed to healthier regions based on their own capacity scores.

Brownout detection requires baseline models. You need to know what normal looks like for each region — what is the typical p50 and p99 latency at 10 AM local time on a Tuesday, what is the typical error rate, what is the typical GPU utilization pattern. Anomaly detection against these baselines catches brownouts that fixed thresholds miss. A latency of 400 milliseconds might be a brownout in a region that normally runs at 200 milliseconds but perfectly normal in a region that serves a larger model with 400-millisecond baseline latency.

## Latency-Based and Weighted Routing

**Latency-based routing** measures the actual network round-trip time from the user to each available region and routes to the region with the lowest measured latency. AWS Route 53, Google Cloud DNS, and most global load balancers support latency-based routing policies. The routing layer maintains a latency map — updated periodically by synthetic probes or derived from real user measurements — and consults it for every routing decision.

Latency-based routing corrects the primary weakness of GeoDNS. A user in Turkey might be geographically equidistant between Frankfurt and Mumbai, but network latency to Frankfurt might be 35 milliseconds while latency to Mumbai is 120 milliseconds due to the routing path through submarine cables. Latency-based routing sends the request to Frankfurt. GeoDNS might have sent it to Mumbai.

**Weighted routing** distributes traffic across regions according to configured ratios rather than sending all traffic to a single preferred region. You might configure 50 percent of traffic to US-East, 30 percent to EU-West, and 20 percent to AP-Southeast, reflecting relative GPU capacity in each region. Weighted routing is useful during migration scenarios (gradually shifting traffic from old regions to new ones), capacity-limited situations (no single region can handle all traffic), and cost optimization (distributing traffic to regions with cheaper GPU rates).

The most sophisticated routing layers combine all of these signals. For each incoming request, the routing logic evaluates: which regions are healthy (health checks), which regions have the required model (model availability map), which regions are compliant for this user's data (residency rules from the previous subchapter), which regions have available capacity (capacity scores), and among the remaining candidates, which one offers the lowest latency (latency measurements). This multi-factor routing decision happens in single-digit milliseconds at the edge and considers five dimensions simultaneously. Implementing it requires careful engineering, but the result is a routing layer that makes intelligent per-request decisions rather than static geographic assignments.

## The Failover Cascade

When a region fails and its traffic shifts to the remaining regions, the critical question is: can they handle it? If your three regions each run at 70 percent GPU utilization under normal load, and one fails, the remaining two must absorb an additional 50 percent traffic each — pushing them to 105 percent of their original capacity. They will not scale in time. Latency climbs. Requests start timing out. The overloaded regions trigger their own health check failures, and now you have a **failover cascade** — one region's failure causes the others to fail under the redistributed load.

The only reliable defense against failover cascades is **headroom planning**. Size each region to handle its own traffic plus a proportional share of traffic from the region most likely to fail. If you run three equally-loaded regions and want to survive any single-region failure, each region must be provisioned at no more than 67 percent of its capacity. For two regions, each must run at no more than 50 percent. This headroom costs money — 33 to 50 percent of your GPU capacity is reserved for failure scenarios that may never occur. The alternative is accepting that a regional failure becomes a global outage, which is a valid business decision as long as it is made deliberately and not discovered during an incident.

Some teams reduce headroom costs by designating primary and secondary failure pairs. US-East fails over to US-West. EU-West fails over to EU-Central. AP-Southeast fails over to AP-Northeast. Each secondary region only needs headroom for its paired primary, not for the entire fleet. This reduces total headroom requirements but creates dependency chains — if both paired regions fail simultaneously, there is no fallback. The right headroom model depends on your failure tolerance, your budget, and whether your SLAs promise multi-region resilience or merely best-effort failover.

Section 20 covers application-level resilience patterns — circuit breakers, bulkheads, graceful degradation — that complement the infrastructure-level traffic steering discussed here. The two layers work together: traffic steering keeps requests flowing to healthy regions, and application-level resilience handles the cases where individual requests fail even in a healthy region.

---

Traffic steering determines which region handles each request. But before the request arrives, physics has already determined how long the network trip will take — and for latency-sensitive AI applications, that network time can consume most of the response budget. The next subchapter examines the latency geography: how the speed of light constrains AI response times and what it means for region placement, edge caching, and architectural decisions.
