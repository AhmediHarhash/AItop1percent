# 27.16 — The Utilization Ceiling: Why GPU Clusters Waste Most of Their Capacity

The most expensive resource in your infrastructure is not the GPUs you use. It is the GPUs you pay for and do not use. This distinction sounds obvious, but the gap between allocation and actual utilization in most organizations is staggering. Industry data from 2025 consistently shows that enterprise GPU clusters operate at 20 to 35 percent average utilization. Some are worse. A team provisions a pool of eight H100s for model training. The training runs consume all eight GPUs for six hours. Then the job finishes, and the GPUs sit idle for eighteen hours until the next run. The team is paying for twenty-four hours. They are using six. That is 25 percent utilization, and it is not an edge case. It is the norm.

**The utilization ceiling** is the maximum effective utilization that a GPU cluster achieves before organizational, technical, and scheduling constraints prevent further improvement. **The capacity paradox** is the deeper irony: GPUs are simultaneously the scarcest resource in your infrastructure — difficult to procure, with lead times stretching months — and the most wasted. Understanding why this happens is the first step to closing the gap.

## Why Utilization Is Structurally Low

GPU underutilization is not caused by laziness or ignorance. It is caused by structural forces that push utilization down even when every team is trying to be efficient. Each force operates independently, and in combination they create a ceiling that most organizations cannot breach without deliberate engineering investment.

The first force is **allocation-based scheduling**. Kubernetes schedules GPUs by allocating whole devices to pods. A team requests four GPUs for inference serving. The inference workload uses an average of 40 percent of each GPU's compute capacity, spiking to 85 percent during traffic peaks. But the GPUs are allocated — no other workload can use the remaining 60 percent during off-peak hours. The team sized their allocation for peak load, which is the rational thing to do when latency matters. The result is that peak-sized allocations serve average-load workloads, and the difference is waste.

The second force is **scheduling fragmentation**. Your cluster has 64 GPUs across 8 nodes, each with 8 GPUs. Three teams have jobs running: one uses 6 GPUs on node A, one uses 5 on node B, and one uses 7 on node C. The remaining GPUs — 2 on node A, 3 on node B, 1 on node C — total 6 free GPUs. But a new training job that needs 4 GPUs on the same node cannot be scheduled, because no single node has 4 free. Six GPUs are available in aggregate but unusable for this workload. This is the fragmentation problem, and it worsens as the ratio of job size to node size increases.

The third force is **temporal waste**. Development GPUs sit idle during nights and weekends. In a typical workweek, developers are actively using their GPU allocations for roughly 40 to 50 hours out of 168 total hours. That means 70 percent of the hours are pure idle time. Multiply this across a team of twenty developers, each holding a GPU allocation, and the waste is enormous — hundreds of GPU-hours per week that generate cost but no compute.

The fourth force is **memory fragmentation**. A model requires 30 gigabytes of GPU memory. The available GPUs have 80 gigabytes each. The workload is allocated a full GPU, but only uses 37 percent of its memory. The remaining 50 gigabytes of VRAM cannot be used by another workload because, without Multi-Instance GPU or memory-aware scheduling, the entire device is consumed by the single allocation.

The fifth force is **batch job gaps**. Training jobs are bursty. A training run takes four hours. The pipeline that launches the next run does not start for another two hours because of data processing, evaluation, and manual review steps. During those two hours, the GPUs are allocated but idle — held in reserve for the next run that hasn't been submitted yet. In some organizations, the gap between sequential training runs accounts for 20 to 30 percent of total GPU time.

The sixth force is **fear-based over-provisioning**. When GPU procurement takes months, teams hoard capacity. A team that needs eight GPUs today requests twelve because they anticipate growth and know that requesting more later means waiting in a procurement queue. Once allocated, those four extra GPUs are rarely voluntarily returned. The team's reasoning is defensible on an individual level — they are protecting their future capacity — but the aggregate effect is systematic over-provisioning across the organization.

## The Utilization Spectrum

Not all organizations sit at 25 percent. The utilization spectrum ranges widely, and where you sit depends on the engineering investment you have made in GPU efficiency.

At the bottom — 15 to 25 percent — are organizations that use Kubernetes default scheduling with no sharing, no reclamation, and no temporal policies. GPUs are allocated to teams, and those allocations persist regardless of actual use. This is the baseline for most enterprises in 2026.

In the middle — 30 to 50 percent — are organizations that have implemented basic optimizations: time-slicing or MIG for inference workloads, automated reclamation of idle allocations after a defined timeout, and separate node pools for development and production that allow development GPUs to be reallocated during off-hours.

At the upper range — 50 to 70 percent — are organizations with dedicated platform engineering investment in GPU efficiency: preemptible workloads that backfill idle capacity, workload-aware scheduling that bin-packs complementary jobs onto shared nodes, chargeback systems that create financial incentives for right-sizing, and continuous monitoring with automated alerts when utilization drops below thresholds.

Beyond 70 percent is where only hyperscalers and the most sophisticated AI-native companies operate. When Meta reported training Llama 3 405B on 16,384 H100 GPUs, their model floating-point operation utilization rate was approximately 38 percent — but this measures compute efficiency of the training algorithm, not hardware utilization. Their GPUs were fully busy during training; the 38 percent reflects how much of the GPU's theoretical peak was used by the specific computation pattern. For sustained hardware utilization above 70 percent, you need the kind of workload diversity, scheduling sophistication, and operational discipline that requires a dedicated team of ten or more engineers focused solely on GPU efficiency.

## The Economics of Wasted GPUs

The financial impact of low utilization scales linearly with cluster size and becomes impossible to ignore past a threshold.

Consider a 100-GPU H100 cluster. Cloud pricing for H100 instances in early 2026 ranges from roughly two dollars per GPU-hour on discount providers to four dollars per GPU-hour on major cloud platforms. At three dollars per GPU-hour — a reasonable midpoint — a 100-GPU cluster costs $262,800 per month running continuously. At 25 percent utilization, you are getting productive work from the equivalent of 25 GPUs. The other 75 GPUs generate cost but no value, at a waste rate of roughly $197,000 per month.

If you move that cluster from 25 percent to 50 percent effective utilization, you either extract twice as much work from the same hardware or you can achieve the same workload with half the GPUs. Either way, the savings exceed $1 million per year for a 100-GPU cluster. For a 500-GPU cluster — common at mid-to-large enterprises running multiple models across training and inference — the savings at the same improvement ratio approach $5 million annually.

These numbers make GPU utilization one of the highest-leverage cost optimization opportunities in AI infrastructure. A single engineer dedicated to utilization improvement who moves the needle by ten percentage points pays for their fully-loaded cost within the first month.

## Right-Sizing Through Monitoring

You cannot improve what you do not measure, and most organizations do not measure GPU utilization at the workload level. They see aggregate utilization across the cluster — which looks reasonable because some nodes are fully busy while others are idle — but they do not see which specific workloads are wasting capacity.

The foundation of utilization improvement is per-workload GPU monitoring. The NVIDIA Data Center GPU Manager exporter for Prometheus provides the core metrics: GPU compute utilization percentage, GPU memory utilization percentage, GPU memory allocated versus used, and power consumption as a proxy for actual work. These metrics, collected per pod and aggregated per namespace, reveal the true utilization landscape.

The patterns you will find are predictable. Inference workloads that are allocated a full GPU but average 20 to 30 percent compute utilization, spiking to 70 percent during peak hours. Training workloads that hit 90 percent utilization during runs but account for only 40 percent of their allocated time due to gaps between runs. Development workloads that show utilization spikes during business hours and flatline to zero every evening.

Each pattern has a specific remedy. Inference workloads with low average utilization are candidates for GPU sharing through time-slicing or MIG — the techniques covered in subchapter 2.4. Training workloads with high peak utilization but low duty cycle are candidates for preemptible scheduling, where their idle GPUs are lent to lower-priority workloads between runs. Development workloads with temporal patterns are candidates for automated scaling to zero, where their GPU allocations are released outside business hours and re-provisioned on demand.

## Preemptible Workloads and Backfill Scheduling

The single most effective technique for improving cluster-wide utilization is preemptible backfill scheduling. The concept is straightforward: when GPUs are allocated but idle — because a training job finished, because it is a weekend, because an inference workload is in an off-peak trough — a lower-priority workload automatically fills the gap. When the primary workload needs its GPUs back, the backfill workload is preempted — paused or terminated — and the GPUs are returned.

The workloads best suited for backfill are those that can tolerate interruption. Batch evaluation jobs that can be restarted from the last checkpoint. Hyperparameter sweeps where individual trials are independent and losing one trial is a minor inconvenience. Data preprocessing pipelines that process records in parallel and can resume from the last committed offset. Synthetic data generation runs that produce value proportional to how long they run and lose only the current batch on preemption.

Kueue, the Kubernetes workload queuing system, supports preemption natively. You define priority classes — production inference is highest, scheduled training is medium, backfill jobs are lowest. When a higher-priority workload needs resources, Kueue preempts the lowest-priority workloads that free up sufficient capacity. The preempted pods receive a termination signal and have a configurable grace period — typically 30 to 60 seconds — to checkpoint their state before being killed. When capacity becomes available again, Kueue re-queues the preempted workload.

The engineering investment required is modest. You need checkpoint and resume logic in your backfill workloads, priority class definitions in Kubernetes, and Kueue configured with preemption policies. The return is substantial. Organizations that implement preemptible backfill scheduling typically see cluster-wide utilization improve by 15 to 25 percentage points, because the temporal gaps that previously wasted GPUs are now filled with productive work.

The key metric to watch is preemption frequency. If backfill workloads are preempted so often that they never make meaningful progress, the system is thrashing rather than filling. Track the ratio of productive compute time to total allocated time for preemptible workloads. If that ratio drops below 50 percent, you need to either add capacity for primary workloads or reduce the volume of backfill work submitted to the cluster.

## Chargeback and the Behavioral Dimension

Technical optimizations — sharing, preemption, right-sizing — address the mechanical causes of waste. But the behavioral causes are equally important, and they require economic mechanisms, not engineering solutions.

When GPU cost is a line item in the platform team's budget, individual AI teams have no financial incentive to right-size their allocations. Requesting more GPUs than needed costs them nothing. Returning unused GPUs saves them nothing. The rational behavior under this incentive structure is to request as much as possible and hold it as long as possible, because the cost of over-provisioning is invisible to the team that benefits from it.

**Chargeback** changes this dynamic by attributing GPU costs to the teams that consume the resources. Each namespace's GPU consumption is measured — allocation-hours or utilization-hours, depending on what behavior you want to incentivize — and the corresponding cost appears on that team's budget. When a team sees that they are spending $40,000 per month on GPUs and only using 30 percent of them, the conversation about right-sizing happens naturally. No platform engineer needs to play the role of cost police. The financial visibility creates self-correcting behavior.

The design choice between charging for allocation (what you requested) and charging for utilization (what you used) matters. Allocation-based chargeback incentivizes right-sizing requests but penalizes teams that responsibly handle peak loads with buffer capacity. Utilization-based chargeback incentivizes actual efficiency but does not penalize teams that hoard unused allocations. The most effective hybrid charges for the higher of actual utilization or 50 percent of allocation — teams that use their GPUs efficiently pay for what they use, while teams that allocate heavily but use little pay at least half the allocation cost as a holding fee.

## The Utilization Dashboard

The operational artifact that ties all of these strategies together is a utilization dashboard visible to every team, every engineering manager, and every finance stakeholder in the organization.

The dashboard shows four views. The first is cluster-wide utilization over time — the headline number that tells leadership whether their GPU investment is being used. The second is per-team utilization, broken down by namespace, showing each team's allocation, actual use, and the gap between them. The third is per-workload utilization, showing individual deployments and training jobs with their GPU efficiency metrics. The fourth is a waste report that identifies the top opportunities for improvement — the ten workloads with the lowest utilization, the allocations that have been idle for more than 24 hours, the node pools with the most fragmented capacity.

This dashboard is not a nice-to-have. It is the operational control surface for GPU cost management. Without it, utilization improvement is a guessing game. With it, every optimization has a measurable before and after, every regression is detected promptly, and every team understands their contribution to the organization's GPU efficiency.

## The Realistic Target

Perfection is not the goal. A 100 percent utilized GPU cluster means zero headroom for new workloads, zero capacity for traffic spikes, and zero ability to absorb failures. The realistic target for most organizations is 50 to 65 percent effective utilization — the range where GPUs are productively busy the majority of the time while maintaining sufficient buffer for operational flexibility.

Reaching this range from a starting point of 25 percent requires a specific sequence of investments. First, instrument. You cannot improve what you do not see. Second, identify the largest sources of waste through the per-workload data. Third, implement the lowest-effort, highest-impact change — which is almost always automated reclamation of idle development allocations during off-hours. Fourth, implement GPU sharing for inference workloads. Fifth, implement preemptible backfill for temporal gaps. Sixth, implement chargeback to address behavioral waste. Each step moves utilization up by 5 to 10 percentage points, and the first three can typically be completed within a quarter.

The organizations that reach the 50 to 65 percent range and sustain it treat utilization as a first-class operational metric, reviewed weekly alongside availability, latency, and error rates. It appears in engineering leadership reviews. It informs procurement decisions. It is part of the platform team's goals and the engineering organization's financial health reporting. GPU utilization is not a one-time optimization project. It is an ongoing operational discipline, and the organizations that treat it as such save millions of dollars per year while getting more work from the same hardware.

---

With the mechanics of Kubernetes for AI workloads established — from scheduling and queuing to GPU sharing, autoscaling, compatibility management, and utilization optimization — the next chapter shifts to the organizational and governance layer. Multi-tenancy, resource isolation, and cluster governance determine whether your Kubernetes platform serves one team well or serves dozens of teams fairly, and the design decisions involved are as much about policy as they are about technology.
