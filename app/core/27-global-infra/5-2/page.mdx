# 27.34 — Data Residency and Sovereignty: Where Data Must Live and Why It Constrains Architecture

Where does your data live? Not where you think it lives. Not where your cloud console says the region is. Where does it actually, physically, legally live — and under whose jurisdiction? If you cannot answer that question for every piece of user data your AI system processes, including the prompts users send, the model outputs you generate, the logs you store, and the embeddings you create from their content, you have a compliance problem that no amount of engineering can fix after the fact.

Most AI teams treat data residency as a deployment detail — pick the right cloud region, check a box, move on. That mental model worked in 2022. By 2026 it is dangerously naive. Data residency is an architectural constraint that propagates through every layer of your system, from where inference runs to where logs are stored to which model weights are allowed in which jurisdictions. Get it wrong and the consequences are not theoretical. They are fines, injunctions, and in some jurisdictions, criminal liability for company officers.

## The Residency Spectrum: From Flexible to Absolute

Data residency requirements exist on a spectrum, and understanding where your obligations fall determines how constrained your architecture needs to be.

At the permissive end, some jurisdictions allow data to be processed anywhere as long as adequate protections are in place. The United States has no general federal data residency law. Personal data of US residents can be processed on servers in Ireland, Singapore, or anywhere else without violating federal law. Sector-specific rules add constraints — HIPAA requires safeguards for health data, and certain government contracts mandate US-soil processing — but the baseline is permissive.

At the moderate end, GDPR allows transfers of EU personal data to countries with an **adequacy decision** — a finding by the European Commission that the destination country provides data protection "essentially equivalent" to EU law. As of early 2026, adequacy decisions cover the UK, Japan, South Korea, Canada, Israel, and a handful of others. The EU-US Data Privacy Framework, adopted in 2023, provides a mechanism for transfers to certified US organizations, though its long-term legal stability remains uncertain after the Schrems I and Schrems II precedents. For countries without adequacy, Standard Contractual Clauses or Binding Corporate Rules provide a legal basis for transfers, but they require a case-by-case assessment of the destination country's surveillance laws — an assessment that data protection authorities in France and Austria have found insufficient for US transfers on multiple occasions.

At the strict end, several countries impose absolute data localization. Russia's Federal Law No. 242-FZ requires personal data of Russian citizens to be stored and initially processed on servers physically within Russia. China's Data Security Law and Personal Information Protection Law impose strict cross-border transfer requirements including government security assessments for any personal data leaving the country. India's Digital Personal Data Protection Act grants the government power to designate countries where personal data may not be transferred — a "blacklist" approach that can change with a ministerial notification. Vietnam's Cybersecurity Law requires local storage of certain data categories, and its implementing decree extends this to data about user relationships and user-generated content.

The practical impact is that your AI system's architecture must account for multiple points on this spectrum simultaneously if you serve a global user base. An EU user's prompt to your chatbot is GDPR-protected personal data. A Russian user's query is subject to localization requirements. A Chinese user's interaction falls under PIPL. A single multi-region AI service must route, process, and store data differently based on where the user is — and it must do this correctly on every request, not just most of them.

## What Counts as "Data" in an AI Context

The residency question is more complex for AI systems than for traditional applications because AI creates derivative data that inherits the regulatory status of its source.

A user sends a prompt containing personal information. That prompt is unambiguously personal data under GDPR, LGPD, PIPL, and most other privacy frameworks. It must be processed and stored in compliance with the applicable residency rules. This is the easy case.

The model's response may also be personal data if it contains or references information about the user. A customer support chatbot that repeats back the user's account details in its response has created a new piece of personal data that carries the same residency obligations as the original.

**Embeddings** — the vector representations your system creates from user content for retrieval-augmented generation — are the first tricky derivative. An embedding is a mathematical transformation of the original text. It does not contain the original words. But it encodes semantic meaning derived from personal data, and regulators have increasingly taken the position that embeddings of personal data are themselves personal data because the original information can be partially reconstructed or re-identified through nearest-neighbor lookups in the embedding space. If your embeddings live in a centralized vector database in US-East while the original data stays in EU-West, you may have a compliance gap that is technically invisible but legally real.

**Model logs and telemetry** are the second tricky derivative. If you log prompts and responses for quality evaluation — and you should, because evaluation requires it — those logs contain personal data and carry the same residency obligations. A common architecture mistake is building a centralized logging and evaluation pipeline that aggregates data from all regions into a single analytics cluster. That cluster becomes a residency violation if it contains EU data processed outside the EU, even if the inference itself ran in the right region.

**Fine-tuning data** is the third derivative. If you fine-tune a model on user interactions, and those interactions include personal data from regulated jurisdictions, the fine-tuning dataset is subject to residency requirements. More controversially, some data protection authorities have investigated whether the model weights themselves constitute personal data if the training data included personal information — a question that remains legally unresolved in 2026 but that risk-averse organizations are monitoring closely.

## The CLOUD Act Problem

A misconception that persists among engineering teams is that choosing a European cloud region of a US-headquartered provider satisfies European data residency requirements. It does not, and understanding why is critical to making sound infrastructure decisions.

The US CLOUD Act, enacted in 2018, grants US law enforcement the authority to compel US-headquartered companies to produce data in their possession, custody, or control — regardless of where that data is physically stored. If your data sits on AWS servers in Frankfurt, and the US government issues a valid order to Amazon, Amazon is legally obligated to produce that data even though it is stored in Germany. The physical location of the server is irrelevant. What matters is the legal jurisdiction of the entity that controls the infrastructure.

This creates a direct conflict with GDPR. Article 48 of GDPR prohibits transferring personal data to a third country based on a court judgment or administrative decision unless there is a mutual legal assistance treaty or similar international agreement. No such agreement exists between the US and the EU that satisfies GDPR's requirements. A US-headquartered provider caught between a CLOUD Act order and GDPR faces an impossible choice: comply with US law and violate EU law, or comply with EU law and violate US law.

In practice, this tension is handled through legal risk assessment rather than absolute prohibition. Most companies continue using US-provider European regions for GDPR-covered data, relying on the argument that CLOUD Act orders targeting their specific data are statistically unlikely and that supplementary measures (encryption with customer-managed keys, access controls, legal challenge commitments) mitigate the risk. But "statistically unlikely" is not the same as "legally compliant," and data protection authorities in multiple EU member states have signaled that this approach may not withstand scrutiny.

For organizations with high regulatory exposure — financial services, healthcare, government — the practical response is the **sovereign cloud**. AWS launched its European Sovereign Cloud in Germany in late 2025, operated by EU-resident personnel under EU legal jurisdiction. Microsoft announced sovereign cloud capabilities in June 2025 for 15 nations. Google Cloud's sovereign offering, built in partnership with T-Systems and Thales, provides encryption key management fully outside Google's control. These sovereign clouds are architecturally isolated from the parent company's global infrastructure, designed to place them outside the reach of the CLOUD Act. They cost 15 to 30 percent more than standard cloud regions, but for regulated workloads, the premium buys legal defensibility that standard regions cannot provide.

## How Residency Reshapes AI Architecture

Once you accept that data residency is not a deployment detail but an architectural constraint, the implications cascade through your system design.

**Inference must run where the data lives.** If EU personal data must stay in the EU, your model must be available in an EU region to process it. This sounds simple until you have 15 models serving different features and your EU region has half the GPU capacity of your US region. You must either secure sufficient GPU capacity in every required jurisdiction or make hard choices about which models are available in which regions.

**Logs must stay where they are generated — or be anonymized before moving.** Your centralized evaluation pipeline that ingests prompts and responses from all regions must either run independently in each region or receive only anonymized or aggregated data from non-local regions. True anonymization of free-text prompts is extremely difficult — removing names and identifiers is not sufficient if the remaining text is unique enough to re-identify individuals. Pseudonymization (replacing identifiers with tokens) is more practical but does not change the data's legal status under GDPR. Aggregation — sending statistical summaries rather than individual records — is the most reliable approach for cross-region data flows that must avoid residency constraints.

**Embedding databases must be regional.** If your RAG pipeline creates embeddings from user documents that contain personal data, those embeddings inherit the residency obligation. A centralized global vector database is a compliance liability. Regional vector databases are compliant but introduce the challenge of maintaining search quality when the index is split across jurisdictions.

**Model updates must flow to regions, not data to models.** The traditional machine learning pattern of pulling training data to a central location for model training and then pushing the updated model back out may violate data residency if the training data includes personal information from regulated jurisdictions. The compliant alternative is federated approaches — training locally in each region and aggregating model updates rather than raw data — or ensuring that training data is properly anonymized before centralization.

## Building a Residency Map

Before making any architectural decision, build a residency map for your AI system. For each jurisdiction you serve, document four things. First, what data your AI system processes from users in that jurisdiction — prompts, documents, voice recordings, behavioral data. Second, what residency requirements apply to that data — absolute localization, adequacy-based transfer, contractual safeguards. Third, where that data currently flows in your system — inference region, logging destination, embedding storage, training pipelines, analytics systems. Fourth, where gaps exist between the requirement and the reality.

This map will almost certainly reveal flows you did not know about. A third-party monitoring tool that sends error logs to a US-based SaaS endpoint. A centralized A/B testing framework that stores user interactions in a single global database. A model evaluation pipeline that pulls a random sample of prompts from all regions to a central notebook. Each of these is a potential residency violation that exists not because anyone chose to violate the rules but because nobody thought to check.

The residency map is not a one-time exercise. As you enter new markets, add new AI features, and integrate new tools, new data flows emerge. Review the map quarterly, or whenever you launch a new product or enter a new jurisdiction. The organizations that avoid regulatory surprises are the ones that know where their data is before the regulator asks.

---

Data residency tells you where inference must run. But knowing where you need compute capacity and actually having models ready to serve in those locations are very different problems. The next subchapter examines the three strategies for replicating model weights across regions — full copy, partial replication, and on-demand loading — and the tradeoffs each one forces between cost, latency, and operational complexity.
