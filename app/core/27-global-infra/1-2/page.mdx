# 27.2 — The GPU Scheduling Problem: Resources That Break Traditional Abstractions

A GPU is not a faster CPU. It is a fundamentally different resource with fundamentally different scheduling requirements. Every scheduling system you have ever used — from the Linux kernel's CFS to Kubernetes's kube-scheduler to cloud autoscaling groups — was built on a single assumption: compute resources are fungible, divisible, and overcommittable. CPU cores can be sliced into millicores. Memory can be swapped to disk. A workload that needs "4 cores and 8 gigs" can run on any node that has those resources available, and if the node is slightly overcommitted, the kernel will time-slice gracefully. GPUs violate every part of this assumption. GPU memory cannot be swapped. GPU compute cannot be transparently time-sliced without significant overhead. Two GPUs of the same model but different memory sizes are not interchangeable. Two GPUs on the same node connected via NVLink perform fundamentally differently from two GPUs on separate nodes connected via standard networking. And the default Kubernetes scheduler, which represents the state of the art in container orchestration, treats GPUs as a flat integer count — no different from how it counts CPU cores, except that it cannot subdivide them. This abstraction gap is the root cause of the GPU scheduling problem, and closing it is the single most impactful thing you can do for your AI infrastructure.

## Why GPU Memory Changes Everything

The defining characteristic of a GPU as a schedulable resource is its dedicated memory. A CPU workload uses system RAM, which the operating system manages through virtual memory, paging, and swap. If a process needs more memory than is physically available, the OS pages data to disk. Performance degrades, but the workload survives. GPU memory is entirely separate. It sits on the GPU card itself — 40GB on an A100-40, 80GB on an A100-80, 80GB on an H100, 141GB on an H200, 192GB on a B200. When a model is loaded for inference, its weights must fit entirely in GPU memory. There is no swap. There is no paging to a slower tier. If the weights do not fit, the model does not load. Period.

This binary constraint — fits or does not fit — makes GPU scheduling a hard constraint satisfaction problem rather than a soft resource allocation problem. A model that requires 72GB of GPU memory can run on an H100 with 80GB or an H200 with 141GB. It cannot run on an A100-40 with 40GB. It does not run slowly on the A100-40. It does not run at all. The scheduler must know the memory requirement of the workload and the memory capacity of the target GPU before making a placement decision. If the scheduler gets this wrong, the workload crashes on startup — after the pod has been scheduled, the container has been pulled, and the GPU has been allocated. The failure is not graceful. It is a hard crash after consuming resources and time.

Beyond the model weights, GPU memory must also hold the **KV cache** — the intermediate state that accumulates during inference as the model processes tokens. For large language models, the KV cache can consume significant memory that grows with sequence length and batch size. A model whose weights occupy 40GB might need an additional 10 to 30GB for the KV cache during peak inference, depending on the number of concurrent requests being batched and the maximum sequence length. A scheduler that allocates a 40GB model to a GPU with exactly 40GB of free memory has made a technically correct but practically fatal decision — the model loads, serves one or two short requests, and then crashes with an out-of-memory error when the KV cache grows under real traffic. Effective GPU scheduling must account for the total memory footprint: weights plus KV cache plus activation memory plus a safety margin. Getting this right requires understanding the workload at a level of detail that no generic scheduler provides out of the box.

## The Topology Problem: Not All GPU Pairs Are Equal

When a workload requires multiple GPUs — for tensor parallelism during inference of large models, or for data parallelism during training — the physical relationship between those GPUs matters enormously. Two GPUs on the same node connected via NVLink can communicate at 900 gigabytes per second on the latest NVIDIA platforms. Two GPUs on different nodes communicate over the network — typically InfiniBand or RDMA over Converged Ethernet — at speeds that are ten to fifty times slower. For workloads that require frequent inter-GPU communication, such as distributed training with gradient synchronization or tensor-parallel inference where every token generation requires cross-GPU communication, this difference is not a minor performance penalty. It is a three-to-eight-times throughput degradation. Research from NVIDIA's own benchmarking confirms that topology-unaware placement of multi-GPU workloads can reduce effective throughput by three to eight times compared to topology-aware placement.

The topology is not limited to "same node versus different node." Within a single node, GPUs are connected in a hierarchy. On a typical eight-GPU node using the DGX architecture, GPUs are arranged in two groups of four, with full NVLink connectivity within each group and a bridge between groups. Two GPUs within the same NVLink group communicate faster than two GPUs in different groups on the same node, which in turn communicate faster than two GPUs on different nodes. A scheduler that understands this topology can place a four-GPU workload on four GPUs in the same NVLink domain. A scheduler that does not — including Kubernetes's default scheduler — might scatter those four GPUs across two NVLink domains or even two nodes, turning a workload that should run in thirty minutes into one that runs in two hours.

The topology problem extends to multi-node training. Large training jobs that span 16, 32, or 64 GPUs need network topology awareness as well as intra-node topology awareness. GPUs on nodes that share the same top-of-rack switch communicate faster than GPUs on nodes in different racks. Nodes in the same availability zone communicate faster than nodes across zones. A scheduler that places a 32-GPU training job across three availability zones has introduced inter-zone latency into every gradient synchronization step — latency that accumulates over millions of training steps. The training job will complete, but it will take 30 to 50 percent longer than if the GPUs had been co-located, and that extra time on a 32-GPU cluster at $4 per GPU per hour adds up to tens of thousands of dollars in wasted compute.

## The Evolution of GPU Scheduling in Kubernetes

Kubernetes's original approach to GPU scheduling was the **NVIDIA device plugin**, introduced in 2018. The device plugin does one thing: it reports the number of GPUs available on each node and allows pods to request a whole number of GPUs. If you request "nvidia.com/gpu: 2," the scheduler finds a node with at least two free GPUs and assigns the pod. It does not know the GPU model. It does not know the memory size. It does not know the interconnect topology. It does not know the driver version. It counts GPUs the way a vending machine counts coins — by quantity, not by type. For simple, single-GPU inference workloads on homogeneous clusters where every node has the same GPU, this is adequate. For anything more complex, it is dangerously insufficient.

The community recognized this limitation, and between 2020 and 2024, a series of layered solutions emerged. Node labels and node affinity rules allowed operators to tag nodes with GPU model names and memory sizes, so that workloads could target specific hardware. This helped, but it was manual, brittle, and could not express topology constraints. The NVIDIA GPU Operator automated driver installation, device plugin deployment, and basic monitoring, reducing the operational burden of running GPUs in Kubernetes. Batch schedulers like **Volcano** and **Kueue** added gang scheduling — the ability to schedule multi-GPU workloads atomically, ensuring that all GPUs are available before any are allocated. This prevented the deadlock scenario where a 4-GPU job gets 3 GPUs and waits forever for the fourth while blocking other jobs from using those 3 GPUs.

NVIDIA's **Multi-Instance GPU** technology, available on A100 and later architectures, allowed a single physical GPU to be partitioned into up to seven isolated instances, each with its own dedicated memory and compute resources. MIG partitioning means a single A100-80 can be split into, say, two instances of 40GB each, or one instance of 40GB and two instances of 20GB. This is powerful for bin-packing smaller inference workloads onto expensive hardware, but it requires the scheduler to understand partitioning configurations, which the default device plugin does not support. The GPU Operator added MIG support, but the configuration was static — you set the partitioning profile on a node and it stayed that way until an administrator changed it. Dynamic repartitioning based on workload demand required additional tooling.

The most significant architectural change came with **Dynamic Resource Allocation**, which reached general availability in Kubernetes 1.34 in August 2025. DRA replaces the device plugin's flat integer model with a structured resource description framework. Instead of reporting "this node has 4 GPUs," a DRA driver reports detailed attributes of each device: model name, memory capacity, compute capability, interconnect topology, driver version, and partition state. Workloads request resources by describing what they need — "two GPUs with at least 80GB each, connected via NVLink, with CUDA 12.4 compatibility" — rather than asking for a count. The scheduler matches requests to available resources using attribute-based selection, and the allocation is tracked with fine-grained metadata that enables observability, quota management, and topology-aware placement.

DRA changes the scheduling model from "count-based" to "attribute-based," and that change is as fundamental for GPU scheduling as the introduction of resource requests and limits was for CPU scheduling in early Kubernetes. It means the scheduler can finally reason about GPUs as the heterogeneous, topology-sensitive, memory-constrained resources they actually are, rather than treating them as generic integers.

## The Bin Packing Problem: Heterogeneous Workloads on Heterogeneous Hardware

Even with topology-aware, attribute-based scheduling, GPU clusters face a combinatorial optimization problem that CPU clusters largely avoid: the bin packing problem. You have a fleet of GPUs with varying memory sizes, compute capabilities, and interconnect configurations. You have a queue of workloads with varying memory requirements, compute needs, and topology constraints. Your goal is to place workloads on GPUs such that overall utilization is maximized and no workload is starved.

This is hard because GPU memory is not fungible. If you have a node with 4 A100-80 GPUs and you place one workload that uses 50GB on one GPU, that GPU has 30GB of memory remaining. But there may be no workload in your queue that fits in exactly 30GB. The remaining memory is stranded — allocated to the GPU but unusable. This stranded memory problem is the primary driver of low GPU utilization in production clusters. Industry surveys from 2024 and 2025 consistently reported average GPU utilization of 20 to 35 percent across enterprise AI clusters. Not because the GPUs are idle, but because the memory is fragmented. Each GPU is partially used but cannot accept another workload because the remaining memory does not match what any queued workload needs.

MIG partitioning helps with bin packing for inference workloads. If you partition an A100-80 into two 40GB instances, you can run two 35GB models on a single GPU instead of allocating a full GPU to each. But partitioning is not free. Each MIG instance has less compute throughput than a full GPU. The partitioning configuration is typically static for a node — you choose a profile at node setup time, and changing it requires draining all workloads. Dynamic MIG management through the GPU Operator and newer DRA-based approaches is improving, but in early 2026 most production clusters still treat MIG configuration as a semi-static decision.

The bin packing problem gets worse with heterogeneous clusters. If your fleet includes A100-40, A100-80, H100, H200, and B200 GPUs — which is increasingly common as teams upgrade hardware incrementally rather than replacing entire fleets — the scheduler must understand the performance and memory characteristics of each GPU type and match workloads to the most appropriate hardware. A workload that fits on an A100-80 also fits on an H200, but placing it on the H200 wastes 60GB of premium memory. Conversely, placing a workload that could benefit from the H200's faster memory bandwidth on an A100 saves money but delivers worse performance. The scheduler is not just bin packing for utilization. It is bin packing for the intersection of utilization, cost, and performance — a multi-objective optimization problem that has no single correct answer, only trade-offs.

## The KAI Scheduler and the State of the Art

In early 2025, NVIDIA open-sourced the **KAI Scheduler** — the scheduler that powered its Run:ai platform — under the Apache 2.0 license. KAI represents the current state of the art in Kubernetes-native GPU scheduling. It provides hierarchical queuing with fair-share policies, gang scheduling for multi-GPU workloads, topology-aware placement that understands NVLink domains and network proximity, GPU sharing and fractional GPU allocation, and dynamic quota adjustment based on real-time demand. The scheduler operates alongside the default Kubernetes scheduler, handling GPU-specific workloads while the default scheduler continues to manage non-GPU pods.

KAI's significance is not just its feature set. It is the validation that GPU scheduling is a problem that requires a dedicated scheduling subsystem. The default Kubernetes scheduler was never designed for this. Bolt-on solutions like node affinity rules and taints are workarounds, not solutions. The emergence of KAI, Kueue, Volcano, and DRA as a coordinated ecosystem — each handling a different aspect of the GPU scheduling problem — signals that the industry has moved past the "just add a device plugin" phase and into a phase where GPU-aware scheduling is a first-class infrastructure capability.

Production clusters in 2026 that achieve 70 to 80 percent GPU utilization — compared to the 20 to 35 percent industry baseline — typically use a combination of these tools: DRA for hardware-aware resource description, Kueue or KAI for workload queuing and fair-share scheduling, MIG for partitioning GPUs to match smaller workloads, and topology-aware placement for multi-GPU jobs. The difference between 25 percent utilization and 75 percent utilization on a 100-GPU cluster at $4 per GPU-hour is $720,000 per month. GPU scheduling is not a technical detail. It is a financial lever.

## What This Means for Your Infrastructure

If you are running AI workloads on Kubernetes and you have not upgraded beyond the basic device plugin, you are almost certainly wasting more than half your GPU spend. Not through carelessness, but because the default scheduling abstraction does not have the information it needs to make good placement decisions. It cannot see GPU memory. It cannot see interconnect topology. It cannot distinguish an A100 from an H200. It is scheduling blindly, and the result is stranded memory, topology-unaware placement, and utilization numbers that would get any traditional infrastructure engineer fired.

The GPU scheduling problem is not optional to solve. It is the foundation of everything else in AI infrastructure. Your autoscaling depends on it. Your cost model depends on it. Your multi-tenancy isolation depends on it. Your training cluster efficiency depends on it. Every hour you delay implementing proper GPU-aware scheduling is an hour of wasted capacity you will never recover.

The next subchapter shifts from how you schedule GPU resources to why inference — the workload that actually faces your users — has become the dominant cost in AI infrastructure, surpassing training for the first time in 2025 and reshaping every infrastructure investment decision for 2026 and beyond.
