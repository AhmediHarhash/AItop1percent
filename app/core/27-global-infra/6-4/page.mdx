# 27.44 — Cloud Provider GPU Availability: Navigating Quotas, Reservations, and Capacity Constraints

GPU availability on major cloud providers is not guaranteed. It never has been, and as of 2026 the gap between demand and supply has only widened. You cannot log into a cloud console, select an H200 instance, click launch, and expect it to start. For most accounts, the default GPU quota for high-end accelerators is zero. For most regions, the on-demand capacity for training-class GPUs is fully allocated. The teams that run reliable AI infrastructure are not the teams with the best models or the cleverest code. They are the teams that secured GPU capacity three to six months before they needed it and built their scheduling around the reality that capacity is a finite, contested resource.

Understanding how cloud providers allocate GPU capacity — the quota systems, the reservation models, the pricing tiers, and the secondary market of specialized GPU clouds — is the difference between launching on schedule and spending your first quarter on a waitlist.

## The Quota System: Your Ceiling Before You Even Start

Every major cloud provider gates GPU access through **quotas** — per-account, per-region limits on how many GPU instances you can run simultaneously. Quotas exist because GPU capacity is physically constrained. A data center has a fixed number of GPU servers in each rack, and the provider must allocate that capacity across thousands of customers.

Default quotas for high-end GPUs are deliberately low. A new AWS account typically gets a default of zero for p5 instances (H100 and H200 class). A new Google Cloud project gets zero for a2-ultragpu instances. A new Azure subscription gets zero for ND-series H100 VMs. You cannot provision a single GPU of these types without requesting and receiving a quota increase. Even for older GPU types like A100s, default quotas are often set to single digits.

The quota increase process varies by provider and by how much you are asking for. AWS requires a Service Quotas request through the console, specifying the region, instance type, and desired count. Small requests — going from zero to 8 GPUs — are often approved automatically within hours. Larger requests — 64 or more GPUs — trigger a manual review that involves capacity planning teams and can take one to three weeks. Very large requests — hundreds of GPUs — often require a conversation with your account manager and may involve signing a commitment or demonstrating existing spend. Google Cloud and Azure follow similar patterns, though the interfaces and team names differ.

Region matters enormously. Requesting 32 H100 GPUs in AWS us-east-1 (Virginia) has a higher probability of approval than requesting the same count in ap-southeast-1 (Singapore), because Virginia has far more installed GPU capacity. Some regions simply do not have certain GPU types at all. If your compliance requirements mandate inference in a specific geography, check GPU availability for that region before committing to an architecture that assumes it.

Quotas are also per-GPU-type, not per-GPU-family. Having a quota of 64 for H100 instances does not give you any quota for H200 instances. When NVIDIA releases a new GPU generation and the cloud provider offers instances based on it, you start at zero for the new type and must request increases separately. Teams that planned their 2025 infrastructure around H100 availability discovered in early 2026 that their H200 quotas were zero and had to go through the full request cycle again, sometimes waiting weeks during a period of peak demand for the new hardware.

## Reserved Instances: Commitment for Certainty

The most reliable way to guarantee GPU capacity is through **reserved instances** — contractual commitments where you agree to pay for a fixed amount of capacity for one to three years in exchange for a guaranteed allocation and a significant discount. AWS calls them Reserved Instances and Savings Plans. Azure calls them Reserved VM Instances. Google Cloud calls them Committed Use Discounts. The terminology varies but the economics are the same: commit now, pay less, and guarantee that the capacity exists when you need it.

Discounts on reserved GPU capacity range from 30 to 60 percent compared to on-demand pricing, depending on the commitment length and payment structure. A one-year all-upfront reservation on AWS typically saves 35 to 40 percent. A three-year all-upfront reservation saves 55 to 60 percent. The savings are real and substantial — on a cluster of 64 H100 GPUs running continuously, a three-year reservation versus on-demand pricing can save over $2 million per year. But the commitment is equally real. If your capacity needs change, if a newer GPU generation becomes available, or if your project is canceled, you are still paying for the reserved capacity.

Reserved instances also guarantee allocation in a specific region and availability zone. This is their operational advantage beyond cost. When GPU demand spikes — during model training cycles, during new model releases that trigger fine-tuning waves across the industry — on-demand capacity can become unavailable for weeks. Teams with reservations continue operating. Teams without reservations join a queue.

The downside is rigidity. Reservations lock you into a specific instance type in a specific region. If you reserve 128 H100 GPUs but the market shifts to H200s for your workload, your H100 reservation does not convert. Some providers offer limited exchange programs — AWS allows converting between instance families within the same commitment — but the flexibility is constrained. The best practice is to reserve capacity for your stable, predictable baseline workload and handle variable or experimental workloads through other mechanisms.

## On-Demand and Spot: The Flexibility Spectrum

**On-demand instances** provide GPU capacity with no commitment at full list price. You pay per hour or per second for exactly the time you use, and you can launch and terminate instances as needed. The advantage is pure flexibility. The disadvantage is that on-demand capacity is not guaranteed. During high-demand periods, launching on-demand GPU instances may fail with insufficient capacity errors. In practice, on-demand H100 and H200 availability has been unreliable throughout 2025 and into 2026, particularly in popular regions.

**Spot instances** (called Preemptible VMs on Google Cloud and Spot VMs on Azure) offer the deepest discounts — 60 to 90 percent below on-demand pricing — but with a critical trade-off: the provider can reclaim the instance at any time with minimal notice, typically 30 seconds to two minutes. Spot pricing fluctuates based on supply and demand. When capacity is plentiful, spot prices drop to 10 to 20 percent of on-demand. When demand surges, spot prices rise and availability drops to zero.

Spot instances are viable for AI workloads that can tolerate interruption. Hyperparameter search jobs, where each trial is independent and a reclaimed instance means losing only one trial, fit the spot model well. Evaluation pipeline runs that can be retried from the beginning also work. Distributed training runs can use spot instances if you implement aggressive checkpointing — saving model state every 5 to 15 minutes so that a reclaimed instance costs at most 15 minutes of recompute. Some teams run their entire training infrastructure on spot with a checkpointing cadence of five minutes and report that the 70 percent cost reduction more than compensates for the 5 to 10 percent overhead of frequent checkpointing and occasional restarts.

Spot instances are not viable for real-time inference serving. A user sending a query and getting a "server reclaimed" error is not acceptable for production traffic. Inference workloads belong on reserved or on-demand capacity.

## The GPU Cloud Specialists

When the three hyperscalers — AWS, Azure, and Google Cloud — cannot provide the capacity you need, a growing ecosystem of GPU cloud specialists can. **CoreWeave** built its entire business around GPU-native cloud infrastructure and offers H100 and B200 capacity at prices 30 to 50 percent below hyperscaler on-demand rates. **Lambda Labs** provides H100 instances at roughly $2.99 to $3.79 per GPU-hour, some of the lowest rates in the market, though availability can be constrained during peak periods. **Crusoe Energy** operates GPU data centers powered by stranded natural gas, offering competitive pricing with a sustainability angle. **RunPod** and **Vast.ai** operate marketplace models where independent GPU owners list capacity, creating a spot-like market with high variability in price and availability. **Together AI**, **Anyscale**, and similar platforms offer GPU access bundled with managed training and inference frameworks.

The operational trade-off with specialized providers is ecosystem breadth. AWS gives you GPUs plus S3 plus RDS plus Lambda plus 200 other services in a single network. CoreWeave gives you GPUs and Kubernetes and block storage. If your workload is a standalone training job that reads data from an object store and writes checkpoints back, a GPU specialist provides equivalent capability at lower cost. If your workload requires tight integration with cloud-native databases, serverless functions, or managed ML pipelines, the hyperscaler ecosystem has significant value that a GPU specialist does not replicate.

## Custom Silicon: The Provider-Specific Accelerators

Each major cloud provider now offers proprietary AI accelerators as alternatives to NVIDIA GPUs. AWS offers **Trainium** chips for training and **Inferentia** chips for inference, with Trainium3 announced in late 2025 delivering 2.52 petaflops of FP8 compute per chip. Azure is deploying the **Maia 100** ASIC for AI workloads, with the Maia 200 expected in 2026. Google offers **TPU v6** (codenamed Trillium), claiming 4.7 times the performance of TPU v5e with 67 percent better energy efficiency.

Custom silicon offers price-performance advantages that can be dramatic. AWS Trainium instances for training large language models can cost 40 to 50 percent less than equivalent H100 instances on the same provider. Google TPU pods offer per-token inference costs that compete with the best NVIDIA options. The catch is portability. Code optimized for Trainium uses AWS Neuron SDK. Code optimized for TPU uses JAX with TPU-specific XLA compilation. Code optimized for Maia uses Azure's ML acceleration stack. None of these are interchangeable. Choosing custom silicon means accepting deep lock-in to a specific provider's hardware and software ecosystem in exchange for lower unit costs.

For teams pursuing a multi-cloud strategy, custom silicon creates an interesting architecture. You might run inference on Trainium in AWS for US traffic, on TPU v6 in Google Cloud for Asian traffic, and on H200 in Azure for European traffic. Each region uses the most cost-effective hardware available from the provider that has the best presence in that geography. The cost of this approach is maintaining three different model compilation and serving stacks — a significant engineering investment that only makes sense at scales where the per-unit savings justify the tooling overhead.

## The Mixed-Provider Capacity Strategy

The most resilient approach to GPU capacity planning combines multiple tiers. Reserve base capacity on your primary cloud provider — enough to handle your steady-state inference traffic and scheduled training jobs with a 20 percent buffer. Use on-demand capacity from the same provider for predictable burst periods — model retraining cycles, evaluation sweeps, seasonal traffic increases. Use a GPU cloud specialist as your overflow tier — pre-negotiate pricing and validate your deployment pipeline so that you can burst to the secondary provider within hours rather than days. Keep spot instances for background workloads that are interruption-tolerant.

This tiered strategy means maintaining deployment pipelines for at least two providers, which is the engineering cost discussed in the earlier subchapters on abstraction layers. But the operational benefit is substantial. When your primary provider experiences a capacity crunch — and they will, at least once a year — you have a pre-validated alternative that keeps your workloads running. When a new GPU generation launches and the initial supply is allocated through reservations you did not secure, your secondary provider may have availability that your primary does not.

Capacity planning for AI infrastructure is not a one-time exercise. Review your reservation portfolio quarterly. Monitor spot pricing trends monthly. Re-evaluate secondary provider pricing and availability when contracts come up for renewal. The GPU market in 2026 is more liquid and more competitive than it was in 2024, with more providers, more hardware options, and more pricing models. The teams that treat capacity as a strategic asset rather than a line item to be minimized are the teams that never have to explain to their CEO why the product is down because they could not get enough GPUs.

---

Securing GPU capacity across providers is only half the multi-cloud challenge. The other half is connecting that capacity so data flows between clouds without latency surprises or egress bills that dwarf your compute costs. The next subchapter covers cross-cloud networking — the bandwidth, latency, and cost realities of moving data between providers.
