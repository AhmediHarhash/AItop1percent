# 27.61 â€” Experiment Tracking at Platform Scale: Shared Infrastructure for Multiple Teams

The most common experiment tracking anti-pattern has a name: **The Experiment Silo**. Team A runs MLflow on a VM they spun up eighteen months ago. Team B uses Weights and Biases because their lead researcher prefers it. Team C logs metrics to a custom PostgreSQL schema that one engineer built and only that engineer understands. The data science team tracks experiments in spreadsheets. No team can compare results with another team. No one can find the experiment that produced the model currently serving production traffic. When a production incident requires investigating the model's training provenance -- what data was it trained on, what hyperparameters were used, when was the last eval run before deployment -- the answer is "check with the person who trained it." If that person has left the company, the answer is "we don't know."

The Experiment Silo is not a tooling problem. It is an organizational failure that becomes an infrastructure problem. Every team that runs its own tracking system creates an island of metadata that is invisible to the rest of the organization. The cost is not obvious when you have two teams and five experiments per week. It becomes catastrophic when you have twelve teams, hundreds of experiments per month, and regulatory requirements to demonstrate model provenance for every system that touches customer data.

## Why Centralized Experiment Tracking Matters

Centralized experiment tracking serves four distinct purposes that siloed systems cannot fulfill, and each one becomes more critical as the organization scales.

The first is **cross-team reproducibility**. When Team B wants to build on work Team A started -- extending a model, trying a different architecture on the same dataset, comparing approaches to the same problem -- they need access to Team A's experiment records. Not just the final model, but the hyperparameters, the dataset version, the training configuration, the eval results at each checkpoint. Without this, Team B starts from scratch. They re-discover decisions Team A already made. They repeat experiments Team A already ran. In a large organization with multiple teams working on related problems, this duplication wastes thousands of GPU hours per year.

The second is **production lineage**. Every model in production should be traceable back to the exact experiment that produced it. What dataset version was used. What hyperparameters. What eval scores were recorded before deployment. What other experiments were compared and why this one was chosen. This lineage is not a nice-to-have -- it is a regulatory requirement under the EU AI Act for high-risk systems, and it is a practical necessity for any team that needs to debug production quality issues. When a production model starts producing bad results and the team cannot determine what data it was trained on, the debugging process becomes archaeology instead of engineering.

The third is **cost attribution**. Training experiments consume GPU hours, and GPU hours cost money. A centralized tracking system that records the infrastructure resources consumed by each experiment enables cost attribution per team, per project, and per experiment. Without this, the infrastructure bill is a single number that no one can break down. With it, leadership can identify which projects are generating ROI on their compute investment and which are burning GPU hours without producing deployable models. This visibility changes how teams prioritize their work.

The fourth is **institutional knowledge**. Experiments that fail teach as much as experiments that succeed -- if the failure is recorded. A centralized system where every experiment is logged, including the ones that did not work, builds an organizational memory of what has been tried, what worked, what failed, and why. A new engineer joining the team can search the experiment history and learn in hours what took the previous team months to discover.

## Platform Options in 2026

The experiment tracking landscape has matured significantly. By 2026, there are several production-grade options, each with distinct strengths and trade-offs.

**MLflow** remains the most widely adopted open-source platform, with over 20,000 GitHub stars and millions of monthly downloads. Its strength is flexibility: it runs on any infrastructure, integrates with any training framework, and stores experiment metadata in a backend database with artifacts in object storage. Databricks offers a managed MLflow service, but self-hosted MLflow on Kubernetes is the standard deployment for organizations that want to control their data. MLflow's weakness at scale is query performance -- organizations with millions of runs and billions of metric data points report that the default SQLite backend and even PostgreSQL backends struggle with complex queries across large experiment histories without careful indexing and archival strategies.

**Weights and Biases** is the dominant SaaS option, serving over 200,000 practitioners at organizations including OpenAI, Toyota, and Samsung. Its strength is the user experience -- the dashboard, visualization tools, and collaboration features are polished and opinionated in ways that researchers find productive. In 2025, Weights and Biases vertically integrated with CoreWeave's GPU cloud, signaling a move toward end-to-end training infrastructure. The trade-off is that your experiment data lives on their infrastructure. For organizations with strict data sovereignty requirements or regulatory constraints on where training metadata can reside, this is a blocker.

**Neptune** has carved out a niche as an enterprise-grade metadata database built for extreme scalability. Organizations running foundation model training -- experiments with millions of metric data points per run -- report that Neptune handles the query load more gracefully than MLflow at comparable scale. **ClearML** offers an open-source alternative with strong pipeline orchestration features. **Aim** provides a lightweight, performance-focused option for teams that find MLflow too heavy and Weights and Biases too opinionated.

The choice is not permanent. What matters more than the specific tool is the organizational commitment to a single, centralized system. A mediocre tool used consistently by every team beats an excellent tool used by half the teams while the other half tracks experiments in notebooks.

## What to Track: The Non-Negotiable Fields

Every experiment record should capture four categories of information, and the tracking system should enforce that these categories are populated -- not as an optional best practice but as a hard requirement that blocks experiment submission if the fields are empty.

**Training configuration** includes every parameter that determines how the model trains: learning rate, batch size, optimizer type, weight decay, number of epochs or steps, warmup schedule, gradient accumulation steps, distributed training strategy (FSDP, DeepSpeed ZeRO stage), precision (FP32, BF16, FP8), and any framework-specific settings. The goal is that anyone reading this record can reproduce the training run exactly by using the recorded configuration.

**Data provenance** includes the dataset name, version hash, split ratios, any preprocessing or filtering applied, the number of examples in each split, and the data pipeline version. If the training data is assembled from multiple sources -- a common pattern for fine-tuning -- each source should be recorded with its version and contribution percentage. Cross-reference Section 12 for dataset versioning practices.

**Evaluation metrics** include every metric computed during and after training: training loss per step, validation loss per evaluation interval, task-specific metrics (accuracy, F1, BLEU, ROUGE, human preference scores), and any custom metrics the team defines. Metrics should be logged at regular intervals during training, not just at the end. The ability to compare loss curves between experiments is one of the most valuable features of any tracking system.

**Infrastructure metadata** includes the GPU type and count, the cluster or cloud region, wall-clock training time, total GPU hours consumed, peak memory utilization, and estimated cost. This category enables cost attribution and helps teams understand the relationship between infrastructure choices and training outcomes.

## Experiment-to-Production Lineage

The highest-value capability of a centralized tracking system is the ability to trace from a production model backward through the entire chain: production model, model registry entry, training experiment, dataset version, evaluation results, and approval decision. This chain is called **experiment-to-production lineage**, and it is the single feature that separates experiment tracking from experiment tracking as a platform capability.

Building this lineage requires integration points. The experiment tracker must assign a unique identifier to each training run. The model registry must record that identifier when a trained model is registered. The deployment system must record the model registry version when a model is deployed to production. The monitoring system must record the deployed model version alongside production metrics. Each link in the chain is a foreign key that connects two systems.

When a production quality issue arises -- a customer reports degraded output, a monitoring alert fires, an evaluator flags a regression -- the lineage chain lets you answer the critical questions in minutes rather than days. Which experiment produced this model? What data was it trained on? Has that data been updated since training? What were the eval scores before deployment? Did the scores change between the training eval and the pre-deployment eval? Has someone modified the model's configuration since deployment? Without lineage, each of these questions requires manual investigation across multiple systems. With lineage, they are database lookups.

## Multi-Tenant Architecture

A platform-scale tracking system serves multiple teams with different needs, different access levels, and different compliance requirements. The multi-tenant architecture must provide three properties simultaneously: isolation, shared infrastructure, and centralized administration.

**Isolation** means each team's experiments are visible only to that team by default. A research team exploring experimental architectures should not have their in-progress results visible to the product team. An internal red-teaming group should not have their adversarial experiments visible to the broader organization. Isolation is implemented through project or workspace abstractions with role-based access control -- team members see their team's experiments, team leads see their team's and cross-team shared experiments, platform administrators see everything.

**Shared infrastructure** means all teams use the same storage, the same database, and the same query engine. This sharing enables cross-team comparison (when access is explicitly granted), reduces operational overhead (one system to maintain, not twelve), and enables organization-wide analytics (total GPU hours consumed, experiments per team, time from experiment to production).

**Centralized administration** means the platform team can enforce organization-wide policies: mandatory metadata fields, retention policies for old experiments, storage quotas per team, and compliance requirements like automatic tagging of experiments that use personal data. Without centralized administration, each team's compliance posture is ad hoc -- some teams are rigorous, others are not, and the organization's overall compliance posture is only as strong as its weakest team.

## Scale Challenges

A single researcher running ten experiments per week generates modest metadata. An organization with fifty researchers, each running ten to twenty experiments per week, with each experiment logging hundreds of thousands of metric data points across thousands of training steps, generates a volume that tests any tracking system's architecture.

The primary bottleneck is **metric storage and query performance**. A single training run that logs loss and five evaluation metrics every hundred steps for a run of one million steps produces sixty thousand metric data points. Multiply by a thousand runs per month and you have sixty million new data points monthly. Querying this data -- "show me all experiments in the past three months where final validation loss was below 2.0 and the learning rate was between 1e-4 and 5e-4" -- requires an indexing strategy that most default configurations do not provide.

The practical solutions are tiered storage and materialized summaries. Raw step-level metrics are stored in a time-series database or object storage and queried only when a user drills into a specific experiment. Summary metrics -- final values, best values, key percentiles -- are materialized into a relational database that supports fast queries across the entire experiment catalog. This two-tier approach lets teams browse and filter thousands of experiments interactively while still having access to full granularity when they need it.

**Artifact storage** is the other scaling challenge. Each experiment may produce checkpoint files, evaluation outputs, configuration files, and log files. A large training run's checkpoints alone can be tens or hundreds of gigabytes. Multiply by hundreds of experiments per month and artifact storage becomes a cost center that requires explicit lifecycle management. The tracking system should integrate with the checkpoint retention policy described in the previous subchapter -- expired checkpoints are deleted from the artifact store automatically, with only the metadata record retained for lineage purposes.

## Integration with the Training Pipeline

Experiment tracking delivers the most value when it is automatic -- when submitting a training job through the cluster's scheduling system (Kueue, Volcano, or a custom scheduler) automatically creates an experiment record, logs the training configuration and infrastructure metadata, and links the experiment to the resulting checkpoint and model artifacts. Manual experiment logging is better than no logging, but it depends on individual discipline and degrades under deadline pressure.

The integration pattern is an agent that runs alongside the training job. When the job scheduler launches a training pod, a sidecar container or an initialization hook registers the experiment with the tracking system, using the job's configuration as the experiment parameters. During training, the training framework's logging callbacks push metrics to the tracking system's API. At job completion, the agent records the final metrics, links the checkpoint artifacts, and marks the experiment as complete. If the job fails, the agent records the failure mode and the partial metrics -- failed experiments are as valuable for organizational learning as successful ones.

This automation ensures that every training job that runs on the cluster is tracked, regardless of which team submitted it, which framework it uses, or whether the researcher remembered to add logging code. The platform handles the tracking. The researcher focuses on the research.

---

Experiment tracking tells you what has been tried and what worked. But many production AI systems do not train once and deploy forever -- they need to retrain regularly as data drifts, user behavior shifts, and quality degrades. The next subchapter examines continuous fine-tuning pipelines: the infrastructure that supports recurring model updates on a schedule, triggered by quality degradation, or both.
