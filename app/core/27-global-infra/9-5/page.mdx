# 27.68 â€” FinOps for AI Infrastructure: Cost Attribution to Teams, Projects, and Workloads

In September 2025, the VP of Finance at a mid-size healthcare AI company walked into the platform team's standup with a single question: where is the $280,000 per month GPU bill going? The company ran fourteen AI models across three product lines. Six teams used the shared GPU cluster for training, fine-tuning, and inference. The monthly cloud bill had grown from $60,000 to $280,000 in eleven months, and nobody could attribute more than a third of it to specific teams or projects. Finance needed to cut 20 percent -- roughly $56,000 per month -- to meet quarterly targets. Engineering could not identify what to cut. The training team insisted their workloads were essential. The inference team argued they were already running lean. A data science team that had spun up experimental fine-tuning jobs six weeks earlier had forgotten to shut them down, but nobody knew because no dashboard tracked per-team spend. Without cost attribution, the company ended up slashing the wrong budget line: they reduced the number of reserved GPU instances used by their production inference pipeline, causing latency spikes that triggered an SLA breach with their largest customer. The penalty cost more than the savings.

This is the **FinOps problem** for AI infrastructure, and it is fundamentally different from FinOps for traditional cloud workloads. In a CPU-based microservices environment, cost attribution is relatively straightforward -- each service runs in its own deployment, each deployment maps to a team, and cloud billing tags trace cost to owner. GPU infrastructure shatters that simplicity because GPUs are shared, expensive, and consumed in patterns that do not map neatly to the team boundaries your organization chart defines.

## Why AI FinOps Is Different from Traditional Cloud FinOps

Traditional FinOps assumes that the dominant cost categories are compute, storage, and network, and that each workload runs on resources that can be individually tagged and tracked. AI infrastructure breaks these assumptions in three ways.

First, GPU costs dominate everything else. In a typical AI-heavy organization, GPU and accelerator spend accounts for 70 to 85 percent of total infrastructure cost. Storage, networking, CPU, and memory combined make up the remaining 15 to 30 percent. This concentration means that traditional FinOps dashboards that carefully track S3 storage costs and data transfer fees while treating "compute" as a single line item miss the category that actually matters. A one percent optimization in GPU utilization saves more money than eliminating half your storage bill.

Second, GPUs are shared in ways that CPUs are not. Time-slicing lets multiple workloads share a single GPU. Multi-Instance GPU partitioning on NVIDIA hardware splits a physical GPU into isolated slices. Inference servers batch requests from multiple teams through a single model deployment. Training clusters assign GPUs dynamically through schedulers like Kueue that allocate capacity based on queue priority, not static reservation. When three teams share a pool of 64 GPUs, and a single training job uses 16 of them for eight hours while inference pods consume fragments of the remaining 48, attributing cost to each team requires tracking at a granularity that most monitoring systems do not provide by default.

Third, cost per query varies enormously depending on the model, the input size, and the generation length. An inference request to a 7-billion-parameter model costs a fraction of a cent. The same request to a 70-billion-parameter model costs ten to twenty times more. A request that generates 50 tokens costs half as much as one that generates 500. A batch training job that runs overnight on spot instances costs 60 to 70 percent less per GPU-hour than the same job running on-demand during business hours. Without model-level and workload-level cost tracking, the aggregate GPU bill is meaningless for decision-making.

## The Cost Attribution Stack: Labels, Namespaces, and Queues

Effective cost attribution for AI infrastructure requires tagging at three levels, each serving a different audience.

**Kubernetes labels** are the foundation. Every pod, job, deployment, and persistent volume claim should carry a standard set of labels that identify the owning team, the project or product, the environment (development, staging, production), the model being served or trained, and the workload type (training, fine-tuning, inference, evaluation). These labels are the raw material that cost allocation tools consume. Without consistent labeling, no tool can attribute costs accurately. The most common failure mode is inconsistency -- one team labels their pods with "team: ml-platform" while another uses "owner: data-science" and a third uses no team label at all. Enforce label requirements through admission webhooks that reject any workload missing mandatory labels. This is not optional governance. It is the prerequisite for everything else in this chapter.

**Namespace-based grouping** provides coarser attribution that maps naturally to organizational boundaries. Assign each team or product line its own namespace. Namespace-level resource quotas provide both cost control and attribution -- if the data science team's namespace is limited to 16 GPUs, their maximum possible spend has a ceiling. Namespace costs are easy to calculate and easy to explain to non-technical stakeholders. The downside is that shared services -- a centralized inference gateway that serves multiple products, a shared training queue that multiple teams submit to -- do not map cleanly to a single namespace. Shared services require secondary attribution mechanisms.

**Queue-based tracking through Kueue** provides the finest-grained attribution for batch workloads. Kueue assigns every training and fine-tuning job to a named queue, and each queue maps to a team or project. Because Kueue tracks exactly which GPUs were allocated to which job for exactly how long, the attribution is precise to the GPU-minute. Queue-based tracking captures the actual consumption pattern rather than the static allocation, which matters enormously for shared clusters where GPUs are dynamically assigned and reclaimed.

## Chargeback Versus Showback

Once you can attribute costs to teams, you face a policy decision: do teams pay for what they consume, or do they just see what they consume?

**Showback** means the platform team publishes cost dashboards showing each team how much GPU time, storage, and network bandwidth they consumed, translated into dollar amounts. The money still comes from a central infrastructure budget. Teams see their costs but do not directly feel them in their budgets. Showback is easier to implement and less politically contentious. It creates awareness without accountability. In practice, showback changes behavior modestly -- teams notice their costs, occasionally optimize the obvious waste, but rarely make hard trade-offs because the money is not coming out of their budget.

**Chargeback** means each team's infrastructure consumption is deducted from their own budget. If the recommendation team consumes $45,000 in GPU time this month, that $45,000 comes out of the recommendation team's engineering budget. Chargeback creates direct financial accountability. Teams that pay for their own GPU time make different decisions: they right-size their model deployments, they shut down experiments when they are done, they choose smaller models when quality permits, they schedule training during off-peak hours to use cheaper spot capacity. Chargeback drives behavior change faster and more reliably than showback.

The trade-off is organizational friction. Chargeback requires agreement on pricing -- how much does a GPU-hour cost? Does the price include the platform team's operational overhead? Do teams pay the on-demand rate or the blended rate that accounts for reserved instances and spot savings? These pricing decisions are contentious and require executive alignment. Most organizations start with showback to build visibility and trust, then transition to chargeback once the cost data is stable and the pricing model is agreed upon. The transition typically takes six to twelve months.

## GPU Cost Allocation Models

Within either chargeback or showback, you must decide how to calculate each team's GPU cost. Three models exist, each with different trade-offs.

**Allocation-based costing** charges teams for the GPU time they reserved, regardless of whether they used it. If a team requests four GPUs for a training job that runs for eight hours, they are charged for 32 GPU-hours even if the GPUs were only 40 percent utilized during that time. This model is simple to implement and simple to explain. It also incentivizes teams to right-size their requests because over-provisioning costs them directly. The downside is that teams may feel it is unfair when the scheduler forces them to reserve more GPUs than they need due to bin-packing constraints, or when their jobs are waiting in a queue and still being charged for reserved but idle capacity.

**Utilization-based costing** charges teams for actual GPU-seconds consumed, measured through GPU utilization metrics. If a team reserved four GPUs for eight hours but only used 60 percent of the available compute, they pay for 19.2 GPU-hours instead of 32. This model is fairer from the consumer's perspective, but it creates a perverse incentive: teams have no reason to release GPUs they are not fully using because idle GPUs do not cost them. It also penalizes workloads that are memory-bound rather than compute-bound -- a model that needs 80 gigabytes of GPU memory but only uses 30 percent of the compute cores is consuming a scarce resource but appears cheap under utilization-based costing.

**Request-based costing** charges per inference request or per training step, with the cost determined by the model size and the request complexity. This model is the most intuitive for application teams because it maps directly to their usage patterns -- they pay per prediction, per generation, per training epoch. It is also the hardest to implement because it requires accurate measurement of per-request GPU time, which varies with input length, generation parameters, batching efficiency, and queuing delays. Request-based costing works best for inference workloads where per-request metering is already built into the serving infrastructure.

The pragmatic recommendation is to use allocation-based costing for training and fine-tuning workloads, where jobs reserve dedicated GPUs for defined durations, and request-based costing for inference workloads, where shared serving infrastructure processes requests from multiple teams. This hybrid approach matches the natural consumption pattern of each workload type.

## Tools That Make Attribution Possible

The FinOps tooling ecosystem for Kubernetes and GPU workloads matured significantly through 2024-2025, and by 2026 several tools provide production-grade cost attribution.

**OpenCost**, the open-source CNCF project originally developed by Kubecost, provides real-time cost allocation for Kubernetes workloads including GPU resources. OpenCost reads resource requests and utilization metrics from the Kubernetes API and cloud billing data, then attributes costs to namespaces, labels, and individual pods. GPU utilization tracking was added in 2025 with support for NVIDIA GPU metrics through DCGM, allowing cost attribution at the per-GPU, per-pod level. OpenCost's strength is its vendor-neutral, open-source foundation. Its limitation is that it does not account for negotiated discounts, spot pricing, or committed-use savings without additional configuration.

**Kubecost**, the commercial product built on OpenCost, extends the open-source foundation with GPU-specific cost monitoring, cost anomaly detection, savings recommendations, and multi-cluster aggregation. Kubecost's 2.4 release brought detailed NVIDIA GPU utilization into the cost allocation model, allowing teams to see not just which GPUs were reserved but how effectively they were used. For organizations running multiple clusters across regions or cloud providers, Kubecost's unified view provides the aggregation layer that OpenCost alone does not.

**CAST AI** moved into GPU cost optimization in 2025 with features for GPU sharing through time-slicing and Multi-Instance GPU, automated right-sizing of GPU requests, and a unified compute control plane that launched in early 2026. CAST AI's approach is more active than passive -- rather than just reporting costs, it automatically adjusts resource allocations to reduce waste. Cloud provider native tools -- AWS Cost Explorer with SageMaker cost breakdowns, Google Cloud's GPU cost reporting, Azure Cost Management with ML-specific views -- provide attribution within a single cloud but lack the cross-cloud and Kubernetes-native granularity that dedicated tools provide.

## The FinOps Operating Cycle

Tools provide data. The FinOps discipline provides the operating cadence that turns data into decisions.

The **Inform phase** builds visibility. Deploy cost attribution tooling, ensure all workloads are labeled, publish dashboards showing per-team and per-project GPU spend, and establish a weekly cost report that goes to engineering leadership. The goal is not to cut costs immediately -- it is to make costs visible so that everyone understands the baseline. Most organizations discover surprising things in this phase: zombie workloads that nobody owns consuming thousands of dollars per month, development environments running production-sized GPU allocations, training jobs that completed weeks ago but left allocated resources running.

The **Optimize phase** acts on the visibility. Right-size GPU allocations based on utilization data. Move interruptible workloads to spot instances, which reduce GPU costs by 60 to 70 percent for training jobs that can tolerate preemption. Consolidate inference models onto fewer GPUs through batching and model co-location. Negotiate reserved capacity for predictable baseline workloads. Each optimization has a measured impact in dollars saved, which builds organizational confidence in the FinOps practice.

The **Operate phase** makes FinOps continuous rather than periodic. Set per-team and per-project GPU budgets with automated alerts at 80 percent and 100 percent thresholds. Implement governance policies that require cost estimates for new workloads before they are approved. Run monthly FinOps reviews where teams present their GPU spend, justify significant increases, and commit to optimization targets. The FinOps Foundation launched its FinOps for AI certification program in 2025, and by 2026 the practice is mature enough that roles specifically focused on AI cost management exist at companies spending more than $100,000 per month on GPU infrastructure.

## Cost Anomaly Detection

Even with budgets and dashboards, unexpected cost spikes happen. A misconfigured autoscaler that scales inference to 128 GPUs instead of 12. A training job submitted with the wrong resource request that allocates eight GPUs per worker instead of one. A development notebook running on a production GPU node that nobody noticed for three weeks.

**Cost anomaly detection** uses historical spend patterns to establish baselines and alerts when actual spend deviates beyond a configurable threshold. Kubecost and cloud provider tools offer built-in anomaly detection, but you can build simple version using the same time-series data that powers your dashboards. Track daily GPU spend per team and per project. Compute a rolling 14-day average and standard deviation. Alert when any single day exceeds two standard deviations above the rolling average. This catches gradual creep -- spend that increases 5 percent per week until it has doubled without anyone noticing -- as well as sudden spikes from misconfiguration or runaway jobs.

The alert should go to both the platform team and the owning team simultaneously. The platform team investigates whether the spike is an infrastructure issue -- autoscaler misconfiguration, scheduling bug, resource leak. The owning team investigates whether the spike is an application issue -- new model deployment, changed traffic pattern, experimental workload that was not supposed to run this long. Between the two teams, most anomalies are explained and resolved within hours rather than accumulating into a surprise on the monthly bill.

---

Cost attribution tells you where the money goes. But the platform's value to the organization depends on more than cost efficiency -- it depends on reliability. The next subchapter defines the SLOs that the infrastructure team promises to application teams, and the measurement systems that hold both sides accountable.
