# Chapter 6 — Multi-Cloud and Hybrid Strategies

Every cloud provider wants to be your only cloud provider. Their managed AI services, their proprietary GPU instances, their custom networking fabric, their model hosting platforms are all designed to make leaving expensive and staying easy. For web infrastructure, single-cloud commitment is a defensible choice. The switching costs are manageable and the vendor risk is tolerable. For AI infrastructure, single-cloud commitment is a strategic liability. GPU availability shifts quarterly. A provider that had abundant H100 capacity in January may have a six-month waitlist by July. Pricing models change with little warning. And when your entire inference fleet depends on one provider's GPU allocation, a capacity crunch in your region does not just degrade performance. It stops your product.

The alternative is multi-cloud or hybrid deployment, and it comes with its own costs. Every abstraction layer you build for portability is a layer of performance you sacrifice. Kubernetes provides container portability, but GPU scheduling, networking, and storage behave differently on every provider. A model serving stack tuned for AWS Inferentia does not transfer to Google Cloud TPUs without significant rework. The infrastructure-as-code that provisions your GKE cluster does not provision your EKS cluster without a rewrite. Multi-cloud is not free portability. It is a tradeoff between vendor risk and engineering complexity, and the right answer depends on your scale, your GPU requirements, and how much operational overhead your team can absorb.

This chapter navigates that tradeoff honestly. You will learn where abstraction layers pay for themselves and where they create more problems than they solve, how to evaluate on-premises GPU economics against cloud pricing, how to build unified control planes that work across providers without lowest-common-denominator compromises, and how GitOps practices keep heterogeneous infrastructure from becoming unmanageable.

---

- 6.1 — The Multi-Cloud Reality: Lock-In Risk and GPU Supply Volatility
- 6.2 — Abstraction Layers for Model Portability
- 6.3 — On-Premises and Private Cloud GPU Economics
- 6.4 — Cloud Provider GPU Availability and Capacity Planning
- 6.5 — Cross-Cloud Networking: Latency, Bandwidth, and Egress Costs
- 6.6 — Unified Control Planes for Heterogeneous Infrastructure
- 6.7 — GitOps and Infrastructure-as-Code for AI Platforms

---

*By the end of this chapter, you will know exactly where multi-cloud abstraction earns its cost and where single-provider depth is the smarter bet for your team, your workload, and your budget.*
