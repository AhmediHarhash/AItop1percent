# 27.63 â€” Training-to-Serving Handoff: From Checkpoint to Production Endpoint

A training job completes. The final checkpoint sits in object storage -- a collection of tensor files, optimizer states, and configuration metadata totaling tens or hundreds of gigabytes. Between that checkpoint and a production endpoint serving real user traffic, there are twelve distinct steps. Most teams discover them one at a time, usually when something breaks. The serving engine rejects the checkpoint format. The quantized model produces garbled output on inputs the full-precision model handled perfectly. The model registry has no record of the training run that produced the checkpoint. The canary deployment reveals a 15 percent latency regression that nobody checked for during eval. Each of these failures is avoidable, but only if the handoff is treated as an engineered pipeline rather than a series of manual steps.

The training-to-serving handoff is the boundary between two entirely different engineering domains. Training infrastructure cares about throughput, fault tolerance, and efficient use of many GPUs for a single long-running job. Serving infrastructure cares about latency, concurrency, and efficient use of fewer GPUs for millions of short requests. The model that worked perfectly in one domain must be translated for the other, and the translation introduces risks that neither domain's tooling catches by default.

## The Twelve Steps of Handoff

The full handoff pipeline, from training completion to production traffic, includes twelve steps that should execute in sequence, each gating the next. Skipping steps does not save time -- it borrows time from the future and repays it with interest during production incidents.

First, **checkpoint validation** -- the integrity checks described in the previous subchapters. Verify checksums, validate tensor distributions, run validation inference. Do not proceed if any check fails. Second, **model export** -- convert the checkpoint from the training framework's native format to the serving engine's required format. Third, **quantization** -- reduce the model's numerical precision from training precision (FP32 or BF16) to serving precision (INT8, FP8, or mixed precision) if inference cost or latency requires it. Fourth, **model registration** -- record the exported, quantized model as a versioned artifact in the model registry with full metadata linking it to its training experiment. Fifth, **serving configuration** -- define the runtime parameters: maximum batch size, request timeout, maximum concurrent requests, memory allocation, and model-specific settings like maximum sequence length.

Sixth, **deployment to staging** -- deploy the configured model to a staging environment that mirrors production infrastructure but serves no external traffic. Seventh, **evaluation suite execution** -- run the full evaluation suite against the staged model, comparing results against both the training eval scores and the current production model's scores. Eighth, **canary deployment** -- route a small percentage of production traffic to the new model while monitoring quality and latency metrics. Ninth, **progressive rollout** -- gradually increase the traffic percentage as the canary metrics confirm acceptable performance. Tenth, **monitoring setup** -- ensure that production dashboards, alerts, and logging are configured for the new model version. Eleventh, **documentation** -- record the deployment decision, the eval results, any configuration changes, and the rollout timeline. Twelfth, **team notification** -- inform the teams that depend on the model that a new version is in production.

The first five steps are engineering automation. The middle four are operational process. The last three are organizational hygiene. All twelve matter.

## The Model Registry: Where Training Meets Serving

The **model registry** is the central artifact store that bridges the training and serving worlds. It is not the same as the experiment tracker, though the two are tightly integrated. The experiment tracker records every training run, including the ones that produced bad models. The model registry records only the models that are candidates for production -- the ones that passed evaluation and are available for deployment.

Every entry in the model registry includes a unique version identifier, the training experiment ID (linking back to the experiment tracker for full provenance), the dataset version used for training, the evaluation scores at the time of registration, the model format (original, exported, quantized), the author or automated pipeline that registered it, and an approval status (pending, approved, rejected, deprecated). The approval status is a gate: only models with "approved" status can be deployed to production. The approval can be automatic (the eval suite passes, the model is approved) or manual (a human reviews the eval results and approves), depending on the organization's risk tolerance and regulatory requirements.

The model registry also maintains the history of which version is currently deployed, which versions were previously deployed, and when each transition occurred. This history is essential for rollback -- if the current production model causes problems, the registry tells you exactly which previous version to restore. It is also essential for audit -- regulators and internal governance teams can trace the complete lifecycle of any production model from training data through deployment.

MLflow's model registry, SageMaker Model Registry, and custom registries built on object storage with a metadata database are all viable implementations. The specific tool matters less than the discipline of using it: every production model enters through the registry, every deployment references a registry version, and no model reaches production without a registry entry.

## Format Conversion: Where Subtle Bugs Hide

Training frameworks and serving engines speak different languages. PyTorch saves checkpoints as collections of tensor files in its native serialization format. Hugging Face Transformers uses its own checkpoint structure with configuration JSON files. DeepSpeed saves sharded checkpoints with a different layout than FSDP. On the serving side, vLLM expects models in Hugging Face format or GGUF. TensorRT-LLM requires models compiled into TensorRT engine files. NVIDIA Triton Inference Server supports multiple backends, each with its own format requirements.

The format conversion step transforms the training checkpoint into the serving engine's expected format. This sounds mechanical, and for common model architectures with well-tested conversion scripts, it usually is. But conversion is where subtle bugs hide because the conversion code must map every tensor name, every dimension ordering, and every data type exactly right. A transposed weight matrix, a mismatched embedding dimension, a layer normalization parameter assigned to the wrong layer -- any of these produces a model that loads without error, runs inference without crashing, and produces outputs that are subtly wrong in ways that only become apparent through evaluation.

The defense is simple: never skip evaluation after conversion. Run the same evaluation suite against the converted model that you ran against the original checkpoint. The results should be identical within floating-point precision tolerances (typically less than 0.1 percent deviation on any metric). If the converted model's eval scores diverge from the original by more than the tolerance, the conversion introduced a bug. Do not deploy.

For teams that convert frequently between the same source and target formats, the conversion step should be a tested, versioned pipeline component -- not a script someone runs manually. Version the conversion code, test it against known-good checkpoint-to-model pairs, and alert when a conversion produces results outside the expected tolerance.

## Quantization as a Handoff Step

Production inference often runs at lower numerical precision than training. Training in FP32 or BF16 uses 32 or 16 bits per parameter. Production serving in INT8 uses 8 bits. FP8, increasingly supported by modern GPU architectures like NVIDIA H100 and H200, uses 8 bits with floating-point representation. FP4 pushes to 4 bits for the most cost-sensitive deployments. Each reduction in precision reduces memory footprint, increases inference throughput, and decreases per-request cost -- but at the risk of quality degradation.

Quantization happens during the handoff, not during training. The model is trained at full precision to maximize quality, then quantized for serving to maximize efficiency. The quantization step must be followed by evaluation against the full eval suite. A model that scores 91 percent accuracy at BF16 might score 90.5 percent at INT8 -- an acceptable trade-off -- or it might score 84 percent because a particular layer is sensitive to quantization and the calibration data did not cover the relevant input distribution. You will not know which outcome you get until you evaluate.

NVIDIA's Model Optimizer (formerly TensorRT Model Optimizer) provides tools for post-training quantization and quantization-aware training that target both TensorRT-LLM and vLLM backends. For Hugging Face models, the AutoGPTQ and bitsandbytes libraries handle quantization with different trade-offs between speed and quality. The choice of quantization method, calibration dataset, and target precision should be documented in the model registry alongside the quantized model artifact, because a production model's quality depends on its quantization configuration as much as its training configuration.

## Automated Versus Manual Handoff

For continuous fine-tuning pipelines where the changes are incremental -- a weekly LoRA adapter update, a monthly retraining on refreshed data -- the entire handoff should be automated. The pipeline runs without human intervention: training completes, the checkpoint is validated, the model is exported and quantized, eval scores are computed, the model is registered, and if the quality gate passes, deployment proceeds through canary and progressive rollout. A human is notified of the deployment but does not need to approve it. The quality gate provides the approval.

For major model changes -- upgrading to a new base model, switching architectures, adding new capabilities, retraining on a significantly different dataset -- human review is appropriate. A human reviews the evaluation results, compares them against the current production model, examines edge cases, and makes an explicit approval decision. The handoff pipeline still automates the mechanical steps (conversion, quantization, staging deployment, eval execution), but the deploy-to-production step waits for human sign-off.

The boundary between automated and manual handoff is a policy decision that should be explicit and documented. One common pattern defines a threshold: if the new model's eval scores differ from the current production model's scores by less than a defined percentage on all metrics, automated handoff proceeds. If any metric differs by more than that threshold -- improvement or regression -- human review is triggered. This pattern automates routine updates while ensuring human oversight for significant changes.

## The Handoff Contract

Every handoff pipeline should be governed by a documented contract that specifies the inputs, outputs, and quality criteria for each step. The contract answers questions that teams otherwise answer ad hoc: what checkpoint format does the pipeline accept? What serving format does it produce? What evaluation suite must pass? What quantization configurations are supported? What is the maximum acceptable latency for the quantized model? What approval is required for production deployment?

The contract is not a document that sits in a wiki unread. It is encoded in the pipeline itself -- as configuration files, as automated validation checks, as quality gate thresholds. When the contract is violated (an unsupported checkpoint format is submitted, an eval score falls below the threshold, the quantized model exceeds the latency budget), the pipeline halts and surfaces the violation. The contract makes the handoff deterministic: given valid inputs, the pipeline produces valid outputs or a clear failure signal. There is no ambiguous middle ground where a partially-valid model might reach production because someone decided to skip a step.

## Cross-Referencing Deployment and Release Practices

The training-to-serving handoff does not exist in isolation. It feeds into the deployment patterns described in Section 19 -- canary deployment, blue-green switching, progressive rollout -- and must pass the release gates described in Section 18 -- regression testing, quality thresholds, rollback criteria. The handoff pipeline's output is a deployment-ready artifact with a known quality profile. The deployment infrastructure's job is to get that artifact into production safely.

The integration point is the model registry. The handoff pipeline registers the deployment-ready model. The deployment pipeline reads from the registry. The registry acts as the handoff point where training infrastructure's responsibility ends and serving infrastructure's responsibility begins. Both sides reference the same artifact, the same version, and the same metadata. The model's journey from training to production is traceable, reproducible, and reversible at every step.

---

Training infrastructure -- from cluster architecture through job scheduling, checkpointing, experiment tracking, continuous fine-tuning, and the handoff to production -- forms the foundation on which every AI model is built. But building the foundation is not the same as operating it reliably over time. The next chapter shifts from constructing the platform to running it: the observability practices, cost attribution systems, on-call structures, and maturity models that determine whether your AI infrastructure is a shared experiment or a production-grade internal product.
