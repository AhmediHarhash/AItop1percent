# 27.13 — Node Pool Design: Heterogeneous Hardware and Accelerator Diversity

The era of homogeneous GPU clusters is over. By 2026, production AI platforms manage three to five different accelerator types simultaneously, and that heterogeneity is a feature, not a bug. The teams running a single GPU model across their entire fleet are either very early in their journey or spending far more money than they need to.

The reason is economic and practical. GPU hardware evolves on a roughly annual cycle. An organization that bought A100-40GB instances in 2022, added A100-80GB in 2023, moved to H100 in 2024, and started acquiring H200 or B200 capacity in 2025 now has four generations of hardware in its fleet. None of the older hardware disappeared. The A100s still work. They still serve inference workloads reliably. They are simply less cost-effective per token-per-second than the newer generations for some workloads and more cost-effective for others. Throwing them away and standardizing on the latest hardware is wasteful. Keeping them and routing workloads intelligently is engineering.

Add to this the growing multi-vendor reality. AMD's Instinct MI300X entered production Kubernetes environments in 2025 with mature ROCm support and its own device plugin. Intel's Gaudi accelerators offer competitive price-performance for specific inference patterns. Google's TPUs dominate certain training workloads on GKE. A platform that only understands NVIDIA GPUs is leaving performance and cost optimization on the table. Designing node pools that accommodate this diversity — and that make the diversity invisible to the teams deploying models — is the core challenge of modern GPU infrastructure.

## Group by Capability Class, Not by GPU Model

The first instinct when designing node pools is to create one pool per GPU model — an A100-40GB pool, an A100-80GB pool, an H100 pool, an H200 pool. This seems logical. Each model has different specs. Each needs different drivers. Each offers different performance. But this approach creates a scheduling nightmare as your fleet grows. Teams writing deployment manifests must know which GPU model their workload needs. Changes to the fleet — retiring old hardware, adding new hardware — require updating every deployment that referenced the old model. The scheduler cannot make intelligent placement decisions because the pools are defined by hardware identity rather than by workload capability.

The better approach is to define node pools by **capability class** — what the hardware can do, not what it is. An inference-small class might include L4 GPUs, A10G GPUs, and the smallest MIG partitions on A100s. An inference-large class might include A100-80GB, H100, and H200 devices suitable for serving models with tens of billions of parameters. A training-standard class might include H100 and H200 with NVLink interconnects for distributed training. A training-premium class might include B200 or multi-node NVLink domains for the largest jobs.

When a team deploys an inference workload, they request the inference-small or inference-large capability class. The scheduler picks the best available hardware within that class based on current availability, cost, and utilization. When you add new hardware — say, AMD MI300X instances that offer competitive inference throughput — you add them to the appropriate capability class. No existing deployments change. No manifests need updating. The new hardware simply becomes another option the scheduler can choose from within that class.

## Labels: The Lingua Franca of GPU Scheduling

Node pools defined by capability class work because Kubernetes scheduling is driven by labels, taints, and tolerations. The richness and consistency of your labeling scheme determines how intelligently your scheduler can place workloads. Skimpy labels produce stupid scheduling. Rich labels produce smart scheduling.

Every GPU node in your fleet should carry a standard set of labels. The GPU vendor — nvidia, amd, intel. The GPU model — a100-40gb, h100-sxm, mi300x, b200. The GPU memory in gigabytes. Whether the GPU supports MIG and what profiles are configured. The interconnect type — PCIe, NVLink, NVSwitch, xGMI for AMD. The CUDA compute capability or ROCm version. The driver version installed. The capability class the node belongs to. Whether the node runs on spot or preemptible instances versus on-demand. The cloud provider and region.

These labels are not decorative. They are the scheduling substrate. A workload that requires NVLink multi-GPU communication uses a node affinity rule that matches on interconnect type. A workload that needs at least eighty gigabytes of GPU memory uses a label selector for memory capacity. A cost-sensitive batch job specifies that it tolerates spot instances. A compliance-constrained workload restricts to specific regions. Without these labels, every scheduling decision is a manual negotiation between the team deploying the workload and the team managing the infrastructure. With them, scheduling becomes declarative and automatic.

The NVIDIA GPU Operator and AMD's GPU Operator both support automatic node labeling through their respective node feature discovery components. The GPU Operator's NFD integration detects the GPU model, driver version, MIG configuration, and compute capability, then applies labels automatically when a node joins the cluster. Your responsibility is defining the capability class labels on top of this hardware discovery — the semantic layer that maps hardware attributes to workload categories.

## Taints and Tolerations: Preventing Expensive Mistakes

Labels tell the scheduler what a node can do. **Taints** tell the scheduler what a node refuses to do. This distinction is critical for GPU infrastructure because GPU nodes are expensive, and the wrong workload on the wrong node is a direct cost leak.

Every GPU node pool should carry a taint that prevents non-GPU workloads from scheduling onto it. Without this taint, Kubernetes' default scheduler will happily place a CPU-only monitoring pod onto your four-dollar-per-hour H100 node because the node has available CPU and memory resources. The monitoring pod does not need a GPU. It does not even know a GPU exists. But it occupies a scheduling slot on an expensive node, and in the worst case, it prevents a legitimate GPU workload from scheduling there.

The pattern is straightforward. Apply a taint like nvidia.com/gpu=present with effect NoSchedule to all GPU nodes. Only pods that carry a matching toleration — which your GPU workload templates include by default — will be scheduled onto those nodes. This is table stakes for any GPU cluster, yet a surprising number of organizations skip it and discover months later that ten to fifteen percent of their GPU node capacity is occupied by workloads that have no business being there.

Beyond the basic GPU taint, use additional taints to enforce workload separation between capability classes. Training nodes carry a taint that only training jobs tolerate. Inference nodes carry a different taint. This prevents a long-running training job from consuming all the GPU memory on a node designated for latency-sensitive inference, and it prevents a fleet of small inference pods from fragmenting a multi-GPU training node into unusable pieces. The overhead of managing these taints is minimal. The cost of not managing them is discovered at two in the morning when inference latency spikes because a training job stole the GPU resources.

## Spot Instances for Training, On-Demand for Inference

The single most impactful cost optimization in heterogeneous node pool design is matching instance purchasing model to workload fault tolerance. Training jobs are fault-tolerant by design. Modern distributed training frameworks — DeepSpeed, FSDP, PyTorch's elastic training — support checkpointing, automatic restart, and elastic scaling. When a spot instance is reclaimed by the cloud provider, the training job loses at most the work since the last checkpoint, resumes on a replacement node, and continues. The savings are substantial: spot pricing for GPU instances ranges from sixty to ninety percent less than on-demand pricing depending on instance type, region, and time of day.

Inference workloads are the opposite. A spot instance reclamation means dropped requests, failed health checks, load balancer errors, and user-facing latency spikes while the replacement node provisions and loads the model. For production inference with SLAs, this is unacceptable. On-demand or reserved instances provide the stability that inference requires. The higher cost is justified because the alternative — serving errors to users — is more expensive.

Design your node pools to encode this distinction. Training node pools use spot instance configurations with appropriate interruption handling. Inference node pools use on-demand instances with reserved capacity for baseline traffic and on-demand burst capacity for traffic spikes. The scheduler routes workloads to the correct pool through the capability class labels and taints described above. A training job requesting the training-standard capability class automatically lands on spot-backed nodes. An inference deployment requesting inference-large lands on on-demand nodes. The cost optimization is structural, not accidental.

## Multi-Vendor GPU Orchestration

By 2026, supporting only NVIDIA GPUs is a defensible choice for many organizations but no longer the only choice. AMD's MI300X offers 192 gigabytes of HBM3 memory — more than an H100 — at competitive pricing. Intel's Gaudi 3 accelerator delivers strong inference throughput for transformer models. Google's TPU v5e remains the most cost-effective option for certain training workloads on GKE. Each vendor has its own device plugin, its own driver stack, its own container runtime requirements, and its own set of supported frameworks.

The Kubernetes abstraction layer handles multi-vendor orchestration through the device plugin interface. NVIDIA's device plugin advertises nvidia.com/gpu. AMD's device plugin advertises amd.com/gpu. Intel's advertises gpu.intel.com/i915 or intel.com/gaudi. From the scheduler's perspective, these are simply different resource types. A workload that requests nvidia.com/gpu will only schedule onto NVIDIA nodes. A workload that requests amd.com/gpu will only schedule onto AMD nodes.

The challenge is not scheduling — it is the software stack above the device plugin. A model optimized for CUDA may not run on ROCm without modification. A container image built with NVIDIA's CUDA toolkit will not work on AMD hardware. Your build pipeline must produce vendor-specific container images, or you must adopt frameworks like vLLM or TensorRT-LLM that abstract the hardware layer. Your capability class definitions must account for software compatibility, not just hardware capability. An inference-large class that includes both H100 and MI300X nodes must ensure that workloads scheduled to either can actually run on both, or the class must be split into vendor-specific sub-classes.

The organizations that extract the most value from multi-vendor fleets treat hardware selection as a cost optimization problem. For each workload, they benchmark throughput and latency on each available hardware type, calculate the cost-per-query or cost-per-token on each, and route the workload to the cheapest option that meets its performance requirements. This is not a one-time exercise. Pricing changes, new hardware becomes available, and driver updates shift the performance characteristics. The most mature platforms automate this benchmarking and re-routing on a monthly cadence.

## Planning for Hardware Transitions

GPU hardware does not become obsolete overnight, but it does become less cost-effective over a twelve-to-eighteen-month cycle. The A100-40GB that was premium hardware in 2022 is now the economy option in many cloud providers' catalogs. Planning for these transitions — without disrupting production workloads — is an essential part of node pool design.

The capability class model makes transitions manageable. When H200 instances become widely available and cost-competitive, you add them to the appropriate capability classes alongside existing hardware. Workloads automatically benefit from the new hardware as the scheduler places new pods. You do not need to migrate workloads explicitly. You do not need to update deployment manifests. The old hardware remains in the capability class until you decide to retire it, at which point you taint the old nodes with a NoSchedule taint, let existing workloads drain naturally, and decommission the hardware.

The trap to avoid is premature retirement. Teams often assume that the newest hardware is universally better, but the cost-performance calculation is nuanced. An older A100-80GB on a spot instance at sixty cents per hour may deliver better cost-per-query for a small inference workload than a new H200 on-demand at five dollars per hour. The right node pool design keeps older hardware available for cost-sensitive workloads and reserves premium hardware for workloads that genuinely need the performance. Retire hardware when no workload benefits from it at any price point — not when a newer model appears in the product catalog.

## The Scheduling Tax of Complexity

Heterogeneous node pools create scheduling complexity, and that complexity has a cost. More capability classes mean more labels, more taints, more tolerations, more node affinity rules, and more opportunities for misconfiguration. A workload that specifies overly narrow requirements — "must run on H100-SXM with NVLink in us-east-1 on an on-demand instance" — may fail to schedule when that specific combination is fully utilized, even though perfectly suitable capacity exists in other configurations.

Manage this complexity through defaults. Every workload type should have a default capability class and a default set of tolerations that work for ninety percent of cases. Only workloads with genuinely specific requirements should override the defaults. Provide a platform-level abstraction — a deployment template or a CRD — that translates simple declarations like "deploy this model for inference, size medium, production tier" into the correct labels, tolerations, and affinity rules. The platform hides the node pool complexity from the teams deploying models, just as a cloud provider hides the physical hardware from the teams deploying virtual machines.

Run a weekly audit of scheduling failures. Every pod that enters a Pending state due to unschedulable conditions reveals a gap in your node pool design — either the workload's requirements are too narrow, or the capability class lacks sufficient capacity, or a taint is blocking legitimate scheduling. These failures are not just inconveniences. They are direct signals about where your node pool design does not match your workload reality.

---

Hardware diversity and node pool design determine what capacity exists in your cluster. But capacity that exists is not the same as capacity that is available when traffic demands it. The next subchapter addresses the harder challenge of autoscaling GPU infrastructure — where the standard Kubernetes autoscaling model breaks down and what to build instead.