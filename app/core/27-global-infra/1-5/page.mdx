# 27.5 — The Platform-Application Split: What Infrastructure Teams Own

The most important organizational decision in AI infrastructure is not which GPU to buy or which cloud provider to use. It is where you draw the line between what the platform team owns and what the application team owns. **The Platform-Application Split** is this boundary — the contract between the people who build and maintain shared infrastructure and the people who build products on top of it. Get this boundary right and your organization scales. Product teams move fast because the platform handles the hard problems underneath. Infrastructure teams focus on reliability and efficiency because they are not buried in application-specific requests. Get it wrong and you get one of two failure modes: either the platform team becomes a bottleneck that blocks every product decision, or every product team builds its own infrastructure from scratch, duplicating effort and creating ungovernable sprawl.

This split is not unique to AI. Platform engineering as a discipline has been maturing for a decade, with internal developer platforms becoming standard practice at companies of all sizes. But AI workloads make the split harder to get right and more consequential when you get it wrong. The hardware is expensive, the failure modes are different from traditional compute, and the skills required to manage GPU clusters, training jobs, and inference serving infrastructure are specialized enough that most application engineers cannot and should not be expected to handle them. A 2026 industry survey found that roughly fifty-six percent of organizations now operate multiple internal platforms — separate platform teams for applications, data, and AI — precisely because the requirements diverge enough that a single platform cannot serve all workloads effectively.

## What the Platform Team Owns

The platform team's domain is everything that sits between the raw infrastructure — hardware, cloud accounts, network — and the application layer where product teams build features. Their job is to provide a stable, well-abstracted surface that application teams can build on without needing to understand the details underneath.

**Cluster provisioning and management** is the foundation. The platform team owns the Kubernetes clusters, the GPU node pools, the networking configuration, and the lifecycle management of the infrastructure. When a new GPU type becomes available, the platform team integrates it. When a cluster needs upgrading, the platform team handles the rollout. When a node fails, the platform team's automation detects it and replaces it. Application teams should never need to think about which physical node their workload runs on, how the GPU driver is installed, or what happens when a machine reboots.

**GPU scheduling, queuing, and capacity management** is where AI platforms diverge most sharply from traditional platforms. GPUs cannot be overcommitted the way CPUs can. A team that requests four GPUs for a training job needs four actual, physical GPUs dedicated to that job for its entire duration. The platform team owns the scheduling system — tools like Kueue or Volcano that manage job queues, enforce priorities, and ensure fair allocation of scarce GPU resources across multiple teams. This includes preemption policies: when the cluster is full, who gets kicked off to make room for a higher-priority job? That decision is politically sensitive and technically complex, and it belongs to the platform team.

**Multi-region infrastructure and failover** is the platform team's responsibility because application teams should not need to manage the mechanics of geographic distribution. The platform team provisions clusters in multiple regions, manages cross-region networking, handles model weight synchronization across regions, and operates the traffic steering systems that route requests to healthy regions. Application teams specify where they need to be available and what their latency requirements are. The platform team figures out how to deliver that.

**Observability and monitoring infrastructure** means the platform team provides the metrics pipelines, logging systems, distributed tracing, and alerting frameworks that application teams use to monitor their workloads. The platform team does not define what application-specific metrics to track — that is the application team's decision. But the platform team ensures that the collection, storage, querying, and alerting infrastructure is available, reliable, and cost-efficient. For AI workloads, this includes GPU utilization monitoring, memory consumption tracking, inference latency histograms, and training job progress metrics.

**Cost attribution and chargeback** ensures that every team knows what it spends and every dollar of GPU cost is assigned to the team that consumed it. Without this, GPU costs become a shared overhead that nobody optimizes because nobody feels the cost directly. The platform team builds the metering infrastructure that tracks GPU-hours, memory-hours, and network consumption by team, project, and workload. They provide dashboards that show cost trends and alerts that fire when spending exceeds budgets. The chargeback mechanism — whether it is actual financial transfers or just visible accounting — is a platform responsibility.

**Security baseline** includes network policies that isolate workloads, container image scanning that prevents deployment of vulnerable software, secrets management that keeps API keys and credentials out of source code, and RBAC configurations that control who can access what. The platform team establishes the security floor that all workloads must meet. Application teams can add additional security measures on top, but they cannot go below the baseline.

**Storage and artifact management** covers the shared storage systems — object stores for datasets, model registries for weights, artifact caches for container images — that application teams depend on. The platform team manages capacity, replication, backup, and performance tuning. Application teams consume storage through well-defined interfaces without managing the underlying infrastructure.

## What the Application Team Owns

The application team's domain is everything above the platform surface. They own the decisions that determine what the AI product does, how well it performs, and whether it meets user needs.

**Model selection and prompt engineering** is squarely in the application team's territory. Which model to use, how to prompt it, what temperature and sampling parameters to apply, whether to use a fine-tuned variant — these are product decisions that require understanding of the use case, the users, and the quality requirements. The platform provides the serving infrastructure. The application team decides what to serve on it.

**Application-level routing and fallback logic** includes decisions like which model to route a given request to, when to fall back from a larger model to a smaller one, how to handle degraded mode when a model is unavailable, and how to implement A/B testing across model versions. The platform may provide the traffic management primitives — weighted routing, canary deployments, health checks — but the application team configures them for their specific use case.

**Feature-level quality metrics and evaluation** is the application team's responsibility because only the application team understands what "good" means for their product. A customer support chatbot has different quality criteria than a code generation tool. The platform provides the infrastructure for running evaluations — compute, storage, scheduling. The application team defines the evaluations themselves.

**User-facing API design** covers how the application presents itself to users and downstream systems. Request formats, response schemas, authentication mechanisms, rate limiting policies — these are product-level concerns that the application team owns.

**Business logic and product decisions** is the broadest category and the most important. When to show AI-generated content versus static content, how to handle user feedback, what safety filters to apply, what data to log for improvement — these are all application-team decisions that the platform should support but never dictate.

## Why the Split Matters More for AI

In traditional web application infrastructure, getting the platform-application boundary slightly wrong is annoying but survivable. A product team that manages its own Kubernetes namespace or runs its own Postgres instance is doing extra work, but the blast radius of a mistake is limited. The hardware is cheap, the failure modes are well-understood, and the skills required are common.

AI changes the calculus in three ways. First, the hardware is expensive. A single GPU node can cost ten to thirty thousand dollars per month in cloud costs. If five product teams each provision their own GPU clusters with twenty percent average utilization because they do not share capacity, the organization is paying for five clusters worth of idle GPUs. A platform team that manages shared capacity and enables multiple teams to use the same GPU pool can achieve fifty to seventy percent utilization, cutting costs by more than half.

Second, the failure modes are specialized. GPU driver incompatibilities, CUDA version mismatches, memory fragmentation from inference serving, NVLink topology misconfiguration, training job failures from network partition — these are problems that require deep infrastructure expertise to diagnose and resolve. When every product team manages its own GPU infrastructure, every product team needs this expertise. When a platform team manages it centrally, the expertise is concentrated where it has the most leverage.

Third, the pace of hardware change is brutal. New GPU architectures ship every twelve to eighteen months. Each new generation brings different driver requirements, different memory capacities, different interconnect topologies, and different performance characteristics. A platform team can absorb a new GPU generation once and expose it to all application teams through a stable interface. Without a platform team, every application team must independently learn how to use new hardware, update their configurations, and handle the migration. The organizational cost of hardware transitions scales linearly with the number of teams doing their own infrastructure, but is nearly constant with a centralized platform.

## The Three Anti-Patterns

Three failure modes emerge when the platform-application split is drawn incorrectly. Each one is common, each one is expensive, and each one is driven by organizational dynamics rather than technical limitations.

**The Overbearing Platform** occurs when the platform team controls too much. The boundary creeps upward until the platform team is making application-level decisions: which models to support, what inference parameters are allowed, which evaluation frameworks to use, what the deployment schedule should be. The platform becomes a bureaucratic gatekeeper. Product teams file tickets requesting permission to run a new model. The platform team's backlog grows to months. Engineering velocity collapses because every product change requires a platform change. The symptom is easy to spot: product teams complain that the platform is the bottleneck, and the platform team's response is to hire more people rather than to push ownership downward.

**The Absent Platform** is the opposite. The platform team provides too little — maybe just raw Kubernetes access and GPU nodes — and leaves application teams to figure out the rest. Each product team builds its own GPU scheduling system, its own model serving pipeline, its own monitoring dashboards, its own cost tracking. The result is duplication at massive scale. Six teams build six different inference serving stacks. Three teams independently discover the same GPU driver bug. Cost visibility is nonexistent because there is no central metering. The symptom is that every product team has one or two people spending most of their time on infrastructure rather than product work, and none of them are doing it as well as a dedicated platform team would.

**The Undefined Split** is the most dangerous because it looks functional until it isn't. Nobody has explicitly decided what the platform team owns. There is no written contract, no API boundary, no documentation of responsibilities. Some things fall to the platform team by convention. Other things are handled by whichever product team hit the problem first. Knowledge about who owns what lives in people's heads. When someone leaves, ownership of critical systems becomes ambiguous. When an incident occurs, the first thirty minutes are spent figuring out who should be responding. The symptom is that the same question — "who owns this?" — is asked repeatedly, and the answer is different depending on who you ask.

## Drawing the Boundary in Practice

The most effective approach is to define the split explicitly, document it, and revisit it quarterly. Start with a principle: the platform team owns everything that is shared across multiple product teams and requires specialized infrastructure expertise. The application team owns everything that is specific to one product and requires domain expertise.

When a capability is needed by only one team, it starts as an application-team responsibility. If a second team needs the same capability, the two teams can collaborate or the platform team can evaluate whether to absorb it. If three or more teams need it, the platform team should own it. This prevents the platform from growing speculatively — building infrastructure nobody needs — while ensuring that genuinely shared problems get centralized solutions.

The boundary should be expressed as an interface, not just an organizational chart. What APIs does the platform expose? What self-service capabilities do application teams have? What requires a platform team ticket and what can be done independently? The clearest platform-application splits look like a product: the platform team publishes a catalog of services, each with defined capabilities, SLAs, and usage documentation. Application teams consume those services through well-defined interfaces. Changes to the platform are versioned and communicated. Breaking changes get migration periods. This level of formality might seem excessive for an internal team, but the alternative — ad hoc ownership with tribal knowledge — is what produces the undefined split anti-pattern.

Ninety-four percent of organizations now view AI as critical to platform engineering's future, and eighty-six percent believe platform engineering is essential to realizing AI's business value. These numbers reflect a dual mandate: the platform must both accelerate AI development and constrain its costs and risks. The platform-application split is the mechanism through which that dual mandate is operationalized. Draw the line, document it, enforce it, and adjust it as your organization learns what works.

## The Maturity Gradient

Not every organization needs the same level of platform sophistication. A ten-person startup with one AI product and one model can get by with a shared Kubernetes cluster and some light conventions about who manages what. A hundred-person company with five AI products needs a dedicated platform team with defined APIs and self-service tooling. A thousand-person organization with dozens of AI products needs a full internal developer platform with golden paths, cost attribution, governance controls, and formal SLAs.

The key is to build the platform that matches your current scale while designing it to evolve toward the next level. If you are at ten people, establish the conceptual split even if one person wears both hats. Document what is platform and what is application. When you grow, you will not need to untangle responsibilities — they are already defined. If you are at a hundred people and still have no clear platform-application boundary, you are already accumulating organizational debt that will compound with every new team and every new product.

The platform-application split is the organizational foundation on which everything else in this section is built. Kubernetes cluster design, GPU scheduling, multi-region architecture, cost management — all of these chapters assume that a platform team exists and that its boundaries are defined. Without that foundation, the technical details are academic. With it, they become actionable. The next subchapter maps the technical layers of the AI infrastructure stack from physical hardware to application services, giving you a reference model for understanding where every component lives and who is responsible for keeping it running.
