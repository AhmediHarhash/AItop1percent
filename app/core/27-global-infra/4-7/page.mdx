# 27.31 — Data Gravity and Placement: Why Moving Data Costs More Than Moving Compute

Data has gravity. The larger the dataset, the harder it is to move, and the more it pulls compute toward it. This is not a metaphor. It is a measurable infrastructure constraint that determines where your AI workloads run, what your cloud bill looks like, and how fast your systems respond. Every architecture decision you make about multi-region inference, distributed training, and RAG pipeline placement is ultimately a decision about data gravity — whether you recognize it or not.

The concept is straightforward. Moving a ten-terabyte dataset over a ten-gigabit-per-second link takes approximately 2.2 hours under ideal conditions. Move that same dataset over a cross-region link throttled to one gigabit per second and you are looking at 22 hours. Scale to a hundred terabytes — a modest training corpus for a domain-specific fine-tuning pipeline — and the cross-region transfer takes nine days. These are physics problems, not engineering problems. No amount of clever software eliminates the time it takes to push bytes through a pipe. And the math only gets worse from there, because physics is just the first tax. The second tax is money.

## The Physics Tax and the Money Tax

Every byte you move across a cloud boundary has two costs. The first is time — the latency and throughput constraints of the network link. The second is the egress charge your cloud provider levies for every gigabyte that leaves a region or leaves the cloud entirely.

As of 2026, the major cloud providers charge between $0.01 and $0.02 per gigabyte for inter-region data transfer within the same cloud, and between $0.05 and $0.09 per gigabyte for data that exits the cloud to the internet or to a different provider. These numbers sound small until you do the multiplication. Moving one hundred terabytes of training data between AWS regions costs between $1,000 and $2,000 per transfer. Moving that same data out to a different cloud costs $5,000 to $9,000. And you will not move it once — you will move updated versions, delta snapshots, preprocessed variants. Over a year of active model development, a single large dataset can generate tens of thousands of dollars in transfer costs before the first GPU-hour is consumed.

The physics tax and the money tax together create gravitational pull. Once a dataset reaches a certain size in a certain location, it becomes cheaper to bring compute to the data than to bring data to the compute. This is the core principle of **data gravity**, and it reshapes AI infrastructure architecture in ways that most teams do not anticipate until they see their first quarterly cloud bill.

## Training Data Gravity

Training datasets are the heaviest objects in your infrastructure. A fine-tuning corpus might be ten to fifty terabytes. A pretraining dataset can reach hundreds of terabytes or more. These datasets live where they were generated — transaction logs in the production database region, sensor data in the factory's nearest cloud zone, medical records in the hospital system's compliant jurisdiction.

The naive approach is to copy the training data to wherever you have GPU capacity. You have H100s in US-East, so you copy the data to US-East. This works once. It stops working when the dataset is updated weekly, when you have three teams fine-tuning different models on overlapping data, and when your compliance team discovers that copying European patient records to a US region violates GDPR data residency requirements.

The data-gravity-aware approach inverts the relationship. Instead of moving data to GPUs, you provision GPUs where the data lives. If your training corpus is in EU-West because that is where the production systems that generated it run, you secure GPU capacity in EU-West. The upfront cost of provisioning compute in a non-preferred region is almost always lower than the recurring cost of moving terabytes of data across regions every training cycle. Teams that adopt this principle typically see 30 to 60 percent reductions in their data pipeline costs within the first quarter, because they eliminate the transfer charges, the pipeline orchestration complexity, and the multi-hour delays that cross-region copies introduce into the training iteration loop.

## Inference Data Gravity

Inference workloads have a different gravity profile than training workloads, but the pull is equally strong.

For inference, the data that creates gravity is not the training corpus — it is the user data and the context data that the model needs at query time. A customer support model needs access to the customer's ticket history and account records. A legal document model needs access to the firm's contract database. A medical diagnosis model needs access to the patient's records. This data lives in the region where the application runs, and it cannot move for reasons that go beyond cost.

Data residency regulations — GDPR in Europe, LGPD in Brazil, PIPL in China, sector-specific rules like HIPAA in healthcare — often require that personal data remain within specific jurisdictions. You cannot ship a German user's medical records to a US data center for inference and ship the results back, no matter how much faster or cheaper the US GPUs are. The inference must happen where the data is allowed to exist. Data gravity for inference is not just an optimization preference. It is a legal requirement that determines your multi-region architecture.

This is why global AI products end up with inference deployments in every major regulatory region. You deploy model replicas in EU-West, US-East, AP-Southeast, and SA-East not because you want geographic redundancy for availability — though you get that as a side benefit — but because the data each region's users generate must stay in that region, and the model must be co-located with that data to serve requests.

## The Embedding Gravity Problem

RAG architectures introduce a particularly stubborn form of data gravity that catches teams off guard. Your vector database contains embeddings generated from your document corpus — millions or billions of vectors that encode the semantic content of your knowledge base. These embeddings must be co-located with the inference service that queries them, because a RAG query requires a vector similarity search followed by context injection and then model inference, all within a single request's latency budget.

A typical RAG query has a target latency of 500 milliseconds to two seconds end to end. The vector search itself takes 10 to 50 milliseconds when the database is local. Add a cross-region network hop — 50 to 150 milliseconds round trip depending on the regions — and you have consumed 10 to 30 percent of your latency budget on data transfer before the model has generated a single token.

The deeper problem is replication cost. A vector database with 50 million embeddings at 1536 dimensions in 32-bit float precision consumes roughly 300 gigabytes of storage. Replicating this across five regions is straightforward for storage. The hard part is keeping the replicas synchronized. When your document corpus changes — new articles published, old contracts updated, fresh support tickets ingested — you must re-embed the changed documents and propagate the new vectors to every region. If your corpus changes frequently, this synchronization pipeline becomes a significant ongoing cost in both compute (re-embedding) and transfer (distributing vectors).

Teams that underestimate embedding gravity end up in one of two failure modes. Either they centralize the vector database in a single region and accept degraded latency for every other region, or they replicate aggressively and discover that their embedding synchronization pipeline costs more to run than the inference service itself. The right approach is tiered replication — full replicas in your high-traffic regions, partial replicas (most-queried documents only) in secondary regions, and fallback to cross-region queries for rare documents. This requires understanding your query distribution, which most teams do not have when they first deploy.

## Data Gravity Versus GPU Availability

Data gravity tells you where your workloads should run. GPU availability tells you where they can run. These two forces frequently disagree.

As of 2026, the global distribution of high-end AI accelerators is uneven. The major cloud providers have concentrated their largest GPU clusters in North American regions — US-East, US-West, US-Central — because that is where demand first materialized and where power and cooling infrastructure were built first. European regions have growing capacity but shorter supply, particularly for the latest-generation hardware. Asian regions vary widely, with major hubs in Tokyo, Singapore, and Mumbai but limited availability in smaller markets. South American and African regions have the least GPU capacity of all.

When your data lives in Frankfurt because your European users generate it there and GDPR keeps it there, but the H100 cluster with availability is in Virginia, you face a genuine architectural decision. Option one: move the data to Virginia, pay egress costs and accept regulatory risk. Option two: wait for GPU capacity in Frankfurt, delaying your training pipeline by days or weeks. Option three: use a private interconnect between your Frankfurt data store and a Virginia GPU cluster, accepting the latency but avoiding a full data copy. Option four: use a smaller or previous-generation GPU in Frankfurt that has availability, accepting slower training but keeping data in place.

There is no universally correct answer. The decision depends on the data volume, the regulatory constraints, the training frequency, and the cost difference between the regions. What matters is that you make this decision explicitly, with real numbers, rather than defaulting to wherever GPUs happen to be available and discovering the data movement costs after the fact.

## Architecture Patterns for Data-Gravity-Aware Infrastructure

Mature AI platforms adopt one of three placement patterns depending on their workload mix.

**Data-proximate inference** places the model and the inference service in the same region as the user data and the application data. This is the default for any product with data residency requirements or latency-sensitive RAG queries. The model weights are replicated to each region — a one-time cost of 50 to 200 gigabytes per model per region — and all user data stays local. The ongoing transfer cost is limited to model weight updates when you deploy a new version, which happens weekly or monthly rather than per-request.

**Compute-proximate training** places the training pipeline in the region with the best GPU availability and moves the training data there. This pattern works when the training data is not subject to residency constraints, when it changes infrequently enough that the transfer cost is manageable, and when the GPU availability gap between regions is large enough to justify the move. A common example is training on anonymized, aggregated data that has been stripped of personal identifiers — the anonymized corpus can be moved to wherever GPUs are cheapest without regulatory concern.

**Hybrid placement** splits the pipeline. Preprocessing and data preparation happen where the data lives — filtering, deduplication, tokenization, embedding generation. The processed artifacts, which are typically much smaller than the raw data, are then transferred to the training region. A hundred terabytes of raw customer interactions might reduce to five terabytes of processed training examples after filtering and deduplication. Moving five terabytes costs one-twentieth of what moving the raw data would cost, and the preprocessing step adds value (quality filtering, format standardization) that you would need regardless of placement.

The choice between these patterns is not static. It shifts as your data grows, as GPU availability changes across regions, and as new regulatory requirements come into effect. The teams that handle this well treat placement as a quarterly architecture review item, not a one-time decision made at launch and never revisited.

## Measuring Data Gravity

You cannot manage data gravity if you do not measure it. The key metrics are transfer volume (how many gigabytes move between regions per day, per week, per month), transfer cost (the actual cloud egress charges on your bill), transfer latency (how long data movement adds to your pipeline end-to-end time), and co-location ratio (what percentage of your inference requests are served from the same region as the data they depend on).

Most teams are surprised by their first data gravity audit. They discover that a centralized logging pipeline is moving more bytes per month than their entire training pipeline. They find that a telemetry aggregation job is copying model outputs from five regions to a single analytics cluster, generating thousands of dollars in monthly egress charges that nobody budgeted for. They realize that their RAG pipeline's embedding synchronization is the single largest data transfer cost in their infrastructure.

Once you see the numbers, the optimization path becomes clear. Move compute to data where the data is heavy and sticky. Move data to compute only when it is small, infrequent, and unconstrained by regulation. And when data must move, compress it, deduplicate it, and aggregate it before it crosses a region boundary — because every gigabyte that does not need to move is a gigabyte you do not pay for.

---

Data gravity determines where your workloads should live. But even after you make the right placement decisions, data still moves — model weights get replicated, telemetry gets aggregated, outputs get centralized for analytics. The next subchapter examines the egress costs that accumulate from this unavoidable movement and the strategies that keep those costs from consuming your infrastructure budget.
