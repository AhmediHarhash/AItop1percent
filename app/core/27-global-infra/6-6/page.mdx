# 27.46 — Unified Control Planes for Heterogeneous Infrastructure

How do you manage five Kubernetes clusters across three clouds with two on-premises racks without losing your mind? The answer, in theory, is a unified control plane — a single management layer that sees every cluster, enforces consistent policies, and lets your platform team operate heterogeneous infrastructure as if it were one system. In practice, unified does not mean simple. Every control plane product promises a single pane of glass. What they deliver is a single pane of glass with slightly different tinting for each provider, and some windows that do not open at all.

The gap between the promise and the reality is not a reason to avoid unified control planes. It is a reason to choose them with clear expectations about what they solve and what they leave for your team to handle.

## What a Unified Control Plane Actually Gives You

At their core, unified control planes provide four capabilities that matter for multi-cloud AI infrastructure. First, **fleet visibility** — the ability to see all clusters, their health, their resource utilization, and their running workloads from a single interface. Without this, your platform team maintains mental models of each cluster independently, switching between AWS console tabs, Google Cloud console tabs, and on-premises dashboards. At five clusters this is annoying. At fifteen it is a full-time job.

Second, **fleet-wide policy enforcement**. Security policies, network policies, resource quotas, and RBAC rules should be consistent across environments. When your security team mandates that all inference pods must run as non-root with read-only filesystems, that policy needs to apply to your AWS cluster and your on-premises cluster and your Google Cloud cluster equally. Without a control plane, someone must manually apply that policy to each cluster and verify compliance across all of them. With a control plane, you define the policy once and the system distributes and enforces it.

Third, **centralized RBAC**. Who can deploy to which cluster? Who can view GPU utilization? Who can modify resource quotas? Managing identity and access per-cluster means maintaining separate role bindings in each environment, which inevitably diverges. A unified control plane federates identity — your team authenticates once and their permissions apply across all clusters.

Fourth, **cost visibility**. When GPU spend is distributed across three clouds and two on-premises deployments, getting a complete picture of total infrastructure cost requires aggregating billing data from every provider. Control planes that integrate cost reporting give your finance team a single view without building custom dashboards that scrape five different billing APIs.

These four capabilities — visibility, policy, access, cost — are the actual value. They do not make multi-cloud easy. They make it manageable at scales where managing each cluster independently would consume your entire platform team's capacity.

## Azure Arc: Extending Azure to Everything

**Azure Arc** takes the Azure Resource Manager — the control plane that manages all Azure resources — and extends it to Kubernetes clusters running anywhere. You install a lightweight agent on each non-Azure cluster (EKS, GKE, on-premises, bare metal), and that agent establishes a connection back to Azure's control plane. Once connected, the cluster appears as an Azure resource. You can apply Azure policies to it, assign Azure RBAC roles, deploy applications through Azure's tooling, and view its telemetry in Azure Monitor.

For organizations that have already standardized on Azure as their primary cloud, Arc is the natural choice. The integration with Azure Active Directory (now Entra ID) for identity, Azure Policy for governance, and Azure Monitor for observability means your existing operational workflows extend to non-Azure clusters without building new tooling. Arc supports GitOps natively through Flux — you configure a Git repository as the source of truth for a cluster's desired state, and Arc's Flux extension ensures the cluster converges to that state automatically.

Arc's limitation is its Azure-centricity. While it manages non-Azure clusters, it manages them through Azure's lens. Configuration is expressed in Azure's resource model. Policies use Azure Policy's syntax. Monitoring flows to Azure Monitor. If your team is not already fluent in Azure's management paradigm, adopting Arc means learning it — and if your primary cloud is AWS or Google Cloud, you now have two cloud management paradigms to maintain rather than one. Arc is the strongest choice when Azure is your center of gravity. For teams with no Azure investment, the adoption cost is harder to justify.

## Google GKE Enterprise: Fleet Management at Google Scale

Google's answer to multi-cluster management is **GKE Enterprise** (the evolution of what was previously called Anthos). GKE Enterprise introduces the concept of a **fleet** — a logical grouping of Kubernetes clusters that can span Google Cloud, AWS, Azure, and on-premises environments. You register clusters into a fleet, and fleet-level features apply to all of them.

Fleet-level features include Config Sync for GitOps-based configuration management, Policy Controller for OPA Gatekeeper-based policy enforcement, and Service Mesh for cross-cluster networking through Istio. The multi-cluster ingress capability routes traffic across clusters in a fleet, enabling geographic load balancing without external tooling. For AI workloads, the fleet model lets you define GPU resource policies that apply uniformly — minimum reservation percentages, maximum spot ratios, scheduling constraints — across clusters in different environments.

GKE Enterprise's unique advantage is its deep integration with Kubernetes itself. Google invented Kubernetes, and their multi-cluster tooling reflects that heritage. Features like Config Sync are Kubernetes-native — they work with standard Kubernetes resources rather than provider-specific abstractions. This makes the skills transferable. What your team learns managing fleets through GKE Enterprise applies to managing any Kubernetes cluster, on any provider.

The practical limitation is cost and complexity. GKE Enterprise pricing is per-cluster-per-month on top of the underlying compute costs, and the per-vCPU pricing for registered non-GKE clusters can add up quickly for GPU-heavy nodes with high vCPU counts. The installation and configuration of the full GKE Enterprise stack — Config Sync, Policy Controller, Connect agent, Fleet feature management — requires significant upfront investment. Teams consistently report that the time to full productivity with GKE Enterprise is three to six months, not three to six weeks.

## Rancher by SUSE: Open-Source Multi-Cluster Management

**Rancher** takes a different approach from the hyperscaler offerings. It is open-source, provider-agnostic, and designed from the ground up for multi-cluster Kubernetes management. You install Rancher on any Kubernetes cluster — it does not need to run on a specific cloud — and from there you import, create, and manage clusters across any provider.

Rancher's dashboard provides the fleet visibility that multi-cloud operations require: cluster health, workload status, resource utilization, and alerting across all registered clusters. Its RBAC model is global — define a role once, assign it to a user, and specify which clusters or projects the role applies to. Fleet, Rancher's GitOps engine, provides multi-cluster application deployment using standard Kubernetes manifests and Helm charts.

For AI platform teams, Rancher's appeal is independence. It does not lock you into any cloud provider's management paradigm. Your control plane runs wherever you choose — on a small Kubernetes cluster in your primary cloud, on-premises, or even on a developer's machine for testing. If you move from AWS to Google Cloud or add a fourth cloud provider, Rancher does not care. You install an agent, register the cluster, and manage it through the same interface.

The trade-off is that Rancher gives you less out of the box than the hyperscaler offerings. Azure Arc comes with deep Azure Policy integration. GKE Enterprise comes with Config Sync and Istio mesh. Rancher comes with Fleet for GitOps and a solid RBAC system, but advanced policy enforcement requires adding OPA Gatekeeper or Kyverno yourself. Service mesh requires adding Istio or Linkerd yourself. Cost management requires integrating OpenCost or Kubecost yourself. The building blocks are all available, but you assemble them rather than receiving a pre-integrated package.

## Platform9 and the Managed Kubernetes Alternative

**Platform9** occupies a different niche — fully managed Kubernetes across any infrastructure. Where Rancher gives you tools to manage clusters yourself, Platform9 manages the clusters for you. Their team handles Kubernetes upgrades, etcd backups, certificate rotation, and cluster health monitoring across your fleet, whether the clusters run on AWS, Azure, bare metal, or edge devices.

For AI teams that want multi-cloud Kubernetes without dedicating two or three engineers full-time to cluster operations, the managed model is compelling. Platform9's value proposition is that your team spends time on AI workloads — model serving, GPU scheduling, evaluation pipelines — rather than on Kubernetes maintenance. The cost is the Platform9 subscription plus the requirement that their agents run on your infrastructure and have the access they need to manage it.

The limitation is control. A managed platform makes decisions about upgrade timing, component versions, and configuration defaults. If your AI workload has specific requirements — a particular Kubernetes version for GPU operator compatibility, a specific CNI plugin for InfiniBand support, a non-standard etcd configuration for large-scale Kueue deployments — you need to verify that Platform9's management approach accommodates those requirements rather than overriding them.

## The Illusion of Uniformity

Here is the uncomfortable truth about every unified control plane: they make different clouds look the same from the management interface, but they do not make different clouds behave the same underneath. A GPU scheduling policy that works perfectly on GKE may hit edge cases on EKS because the NVIDIA device plugin is configured differently. A network policy that blocks unwanted traffic on Azure may not work on bare metal because the CNI plugin does not support the same policy selectors. A storage class that provisions fast NVMe volumes on AWS maps to a completely different storage backend on Google Cloud with different performance characteristics.

The control plane abstracts the management operations — deploy this workload, enforce this policy, scale this resource — but it does not abstract the infrastructure behavior. Your platform team still needs to understand each provider's networking model, storage classes, GPU device plugins, and node lifecycle management. The control plane reduces the number of consoles they log into. It does not reduce the amount they need to know.

For AI infrastructure specifically, the differences that control planes cannot hide include GPU device plugin versions and capabilities across providers, CUDA driver management and version compatibility per node image, InfiniBand versus RoCE networking configurations for multi-node training, topology-aware scheduling semantics that vary by cloud, and model loading behavior that depends on the underlying storage performance. These are the areas where your team must invest in provider-specific expertise regardless of how unified their management interface is.

## Choosing the Right Control Plane for Your Scale

For teams managing two to three clusters on a single primary cloud with one or two satellite clusters, Azure Arc or GKE Enterprise (depending on your primary cloud) provides the fastest time to value. The integration with your existing cloud tooling means less new infrastructure to learn and maintain.

For teams managing five or more clusters across three or more providers, Rancher or a similar open-source solution provides the provider independence that prevents your management layer from becoming another lock-in vector. If your control plane only manages Azure clusters well, adding a Google Cloud cluster for TPU access creates a management gap that undermines the "unified" promise.

For teams that cannot staff a dedicated platform engineering function, Platform9 or a similar managed service trades cost for operational overhead in a way that makes multi-cloud feasible without a large infrastructure team.

Regardless of which control plane you choose, the deployment mechanism that actually pushes changes to clusters should be GitOps-based — configurations in Git, reconciled by controllers, auditable and reversible. The control plane provides visibility and policy. GitOps provides the deployment machinery. Together they make heterogeneous infrastructure manageable. Separately, each only solves half the problem.

---

A unified control plane tells you what your infrastructure looks like and enforces rules about what it should look like. But the mechanism that keeps reality aligned with intent — that takes a configuration change in a repository and propagates it to five clusters across three clouds — is GitOps. The next subchapter covers how GitOps and infrastructure-as-code principles apply specifically to AI platform management, where the assets being managed include GPU node pools, model serving configurations, and scheduler quotas that do not appear in traditional application deployment playbooks.
