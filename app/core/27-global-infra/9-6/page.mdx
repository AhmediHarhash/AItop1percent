# 27.69 â€” Platform SLOs: What the Infrastructure Team Promises and How to Measure It

If the platform team does not define SLOs, application teams will define their own expectations -- and those expectations will be unrealistic, inconsistent, and discovered only during incidents. One team assumes GPU scheduling will complete in under a minute. Another team assumes model deployment takes five minutes. A third team files an urgent incident when autoscaling takes eight minutes to add capacity, even though eight minutes is faster than the infrastructure has ever delivered. Without published SLOs, every performance conversation becomes a negotiation between what the application team wanted and what the platform team thought was reasonable. The platform team feels they are delivering a reliable service. The application team feels they are being ignored. Both are operating on assumptions they never made explicit.

**Platform SLOs** are the contract that prevents this dysfunction. They define what the infrastructure team promises, in measurable terms, and what the infrastructure team does not promise. They give application teams realistic expectations for capacity planning. They give the platform team a clear mandate for where to invest engineering effort. And they create a shared vocabulary for reliability that replaces subjective frustration with objective measurement.

## Platform SLOs Are Not Application SLOs

The distinction matters and teams confuse it constantly. An **application SLO** defines what end users experience: response latency under 500 milliseconds at the 99th percentile, availability of 99.95 percent measured by successful requests, quality scores above a defined threshold. The application team owns these SLOs and is accountable for meeting them.

A **platform SLO** defines what the infrastructure provides to the application team: GPU scheduling latency, infrastructure availability, deployment speed, autoscaling responsiveness. The platform team owns these SLOs and is accountable for meeting them. When the platform meets its SLOs but the application misses its SLOs, the problem is in the application layer -- model configuration, prompt design, traffic management. When the platform misses its SLOs and the application misses its SLOs as a consequence, the problem is in the infrastructure and the platform team is responsible.

This separation of accountability is essential for productive incident response. Without it, every degradation turns into finger-pointing. With it, the investigation starts with a clear question: did the platform meet its SLOs during the incident window? If yes, look at the application. If no, look at the infrastructure. The SLO data answers the question before the argument starts.

## Core Platform SLOs for AI Infrastructure

Not every metric deserves an SLO. SLOs should cover the capabilities that application teams depend on most and that the platform team can meaningfully control. For AI infrastructure, four categories cover the critical surface.

**GPU scheduling latency** measures the time from job submission to the first pod in the job reaching the running state. This is the delay application teams experience when they submit a training job, a fine-tuning run, or a new inference deployment. For inference workloads, where a new deployment or a scaled-up replica needs to start serving traffic, the target should be aggressive: less than five minutes from submission to serving. For training workloads, where jobs are larger and scheduling involves finding contiguous GPU blocks across nodes, the target is more relaxed: less than thirty minutes for jobs requesting up to 32 GPUs, and less than sixty minutes for jobs requesting 64 or more. These targets account for the reality that large training jobs compete for scarce GPU capacity and may need to wait for running jobs to complete before enough contiguous resources become available.

The measurement must capture the full scheduling path, not just the Kubernetes scheduler's decision time. The clock starts when the user submits the job and stops when the pod is running -- which includes queue wait time in Kueue, node provisioning time if the cluster autoscaler needs to add capacity, image pull time for large model containers, and GPU device plugin initialization. Application teams do not care which component caused the delay. They care about total time from submission to running.

**Infrastructure availability** measures the percentage of time the platform accepts and successfully serves workloads. For inference infrastructure, where downtime directly impacts end users, the target is 99.9 percent -- roughly 8.7 hours of allowed downtime per year, or 43 minutes per month. For training infrastructure, where jobs are long-running and can tolerate brief interruptions if checkpointing is configured correctly, the target is 99.5 percent -- roughly 3.6 hours of allowed downtime per month. These targets exclude scheduled maintenance windows that are communicated at least 72 hours in advance, because application teams can plan around announced downtime but cannot plan around surprise outages.

Measuring infrastructure availability requires defining what "available" means precisely. A cluster where the Kubernetes API server is responding but no GPU nodes are schedulable is not available for AI workloads, even though traditional infrastructure monitoring would report it as healthy. Availability for the AI platform means: the cluster accepts workload submissions, GPU resources can be scheduled, running workloads continue to run without platform-caused interruption, and model serving endpoints return responses. Each of these conditions must be continuously checked by synthetic probes, not inferred from component health.

**Model deployment time** measures the elapsed time from a model artifact being pushed to the registry to that model serving live traffic. This covers the full deployment pipeline: container image build or pull, model weight download from object storage, GPU memory allocation, model loading and warmup, health check passage, and traffic routing. For most model sizes, the target is less than fifteen minutes. For very large models -- 70 billion parameters or above -- that require multi-GPU sharding and weight distribution across nodes, the target extends to thirty minutes.

Deployment time matters because it directly determines how quickly application teams can iterate on model updates, respond to quality regressions by rolling back, or scale capacity in response to traffic spikes. A platform where deploying a new model version takes an hour creates organizational pressure to deploy less frequently, which means longer feedback cycles and slower improvement.

**Autoscaling response time** measures the elapsed time from an autoscaling trigger -- CPU or GPU utilization crossing a threshold, request queue depth exceeding a limit -- to additional capacity actively serving requests. The target is less than ten minutes for adding replicas of already-deployed models, and less than twenty minutes when scaling requires provisioning new GPU nodes from the cloud provider. These targets reflect the physical reality that GPU node provisioning is slower than CPU node provisioning because GPU instance types have more constrained availability and longer boot times due to driver initialization.

## Error Budgets: What Happens When the SLO Breaks

An SLO without consequences is a wish, not a commitment. **Error budgets** are the mechanism that makes SLOs meaningful by defining what changes when the SLO is violated.

The error budget is simply the inverse of the SLO target. A 99.9 percent availability SLO means you have a 0.1 percent error budget -- 43 minutes of allowed downtime per month. As long as actual downtime stays below 43 minutes, the platform team is free to ship features, perform upgrades, and make changes to the infrastructure. When the error budget is consumed -- when actual downtime exceeds 43 minutes -- the team shifts from feature work to reliability work until the error budget is replenished in the next measurement window.

The error budget policy should be explicit and agreed upon by platform and application leadership before anyone is in the middle of an incident. A well-designed policy specifies graduated responses. When 50 percent of the error budget is consumed, the platform team reviews recent changes for reliability risks and pauses non-essential infrastructure experiments. When 75 percent is consumed, all non-critical changes are frozen and the team focuses on reliability improvements. When 100 percent is consumed, the team enters a reliability sprint: no new features, no optional upgrades, all engineering effort goes to reducing the risk of further outages.

Error budgets also create productive conversations about investment. When the platform team consistently burns through their error budget, it is evidence that the infrastructure needs more investment -- more redundancy, better failover, additional on-call staffing. When the team rarely touches the error budget, it may be evidence that the SLO is too loose and can be tightened, or that reliability investment is adequate and the team can afford to take on more feature work. The error budget turns reliability into a quantitative discussion rather than a qualitative argument.

## Measuring SLOs: Continuous, Not Periodic

SLOs measured through monthly reports are useless for operational decision-making. By the time you see the monthly number, the error budget has already been burned and the incident is over. Effective SLO measurement is continuous and real-time.

The core measurement system is a **burn rate dashboard** that shows how quickly the error budget is being consumed at any given moment. If the monthly error budget is 43 minutes and you have already consumed 20 minutes in the first week, the burn rate is roughly four times faster than sustainable. The dashboard should show this acceleration prominently, because a high burn rate early in the measurement window is a leading indicator that the SLO will be violated by month end unless something changes.

Alerting should be based on burn rate, not on raw SLO violation. A five-minute outage that consumes 12 percent of the monthly error budget warrants investigation but not panic. A pattern of one-minute blips that has consumed 40 percent of the error budget in the first ten days warrants urgent action even though no individual incident seemed severe. Burn-rate alerting catches the pattern that raw incident alerting misses.

The measurement infrastructure itself must be more reliable than the platform it measures. If the SLO measurement system runs on the same cluster it is measuring, a cluster outage takes down both the platform and the proof that the platform is down. Run SLO measurement on independent infrastructure -- a separate cluster, a managed monitoring service, or an external probe system that is not affected by platform failures.

## The SLO Review Cadence

SLOs are not permanent. They evolve as the platform matures, as application requirements change, and as the organization's reliability expectations shift.

Monthly SLO reviews bring together the platform team and representatives from the major application teams. The review covers three questions. First, were the SLOs met last month? If not, what caused the miss and what has been done to prevent recurrence? Second, are the SLOs still appropriate? An application team that has launched a real-time voice AI product may need tighter autoscaling response times than the current target. A training team that has shifted from large single runs to many small runs may need tighter scheduling latency targets. Third, are there new capabilities that need SLOs? If the platform recently added multi-region failover, an SLO for failover time should be defined.

Quarterly reviews take a broader view: is the overall SLO framework working? Are teams using the SLOs for capacity planning? Are error budgets driving the right behavior? Do the SLO targets need recalibration based on what the platform has actually delivered? Recalibration works in both directions -- tightening targets when the platform consistently exceeds them by a wide margin, and loosening targets if they are creating unsustainable on-call burden without corresponding business value.

## The SLO as a Two-Way Contract

SLOs are not a one-way promise from the platform team to the world. They are a contract with obligations on both sides.

The platform team commits to meeting the SLO targets, investing in reliability when error budgets are consumed, communicating planned maintenance windows in advance, and providing transparent measurement of SLO performance. Application teams commit to using the platform correctly: setting appropriate resource requests and limits on their workloads, implementing health checks and readiness probes so the platform can route traffic intelligently, handling graceful shutdown signals so the platform can drain and reschedule workloads during maintenance, and not circumventing platform guardrails like resource quotas or scheduling policies.

When an application team bypasses the platform's resource management -- requesting raw GPU access instead of using the managed serving layer, hard-coding node affinities that defeat the scheduler's optimization, ignoring pod disruption budgets that the platform uses for rolling upgrades -- the platform SLO no longer applies to that workload. This is not punitive. It is logical. The SLO covers the behavior the platform was designed and tested to deliver. Workloads that bypass the platform's design cannot be covered by the platform's reliability commitments.

Document these mutual obligations in a platform service guide that every application team receives when they onboard. The guide lists the SLOs, the measurement methodology, the error budget policy, and the application team's responsibilities. Treat it as the platform's user agreement. When an incident occurs, the guide is the reference document that determines accountability.

---

SLOs define what the platform promises. But promises require people to keep them, especially at three in the morning. The next subchapter examines what changes about on-call operations when the infrastructure runs GPUs instead of CPUs -- different failure modes, different blast radiuses, and different expertise requirements.
