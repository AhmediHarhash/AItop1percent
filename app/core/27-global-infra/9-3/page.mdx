# 27.66 â€” Distributed Tracing for AI Pipelines: Following a Request Across the Globe

The team had all the dashboards they needed -- GPU utilization, model latency, request rates. What they did not have was the ability to answer a simple question: why do some requests take forty times longer than others? Their AI product, a document analysis service for legal teams, showed a median latency of 180 milliseconds. Acceptable. But the p99 latency was 7.2 seconds. Unacceptable. Users who hit that tail latency assumed the system had crashed and resubmitted their requests, which made the problem worse. The team spent three weeks in speculation. The model team blamed the retrieval layer. The retrieval team blamed the vector database. The database team blamed network latency. Everyone was guessing because nobody could trace a single slow request through the full pipeline and identify exactly where the time was spent.

When they finally instrumented their stack with distributed tracing, the answer emerged within a day. Eighty percent of the slow requests shared a common pattern: they hit a specific vector database shard in their Singapore region that was running on degraded storage. The shard's query latency was 50 to 100 milliseconds for most requests but spiked to 3 to 5 seconds when it needed to read from a slow disk segment. The retrieval timeout was set to 6 seconds, so the requests technically succeeded -- they just succeeded slowly. Without tracing, this would have remained a mystery for months, hidden inside aggregate latency distributions.

## Why Tracing Matters More for AI Than for Traditional Services

Traditional web services have relatively shallow request paths. A request arrives at a load balancer, hits an application server, queries a database, and returns. Three or four hops. When something is slow, the culprit is usually obvious because there are so few places to look.

AI inference pipelines are different. A single user request may traverse ten or more services before a response is generated. The request arrives at a load balancer, passes through an API gateway, hits a request router that decides which model or model version should handle it, triggers a retrieval step that queries a vector database for relevant context, returns to an orchestration layer that assembles the prompt from the retrieved context and the user input, sends the assembled prompt to the inference engine, waits for the GPU to process the prefill and generate output tokens, passes the output through a guardrail service that checks for safety and compliance, potentially routes through a post-processing step for formatting or citation insertion, and finally returns through the gateway to the user. Each of these services adds latency. Each can fail independently. Each can degrade silently.

In multi-region deployments, the request path becomes even more complex. A request that arrives in Frankfurt might be routed to a model deployment in Amsterdam because Frankfurt's GPUs are at capacity. The retrieval step might query a vector database replica in Frankfurt while the inference happens in Amsterdam. If the retrieval result needs to cross a regional boundary to reach the inference engine, that adds tens of milliseconds of network latency that is invisible without tracing. In failover scenarios, a request might bounce between regions before finding available capacity, accumulating latency at each hop.

The fundamental problem is that AI pipeline latency is not dominated by any single component. In a well-tuned system, inference accounts for 40 to 60 percent of total latency, retrieval accounts for 15 to 30 percent, and the remaining 10 to 25 percent is distributed across routing, guardrails, post-processing, and network transit. When something goes wrong, the culprit can be any of these components. Without tracing, you are left with logs from each service that show timestamps but no causal chain.

## OpenTelemetry as the Standard

**OpenTelemetry** has become the de facto standard for distributed tracing in 2026, and its adoption for AI workloads has accelerated significantly. OpenTelemetry provides a vendor-neutral framework for generating, collecting, and exporting traces, metrics, and logs. It supports every major programming language, integrates with every major observability platform -- Datadog, Grafana Cloud, Honeycomb, Jaeger, AWS X-Ray -- and is backed by the Cloud Native Computing Foundation.

For AI pipelines specifically, the OpenTelemetry community has developed **semantic conventions for generative AI** that standardize how AI-specific attributes are recorded in traces. These conventions define standard attribute names for model identifiers, token counts (input and output), model temperature settings, request types (chat, completion, embedding), and response metadata. Using these conventions means that traces from different AI services are immediately comparable, and observability platforms can build AI-specific visualizations without custom configuration.

The practical setup involves instrumenting each service in the AI pipeline with an OpenTelemetry SDK. When a request enters the system, the first service creates a trace -- a unique identifier that follows the request through its entire lifecycle. Each service that handles the request creates a **span** within that trace, recording when it received the request, what it did, and when it finished. Spans are nested: the top-level span covers the entire request, and child spans cover each sub-operation. The trace, with all its spans, is exported to a backend like Jaeger or Grafana Tempo for storage, visualization, and querying.

## Designing Trace Structure for AI Requests

Not all spans are equally useful. A trace with fifty undifferentiated spans is noise. A well-designed trace with eight carefully structured spans tells a clear story about what happened to the request. The span design should reflect the logical stages of AI request processing, not the internal implementation details of each service.

A production AI inference trace should include, at minimum, seven distinct span types. The **ingress span** covers the time from request arrival at the load balancer to delivery to the first application service, capturing network and routing latency. The **routing span** covers the model selection or routing decision -- which model, which version, which GPU pool. The **retrieval span** covers any RAG or context retrieval, including vector database queries, document fetching, and context assembly. The **inference span** covers the core model computation, from prompt receipt by the inference engine through token generation to completion. This span should include attributes for GPU ID, batch position, input token count, output token count, time to first token, and whether the request was queued. The **guardrail span** covers any post-inference safety or compliance checks. The **post-processing span** covers response formatting, citation insertion, or any transformation applied to the model output. The **response span** covers the return path from the application to the user, capturing any serialization or network latency.

Each span should carry AI-specific attributes as tags. The inference span should tag the model version, the quantization level, the serving framework, and the GPU type. The retrieval span should tag the number of documents retrieved, the vector database cluster, and the embedding model used. These attributes become queryable dimensions in your observability platform, letting you answer questions like "what is the p95 inference latency for requests using model version 3.2 on H100 GPUs with INT8 quantization?"

## Cross-Region Tracing

When your AI infrastructure spans multiple regions, tracing becomes both more complex and more valuable. A request that originates in one region and is served in another crosses network boundaries that add latency and introduce failure modes that do not exist in single-region deployments.

Cross-region tracing requires that the trace context -- the trace ID and the parent span ID -- propagates across every network boundary, including inter-region calls. OpenTelemetry supports context propagation through HTTP headers using the W3C Trace Context standard. When your request router in Frankfurt decides to forward a request to the inference cluster in Amsterdam, the trace context must be included in the forwarded request so that the Amsterdam inference span becomes a child of the Frankfurt routing span. If the context is dropped at any boundary, you lose visibility into what happened on the other side.

The most common failure in cross-region tracing is context loss at service mesh boundaries. If you use a service mesh like Istio, verify that the mesh's sidecar proxies propagate trace context headers. By default, some mesh configurations drop custom headers during inter-region routing. This creates traces that appear to end at the regional boundary, making it impossible to diagnose cross-region latency issues.

Cross-region traces also reveal geographic latency patterns that inform capacity planning. If traces consistently show 45 milliseconds of network latency between Frankfurt and Amsterdam for inference forwarding, and your latency SLO is 500 milliseconds, that 45-millisecond tax consumes 9 percent of your latency budget before any computation starts. This data drives the decision to deploy model replicas in Frankfurt rather than forwarding to Amsterdam.

## The Cost of Tracing and Sampling Strategies

Full distributed tracing at scale generates enormous volumes of data. Each trace includes multiple spans, each span includes multiple attributes, and a busy AI service handles millions of requests per day. Storing every trace is expensive and often unnecessary. The solution is sampling -- capturing a representative subset of traces while ensuring that the most valuable traces are always captured.

**Head-based sampling** makes the sampling decision at the start of the request. Before any processing occurs, the system decides whether this request will be traced. A 10 percent head-based sample captures roughly 10 percent of requests, which is sufficient for understanding normal behavior and calculating latency distributions. The downside is that you might miss the rare, interesting requests -- the ones that failed, timed out, or exhibited unusual behavior.

**Tail-based sampling** solves this problem by making the sampling decision after the request completes. A tail-based sampler captures all traces that meet certain criteria: latency exceeding a threshold, error status, specific model versions, or any other attribute. This ensures that you always have traces for the requests you most need to investigate. The trade-off is that tail-based sampling requires temporarily storing all traces until the sampling decision is made, which increases the infrastructure requirements for the tracing pipeline itself.

The practical approach for most AI platforms combines both strategies. Head-based sampling at 5 to 10 percent captures baseline behavior. Tail-based sampling captures 100 percent of traces that exceed your latency SLO, return errors, involve failover routing, or trigger guardrail interventions. This hybrid approach keeps storage costs manageable while ensuring that every interesting request is fully traced.

For AI inference specifically, consider sampling based on AI-specific criteria. Trace every request that was truncated due to context length limits. Trace every request that was rerouted to a different model or region. Trace every request where the guardrail service flagged the output. These are the requests where tracing provides the most diagnostic value, and they are often rare enough that capturing 100 percent of them does not significantly increase storage volume.

## From Traces to Operational Insights

Traces are diagnostic tools, not just debugging tools. Beyond investigating individual slow requests, trace data in aggregate reveals systemic patterns that drive infrastructure decisions.

Latency breakdown analysis shows where time is spent across the pipeline. If retrieval consistently accounts for 40 percent of total latency, optimizing the vector database or adding caching will have more impact than adding GPU capacity. If inference accounts for 70 percent and retrieval for only 10 percent, the opposite is true. Without trace data, teams often invest in optimizing the wrong component because they assume the most expensive hardware (GPUs) is the bottleneck.

Dependency analysis shows which services are on the critical path. If the guardrail service adds 30 milliseconds to every request but never actually blocks anything in your traffic pattern, you might move it to an asynchronous path to reduce latency. If the routing decision adds 5 milliseconds and occasionally routes to a suboptimal model, that 5 milliseconds might be worth paying. These trade-offs become visible only when you can see the full request path.

Failure correlation analysis shows which component failures cascade to user-visible errors. A vector database timeout might cause the retrieval span to fail, which causes the orchestration layer to retry, which increases inference queue depth, which raises latency for unrelated requests. This cascade is visible in trace data as a chain of spans with increasing latency, originating from a single failure point. Without tracing, you see elevated latency across the board and have no idea which component started the cascade.

---

Tracing reveals where time and resources are spent across your AI pipeline. But there is a physical layer beneath all the software -- the electricity powering your GPUs, the cooling systems keeping them from throttling, and the carbon footprint of every inference request. The next subchapter examines the power equation: why energy, cooling, and sustainability have become infrastructure constraints that shape AI platform design in 2026.
