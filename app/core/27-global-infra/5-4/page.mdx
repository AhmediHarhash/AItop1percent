# 27.36 — Cross-Region Model Synchronization: Keeping Weights, Configs, and Adapters Consistent

A model that is version 3.2 in us-east and version 3.1 in eu-west is not a multi-region deployment. It is a consistency bug waiting to produce different answers for the same question depending on where the user connects. Model replication, covered in the previous subchapter, solves the availability problem — the model exists in the regions where it needs to exist. Synchronization solves the consistency problem — every region serves the same model version, with the same configuration, the same tokenizer, and the same adapter weights, at all times. These are different problems. Teams that conflate them discover the difference when a customer reports that the same prompt returns a confident answer in one region and a hallucinated non-answer in another, and the root cause turns out to be a version mismatch that nobody detected for eleven days.

The synchronization challenge is not limited to raw model weights. A modern inference deployment is a composite artifact: the base model weights, the tokenizer vocabulary and merge rules, the generation configuration (temperature, top-p, repetition penalty, stop sequences), any LoRA adapters applied at serving time, and the prompt templates that wrap user input before it reaches the model. Change any one of these components, and the model's behavior changes. Synchronize the weights but forget the prompt template update, and you have a deployment where three regions produce the new behavior and two regions produce the old one. The weights are identical. The outputs are not.

## The Consistency Window Problem

When you push a model update across five regions, you cannot update all five simultaneously down to the millisecond. Network transfers take time. Regional mirrors must receive, verify, and stage the new artifacts. Inference pods must be rolled from the old version to the new one. During this window — which ranges from minutes to hours depending on your architecture — different regions are serving different model versions. This is **the consistency window**, and every multi-region AI system has one. The question is not whether it exists but how long it lasts and whether you manage it deliberately or discover it accidentally.

During the consistency window, a user whose requests are load-balanced across regions — or who switches from a mobile connection to a Wi-Fi connection, changing their geographic routing — can receive responses from two different model versions within a single session. For a chatbot that improved its tone between versions, the difference might be subtle. For a model that changed its classification boundaries — say, a content moderation model that tightened its thresholds — the difference can be functionally visible. A post that is flagged as policy-violating in one region and approved in another, purely because the regions are serving different model versions, creates a user trust problem and a potential compliance issue.

The teams that manage this well treat the consistency window as a first-class operational metric. They measure it, alert on it, and set SLOs for it. A common target is a consistency window of less than 15 minutes for non-critical model updates and less than five minutes for critical updates that change classification or safety behavior. Achieving these targets requires deliberate architecture — not heroic manual coordination.

## Versioned Immutable Artifacts

The foundation of reliable synchronization is treating every model version as an **immutable artifact** with a unique, content-addressed identifier. When you publish model version 3.2, it is not a mutable file that gets overwritten — it is a new, distinct object with a cryptographic hash that serves as its version identity. The weights, tokenizer, generation config, and any adapters are bundled or referenced together under a single version manifest. That manifest is the unit of deployment: when you say "deploy version 3.2 to eu-west," you mean "deploy the exact bundle identified by this hash," and there is no ambiguity about what that bundle contains.

Content-addressed versioning eliminates an entire class of synchronization failures. When version identifiers are sequential numbers or human-readable tags, a region can report it is running "version 3.2" while actually serving a corrupted or incomplete copy. With content-addressed hashes, verification is deterministic: compute the hash of the artifact in each region and compare it to the expected hash. If the hashes match, the artifacts are identical down to the byte. If they don't, the region has a bad copy and must re-pull. This verification takes seconds and catches corruption, partial transfers, and storage bit-rot that sequential version numbers cannot detect.

The OCI (Open Container Initiative) artifact format has become the standard packaging mechanism for model artifacts in 2026. Originally designed for container images, OCI registries now store model weights, adapters, and configuration files as first-class artifacts. Tools like ORAS (OCI Registry as Storage) allow teams to push, pull, tag, and verify model artifacts using the same infrastructure and workflows they already use for container images. Storing model artifacts in OCI registries means you get content-addressed digests, multi-region replication through standard registry mirroring, and a pull-verify-deploy pipeline that integrates natively with Kubernetes.

## Push-Based Versus Pull-Based Synchronization

There are two fundamental patterns for distributing model updates to multiple regions, and each has a distinct failure mode that shapes your operational posture.

**Push-based synchronization** uses a central controller that, upon detecting a new model version in the registry, initiates parallel transfers to every regional mirror. The controller owns the distribution process: it tracks which regions have received the new version, retries failed transfers, and gates deployment on successful distribution. The advantage is centralized visibility — the controller knows exactly which regions are up to date and which are lagging. The disadvantage is that the controller is a single point of failure and a bottleneck. If the controller crashes, fails to detect a new version, or loses track of a region's state, synchronization stalls silently.

**Pull-based synchronization** has each region independently poll the central registry for new versions on a fixed schedule — every five minutes, every fifteen minutes, or whatever cadence matches your deployment velocity. When a region detects a new version, it pulls the artifact locally and stages it for deployment. The advantage is resilience: no single controller can stall the entire fleet, and each region manages its own sync lifecycle. The disadvantage is the polling delay. If regions poll every ten minutes, the worst-case consistency window is ten minutes plus transfer time plus rollout time. For safety-critical models where you need all regions updated within five minutes, pull-based sync at a ten-minute interval does not meet the SLO.

The hybrid approach used by most mature teams combines both patterns. A push notification — a lightweight message, not the full artifact — tells regions that a new version is available immediately. Each region then pulls the artifact on its own. This gives you the immediacy of push with the resilience of pull. If the push notification fails to reach a region, the periodic pull catches up within the polling interval. If the pull fails, the central controller detects the missing confirmation and can retry the push notification or alert the on-call team.

## Atomic Rollout Versus Staged Rollout

Once every region has the new model artifact staged locally, the next question is how to activate it. There are two schools of thought, and the right choice depends on what you are more afraid of: inconsistency or blast radius.

**Atomic rollout** means all regions switch to the new version simultaneously, within as tight a window as coordination allows. The central controller sends a "switch" signal, and every region begins rolling its inference pods to the new version at the same time. The consistency window is as short as the slowest region's rollout time — typically two to five minutes for a rolling pod update. The advantage is minimal version skew: users in different regions see the same model behavior within minutes. The disadvantage is blast radius. If the new version has a quality regression, it affects every region at once. There is no canary period where you serve the new version to a subset of users and observe the results.

**Staged rollout** deploys the new version to regions sequentially — canary region first, then tier-one regions, then the remainder — with observation periods between each stage. The canary region serves the new version for 15 to 60 minutes while monitoring dashboards track quality metrics, error rates, and latency. Only if the canary passes does the rollout proceed. The advantage is controlled blast radius: a bad model version is detected in the canary and never reaches the rest of the fleet. The disadvantage is a longer consistency window — potentially hours — during which different regions serve different versions.

The practical compromise is to separate safety-critical updates from routine updates. Routine model updates — minor quality improvements, training data refreshes — use staged rollouts because the blast radius concern outweighs the consistency concern. Safety-critical updates — changes to content moderation thresholds, compliance adjustments, vulnerability patches — use atomic rollout because the consistency concern outweighs the blast radius concern. Having both mechanisms available and choosing per deployment gives you the flexibility to match the rollout strategy to the risk profile.

## The Adapter Synchronization Challenge

LoRA adapters create a synchronization problem that is smaller in bytes but larger in frequency. Base model weights might update weekly or monthly. Adapters — especially customer-specific adapters in a multi-tenant system — might update daily or even multiple times per day as new fine-tuning runs complete. A system with 40 customer adapters, each updating on its own schedule, produces 40 independent synchronization events per day across five regions. That is 200 regional sync operations daily, each requiring artifact transfer, verification, and pod-level adapter hot-swap.

The saving grace is size. A LoRA adapter is typically 50 to 500 megabytes, compared to tens of gigabytes for a base model. Transfer time is seconds, not minutes. The operational challenge shifts from bandwidth management to coordination management. You need to track which adapter version is active for which customer in which region, ensure that an adapter update for customer A does not accidentally affect customer B's serving path, and handle the case where an adapter update fails in one region but succeeds in others.

**Adapter registries** — a metadata layer on top of the artifact store — solve the tracking problem. Each adapter is registered with its base model compatibility, customer identifier, version hash, and deployment status per region. When a new adapter version is published, the registry records the pending update and the deployment pipeline distributes it. The inference server queries the registry at request time to determine which adapter version to load for each customer. If a region has not yet received the latest adapter, the inference server can either serve the previous version (with a version header indicating the lag) or route the request to a region that has the update.

Hot-swapping adapters — loading a new adapter version without restarting the inference process or evicting the base model from GPU memory — is a capability that modern inference frameworks like vLLM and TensorRT-LLM support in 2026. The adapter swap takes milliseconds because only the small adapter weights change while the base model remains loaded. This means adapter synchronization does not require pod restarts, rolling updates, or downtime. A background process detects the new adapter on local storage, loads it into GPU memory alongside the previous version, atomically switches new requests to the new version, and drains in-flight requests from the old version. The entire swap is invisible to users.

## Config Drift Detection

Model weights can be identical across regions while model behavior differs because of configuration drift. Temperature, top-p, repetition penalty, max token length, stop sequences, system prompts — any of these can differ between regions if they are managed outside the versioned artifact bundle. A common pattern is storing generation parameters in a separate configuration service (a key-value store, a feature flag system, or environment variables) rather than bundling them with the model artifact. This gives teams the flexibility to tweak parameters without redeploying the model. It also creates a synchronization gap: when someone changes the temperature from 0.7 to 0.3 in us-east and forgets to propagate the change to eu-west, the two regions produce measurably different output distributions from the same model weights.

**The Config Parity Check** is an automated process that runs on a fixed schedule — every five minutes is a reasonable cadence — and compares the full serving configuration across all regions. It retrieves the active model version hash, adapter version hash, generation parameters, prompt templates, and any feature flags that affect inference behavior from every region. It diffs the results. Any discrepancy triggers an alert to the platform team and optionally blocks new traffic from being routed to the drifted region until the configuration is reconciled.

The parity check should also run deterministic test queries. Send the same prompt with the same random seed to every region and compare the outputs token by token. If the outputs diverge, something in the serving stack differs — even if the configuration dump looks identical. This catches differences that configuration comparison alone misses: different CUDA driver versions, different quantization rounding behavior, different batch scheduling that changes output when the batch size affects attention computation. Deterministic test queries are the integration test for model synchronization. Config comparison is the unit test. You need both.

## Rollback Coordination

Rolling forward is hard. Rolling back is harder. When a new model version causes quality degradation that is detected after two of five regions have already switched, you need to roll those two regions back to the previous version while halting the rollout for the remaining three. The previous version's artifacts must still be cached in every region — if they were evicted to make room for the new version, rollback requires a fresh download, extending the incident window.

The rollback protocol follows a deterministic sequence. First, halt the rollout: the central controller immediately stops any in-progress deployments. Second, verify that the previous version's artifacts are present in every region that needs to roll back. If not, initiate an emergency pull from the central registry or from a region that still has the artifacts cached. Third, roll affected regions back by redeploying pods with the previous version. Fourth, update the adapter registry and config service to point back to the previous version's parameters. Fifth, run the config parity check to confirm all regions are now consistent. Sixth, mark the failed version in the model registry so it cannot be accidentally redeployed.

The most common rollback failure is the cache eviction race. The platform team pre-stages the new version, which evicts the old version from local NVMe caches to free space. The new version rolls out and fails. The team initiates a rollback and discovers the old version is no longer cached locally. Pods must download it from the regional object store — a process that takes minutes under normal conditions and longer under the elevated network load of a multi-region rollback event. The defense is a **rollback hold policy**: keep the previous model version cached in every region for at least 24 hours after a new version is deployed, even if it means maintaining two versions simultaneously on local storage. The storage cost is trivial compared to the incident cost of a delayed rollback.

---

Synchronized model artifacts give you consistency: every region serves the same model. But consistency alone does not determine which region serves which user. The next subchapter covers global traffic steering — how to route each request to the right region based on geography, health, capacity, and latency, and what happens when the right region is no longer available.
