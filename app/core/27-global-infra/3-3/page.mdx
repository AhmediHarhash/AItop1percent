# 27.19 — The Kubernetes Control Plane at Scale: etcd, API Limits, and Failure Modes

In late 2025, a computer vision company running autonomous driving workloads hit a wall that had nothing to do with GPUs. Their cluster had grown to 400 nodes with over 2,000 GPU pods, managed by Kueue, a custom training operator, and three different autoscaling controllers. One Monday morning, the API server stopped responding. kubectl commands timed out. New pods could not be created. Running pods continued to execute — the data plane was fine — but the control plane was effectively dead. The root cause was etcd. The backing store had reached its default two-gigabyte database size limit because thousands of Kueue workload objects, DRA ResourceClaim objects, and training job custom resources had accumulated without cleanup. Compaction was running but could not keep pace with the write rate. The team spent four hours in an emergency that affected no running workload directly but prevented any new workload from starting, any scaling event from completing, and any monitoring that depended on API server queries from functioning. The lesson was expensive but simple: in AI clusters, the control plane is not a passive background service. It is an active bottleneck that requires its own capacity planning, its own monitoring, and its own scaling strategy.

## Why AI Workloads Stress the Control Plane

Traditional Kubernetes workloads create a modest number of objects that change infrequently. A web service deployment creates a Deployment, a ReplicaSet, a Service, a few ConfigMaps, and some pods. Those objects are created once and updated rarely — maybe during a rollout or a scaling event. The API server and etcd handle this comfortably even at thousands of services.

AI workloads are different in three ways that compound. First, they create far more objects per workload. A single training job managed by Kueue creates a Workload object, a Job object, one or more pod objects, ResourceClaim objects if using Dynamic Resource Allocation, PodGroup objects if using gang scheduling, and potentially LeaderWorkerSet objects for distributed training coordination. A cluster running 200 concurrent training jobs can easily have 5,000 to 10,000 custom resource objects beyond the pods themselves. Second, these objects change state frequently. Training jobs transition through queued, admitted, running, suspended, and completed states. Kueue continuously updates workload status, quota usage, and admission decisions. Autoscalers update node counts and GPU allocations. The write rate to etcd is an order of magnitude higher than a web-services cluster of similar pod count. Third, AI workloads use watchers aggressively. Every controller — Kueue, the training operator, the DRA controller, the metrics exporter — watches the API server for changes to its managed resources. Each watcher maintains an open connection and receives every relevant update. At scale, the number of active watch connections can overwhelm the API server's connection pool.

## The etcd Bottleneck

**etcd** is the distributed key-value store that holds all Kubernetes cluster state. Every object you create — every pod, service, configmap, secret, custom resource — is serialized and written to etcd. Every read from the API server that cannot be served from cache hits etcd. The health of your cluster is, in a very literal sense, the health of etcd.

The default etcd database size limit is two gigabytes. This sounds small, and it is. A cluster with heavy custom resource usage — Kueue workloads, DRA ResourceClaims, training job CRDs, inference serving CRDs — can approach this limit within months if old objects are not cleaned up. When etcd hits the size limit, it enters a read-only alarm state. No new objects can be created. No existing objects can be updated. The cluster is functionally frozen until an administrator manually compacts the database, defragments it, and clears the alarm.

The first mitigation is increasing the database size limit. Eight gigabytes is a reasonable starting point for AI clusters; some organizations push to sixteen. But size alone is not the fix. The real problem is write throughput and compaction cadence. etcd compacts old revisions automatically — the Kubernetes API server triggers compaction every five minutes by default — but compaction only marks space as reclaimable. It does not actually free that space in the database file. Defragmentation reclaims the space, but it is a blocking operation that prevents reads and writes while it runs. On a busy cluster, defragmentation can take ten to thirty seconds per gigabyte of data, during which the API server's requests to that etcd member fail or queue.

## Compaction and Defragmentation Strategy

**Compaction** removes obsolete key revisions from etcd. When an object is updated, etcd keeps the old revision until compaction removes it. The Kubernetes API server runs compaction every five minutes, retaining the last 10,000 revisions by default. For clusters with high write rates, this retention window may be too large, keeping revisions that no client will ever read.

**Defragmentation** reclaims the physical disk space that compaction marked as free. Without defragmentation, the etcd database file grows monotonically even as compaction removes logical data. The file becomes internally fragmented — full of holes where deleted data used to live — and read performance degrades as the storage engine navigates around the gaps.

The strategy for AI clusters is to run defragmentation during low-activity windows, staggered across etcd members so that at least two of three members (or three of five) remain available at all times. Never defragment all members simultaneously. Monitor the ratio of database size to actual data size — the etcd metrics endpoint exposes both. When the database file is more than twice the size of the actual data, defragmentation is overdue. Automate this with a CronJob or an external tool like etcd-defrag, which handles the staggered execution and health checks automatically. Teams that wait for etcd to hit its size limit before defragmenting are playing a game they will eventually lose.

## API Server Rate Limiting and Priority

The Kubernetes API server processes every request — creates, updates, deletes, lists, watches — in the cluster. Without protection, a single misbehaving controller can flood the API server with requests and degrade it for every other client, including the scheduler, the kubelet, and human operators running kubectl.

**API Priority and Fairness** is the built-in mechanism for protecting the API server. It classifies incoming requests into priority levels and flow schemas, then allocates a share of the server's total request-handling capacity to each level. System-critical requests — from the scheduler, the kubelet, the controller manager — get the highest priority and are never starved. Requests from custom controllers get a lower share and can be queued or rejected when the server is under pressure.

For AI clusters, the default API Priority and Fairness configuration is rarely sufficient. You need to create custom FlowSchemas that identify your Kueue controller, your training operator, and your DRA controller as higher-priority than generic user requests but lower-priority than system components. Without this, a user running a broad kubectl get command across all namespaces can compete with the Kueue controller for API server capacity, causing workload admission delays. Conversely, a buggy custom controller that enters a tight reconciliation loop can starve the scheduler unless it is placed in a flow schema with strict concurrency limits.

Monitor API server request latency at the P99 level. When P99 latency for mutating requests exceeds 500 milliseconds, the control plane is under pressure. When it exceeds one second, scheduling delays become visible to users. When it exceeds five seconds, the cluster is in a degraded state that requires immediate intervention — either by throttling the offending client, adding API server replicas, or both.

## Scheduler Pressure and Topology Complexity

The Kubernetes scheduler assigns pods to nodes, and for traditional workloads this is a fast operation — the scheduler evaluates available CPU and memory on each node and picks a suitable one in milliseconds. AI workloads make scheduling dramatically harder.

Dynamic Resource Allocation turns GPU scheduling from a simple integer comparison into a constraint satisfaction problem. The scheduler must evaluate which nodes have GPUs of the correct type, with sufficient memory, connected via the right topology — NVLink versus PCIe — and available through the DRA driver. For a distributed training job requesting thirty-two GPUs across four nodes, the scheduler must find four nodes with eight GPUs each, all in the same network topology for efficient NCCL communication, all with available ResourceClaims that match the requested GPU flavor. This is computationally expensive. On clusters with hundreds of nodes and dozens of GPU types, scheduling a single gang-scheduled training job can take seconds rather than milliseconds.

The cumulative effect is that at high submission rates — dozens of training jobs per minute during a hyperparameter sweep — the scheduler falls behind. Jobs sit in Pending state not because resources are unavailable but because the scheduler has not yet processed them. The mitigation is twofold. First, use Kueue to gate admission so that only workloads with available resources reach the scheduler, reducing the number of unschedulable pods it must evaluate and reject. Second, consider scheduler sharding or deploying a secondary scheduler for specific workload types, though this adds operational complexity.

## The 5,000-Node Question

Kubernetes officially supports clusters up to 5,000 nodes with up to 150,000 pods. These limits assume web-service workloads with modest object counts and low churn. AI clusters hit control plane strain well before these numbers because of the object multiplication and write amplification described above.

In practice, most AI clusters start encountering control plane pressure between 500 and 1,000 nodes, depending on workload density and custom resource usage. A 500-node cluster running dense GPU workloads with Kueue, DRA, and a training operator can generate the same API server load as a 3,000-node cluster running web services. Google demonstrated a 130,000-node GKE cluster in late 2025, but that achievement required replacing etcd with a custom Spanner-backed storage layer, sharding the API server across multiple instances, and custom optimizations that are not available in upstream Kubernetes. For the rest of us, the practical ceiling is much lower.

When you approach control plane limits, the decision is whether to scale the control plane vertically — larger etcd nodes, more API server replicas, faster storage — or to split into multiple clusters. Vertical scaling buys time but has a ceiling. Larger etcd nodes help with storage but not with write throughput. More API server replicas help with read throughput but not with etcd write contention. At some point, splitting into multiple clusters with a multi-cluster orchestration layer like Admiralty or Liqo becomes the more sustainable path.

## Dedicated Control Plane Nodes

By default in self-managed clusters, control plane components run on a few designated nodes that also taint themselves to prevent user workloads from being scheduled alongside them. This is sufficient for small clusters but insufficient for AI-scale operations.

At scale, dedicate beefy hardware to control plane nodes. etcd in particular benefits from fast NVMe storage — its performance is almost entirely IO-bound. The API server benefits from high core counts and large memory for caching. Allocate at least three dedicated control plane nodes, each with NVMe storage for etcd, 32 or more CPU cores, and 128 gigabytes of memory. Monitor their resource utilization separately from the GPU worker nodes. A control plane node at ninety percent CPU is a warning sign; at ninety-five percent, you are one burst away from API server timeouts.

For managed Kubernetes services like EKS, GKE, and AKS, the control plane is managed by the provider, but you can often influence its sizing. EKS introduced Provisioned Control Plane in 2025, which lets you specify larger etcd and API server configurations for clusters running high-density AI workloads. GKE automatically scales the control plane based on cluster size but may need manual intervention for clusters with unusually high custom resource counts. AKS offers similar auto-scaling of the managed control plane. Regardless of provider, monitor the control plane metrics they expose — API server latency, etcd size, request queue depth — and treat them with the same seriousness as GPU utilization metrics.

## When to Split Versus When to Scale

The decision between scaling one cluster and splitting into multiple clusters is not purely technical. It is organizational.

Scale one cluster when all teams need access to a shared GPU pool, when borrowing and preemption across teams is a priority, and when the platform team has the expertise to tune control plane performance. The benefit is maximum GPU utilization and simpler workload migration between teams.

Split into multiple clusters when control plane pressure exceeds what vertical scaling can handle, when teams operate under different compliance requirements, when blast radius containment matters — a control plane failure in cluster A should not affect cluster B — or when the organizational overhead of mediating multi-tenant priority disputes in a single cluster exceeds the utilization benefit.

Most organizations reach the split decision between 800 and 1,500 GPU nodes, depending on workload density. The split usually follows workload class boundaries: a training cluster optimized for high-throughput batch jobs with large etcd limits and aggressive compaction, and an inference cluster optimized for low-latency serving with tighter control plane SLOs. This separation lets you tune each cluster's control plane for its specific workload pattern rather than compromising on a one-size-fits-all configuration that serves neither well.

---

A healthy control plane keeps the cluster running. But even a healthy control plane can be disrupted by upgrades — and in GPU clusters, upgrades involve not just Kubernetes versions but GPU drivers, CUDA toolkits, and device plugins that all have their own compatibility matrices. The next subchapter covers how to upgrade your AI cluster without taking it down.
