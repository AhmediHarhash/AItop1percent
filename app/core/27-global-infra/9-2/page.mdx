# 27.65 â€” GPU Metrics That Matter: Utilization, Memory, Thermal, and Error Rates

GPU utilization is the metric everyone watches and nobody understands. A GPU reporting 90 percent utilization can still be wasting half its potential throughput. The utilization number that NVIDIA's tools report by default measures whether the GPU is doing anything at all -- whether at least one kernel is executing on at least one Streaming Multiprocessor during the sampling interval. It does not measure how many SMs are active, how efficiently they are being used, or whether the workload is compute-bound or memory-bound. An inference engine running a small model with tiny batch sizes might report 85 percent GPU utilization while using only a fraction of the GPU's compute capability, because the GPU is technically busy on every sample but the kernels are too small to fill the hardware. Treating this number as "we are using 85 percent of our GPU" leads to terrible capacity planning decisions.

This subchapter breaks down the GPU metrics that actually matter for AI infrastructure operations. Each metric tells you something specific about what your hardware is doing, and more importantly, what it is failing to do. Knowing which metric to look at for which problem is the difference between diagnosing an issue in minutes and chasing ghosts for days.

## SM Utilization Versus GPU Utilization

The distinction between GPU utilization and SM utilization is the first concept every AI infrastructure operator needs to internalize. **GPU utilization**, reported as `DCGM_FI_DEV_GPU_UTIL` in DCGM metrics, measures the percentage of time during the sampling period when at least one GPU kernel was executing. Think of it as an occupancy indicator for the GPU as a whole: was it doing something, or was it idle? This metric tells you whether the GPU has work to do. It does not tell you how much of the GPU's capacity that work consumed.

**SM occupancy** measures something far more useful: how many of the GPU's Streaming Multiprocessors are active and how many warps (groups of 32 threads) are in flight on each SM. An H100 GPU has 132 SMs. If your inference workload keeps only 40 of them busy during each kernel launch, the GPU reports high utilization (it is always busy) but low SM occupancy (most of the compute hardware sits idle within each kernel). This pattern is common with small batch sizes or short sequence lengths, where the workload does not generate enough parallel work to fill the GPU.

For AI inference operations, GPU utilization is a necessary but insufficient metric. If GPU utilization is low, you have an obvious problem: the GPU is idle and you are wasting money. If GPU utilization is high, you need to check SM occupancy and memory bandwidth to determine whether the GPU is actually performing well or just staying busy with inefficient work. The operational target for inference workloads is GPU utilization above 80 percent with SM occupancy above 60 percent. If utilization is high but occupancy is low, investigate batching configuration, request routing, and model-level optimizations.

## GPU Memory: Allocation, Fragmentation, and the KV Cache

GPU memory metrics are the single most important category for AI inference operations, because memory is the resource that runs out first and fails most quietly.

**Framebuffer memory used** (`DCGM_FI_DEV_FB_USED`) reports how many megabytes of the GPU's high-bandwidth memory are currently allocated. For an H100 with 80 gigabytes of HBM3, this number tells you how much of that 80 gigabytes is in use. But "in use" is misleading. Memory can be allocated but not actively used -- reserved by the inference engine for potential future requests, held by the CUDA memory allocator in free pools, or fragmented into small blocks that cannot be combined.

**Memory fragmentation** is the silent killer of AI inference performance. The KV cache -- the key-value pairs stored during autoregressive generation -- is the largest dynamic memory consumer in LLM inference. As requests arrive and complete, the KV cache allocates and frees memory blocks of varying sizes. Over time, the free memory becomes fragmented into small, non-contiguous blocks. The total free memory might be 20 gigabytes, but the largest contiguous block might be only 2 gigabytes. When a request arrives that needs a 4-gigabyte KV cache allocation, it fails even though total free memory is sufficient. The inference engine must either reject the request, truncate the context to fit a smaller allocation, or trigger a costly compaction operation.

Modern inference engines like vLLM address fragmentation through **PagedAttention**, which manages KV cache memory in fixed-size pages similar to how operating systems manage virtual memory. PagedAttention reduced memory waste from 60 to 80 percent in naive implementations to under 4 percent, enabling two to four times higher throughput on the same hardware. But even with PagedAttention, memory monitoring remains critical. Track the percentage of KV cache pages in use, the rate of page evictions, and the average number of pages per request. Rising eviction rates signal that your cluster is approaching memory capacity -- add GPUs or reduce maximum context lengths before users start experiencing truncated responses.

**Memory bandwidth utilization** (`DCGM_FI_DEV_MEM_COPY_UTIL`) measures how much of the GPU's memory bandwidth is being consumed. An H100's HBM3 provides roughly 3.35 terabytes per second of memory bandwidth. Many LLM inference workloads are memory-bandwidth-bound rather than compute-bound, particularly during the autoregressive decode phase where each token generation reads the entire model's weights but performs relatively little computation per parameter. If memory bandwidth utilization is consistently above 80 percent, adding more compute will not help -- you need either more GPUs to distribute the memory reads or a quantized model that reduces the bytes read per parameter.

## Thermal Metrics: The Silent Performance Thief

GPUs have thermal envelopes, and exceeding them triggers automatic protective measures that reduce performance without generating errors or alerts in most monitoring systems. This makes thermal throttling one of the most insidious causes of AI infrastructure degradation.

An H100 GPU has a maximum operating temperature of approximately 83 degrees Celsius for the GPU die and a separate limit for the HBM memory. When the GPU approaches these limits, the hardware automatically reduces clock speeds -- a process called **thermal throttling**. A GPU that normally runs its SM clocks at 1,980 megahertz might throttle down to 1,500 megahertz or lower, reducing throughput by 25 percent or more. The GPU does not report an error. It does not fail a health check. It simply runs slower. If your inference engine has a fixed timeout, requests that normally complete in 300 milliseconds now take 400 milliseconds, and some start timing out. If your monitoring tracks only success and failure, you see an increase in timeouts but no indication of the cause.

Monitor GPU temperature (`DCGM_FI_DEV_GPU_TEMP`), memory temperature (`DCGM_FI_DEV_MEMORY_TEMP`), and thermal violation events (`DCGM_FI_DEV_THERMAL_VIOLATION`). Set alerts at 75 degrees Celsius for the GPU die as an early warning and at 80 degrees as a critical alert. Track the correlation between temperature and tokens-per-second output. If you see a consistent pattern where throughput dips as temperature rises during peak traffic hours, your cooling infrastructure is the bottleneck -- not your compute capacity.

In air-cooled environments, thermal issues often emerge gradually as ambient temperature changes with seasons or as dust accumulates in cooling systems. In liquid-cooled environments, monitor coolant flow rates and inlet temperatures. A clogged liquid cooling loop or a failing pump will raise GPU temperatures slowly over weeks before reaching the throttling threshold, giving you time to act if you are watching the metrics.

## ECC Errors: Hardware Degradation Made Visible

**Error Correcting Code** memory on GPUs detects and corrects bit errors in the high-bandwidth memory. ECC errors come in two flavors: single-bit errors (correctable) and double-bit errors (uncorrectable). The distinction matters enormously for AI infrastructure operations.

Single-bit ECC errors (`DCGM_FI_DEV_ECC_SBE_VOL_TOTAL`) are corrected automatically by the hardware. A small number of single-bit errors is normal -- cosmic rays, minor manufacturing variations, and thermal effects all cause occasional bit flips. A healthy GPU might accumulate a handful of single-bit errors per day. But a rising trend in single-bit errors indicates that the GPU's memory is degrading. Track the rate of single-bit errors over time, not just the instantaneous count. A GPU that goes from two errors per day to twenty errors per day to two hundred errors per day is heading toward failure.

Double-bit ECC errors (`DCGM_FI_DEV_ECC_DBE_VOL_TOTAL`) are not correctable. When a double-bit error occurs, the data in that memory location is corrupted. In an inference workload, this can mean corrupted model weights, corrupted KV cache entries, or corrupted intermediate activations. The result is model output that is wrong in unpredictable ways -- garbled text, nonsensical numbers, hallucinated content that differs from what the same model produces on a healthy GPU. A single uncorrectable ECC error should trigger an immediate alert. The GPU should be drained of workload and replaced or at minimum have its memory retested.

NVIDIA assigns XID error codes to specific GPU fault conditions. XID 48 indicates a double-bit ECC error. XID 63 indicates a row remapping event where the GPU has permanently disabled a faulty memory row and activated a spare. XID 74 indicates NVLink errors. Monitor XID events through the NVIDIA kernel driver logs and integrate them into your alerting pipeline. A GPU that generates XID 63 events is running on spare memory rows and has less redundancy available -- prioritize it for replacement at the next maintenance window.

## NVLink and PCIe: Interconnect Health

For multi-GPU inference where a single model spans two or more GPUs, interconnect bandwidth is a direct determinant of throughput. NVLink connects GPUs within a node at speeds far exceeding PCIe -- up to 900 gigabytes per second bidirectional in the current generation. PCIe Gen5 provides roughly 64 gigabytes per second per x16 slot. When the model's tensor parallel shards need to exchange activations at every layer, a degraded NVLink that drops to half speed cuts the effective throughput of the entire multi-GPU inference pipeline.

Monitor NVLink bandwidth utilization and NVLink error counts through DCGM. A rising NVLink error rate indicates a failing link or a loose physical connection -- both require hardware intervention. Also monitor PCIe replay counts, which indicate that data transmitted over the PCIe bus was corrupted and needed retransmission. Occasional replays are normal. Persistent high replay counts indicate a hardware issue with the PCIe link, the riser card, or the motherboard slot.

For clusters using InfiniBand for multi-node inference, extend interconnect monitoring to IB port error counters, link state, and effective bandwidth. A single degraded InfiniBand port can create a bottleneck for all traffic through that switch, affecting multiple GPUs and multiple model deployments.

## Power Draw: Cost, Capacity, and Throttling

GPU power consumption (`DCGM_FI_DEV_POWER_USAGE`) matters for three reasons. First, it directly correlates with cost -- electricity is a significant component of AI infrastructure spend. Second, it determines whether your data center can support additional GPUs -- power capacity is a hard limit. Third, power limit throttling occurs when a GPU reaches its configured power ceiling, reducing performance similarly to thermal throttling.

An H100 SXM has a thermal design power of 700 watts. An H200 also operates at 700 watts TDP. Blackwell B200 GPUs are rated for up to 1,000 watts in the GB200 configuration, though real-world draw for individual chips often runs closer to 600 watts depending on workload. Track average power draw per GPU over time. A GPU consistently drawing near its TDP is working hard, which is fine. A GPU that suddenly draws 200 watts less than its peers while running the same workload is likely throttling -- thermal or power-limited -- and deserves investigation.

Power metrics also enable per-query energy cost attribution. If a GPU draws 500 watts on average and processes 120 requests per minute, each request consumes approximately 250 watt-seconds, or about 0.07 watt-hours. Multiply by your electricity rate and you have a per-query energy cost. This feeds directly into the cost engineering practices described in Section 24.

## From Metrics to Operational Action

Collecting GPU metrics is step one. Translating them into operational decisions is where value is created. Build runbooks that map metric patterns to actions. GPU utilization high but SM occupancy low: investigate batch configuration and request routing. Memory utilization rising steadily: add capacity or reduce maximum context lengths. Temperature trending upward over weeks: schedule cooling maintenance. ECC errors increasing: schedule GPU replacement. Power draw inconsistent across identical GPUs: investigate hardware variance.

The best GPU operations teams build composite health scores that combine multiple metrics into a single per-GPU health indicator. A GPU is "green" when utilization, temperature, ECC errors, and interconnect health all fall within normal ranges. It transitions to "yellow" when any metric enters a warning band. It goes "red" when any metric exceeds its critical threshold. This composite score drives automated workload draining, where the scheduler stops sending new requests to a degraded GPU and migrates existing sessions before the GPU fails completely.

---

Understanding GPU metrics tells you whether your hardware is healthy and performing well. But hardware health is only one dimension of AI infrastructure observability. The next subchapter examines how to follow a single request across your entire AI pipeline -- from the moment it arrives at your load balancer to the moment the response leaves your network -- using distributed tracing.
