# 27.11 — Workload Queuing and Priority: Kueue, Volcano, and Production Job Management

In early 2025, a healthcare AI company with sixty-four H100 GPUs shared across four teams hit a crisis that had nothing to do with models, data, or product quality. The radiology team submitted a distributed training job requesting thirty-two GPUs. At the same time, the pathology team submitted a fine-tuning job requesting sixteen GPUs, and the NLP team submitted an inference benchmark requesting eight GPUs. All three jobs entered the Kubernetes scheduler simultaneously. The scheduler allocated thirty-two GPUs to radiology, sixteen to pathology, and eight to NLP — sixty GPUs total, with four remaining idle. Then the drug discovery team submitted an urgent training job requesting sixteen GPUs. Only four were available. The job sat pending. Sixteen hours later, it was still pending because no existing job had finished. The drug discovery team, facing a deadline, asked platform engineering to preempt the NLP benchmark. Platform engineering had no mechanism to do it. They manually deleted the NLP pods, freeing eight GPUs — still not enough. They then deleted the pathology job, which had been running for eleven hours without checkpointing, losing all progress. The pathology team lost eleven hours of compute. The drug discovery team finally got their GPUs. The NLP team resubmitted their benchmark and waited another six hours. Total wasted GPU-hours across all teams: over four hundred. Total cost at roughly thirty dollars per GPU-hour: twelve thousand dollars. Total engineer time spent on manual coordination: three days across four teams.

This is what happens when you run GPU workloads on Kubernetes without a queuing layer. The story is unremarkable. Some version of it plays out at every organization that shares GPU clusters across multiple teams without workload management. The default Kubernetes scheduler was not designed for this problem, and pretending it was is one of the most expensive mistakes in AI platform engineering.

## Why the Default Scheduler Fails for AI

The Kubernetes default scheduler is excellent at what it was designed to do — place pods on nodes with sufficient resources as quickly as possible. For web services, this is the right behavior. A new replica of a stateless API server should start immediately on any node that has capacity. Speed of placement is the priority.

GPU workloads invert these priorities. Speed of placement matters less than quality of placement. A distributed training job that starts immediately on suboptimal GPU topology wastes more resources than a job that waits ten minutes for the right hardware to become available. A fine-tuning run that preempts a nearly-complete training job destroys more value than a run that queues for an hour. The default scheduler has no concept of waiting, no concept of workload value, and no concept of resource fairness across teams. It operates on a simple principle: if resources are available, place the pod now. If resources are not available, mark the pod as unschedulable and move on.

This creates three specific failure modes on shared GPU clusters. The first is **starvation** — a team that submits large jobs monopolizes the cluster while smaller teams cannot get any allocation at all. The default scheduler has priority levels, but priority only determines which pending pod gets resources first when they become available. It does not redistribute resources from low-priority running workloads to high-priority incoming ones without manual intervention. The second failure mode is **resource hoarding** — teams learn that resources are scarce and competitive, so they over-request to guarantee allocation. A team that needs sixteen GPUs requests thirty-two, "just in case," and sits on idle capacity that other teams could use. The third failure mode is **partial allocation deadlock** — multiple distributed jobs each receive some but not all of their required GPUs. None can start, but each holds resources the others need. The cluster is fully allocated but completely unproductive.

## The Queuing Layer: What It Actually Does

A workload queuing layer sits between job submission and the Kubernetes scheduler. When a team submits a GPU workload, it does not go directly to the scheduler. Instead, it enters a queue managed by a controller that makes admission decisions — should this job be scheduled now, should it wait, should it preempt something else, or should it be rejected entirely. Only after the queuing controller admits a workload does it reach the Kubernetes scheduler for actual pod placement.

This separation is powerful because it introduces the concept of time into scheduling decisions. The default scheduler operates in a single instant — are resources available right now? The queuing layer operates across time — are resources available now, will they be available soon, and what is the fairest way to distribute them across all waiting workloads? This temporal awareness is what enables fair-share scheduling, resource borrowing, and intelligent preemption.

## Kueue: The 2026 Standard for Multi-Tenant GPU Clusters

**Kueue** is a Kubernetes-native job queuing system developed as an official Kubernetes SIG project with significant investment from Google and broad community contribution. By 2026, it is the default recommendation from the CNCF and major cloud providers for managing batch, training, and inference workloads on shared GPU clusters. Its API has stabilized at v1beta2, and it powers production workloads at organizations ranging from AI startups to CoreWeave's infrastructure, where it manages scheduling across tens of thousands of GPUs for some of the most demanding AI labs in the world.

Kueue's architecture revolves around four core concepts, and understanding them is necessary for operating any serious GPU cluster.

A **ClusterQueue** is a cluster-wide resource pool. It defines how many GPUs, how much CPU, and how much memory are available for a specific category of workloads. A cluster might have three ClusterQueues — one for training with a quota of forty GPUs, one for inference with twenty GPUs, and one for experimentation with four GPUs. The ClusterQueue is where you express your organization's resource allocation policy at the highest level.

A **LocalQueue** is a namespaced resource that maps team boundaries to ClusterQueues. Each Kubernetes namespace — typically representing a team or project — has a LocalQueue that points to the ClusterQueue where its workloads will be admitted. When the NLP team submits a job in their namespace, it enters the NLP team's LocalQueue, which is bound to the training ClusterQueue. This mapping gives platform teams centralized control over capacity allocation while giving application teams a familiar, namespace-scoped interface for job submission.

A **ResourceFlavor** describes a type of hardware in the cluster. If your cluster has both H100-80GB and A100-40GB GPUs, you define a ResourceFlavor for each. ClusterQueues reference ResourceFlavors in their quota definitions — the training queue might have thirty H100 GPUs and ten A100 GPUs. When a workload enters the queue, Kueue matches its resource requirements against available flavors. This prevents the scenario where a training job that needs H100s consumes the A100 quota and starves inference workloads that could run on A100s.

**Admission control** is the mechanism that ties everything together. When a workload enters a LocalQueue, Kueue evaluates whether the associated ClusterQueue has sufficient quota to run it. If yes, the workload is admitted — Kueue unsuspends the job and lets the Kubernetes scheduler place its pods. If no, the workload remains in the queue, suspended, consuming no cluster resources. It waits until quota becomes available through job completion, preemption, or borrowing.

## Fair-Share, Borrowing, and Preemption

The real power of Kueue emerges in how it handles contention — the inevitable situation where demand for GPUs exceeds supply.

**Fair-share scheduling** distributes resources proportionally across teams based on their historical usage. A team that has consumed fewer GPU-hours relative to its share gets priority over a team that has consumed more. This prevents the starvation problem — no single team can monopolize the cluster indefinitely. Kueue's fair-share implementation tracks cumulative resource consumption per LocalQueue over a configurable time window and uses this history to order pending workloads. Teams that have been underserved receive priority. Teams that have been overserved wait.

**Borrowing** is Kueue's mechanism for maximizing utilization. When the training ClusterQueue has forty GPUs allocated but only twenty in use, the remaining twenty sit idle under strict quota enforcement. Borrowing allows the inference queue or the experimentation queue to temporarily use those idle GPUs. When the training queue needs them back — because a training job arrives that would consume the full quota — the borrowed capacity is reclaimed. The key configuration is the **Cohort**, a grouping of ClusterQueues that can borrow from each other. ClusterQueues in the same Cohort share idle capacity. ClusterQueues in different Cohorts are isolated. This gives platform teams fine-grained control over which teams can share with each other.

**Preemption** is the mechanism of last resort. When a high-priority workload arrives and there is not enough idle or borrowable capacity, Kueue can preempt lower-priority running workloads to free resources. Preemption policies are configurable — you can preempt only workloads in the same ClusterQueue, only borrowed workloads in the Cohort, or workloads across queues based on priority level. The preemption is graceful: Kueue suspends the preempted workloads, sending a termination signal that gives them time to checkpoint state before yielding resources. Well-designed training jobs that checkpoint regularly lose minutes of progress, not hours. Poorly designed jobs that never checkpoint lose everything — which is itself a powerful incentive for teams to implement proper checkpointing.

## Volcano: Gang Scheduling for Distributed Training

**Volcano** solves a different but equally critical problem. While Kueue manages admission and fair-share across a cluster, Volcano focuses on the scheduling mechanics of workloads that require coordinated multi-pod placement — specifically, distributed training jobs where all pods must start simultaneously or not at all.

The gang scheduling problem is deceptively dangerous. Consider a distributed training job that requires four pods, each with eight GPUs. Without gang scheduling, the Kubernetes default scheduler will place pods as resources become available. It might place three pods immediately and leave the fourth pending because only twenty GPUs are free and the fourth pod needs eight more. Those three running pods are now consuming twenty-four GPUs, doing nothing, waiting for the fourth pod to start so the distributed training collective can initialize. If another job's pods are in the same situation — three of four running, waiting for the fourth — the cluster reaches a deadlock. Both jobs hold resources the other needs. Neither can make progress. The resolution requires manual intervention: an operator must identify the deadlock, decide which job to kill, and restart it.

Volcano prevents this by treating the gang as a single scheduling unit. Either all four pods can be placed simultaneously, or none of them start. The resources remain available for workloads that can actually use them. This all-or-nothing semantics eliminates partial allocation deadlock entirely.

Beyond gang scheduling, Volcano provides queue management with fair-share policies, job dependency chains where one job can wait for another to complete before starting, and plugin-based scheduling algorithms including bin-packing, priority-based placement, and topology-aware scheduling. Volcano's 2025 releases added LeaderWorkerSet support for large model inference, network topology-aware scheduling, and dynamic MIG partitioning for GPU virtualization. Its feature set has grown substantially beyond its original gang-scheduling focus.

## Kueue or Volcano: When to Use Each

The choice between Kueue and Volcano is not a binary — many organizations run both — but the default recommendation for most teams in 2026 starts with Kueue and adds Volcano only when specific scheduling needs require it.

Kueue is the better choice for mixed workloads — clusters that run training, inference, and batch processing from multiple teams. Its ClusterQueue and LocalQueue model maps naturally to organizational boundaries. Its fair-share, borrowing, and preemption mechanisms are designed for the multi-tenant reality of shared GPU infrastructure. Kueue integrates with the standard Kubernetes scheduler rather than replacing it, which means it works with existing scheduling features — node affinity, pod topology spread, DRA claims — without conflict.

Volcano is the better choice when your primary workload is large-scale distributed training with strict gang scheduling requirements. If you routinely run training jobs that span dozens of nodes and cannot tolerate partial starts, Volcano's scheduling engine is purpose-built for that pattern. Volcano replaces the default Kubernetes scheduler for the workloads it manages, which gives it deeper control over placement decisions but also means it operates as a parallel scheduling path.

The common deployment pattern in production clusters is to use Kueue for admission control, queuing, and fair-share across the cluster, and to use Volcano's gang scheduling plugin for specific distributed training jobs that require all-or-nothing placement. Kueue manages who gets resources and when. Volcano manages how multi-pod training jobs are placed once admitted. This layered approach gives you the organizational fairness of Kueue and the scheduling precision of Volcano without choosing one at the expense of the other.

It is worth noting that Kubernetes 1.35, released in December 2025, introduced a native Workload API with built-in gang scheduling support. This upstream feature, still in alpha, aims to bring gang scheduling into the core scheduler over the next several releases. When it matures, it may reduce the need for Volcano as a separate component. But in 2026, Volcano remains the production-proven option for gang scheduling on Kubernetes.

## Configuring Kueue for a Real Cluster

Setting up Kueue on a production cluster requires mapping your organizational structure to its API objects. A typical configuration for a company with three AI teams sharing a pool of sixty-four H100 GPUs and thirty-two A100 GPUs might look like this.

You create two ResourceFlavors — one for H100-80GB and one for A100-40GB. You create a training ClusterQueue with a nominal quota of forty H100 GPUs and twenty A100 GPUs. You create an inference ClusterQueue with twenty H100 GPUs and twelve A100 GPUs. You create an experimentation ClusterQueue with four H100 GPUs and no A100 quota. All three ClusterQueues join the same Cohort, enabling borrowing between them. Each team gets a LocalQueue in their namespace bound to the appropriate ClusterQueue. Fair-share is enabled with a one-week rolling window for usage tracking. Preemption is configured to allow preemption of borrowed resources first, then same-queue low-priority workloads, and never cross-queue preemption of non-borrowed resources.

This configuration takes less than an hour to implement. The operational impact is immediate. Jobs that previously failed to schedule now queue gracefully with a clear position indicator. Teams that previously over-requested resources stop doing so because the queuing system guarantees they will eventually get capacity without hoarding. GPU utilization increases because borrowing ensures idle capacity is used. The twelve-thousand-dollar incident from the opening of this subchapter becomes impossible because the platform itself enforces the allocation policies that humans were trying to enforce with email threads and Slack messages.

## The Operational Discipline of Queuing

Deploying Kueue or Volcano is the engineering step. The harder step is the organizational one — defining the allocation policies that the queuing system enforces. How much GPU capacity does each team get? What happens when a team consistently exceeds its quota? Who decides which workload gets preempted during contention? What is the priority of a production inference endpoint versus a research training run versus a deadline-driven fine-tuning job?

These are not technical questions. They are organizational decisions that require input from engineering leadership, product management, and sometimes executive stakeholders. The queuing system makes these decisions enforceable and transparent, but it cannot make them for you. A platform team that deploys Kueue without clear allocation policies from leadership will spend its time mediating disputes between teams rather than building infrastructure.

The best practice is to establish allocation policies before deploying the queuing system, then encode those policies in ClusterQueue configurations. Review utilization data monthly. Adjust quotas quarterly. Use the fair-share data to have evidence-based conversations about allocation rather than subjective debates about whose work is more important. The queuing system turns resource allocation from a political negotiation into a data-driven process — but only if the organization commits to using the data.

---

With queuing and priority established — how workloads enter the system, how teams share capacity fairly, and how contention is resolved without human intervention — the next subchapter addresses a complementary strategy for GPU efficiency: splitting individual GPUs into isolated partitions through Multi-Instance GPU technology and time-slicing, allowing multiple workloads to share a single physical device without interfering with each other.
