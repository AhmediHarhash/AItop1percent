# 27.39 — Regional Capacity Planning in a GPU-Scarce World

In early 2026, a Series C logistics company decided to expand its AI-powered route optimization product into Asia-Pacific. The product ran on 70-billion-parameter models hosted on H100 GPUs. The engineering team had budgeted three months for the rollout: one month for infrastructure provisioning, one for testing, one for gradual traffic migration. They opened quota requests with their cloud provider for GPU instances in Singapore and Tokyo on day one. Two weeks later, they received a response. Singapore had zero H100 availability for on-demand instances and a four-month waitlist for reserved capacity. Tokyo had limited H100 inventory — enough for roughly 30 percent of what the team needed — with no timeline for additional supply. The team's US-East deployment, where they had provisioned GPU capacity eighteen months earlier during a period of relative abundance, had given them a false mental model. They assumed GPU availability was a global constant. It is not. It is a regional variable that shifts quarterly, differs by GPU generation, and varies between cloud providers in the same city. Their three-month rollout became a nine-month ordeal that required rearchitecting their model serving strategy, switching to quantized models that could run on more available GPU types, and negotiating reserved capacity contracts they had not budgeted for.

This failure is not unusual. GPU capacity planning for multi-region AI systems is a fundamentally different discipline from capacity planning for traditional compute. CPUs are commodities available in every region at effectively unlimited scale. GPUs are scarce, regionally concentrated, and subject to allocation policies that change without notice.

## The Regional GPU Map

GPU availability across cloud regions follows a pattern shaped by data center investment history, power infrastructure, and demand concentration. Understanding this map is the starting point for any multi-region capacity plan.

**North American regions** generally offer the best GPU availability. US-East (Virginia), US-West (Oregon), and US-Central (Iowa) are where AWS, Google Cloud, and Azure built their first large-scale GPU clusters. These regions received the earliest H100 deployments, have the deepest inventory, and typically have the shortest waitlists for new capacity. Even within North America, availability is not uniform. Smaller regions like Canada-Central or US-South may have limited GPU types or lower quotas. But if you need to provision a hundred H100 GPUs on short notice, a major US region is your most reliable bet.

**European regions** sit in the middle tier. EU-West (Ireland, Netherlands, Frankfurt) has solid GPU availability across the major providers, driven by GDPR data residency requirements that forced providers to invest in European GPU capacity. Nordic regions (Stockholm, Finland) are growing quickly thanks to cheap renewable power and favorable climate for cooling. Southern and Eastern European regions remain thin on GPU supply. If your compliance requirements demand inference within EU borders, the Western European hubs will serve you, but planning lead times of four to eight weeks for large allocations is prudent.

**Asia-Pacific regions** are where capacity planning gets hard. Singapore, Tokyo, and Sydney are the primary hubs, but GPU availability in these regions lags North America and Europe significantly. The Asia-Pacific data center GPU market is expanding rapidly — analysts project it will grow from approximately $6.7 billion in 2024 to over $44 billion by 2034 — but the demand growth is outpacing the supply expansion. Southeast Asian markets like Malaysia and Thailand are building new data center capacity, but most of that capacity is not yet GPU-dense enough for large-scale AI inference. Mumbai has emerging GPU availability but with higher latency variance and less mature networking infrastructure than the established hubs. If your Asia-Pacific expansion requires more than a handful of GPU instances, expect procurement timelines measured in months, not weeks.

**Middle East, Africa, and South America** remain the most constrained regions. Some providers offer GPU instances in Bahrain, Sao Paulo, or Johannesburg, but the available GPU types are often one or two generations behind the latest, quotas are tight, and reserved capacity options are limited. For these regions, the practical strategy is often to serve from the nearest well-provisioned region — EU-West for Middle East and Africa, US-East for South America — and accept the latency penalty, or to deploy smaller quantized models that can run on the GPU types that are available.

## Cloud Provider Quotas: The Hidden Gate

Even in regions with physical GPU capacity, you cannot use it until your quota allows it. Every major cloud provider enforces per-region GPU quotas, and the defaults for newer GPU types are frequently zero.

When you create a new cloud project or account, your default GPU quota for high-end instances like H100 or A100 is typically zero in every region. You must submit a quota increase request, which triggers a review process. For modest requests — four to eight GPUs — approval can come within 24 to 72 hours. For larger requests — 32 GPUs and above — the review can take one to four weeks, and the provider may approve less than you requested or suggest an alternative region with more availability.

The quota system exists because GPU capacity is physically limited. A provider cannot allocate what does not exist in a region, and they must balance allocations across hundreds of customers competing for the same pool. Your quota request is not just an administrative formality. It is a capacity reservation request that depends on what physical hardware is racked and powered in that specific data center.

The practical implication is that GPU quotas must be requested weeks or months before you need the capacity. If you wait until your traffic demands a new region, you are already behind. The teams that succeed at multi-region GPU deployment treat quota management as a procurement process, not an infrastructure provisioning step. They request quotas in target regions six months before planned expansion, even if they do not intend to use the capacity immediately. Having an approved quota does not cost you anything until you actually launch instances, so early requests carry zero financial risk and buy critical optionality.

## Reserved Capacity Versus On-Demand: The Commitment Tradeoff

Once your quota is approved, you face the reservation decision. On-demand GPU instances are available without commitment but may not be available when you need them. Reserved instances guarantee capacity but require a financial commitment.

On-demand pricing gives you flexibility. You pay by the hour, scale up and down freely, and owe nothing when you stop. The problem is availability. In constrained regions, on-demand GPU instances may simply not be available. You submit a request to launch eight H100 instances in Singapore, and the API returns a capacity error. There is no waitlist, no estimated availability — just a refusal. Your application either routes traffic elsewhere or degrades.

Reserved instances solve the availability problem. When you commit to a one-year or three-year reservation, the cloud provider allocates specific hardware to your account. That capacity is yours regardless of demand spikes from other customers. The tradeoff is cost commitment. One-year reserved H100 instances typically cost 30 to 40 percent less per hour than on-demand, but you pay whether you use the capacity or not. Three-year commitments push discounts to 50 to 60 percent but lock you into hardware that may be superseded by next-generation GPUs before the term ends.

The hybrid approach that most mature teams adopt is to reserve a baseline — the minimum GPU count that their steady-state traffic requires in each region — and rely on on-demand for burst capacity. If your Singapore deployment needs sixteen H100 GPUs at steady state and peaks to twenty-four during business hours, reserve sixteen and provision eight on-demand during peak. If on-demand is unavailable during a spike, your reserved baseline still handles the core load while overflow routes to the next nearest region. This hybrid model balances cost predictability with the reality that GPU availability fluctuates.

For large allocations exceeding 100 GPUs, the procurement timeline extends further. Cloud providers now offer future reservation programs where you can reserve capacity for a specific date up to one year in advance. These programs are designed for organizations that can forecast their needs and are willing to plan on enterprise procurement timelines rather than startup sprint cycles. If your multi-region expansion involves hundreds of GPUs, engage your cloud provider's sales team directly. The self-service quota and reservation systems are not designed for allocations at that scale.

## Sizing Regional Capacity to Actual Traffic Patterns

User traffic is not evenly distributed across the globe, and your regional capacity should not be either. A common mistake is to provision symmetric capacity in every region — the same number of GPUs in US-East, EU-West, and AP-Southeast — because symmetric feels safe. In practice, it means you are overspending in low-traffic regions and underspending in high-traffic ones.

Start with your traffic data. For most products with a global audience, US and EU traffic accounts for 60 to 80 percent of total requests. Asia-Pacific traffic typically represents 15 to 30 percent. Other regions contribute single-digit percentages. These proportions vary significantly by product and industry, so use your actual numbers, not industry averages. A gaming company with a strong Japanese market might see 40 percent of traffic from Asia-Pacific. A European fintech might see 70 percent from EU regions. Your capacity plan must reflect your traffic, not a generic global distribution.

Once you know the traffic distribution, map it to GPU requirements. If your model requires two H100 GPUs per instance and each instance handles 50 requests per second, a region handling 500 requests per second at peak needs twenty GPUs at peak. Add a 20 percent buffer for headroom and your target is twenty-four. But a region handling only 50 requests per second at peak needs two GPUs plus buffer — a fundamentally different capacity plan.

Time zone patterns matter as much as geographic distribution. A US-East deployment sees peak traffic between 9 AM and 9 PM Eastern Time. AP-Southeast peaks roughly twelve hours offset. If both regions share overflow routing, the time zone offset works in your favor: when US-East is peaking, AP-Southeast is in its low-traffic window and has spare capacity to absorb overflow, and vice versa. This natural load balancing across time zones can reduce total global GPU allocation by 10 to 15 percent compared to sizing each region for its peak independently.

## Overflow Routing: When a Region Hits Capacity

No matter how carefully you plan, individual regions will occasionally hit their capacity ceiling. Hardware failures take GPUs offline. Unexpected traffic spikes exceed your headroom. A new feature launch drives request volume that your forecasts missed. When a region cannot serve its load, you need a clear overflow strategy.

**Nearest-region overflow** is the simplest pattern. When AP-Southeast reaches capacity, route excess traffic to the next nearest region — typically AP-Northeast or US-West. The user experiences higher latency on the overflow requests, but the requests succeed rather than failing. Implement this with health-aware global load balancing that monitors GPU utilization per region and triggers overflow when utilization exceeds a threshold — typically 80 to 85 percent.

**Tiered overflow** is more sophisticated. Instead of routing all overflow to a single backup region, define a priority list. AP-Southeast overflows first to AP-Northeast (lower latency penalty), then to US-West (moderate penalty), then to US-East (highest penalty). Each tier triggers only when the previous tier is also at capacity. This approach requires more configuration but keeps latency penalties as small as possible for the majority of overflow requests.

**Degraded serving** is the escape valve when all regions are constrained. Instead of routing to a distant region, serve a smaller or faster model locally. If your primary model is a 70-billion-parameter model and your region is out of H100 capacity, fall back to a quantized version that runs on available hardware, or route to a smaller model that provides lower quality but faster responses. The user gets a slightly degraded experience rather than an error page or a five-second response from a region on the other side of the planet. This requires having fallback models pre-deployed in each region, which adds operational overhead but provides a critical safety net.

## Capacity Buffers and Headroom Targets

The buffer you maintain above your projected peak determines how gracefully your system handles surprises. Too little headroom and every traffic spike becomes an incident. Too much and you are paying for GPUs that sit idle.

Industry experience consistently shows that 15 to 25 percent headroom above projected peak is the right range for most AI inference deployments. Below 15 percent, you have no room for hardware failures. A single GPU node going offline in a region with 10 percent headroom can push utilization past 100 percent and trigger cascading overflow. Above 25 percent, the idle cost becomes significant — GPU instances are expensive enough that 30 percent idle capacity represents real money, especially across multiple regions.

The headroom target should also account for failover absorption. If your architecture requires any single region to absorb traffic from a failed peer, that region needs enough headroom to handle both its own traffic and the overflow. In a two-region active-active deployment where each region normally handles 50 percent of traffic, each region needs enough capacity for 100 percent — effectively doubling your GPU footprint for resilience. In a three-region deployment, each region needs capacity for approximately 50 percent above its normal load to absorb a third of a failed peer's traffic, assuming the other surviving region absorbs the other third.

These calculations compound. Buffer for traffic spikes plus buffer for hardware failures plus buffer for failover absorption can push your total provisioned capacity to 1.5 to 2 times your average utilization. This is the real cost of multi-region resilience, and it is the cost that most capacity plans underestimate because each buffer factor is analyzed independently rather than multiplicatively.

## Planning on Quarterly Cycles

GPU capacity planning is not a one-time exercise. Availability shifts, pricing changes, new GPU generations launch, and your own traffic patterns evolve. The teams that maintain reliable multi-region deployments review and adjust their capacity plans on a quarterly cadence.

Each quarterly review should cover four dimensions. First, actual versus projected utilization by region — did any region consistently exceed 80 percent, signaling the need for more capacity, or sit below 40 percent, signaling an opportunity to reduce spend? Second, quota health — do your approved quotas in each region still have headroom for growth, or do you need to request increases before the next quarter? Third, hardware generation strategy — has a newer, more cost-effective GPU type become available in your target regions, and should you begin migrating? Fourth, traffic trend analysis — are any regions growing faster than expected, requiring an acceleration of capacity expansion plans?

This quarterly rhythm aligns with the timescale of GPU supply changes. New GPU deployments roll out to cloud regions on roughly quarterly cadences. Reserved capacity pricing updates are typically announced quarterly. And traffic patterns shift seasonally in ways that annual planning cycles miss.

The companies that treat GPU capacity like traditional compute — provision it when you need it, assume it will be there, and deal with problems reactively — are the ones caught off guard when a region runs dry. The companies that treat it like supply chain management — forecast demand, secure capacity in advance, maintain buffers, and review quarterly — are the ones whose multi-region deployments survive the GPU scarcity that defined 2024 through 2026 and shows no signs of easing.

---

GPU availability defines the outer boundary of what your multi-region architecture can deliver, but the budget defines the inner boundary. The next subchapter examines the cost tradeoffs that determine how much of that global footprint you can actually afford — and where smart asymmetry can save you from the most expensive mistake in multi-region design.
