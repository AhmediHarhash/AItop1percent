# 27.50 — Edge Serving Frameworks: OpenVINO, LiteRT, and On-Device Runtimes

In early 2025, a robotics company tried to deploy a PyTorch transformer model directly onto their warehouse inspection devices — ARM-based industrial tablets running a stripped-down Linux distribution. The model worked perfectly on their development machines. On the target hardware, PyTorch's runtime consumed 1.8 gigabytes of memory before loading a single weight. Startup took 40 seconds. Inference for a single image classification took 3 seconds. The devices had 4 gigabytes of total memory and needed to respond in under 500 milliseconds. The team spent six weeks trying to optimize the PyTorch runtime — reducing dependencies, stripping unused operators, compiling custom builds. They got startup down to 12 seconds and inference to 1.4 seconds. Still unusable. They switched to ONNX Runtime with the ARM execution provider, and inference dropped to 180 milliseconds with a runtime footprint under 200 megabytes. The model was identical. The runtime made the difference.

Training frameworks and inference runtimes serve fundamentally different purposes. PyTorch and TensorFlow are designed for flexibility during development — dynamic computation graphs, automatic differentiation, eager execution, rich debugging tools. These features add overhead that is invisible on a workstation with 64 gigabytes of RAM and an NVIDIA A100 but crippling on a device with limited memory and no CUDA support. Edge serving frameworks strip away everything that is not needed at inference time. They optimize the computation graph for the target hardware, fuse operations, eliminate redundant memory allocations, and leverage hardware-specific accelerators that general-purpose frameworks cannot access. Choosing the right runtime for your target hardware is not an optimization step. It is a prerequisite for making edge inference work at all.

## ONNX Runtime: The Cross-Platform Workhorse

**ONNX Runtime** is the closest thing to a universal edge inference engine that exists in 2026. Developed by Microsoft and widely adopted across the industry, it runs ONNX-format models on virtually any hardware through a pluggable system of execution providers. The same ONNX model can run on an Intel CPU using the default provider, on an NVIDIA GPU using the CUDA or TensorRT provider, on a Qualcomm NPU using the QNN provider, on an Apple device using the Core ML provider, on an Android device using the NNAPI provider, and on a Windows machine using DirectML for GPU acceleration.

This universality comes with a tradeoff. ONNX Runtime will run your model everywhere, but it will not run it optimally everywhere. Hardware-specific runtimes that are built for a single target — Core ML for Apple, OpenVINO for Intel — typically extract 10 to 30 percent more performance from their target hardware because they can exploit architectural details that a cross-platform runtime abstracts away. ONNX Runtime's strength is breadth. When your deployment spans Android phones, Windows laptops, Linux edge servers, and embedded devices, maintaining a single ONNX model with multiple execution providers is dramatically simpler than maintaining four separate model formats with four separate optimization pipelines.

The ONNX export pipeline is where most teams hit their first friction. Converting a PyTorch model to ONNX format requires tracing the computation graph, which means every dynamic control flow path — if statements that depend on input shape, loops with variable iteration counts — must be either eliminated or converted to static operations that ONNX can represent. Models that use dynamic shapes extensively may require modifications before they export cleanly. The export step is not a one-click operation. Budget time for it, test the exported model against the original on your evaluation suite, and expect to iterate.

## OpenVINO: Deep Optimization for Intel Hardware

**OpenVINO**, Intel's open-source inference toolkit, is the right choice when your edge deployment targets Intel hardware — Core and Xeon CPUs, Arc GPUs, and Movidius VPUs. OpenVINO takes models from any training framework (PyTorch, TensorFlow, ONNX) and optimizes them through a pipeline of graph transformations, layer fusions, and precision conversions tailored to Intel's instruction sets.

What makes OpenVINO valuable beyond a generic runtime is its depth of hardware-specific optimization. On Intel CPUs, it leverages AVX-512 and AMX instructions for matrix operations that general-purpose runtimes do not exploit. On Intel's discrete GPUs, it compiles operations into hardware-native kernels. On the Movidius VPU — a low-power vision processing unit found in industrial cameras and drones — it runs computer vision models at a fraction of the power consumption of CPU inference.

Recent OpenVINO releases have added a GGUF reader that enables seamless loading of quantized language models originally packaged for llama.cpp. This bridges two ecosystems: teams that quantize models using llama.cpp's well-tested quantization tools can now run those same models through OpenVINO's optimized Intel backend. For organizations deploying AI on Intel-based edge servers — a common configuration in industrial and retail environments — this combination delivers the best of both worlds: community-driven quantization quality with hardware-specific runtime optimization.

The limitation is obvious: OpenVINO provides little benefit on non-Intel hardware. If your edge fleet includes ARM-based devices, Apple hardware, or Qualcomm-powered phones, OpenVINO is not the right primary runtime. It is a specialist, not a generalist.

## LiteRT: Google's Edge Runtime for Android and Embedded

**LiteRT**, the evolution of what was previously known as TensorFlow Lite, is Google's runtime for machine learning inference on mobile devices, embedded systems, and microcontrollers. It has the deepest integration with the Android ecosystem and the broadest support for low-end hardware targets.

LiteRT's strength is its reach at the bottom of the hardware spectrum. While most runtimes target devices with gigabytes of memory and multi-core processors, LiteRT runs on microcontrollers with kilobytes of RAM. The LiteRT Micro variant enables models to run on Cortex-M class processors — the kind found in smart thermostats, wearable health monitors, and industrial sensors. No other mainstream runtime covers this tier.

On Android, LiteRT integrates with the NNAPI hardware abstraction layer, which routes computation to whatever accelerator is available — Qualcomm's Hexagon NPU, MediaTek's APU, Samsung's NPU, or the device GPU. This means a single LiteRT model binary can leverage hardware acceleration across the fragmented Android device landscape without requiring per-vendor optimization. Google has also integrated LiteRT with the Qualcomm Hexagon NPU directly, enabling direct delegation that bypasses NNAPI for even lower latency on Snapdragon devices.

The tradeoff is that LiteRT's model format is separate from the formats used by other runtimes. Converting a model to LiteRT format requires its own export pipeline, its own quantization tools, and its own validation process. Teams deploying on both Android via LiteRT and other platforms via ONNX Runtime maintain two parallel model pipelines. This duplication is one of the most common sources of edge deployment complexity.

## Core ML: The Apple Ecosystem Runtime

**Core ML** is not optional for Apple deployment. It is the only path to the Neural Engine, the dedicated AI accelerator in every iPhone, iPad, and Mac. If your edge deployment includes Apple devices — and given Apple's market share in premium smartphones and laptops, it almost certainly does — Core ML is a required part of your runtime stack.

Core ML's advantage is vertical integration. Apple controls the hardware, the operating system, the runtime, and the developer tools. This end-to-end control enables optimizations that cross-platform runtimes cannot replicate. Core ML automatically partitions a model across CPU, GPU, and Neural Engine, assigning each operation to the processor that executes it fastest. It manages memory sharing between processors to avoid redundant copies. It schedules inference to balance performance with power consumption, extending battery life during sustained workloads.

For language model inference specifically, Core ML supports low-bit quantization methods — four-bit block-wise linear quantization and channel group-wise palettization — that reduce memory footprint while preserving quality. On iPhone 15 Pro, Apple demonstrated Llama-class 8B models running at 30 tokens per second with time-to-first-token latency of 0.6 milliseconds per prompt token. On M-series Macs, the performance scales further, and the latest M5 chip's neural accelerators, accessible through the MLX framework, push throughput into ranges that enable real-time document processing and extended conversation.

The limitation is ecosystem lock-in. A Core ML model runs only on Apple hardware. Your model pipeline must produce Core ML packages alongside whatever other formats your non-Apple devices require. The optimization work you invest in Core ML does not transfer to other platforms.

## llama.cpp and GGUF: The Community-Driven Runtime

**llama.cpp** started as a single developer's project to run Meta's LLaMA model on a MacBook without a GPU. It has become one of the most widely used inference runtimes for running large language models on consumer and edge hardware. Written in pure C and C++ with no external dependencies, it runs on virtually any platform — Windows, macOS, Linux, Android, iOS — and on virtually any processor — x86, ARM, Apple Silicon, NVIDIA GPUs via CUDA, AMD GPUs via ROCm.

The **GGUF format** that llama.cpp uses has become the de facto standard for distributing quantized language models. GGUF files are self-contained — they include the model weights, the tokenizer, and metadata describing the architecture — and they load directly into llama.cpp with no conversion step. The quantization ecosystem around GGUF is extensive, offering precision levels from 8-bit down to 1.5-bit, with the K-quant family (Q4_K_M, Q5_K_S, Q6_K, and others) providing fine-grained control over the size-quality tradeoff.

llama.cpp's performance is surprisingly competitive. On Apple Silicon, it matches or approaches Core ML speeds for text generation. On CPUs without dedicated AI accelerators, it is often the fastest option available because its pure C++ implementation avoids the overhead of framework abstractions. For teams that need to run language models on heterogeneous hardware without maintaining multiple format-specific pipelines, llama.cpp plus GGUF is the path of least resistance.

The tradeoff is maturity. llama.cpp is a community project with rapid iteration but less stability guarantee than corporate-backed runtimes. Breaking changes happen. Documentation lags behind development. Production deployments require more testing discipline because the runtime evolves weekly.

## ExecuTorch: PyTorch-Native Edge Deployment

**ExecuTorch** is Meta's answer to the model export problem. If your models are built in PyTorch — and in 2026, most are — ExecuTorch lets you deploy them to edge devices without converting to a different format. It uses ahead-of-time compilation to export a PyTorch model into an optimized binary that runs on a lightweight C++ runtime with a 50-kilobyte base footprint.

ExecuTorch supports over twelve hardware backends, including Apple Neural Engine, Qualcomm Hexagon, ARM CPU and GPU, MediaTek APU, and Vulkan for cross-platform GPU access. The compilation pipeline handles quantization, graph optimization, and hardware-specific partitioning as part of the export process. The promise is that you write your model in PyTorch, export it through ExecuTorch, and get an optimized binary for your target device without learning a new model format or a new optimization toolkit.

The framework powers Meta's own on-device AI across Instagram, WhatsApp, Ray-Ban Meta Smart Glasses, and Quest headsets, which provides confidence that it works at production scale. Version 1.1, released in early 2026, expanded support for large language models and vision-language models on mobile devices.

## The Model Export Pipeline: Where Things Break

Regardless of which runtime you choose, the path from a trained model to a running edge inference engine passes through a conversion step. This step — exporting from your training framework to your target runtime format — is where the majority of edge deployment failures occur.

The typical pipeline looks like this: train in PyTorch, export to ONNX (or directly to your target format), optimize the graph for the target hardware (layer fusion, constant folding, precision conversion), quantize to the target bit width, validate the output against the original model on your evaluation suite, and package for distribution. Each step can introduce subtle errors. An ONNX export might handle a custom attention mechanism differently than PyTorch. A graph optimization might fuse operations in a way that changes numerical behavior. A quantization step might degrade quality on inputs that your evaluation suite does not cover.

The mitigation is rigorous end-to-end validation. Run your full evaluation suite against the final, quantized, runtime-specific model binary on the actual target hardware — not on a simulator, not on a different device class, not on the unquantized version. The number of teams that validate their model on a development machine and then discover quality regressions on the target device is alarmingly high. Validate on the target. Always.

## Choosing the Right Runtime

Runtime selection is not a taste preference. It is determined by three factors: your target hardware, your model type, and your team's existing expertise.

If you deploy exclusively on Apple devices, use Core ML. The performance advantage from vertical integration is too large to leave on the table. If you deploy on Intel-based edge servers, use OpenVINO. If you deploy on Android and need to cover the widest range of devices including low-end hardware, use LiteRT. If you need to run language models on heterogeneous consumer hardware with minimal per-platform engineering, use llama.cpp with GGUF. If your team is PyTorch-native and you want to minimize format conversion, evaluate ExecuTorch. If you need a single runtime that works acceptably across the widest range of hardware, use ONNX Runtime with appropriate execution providers.

Most real-world edge deployments use more than one runtime. An application that serves Apple, Android, and industrial Linux devices will likely use Core ML for iOS, LiteRT or llama.cpp for Android, and ONNX Runtime or OpenVINO for Linux edge servers. The model is one logical artifact. The deployment produces multiple physical artifacts, each optimized for its target.

---

Choosing the right runtime gets your model running on the device. But many requests need more than what the device can provide. The next subchapter examines hybrid cloud-edge routing — the architecture that decides, in real time, whether a request runs on-device or escalates to the cloud, and how to make that decision fast enough that the user never notices it happening.
