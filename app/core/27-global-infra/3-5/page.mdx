# 27.21 — Cluster Federation and Multi-Cluster Management

At some point, one cluster is not enough. It does not matter how well you partition namespaces, how carefully you tune resource quotas, or how sophisticated your scheduling policies become. Organizational growth, regulatory boundaries, hardware diversity, and blast radius management will push you past the single-cluster ceiling. The question is not whether you will run multiple clusters. The question is whether you will manage them deliberately or stumble into a sprawl of disconnected environments that no one fully understands.

The transition from one cluster to many is the highest-leverage inflection point in platform maturity. Done well, it multiplies your capacity, reduces your risk surface, and lets teams operate independently without stepping on each other. Done poorly, it multiplies your operational burden, fragments your visibility, and turns every deployment into a multi-cluster coordination problem that nobody signed up for.

## When a Single Cluster Breaks Down

The signals that force multi-cluster architecture fall into four categories, and each one is a sufficient reason on its own.

**Regulatory boundaries** are the clearest trigger. When one team handles patient records under HIPAA and another processes payment data under PCI-DSS, keeping both workloads on the same cluster means the entire cluster must satisfy both compliance regimes simultaneously. That means HIPAA-grade audit logging, PCI-grade network segmentation, and a control plane that satisfies auditors for both. This is possible in theory and miserable in practice. By the time you layer enough compliance controls onto a single cluster to satisfy both auditors, you have built something more complex and more expensive than two purpose-built clusters would have been. The EU AI Act, now in enforcement with its August 2026 compliance window for systemic risk AI, adds another layer — high-risk AI systems require documentation and governance structures that are much easier to isolate per-cluster than to weave into a shared environment.

**Blast radius** is the second trigger. A single cluster has a single control plane. An etcd failure, a misconfigured admission webhook, or a botched cluster upgrade takes down every workload simultaneously. For web services, this is a recovery-time problem — pods restart, requests are retried. For AI workloads, this is a data-loss problem. A multi-day training run interrupted by a control plane outage loses hours of progress if checkpointing is infrequent, and the recovery time includes not just cluster restoration but model reload, data pipeline restart, and checkpoint validation. Splitting workloads across clusters means an incident in one cluster affects only the workloads in that cluster. Your production inference stays online while your training cluster recovers.

**Hardware specialization** forces splits as GPU fleets become heterogeneous. A cluster that manages A100 nodes, H100 nodes, and B200 nodes simultaneously must maintain three driver branches, three CUDA compatibility tiers, and scheduling rules that prevent workloads from landing on incompatible hardware. At smaller scales, node labels and taints handle this. At larger scales, the combinatorial complexity of driver versions, CUDA toolkits, and framework compatibility across three GPU generations makes a single cluster's CUDA matrix unmanageable. Dedicated clusters per GPU generation — or at minimum per driver branch — simplify compatibility at the cost of cross-cluster coordination.

**Trust boundaries** are the fourth trigger. When external customers, partner organizations, or teams with fundamentally different security postures share infrastructure, namespace isolation is insufficient. A vulnerability in one team's custom operator that escalates to node-level access compromises every workload on that node, regardless of namespace boundaries. Hard multi-tenancy through separate clusters provides the isolation guarantees that soft multi-tenancy cannot.

## Multi-Cluster Architecture Patterns

Three patterns dominate multi-cluster architectures in 2026, and each reflects a different set of priorities.

The first pattern is **independent clusters with shared tooling**. Each cluster is fully autonomous — its own control plane, its own node pools, its own scheduling policies. What they share is the management plane: a common GitOps repository, shared monitoring infrastructure, unified cost reporting, and standardized cluster templates. Teams deploy to their assigned cluster without awareness of other clusters. The platform team manages each cluster independently but uses identical automation for provisioning, upgrading, and monitoring. This pattern prioritizes isolation and simplicity. Each cluster is easy to reason about. The cost is that resource sharing across clusters requires manual intervention — idle GPUs in the training cluster cannot be borrowed by the inference cluster without explicit cross-cluster orchestration.

The second pattern is **hub-spoke**, where a central management cluster — the hub — controls the lifecycle and configuration of worker clusters — the spokes. The hub runs the GitOps controllers, the policy engines, and the fleet management tooling. The spokes run workloads. This is the model that Red Hat Advanced Cluster Management, Rancher Fleet, and Google Anthos implement. The hub provides centralized visibility and policy enforcement. The spokes provide isolation and locality. When you deploy a new application, you target it at the hub, which distributes it to the appropriate spoke clusters based on placement rules. When you update a policy, you update it once on the hub, and it propagates to all spokes. The risk with hub-spoke is that the hub becomes a single point of failure for management operations. If the hub goes down, you cannot deploy new workloads or update policies on any spoke, even though running workloads continue to function.

The third pattern is **true federation**, where multiple clusters are loosely coupled through a federation layer that enables cross-cluster resource discovery, scheduling, and service routing. Projects like Liqo take this approach — connecting clusters at the networking level so that a pod scheduled in cluster A can communicate with a service in cluster B as if they were local. Admiralty extends this to scheduling, allowing a workload submitted to one cluster to be transparently placed on another cluster that has available capacity. True federation promises the best of both worlds: independent clusters that behave as a single resource pool when needed. The reality is that federation adds latency to scheduling decisions, complicates debugging because workloads may run in unexpected clusters, and creates distributed-system failure modes that are harder to diagnose than single-cluster failures.

## Fleet Management Tooling

Managing a fleet of clusters manually is an exercise in configuration drift. By the third month, clusters that were provisioned identically have diverged in ways that nobody documented. One cluster has a newer version of the ingress controller because a team needed a specific feature. Another has a custom admission webhook that was added during an incident and never removed. A third has stale RBAC policies because the team that created them moved to a different project.

**Cluster API** addresses the provisioning and lifecycle layer. It treats cluster creation as a declarative Kubernetes resource — you define the desired state of a cluster (machine count, instance type, Kubernetes version, network configuration), and Cluster API controllers reconcile the actual infrastructure to match. Adding nodes, upgrading Kubernetes versions, and scaling clusters happen through the same kubectl-based workflow that you use for workloads. Cluster API providers exist for AWS, Azure, GCP, VMware, and bare metal, meaning the same cluster specification can target different infrastructure backends.

For configuration management across the fleet, **Argo CD** and **Flux** are the two GitOps controllers that dominate in 2026. Both watch a Git repository for desired state and reconcile clusters to match. The architectural difference matters for multi-cluster management. Argo CD runs as a centralized instance that manages multiple clusters from a single control point, providing a dashboard that shows the sync state of every application across every cluster. Flux runs as a per-cluster instance, with each cluster independently pulling its configuration from Git. Argo CD gives you a single pane of glass. Flux gives you decentralized resilience — if one cluster's Flux instance fails, other clusters are unaffected. The Argo CD agent model, which reached preview in late 2025, combines both approaches: a central dashboard backed by lightweight agents on each cluster that handle reconciliation locally.

For cross-cluster scheduling, **Kueue multi-cluster** — branded as MultiKueue — allows a workload submitted to one cluster to be dispatched to whichever cluster in the fleet has available capacity. This is particularly valuable for training workloads where the submitting user does not care which cluster runs the job, only that it starts as quickly as possible. MultiKueue maintains a global view of queue capacity across clusters and routes jobs to minimize wait time.

## The Single-Pane-of-Glass Illusion

Every multi-cluster platform promises a single pane of glass — one dashboard, one API, one place to see everything. This promise is seductive and partially true. You can build a dashboard that shows every cluster's health, every workload's status, every queue's depth. The illusion breaks when you try to act through that pane of glass.

The problem is that actions have cluster-local context that a centralized view cannot fully represent. Scaling a deployment on cluster A requires understanding cluster A's resource constraints, priority policies, and current scheduling queue. Debugging a failed pod on cluster B requires access to cluster B's logs, events, and node conditions. Modifying a resource quota requires understanding the governance agreements specific to that cluster's tenants. A centralized dashboard can display information aggregated from all clusters, but the moment you need to investigate or act, you need the cluster-local context that the aggregated view hides.

The mature approach is to accept that centralized visibility and distributed operations are separate concerns. Build the centralized view for monitoring, alerting, and capacity planning. Keep the operational tools — deployment, debugging, scaling — cluster-local, accessed through a consistent interface but aware of each cluster's unique configuration. This is less elegant than a true single pane of glass but far more reliable in practice.

## The Cost of Multi-Cluster

Multi-cluster architecture has a real and ongoing cost that you must budget for honestly.

Each cluster requires its own control plane — either managed (which costs $70 to $150 per month per cluster on major cloud providers) or self-hosted (which costs platform engineering time for API server, etcd, and scheduler management). Each cluster requires its own monitoring stack, or at minimum a monitoring agent that ships data to a centralized system. Each cluster requires its own certificate management, its own secret storage, and its own ingress configuration.

The operational cost compounds. Cluster upgrades happen per-cluster, meaning a fleet of ten clusters turns a single upgrade operation into ten sequential upgrade events, each requiring its own validation. Driver updates, GPU Operator upgrades, and security patches all multiply by the cluster count. A platform team of four people can comfortably manage three to five clusters with strong automation. Beyond five, you need dedicated fleet management tooling and headcount, or the operational burden consumes the team.

The resource fragmentation cost is the subtlest. Ten clusters with 32 GPUs each have the same total capacity as one cluster with 320 GPUs, but the scheduling flexibility is dramatically lower. A training job that needs 64 GPUs can run on the single large cluster but cannot run on any of the ten small clusters. Idle GPUs in one cluster cannot absorb overflow from another without explicit cross-cluster orchestration. The utilization ceiling in a multi-cluster architecture is always lower than in a single cluster of equivalent total capacity, unless you invest in federation tooling that enables cross-cluster scheduling.

The right decision is not "as few clusters as possible" or "one cluster per concern." It is the smallest number of clusters that satisfies your regulatory, blast-radius, and trust requirements — with the understanding that each additional cluster you add comes with operational overhead that grows linearly with cluster count and resource efficiency that decreases nonlinearly.

## Cluster Lifecycle as a Service

The organizations that manage multi-cluster well treat cluster creation and destruction as a self-service operation with guardrails. A team that needs a new cluster for a regulatory-isolated workload should be able to request one through a standard workflow — a pull request to the fleet repository, reviewed by the platform team, provisioned by Cluster API, configured by the GitOps pipeline, and available within hours rather than weeks.

This requires templates. The platform team maintains a library of cluster archetypes — a training cluster template with GPU node pools and Kueue queues, an inference cluster template with autoscaling and ingress, a development cluster template with relaxed quotas and preemptible scheduling. Teams select an archetype, customize a few parameters (region, size, team ownership), and the automation handles the rest. Without templates, every cluster becomes a bespoke snowflake. With templates, clusters are cattle — replaceable, reproducible, and managed through version-controlled definitions.

Cluster destruction is equally important. The cluster that was created six months ago for a proof of concept that never reached production is still running, still consuming control-plane resources, still requiring security patches. Mature fleet management includes lifecycle policies — clusters that have not run a workload in thirty days trigger a review, clusters idle for sixty days trigger an automatic decommissioning workflow. Without lifecycle policies, your fleet grows monotonically, and you pay for clusters that serve no one.

---

Splitting into multiple clusters solves governance and isolation problems but introduces a new one: the GPUs you own are now scattered across a fleet, and the capacity you see on a dashboard does not match the capacity you can actually schedule. The next subchapter examines capacity fragmentation — why available GPUs are not always usable GPUs, and what it takes to build a schedulable fleet from uneven supply.
