# 27.18 — Quota Enforcement as Governance: ResourceQuotas, PriorityClasses, and Budget Boundaries

Quotas are not resource management. Quotas are organizational governance expressed as infrastructure constraints. The moment you set a ResourceQuota on a namespace, you are making a political statement: this team gets this much capacity, no more, regardless of how loud they are or how important they believe their workload is. The moment you define a PriorityClass, you are encoding a business decision into the scheduler: production inference matters more than overnight experiments. The moment you configure a Kueue ClusterQueue, you are translating a budget line item into a GPU allocation that the cluster enforces automatically. Teams that treat quotas as a Kubernetes feature miss the point entirely. Quotas are the mechanism through which your organization's resource priorities become real, enforceable, and visible. Without them, GPU allocation devolves into whoever submits first, whoever asks loudest, or whoever knows enough kubectl to evict someone else's pods.

## ResourceQuotas Per Namespace

A **ResourceQuota** is a Kubernetes object that limits the total resources a namespace can consume. You set it once, and the API server rejects any pod creation that would push the namespace over its limit. For AI clusters, the critical resources to quota are GPU count, GPU memory, CPU, RAM, and persistent volume storage.

The quota for a team running production inference might be sixteen GPUs, 128 gigabytes of CPU memory, and 500 gigabytes of persistent storage. The quota for a research team experimenting with fine-tuning might be eight GPUs during business hours, expandable to thirty-two overnight through a secondary quota or a Kueue borrowing policy. The quota for a data pipeline team that does not use GPUs at all might be zero GPUs, 64 CPU cores, and two terabytes of storage. Each quota reflects the team's workload pattern, not just their headcount or organizational rank.

The enforcement is hard. If the research namespace has a quota of eight GPUs and a researcher submits a job requesting twelve, the API server rejects the submission immediately. There is no queue, no wait, no degradation. The pod simply is not created. This bluntness is a feature. It forces teams to right-size their requests before submission rather than hoarding resources they do not need. But it also means that poorly configured quotas create friction. If the research team genuinely needs twelve GPUs for a critical experiment, the platform team must adjust the quota — and that adjustment should go through a governance process, not a Slack message.

## LimitRanges as Safety Rails

ResourceQuotas cap the total. **LimitRanges** cap the individual. A LimitRange is a namespace-level policy that constrains the resources any single pod or container can request. Without LimitRanges, a team with a quota of sixteen GPUs can submit a single pod requesting all sixteen — blocking every other job in their namespace until that one pod finishes.

For AI clusters, the LimitRange should set maximum resource requests per pod, minimum resource requests to prevent trivially small pods from wasting scheduling overhead, and default values for pods that do not specify their own requests. A typical configuration sets the maximum per-pod GPU request at four or eight, depending on the largest single-node training job the team is expected to run. This forces teams to design distributed training across multiple pods rather than requesting one massive pod that monopolizes a node.

The deeper value of LimitRanges is enforcement of the resource request and limit contract. Kubernetes schedules pods based on their resource requests and throttles or evicts them based on their limits. A pod with no requests gets treated as best-effort: scheduled wherever there is room, evicted first when resources are scarce. A pod with requests but no limits can consume unbounded resources on its node. LimitRanges eliminate both failure modes by ensuring every pod in the namespace has explicit requests and limits, even if the developer forgot to set them. For GPU workloads, where an unbounded pod can crash a shared node, this is not bureaucratic overhead. It is crash prevention.

## PriorityClasses as Organizational Hierarchy

**PriorityClasses** define the scheduling and preemption hierarchy for your entire cluster. Every pod runs at a priority level, and when the cluster is full, the scheduler can evict lower-priority pods to make room for higher-priority ones. This is where organizational politics meets infrastructure.

A well-designed priority hierarchy for an AI cluster has at minimum four tiers. The highest priority, typically a value of one million or above, is reserved for system-critical workloads — the GPU device plugin, the NVIDIA DCGM exporter, the Kueue controller, monitoring agents. These pods must never be evicted because their absence breaks the entire GPU scheduling pipeline. The second tier, around 100,000, is production inference — the serving endpoints that external users depend on. These pods have strict latency requirements and cannot tolerate restart delays. The third tier, around 10,000, is production training — scheduled jobs with checkpointing that can tolerate brief interruptions but should not be preempted for trivial reasons. The fourth tier, at 1,000 or below, is development and experimentation — interactive notebooks, ad-hoc fine-tuning runs, hyperparameter sweeps — workloads that the scheduler can preempt when higher-priority work needs capacity.

The mistake teams make is creating too many priority levels. Twelve tiers of priority with fine-grained numeric differences between teams create a system that nobody can reason about. When team A's training job at priority 10,500 preempts team B's training job at priority 10,000, the resulting argument consumes more engineering time than the preemption saved. Keep the tiers few, make them meaningful, and tie each tier to a clear business justification that engineering leadership has approved.

## Kueue ClusterQueues as Budget Boundaries

Kueue transforms Kubernetes from a scheduler that admits pods on a first-come basis into a queuing system that admits workloads based on quota availability, priority, and organizational policy. The core abstraction is the **ClusterQueue**, which defines a pool of resources — GPU flavors, CPU, memory — that one or more teams can draw from. LocalQueues within namespaces point to ClusterQueues, and when a workload is submitted, Kueue checks whether the ClusterQueue has sufficient unused quota before admitting the workload to the cluster.

The power of Kueue is that it separates admission from scheduling. Without Kueue, you submit a pod and the scheduler immediately tries to place it. If resources are not available, the pod sits in a Pending state, consuming no resources but providing no feedback about when it will run. With Kueue, the workload sits in the queue with a clear position, an estimated wait time based on current utilization, and visibility into what is ahead of it. This turns the invisible "why is my pod Pending" frustration into a transparent queue that teams can reason about.

For budget enforcement, the mapping is direct. If your organization allocates $80,000 per month to the machine learning team for GPU compute, you calculate the equivalent GPU-hours at your provider's rate, convert that to a ClusterQueue quota, and Kueue enforces the boundary. The ML team can submit as many jobs as they want, but Kueue will only admit workloads up to the quota limit. Everything else queues. When finance reviews the monthly cloud bill, the GPU spend for each team maps cleanly to their ClusterQueue utilization. No surprises. No overruns that require awkward conversations.

## The Borrowing Model

Static quotas waste resources. If the research team's quota is thirty-two GPUs and they are only using eight on a Thursday afternoon, twenty-four GPUs sit idle while the production training team has jobs queued. Kueue solves this through **cohort-based borrowing**. ClusterQueues that belong to the same cohort can borrow unused capacity from each other.

The borrowing model works on a guarantee-and-reclaim basis. Each ClusterQueue has a nominal quota — its guaranteed allocation. When a queue has workloads that exceed its nominal quota, it can borrow from other queues in the same cohort, but only if those queues have idle capacity. The moment the lending queue needs its capacity back — because its own team submitted new work — Kueue preempts the borrowing workloads and returns the resources to their owner. The original owner's guarantee is never violated. The borrower gets access to idle GPUs they otherwise would not have. Utilization across the cluster improves dramatically. Without borrowing, clusters with bursty AI workloads commonly see thirty to forty percent of GPUs idle at any given time. With borrowing enabled, that idle rate drops to single digits.

The configuration requires careful thought about preemption thresholds. You do not want a borrowed training job that has been running for six hours to get preempted fifteen minutes before completion because the lending team submitted a low-priority notebook experiment. Kueue's preemption policies let you set rules: only preempt borrowed workloads if the reclaiming workload has higher priority than the borrowing workload. Only preempt workloads that have been running for less than a configurable duration. Only preempt workloads that have not reached a checkpoint within the last N minutes. These policies turn the blunt instrument of preemption into a nuanced governance tool that respects both the owner's guarantee and the borrower's investment.

## The Over-Commitment Trap

Over-commitment means promising more total quota across all teams than the cluster physically has. If you have sixty-four GPUs and you allocate thirty-two to the training team, twenty-four to the inference team, and sixteen to the research team, you have committed seventy-two GPUs against sixty-four available. The math only works if all teams never simultaneously use their full allocation.

Some platform teams over-commit intentionally, betting on staggered usage patterns the way airlines overbook flights. This works until it does not. The moment all three teams hit peak utilization simultaneously, you have eight GPUs of demand that cannot be satisfied. Pods queue, SLAs break, and the platform team spends a day mediating priority disputes. The airline analogy breaks down because an overbooked passenger gets rerouted with a voucher. An overbooked training job gets terminated, and eight hours of GPU time is wasted.

The safe approach is to over-commit only at the borrowing layer, not at the quota layer. Set each team's nominal quota to fit within the physical capacity of the cluster, with a small buffer for system overhead. Then enable borrowing through Kueue cohorts so that idle capacity is automatically shared. The total nominal quota never exceeds physical capacity, so there is always enough room for every team's guaranteed allocation. The borrowing layer provides the elasticity that over-commitment tries to achieve, but without the risk of simultaneous peak collisions.

## Setting Quotas From Data

The worst way to set quotas is to ask each team how many GPUs they want. Every team inflates their request. The second-worst way is to divide equally. Equal allocation ignores that some teams run continuous inference while others run periodic training.

The right approach starts with historical utilization data. Pull ninety days of resource usage per namespace. Calculate the P50, P90, and P99 utilization for GPU, CPU, and memory. The P90 becomes the nominal quota — this covers normal operations with some headroom. The gap between P90 and P99 is handled by borrowing from idle capacity in the cohort. The gap between P99 and the team's theoretical maximum is handled by a request to the platform team for a temporary quota increase, approved through the governance process.

Add a growth buffer of ten to twenty percent above the P90 to account for new projects and seasonal patterns. Review the buffer quarterly against actual growth. If a team consistently uses less than seventy percent of their quota, reduce it and reallocate. If a team consistently borrows from the cohort, increase their nominal quota to match real demand. Quota setting is not a one-time calculation. It is a feedback loop that requires monthly attention.

## The Monthly Quota Review

Quotas degrade without maintenance. Teams change. Projects launch and wind down. New models require different hardware profiles. A quota that was right in January is wrong by June.

The monthly quota review is a thirty-minute meeting where the platform team presents utilization data for every ClusterQueue: average utilization, peak utilization, borrowing frequency, queue wait times, and preemption counts. Teams whose quotas are underutilized get asked whether they can release capacity. Teams whose workloads are consistently queuing get evaluated for a quota increase. Teams whose preemption rates are high get investigated for workload design issues — submitting large jobs that could be broken into smaller ones, or running at the wrong priority tier.

This meeting also surfaces cost data. When each ClusterQueue maps to a budget allocation, the monthly review shows each team whether they are tracking to their annual GPU budget. A team that has burned sixty percent of their annual allocation by April needs either a budget increase or a workload optimization plan. The quota review turns infrastructure utilization into a financial conversation that engineering leadership and finance can both participate in, eliminating the "surprise cloud bill" pattern that plagues organizations without disciplined GPU governance.

---

Quotas tell the scheduler what to protect. But the scheduler itself, the API server, and the etcd store that backs it all have their own limits — and AI workloads push those limits faster than any web workload ever did. The next subchapter examines what happens to the Kubernetes control plane when you operate at scale, and how to prevent etcd exhaustion, API server saturation, and scheduler overload from taking down your entire AI platform.
