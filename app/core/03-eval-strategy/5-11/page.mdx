# 5.11 â€” Dataset Evolution and Maintenance

In early 2025, a customer support platform built a comprehensive evaluation dataset for their AI agent. The dataset had 1500 cases, balanced across intents and difficulty levels. Coverage was excellent. Every major user journey was tested. The rubric was well-calibrated. The dataset took four months and 60000 dollars to build. They ran evaluation quarterly and tracked quality over time. For the first two quarters, the dataset performed well. It caught regressions. It validated improvements. Eval scores correlated with user satisfaction. Then, in Q3, something changed.

The product team shipped three new features: a returns portal, a subscription management interface, and proactive order tracking notifications. None of these features appeared in the eval dataset because the dataset was frozen after initial construction. The eval scores stayed high, 91 percent quality across the board. But user satisfaction dropped five points. Support ticket volume increased 20 percent. When they investigated, they discovered that 40 percent of incoming queries related to the new features. The model handled some of these queries well and others poorly. The eval set had no way to measure this because it contained zero cases for the new features. The dataset was six months old and already obsolete.

The root cause was treating dataset construction as a one-time project rather than an ongoing practice. Eval datasets are not static artifacts. They are living things that must evolve as products, users, models, and standards change. If you build once and freeze, your dataset decays. Within two quarters, it is measuring yesterday's product against yesterday's problems. This subchapter covers how to maintain and evolve your eval datasets so they remain current, relevant, and predictive of production quality.

## Why Datasets Decay

Datasets decay through five distinct mechanisms. Each happens independently, and all happen simultaneously. If you do not actively maintain your dataset, all five forces are working against you at all times. First is product drift. Your product ships new features, retires old features, changes workflows, updates policies. Each change creates gaps in your eval coverage. New features have no eval cases. Retired features have eval cases that no longer matter. Changed workflows have eval cases that test the old workflow. Your eval set reflects the product as it was when you built the dataset, not the product as it is today. The coverage map has blind spots that grow with every product release.

Second is user drift. Your user base changes over time. New markets, new personas, new use patterns. When you launch in a new geography, users from that region have different language patterns and cultural expectations. When you add a new product tier, users at that tier have different needs and behaviors. When your product matures, power users develop workflows casual users do not use. The queries in your eval set reflect who your users were six months ago, not who they are today. If your user base shifted but your eval set did not, you are testing against the wrong distribution.

Third is model drift. You upgrade models every few months. GPT-4 to GPT-5 to GPT-5.1. Claude Opus 4.5 to Claude Opus 4.5. Gemini 1.5 to Gemini 2. Each upgrade shifts capabilities. What was hard six months ago becomes easy. What was impossible becomes feasible. Your difficulty distribution skews. Cases labeled hard are now normal. Cases labeled normal are now easy. Your adversarial cases no longer challenge the model because model defenses have improved. Your eval scores go up, but you are not learning anything new because the model has outgrown your dataset. You are measuring against a difficulty calibration from six months ago.

Fourth is rubric drift. Quality standards evolve. "Good enough" last quarter is not good enough now. User expectations increase. Regulatory requirements change. Competitive pressure raises the bar. Your rubric has not been updated, so you are scoring against outdated expectations. A response that scored four out of five six months ago might score three out of five today because standards have risen. If you do not update your rubric, your scores are measuring against historical standards that no longer reflect what quality means.

Fifth is world drift. Facts change. Regulations update. Products and pricing change. Company policies change. Any eval case testing factual knowledge becomes stale as the underlying reality shifts. A case asking about product features is wrong if the features changed. A case asking about pricing is wrong if prices changed. A case asking about policy is wrong if policy changed. A case referencing current events is wrong as those events recede into the past. World drift happens continuously. Every day, some subset of your eval cases becomes less accurate.

If you are not actively maintaining your dataset, all five of these forces are working against you simultaneously. Product drift opens coverage gaps. User drift skews your distribution. Model drift miscalibrates difficulty. Rubric drift obsoletes your standards. World drift stales your facts. Within six months, your dataset is partially obsolete. Within a year, it is mostly obsolete. Within two years, it is worse than useless because it gives false confidence while measuring the wrong things.

## The Maintenance Cadence

Maintenance happens at four timescales: weekly production signal integration, monthly coverage and difficulty audits, quarterly deep refreshes, and annual rebuild assessments. Each timescale addresses different decay mechanisms. Together, they keep your dataset current.

Weekly is for catching emerging issues fast. Review production logs for new failure patterns. When you see a new type of failure, add it to your dataset backlog. Do not wait for quarterly reviews to catch problems that are happening now. Track new query types that do not fit existing taxonomy categories. If your classification model labels queries as other more than ten percent of the time, your taxonomy has gaps. Real users are sending queries your eval set does not cover. Log these as dataset backlog items. This review takes 30 minutes per week. One person scanning recent failures and taxonomy mismatches. The output is a prioritized list of dataset additions needed. Most items stay in the backlog. Some items are urgent and get added immediately if they represent high-severity gaps.

Monthly is for systematic coverage and difficulty audits. Run your coverage map from Chapter 3.2 against current product features. Flag any feature or intent that has fewer than five eval cases. Check difficulty distribution per slice. If any difficulty bucket has shifted more than 15 percent from target, rebalance. A slice that was 50 percent normal, 30 percent hard, 20 percent easy six months ago might now be 40 percent normal, 40 percent hard, 20 percent easy because cases got easier. Add new hard cases to restore balance. Review production metrics against eval predictions. If eval says quality is 90 percent but production satisfaction is 70 percent, your dataset is not testing what matters. Investigate the gap. Find cases where eval scores are high but production quality is low. Understand what your eval is missing. Add cases that would catch these issues.

This monthly audit takes half a day. Run the coverage analysis scripts. Run the difficulty distribution analysis. Pull production metrics and compare to eval scores. Identify gaps and misalignments. Prioritize which gaps to fill first. The output is a ranked list of dataset maintenance tasks for the coming quarter. Some tasks are adding cases for new features. Some tasks are rebalancing difficulty. Some tasks are updating rubrics to better align with production quality.

Quarterly is for deep refreshes that address accumulated decay. Every quarter, you retire stale cases, recalibrate difficulty, rotate holdouts, refresh adversarial cases, and add new coverage. Retire stale cases that test deprecated features, outdated facts, or removed workflows. Do not delete them. Archive with a retired tag and a reason. You might need them if you re-introduce a feature or want to understand historical quality. A retired case is not gone, just inactive. Track why cases were retired. If you are retiring 30 percent of cases every quarter, your product is changing too fast for annual eval maintenance. You need more frequent updates.

Recalibrate difficulty by re-running difficulty heuristics from Chapter 5.3 on the full dataset. Cases the model now handles easily get downgraded from hard to normal or from normal to easy. New hard cases get added to replace them. This keeps your difficulty distribution stable even as model capabilities improve. Track difficulty recalibration trends. If 40 percent of hard cases get downgraded every quarter, your model is improving rapidly. You need aggressive hard case generation to keep up.

Rotate holdouts by swapping which cases are in the hidden holdout set versus the visible development set. This prevents eval overfitting from Chapter 5.6. If the same cases stay in the development set forever, teams will unconsciously optimize for those specific cases. Rotating cases keeps the development set fresh. Aim to rotate 30 to 50 percent of holdout cases every quarter. The cases leaving the holdout go into the development set. The cases entering the holdout come from the development set. This maintains holdout size while changing composition.

Refresh adversarial cases because adversarial techniques evolve. Prompt injection patterns that worked last quarter might not be relevant anymore. New ones have emerged. Update your adversarial suite based on current red team findings, security research, and production abuse patterns. Do not just add new adversarial cases. Replace old ones that no longer challenge the model. If an adversarial case has 95 percent pass rate, it is not adversarial anymore. Retire it and add a harder one.

Add new coverage to fill gaps identified in monthly audits. Cases for new features, new user personas, new languages, new edge cases discovered in production. This is proactive dataset growth based on product evolution. Target 10 to 20 percent dataset growth per quarter. If your dataset is not growing, your product is not evolving, or you are not keeping up. If your dataset is growing more than 30 percent per quarter, you are probably adding volume without discipline. Focus on gaps, not growth.

The quarterly deep refresh takes two to three days of focused effort. Review and retire stale cases. Run difficulty recalibration. Plan and execute holdout rotation. Review and refresh adversarial suite. Plan and add new coverage. Version the dataset with a changelog. This is real work that cannot be delegated to interns or automated away. It requires judgment about what matters and what no longer matters.

Annually is for rebuild assessment. Ask the hard question: is this dataset still fundamentally sound, or does it need a ground-up rebuild? If more than 40 percent of cases have been retired or replaced in the past year, consider rebuilding. The dataset might have drifted so far from current reality that patching is less effective than starting fresh with current production data as the foundation. Rebuilds are expensive, months of work and tens of thousands of dollars. But continuing to maintain a fundamentally outdated dataset is more expensive in the long run because it provides false confidence while missing real issues.

## Who Owns Maintenance

Dataset maintenance fails when nobody owns it. It is not glamorous work. It does not ship features. It does not close deals. It does not generate revenue. It exists in the background, invisible until something breaks. Without clear ownership and accountability, it gets deprioritized. Other work always feels more urgent. Months pass. The dataset decays. By the time someone notices, the damage is done.

The eval team owns the maintenance process: setting cadence, running audits, enforcing quality standards. If you have a dedicated eval team, this is part of their core responsibility. They own the calendar. They schedule monthly audits and quarterly refreshes. They run the coverage analysis and difficulty recalibration. They version the dataset and write changelogs. They communicate changes to stakeholders. They track maintenance metrics and escalate when health degrades. If you do not have a dedicated eval team, someone must own it anyway. Assign a dataset steward who runs the monthly audit and coordinates the quarterly refresh. Make it a real responsibility with time allocated, not a side task squeezed between other work. Put it in their job description. Include it in their performance goals. Make it count.

The dataset steward role is 20 to 30 percent of a full-time role for a moderately complex product with a 2000-case dataset. That is roughly ten hours per week. Two hours for weekly production signal review. Four hours for monthly audit. Twelve hours for quarterly refresh amortized across the quarter. Ad hoc time for responding to urgent coverage gaps. This is substantial time. It will not happen if you assign it to someone who is already fully loaded with other work. You need dedicated capacity. Either dedicate someone's time or hire for it.

Product teams own coverage requests. They know what features are shipping and what needs testing. They understand product roadmaps and user needs. They are responsible for flagging when new features launch that need eval coverage. This cannot be the eval team's responsibility because the eval team does not know what is shipping until it ships. By then, the gap already exists. Product teams should be submitting dataset requests as part of feature development, not waiting for the eval team to discover gaps in production. Establish a process where product launches include eval coverage as a required artifact. Before a feature ships, the product team must ensure eval cases exist for that feature or explicitly accept the risk of shipping without eval coverage.

Make this concrete with a template. When a product team proposes a new feature, they fill out an eval coverage request: feature name, expected launch date, key user journeys, edge cases to test, expected output behaviors. They estimate how many eval cases are needed, typically five to ten for a small feature, 20 to 30 for a major feature. They submit this to the eval team at least one sprint before launch. The eval team reviews, asks clarifying questions, and either writes the cases or delegates to domain experts. If timing does not allow for case creation before launch, the product team explicitly accepts the risk. This acceptance is documented. It is not a failure to ship without eval coverage if the risk is understood and accepted. It is a failure to ship without eval coverage without knowing you are doing it.

Product teams also own deprecation notifications. When a feature is deprecated, they notify the eval team. The eval team retires relevant cases. This prevents the dataset from accumulating dead weight. A feature deprecated six months ago should not still have 50 active eval cases. Those cases waste eval budget on tests that no longer matter.

Domain experts own content accuracy. They catch factual staleness and rubric misalignment. They are the people who understand what correct looks like in your domain. For a healthcare product, domain experts are clinicians. For a legal product, domain experts are lawyers. For a financial product, domain experts are financial professionals. They review cases quarterly to identify facts that have changed, policies that have updated, or standards that have evolved. They validate that expected outputs are still correct. A case written six months ago might have a correct expected output then that is wrong now because policy changed. Domain experts catch this. They propose rubric updates when quality expectations shift. What counted as good quality last year might not count this year because user expectations or regulatory standards increased. Domain experts detect this drift.

Domain expert involvement is expensive. Their time costs more than engineer time or annotator time. But it is necessary. Without domain expert involvement, your dataset becomes technically maintained but factually wrong. You run eval, get scores, trust them, and ship. Then users find factual errors that your eval did not catch because your expected outputs were wrong. This destroys trust in evaluation. Budget domain expert time as part of maintenance cost. For a healthcare dataset, budget ten to 20 hours of clinical expert time per quarter. For a financial dataset, budget similar time from financial compliance experts. This is not optional overhead. It is quality assurance.

This three-way ownership model distributes maintenance responsibility across the people who have the relevant knowledge. The eval team has process knowledge: how to run coverage audits, how to version datasets, how to track metrics. Product teams have feature knowledge: what is shipping, what is deprecated, what users need. Domain experts have content knowledge: what facts are correct, what quality means, what standards apply. All three are necessary. If any one is missing, maintenance suffers. If the eval team is missing, maintenance does not happen. If product teams are not engaged, coverage gaps open. If domain experts are not involved, factual errors accumulate. The three roles are complementary, not redundant.

## Version Management for Evolving Datasets

Every change to your dataset creates a new version. This is not optional bureaucracy. It is how you trace which dataset produced which eval result. Without version management, you cannot compare results across time, cannot roll back bad changes, cannot understand whether score changes reflect model improvements or dataset changes. Imagine running eval in January and scoring 85 percent. You run again in April and score 90 percent. Is the model better, or did the dataset get easier? Without version management, you cannot answer this. You do not know what changed between January and April. You might celebrate a five-point improvement that is actually an artifact of dataset drift.

Use semantic versioning for datasets, adapted from software versioning. The format is major.minor.patch, like 1.3.2. Major version changes, like v1 to v2, represent schema changes, rubric overhauls, or full rebuilds. The structure or measurement criteria changed fundamentally. Scores from v1 and v2 are not directly comparable. You are measuring different things or measuring the same things differently. Document why the major version changed and what changed. Examples: migrated from three-point rubric to five-point rubric, rebuilt dataset from scratch with new production data, changed schema to include new metadata fields. When you bump major version, scores before and after are not comparable. You reset the baseline.

Minor version changes, like v1.2 to v1.3, represent new cases added, coverage expanded, or difficulty rebalanced. The measurement did not change fundamentally, but the case distribution changed. Scores are comparable with caveats. You are measuring the same capabilities, but you are testing a broader or different distribution. Document what was added and why. Examples: added 50 cases for new billing feature, rebalanced difficulty distribution, expanded language coverage to include Spanish. When you bump minor version, scores are approximately comparable but may shift due to coverage changes. If you added hard cases, scores may drop. If you added easy cases, scores may rise. Neither reflects model performance change. Both reflect dataset distribution change.

Patch version changes, like v1.3.1 to v1.3.2, represent bug fixes: wrong labels corrected, broken cases removed, factual errors fixed. The measurement and distribution did not change. You fixed errors in implementation. Scores should be directly comparable. The measurement did not change materially, just became more accurate. Examples: corrected ten mislabeled expected outputs, removed five duplicate cases, fixed three cases with outdated facts. When you bump patch version, scores should not shift significantly. If they do, investigate. The patch fixed bigger problems than expected.

Always log what changed and why in a changelog. A changelog entry like "Added 50 cases for new billing feature, retired 20 cases for removed plan types, recalibrated 30 difficulty labels" tells future-you exactly what happened. Without this, you look at version 1.8 six months later and have no idea how it differs from version 1.6. You know eight minor versions passed, but you do not know what changed. The changelog is not documentation overhead. It is operational necessity. It is how you interpret score changes over time.

Structure changelogs with sections: added, removed, changed. Under added, list new cases with counts and descriptions. Under removed, list retired cases with counts and reasons. Under changed, list recalibrated difficulty, updated rubrics, corrected errors. Keep entries concise but informative. "Added 20 cases" is too vague. "Added 20 hard-difficulty adversarial cases for prompt injection testing" is clear. Future readers can understand what changed without reading the full dataset diff.

Never modify a released version in place. If you find errors in v1.3, fix them in v1.3.1. Do not silently patch v1.3. Someone might be comparing results against v1.3. If you change it without versioning, their comparisons are invalid. They think they are comparing against v1.3, but they are actually comparing against a modified version that is not v1.3 anymore. This breaks reproducibility. Eval results become un-reproducible if datasets change silently. Every change must be a new version, even small changes. Found one wrong expected output? That is a patch version bump. Fixed a typo that does not affect scoring? That is still a patch version bump because the dataset content changed. The cost of versioning is negligible. The cost of silent changes is catastrophic loss of reproducibility.

Version management overhead is minimal with proper tooling. Tag your dataset in version control with semantic version numbers. Store the dataset as files in a git repository or equivalent. Each version is a tagged commit. Store the changelog in the repository as a markdown file or within dataset metadata. Automate version bumping: when you merge changes, the system prompts for version type and changelog entry. This takes 30 seconds per change and saves hours of confusion later. Tools like DVC for data version control integrate with git and handle large binary dataset files. Use them. Do not reinvent version control for datasets. Adapt existing tools to dataset use cases.

Track which model evaluations used which dataset versions. When you run eval, log the dataset version alongside the eval results. Store this in your eval results database or spreadsheet. When comparing results across time, check dataset versions. If dataset versions differ, note that in the comparison. Do not directly compare results from different dataset versions without acknowledging the version difference. This discipline prevents mistaking dataset changes for model changes.

## Practical Maintenance Workflows

The weekly production signal review is 30 minutes every Monday morning. Pull last week's production failure logs. Group failures by type. Identify patterns that repeat more than three times. Ask: do we have eval cases for these patterns? If not, add to backlog. Prioritize by user impact and frequency. If a failure affects 100 users per day, that is higher priority than a failure affecting five users per day. If a failure causes escalations or refunds, that is higher priority than a failure that annoys but does not block. Create backlog items with priority, description, and target quarter for addition. Most items stay in backlog for quarterly addition. Critical items get added immediately.

The monthly coverage audit is half a day on the first Friday of each month. Run automated coverage analysis against current product features. Compare to last month. Flag new features that launched since last audit. Check whether eval cases exist. Calculate cases per feature, per intent, per difficulty level. Generate a coverage heatmap. Red cells have zero to two cases. Yellow cells have three to five cases. Green cells have six or more cases. Target is all cells green or yellow, no red cells. For each red cell, decide whether to add cases or accept the gap. Not every red cell needs immediate filling. Low-use features can stay yellow or even red if risk is low. High-use features must be green.

Run difficulty distribution analysis per slice. Compare current distribution to target. If normal cases dropped from 50 percent to 40 percent because they got easier, flag for rebalancing. Check model performance per difficulty level. If hard cases have 85 percent pass rate, they are not hard anymore. Downgrade or retire. Pull production quality metrics: user satisfaction scores, escalation rates, task completion rates. Compare to eval scores per slice. Calculate correlation. If eval predicts production quality with correlation above 0.7, your eval is working. If correlation is below 0.5, your eval is measuring something other than what users care about. Investigate why. Find cases where eval passes but production fails. These are gaps. Add similar cases.

The quarterly deep refresh is two to three days of focused work. Block the calendar. This is not something you squeeze between meetings. Start by reviewing all cases marked for retirement. Common retirement reasons: feature deprecated, fact outdated, workflow changed, policy updated. For each retirement candidate, verify the reason is still valid. Retire confirmed cases. Move them to an archive table or tag them as retired with reason and date. Do not delete. You may need them for historical analysis or if the feature returns.

Run difficulty recalibration on active cases. Use your difficulty heuristics from Chapter 5.3 plus empirical model performance. Flag cases where labeled difficulty and measured difficulty diverge. A case labeled hard with 90 percent pass rate is mislabeled. Downgrade to normal. A case labeled normal with 60 percent pass rate might be mislabeled. Upgrade to hard or investigate whether the model regressed. Review flagged cases with domain experts. Adjust labels based on their judgment plus performance data. Rebalance difficulty distribution after recalibration. If you downgraded 50 hard cases, add 50 new hard cases to maintain distribution.

Plan holdout rotation. Review which cases are currently in holdout. Select 30 to 50 percent to rotate out. Prioritize rotating cases that have been in holdout longest. Move them to development set. Select replacement cases from development set. Prioritize cases that have never been in holdout or have not been in holdout for at least a year. Move them into holdout. Validate that holdout set maintains coverage and difficulty distribution. A holdout set with only easy cases or only one intent is not useful. The holdout must be representative of the full distribution.

Review adversarial suite with red team or security team. What new attack patterns emerged this quarter? What patterns did we defend against successfully? Retire adversarial cases with pass rates above 90 percent. They are no longer challenging. Add new adversarial cases based on current attack patterns. Validate new adversarial cases: do they actually work? Can they fool the current model? An adversarial case that never succeeds is too hard. An adversarial case that always succeeds is too easy. Target 40 to 60 percent pass rate for adversarial cases. They should be hard but not impossible.

Add new coverage from monthly backlog. Prioritize by user impact and frequency. Write or generate new cases. Validate all new cases: correct expected outputs, appropriate difficulty labels, proper metadata. Add to active dataset. Version the dataset. Write changelog. Tag in version control. Announce the new version to stakeholders. Provide summary of changes: cases added, cases retired, difficulty recalibrated, adversarial refreshed. Include guidance on score comparability: are this version's scores directly comparable to last version, or did coverage changes affect score distribution?

## Failure Modes and Fixes

Nobody maintains the dataset, and it becomes stale. This is the most common failure mode. No ownership, no accountability, no time allocation. Six months pass. The dataset has not been touched. Product has shipped five features. None are in the eval set. Fix by assigning a dataset steward with explicit time allocation. Make monthly audit and quarterly refresh part of someone's job description. Track completion. If audits are skipped, escalate. Treat dataset maintenance as infrastructure maintenance. You would not skip security patches or database backups. Do not skip dataset maintenance.

Eval scores keep going up, but users are not happier. Your dataset has not evolved with the product or users. You are testing old patterns that the model has already mastered. Cases that were hard are now easy. New challenges that users face are not in the eval set. Fix by running production-eval alignment check. Calculate correlation between eval scores and production satisfaction. If correlation is dropping, your eval is losing predictive power. Add cases from recent production failures. Refresh difficulty labels. Investigate score inflation: why are scores rising? Is the model actually better, or has the dataset gotten easier?

You retire too many cases and lose coverage. Aggressive cleanup without checking the coverage map. You remove 200 outdated cases and discover you accidentally removed all cases for a major intent. Fix by running coverage analysis before retirement, not after. For each case marked for retirement, check: what does this case cover? Are there other cases covering the same thing? If not, write replacement cases before retiring the original. Never retire cases that are the sole coverage for an important feature. Archive them as low-priority if they are outdated, but do not remove coverage entirely.

Adversarial suite has not changed in a year. Nobody refreshes adversarial cases because it is hard work that requires security expertise. The model has effectively memorized them or defenses have improved. Pass rates are 95 percent. The adversarial suite is not adversarial anymore. Fix by making quarterly adversarial refresh non-negotiable. Involve red team or security team. Pull from current prompt injection research, jailbreak attempts, and production abuse patterns. Replace at least 20 percent of adversarial cases every quarter. Track adversarial pass rates. If rates exceed 85 percent, cases are too easy. Add harder ones.

You add cases without discipline, and the dataset bloats. Growth for growth's sake. You went from 1000 cases to 5000 cases in a year. But only 2000 cases are actually useful. The rest are redundant or test low-value edge cases. Dataset bloat increases eval cost without improving signal. Fix by having clear criteria for case addition. Every new case must fill a coverage gap, add a new difficulty level, or test a new feature. If a proposed case is similar to ten existing cases, do not add it. Run deduplication from Chapter 5.6 on new cases before adding. Target 10 to 20 percent annual growth. If you are growing faster, audit whether the growth is adding signal or noise.

Difficulty recalibration is skipped, and your scores inflate. Cases stay labeled hard even though the model handles them easily now. Your hard cases have 90 percent pass rate. Your overall score is 88 percent and rising every quarter. But production quality is flat. The score increase is an artifact of dataset decay, not actual improvement. Fix by mandatory quarterly difficulty recalibration. Track pass rates per difficulty level. Hard cases should have 50 to 70 percent pass rate. Normal cases should have 75 to 85 percent pass rate. Easy cases should have 90 to 95 percent pass rate. If pass rates exceed these targets by more than ten points, downgrade difficulty labels. Add new hard cases to replace downgraded ones. Maintain difficulty distribution at target percentages.

Holdout sets are never rotated, and teams overfit. The same 200 cases have been in the development set for two years. Teams have looked at these cases hundreds of times. They unconsciously optimize for them. The model performs well on development set but poorly on holdout. This is eval overfitting. Fix by rotating 30 to 50 percent of holdout cases every quarter. The development set becomes less familiar. Teams cannot memorize it. Overfitting risk decreases. Track development set versus holdout set performance divergence. If development performance exceeds holdout performance by more than five points, you have overfitting. Rotate holdouts more aggressively.

## Automation and Tooling for Maintenance

Automate what can be automated. Manual maintenance is expensive and error-prone. Automation reduces cost and increases consistency. But some parts of maintenance cannot be automated. They require human judgment. The art is knowing which is which.

Automate staleness detection. Write scripts that scan eval cases for content that becomes stale: product names, pricing, dates, feature names, policy text. Flag cases that reference deprecated features based on your product changelog. Flag cases with dates older than six months. Flag cases with pricing that does not match current pricing. These scripts do not decide what to retire. They surface candidates for human review. A case with an outdated date might need its expected output updated, not retirement. A case with a deprecated feature name might still be valid if renamed. Human judgment determines action, but automation surfaces issues.

Automate coverage analysis. Scripts compare your eval dataset to your product feature list and taxonomy. Generate coverage heatmaps showing cases per feature, per intent, per difficulty level. Flag red cells with zero to two cases. This runs monthly without human intervention. The output is a report showing coverage gaps. Humans prioritize which gaps to fill, but automation identifies them.

Automate difficulty distribution tracking. Scripts calculate pass rates per difficulty level. Flag difficulty levels where pass rates exceed target by more than ten points. Hard cases with 85 percent pass rate need downgrading. Scripts do not change labels automatically. They flag cases for human review. A domain expert looks at flagged cases, considers context, and decides whether to downgrade. Automation detects drift, humans correct it.

Automate production-eval correlation tracking. Scripts pull production quality metrics: user satisfaction, task completion, escalation rate. Pull eval scores for the same time period and case types. Calculate correlation. Plot over time. Flag when correlation drops below 0.5. This signals that eval is losing predictive power. Automation measures correlation, humans investigate why and fix the root cause.

Do not automate case writing. You cannot automate the judgment of what to test. You can use synthetic generation from Chapter 5.9 to accelerate case creation, but human validation is mandatory. Do not automate rubric updates. Quality standards require human judgment. What counted as good quality six months ago might not count now, but this is a subtle judgment that automation cannot make reliably. Do not automate expected output updates. A case with outdated facts needs a human to determine the correct new expected output, not a script guessing based on current product state.

The balance is automation for detection and measurement, human judgment for decisions and content creation. Automation makes maintenance scalable. Without it, you cannot maintain a 2000-case dataset with reasonable effort. But automation without human oversight produces garbage. Scripts flag stale cases that are actually fine. Scripts miss coverage gaps that require domain knowledge to detect. Humans in the loop ensure automation serves maintenance rather than creating busywork.

Build maintenance dashboards that visualize health metrics. Dataset age, coverage completeness, difficulty distribution, production-eval correlation, retirement rate: all visible in one view. Update daily or weekly. Make the dashboard accessible to all stakeholders. Transparency drives accountability. If everyone can see that coverage completeness dropped from 85 percent to 70 percent, people ask why. If metrics are hidden, decay happens silently. Dashboards turn invisible decay into visible accountability.

## Maintenance Metrics and Governance

Track maintenance health with metrics. Dataset age: time since last update. Target is quarterly updates. If age exceeds four months, flag. Coverage completeness: percentage of features with at least five eval cases. Target is 100 percent for core features, 80 percent for all features. If below 70 percent, coverage is poor. Difficulty distribution drift: deviation from target difficulty percentages. Target is within five points. If deviation exceeds ten points, rebalancing is needed. Production-eval correlation: correlation between eval scores and production quality metrics. Target is above 0.7. If below 0.5, eval is not predictive. Retirement rate: percentage of cases retired per quarter. Normal is five to 15 percent. If above 30 percent, product is changing too fast for current maintenance cadence. If below three percent, you are not retiring aggressively enough.

Govern maintenance through quarterly reviews. Every quarter, present maintenance metrics to stakeholders. Show dataset age, coverage completeness, difficulty distribution, production-eval correlation, retirement rate. Highlight gaps and remediation plans. Get buy-in for quarterly refresh effort. Ensure time is allocated. If metrics show poor health, escalate. Treat it as technical debt that must be paid. Dataset decay is invisible until it causes production incidents. Metrics make decay visible before it causes harm. Include maintenance health in quarterly business reviews alongside product metrics. Eval dataset health is infrastructure health. It deserves the same visibility as system uptime or security posture.

Establish maintenance SLAs. Coverage completeness above 80 percent within one sprint of feature launch. Difficulty recalibration completed within two weeks of quarterly refresh. Production-eval correlation above 0.6 measured monthly. If SLAs are missed, escalate. Treat missed maintenance SLAs like missed product SLAs. They indicate systemic problems that need attention. SLAs create accountability. Without them, maintenance slides because it competes with more visible work.

## Enterprise Expectations

Organizations doing this well have a named owner, person or team, responsible for dataset maintenance. This is someone's job, not everyone's side project. They run monthly coverage audits against current product features. They flag gaps. They track coverage completeness. They recalibrate difficulty and rotate holdouts quarterly. This happens on schedule, not when convenient. They version every dataset change with semantic versioning and changelogs. They can trace any eval result back to the exact dataset version that produced it. They track production-eval alignment: if eval scores and production quality diverge, they investigate immediately. They refresh adversarial suites quarterly with current attack patterns from red team exercises and security research. They never silently modify a released dataset version. Every change is a new version with documentation.

They budget for maintenance. Dataset maintenance is not free. It requires time, tools, and expertise. They allocate 20 to 30 percent of eval team capacity to maintenance, not just new dataset construction. They treat maintenance as ongoing operational cost, like infrastructure monitoring or security patching. They establish clear handoffs between product teams and eval teams. When a feature launches, product team notifies eval team. Eval team adds coverage within one sprint. This prevents coverage gaps from accumulating. They use automated tooling to detect staleness. Scripts flag cases with outdated facts, deprecated features, or drifting difficulty. Human judgment determines what to retire, but automation surfaces candidates. They run retrospective analysis on retired cases: why were they retired? What patterns do retirements reveal? If many cases were retired due to fact staleness, improve fact update processes. If many cases were retired due to feature deprecation, improve coordination with product teams.

They maintain multiple dataset versions in parallel for comparison. When testing a major model upgrade, they run evaluation on both the current dataset and the previous dataset version. This separates model improvement from dataset change. If scores increase on both datasets, the model improved. If scores increase only on the current dataset, the dataset got easier. They document dataset limitations openly. No eval dataset is perfect. They know which areas have weak coverage, which difficulty levels are under-tested, which user personas are under-represented. They communicate these limitations to stakeholders. Evaluation provides signal, not certainty. Understanding the signal's limitations is part of using it well.

The overall discipline is treating evaluation datasets as production systems that require ongoing investment, not as one-time artifacts. They evolve. They decay. They need maintenance. Organizations that maintain datasets rigorously get reliable quality signal that guides product decisions. Organizations that neglect maintenance get false confidence that leads to preventable production failures.

