# 12.10 — Release Documentation & Quality Evidence

**Who approved this release, and what evidence did they have?**

A production outage hits at 2 AM. Your VP of Engineering is on a bridge call with customers. Quality has clearly degraded. Someone asks the question every engineering leader dreads: "What changed in the last deployment?"

You pull up your deployment logs. You see a commit hash. A timestamp. A username. Nothing else.

You have no record of what was tested. No record of what quality looked like before the release. No record of who reviewed the changes or what evidence they used to approve it. You have a deployment, but you have no **release documentation**.

In 2026, this is unacceptable. The EU AI Act requires documentation of material changes to AI systems. SOC 2 audits demand evidence of change management processes. Your insurance policy may require proof that you followed internal quality standards. And when something goes wrong, your executive team will ask: "How did this pass our quality gates?"

Release documentation is your answer. It's the artifact that captures **what changed**, **what was tested**, **what the results were**, and **who approved it**. It transforms a deployment from a black box event into an auditable, accountable decision backed by evidence.

This chapter covers how to build release documentation practices that create accountability, satisfy compliance requirements, and give you the evidence you need when quality incidents occur.

---

## The Reality of Undocumented Releases

Most teams deploy AI systems the same way they deploy traditional software: commit code, run CI/CD, push to production. The deployment succeeds, and everyone moves on.

But AI systems are different. Traditional software either works or crashes—you get stack traces, error logs, clear signals. AI systems **degrade gracefully**. Quality drops 5%. Then 10%. Users start complaining, but there's no smoking gun. You need to investigate.

Without release documentation, the investigation is painful:

- **What model version is running?** You check Git tags, deployment configs, multiple sources of truth that might not agree.
- **What prompt changes went out?** You diff the codebase, hope the prompts are in version control, hope they haven't been edited in a database.
- **What was the quality before this release?** You dig through old evaluation logs, try to reconstruct what "normal" looked like.
- **Who approved this?** You check Slack messages, JIRA tickets, email threads, trying to piece together the decision trail.

This takes hours. Sometimes days. And the whole time, customers are experiencing degraded quality.

**Release documentation prevents this.** Every deployment produces a **release artifact**—a structured document that answers all these questions immediately.

---

## The Release Artifact: Structure and Contents

A release artifact is a document (usually JSON, Markdown, or YAML) that captures the complete context of a deployment. It's generated automatically by your CI/CD pipeline and stored permanently in a searchable release history.

**Minimum contents of a release artifact:**

```yaml
release_id: rel_2026_01_29_001
release_date: 2026-01-29T14:23:00Z
environment: production
status: deployed

# What changed
changes:
  model_version: gpt-4.5-turbo-20260115
  previous_model_version: gpt-4.5-turbo-20251201
  prompt_version: v2.8.3
  previous_prompt_version: v2.8.2
  config_changes:
    - temperature: 0.7 → 0.6
    - max_tokens: 2000 → 2500
  data_version: dataset_20260125

# What was tested
tests_executed:
  - regression_suite_golden_500: passed
  - safety_checks_harmful_content: passed
  - performance_latency_p95: passed
  - cost_per_query_threshold: passed

# Quality evidence
quality_metrics:
  golden_set_accuracy: 94.2%
  previous_golden_set_accuracy: 93.8%
  regression_delta: +0.4%
  safety_violations: 0
  avg_latency_ms: 1250
  cost_per_1k_queries: $2.34

# Approval trail
approvals:
  - approver: sarah.chen@company.com
    role: ML Lead
    timestamp: 2026-01-29T14:15:00Z
    evidence_reviewed: regression_report_20260129.pdf
  - approver: james.park@company.com
    role: Engineering Manager
    timestamp: 2026-01-29T14:20:00Z

# Overrides and exceptions
overrides: []
risk_level: low
compliance_checks: passed
```

This artifact lives forever. When quality drops in production, you pull up the release history and immediately see what changed, what quality evidence supported the decision, and who approved it.

---

## Quality Evidence: Concrete Proof

The most important section of a release artifact is **quality evidence**—concrete, numerical proof that quality standards were maintained.

**Types of quality evidence:**

**1. Golden set results**
Your regression suite ran against the golden set. You got a score. That score is evidence:

```yaml
golden_set_evidence:
  test_suite: golden_500_customer_queries
  accuracy: 94.2%
  threshold: 92.0%
  status: passed
  failed_cases: 12
  failure_report: s3://releases/rel_001/golden_failures.json
```

If someone challenges the release, you point to this: "We tested against 500 golden queries. We hit 94.2% accuracy. That's above our 92% threshold."

**2. Regression comparison**
Before-and-after comparison showing the release improved quality or stayed neutral:

```yaml
regression_comparison:
  metric: accuracy
  before: 93.8%
  after: 94.2%
  delta: +0.4%
  interpretation: quality_improved
```

This is critical. You're not just showing absolute quality—you're showing **the release didn't break anything**.

**3. Safety check results**
Evidence that safety gates passed:

```yaml
safety_evidence:
  harmful_content_check: 0 violations in 1000 samples
  pii_leakage_check: 0 violations in 1000 samples
  bias_check: passed demographic parity threshold
  jailbreak_resistance: 98.5% successful blocks
```

If a safety incident occurs post-release, you can show: "We tested for this. The system passed safety checks."

**4. Performance metrics**
Latency, cost, throughput evidence:

```yaml
performance_evidence:
  latency_p50: 850ms
  latency_p95: 1250ms
  latency_p99: 2100ms
  cost_per_1k_queries: $2.34
  throughput_qps: 450
  all_thresholds: passed
```

These metrics prove the release won't cause performance or cost regressions.

**Why this matters:** When an executive asks "How do we know this release was safe?", you don't hand-wave. You show them a document with numbers, thresholds, pass/fail statuses. That's evidence.

---

## Change Documentation: Reproduce or Roll Back

Quality evidence tells you the release was good. **Change documentation** tells you **exactly what changed**, with enough detail to reproduce the release or roll it back precisely.

**Critical change details:**

**1. Model version**
Exact model identifier, including date suffix if available:

```yaml
model_version: gpt-4.5-turbo-20260115
provider: openai
model_family: gpt-4.5-turbo
context_window: 128k
training_cutoff: 2025-12-01
```

Not just "GPT-5.1 Turbo"—the **exact snapshot**. Model providers version their models by date. You need to track which snapshot you deployed.

**2. Prompt version**
Complete prompt text or a version-controlled reference:

```yaml
prompt_version: v2.8.3
prompt_repo: github.com/company/prompts
prompt_commit: a3f8d2c
prompt_checksum: sha256:8f3d2a...
```

If prompts are stored in a database, include a snapshot:

```yaml
prompt_snapshot: s3://releases/rel_001/prompt_snapshot.json
```

You need to be able to reproduce **exactly** what the model saw.

**3. Configuration changes**
Every config parameter that changed:

```yaml
config_changes:
  - parameter: temperature
    old_value: 0.7
    new_value: 0.6
    reason: reduce output variability per user feedback

  - parameter: max_tokens
    old_value: 2000
    new_value: 2500
    reason: support longer reasoning chains

  - parameter: retry_policy
    old_value: 3 attempts with exponential backoff
    new_value: 5 attempts with exponential backoff
    reason: reduce timeout failures in production
```

Don't just list the new values—show **old vs new** and **why it changed**. When you need to roll back, you know exactly what to revert.

**4. Data version**
If your system uses retrieval, fine-tuning, or embeddings, track the data version:

```yaml
data_version: embeddings_20260125
embedding_model: text-embedding-3-large
documents_indexed: 1.2M
index_checksum: sha256:9c4e1f...
```

Data changes affect behavior. You need to know which data version was live.

**Why this matters:** Six months from now, you discover a quality issue that started in January. You pull up the release from January 29. You see the exact model version, prompt version, config, and data version. You can reproduce the environment, investigate the issue, and roll back if needed.

---

## Approval Trail: Accountability

A release artifact must capture **who approved the release** and **what evidence they reviewed**. This creates accountability and satisfies audit requirements.

**Basic approval trail:**

```yaml
approvals:
  - approver: sarah.chen@company.com
    role: ML Lead
    timestamp: 2026-01-29T14:15:00Z
    evidence_reviewed:
      - regression_report_20260129.pdf
      - golden_set_results.json
    decision: approved
    notes: "Golden set accuracy improved, no safety violations"

  - approver: james.park@company.com
    role: Engineering Manager
    timestamp: 2026-01-29T14:20:00Z
    evidence_reviewed:
      - cost_analysis_20260129.pdf
    decision: approved
    notes: "Cost increase within budget, latency acceptable"
```

**This answers critical questions:**

- **Who made the decision?** Sarah Chen and James Park.
- **What did they look at?** Regression reports, golden set results, cost analysis.
- **When did they approve?** Timestamps for each approval.
- **What was their reasoning?** Notes capturing their judgment.

**Override tracking:**

Sometimes releases bypass quality gates. Maybe a critical production bug requires a hotfix. Maybe a business deadline overrides a minor quality drop. These **overrides must be documented**:

```yaml
overrides:
  - gate: golden_set_accuracy_threshold
    threshold: 92.0%
    actual: 91.5%
    status: overridden
    override_approver: vp.engineering@company.com
    override_reason: "Critical production fix for customer-facing bug, minor quality drop acceptable for hotfix"
    override_timestamp: 2026-01-29T16:45:00Z
    planned_remediation: "Follow-up release scheduled for Jan 31 to restore quality"
```

**Why this matters:** When auditors ask "Did you ever bypass your quality gates?", you say "Yes, here are the three times we did it in 2025, here's who approved each override, here's why, and here's what we did to remediate." That's accountability.

Without this documentation, an override looks like negligence. With documentation, it looks like a conscious decision made under pressure with clear accountability.

---

## Automated Release Notes: Don't Rely on Humans

The **worst approach** to release documentation is asking engineers to write release notes manually. They forget. They write incomplete notes. They write them after the fact, from memory. They focus on what they think is important, not what auditors or future investigators will need.

**Automated release notes solve this.** Your CI/CD pipeline generates the release artifact automatically by aggregating outputs from your testing and deployment workflow.

**How it works:**

1. **CI/CD runs regression tests** → test results are written to a structured file (JSON, YAML)
2. **CI/CD runs safety checks** → safety results are written to a structured file
3. **CI/CD collects config and version metadata** → from Git, deployment manifests, environment variables
4. **CI/CD calls your eval platform API** → pulls quality metrics from the latest eval run
5. **CI/CD aggregates all outputs** → combines them into a single release artifact
6. **CI/CD publishes the artifact** → uploads to S3, writes to a database, posts to a release history dashboard

**Example GitHub Actions workflow step:**

```yaml
- name: Generate release artifact
  run: |
    python scripts/generate_release_artifact.py \
      --regression-results tests/regression_results.json \
      --safety-results tests/safety_results.json \
      --model-version $MODEL_VERSION \
      --prompt-version $PROMPT_VERSION \
      --output release_artifacts/rel_$RELEASE_ID.yaml

- name: Publish release artifact
  run: |
    aws s3 cp release_artifacts/rel_$RELEASE_ID.yaml \
      s3://company-releases/$RELEASE_ID/release_artifact.yaml
    curl -X POST https://release-dashboard.company.com/api/releases \
      -d @release_artifacts/rel_$RELEASE_ID.yaml
```

The script pulls data from your test outputs, your Git metadata, your deployment config. It doesn't ask humans for input—it extracts everything automatically.

**Approval integration:**

For the approval trail, integrate with your deployment approval system (GitHub Environments, PagerDuty, custom approval tool). When someone approves a deployment, the approval system writes an approval record to the release artifact.

**Result:** Every deployment automatically produces a complete, accurate, auditable release document. No human effort required beyond the initial setup.

---

## Release History: Searchable Record

Release artifacts are useless if they're scattered across S3 buckets and Slack threads. You need a **centralized, searchable release history**.

**What a release history provides:**

**1. Chronological view**
See all releases in order, with quality metrics over time:

```
Release Timeline (Last 30 Days)

Jan 29: rel_001 | Golden Set: 94.2% | Latency: 1250ms | Approved
Jan 27: rel_002 | Golden Set: 93.8% | Latency: 1180ms | Approved
Jan 24: rel_003 | Golden Set: 92.5% | Latency: 1200ms | Approved (override: latency threshold)
Jan 22: rel_004 | Golden Set: 93.1% | Latency: 1150ms | Approved
```

You can immediately see trends. Quality climbing? Good. Quality dropping? Investigate.

**2. Search by change type**
Filter releases by what changed:

- "Show me all releases where the model version changed"
- "Show me all releases where prompts changed but the model stayed the same"
- "Show me all releases with config changes to temperature"

This helps you isolate what caused a quality shift.

**3. Search by approval or override**
- "Show me all releases approved by Sarah Chen"
- "Show me all releases where quality gates were overridden"
- "Show me all releases with safety violations detected"

Critical for audits and compliance reviews.

**4. Diff between releases**
Click two releases, see exactly what changed between them:

```
Diff: rel_003 vs rel_001

Model: gpt-4.5-turbo-20260115 (unchanged)
Prompt: v2.8.2 → v2.8.3 (changed)
Temperature: 0.7 → 0.6 (changed)
Golden set accuracy: 92.5% → 94.2% (+1.7%)
Latency p95: 1200ms → 1250ms (+50ms)
```

This is how you answer: "What changed between last week's release and today's release?"

**Implementation:**

Most teams build a simple web dashboard backed by a database (Postgres, DynamoDB) or search index (Elasticsearch). Each release artifact is stored as a record. The dashboard provides filtering, search, and diff views.

Alternatively, use an **AI audit platform** (see 2026 patterns below) that includes release history as a core feature.

---

## Compliance Requirements: EU AI Act, SOC 2, Insurance

In 2026, release documentation isn't just good practice—it's often **legally or contractually required**.

**EU AI Act (High-Risk AI Systems):**

Article 11 requires "technical documentation" for high-risk AI systems, including:

- "Description of the system and its intended purpose"
- "Design and development process, including the versions of the system"
- "Detailed description of the logic of the AI system"
- "Description of the data used for training, validation, and testing"

A release artifact satisfies most of these requirements for **each version of your system**. When you deploy a new model version or prompt version, you're creating a new version of the system. The release artifact is your evidence that you documented the change.

**SOC 2 (Change Management Controls):**

SOC 2 CC8.1 requires organizations to "manage changes to system components." Auditors will ask:

- "How do you track changes to your AI system?"
- "Who approves changes?"
- "What testing is performed before changes are deployed?"

Your release history is the evidence. You show auditors a dashboard with every release, quality evidence for each, and approval trails. That's a passing grade.

**Cyber Insurance Policies:**

Many cyber insurance policies now include AI-specific clauses requiring "evidence of AI system testing and quality assurance." If you experience an AI-related incident (hallucination leads to customer harm, bias incident, safety failure), your insurer may ask: "What testing did you do before this release?"

Your release artifact is your defense. You show them: "We tested against 500 golden queries, ran safety checks on 1000 samples, detected zero violations, and quality was within acceptable thresholds." If you don't have that documentation, the insurer may deny the claim.

**Why this matters:** Release documentation isn't just for internal accountability—it's for **external accountability** to regulators, auditors, and insurers.

---

## The Release Checklist: Practical Workflow

Before any production deployment, run through a **release checklist**. This ensures every release has the documentation and evidence required.

**Sample release checklist:**

```yaml
Pre-Deployment Checklist

[ ] Regression tests executed
    - Golden set: passed
    - Critical user flows: passed

[ ] Safety checks executed
    - Harmful content: 0 violations
    - PII leakage: 0 violations
    - Bias: passed demographic parity

[ ] Quality gates passed
    - Golden set accuracy above threshold
    - Latency within SLA
    - Cost within budget

[ ] Change documentation complete
    - Model version recorded
    - Prompt version recorded
    - Config changes documented
    - Data version recorded

[ ] Quality evidence generated
    - Before/after comparison complete
    - Regression delta calculated
    - Performance metrics collected

[ ] Approval obtained
    - ML Lead approval: yes
    - Engineering Manager approval: yes
    - (If override required) VP approval: yes

[ ] Release artifact generated
    - Artifact file created: rel_001.yaml
    - Artifact uploaded to S3: yes
    - Release history updated: yes

[ ] Deployment plan reviewed
    - Rollback plan documented: yes
    - Monitoring alerts configured: yes
    - On-call engineer assigned: yes
```

**This checklist is automated.** Your CI/CD pipeline enforces it. If any item is incomplete, the deployment is blocked. The checklist becomes part of the release artifact, so auditors can see that every required step was completed.

---

## 2026 Patterns: AI Audit Platforms and Model Cards

In 2026, several patterns have emerged to make release documentation easier and more comprehensive.

**1. AI Audit Platforms**

Dedicated platforms (e.g., Fiddler AI, Arthur, Robust Intelligence) now include **release management modules** that automatically generate, store, and search release artifacts.

You integrate your CI/CD pipeline with the platform. Every deployment triggers a release artifact creation. The platform:

- Ingests test results, quality metrics, model metadata
- Generates a structured release artifact
- Stores it in a searchable release history
- Provides compliance reports for SOC 2, EU AI Act, etc.

**Benefit:** You get release documentation, compliance evidence, and audit trails without building your own infrastructure.

**2. Model Cards for Every Deployment**

Model cards (pioneered by Google, now standardized by several frameworks) are structured documents describing an AI model's capabilities, limitations, and intended use. In 2026, many enterprises generate a **model card for every deployment**, not just for the base model.

A deployment-level model card includes:

- Model version and prompt version
- Training data version (if fine-tuned)
- Intended use case
- Known limitations
- Performance metrics (accuracy, latency, cost)
- Fairness metrics (bias checks, demographic parity)
- Safety metrics (harmful content, PII leakage)

This overlaps heavily with release artifacts. Some teams merge them—the release artifact **is** the model card for that deployment.

**3. Regulatory Compliance Automation**

Tools like OneTrust, DataGrail, and TrustArc now offer AI-specific compliance automation. You configure your release pipeline to send release artifacts to the compliance platform. The platform:

- Validates that required documentation is present
- Flags releases that don't meet regulatory standards
- Generates audit reports for regulators

**Example:** EU AI Act requires high-risk AI systems to maintain logs for at least 6 months. A compliance automation tool monitors your release history, alerts you if documentation is missing, and generates a compliance report showing "100% of releases in the last 6 months have complete documentation."

**4. Automated Release Documentation in CI/CD**

Nearly every major CI/CD platform (GitHub Actions, GitLab CI, CircleCI, Jenkins) now has plugins or templates for AI release documentation. You install a plugin, configure it with your eval platform API and test result paths, and it auto-generates release artifacts.

**GitHub Example:**

```yaml
- name: Generate AI Release Artifact
  uses: ai-release-docs/generate-artifact@v2
  with:
    eval_platform: braintrust
    api_key: the secrets BRAINTRUST_API_KEY variable
    test_results: tests/regression_results.json
    output: release_artifacts/rel_the-github-sha-variable.yaml
```

The plugin pulls quality metrics from Braintrust, ingests your test results, adds Git metadata, and outputs a complete release artifact.

---

## Failure Modes: When Release Documentation Fails

**Failure Mode 1: Documentation Without Evidence**

Team writes beautiful release notes but includes no actual quality metrics. Just "We tested thoroughly and everything looks good."

**Impact:** When quality drops, you can't compare current performance to the release baseline. The documentation is useless.

**Fix:** Automate evidence collection. Force the pipeline to include numerical metrics—accuracy, latency, cost—in every release artifact.

---

**Failure Mode 2: Manual Documentation Bottleneck**

Team requires engineers to manually fill out a release template before every deployment. Engineers forget, rush it, or copy-paste old docs.

**Impact:** 30% of releases have incomplete or inaccurate documentation. Auditors flag this. Compliance fails.

**Fix:** Automate documentation generation. Humans should only provide approval, not write docs.

---

**Failure Mode 3: Documentation Siloed from Deployment**

Team generates release artifacts but stores them separately from the actual deployment. The artifact says "model version X" but production is running "model version Y" because of a hotfix that bypassed the process.

**Impact:** Documentation and reality diverge. Artifact is untrustworthy.

**Fix:** Integrate documentation tightly with deployment. Deployment scripts should read the release artifact to know what to deploy. If the artifact doesn't exist, deployment fails.

---

**Failure Mode 4: No Search or History**

Team generates release artifacts but stores them in a Git repo or S3 bucket with no searchable index. Finding a specific release requires manually browsing files.

**Impact:** When quality drops, investigating "what changed" takes hours instead of seconds.

**Fix:** Build a release history dashboard or use an AI audit platform with search.

---

**Failure Mode 5: Approval Trail Faked or Missing**

Team documents approvals retroactively. Someone deploys, then asks for approval later. Or approvals are recorded verbally ("Sarah said it was fine") without a timestamp or evidence trail.

**Impact:** Auditors don't trust the approval trail. It looks like the team is bypassing process.

**Fix:** Enforce approval in the deployment pipeline. Use GitHub Environments, PagerDuty Approvals, or a custom approval tool that blocks deployment until approval is recorded with timestamp and approver identity.

---

## Enterprise Expectations: What Good Looks Like

At a mature enterprise, release documentation is automatic, comprehensive, and trusted.

**What good looks like:**

**1. Every deployment has an artifact**
Zero exceptions. Even hotfixes produce a release artifact (with an override flag if they bypassed normal testing).

**2. Artifacts are complete**
Every artifact includes: what changed, what was tested, quality metrics, approval trail, and override documentation.

**3. Artifacts are searchable**
Release history dashboard allows filtering by date, approver, change type, quality delta, override status. Any release can be found in under 30 seconds.

**4. Artifacts are automated**
CI/CD generates artifacts. Humans only provide approval and override justifications. No manual documentation writing.

**5. Artifacts satisfy compliance**
Legal and compliance teams have reviewed the artifact format and confirmed it meets SOC 2, EU AI Act, and insurance requirements.

**6. Artifacts are integrated with monitoring**
When production quality drops, monitoring alerts link directly to the most recent release artifact. On-call engineers can immediately see what changed.

**7. Artifacts are immutable**
Once generated, artifacts cannot be edited. This ensures they're a true historical record, not a revised narrative.

---

## Bridge to 12.11: Compliance and Audit Evidence

Release documentation creates the foundation for **compliance and audit evidence**. Every release artifact is a compliance record. When auditors ask "How do you manage AI system changes?", you show them your release history.

But compliance goes beyond releases. You also need evidence of **ongoing monitoring**, **incident response**, **bias testing**, and **data governance**. Chapter 12.11 covers how to build a comprehensive compliance evidence system that satisfies regulators, auditors, and insurance providers—using release documentation as one pillar of a broader compliance strategy.

---
