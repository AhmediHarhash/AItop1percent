# 5.8 â€” Versioning, Lineage, and Dataset Registry

In mid-2025, a VP of Product at a SaaS company asked a simple question during a quarterly business review: "Why did our Q2 customer support quality metrics drop 8% compared to Q1?" The AI quality team pulled up the eval results from both quarters. Q1 showed 89% accuracy on response quality. Q2 showed 81%. The model had not changed. The prompt had not changed. The production system configuration had not changed. But the metrics dropped significantly, and nobody could explain why.

After two days of investigation, they discovered the problem. The Q1 and Q2 evals had used different datasets. The Q1 dataset had 300 examples. The Q2 dataset had 500. The difficulty distribution shifted from 20% hard examples to 35% hard. The rubric changed from a three-point scale to a five-point scale. Someone had updated the test set to improve coverage, but nobody documented the change or tracked which dataset version produced which results. The 8% drop was not a regression in model quality. It was a change in measurement instrument. The team spent a week re-running evals with consistent datasets to establish what actually happened to quality, and the VP lost confidence in the metrics.

This is the problem versioning solves. One rule is non-negotiable: which dataset produced this result must always be answerable. If you cannot trace every eval run back to a specific, immutable dataset version, your metrics are not reproducible, your comparisons over time are invalid, and your quality claims cannot be defended.

## Why Versioning is Non-Negotiable

Most teams start with a folder of evaluation examples, often just a file in a repository. Over time, someone adds 20 new cases to improve coverage. Someone else fixes a mislabeled example. Another person updates the rubric from three-point to five-point scoring. A fourth person removes deprecated task types that no longer reflect production usage. Each of these changes is individually reasonable and improves dataset quality. The problem is not the changes themselves. The problem is treating the dataset as a mutable artifact rather than an immutable versioned artifact.

When software engineers modify code, they commit changes with messages, create pull requests with reviews, tag releases with version numbers, and deploy specific versions to production. Everyone understands that you cannot modify deployed code in place. You version it, test the new version, and deploy the new version. Datasets require the same discipline, but many teams fail to apply it. They treat datasets as living documents that can be edited at will rather than as artifacts that determine system behavior and must be versioned accordingly.

But if you do not version these changes, you create systematic problems that corrupt your entire eval practice. The first problem is false regressions, where scores drop because the dataset got harder, not because the model got worse. You see a 5% accuracy decline and spend days debugging the model, only to discover that someone added 50 adversarial examples to the test set. The second problem is false improvements, where scores rise because bad examples were removed or the dataset got easier, not because the model actually improved. You celebrate a 7% gain and ship the new model, only to find production quality unchanged or degraded. The third problem is unreproducible results, where you cannot replicate last month's eval run because the dataset silently changed. The fourth problem is broken comparisons, where Q1 versus Q2 metrics are meaningless because the datasets used different rubrics, different difficulty mixes, or different example counts.

Think of it like a bathroom scale that secretly recalibrates itself between weigh-ins. Your weight chart would be useless, because you could never tell whether changes reflected actual weight loss or scale drift. Evaluation datasets work the same way. If anything changes between runs, the comparison means nothing. You are measuring with an instrument that keeps changing.

Beyond these operational problems, 2026 brings compliance requirements that make versioning mandatory in regulated industries. Finance, healthcare, and government sectors now require provenance tracking for all data used in automated decision systems. The EU AI Act requires evaluation data to be auditable, which means you must be able to retrieve the exact dataset that produced any historical result and prove that it has not been modified since. Without versioning and immutability, you cannot pass these requirements. This is not a nice-to-have for better engineering practice. It is an operational and legal necessity in many industries.

Regulators ask specific questions that you cannot answer without versioning. Which dataset did you use to validate this model before deploying it to production? Can you prove that dataset has not been modified since the validation run? Can you trace that dataset back to its source and show what transformations were applied? Can you demonstrate that your quality claims are based on reproducible measurements? Without versioning, the answer to all of these questions is no, and that answer fails compliance requirements. Versioning transforms from optional best practice to mandatory operational infrastructure the moment your AI system falls under regulatory oversight.

## What Must Be Versioned

Versioning is not just locking the dataset file. You need to version every input that affects eval results, because changes to any of these inputs invalidate comparisons to historical runs.

The most obvious thing to version is dataset content: the examples themselves. This includes examples added, removed, or modified. Adding even a single example changes the dataset, and that change must be tracked. It includes changes to train-dev-test splits, where moving examples between splits can corrupt your understanding of generalization and make it look like your model improved when you actually just made the test set easier. It includes slice rebalancing, where shifting the difficulty mix or category distribution changes aggregate metrics even if every individual example remains unchanged. If you go from 20% hard examples to 35% hard, your accuracy will drop even if the model performs identically on each difficulty tier. That is a measurement change, not a quality change, but you cannot distinguish the two without versioning.

You must also version labels and ground truth. This includes reference answers corrected based on new information or better understanding. If you discover that 10 expected outputs were factually incorrect and you fix them, that creates a new dataset version. It includes rubric version changes, like moving from a three-point to a five-point scale or redefining what constitutes a correct answer. Rubric changes are particularly insidious because they retroactively redefine quality for all examples. What was acceptable under the old rubric may be unacceptable under the new one. It includes quality tier reassignments, where an example previously labeled as easy gets relabeled as hard after you realize it requires domain expertise. It includes annotator pool changes that affect label consistency, because different annotators may apply the same rubric differently even with training.

Label changes are often invisible to automated systems. The example text looks the same. The schema validates correctly. But the semantics changed, and that semantic change invalidates historical comparisons. Versioning makes these invisible changes visible and traceable.

Metadata schema is the third thing to version. This includes new fields added to examples, like adding a tenant_id field to support multi-tenant analysis or adding a source_type field to track where examples originated. It includes field definitions changed, where the meaning or allowed values for existing fields shift. If difficulty used to mean cognitive complexity but now means user-perceived difficulty, that semantic change breaks historical analysis. It includes enum values added or removed. If you add a new difficulty tier called expert or remove the deprecated tier called trivial, that changes how examples are classified. It includes changes to which fields are required versus optional. Making a previously optional field required can cause old examples to fail validation. Metadata changes can break downstream slicing and analysis logic even if the examples themselves are unchanged. Your slicing code may filter on a field that no longer exists or may assume enum values that are no longer valid.

The fourth category is associated configurations. Dataset validity depends on system context. A dataset designed for GPT-4 may not be appropriate for Claude or Llama because the models have different capabilities and failure modes. Examples that test reasoning chains may work well for models with strong chain-of-thought capabilities but fail for models that do not support that pattern. A RAG dataset validated against corpus version 1.2 may break when the corpus updates to version 2.0 because retrieved context changed. If the corpus added new documents or removed outdated ones, the retrieval results for the same query will change, and expected outputs based on old retrieval may no longer be correct. An agent dataset designed for one tool schema may become invalid when the schema changes. If a tool parameter was renamed or a new required parameter was added, examples that call that tool may need updated expected outputs.

Version the prompt template, model version, retriever version for RAG systems, corpus version, tool schemas for agents, and any other configuration that the dataset assumes or depends on. Store these as metadata alongside the dataset or in a linked configuration file. Without this, you cannot tell whether changes in eval results reflect dataset changes, model changes, configuration changes, or some combination of all three. Debugging becomes impossible because you have too many confounding variables.

If any of these dimensions change without being tracked, you lose the ability to compare results across time. You introduce confounds that make it impossible to attribute metric changes to their true causes.

## The Dataset Registry: Central Catalog for All Eval Datasets

The dataset registry is your central catalog, the single source of truth for what eval datasets exist, what they test, who owns them, and what state they are in. It answers four critical questions that arise constantly during development and debugging. What datasets exist that I can use for this evaluation? Which specific version should I use for this eval run? What is the lineage of this dataset, meaning where did it come from and how has it evolved? Who owns it, when was it last updated, and is it still actively maintained or deprecated?

Without a registry, this knowledge lives in people's heads, scattered documentation, and tribal knowledge that evaporates when people leave. Teams duplicate effort by creating overlapping datasets because they do not know what already exists. Someone creates a new RAG dataset for legal queries without realizing that the legal team already maintains one. People use deprecated datasets because they do not know a newer version exists. They find an old dataset file in a repository and use it, unaware that it was replaced six months ago with a version that has better coverage and fixed labels. Debugging requires asking around to find out who created a dataset or what version was used in a historical run. This informal knowledge transfer breaks down as teams scale. The registry solves all of these problems by making dataset metadata explicit, centralized, and queryable. Anyone can search the registry to find what datasets exist, read their descriptions to understand what they test, and see their status to know whether they are actively maintained or deprecated.

Every registry entry needs a minimum set of fields. Dataset ID is a unique identifier like rag_general_v2 or agent_code_v1. Version is the semantic version number like 2.3.1 or a date-based version like 2026-01-15. Purpose describes what the dataset tests, like general RAG quality for Tier 1 and Tier 2 queries or agent coding ability on Python tasks. Channel identifies what system component this evaluates, like RAG, agent, classifier, or chat. Split indicates train, dev, or test. Owner is the team or individual responsible for maintaining the dataset, typically an email or team alias. Created date records when this version was published. Status indicates whether the dataset is active, deprecated, or archived. Parent version links to the previous version to track evolution. Changelog URL points to documentation of what changed in this version. Artifact URL provides the storage location where the actual dataset file can be retrieved.

Recommended additional fields include size as example count, difficulty mix showing the distribution across difficulty tiers, slice coverage listing what categories or edge cases are represented, last validated date indicating when the dataset was last checked for quality and drift, deprecated reason explaining why a dataset was retired, and successor version pointing to what replaced a deprecated dataset. These fields support common queries like finding all datasets over 500 examples, identifying datasets that have not been validated in over six months, or discovering what replaced an old deprecated dataset.

The registry fields should be queryable, not just documentation. Implement your registry as a structured data store with search and filter capabilities. This enables operational queries like show me all active RAG datasets that have not been updated in the last three months, or list all datasets owned by the legal team that are marked as deprecated. Queryability transforms the registry from a static catalog into an operational tool for dataset management and governance.

Access control is the final critical registry feature. Track who can read the dataset, which is typically most teams who consume eval results. Read access should be broad because transparency builds trust. Track who can write or modify the dataset, which should be limited to dataset maintainers and the eval engineering team. Write access must be restricted to prevent unauthorized modifications that corrupt quality. Track who can approve new versions, which typically requires both eval engineering and domain experts to sign off. Approval gates ensure that quality review happens before datasets reach production use. Track who can deprecate datasets, which should be limited to the eval lead to prevent accidental removal of critical datasets. Deprecation is a high-impact action that should require executive decision-making.

This prevents unauthorized changes that break reproducibility and ensures quality gates are enforced. If anyone can modify a dataset without review, quality discipline breaks down. If anyone can deprecate a dataset, critical test coverage can vanish without warning. Access control makes the governance model explicit and enforceable.

## Semantic Versioning for Datasets

Borrow the same convention software engineering uses for library versions: major.minor.patch. This three-part version number communicates the nature and impact of changes at a glance.

A major version bump, like moving from 1.0 to 2.0, indicates breaking changes. This means the dataset is no longer comparable to the previous major version. Examples include incompatible schema changes where required fields are added or removed, making old examples invalid under the new schema. Examples include different rubric scoring scales like moving from three-point to five-point or changing what constitutes a passing score. If you previously scored responses as pass or fail but now score them on a numeric scale from 1 to 5, the metrics are incomparable. Examples include task taxonomy changes where you redefine categories or merge existing categories. If you split customer support into technical support and billing support, old aggregate metrics for customer support are not comparable to new per-category metrics.

When you bump the major version, you are signaling that historical comparisons to prior major versions are invalid. Metrics from version 1.x and version 2.x cannot be directly compared. If someone tries to compare them, they should be warned that they are comparing apples to oranges. Major version bumps are disruptive but sometimes necessary when your understanding of quality evolves or when task requirements change significantly.

A minor version bump, like moving from 2.1 to 2.2, indicates backward-compatible additions. The dataset expanded coverage but existing test cases remain unchanged. Examples include new examples added to cover additional edge cases or increase sample size. If you had 300 examples and add 100 more, that is a minor version bump. Examples include new slices added like adding a new task category or difficulty tier. If you add an adversarial difficulty tier that did not exist before, that expands coverage without changing existing examples. Examples include non-breaking metadata fields added like adding a source_type field that does not affect scoring logic. New optional fields are backward-compatible because old examples without that field remain valid.

Minor version changes mean you can compare results to previous minor versions within the same major version, though you should account for the fact that coverage increased. If you go from 300 to 400 examples and the new 100 are harder than average, accuracy may drop slightly even if the model performs identically. This is expected and can be explained by noting that coverage expanded. The key is that comparisons are still meaningful with appropriate context.

A patch version bump, like moving from 2.2.0 to 2.2.1, indicates bug fixes and small corrections. Examples include typos corrected in example text or metadata that do not change the meaning or difficulty of the example. Examples include labels fixed where ground truth was factually incorrect. If you discover that 5 out of 400 examples had wrong expected outputs and you fix them, that is a patch bump. Examples include duplicates removed where the same example appeared twice, often due to data pipeline bugs. Removing duplicates does not change coverage, it just eliminates redundancy.

Patch changes should not affect overall comparability. Metrics from 2.2.0 and 2.2.1 should be very similar, and differences reflect the quality of the fixes rather than a change in what is being measured. If accuracy changes by more than 2 percentage points after a patch, investigate whether the patch was actually a more significant change than you realized. Patches should be surgical corrections, not broad modifications.

The immutability rule is critical. Once a dataset version is published and used in an eval run, it becomes immutable. You cannot modify it in place. If you discover an error in version 2.3.0, you cannot just fix it and re-upload. You must create version 2.3.1 with the fix. Keep old versions accessible for at least 12 to 24 months so historical eval runs can be reproduced. Mark deprecated versions clearly in the registry with a status field and a deprecated reason. Why immutability matters: if version 2.3 changes after an eval run, you cannot reproduce that run. Teams stop trusting results if datasets silently mutate. Auditors and regulators require immutable records for compliance. Immutability is what makes reproducibility possible.

Every version must have a changelog. At minimum, document what changed, why it changed, who made the change, who approved it, and what the expected metric impact is. If you added 50 hard examples, note that you expect accuracy to drop by approximately 3 to 5 percentage points on this version. If you fixed 10 mislabeled examples, note which example IDs were corrected and what the errors were. If you rebalanced slices, document the old and new distributions. The changelog answers the question that arises when metrics change unexpectedly: what is different about this dataset version?

Store changelogs in version control alongside the dataset, in your documentation system, or directly in the registry metadata as a structured field. Link them from the registry entry so anyone looking at a dataset version can immediately see what changed from the previous version. Make changelogs required, not optional. Enforce this through tooling by blocking dataset publishing if no changelog is provided. The discipline of writing a changelog forces you to think clearly about what changed and why, which often reveals issues before they reach production.

## Lineage Tracking: Where Did This Dataset Come From

Lineage answers a fundamental question: where did this dataset come from, and what happened to it along the way? Production-grade datasets are not created in a single step. They go through multiple transformation stages, and tracking those stages is critical for debugging quality issues and ensuring compliance.

A typical dataset lifecycle has five stages. First is the raw source, which might be production logs sampled from real user interactions, expert-written drafts created by domain specialists, or synthetically generated examples from a model or template. Second is sanitization, where PII is removed, sensitive data is redacted, and the data is cleaned to make it safe for use in evaluation. Third is labeling, where ground truth is assigned, rubrics are applied, and difficulty tiers are determined. Fourth is QA, where the dataset is spot-checked, agreement is validated, and issues are fixed. Fifth is release, where the dataset is versioned, registered in the catalog, and locked as immutable.

For each stage, track four pieces of information. What was the input to this stage? What transformation was applied? What was the output from this stage? Who performed the transformation, whether a person, a team, or an automated process? This creates a complete audit trail from raw data to final dataset. If a quality issue arises, you can trace back through the lineage to find where it was introduced. If you discover that 15% of examples have incorrect labels, lineage tells you whether the error was in the raw data, the sanitization step, or the labeling step. This narrows the debugging scope from the entire pipeline to a specific stage.

Lineage also supports compliance and auditability. Regulators ask where your data came from and how it was processed. Lineage provides the answer in structured, traceable form. If you cannot document lineage, you cannot prove that your dataset meets regulatory requirements for transparency and provenance.

Dependencies are the second critical part of lineage. Your dataset depends on external artifacts and configurations. For a RAG dataset, you depend on the prompt version used to generate queries, the model version used to generate responses, the retriever version used to fetch context, the corpus version that contains the documents being retrieved, and potentially tool schemas if the RAG system uses tools. Track all of these dependencies in the dataset lineage. Why? Because when any dependency changes, your dataset may need revalidation or even a new version. If the retriever gets upgraded from version 1.5 to 2.0 and retrieval quality changes significantly, any RAG dataset validated against version 1.5 may no longer accurately reflect production performance with version 2.0. The lineage tells you which datasets need to be revisited when dependencies change.

Think of lineage like a recipe for a dish. The dataset is the finished meal. Lineage tells you what ingredients were used, what steps were followed, who cooked it, and what equipment they used. If the dish tastes different the next time, you can compare recipes to figure out what changed. Maybe you used a different brand of flour. Maybe you baked it at a different temperature. Maybe a different person cooked it and interpreted the recipe differently. Without the recipe history, you are just guessing.

## Reproducibility: The Test of a Versioned System

The test of whether your versioning system works is simple. Given a dataset version plus a config version, you must be able to reproduce the exact eval run that produced historical results. If you cannot do this, your versioning system is incomplete.

Reproducibility requires four things working together. First, immutable snapshots where version 2.3 is always version 2.3 and never changes. Implement this through append-only storage or by using content-addressable storage where the version identifier is a hash of the content. Second, artifact storage where datasets are stored in long-term accessible storage like S3, GCS, Azure Blob Storage, or a dedicated artifact repository like Artifactory or Nexus, not on someone's laptop or a shared drive that might get deleted or reorganized. Use redundant storage with backups to prevent data loss. Third, config snapshots where you capture the exact model version, prompt template, tool schemas, and any other configuration used in the eval run. Store configs alongside eval results so you can always retrieve the full context of a historical run.

Fourth, a deterministic harness where the same inputs produce the same outputs, or at least outputs within known variance bounds for stochastic models where temperature and sampling introduce randomness. Control randomness by setting seeds. Measure and document variance by running the same eval multiple times and reporting confidence intervals. Reproducibility does not require perfect determinism, but it does require bounded variance that you can quantify and explain.

If you can reproduce an eval from six months ago, you can prove your quality claims to stakeholders or auditors. When someone questions a number from an old report, you retrieve the dataset and config, re-run the eval, and show that the results match. This builds trust. You can debug regressions by comparing historical and current runs with identical datasets. If metrics dropped, you can isolate whether the drop is due to model changes, data changes, or config changes by holding each variable constant and varying only one at a time. You can validate that a claimed improvement is real by re-running the baseline and verifying the numbers match what you originally reported. If the baseline cannot be reproduced, the improvement claim is suspect.

If you cannot reproduce historical evals, none of these things are possible. You are operating on faith rather than data. Your metrics become unverifiable assertions rather than reproducible measurements. Trust erodes, and stakeholders stop believing your quality reports.

Reproducibility also future-proofs your eval practice. Teams change, people leave, and institutional knowledge evaporates. The person who ran the original eval may no longer be with the company. The documentation may be incomplete or lost. If everything needed to reproduce an eval is captured in versioned artifacts, new team members can understand historical results without relying on tribal knowledge or hunting through old Slack messages. If someone questions a number from a year ago, you can retrieve the exact dataset and config and prove what was run. You do not need to find the person who ran it or reconstruct their process from memory. The artifacts speak for themselves.

This is the difference between an eval system that builds trust over time versus one that constantly fights credibility questions. Systems with poor reproducibility accumulate doubt. Every unexplained metric change raises questions. Every unreproducible result damages credibility. Systems with strong reproducibility accumulate trust. You can answer questions definitively. You can prove claims with evidence. You build a reputation for rigor that pays dividends when stakeholders need to make decisions based on your metrics.

## Knobs and Defaults: Choosing Your Versioning Approach

Versioning scheme has two standard options. Semantic versioning using major.minor.patch is the default and works well when you compare results across versions frequently and need to communicate the nature of changes. Use semantic versioning when dataset updates are deliberate and planned, and when understanding compatibility matters. Date-based versioning like 2026-01-20 is an alternative that works for rapidly-evolving datasets that refresh weekly or daily. Use date-based versioning when simplicity and chronological ordering matter more than communicating change semantics. Choose semantic versioning if you release datasets monthly or quarterly with intentional changes. Choose date-based if you continuously refresh datasets from production logs.

Registry implementation has a spectrum of options. The lightweight option for startups or early-stage teams is a YAML or JSON file checked into version control. This works when you have fewer than 20 datasets, a single team maintaining them, and simple access control needs. The production option for enterprises is a database with an API layer and a web UI. This works when you have hundreds or thousands of datasets, multiple teams creating and consuming them, and complex access control or compliance requirements. Start lightweight and migrate to a database when the file becomes too large to manage or when querying becomes too cumbersome.

Retention policies vary by dataset status and compliance requirements. Active datasets used in current evals should be retained indefinitely. Deprecated datasets replaced by newer versions should be retained for at least 24 months to support historical analysis and debugging. Archived datasets no longer used but required for compliance should be retained for 5 to 7 years in regulated industries like finance and healthcare. Never delete a dataset version that was used in a production release gate, because you may need to reproduce that eval for audit or legal purposes.

## Failure Modes and Recovery Strategies

When you cannot reproduce last quarter's eval results, the root cause is that datasets were mutated in place rather than versioned. Someone fixed an issue or added examples directly to the existing file without creating a new version. Config versions were not tracked, so you do not know which model or prompt was used. This is the most common versioning failure mode and the most damaging because it destroys trust in all historical results.

Fix this immediately by implementing immutable versioning now. Do not wait for the next dataset release. Apply versioning retroactively to existing datasets by snapshotting the current state and declaring it version 1.0.0. Store all dataset versions in append-only storage where writes are allowed but updates and deletes are blocked. Use S3 with versioning enabled, or use a content-addressable store where filenames are content hashes. Snapshot configs alongside datasets by saving the full configuration as a JSON file with the eval results. Include model version, prompt template, tool schemas, and any runtime parameters like temperature or max tokens. Going forward, enforce immutability through tooling rather than relying on discipline. Make your storage layer reject update operations. Make your publishing workflow create new versions rather than modifying existing ones.

When a dataset changed but nobody noticed until results diverged, there was no changelog requirement and no approval gate for dataset modifications. Anyone could modify datasets without review or documentation. This creates silent corruption where datasets drift without anyone realizing it until metrics become inexplicable.

Fix this by requiring changelogs for every version bump. Make changelog a required field in your dataset registry or publishing workflow. Block publishing if the changelog is empty or generic. Add an approval gate where at least two people, typically the eval lead and a domain expert, must sign off on changes before a new version can be published. Implement this as a required review step in your version control system or as a workflow gate in your dataset management tooling. Add dataset health checks to CI that flag if size drops by more than 10% or difficulty mix shifts by more than 15%, which indicates an unintentional change. These are statistical anomaly detectors that catch drift. Make these checks blocking so datasets with unexplained changes cannot be published. Force the dataset owner to either fix the issue or explicitly document why the change is intentional.

When an old deprecated dataset is still being used in production evals despite being replaced, there is no deprecation enforcement or migration path. The registry shows the dataset as deprecated, but nothing prevents people from using it. Teams keep using deprecated datasets either because they do not check the registry or because migrating requires work and they are busy with other priorities.

Fix this by marking deprecated datasets in the registry with a clear status field and a successor version field that points to the replacement. Make this information visible in query results so people see the deprecation warning when they look up a dataset. Add warnings to the eval harness that print a loud message when a deprecated dataset is used. The warning should state that the dataset is deprecated, explain why, and point to the successor version. After a grace period of six months, require an explicit override flag to use deprecated datasets, forcing teams to justify why they are not migrating. Log these overrides so you can track who is still using deprecated datasets. Track usage of deprecated datasets through your eval telemetry and proactively reach out to teams still using them to help them migrate. Offer migration support, not just warnings.

When you lost the lineage and nobody knows where a dataset came from or how it was created, lineage was never tracked in the first place. This is a compliance nightmare and makes debugging impossible. Fix this by making lineage a required field in the registry going forward. Your dataset publishing workflow should reject datasets without lineage documentation. Backfill lineage for existing datasets by checking version control history to see when files were added and by whom, reading old design docs and proposals that describe dataset creation, and interviewing the people who created the datasets if they are still available. This is painful and incomplete, but it recovers some information and is better than having no lineage at all.

Going forward, no dataset can be published without documented lineage that traces it back to raw sources and lists all transformations applied. Treat lineage as a first-class required field, not optional metadata. Make it part of the dataset creation workflow so it gets captured at the time of creation rather than reconstructed retroactively. This prevents the lineage loss problem from recurring and ensures that all new datasets have the audit trail required for compliance and debugging.

## Enterprise Expectations: What Mature Organizations Do

Mature organizations version every dataset and every configuration that affects eval results, with no exceptions. They maintain a central registry with complete lineage, clear ownership, and accurate status for every dataset. They enforce immutability so published versions cannot be modified, even by dataset owners. They require changelogs and approval from at least two qualified reviewers for every version bump. They track dependencies explicitly, so datasets link to the prompt version, model version, retriever version, and corpus version they were designed for.

They test reproducibility quarterly by picking a random eval run from the past, retrieving the exact dataset and config versions, re-running the eval, and verifying that results match within expected variance. This is not theoretical. They actually do this as a recurring operational practice. They archive old dataset versions but keep them accessible for compliance, storing them in long-term retention storage with access controls. They monitor dataset health by tracking size, difficulty mix, and slice balance as service-level indicators that get reviewed in operational metrics dashboards.

They treat dataset versioning with the same rigor they apply to code versioning. Datasets are artifacts that determine system behavior. They get versioned, reviewed, tested, and deployed with the same discipline as application code. This is the mindset shift that separates mature eval practices from ad hoc ones.

The mindset shift is from treating datasets as documentation or reference material to treating them as production artifacts with the same lifecycle discipline as code. You would never deploy code without versioning it, reviewing it, and testing it. You would never modify deployed code in place. You would never deploy code without knowing its dependencies. Apply the same standards to datasets, and versioning becomes obvious rather than optional. This is not additional overhead. It is the minimum discipline required to operate production AI systems with the reliability and accountability that users and regulators expect.

## The Cost of Not Versioning

The cost of not versioning is high and recurring. You waste time debugging false regressions and false improvements that turn out to be measurement changes rather than model changes. You cannot answer basic questions like why did metrics change, because you do not know what changed in the measurement. You lose trust from stakeholders who notice that metrics are inconsistent or unreproducible. You cannot pass compliance audits in regulated industries because you cannot prove what data was used or that it has not been tampered with.

Every one of these costs is avoidable. Versioning is not expensive. It requires discipline and tooling, but the tooling is straightforward and the discipline is trainable. The implementation cost is measured in days or weeks. The ongoing cost is minutes per dataset release to document changes and create a new version. The return on investment is immediate. The first time you avoid a week-long debugging spiral because you can definitively say the dataset did not change, versioning has paid for itself. The first time you pass an audit because you can retrieve historical datasets and prove immutability, versioning has justified its existence. The first time you confidently explain a metric change by pointing to specific dataset version differences, you realize that versioning is not overhead but essential infrastructure.

Organizations that do not version datasets are flying blind. They make decisions based on metrics they cannot reproduce or defend. They waste time on phantom debugging where the problem turns out to be measurement drift rather than system changes. They build systems on quicksand because their quality claims rest on unreproducible evidence. Every stakeholder question becomes a credibility test that they fail.

Organizations that rigorously version datasets build trust in their metrics, make decisions with confidence, and operate with the reproducibility and accountability that modern AI systems require. Their metrics are reproducible, which means quality claims can be verified. Their comparisons are valid, which means trend analysis actually reflects system changes rather than measurement drift. Their compliance posture is defensible, which means they pass audits. Versioning is the foundation of trust in evaluation, and trust in evaluation is the foundation of confident decision-making about AI system quality.

The next step after versioning your datasets is integrating them into a continuous evaluation pipeline that runs automatically on every code change, which we will explore in the chapters on CI integration.
