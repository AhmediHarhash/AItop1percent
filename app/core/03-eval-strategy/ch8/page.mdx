# Chapter 8 — Agent Evaluation

### Plain English
AI agents—systems that reason, plan, use tools, and execute multi-step tasks autonomously—require fundamentally different evaluation approaches than single-turn chatbots. This chapter teaches you how to measure agent success across dimensions traditional evals miss: planning quality, tool use correctness, error recovery, multi-turn coherence, and safety in open-ended environments.
**How do you evaluate an AI system that takes actions, not just generates text?**

---

### Why This Chapter Exists
- Standard LLM evaluation assumes single-turn Q&A; agents operate over dozens of steps with compounding errors
- "Did the agent achieve the goal?" is binary; you need granular metrics on planning, execution, and recovery
- Tool use failures are catastrophic: calling the wrong API, malformed parameters, ignoring error responses
- Most teams discover agent failures in production because pre-deployment testing doesn't cover real-world complexity
- Evaluating agents in live environments is risky (real API calls, real costs); sandboxes are necessary but introduce realism gaps
- Agent benchmarks (SWE-bench, WebArena) exist but may not reflect your specific domain or task distribution
- Safety/guardrail evaluation for agents is immature: how do you prevent an agent from social engineering users or exfiltrating data?

---

### What Agent Evaluation Actually Is (2026 Meaning)
**Agent evaluation is not:**
- Just measuring final output quality; process matters (how the agent got there)
- A single accuracy number; agents have multi-dimensional performance (planning, tool use, robustness)
- Static test sets; agents need dynamic, stateful environments that respond to their actions
- Solved by existing benchmarks; off-the-shelf evals rarely match your deployment scenario
- Optional because "the agent mostly works"; edge case failures in production cause disproportionate harm

**Agent evaluation is:**
- Systematic measurement of agent behavior across task success, execution quality, safety, and robustness
- Trajectory analysis: examining the sequence of reasoning steps, tool calls, and decisions the agent made
- Environment-based testing: running agents in sandboxed simulations (mock APIs, virtual file systems) that mimic real-world conditions
- Multi-turn evaluation: measuring coherence, context retention, and goal-directedness over long conversations or workflows
- Failure mode discovery: testing adversarial scenarios (malformed tool responses, ambiguous instructions, conflicting goals)
- Safety-first mindset: preventing harmful actions (data deletion, unauthorized access, deceptive behavior) before deployment

---

### Core Components
#### 1. Task Success Metrics
Binary outcome (goal achieved or not) plus partial credit scoring (completed 3 of 5 subtasks). Requires clear task definitions and success criteria.

#### 2. Trajectory Evaluation
Analyzing the step-by-step trace of agent actions: Did it plan efficiently? Recover from errors? Avoid redundant steps? Use tools correctly? Metrics include trajectory length, action validity, plan coherence.

#### 3. Tool Use Correctness
Measuring whether the agent calls the right tools with valid parameters, handles responses appropriately, and retries or escalates on errors. Includes unit tests for individual tool calls and integration tests for workflows.

#### 4. Planning Quality
Evaluating goal decomposition (breaking complex tasks into subtasks), sequencing (logical order of actions), and adaptability (replanning when initial approach fails). Often requires LLM-as-judge or human review.

#### 5. Error Recovery & Robustness
Testing agent behavior when tools fail, inputs are ambiguous, or environments change unexpectedly. Measures retry logic, fallback strategies, graceful degradation.

#### 6. Multi-Turn & Stateful Evaluation
Assessing context retention (remembering user preferences, prior actions), consistency (not contradicting earlier statements), and goal persistence (staying on task across interruptions).

#### 7. Sandbox & Environment Design
Creating isolated test environments (mock APIs, simulated file systems, virtual browsers) where agents can act without real-world consequences. Trade-off: realism vs safety.

#### 8. Human-in-the-Loop Evaluation
Humans observe agent trajectories, rate planning quality, identify failure modes automated metrics miss. Essential for subjective dimensions (user experience, trustworthiness).

#### 9. Agent Benchmarks
Standardized evaluation suites (SWE-bench for coding, WebArena for web navigation, GAIA for general agents) that enable comparison across systems. Often supplemented with custom domain-specific benchmarks.

#### 10. Safety & Guardrails
Evaluating whether agents respect constraints: not accessing unauthorized data, refusing harmful requests, escalating uncertain situations to humans. Includes red teaming and adversarial testing.

#### 11. Maturity Model
Framework for assessing your agent evaluation capabilities: ad-hoc manual testing → automated trajectory checks → comprehensive benchmark coverage → continuous production monitoring.

---

### Enterprise Perspective
**Why enterprises invest in agent evaluation:**
- High-stakes deployment: agents with API access, database permissions, or customer-facing actions require rigorous pre-deployment validation
- Compliance requirements: financial/healthcare/legal agents must demonstrate audit trails, safety controls, human oversight
- Cost control: poorly evaluated agents burn budget via inefficient tool use (unnecessary API calls, redundant actions)
- Brand risk: agent failures are public and viral (customer service agent goes rogue, coding agent introduces vulnerabilities)
- Complex workflows: enterprise agents handle multi-step processes (procurement, onboarding, incident response) where partial failures cascade
- Integration validation: agents interact with legacy systems; evaluation must cover edge cases in API contracts
- Liability mitigation: if your agent causes harm (data breach, financial loss), evaluation logs prove due diligence

**Enterprise operational challenges:**
- Sandbox fidelity: test environments must replicate production complexity (authentication, data formats, error modes) without real consequences
- Cross-functional coordination: agent evaluation requires collaboration between eng, product, legal, security
- Scalability: running full agent workflows (minutes-long trajectories) in CI/CD pipelines is compute-intensive
- Ground truth scarcity: many enterprise tasks lack labeled examples of "correct" agent behavior
- Evolving requirements: what constitutes success changes as business processes evolve

---

### Founder / Startup Perspective
**Why startups care about agent evaluation:**
- Product differentiation: "our agent completes tasks 80% of the time vs 50% for competitors" is a compelling pitch
- Debugging velocity: trajectory logs pinpoint where agents fail, accelerating iteration
- User trust: early adopters tolerate some failures but expect rapid improvement; eval metrics track progress
- Investor credibility: demonstrating systematic evaluation (not just demos) signals technical rigor
- Cost management: inefficient agents burn API budgets; evaluation identifies optimization opportunities
- Catastrophic failure prevention: one incident (agent deletes customer data, leaks secrets) can kill a startup

**Startup-specific challenges:**
- Limited eval budget: comprehensive agent testing is expensive (long trajectories, expensive models)
- No benchmark fit: off-the-shelf evals don't cover your niche use case (legal doc automation, scientific literature review)
- Tool ecosystem immaturity: agent eval tooling is nascent; expect to build custom infrastructure
- Rapidly changing agents: evaluation suites become stale as you iterate on prompts, tools, and models weekly
- Small test sets: you don't have 1,000 annotated agent trajectories; maybe 20-50

**Pragmatic startup approaches:**
- Start with success rate on 10-20 core tasks; manually review trajectories
- Build deterministic sandboxes for critical workflows (mock Stripe API, fake file system)
- Log all agent actions in production; sample and review manually for failure modes
- Prioritize safety evals: test refusal of harmful requests, data access boundaries
- Use existing benchmarks where applicable (SWE-bench for coding agents) as baseline

---

### Common Failure Modes
**Evaluation scope failures:**
- Only testing task success: ignoring how the agent got there (inefficient plans, unsafe actions)
- Happy path bias: testing expected inputs, not adversarial or ambiguous scenarios
- Single-turn mindset: evaluating agents like chatbots instead of stateful systems
- No long-horizon testing: agents tested on 3-step tasks but deployed on 20-step workflows

**Trajectory evaluation failures:**
- No visibility into reasoning: treating agents as black boxes, can't diagnose failures
- Ignoring action validity: counting steps without checking if tool calls were correct
- No efficiency metrics: agent succeeds but takes 50 actions when 10 would suffice
- Missing replanning evaluation: not testing if agent adapts when initial plan fails

**Tool use evaluation failures:**
- Mocking too aggressively: fake APIs always return success, missing error handling bugs
- Not testing parameter edge cases: valid schema but semantically nonsensical values
- Ignoring tool response parsing: agent calls API correctly but misinterprets the result
- No integration testing: individual tool calls work, but chaining them fails

**Environment design failures:**
- Sandbox unrealistic: test environment too simple, missing production complexity
- Non-determinism: environment state changes between runs, making tests flaky
- No stateful scenarios: testing agents in fresh environments instead of mid-workflow states
- Missing failure injection: not simulating API downtime, rate limits, malformed responses

**Planning evaluation failures:**
- No decomposition testing: can't measure if agent breaks down complex goals logically
- Ignoring plan adaptability: not testing if agent replans when tools are unavailable
- Over-reliance on LLM judges: using GPT-4 to score plans without human validation
- No comparison to expert plans: don't know if agent's approach is reasonable vs optimal

**Multi-turn evaluation failures:**
- No context retention tests: not checking if agent remembers user preferences across sessions
- Ignoring goal drift: agent starts task A, gets distracted, completes unrelated task B
- No interruption handling: assuming uninterrupted workflows, not testing mid-task context switches
- Missing consistency checks: agent contradicts earlier statements or actions

**Safety evaluation failures:**
- No adversarial testing: not trying jailbreaks, social engineering, goal manipulation
- Trusting guardrail prompts: assuming "never delete data" instruction is sufficient without testing
- No escalation verification: agent should escalate uncertain high-stakes decisions to humans but doesn't
- Missing audit logging: can't reconstruct what agent did when incidents occur

**Benchmark usage failures:**
- Overfitting to public benchmarks: agent performs well on SWE-bench, fails on your internal codebase
- Not supplementing with custom evals: relying solely on off-the-shelf benchmarks
- Ignoring distribution shift: benchmark reflects 2024 tasks, your 2026 deployment is different
- Benchmark gaming: optimizing for leaderboard instead of real-world performance

**Human evaluation failures:**
- No trajectory review: humans only see final output, miss problematic intermediate actions
- Unclear rubrics: "rate planning quality 1-5" without defining criteria
- Sample size too small: reviewing 10 agent runs doesn't reveal rare but critical failures
- No expert involvement: generalist raters can't judge specialized agent behavior (medical diagnosis, legal research)

**Production monitoring failures:**
- No live trajectory logging: can't debug production failures
- Ignoring success rate drift: agent performance degrades over time as environment changes
- No anomaly detection: unusual action patterns (agent looping, excessive retries) go unnoticed
- User feedback disconnected from eval: complaints don't feed back into test suites

**Maturity model failures:**
- Staying in ad-hoc mode: manual testing doesn't scale past early prototypes
- No automation: every agent change requires hours of manual QA
- Siloed evaluation: eng has tests, product does manual checks, no shared framework
- No continuous improvement: eval findings don't drive systematic agent improvements

---

### Key Takeaway
Agent evaluation is fundamentally about process, not just outcomes. You must evaluate planning logic, tool use correctness, error recovery, and safety—not just "did the task succeed?" Build sandboxed environments for safe testing, log trajectories for debugging, combine automated checks with human review, and treat agent eval as an evolving operational system, not a one-time validation.
