# 4.3 — Source-of-Truth Rules: Which Data Wins When Sources Conflict

In October 2025, a healthcare technology company launched an internal support assistant trained to answer billing questions for their 2,400-person operations team. Within the first week, the assistant gave conflicting answers to the same question asked through different channels. A customer service representative asked whether a specific procedure code was eligible for emergency billing rates. The chat interface said yes, citing a six-month-old policy wiki page. The same question asked through the email interface returned no, citing the authoritative billing database that showed the code had been reclassified in August. Both answers felt confident. Both included citations. The customer service team escalated seventeen cases that week because they didn't know which answer to trust, and the billing director had to halt the rollout and manually audit every answer the system had given. The root cause wasn't model hallucination or poor retrieval. The system had never been told which sources were authoritative, what to do when sources disagreed, or when to abstain because the truth was ambiguous. Every interface was retrieving from different data stores, and no one had written down the hierarchy of which data wins.

This pattern repeats across enterprises in 2026. Ground truth is only as good as the sources behind it. If your evaluation system doesn't encode explicit source-of-truth rules, you will get inconsistent labels, conflicting answers across channels, and production incidents every time policy changes or databases diverge from documentation. This subchapter defines how to build and enforce source-of-truth hierarchies so that everyone—your models, your human evaluators, your engineers, and your leadership—shares the same rules for what is allowed to be used, what is authoritative, what must be cited, and when to abstain or escalate.

## The Truth Stack: Layering Sources by Reliability and Authority

You should think of truth as a stack of layers, each with different reliability, update frequency, and access permissions. The first layer is authoritative system records: billing databases, CRM systems, order histories, ticketing systems, product configuration flags, and identity and access management platforms. These are your systems of record. They reflect the current state of the world as your organization operates it. If a question is about account status, usage limits, payment history, or entitlements, the database is the source of truth. Policy documents and wikis are useful context, but they do not override what the database says.

The second layer is official policy documents. These include terms of service, service-level agreements, employee handbooks, support policies, refund policies, compliance guidance, and any other document that defines what is contractually or procedurally allowed. Policy documents are authoritative for questions about rules, obligations, and commitments. They are versioned, legally reviewed, and owned by specific teams. When a question asks what is allowed or what the company promises, the policy document is the truth reference. But policy documents do not override system records when the question is about individual account state. A refund policy might say refunds are available within thirty days, but the billing database might show that this specific customer already received a refund. The database wins.

The third layer is internal knowledge bases: engineering runbooks, support macros, onboarding guides, troubleshooting wikis, and FAQ pages maintained by product or support teams. These are helpful for operational how-to questions and internal procedures, but they are not authoritative in the same way databases and policy documents are. Wikis are often stale, inconsistent, or written by engineers who are documenting their best understanding rather than creating binding rules. You can use wikis as context, but you must define what happens when a wiki contradicts a database or a policy document. The default rule should be that wikis lose.

The fourth layer is human experts: legal counsel, security leads, finance approvers, senior support managers, and domain specialists who can make judgment calls on ambiguous or exceptional cases. Humans are the source of truth when the question requires interpretation, when policies conflict, or when you're deciding whether to make an exception. But humans are slow, expensive, and they disagree with each other too. If you rely on human adjudication for every edge case, you will create a bottleneck. The correct role for humans in the truth stack is as the final escalation layer for high-risk or ambiguous decisions, and as the authors of new policy when gaps are discovered.

The fifth layer is user-provided information. Users tell you their intent, their account details, their preferences, and sometimes facts about their situation that are not stored in any database. User input is the source of truth for intent and for details that don't exist elsewhere. But users can be mistaken, and in adversarial contexts they can lie. For high-risk workflows—approving a refund, changing account ownership, resetting credentials—you must confirm user claims against system records before trusting them.

The sixth layer is model general knowledge: the training data the model learned from, including public documentation, open web content, and general reasoning. In most enterprise systems, general knowledge is allowed only for low-risk generic education. It is not allowed for policy questions, account-specific questions, compliance guidance, or any Tier 2 or Tier 3 workflow. The failure mode is that the model fills gaps with plausible but wrong facts, and because it sounds confident, no one catches the error until production.

You must decide for every task which layers the model is allowed to access and which layer the evaluator uses as the truth reference. If the model is allowed to retrieve from a wiki but the evaluator is checking correctness against the database, you will get false negatives: the model will be marked wrong even though it followed the allowed sources. If the model is allowed to use general knowledge but the evaluator requires citations to policy documents, you will get failures on otherwise reasonable answers. These rules must be explicit, written down, and synchronized between your prompt engineering, your retrieval logic, and your evaluation rubrics.

## The Conflict Resolution Rule: Highest Authority Wins, With Permission Checks

When sources conflict, you need a clear rule for which source wins. The enterprise default is simple: the highest authority source wins, subject to permission checks. First, check if the model is allowed to access the highest authority source for this task. If it is allowed, that source is the truth reference. If it is not allowed—because of tenant isolation, role-based access control, privacy policy, or because the task is scoped to use only certain document types—the model must abstain, ask for permission, or escalate to a human or a system that has the required access.

This prevents a common failure pattern. A support assistant is asked whether a customer is eligible for a discount. The model retrieves a wiki page that says all enterprise customers get ten percent off. The billing database says this specific customer is on a legacy plan that is not eligible. The model answers yes because the wiki was easier to retrieve and sounded authoritative. The customer is promised something they can't have. The support agent has to walk it back. The root cause is that no one told the model that the database is the authority and the wiki is just context. The fix is a written conflict resolution rule: for billing and entitlement questions, the billing database is the source of truth. If the database and the wiki disagree, the database wins. If the database is not accessible, the model must abstain and route to a human who can check the account.

You should encode this rule in two places. First, in your task taxonomy and your source-of-truth registry, which we will define below. For every leaf task, you list the allowed sources and the authority hierarchy. Second, in your evaluation rubrics and ground truth construction. When you label a ground truth answer, you record which source was used and which source would have won if there was a conflict. If a human evaluator labels an answer as correct because it matched a wiki, but the database said something different, the ground truth is wrong. The evaluator needs to be trained on the same conflict resolution rules as the model.

The permission check is equally important. Even if a source is the highest authority, the model might not be allowed to access it. A customer service agent assistant might be allowed to see account status but not payment card details. A voice assistant might be allowed to read order history but not modify it. An internal HR assistant might be allowed to retrieve general benefits policy but not specific salary information unless the user is authenticated and authorized. When the model is blocked from accessing the authoritative source, the correct behavior is not to guess or fall back to weaker sources. The correct behavior is to tell the user what is needed—verification, permission, authentication—or to escalate to a human or system that has access.

## Source Categories and What They Are Good For

Each layer in the truth stack is best suited for certain kinds of questions. Databases and systems of record are best for anything personalized, anything that must be up to date, and anything that reflects the current state of a transaction, account, or resource. Billing databases tell you what the customer has paid, what they owe, what plan they are on, and what credits or refunds have been issued. CRM systems tell you account tier, contact history, support ticket status, and relationship metadata. Order systems tell you what was purchased, shipped, returned, or canceled. Product configuration databases tell you what features are enabled for a tenant or account. Identity systems tell you who is authenticated, what roles they have, and what permissions are active. If the question is about individual state, the database is the authority. The common trap is using policy documents to answer questions that require account-specific truth. A policy might say refunds are possible within thirty days, but the database might show that this customer is outside the window or has already been refunded. The database wins.

Official policy documents are best for contract questions, compliance guidance, what-is-allowed questions, and anything that defines organizational commitments or legal obligations. Terms of service define what users can and cannot do. Service-level agreements define uptime commitments, support response times, and remediation obligations. HR policies define leave entitlements, acceptable use, and escalation procedures. Support policies define refund windows, exception processes, and escalation paths. When a question asks what the rule is, what the company allows, or what the contract says, the policy document is authoritative. But policy documents can become stale. If your organization updates a refund policy and does not update the version in the retrieval index, the model will cite outdated terms. The fix is policy versioning, last-updated timestamps, and a process for propagating policy changes into your knowledge base and into your evaluation ground truth. Every policy document should have an owner, an effective date, and a review cadence. When a policy changes, the old version should be archived and the new version should be the only one retrievable for production.

Internal wikis and runbooks are best for operational how-to guidance, internal procedures, and non-binding advice. Runbooks tell engineers how to debug a service, how to deploy a configuration change, or how to interpret a monitoring alert. Support macros tell agents what to say in common scenarios. Onboarding guides tell new employees where to find resources and who to ask for help. These documents are useful, but they are not authoritative in the way policy documents and databases are. Wikis are often wrong because they are maintained by whoever had time to update them last, and they reflect individual understanding rather than organizational policy. If a wiki says one thing and a system record says another, the system record wins. If a wiki says one thing and a policy document says another, the policy document wins. Wikis are allowed as supplementary context, but they should not be the sole source for Tier 2 or Tier 3 decisions. If your task is high-risk and the only available source is a wiki, the model should abstain and escalate.

Human experts are best for ambiguous policy decisions, exceptions, special cases, and final sign-off for Tier 3 decisions. Legal counsel interprets contract language when the terms are ambiguous or when a customer is asking for something unusual. Security leads decide whether to grant an access exception. Finance approvers decide whether to waive a fee or approve a large credit. Senior support managers decide whether to escalate a complaint to executive leadership. Humans are the escalation layer when the rules don't cover the case or when the case is too important to automate. But humans disagree with each other too. If you route the same question to three different support managers, you might get three different answers. The fix is adjudication, which we will cover in the next subchapter. When humans make a decision, it should be written down, added to policy if it's a new rule, and added to the evaluation gold set if it's a precedent-setting case.

User-provided information is best for intent, context, and details that are not stored anywhere else. The user tells you what they want, what went wrong, what they remember, and what they prefer. User input is the source of truth for intent: if the user says they want to cancel an order, you trust that intent even if you confirm the order number against the database. User input is also the source of truth for context that does not exist in your systems: a user might tell you they are traveling, or that they use a screen reader, or that they prefer email over phone. But users can be mistaken. They might misremember an account number, confuse two products, or report something as broken when it is working as designed. In adversarial contexts, users can lie: claiming they did not receive a shipment when tracking shows delivered, claiming they were overcharged when the invoice matches the agreed price, or claiming they are entitled to a refund when the terms say otherwise. For high-risk flows, confirm user claims against system records. For low-risk flows, trust the user unless there is a reason to doubt them.

Model general knowledge is the lowest authority. In most enterprise systems, general knowledge is allowed only for low-risk generic education: explaining what OAuth is, summarizing what a load balancer does, or defining a technical term. General knowledge is not allowed for policy questions, account-specific questions, or anything that could lead to incorrect advice, incorrect decisions, or compliance violations. The common trap is that the model fills gaps with plausible facts that sound right but are wrong for your organization. The model might explain how refunds generally work in e-commerce, but your refund policy might be stricter or more generous than the general pattern. If the model is allowed to guess, it will. If you want to prevent guessing, you must explicitly forbid general knowledge as a source and require the model to abstain when authoritative sources are not available.

## Encoding Source Rules Into Your Task Taxonomy and Evaluation

For every leaf task in your taxonomy, you should define the allowed sources, the forbidden sources, and the conflict resolution rule. The allowed sources are the data stores, document types, tools, or human contacts the model is permitted to use. The forbidden sources are the data stores or knowledge the model must not use, either because they are not authoritative or because access is restricted. The conflict resolution rule is the explicit ordering: if source A says X and source B says Y, which one wins.

You can encode this as columns in your task taxonomy table. The allowed sources column might list database, policy documents, knowledge base, user input. The forbidden sources column might list general knowledge, external web, internal wikis for Tier 3 tasks. The conflict resolution column might say database wins over policy, policy wins over wiki, wiki wins over general knowledge. When you assign a task to a developer or a rater, they read the allowed sources and know what the model should use and what the evaluator should check against.

You should also define when to abstain. Abstention is the correct behavior when the allowed sources do not contain the answer, when the sources conflict and the conflict cannot be resolved, or when the question requires access that the model does not have. For RAG systems, abstain-when-uncertain is the default for Tier 2 and Tier 3 tasks. For agent systems, abstain or escalate is the default when a tool fails or returns ambiguous results. For voice systems, abstain or ask for confirmation is the default when critical fields are uncertain. These abstention rules must be documented in your rubrics and enforced in your ground truth.

Freshness is another rule you must encode. If a source can change—pricing, availability, policy terms, account status—you must define the required freshness window. For billing data, freshness might be required to be real-time or within five minutes. For policy documents, freshness might be required to be within thirty days of the last review. For knowledge base articles, freshness might be recommended but not enforced. When the model retrieves a source that is stale or has unknown last-updated metadata, the behavior depends on the task tier. For Tier 0 and Tier 1 tasks, you might allow stale data with a disclaimer. For Tier 2 and Tier 3 tasks, you must abstain and escalate when freshness is unknown or outside the acceptable window.

Permission rules are the final knob. Even if a source is authoritative, access may be restricted by tenant isolation, role-based access control, or privacy policy. A multi-tenant SaaS platform must enforce tenant isolation: an assistant serving Tenant A must not retrieve data from Tenant B's database, even if Tenant B's data is more complete. A role-based system must enforce that only users with the Finance role can see billing details. A privacy-compliant system must enforce that PII is not logged, not displayed to users who don't need it, and not used for purposes other than the primary task. When the model is blocked by a permission rule, the correct behavior is to ask for permission, offer a safe alternative, or escalate to a human who is authorized. The incorrect behavior is to silently fall back to a less authoritative source or to return a generic answer that might be wrong.

## Why Source-of-Truth Failures Are So Common

Source-of-truth failures are endemic in 2026 because most organizations build their AI systems incrementally. You start with a chat interface that retrieves from a knowledge base. Later you add a voice interface that queries a different API. Later you add an email bot that uses a third data store. Later you add a Slack bot that calls yet another service. Each interface was built by a different team, at a different time, with different assumptions about what the truth is. No one stops to write down the global rules. No one audits whether the chat interface and the voice interface will give the same answer to the same question.

This incrementalism creates three specific failure modes. First, different channels use different versions of policy documents because each channel indexes documents independently and no one enforces synchronization. Second, different channels have different permissions because the chat team assumed the model could access account data and the voice team assumed it could not. Third, different channels have different conflict resolution rules because no one told the voice team that the billing database wins over the policy wiki. The result is that customers get different answers depending on which channel they use, and your evaluation metrics are meaningless because you're measuring against inconsistent ground truth.

The fix is not to rebuild everything from scratch. The fix is to create a source-of-truth registry, audit every existing interface against that registry, and enforce that new interfaces must comply with the registry before launch. This requires cross-functional coordination. Product owns the registry structure. Engineering owns the implementation. Legal owns policy versioning. Compliance owns regulatory source requirements. Finance owns billing and payment sources. Security owns access control. Your evaluation team owns the enforcement: no interface can pass evaluation unless it retrieves from the approved sources and resolves conflicts according to the approved rules.

## Building a Source-of-Truth Registry for Your Organization

The source-of-truth registry is the artifact that makes these rules concrete. It is a table or document that lists every domain area in your product, identifies the authoritative source for that domain, specifies the owner team, defines the update cadence, and documents the escalation path when the source is unavailable or ambiguous. The registry is shared across Product, Engineering, Legal, Compliance, and your evaluation team. When a new task is added to your taxonomy, the first question is: what is the source of truth for this task? You look it up in the registry. If it's not there, you add it.

For a billing domain, the primary source of record is the billing database. The secondary documentation is the pricing policy document. The owner team is Finance. The update cadence is daily for the database and monthly for policy. The freshness requirement is that database queries must be current and policy documents must be reviewed within thirty days. The escalation path is to the Finance on-call if the database is unavailable or if there is a conflict between the database and a user claim. This information lives in one row of the registry.

For a refunds domain, the primary source is the billing database for whether a refund was already issued, and the refund policy document for what the rules are. The owner team is Support Operations. The update cadence is weekly for policy and real-time for database state. The freshness requirement is that refund status must be current and policy must be updated within thirty days. The escalation path is to the Support lead if the user is claiming they were promised a refund that policy does not allow. This is another row.

For an SLA domain, the primary source is the contract database or the legal repository. The secondary documentation is the SLA document published to customers. The owner team is Legal. The update cadence is as needed when contracts are signed or renewed. The freshness requirement is that the SLA version must match the contract version for the specific customer. The escalation path is to Legal counsel if a customer is claiming an SLA commitment that is not in the contract. This prevents the common failure mode where a sales representative promises a custom SLA and the support assistant cites the standard SLA.

For an account status domain, the primary source is the authentication database. The secondary documentation is the help center. The owner team is Security. The update cadence is real-time for the database. The freshness requirement is that account status must be current. The escalation path is to SecOps if the user is locked out and claiming they should have access. The assistant should not override the authentication system based on user claims. It should escalate.

Every domain in your product should have a row in this registry. If a task touches multiple domains, you list the primary domain and note the dependencies. If a task requires cross-checking two sources, you specify the conflict resolution rule. The registry is not a nice-to-have. It is the foundation of consistent evaluation. Without it, every rater will make their own judgment about what the truth is, and your ground truth will be noise.

The registry should also document what to do when sources are temporarily unavailable. If the billing database is down, should the model abstain entirely, fall back to cached data with a disclaimer, or escalate to a human? If the policy document service is slow, should the model wait up to five seconds, time out and abstain, or proceed with cached policy? These rules depend on the task tier. For Tier 0 tasks, falling back to cached data with a disclaimer is acceptable. For Tier 3 tasks, you must abstain or escalate if the authoritative source is not reachable in real time. Document these fallback rules in the registry so that engineers, evaluators, and operators all follow the same standards.

The registry should be reviewed quarterly. In each review, check whether the owner teams are still correct, whether the update cadences are still accurate, whether the freshness requirements are still appropriate, and whether new sources have been added that are not yet documented. If a team launches a new feature that introduces a new source of truth, that source must be added to the registry before the feature goes to production. If a team deprecates a source, the registry must be updated and all tasks that referenced that source must be migrated to the new source. The registry is a living document, not a one-time artifact.

## Baking Source Rules Into Ground Truth and Scoring

When you construct ground truth for evaluation, you must record which source was used, which source would have won if there was a conflict, and when abstention was the correct answer. For RAG tasks, ground truth should include which documents contain the answer, which passages support the key claims, and when to abstain due to missing evidence. The scoring rule is that unsupported claims lower the grounding score, and confident unsupported claims are harshly penalized. If the model says something true but does not cite the document, that is a grounding failure. If the model says something false and cites a document that does not support it, that is both a correctness failure and a grounding failure.

For agent tasks, ground truth should include which tool or system is the authority, what the required confirmation steps are, and what the expected final state is after the task completes. The scoring rule is that a nice explanation does not count if the system state is not correct. If the agent tells the user the order was canceled but the order is still active in the database, the task failed. If the agent updates the database but does not confirm the update with the user, the task failed. Ground truth must encode both the conversational success criteria and the system state success criteria.

For policy and compliance tasks, ground truth should include the policy version, the effective date, who owns the policy, and the escalation rules for exceptions. If the policy changed between when the ground truth was created and when the model was evaluated, the ground truth is stale and the evaluation results are invalid. The fix is to version-control your ground truth and tie it to the policy versions in production. When a policy changes, you update the relevant ground truth examples and re-run the evaluation to confirm the model has adapted.

You should also encode source-of-truth rules into your scoring rubrics. For a grounding dimension, the rubric might say: score 4 if the answer is supported by the authoritative source with a citation, score 2 if the answer is correct but cites a secondary source when the authoritative source was available, score 0 if the answer cites a source that does not support the claim. For a correctness dimension, the rubric might say: score 4 if the answer matches the system of record, score 2 if the answer matches a policy document but contradicts the system of record, score 0 if the answer is based on general knowledge and contradicts both the system of record and policy. These rubric rules make it explicit that not all correct-sounding answers are equal. The source matters.

When you train raters, you must train them on source-of-truth rules before you train them on rubrics. Show them the registry. Walk them through examples where sources conflict and explain which source wins. Give them practice examples where they must identify the authoritative source, check whether the model cited it, and score accordingly. If a rater does not understand source-of-truth rules, they will label inconsistently. They will mark an answer as correct because it sounds right, even though it cited the wrong source. They will mark an answer as incorrect because it contradicts their intuition, even though it correctly cited the authoritative source. Source-of-truth training is not optional. It is the foundation of ground truth quality.

## Common Mistakes and How to Avoid Them

The first common mistake is assuming that citations are enough. If your model cites a source, you might assume that makes the answer trustworthy. But citations are only as good as the source hierarchy. If the model cites a wiki and the wiki is wrong, the citation does not help. If the model cites an outdated policy document, the citation is worse than no citation because it gives false confidence. Your evaluation must check not just whether the model cited a source, but whether it cited the correct source according to the hierarchy. If the model cited a wiki when the database was available, that is a failure even if the wiki happened to be correct.

The second common mistake is allowing general knowledge as a fallback. You might assume that if the model does not have access to the authoritative source, it is better to answer with general knowledge than to abstain. But general knowledge is dangerous in enterprise contexts because it is not tailored to your policies, your data, or your obligations. The model might explain how SLAs generally work, but your SLA might be different. The model might explain how refunds generally work, but your refund policy might be stricter. If the model is allowed to fill gaps with general knowledge, users will get answers that sound authoritative but are wrong for your organization. The correct behavior is to abstain and escalate when the authoritative source is not available.

The third common mistake is not versioning your sources. You might assume that the policy document in your retrieval index is the current version, but if no one updated the index when the policy changed, the model will cite outdated terms. You might assume that the database snapshot in your evaluation environment matches production, but if the snapshot is three months old, your evaluation results are invalid. Every source in your registry must have version metadata, last-updated timestamps, and a process for propagating updates to all systems that depend on it. If you do not version your sources, you will get drift between production and evaluation, and you will not be able to diagnose whether failures are due to model regression or source staleness.

The fourth common mistake is not testing conflict scenarios in evaluation. You might have a beautiful source-of-truth registry, but if you never test what the model does when sources disagree, you will not know whether it follows the rules. Your evaluation suite must include conflict cases: examples where the wiki says X and the database says Y, examples where the user claims X but the database says Y, examples where two policy documents appear to contradict each other. For each conflict case, the ground truth must document which source wins and what the model should say. If the model does not resolve conflicts correctly, it fails the evaluation even if it would have passed on non-conflict cases.

The fifth common mistake is treating all sources as equal in evaluation metrics. You might calculate a pass rate that counts answering from the wiki and answering from the database as equally good. But if your source-of-truth rules say the database is authoritative, then answering from the wiki when the database is available is a failure, not a pass. Your evaluation metrics must be weighted by source authority. A correct answer from the authoritative source should score higher than a correct answer from a secondary source. An incorrect answer that cited the authoritative source should score higher than an incorrect answer that cited a non-authoritative source or no source at all. This weighting makes your metrics aligned with your operational priorities.

## Failure Modes and How to Fix Them

The first failure mode is conflicting answers across channels. Symptoms are that the chat interface says one thing, the RAG interface says another, and the voice interface says a third thing. The root causes are that different channels are allowed to access different sources, there is no conflict resolution policy, or different versions of policy documents are indexed in different systems. The fix is a single source-of-truth rulebook per product, consistent policy artifacts across channels, and a shared retrieval index that all channels query. If your chat system is querying a Postgres database and your voice system is querying an Elasticsearch index that is updated weekly, you will get drift. Consolidate the source or enforce that all channels query the same versioned snapshot.

The second failure mode is answering with stale policy. The root causes are that documents are not versioned, there are no freshness checks, and the evaluation set was not updated after the policy changed. The fix is policy versioning with ownership and review cadence, ground truth version updates tied to policy changes, and a regression test that runs every time a policy document is updated to confirm the model now retrieves the new version. You should also enforce freshness metadata in your retrieval pipeline. If a document does not have a last-updated timestamp, it should not be retrievable for Tier 2 or Tier 3 tasks. If a document is older than the freshness window, the retrieval system should flag it and the model should include a disclaimer or abstain.

The third failure mode is leaking private data. The root causes are no permission gating, tools that expose too much information, and no privacy red-team suite in the evaluation plan. The fix is strict PII gating and least privilege access, a privacy evaluation suite that tests whether the model refuses to answer questions it should not have access to, and an audit log that records which sources were accessed for every production query. Your evaluation should include adversarial examples where the user asks for another user's account details, where the user tries to infer PII from aggregate queries, or where the user tries to trick the model into revealing data by claiming they are authorized. If the model leaks data on any of these cases, it fails the safety gate.

The fourth failure mode is wiki says X but the system says Y. The root causes are that the wiki is not maintained, there is no explicit authority ordering, and the model was allowed to retrieve from the wiki for a task that should have been scoped to the database. The fix is that the system of record wins, the wiki must reference the system of record or be demoted to supplementary status, and high-risk tasks are scoped to authoritative sources only. If your task is account-specific, the model should not be allowed to retrieve from the wiki at all. If the task is general guidance, the wiki is fine as long as it includes a disclaimer that account-specific questions should be escalated.

The fifth failure mode is that evaluators and developers are using different sources. Symptoms are that the model passes internal testing but fails evaluation, or the model fails internal testing but passes evaluation. The root cause is that the developer is testing against a local database snapshot and the evaluator is checking against production, or the developer is testing against the latest policy draft and the evaluator is checking against the published version. The fix is to synchronize the evaluation environment with the development environment, use the same source-of-truth registry for both, and version-control the snapshots so everyone is testing against the same ground truth.

## How to Audit an Existing System for Source-of-Truth Compliance

If you are inheriting a system that was built without explicit source-of-truth rules, you need to audit it before you can fix it. The audit process has five steps. First, inventory all the data sources your model can access. List every database, every API, every document index, every tool, and every external service. For each source, document what data it contains, who owns it, how often it updates, and what permissions gate access to it. This inventory is the raw material for your source-of-truth registry.

Second, trace every task in your taxonomy to the sources it uses. For each task, ask: where does the ground truth come from? If the task is to retrieve account status, does it query the authentication database, the CRM, or a cached user profile? If the task is to explain a refund policy, does it retrieve from the policy document repository, from a knowledge base article, or from general knowledge? If the task is to approve a transaction, does it check the billing database, the fraud detection service, or both? Document the sources for every task. If you discover that a task is using multiple sources and you don't have a conflict resolution rule, that is a gap that must be filled.

Third, identify conflicts and ambiguities. Look for cases where different sources say different things, cases where the model is allowed to access multiple sources and you haven't defined which one wins, and cases where the source is ambiguous or stale. For each conflict, decide which source is authoritative and document the rule. For each ambiguity, decide whether the model should abstain, escalate, or fall back to a secondary source. These decisions must be made by domain experts, not by engineers guessing.

Fourth, test the conflict resolution rules in production logs. Pull a sample of real queries and trace how the model resolved conflicts. Did it follow the rules? Did it cite the authoritative source? Did it abstain when it should have? If you find cases where the model violated the rules, that is a bug that must be fixed. If you find cases where the rules do not cover the situation, that is a gap that must be filled. The audit is not complete until you have tested the rules on real data.

Fifth, enforce the rules in evaluation. Update your rubrics to score source-of-truth compliance. Add test cases that check whether the model uses the correct sources, whether it resolves conflicts correctly, and whether it abstains when required. Run your existing evaluation set through the new rubrics and measure the pass rate. If the pass rate is low, that tells you the model is not compliant and needs to be retrained or re-prompted. If the pass rate is high, that tells you the model is already following good practices and the audit surfaced implicit rules that should be made explicit.

## How Source-of-Truth Rules Interact With Retrieval and Prompting

Source-of-truth rules are not just for evaluation. They must be encoded into your retrieval logic and your prompts. If your source-of-truth registry says the billing database is the authority for account-specific questions, your retrieval logic must query the billing database first and only fall back to policy documents if the database does not have the answer. If your registry says general knowledge is forbidden for Tier 3 tasks, your prompt must instruct the model to refuse to answer when it cannot find the answer in the allowed sources.

The retrieval logic should enforce source priority. When the model retrieves documents, it should rank authoritative sources higher than secondary sources. If the billing database and the policy wiki both match the query, the retrieval system should return the billing database result first and the wiki result second, or it should return only the billing database result if the task is scoped to authoritative sources only. This prevents the model from being distracted by lower-authority sources when the high-authority source is available.

The prompt should make source-of-truth rules explicit. Instead of saying "answer the user's question," the prompt should say "answer the user's question using only the billing database and the refund policy document. If the database and the policy conflict, the database wins. If neither source contains the answer, respond: I don't have access to that information." This makes the rules visible to the model and reduces the risk that the model will fall back to general knowledge or guess.

Your evaluation must check that the retrieval logic and the prompt are aligned with the source-of-truth rules. If the registry says the database is authoritative but the retrieval logic ranks wiki articles higher, that is a failure. If the registry says general knowledge is forbidden but the prompt allows the model to guess, that is a failure. Source-of-truth compliance is a three-layer property: the registry defines the rules, the retrieval logic and prompt enforce the rules, and the evaluation verifies that the rules are followed.

You should also log which sources were accessed for every query in production. This creates an audit trail that allows you to diagnose failures, detect drift, and validate compliance. If a user complains that they received wrong information, you can pull the logs and see which sources the model queried, which source it used for the answer, and whether it followed the conflict resolution rules. If a regulator asks whether your model uses authoritative sources for financial advice, you can pull the logs and prove that it does. If you suspect that a source is stale, you can pull the logs and see how many queries depended on it. Logging is not optional. It is the evidence you need when something goes wrong.

The logs should include the source name, the source version or timestamp, the query that was sent to the source, the result that was returned, whether the source was used in the final answer, and whether any conflict resolution rules were applied. This level of detail allows you to reconstruct exactly what happened for any query. It also allows you to aggregate statistics: what percentage of queries used the database versus the wiki, what percentage of queries had conflicts, what percentage of queries resulted in abstention because no authoritative source was available. These statistics tell you whether your source-of-truth architecture is working as intended.

If you discover that fifty percent of queries are falling back to the wiki when the database should have been used, that tells you the retrieval logic is broken or the database is unreliable. If you discover that twenty percent of queries are hitting conflicts and you have no conflict resolution rule, that tells you the source-of-truth registry is incomplete. If you discover that users are getting different answers on retry because sources are updating between queries, that tells you need stronger caching or snapshot consistency. Logs turn your source-of-truth rules from abstract policy into measurable operational reality. They are the feedback loop that tells you whether your system is working or failing.

You should review source access logs monthly. Look for patterns that indicate problems. Look for sources that are queried frequently but never used in the final answer, which suggests they are not useful and can be removed from the retrieval scope. Look for sources that are rarely queried but frequently used, which suggests they are high-value and should be prioritized in retrieval. Look for sources that have high conflict rates, which suggests they are inconsistent with other sources and need reconciliation. Logs are not just for debugging. They are for continuous improvement of your source-of-truth architecture.

Next, we will cover how to lock final truth decisions through gold sets and adjudication processes.
