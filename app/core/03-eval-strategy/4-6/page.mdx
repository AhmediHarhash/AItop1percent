# 4.6 — Ground Truth for Open-Ended & Creative Tasks

In 2019, a team at a major tech company spent three months building ground truth for a code generation model. They hired engineers to write the "correct" solution for each programming problem. The eval launched, scores looked terrible, and the team panicked. Then someone manually reviewed the outputs. The model was writing perfectly valid code — just not matching the exact reference solution. Different variable names, alternative algorithms, more concise logic. All correct. All marked wrong.

This is the creative task trap. You can't build ground truth for code generation the same way you build it for classification. When there are thousands of valid outputs, **reference matching breaks down**.

Let me walk you through how to build ground truth systems that actually work for open-ended tasks.

---

## Why Creative Tasks Break Traditional Ground Truth

In Chapter 4.1, we defined **Type B — Multi-answer truth**: tasks where multiple outputs are correct. We said use must-include checklists and rubrics. That works for structured multi-answer scenarios like chatbot responses with required disclaimers.

But code generation, summarization, and creative writing are different beasts. Not just multiple correct answers — **unbounded correct answers**. You can't enumerate all valid Python functions that solve a problem. You can't list every acceptable way to summarize a 10-page document.

Traditional ground truth assumes you can predefine correctness. Creative tasks require you to **define correctness as a property, not a value**.

---

## Task-Specific Ground Truth Strategies

Different creative tasks need different evaluation frameworks. Here's what actually matters for the big four.

### Code Generation

You care about four dimensions:

1. **Functional correctness** — Does it run? Does it pass test cases? This is the only binary dimension.
2. **Style adherence** — Does it follow language conventions, naming standards, formatting rules?
3. **Efficiency** — Is it O(n) or O(n²)? Does it waste memory?
4. **Edge case handling** — Does it crash on empty input, null values, boundary conditions?

Your ground truth isn't a reference solution. It's a test suite plus a rubric. The test suite checks functional correctness. The rubric scores the other three dimensions on a scale.

For a "write a function to merge two sorted arrays" task, your ground truth might be:
- 10 test cases (empty arrays, single elements, duplicates, large arrays)
- Style rubric: variable names, comments, type hints
- Efficiency requirement: must be O(n+m), not O(n*m)
- Edge case checklist: null handling, empty array handling, type validation

The model doesn't need to match your implementation. It needs to pass your tests and meet your quality bar.

### Summarization

You care about preservation and compression:

1. **Factual preservation** — Are all key facts from the source present in the summary?
2. **Compression ratio** — If you asked for 100 words from a 1000-word doc, did you get ~100 words?
3. **Key point coverage** — Did it capture the main argument, not just the first paragraph?
4. **No hallucinated facts** — Did it invent details not in the source?

Your ground truth is a checklist of must-include facts plus a scoring rubric. For a news article about a product launch, your checklist might require: product name, launch date, key feature, price, availability. Your rubric scores completeness, conciseness, and readability.

You need 2-3 human-written reference summaries to calibrate quality. Not to match exactly — to establish the range of acceptable styles and detail levels.

### Creative Writing

This is the hardest because "good writing" is subjective. But you can still evaluate against a brief:

1. **Adherence to brief** — If the prompt said "write a formal email apologizing for a delayed shipment," is it formal? Is it an apology? Is it about a shipment?
2. **Tone matching** — Does it sound professional, casual, empathetic, urgent as requested?
3. **Novelty** — Does it avoid clichés and generic phrases?
4. **Coherence** — Does it make logical sense? Are sentences connected?

Your ground truth is the original brief plus a tone rubric. You score outputs against the brief's requirements, not against a single "correct" example.

For creative writing, **pairwise comparison** often works better than absolute scoring. Show two outputs side by side and ask: "Which better matches the brief?" This sidesteps the problem of defining a 7/10 versus 8/10.

### Open-Ended Q&A

When someone asks "Why did the Roman Empire fall?" there are dozens of valid answers at different depth levels.

You care about:

1. **Completeness** — Did it address the core question?
2. **Accuracy** — Are the facts correct?
3. **Appropriate depth** — Is it too shallow or drowning in detail for the context?

Your ground truth is a set of **required points** plus optional enrichment points. For the Roman question, required points might be: military decline, economic troubles, political instability. Enrichment points: specific dates, specific emperors, historiographical debate.

You score based on required point coverage, factual accuracy, and depth appropriateness for the user's apparent level.

---

## Spectrum Scoring vs Binary Scoring

Traditional ground truth is binary: correct or incorrect. Creative tasks need **spectrum scoring**: a gradient from poor to excellent.

For code generation, you might score:
- 0 points: doesn't run
- 1 point: runs but fails most tests
- 2 points: passes tests but inefficient or poor style
- 3 points: passes tests, efficient, good style
- 4 points: passes tests, efficient, excellent style, handles edge cases gracefully

For summarization:
- 0 points: hallucinated facts or missed all key points
- 1 point: captured some key points, missed critical facts
- 2 points: captured key points, no hallucinations, decent compression
- 3 points: excellent compression, all key facts, readable
- 4 points: perfect compression, prioritizes most important facts, highly readable

This is the rubric approach from Chapter 2.2 applied to creative output. The difference: you're not scoring a fixed response, you're scoring **emergent properties** of a generated output.

---

## Building Reference Sets for Spectrum Tasks

You can't have one "correct" answer, but you can have **quality anchors** — examples that represent different points on the quality spectrum.

For each eval task, create 2-5 reference outputs:

1. **Minimum acceptable** — The lowest quality you'd ship to users
2. **Good** — Meets expectations, nothing wrong, nothing exceptional
3. **Excellent** — Exceeds expectations, would be proud to show stakeholders
4. Optional: **Below minimum** — Example of what to avoid
5. Optional: **Perfect** — Aspirational, rarely achieved in practice

Anchors do three things:

1. **Calibrate human raters** — "This is what a 3/5 looks like"
2. **Train LLM-as-Judge models** — Show the judge examples of each quality level
3. **Sanity-check your rubric** — If you can't clearly place anchors on your scale, your rubric is ambiguous

For a code generation task, your anchors might be:
- Below minimum: runs but fails half the test cases
- Minimum acceptable: passes all tests, messy code
- Good: passes tests, clean code, O(n²) algorithm
- Excellent: passes tests, clean code, O(n) algorithm, handles edge cases

You write these once per task type, not once per eval item. If you have 100 code generation tasks, you might have one set of anchors per problem class (array manipulation, string processing, recursion, etc.).

---

## Pairwise Comparison as Alternative to Absolute Scoring

Sometimes defining a 0-5 scale is too hard or too arbitrary. **Pairwise comparison** sidesteps the scale problem.

Instead of asking "Is this summary a 4/5?" you ask "Is Summary A better than Summary B?"

This works because humans are better at relative judgment than absolute judgment. You might struggle to say if a piece of writing is 7/10 or 8/10, but you can easily say if it's better or worse than another piece.

The process:
1. Generate multiple outputs for the same input (run your model multiple times, or compare multiple models)
2. Show pairs of outputs to human raters
3. Ask: "Which is better?" or "Which better matches the brief?"
4. Aggregate comparisons into a ranking using Elo rating or Bradley-Terry models

Pairwise comparison shines for **creative writing and open-ended Q&A** where quality is highly subjective. It's overkill for code generation (just run the tests) but perfect for "write a compelling product description."

Downside: you need O(n²) comparisons for n outputs, so it doesn't scale to massive eval sets. Use it for high-stakes evaluations where you need high confidence in quality rankings.

---

## Using LLM-as-Judge for Creative Evaluation

As of 2026, **LLM-as-Judge** is the state of the art for evaluating creative tasks at scale. You use a strong model (like Claude Opus 4.5 or GPT-4) to score outputs from your production model.

The pattern:
1. Write a detailed judging rubric
2. Provide quality anchors (your reference set)
3. Give the judge model the task prompt, the output to evaluate, and the rubric
4. Ask it to score the output and justify its reasoning

For summarization, your judge prompt might be:

```yaml
You are evaluating a summary of a news article.

Original article: [full article text]

Summary to evaluate: [model output]

Rubric:
- Factual preservation (0-2): Are all key facts present? 0 = missing critical facts, 1 = most facts present, 2 = all key facts present
- Compression (0-2): Is the length appropriate? 0 = too long or too short, 1 = acceptable length, 2 = perfect compression
- No hallucinations (0-1): Did it invent facts? 0 = yes, 1 = no

Reference examples:
- Score 3/5 (minimum acceptable): [example summary]
- Score 5/5 (excellent): [example summary]

Provide scores for each dimension and total score. Explain your reasoning.
```

LLM-as-Judge advantages:
- **Scales to thousands of examples** — much cheaper than human raters
- **Consistent application of rubric** — doesn't get fatigued like humans
- **Provides reasoning** — you can debug why something scored low

LLM-as-Judge limitations:
- **Biased toward certain styles** — might prefer verbose or formal outputs
- **Unreliable without good rubrics** — garbage rubric in, garbage scores out
- **Can't evaluate factuality without retrieval** — might not catch subtle hallucinations

Best practice: Use LLM-as-Judge for most of your eval set, then **human-validate a sample**. If you have 1000 summarization examples, have LLM-as-Judge score all 1000, then have humans re-score 100 random examples. If human-LLM agreement is above 85%, trust the LLM scores. If not, refine your rubric.

---

## Knobs and Defaults

**Number of quality anchors**: Default to 3 (minimum acceptable, good, excellent). Add more if your scale is finer-grained or if raters struggle with consistency.

**Scoring scale granularity**: Default to 0-5 for creative tasks. More granular (0-10) adds noise without useful signal. Less granular (0-3) loses important distinctions.

**Pairwise comparisons per output**: If using pairwise comparison, default to 5 comparisons per output. Fewer loses statistical power, more wastes budget.

**LLM-as-Judge validation sample size**: Default to 10% of your eval set, minimum 50 examples. If human-LLM agreement is below 80%, double your sample and refine your rubric.

**Judge model choice**: Use the strongest available model (as of 2026, Claude Opus 4.5 or equivalent). Weaker models produce inconsistent scores. Judge model should be at least as capable as the model you're evaluating.

**Rubric dimensions**: Default to 3-5 dimensions for creative tasks. Fewer oversimplifies, more introduces rater fatigue and inconsistency.

**Test suite size for code generation**: Default to 10 test cases covering happy path, edge cases, and failure cases. Fewer misses bugs, more has diminishing returns.

---

## Failure Modes

**Over-constraining creative output**: You want to evaluate code, so you create a reference solution and mark anything that doesn't match as wrong. Now you've turned a creative task into a string-matching task. The model learns to memorize your style, not solve problems. Fix: evaluate properties (correctness, efficiency, style) not exact output.

**Rubric doesn't capture quality**: You build a 5-point rubric for creative writing that checks grammar, spelling, and word count. Outputs score high but read terribly because your rubric missed coherence, tone, and engagement. Fix: iterate on rubric with real examples. If humans say "this is bad" but it scores well, your rubric is incomplete.

**Anchors are ambiguous**: Your "good" anchor and "excellent" anchor are nearly identical. Raters can't tell the difference, scores are noisy. Fix: make anchors clearly distinct. If you can't articulate the difference between levels, collapse them into one level.

**LLM-as-Judge without validation**: You deploy LLM-as-Judge, scores look great, you ship. Later you discover the judge model systematically gives high scores to verbose outputs regardless of quality. Fix: always validate LLM-as-Judge against human ratings on a sample. Check for systematic biases (length, formality, specific phrases).

**Pairwise comparison without tie option**: You force raters to pick a winner even when two outputs are equally good. This adds noise and frustrates raters. Fix: allow ties or "no clear winner" responses.

**Evaluating factuality without source access**: You use LLM-as-Judge to score summarization for factual accuracy, but you don't give the judge the original source document. The judge can't check facts, so it scores based on fluency instead. Fix: always provide source material when evaluating preservation or accuracy.

**No minimum quality threshold**: You use a 0-5 scale but never define "what score is too low to ship?" Teams celebrate improving from 2.5 to 2.8 while shipping outputs users hate. Fix: set explicit quality gates (e.g., "we only deploy if 95th percentile outputs score 4+ and no outputs score below 2").

---

## Enterprise Expectations

**Document your evaluation strategy per task type**: Enterprises don't want ad-hoc scoring. They want documented frameworks. For each creative task type (code, summarization, writing), you should have a written doc explaining: what you evaluate, your rubric, your anchors, your judge setup, your validation process.

**Separate functional correctness from quality**: For code generation, pass/fail on test cases is different from code quality scoring. Enterprises care about both but track them separately. Functional correctness is a release gate. Quality is a improvement metric.

**Track inter-rater reliability for creative tasks**: When humans score creative outputs, measure agreement. If two raters see the same output and give different scores, your rubric is ambiguous. Enterprises expect Fleiss' kappa or Cohen's kappa above 0.6 for creative evaluation. Below that, results aren't trustworthy.

**Explain LLM-as-Judge decisions to stakeholders**: Non-technical execs are skeptical of "we used AI to evaluate AI." You need to show: (1) judge prompts, (2) human-LLM agreement metrics, (3) examples of judge reasoning. Make the judge's logic transparent and auditable.

**Provide output distribution, not just averages**: For creative tasks, average score hides critical information. Enterprises want to see the distribution: what percentage of outputs are below minimum quality? What's the 10th percentile score? What's the 90th percentile? This shows consistency and risk.

**Version your rubrics and anchors**: As your product evolves, your quality bar changes. You need rubric versioning so you can compare eval scores across time. If you change your rubric, re-evaluate historical outputs with the new rubric to maintain trend continuity.

**Have a human review process for contested scores**: Sometimes a model output scores low but the team thinks it's actually good, or vice versa. Enterprises need a review process: who adjudicates? How do you update rubrics based on contested cases? This is your appeals process.

---
