# 4.6 — Ground Truth for Open-Ended and Creative Tasks

In early 2025, a technology company spent four months building ground truth for their code generation model. They hired experienced engineers to write the canonical correct solution for each programming problem in their evaluation set. The evaluation launched, scores looked catastrophic, and the team assumed the model had regressed. Then someone manually reviewed fifty outputs that scored zero. The model had written perfectly valid code. Different variable names, alternative algorithms, more concise implementations. All correct. All marked wrong because they didn't match the reference solution exactly.

This is the creative task trap. You can't build ground truth for code generation the same way you build it for classification. When thousands of valid outputs exist for a single input, reference matching breaks down. The team had treated code generation like a string comparison problem when it's actually a property verification problem. They learned this after wasting sixteen engineer-weeks and delaying their launch by six weeks.

## Why Creative Tasks Break Traditional Ground Truth

Traditional ground truth assumes you can predefine correctness as a specific value or a small set of values. For classification, the correct answer is one of five categories. For extraction, the correct answer is a specific span of text. For yes-no questions, the correct answer is binary. You write down the correct answer, compare the model's output, score based on exact or partial match. This works beautifully for tasks with bounded correct outputs.

Code generation, summarization, creative writing, and open-ended question answering are fundamentally different. Not just multiple correct answers, but unbounded correct answers. You cannot enumerate all valid Python functions that solve a problem. You cannot list every acceptable way to summarize a ten-page document. You cannot predefine every coherent and appropriate response to an open-ended question. The solution space is too large, too varied, too dependent on subjective quality judgments.

Traditional ground truth says "this is correct, everything else is wrong." Creative task ground truth must say "correctness is a property, not a value." You evaluate whether the output exhibits the properties you care about, not whether it matches a reference. Does the code run? Does it pass test cases? Is it efficient? Does it handle edge cases? Those are properties you can verify without requiring exact match to a reference implementation. This shift from value-based correctness to property-based correctness is the foundation of evaluating creative tasks.

## Task-Specific Ground Truth Strategies

Different creative tasks require different evaluation frameworks. The properties you care about for code generation are not the same properties you care about for summarization. Generic rubrics produce generic garbage. You need task-specific strategies that reflect what correctness actually means for each domain.

For code generation, you care about four dimensions. Functional correctness is the only binary dimension: does the code run, does it pass test cases, does it produce the correct output for given inputs. This is verifiable through automated testing. Style adherence is about whether the code follows language conventions, naming standards, formatting rules. This is partly automated through linters and partly subjective based on readability. Efficiency concerns algorithmic complexity: is it O(n) or O(n squared), does it waste memory, does it make unnecessary passes over the data. This is measurable through complexity analysis and profiling. Edge case handling is about robustness: does it crash on empty input, null values, boundary conditions, malformed input. This is testable through adversarial test cases.

Your ground truth for code generation is not a reference solution. It's a test suite plus a rubric. The test suite verifies functional correctness. The rubric scores style, efficiency, and edge case handling on a spectrum. For a task like "write a function to merge two sorted arrays," your ground truth might include ten test cases covering empty arrays, single elements, duplicates, and large arrays. Your style rubric checks variable names, comments, type hints. Your efficiency requirement specifies the solution must be O(n plus m), not O(n times m). Your edge case checklist verifies null handling, empty array handling, and type validation. The model doesn't need to match your implementation. It needs to pass your tests and meet your quality bar.

For summarization, you care about preservation and compression. Factual preservation is whether all key facts from the source document are present in the summary. Compression ratio is whether the summary meets the length target: if you asked for one hundred words from a thousand-word document, did you get approximately one hundred words. Key point coverage is whether the summary captured the main argument, not just the first paragraph or the most dramatic detail. No hallucinated facts means the summary didn't invent details not present in the source.

Your ground truth for summarization is a checklist of must-include facts plus a scoring rubric. For a news article about a product launch, your checklist might require product name, launch date, key feature, price, and availability. Your rubric scores completeness, conciseness, and readability on a scale. You need two to three human-written reference summaries to calibrate quality expectations. Not to match exactly, but to establish the range of acceptable styles and detail levels. This shows raters what good looks like without constraining them to a single correct answer.

For creative writing, quality is inherently subjective, but you can still evaluate against a brief. Adherence to brief checks whether the output matches the requirements: if the prompt said "write a formal email apologizing for a delayed shipment," is it formal, is it an apology, is it about a shipment. Tone matching verifies the output sounds professional, casual, empathetic, urgent as requested. Novelty checks whether the writing avoids clichés and generic phrases. Coherence verifies the output makes logical sense, sentences connect to each other, the argument flows.

Your ground truth for creative writing is the original brief plus a tone rubric. You score outputs against the brief's requirements, not against a single correct example. For creative writing, pairwise comparison often works better than absolute scoring. You show two outputs side by side and ask which better matches the brief. This sidesteps the problem of defining what constitutes a seven out of ten versus an eight out of ten, focusing instead on relative quality which humans judge more reliably.

For open-ended question answering, there are dozens of valid answers at different depth levels. When someone asks why the Roman Empire fell, you could give a one-sentence answer, a one-paragraph answer, or a ten-page answer. Each could be correct depending on context. You care about completeness: did it address the core question. Accuracy: are the facts stated correct. Appropriate depth: is the answer too shallow or drowning in detail for the apparent context and user sophistication.

Your ground truth is a set of required points plus optional enrichment points. For the Roman question, required points might be military decline, economic troubles, and political instability. Enrichment points might include specific dates, specific emperors, or historiographical debate. You score based on required point coverage, factual accuracy, and depth appropriateness for the user's apparent level. This allows for variation in style and depth while ensuring the answer actually addresses the question.

## Spectrum Scoring Versus Binary Scoring

Traditional ground truth is binary: correct or incorrect. Creative tasks need spectrum scoring: a gradient from poor to excellent. Binary scoring forces you to draw arbitrary lines that don't reflect real quality differences. Spectrum scoring acknowledges that a mediocre summary is better than a terrible one but worse than a good one, and you need to capture those distinctions to drive improvement.

For code generation, you might score on a five-point scale. Zero points means the code doesn't run at all. One point means it runs but fails most test cases. Two points means it passes all test cases but is inefficient or has poor style. Three points means it passes tests, is efficient, and has good style. Four points means it passes tests, is efficient, has excellent style, and handles edge cases gracefully. This scale captures meaningful quality differences that a binary pass-fail cannot.

For summarization, spectrum scoring might work like this. Zero points means the summary contains hallucinated facts or missed all key points. One point means it captured some key points but missed critical facts. Two points means it captured key points, contains no hallucinations, and achieves decent compression. Three points means excellent compression, all key facts present, and readable prose. Four points means perfect compression, prioritizes the most important facts, and is highly readable. This lets you distinguish between summaries that are technically correct but mediocre versus summaries that are genuinely excellent.

Spectrum scoring requires rubrics that define each level clearly. Vague rubrics where the difference between levels is unclear produce noisy scores. Clear rubrics where each level has concrete observable criteria produce reliable scores. You're not scoring a fixed response, you're scoring emergent properties of a generated output. The rubric must specify which properties matter and what threshold of quality defines each level.

## Building Reference Sets for Spectrum Tasks

You can't have one correct answer, but you can have quality anchors: examples that represent different points on the quality spectrum. Anchors calibrate raters, train LLM-as-judge models, and sanity-check your rubric. If you can't clearly place anchors on your scale, your rubric is ambiguous.

For each evaluation task, create two to five reference outputs. Minimum acceptable is the lowest quality you'd ship to users: it's not great, but it's not broken. Good meets expectations: nothing wrong, nothing exceptional, exactly what you'd expect from a competent implementation. Excellent exceeds expectations: you'd be proud to show this to stakeholders, it demonstrates craft and attention to detail. Optionally include below minimum as an example of what to avoid, showing common failure modes. Optionally include perfect as an aspirational level, rarely achieved in practice but useful for showing what excellence looks like.

Anchors do three things. They calibrate human raters by showing what a three out of five looks like in concrete terms. They train LLM-as-judge models by providing examples of each quality level, giving the judge a reference frame. They sanity-check your rubric by forcing you to apply it to real examples. If you struggle to place your anchors consistently on the scale, your rubric levels aren't well-defined.

For a code generation task, your anchors might look like this. Below minimum is code that runs but fails half the test cases. Minimum acceptable passes all tests but has messy code with poor naming and no comments. Good passes tests, has clean code, but uses an O(n squared) algorithm. Excellent passes tests, has clean code, uses an O(n) algorithm, and handles edge cases gracefully. You write these once per task type, not once per evaluation item. If you have one hundred code generation tasks, you might have one set of anchors per problem class: array manipulation, string processing, recursion, dynamic programming.

## Pairwise Comparison as Alternative to Absolute Scoring

Sometimes defining a zero-to-five scale is too hard or too arbitrary. Different raters interpret the levels differently, scores are noisy, and you spend more time arguing about whether something is a seven or an eight than actually improving the model. Pairwise comparison sidesteps the scale problem entirely.

Instead of asking "is this summary a four out of five," you ask "is Summary A better than Summary B." This works because humans are far better at relative judgment than absolute judgment. You might struggle to say if a piece of writing is seven out of ten or eight out of ten, but you can easily say if it's better or worse than another piece. Pairwise comparison exploits this asymmetry.

The process is straightforward. Generate multiple outputs for the same input: run your model several times with different random seeds, or compare outputs from multiple models. Show pairs of outputs to human raters. Ask which is better, or which better matches the brief. Aggregate comparisons into a ranking using Elo rating or Bradley-Terry models. These statistical methods convert pairwise comparisons into a global ranking that tells you which outputs are consistently preferred.

Pairwise comparison shines for creative writing and open-ended question answering where quality is highly subjective and context-dependent. It's overkill for code generation where you can just run the tests, but it's perfect for tasks like "write a compelling product description" or "explain quantum computing to a middle school student." These tasks have no objective correctness criterion, but humans can reliably judge which of two attempts is better.

The downside is that pairwise comparison requires many comparisons to produce stable rankings. For n outputs, you need O(n squared) comparisons in the worst case, though in practice you can get reliable rankings with far fewer. This doesn't scale to massive evaluation sets. Use pairwise comparison for high-stakes evaluations where you need high confidence in quality rankings: comparing models before deployment decisions, evaluating major prompt changes, assessing human-written versus model-generated content for quality benchmarking.

## Using LLM-as-Judge for Creative Evaluation

As of 2026, LLM-as-judge is the state of the art for evaluating creative tasks at scale. You use a strong model like Claude Opus 4.5 or GPT-5 to score outputs from your production model. This lets you evaluate thousands of examples without the cost and latency of human raters, while maintaining reasonable scoring quality if done correctly. The economics are compelling: human raters cost five to fifteen dollars per hour and can evaluate perhaps twenty to thirty examples per hour, so you're paying twenty to forty cents per evaluation. LLM-as-judge costs a few cents per evaluation even with frontier models. For a thousand-example eval set, that's the difference between two hundred to four hundred dollars for humans versus twenty to fifty dollars for LLM-as-judge.

The pattern is simple. Write a detailed judging rubric that specifies what you're evaluating and how to score each dimension. Vague rubrics produce vague scores. Your rubric must be specific: not "evaluate quality" but "evaluate factual preservation on a zero to two scale where zero means missing critical facts, one means most facts present, two means all key facts present." Provide quality anchors: your reference set showing what each score level looks like. Anchors calibrate the judge the same way they calibrate human raters. Give the judge model the task prompt, the output to evaluate, and the rubric. Ask it to score the output and justify its reasoning. The justification is critical: it makes the judge's logic transparent and helps you debug cases where the score seems wrong.

For summarization, your judge prompt might specify that you're evaluating a summary of a news article. You provide the original article text in full, so the judge can verify factual accuracy. You provide the summary to evaluate. You provide a rubric with three dimensions. Factual preservation scored zero to two: are all key facts present, with zero meaning missing critical facts, one meaning most facts present, two meaning all key facts present. Compression scored zero to two: is the length appropriate for the target, with zero meaning too long or too short, one meaning acceptable length, two meaning perfect compression. No hallucinations scored zero to one: did it invent facts not in the source, with zero meaning yes and one meaning no.

You include reference examples showing what a three out of five summary looks like versus a five out of five summary. These are your anchors. You instruct the judge to provide scores for each dimension, a total score, and an explanation of its reasoning. The explanation should reference specific parts of the summary: "scored factual preservation as one because the summary included the product name and launch date but missed the price point mentioned in paragraph three." This level of detail makes judge decisions auditable and debuggable.

LLM-as-judge has three major advantages. It scales to thousands of examples at a fraction of the cost of human raters. You can run a thousand-example evaluation overnight for under fifty dollars. With humans, that's days of work and hundreds of dollars. It applies the rubric consistently without fatigue or drift: the judge doesn't get tired after fifty examples and start skimming. It doesn't misremember the rubric halfway through. It doesn't let recent examples bias current judgments the way humans do when they see five bad summaries in a row and rate the sixth one higher just because it's better than the recent context.

It provides reasoning that helps you debug scoring decisions. When a score seems wrong, you can read the judge's explanation and understand whether the rubric is unclear, the anchors are inadequate, or the judge genuinely identified a quality issue you missed. With human raters, you get a score and maybe a one-sentence comment. With LLM-as-judge, you get a full explanation of the logic, which makes the evaluation transparent and improvable.

LLM-as-judge also has three major limitations. It's biased toward certain styles: judges tend to prefer verbose outputs, formal language, and certain rhetorical patterns regardless of whether those match the brief. If your brief says "write a casual, concise response" and the model produces exactly that, the judge might score it lower than a verbose formal response because frontier models are trained on data that rewards those characteristics. You need to explicitly tell the judge "concise is better than verbose for this task" in your rubric to counteract this bias.

It's unreliable without good rubrics: garbage rubric in, garbage scores out. If your rubric is vague or doesn't capture what you actually care about, the judge will score confidently but meaninglessly. It'll invent its own quality criteria based on what it thinks matters, which may not align with what you actually care about. The rubric is the contract between you and the judge: be specific, be concrete, be measurable.

It can't evaluate factuality without retrieval: the judge might not catch subtle hallucinations if it doesn't have access to the source material and doesn't know enough about the domain to recognize errors. If you're evaluating summaries, you must provide the source document. If you're evaluating question answering about specialized domains, the judge might accept plausible-sounding but incorrect answers because it doesn't have deep domain knowledge. For factual tasks, LLM-as-judge needs either the source material to check against or integration with retrieval to look up facts.

Best practice is to use LLM-as-judge for most of your evaluation set, then human-validate a sample. This gives you the cost and speed benefits of automated evaluation with the reliability check of human oversight. If you have one thousand summarization examples, have LLM-as-judge score all one thousand, then have humans re-score one hundred random examples. The sample should be truly random, not cherry-picked. Calculate agreement: what percentage of examples did humans and the judge agree on within one point. If your scale is zero to five, agreement within one point means if the judge said three, human said two, three, or four.

If human-judge agreement is above eighty-five percent, trust the judge scores for the full set. The judge is applying standards close enough to human judgment that its scores are reliable. If agreement is below eighty percent, something is wrong. Your rubric needs refinement to be more specific. Your anchors need improvement to show clearer examples of each level. Or the task is too subjective for automated judgment and you need human evaluation. Refine and re-validate until agreement is acceptable. Don't ship evaluation results based on a judge that disagrees with humans forty percent of the time. That's not evaluation, it's noise.

## Knobs and Defaults

Number of quality anchors should default to three: minimum acceptable, good, and excellent. This gives raters a clear bottom threshold, a midpoint, and a high-quality target. Add more if your scale is finer-grained or if raters struggle with consistency. A five-point scale might warrant five anchors to cover each level. But start with three because they're easier to maintain and sufficient for most tasks. Fewer than three doesn't give enough calibration: with only two anchors, raters have no midpoint reference and struggle with borderline cases. More than five starts to create more confusion than clarity because the levels blur together and raters spend more time debating which anchor applies than actually evaluating outputs.

Write your anchors as actual outputs, not descriptions. Don't write "good code is clean and efficient." Write an actual code sample that exemplifies good: clean variable names, efficient algorithm, proper error handling. Raters need concrete examples, not abstract criteria. The anchors should be real outputs from your model or hand-written examples that represent each quality level authentically.

Scoring scale granularity should default to zero to five for creative tasks. A six-point scale gives you enough resolution to distinguish between poor, mediocre, acceptable, good, very good, and excellent without introducing so much granularity that scores become noisy. This matches how humans naturally think about quality tiers. More granular scales like zero to ten add noise without useful signal because raters can't reliably distinguish between a seven and an eight. What's the observable difference between those levels? Usually there isn't one, so ratings become arbitrary.

Less granular scales like zero to three lose important distinctions between mediocre and good. A three-point scale forces you to bucket acceptable and good together, which hides useful signal. You want to know if outputs are barely acceptable or actually good because those drive different improvement priorities. Zero to five is the sweet spot: granular enough to capture meaningful quality differences, coarse enough that levels are distinguishable.

For pairwise comparisons, default to five comparisons per output. This means each output gets compared to five randomly selected other outputs. Fewer comparisons lose statistical power and produce unstable rankings: with only two comparisons per output, a single bad luck pairing can skew the ranking significantly. More comparisons waste budget without improving ranking quality much. Research shows that five to seven comparisons per item produces stable Elo ratings. Five is the sweet spot for most tasks. If you have critical evaluations where ranking precision matters enormously, increase to ten comparisons per output, but recognize you're doubling your rater budget for marginal gains.

LLM-as-judge validation sample size should default to ten percent of your evaluation set, with a minimum of fifty examples. Ten percent gives you statistical confidence that the sample represents the full distribution. If your eval set has three hundred examples, validate thirty. If your eval set has ten thousand examples, validating one thousand is overkill: you hit diminishing returns on sample size around five hundred, so validate five hundred to one thousand. The minimum of fifty examples ensures you're not trying to measure agreement on too small a sample where noise dominates signal.

If human-judge agreement is below eighty percent, don't trust the judge yet. Double your sample size to verify the low agreement is real, not a fluke of a small sample. Refine your rubric to be more specific about what each level means. Add more anchors to clarify borderline cases. Re-validate until agreement is consistently above eighty-five percent. Some tasks are inherently subjective and you might never hit ninety percent agreement, but below eighty percent means the judge and humans are applying different standards, which invalidates the evaluation.

Judge model choice should be the strongest available model. As of 2026, that means Claude Opus 4.5, GPT-5, or equivalent frontier models. Weaker models produce inconsistent scores and miss nuances in quality. They'll score based on surface features like length and formality instead of deep features like coherence and accuracy. The judge model should be at least as capable as the model you're evaluating, and preferably more capable. Using a weaker model to judge a stronger model produces unreliable results: the judge can't recognize quality it's not capable of producing itself.

This creates a cost tradeoff: stronger models cost more per evaluation. But using a cheaper weak model that gives unreliable scores is false economy. You save money on inference but waste money on bad decisions based on misleading metrics. Use the strong model for judging. If budget is tight, reduce your evaluation set size rather than downgrading your judge.

Rubric dimensions should default to three to five dimensions for creative tasks. Fewer oversimplifies and misses important quality aspects. A single overall quality score doesn't tell you whether a summary failed because it was inaccurate, verbose, or incoherent. Three to five dimensions let you diagnose why outputs succeed or fail. More than five introduces rater fatigue and inconsistency because people can't reliably track seven different scoring criteria simultaneously while reading an output. They start scoring dimensions based on overall impression rather than careful evaluation of each dimension independently. Three to five is the range where rubrics remain comprehensive without becoming burdensome.

Choose dimensions that are independent and cover different quality aspects. Don't have both "clarity" and "readability" as dimensions if they're measuring the same thing. Don't have both "completeness" and "thoroughness." Each dimension should capture something distinct. Good dimensions for summarization: factual preservation, compression, no hallucinations. Good dimensions for code: functional correctness, efficiency, style. Good dimensions for creative writing: adherence to brief, tone matching, coherence.

Test suite size for code generation should default to ten test cases covering happy path, edge cases, and failure cases. Include at least two happy path cases showing normal correct behavior with typical inputs. Include at least four edge cases covering boundary conditions like empty inputs, single-element inputs, maximum-size inputs, unusual but valid inputs like negative numbers or special characters. Include at least two failure cases showing how the code handles invalid inputs: wrong types, null values, out-of-range values.

Fewer than ten test cases miss bugs. Code can pass five tests but fail in production because you didn't test the edge case that real users hit. More than twenty has diminishing returns because you're mostly duplicating coverage. If you have ten well-chosen tests covering happy path, edges, and failures, adding ten more usually just retests the same logic paths. Exceptions: security-critical code or highly complex algorithms might warrant twenty-plus tests, but for typical code generation tasks, ten is sufficient.

## Failure Modes

Over-constraining creative output happens when you want to evaluate code but you create a reference solution and mark anything that doesn't match as wrong. Now you've turned a creative task into a string-matching task. The model learns to memorize your style, not solve problems. It optimizes for matching your variable names and your code structure instead of optimizing for correctness and clarity. The fix is to evaluate properties like correctness, efficiency, and style, not exact output. Use test suites for functional correctness and rubrics for quality, not reference matching.

Rubric doesn't capture quality happens when you build a five-point rubric for creative writing that checks grammar, spelling, and word count. Outputs score high but read terribly because your rubric missed coherence, tone, and engagement. The rubric is measuring shallow surface features instead of the deep qualities that actually matter. The fix is to iterate on your rubric with real examples. If humans consistently say "this is bad" but it scores well on your rubric, your rubric is incomplete. Add dimensions that capture what humans are reacting to.

Anchors are ambiguous happens when your good anchor and your excellent anchor are nearly identical. Raters can't tell the difference, so scores are noisy and inconsistent. The fix is to make anchors clearly distinct. Each level should be obviously different from the adjacent levels. If you can't articulate the difference between levels in concrete terms, collapse them into one level.

LLM-as-judge without validation happens when you deploy LLM-as-judge, scores look great, you ship the model, then you discover the judge systematically gives high scores to verbose outputs regardless of actual quality. You optimized the model to generate long-winded responses because that's what the judge rewarded. The fix is to always validate LLM-as-judge against human ratings on a sample. Check for systematic biases: does the judge prefer longer outputs, more formal language, specific phrases or structures. If you find bias, adjust your rubric to explicitly penalize it or switch to human evaluation for that task.

Pairwise comparison without tie option happens when you force raters to pick a winner even when two outputs are equally good. This adds noise because raters are making arbitrary choices, and it frustrates raters because you're asking them to make distinctions that don't exist. The fix is to allow ties or "no clear winner" responses. Track the tie rate: if more than thirty percent of comparisons are ties, your outputs might be too similar and you need more diversity, or your evaluation criteria might be too vague and you need clearer quality definitions.

Evaluating factuality without source access happens when you use LLM-as-judge to score summarization for factual accuracy, but you don't give the judge the original source document. The judge can't actually check facts, so it scores based on fluency and plausibility instead. Plausible-sounding summaries with subtle errors get high scores. The fix is to always provide source material when evaluating preservation or accuracy. The judge needs the source to verify that facts are present and accurate.

No minimum quality threshold happens when you use a zero-to-five scale but never define what score is too low to ship. Teams celebrate improving from 2.5 to 2.8 while shipping outputs that users hate. The model is getting better according to the metric but still producing unacceptable quality. The fix is to set explicit quality gates. For example, specify that you only deploy if ninety-fifth percentile outputs score four or higher and no outputs score below two. This ensures you're not just improving on average but actually meeting a minimum quality bar across the entire distribution.

## Enterprise Expectations

Document your evaluation strategy per task type. Enterprises don't want ad-hoc scoring decisions. They want documented frameworks that new team members can learn and auditors can review. For each creative task type like code generation, summarization, or creative writing, you should have a written document explaining what you evaluate, your rubric with each level defined, your anchors with examples, your judge setup if using LLM-as-judge, and your validation process. This documentation serves as the source of truth when questions arise.

Separate functional correctness from quality. For code generation, pass-fail on test cases is fundamentally different from code quality scoring. Functional correctness is binary and verifiable: does the code do what it's supposed to do. Quality is spectrum and subjective: is the code well-written. Enterprises track both but separately. Functional correctness is a release gate: you don't ship code that fails tests. Quality is an improvement metric: you want quality to trend up over time, but a score of three out of five doesn't block release if the code is functionally correct.

Track inter-rater reliability for creative tasks. When humans score creative outputs, measure agreement between raters. If two raters see the same output and give different scores, your rubric is ambiguous or your raters need calibration. Enterprises expect Fleiss' kappa or Cohen's kappa above 0.6 for creative evaluation. Below that, results aren't trustworthy and you're measuring rater noise more than output quality. Calculate inter-rater reliability monthly, and if it drops below threshold, run calibration sessions to realign raters.

Explain LLM-as-judge decisions to stakeholders. Non-technical executives are skeptical of "we used AI to evaluate AI." You need to make the process transparent. Show judge prompts so stakeholders understand what the judge is evaluating. Show human-judge agreement metrics to demonstrate the judge is scoring similarly to humans. Show examples of judge reasoning: here's an output, here's the score, here's why the judge scored it that way. This makes the evaluation legible and auditable rather than a black box.

Provide output distribution, not just averages. For creative tasks, average score hides critical information. An average score of 3.5 could mean everything is solidly mediocre, or it could mean half the outputs are excellent and half are terrible. Enterprises want to see the full distribution: what percentage of outputs are below minimum quality, what's the tenth percentile score, what's the ninetieth percentile. This shows consistency and risk. A system with average 3.5 and tight distribution is far more reliable than a system with average 3.5 and wide distribution.

Version your rubrics and anchors. As your product evolves, your quality bar changes. You tighten standards for mature features, you relax standards for experimental features, you add new dimensions as you learn what matters. Every substantive change to your rubric or anchors should be versioned. When you change the rubric, re-evaluate a sample of historical outputs with the new rubric to understand how scores would have differed. This maintains comparability across time and lets you separate genuine model improvement from rubric changes.

Have a human review process for contested scores. Sometimes a model output scores low but the team thinks it's actually good, or vice versa. Enterprises need an adjudication process: who reviews contested cases, how do you decide if the score was correct, how do you update rubrics or anchors based on what you learn. This is your appeals process. Without it, teams lose trust in the evaluation system because they see scores they disagree with and have no recourse.

In the next subchapter, we'll close Chapter 4 by examining the comprehensive ground truth checklist: a systematic review of everything you need to verify before you trust your evaluation to make deployment decisions.
