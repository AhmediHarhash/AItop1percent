# 5.4 â€” Splits and Holdouts: Train, Dev, Eval Separation and Leakage Prevention

In mid-2025, a financial services company fine-tuned GPT-5 on six months of customer support transcripts to improve response quality for their banking chatbot. They ran their evaluation suite against the fine-tuned model. The results were stunning: 97 percent accuracy, up from 84 percent on the base model. Leadership approved the deployment. The model shipped to production serving 200,000 customers. Within the first week, the quality complaints started. Users reported that the chatbot gave confident answers to novel questions that were subtly wrong. The trust and safety team escalated. Engineers pulled logs and found the model was performing well on familiar question patterns but failing badly on anything outside the training distribution.

The post-mortem took three days to uncover the root cause. The evaluation dataset had been constructed two years earlier by sampling production logs and expert-written cases. When the team built the fine-tuning dataset six months ago, they pulled another sample from production logs covering the same time period. Nobody ran deduplication. Nobody checked for overlap. The analysis revealed that 340 of the 800 evaluation cases appeared verbatim or near-verbatim in the fine-tuning data. The model had memorized the answers. The 97 percent score wasn't measuring generalization. It was measuring memorization. The evaluation was worthless.

This is data contamination, and it remains the silent killer of evaluation credibility in 2026. You can have perfect coverage, perfect difficulty balance, perfect labeling, and perfect metric design. But if your evaluation set leaked into training, development, or fine-tuning data, your metrics measure memorization instead of capability. Your evaluation becomes a vanity number that tells you nothing about production readiness.

## The Contamination Surface Has Expanded

In classical machine learning, the train-test split rule is foundational. You split your data. You train on one partition. You test on the other. You never let test data influence training. This discipline remains essential, but in enterprise AI systems in 2026, the contamination surface is far larger than a simple train-test split.

Fine-tuning data overlaps with evaluation cases when teams sample from the same production logs, user feedback, or escalation tickets without deduplication. Prompt templates include examples verbatim from the evaluation set because the person writing the prompt pulled "good examples" from the same source used to build the eval dataset. RAG corpora contain the exact text you're testing retrieval against, so the system can retrieve the answer directly instead of reasoning about it. Synthetic generation echoes evaluation patterns because the generator was conditioned on examples that included eval cases or was seeded with data that overlaps the holdout.

Embedding spaces leak signal between splits if you embed all your data into a single vector store without isolation. At inference time, retrieval can pull evaluation examples as context, giving the model access to answers it's being tested on. Shared development environments allow engineers to inspect eval data while iterating on prompts, then unintentionally encode those patterns into system design. The result is evaluation scores that look excellent in development and collapse in production because the system was optimized against data it will never see again.

What happens when contamination occurs? Metrics inflate because the model scores high by memorizing answers instead of generalizing from principles. Regressions go undetected because the model passes evaluation by retrieving memorized outputs but fails on novel production cases that don't match the training distribution. Trust collapses when production quality diverges from evaluation quality. Engineers stop believing evaluation results. Product managers stop using metrics for launch decisions. The entire evaluation discipline loses credibility because the dataset was compromised.

With models trained on internet-scale corpora and fine-tuned on proprietary user data, contamination is not a rare edge case. It's the default outcome unless you actively prevent it with process, tooling, and discipline.

## The Three Splits

Data separation requires three distinct partitions with different roles and different access controls. These aren't arbitrary categories. They reflect the different ways data influences system behavior and the different levels of trust you can place in metrics derived from each partition.

**Training and fine-tuning data** is everything used to train or fine-tune the model. This includes base pre-training corpora, which for off-the-shelf models like GPT-5 or Claude Opus 4.5 you don't control but need to be aware of for contamination risk. It includes instruction tuning datasets if you're doing custom instruction tuning on top of a base model. It includes reinforcement learning from human feedback data if you're running RLHF or similar preference tuning. It includes domain adaptation datasets for specialized vocabulary, task formats, or style alignment. And it includes any retrieval corpora or few-shot examples embedded in prompts, because the model sees these at inference time and they influence behavior just like training data.

This data must never appear in dev or eval sets, even in paraphrased form, because the model has seen it and optimized against it. Any evaluation against training data measures memorization, not capability. If your eval set asks "What is the capital of France?" and your fine-tuning data included 50 examples of "What is the capital of France? Paris," your model will answer correctly not because it understands geography but because it memorized the pattern.

**Dev and validation data** is used during development to tune prompts, adjust retrieval strategies, debug workflows, iterate on system design, and explore model behavior. This data is burned for objective evaluation. Once you've optimized your system against dev data, you can no longer use it to measure unbiased performance because your design choices were informed by that data. If you tuned your prompt to handle five specific failure modes you observed in dev data, then evaluated on the same dev data, you're measuring how well you fixed known problems, not how well the system generalizes to unseen problems.

Dev data should never be used for final quality metrics, release gates, or external reporting. It's a sandbox for iteration, not a benchmark for capability. Teams often make the mistake of reporting dev set performance as system performance because dev set scores look better than holdout scores. This is dishonest and leads to production surprises when real users encounter cases outside the dev distribution.

**Eval and test data** is the holdout set held completely separate from all training, fine-tuning, and development activities. This data is used only for final evaluation and release decisions. It's read-only in the strictest sense. Once you evaluate against it, you cannot modify the system based on those results and re-evaluate on the same set. If you do, it becomes dev data. The holdout property is fragile. Any feedback loop from eval results back to system design compromises the holdout and invalidates future evaluations on that set.

This rule is stricter than most teams expect. If you run eval, see a 75 percent score, decide that's too low, make prompt changes to improve it, and re-run eval to confirm the changes worked, you've turned eval into dev. The correct process is to run eval once per system version, make changes based on dev data or production feedback, then run eval again on the next version. Each eval run uses a fresh holdout or a rotated holdout to maintain independence.

The separation isn't just logical. It requires access controls, versioning, and auditability to enforce. Engineers need access to dev data to iterate. They should not have read access to eval data during development. If they can browse eval cases, they'll unintentionally encode patterns from those cases into their prompts and designs. Automated systems that run evaluation should have read-only access to eval data with audit logs tracking every access. Leadership reviewing metrics should see eval results without seeing the underlying cases to avoid inadvertently leaking eval patterns into product discussions that feed back into engineering decisions.

## How Contamination Happens in 2026

Contamination in enterprise AI systems happens through predictable patterns. Understanding these patterns allows you to build defenses before contamination occurs rather than discovering it in post-mortems after production failures.

**Prompt template overlap** is one of the most common contamination paths. You write a prompt template, then use the same phrasing in your evaluation cases because both came from the same product requirements or documentation. The model has now seen the structure and specific phrasings during system design. When you evaluate, you're testing whether the model can follow a template it was explicitly given, not whether it can generalize to novel formulations. The fix is to treat prompt templates as versioned artifacts in your dataset registry and run deduplication checks between prompt examples and eval cases before any evaluation run.

**Fine-tuning data includes eval cases** through careless sampling. A customer escalation becomes fine-tuning data because it represents a failure mode you want the model to handle better. Later, someone adds the same escalation to the evaluation set to test whether the fix worked. Now the model has memorized the answer. The fix is hash-based deduplication across all splits before any data enters training, fine-tuning, or evaluation pipelines. Every case gets a unique hash. Any collision across splits triggers a conflict resolution process with a priority rule: eval data takes precedence and is never removed, dev data is second priority, training data is removed if it collides with eval or dev.

**Synthetic generation echoes eval patterns** when the generator is conditioned on data that includes evaluation cases or when synthetic prompts sample from public documentation that your eval dataset also references. The generator produces cases that are semantically similar to eval cases, and while they're not exact duplicates, they're close enough that the model can interpolate from training to eval without true generalization. The fix is to track synthetic generation prompts, seeds, and source data with strict lineage. Ensure the generator has not seen eval data. Use separate generation prompts for training data versus eval data, and run semantic similarity checks to flag near-duplicates even when exact hashes don't match.

**RAG corpus contains eval answers verbatim** when your evaluation tests retrieval quality by asking questions whose answers are direct quotes from documentation, and that same documentation is in your retrieval corpus. The system retrieves the answer and returns it without reasoning. You're testing retrieval mechanics, not reasoning capability. This is intentional for pure retrieval benchmarks, but it's contamination if you're trying to test question-answering or synthesis. The fix is to hash document chunks in your RAG corpus and check for overlap with eval queries or expected answers. For synthesis and reasoning tests, use eval cases where the answer requires combining information from multiple chunks or rephrasing content in ways not present in the source documents.

**Shared embedding spaces leak signal** when you embed training data, dev data, and eval data into a single vector store for retrieval-augmented workflows. At inference time, a query might retrieve an eval case as context, giving the model the answer as part of the prompt. The fix is to isolate eval embeddings into separate vector stores with strict access controls, or to exclude eval data entirely from any retrieval corpus used during inference.

## Separation Disciplines

Preventing contamination requires discipline encoded into process and tooling. These aren't optional best practices. They're foundational controls without which you cannot trust your evaluation results.

**Dataset registry as single source of truth** means every example in your entire data ecosystem must be registered with a unique identifier based on a content hash of the normalized input and expected output, a split assignment indicating whether the example belongs to train, dev, or eval, a creation date and source tracking provenance back to the original production log, expert authoring session, or synthetic generation job, access permissions defining who can read or write the example based on role and split assignment, and a lock status indicating whether the example is immutable once deployed to a specific split.

No data enters training, prompts, fine-tuning, or RAG corpora without passing through this registry. The registry is the enforcement point for deduplication, access control, and audit logging. When someone wants to add 500 cases to the training set, they submit them to the registry. The registry hashes each case, checks for collisions with existing data across all splits, assigns the cases to the train split if no collisions are found, and logs the transaction with timestamp, submitter identity, and source metadata. If collisions are found, the registry rejects the submission and returns a conflict report showing which cases collided with which existing cases in which splits.

**Hash-based deduplication** runs before any split assignment. For every new example, compute a hash of the normalized input and expected output. Normalization matters because you want to catch cases that are semantically identical even if formatting differs. Lowercase the text, remove extra whitespace, strip punctuation that doesn't affect meaning. Hash the normalized version. Compute hashes for all prompt templates and few-shot examples used in system design, because if a prompt template contains an example that's also in your eval set, that's contamination even though it's not training data in the traditional sense.

Compute hashes for all chunks in your RAG corpus. If your eval set tests retrieval and the answer to an eval query is verbatim in your retrieval corpus, you're testing retrieval mechanics, not synthesis capability. That's fine for pure retrieval benchmarks but contamination for reasoning evaluation. Check for collisions across splits. If a collision is detected, apply a priority rule: eval split has highest priority and is never removed because eval integrity is paramount. Dev split is second priority because dev data supports iteration. Training split examples are removed if they collide with eval or dev because training data is the most flexible partition.

This priority rule prevents accidental contamination through careless sampling or overlapping data sources, but it also means you need to assign eval splits first before filling training and dev splits. If you assign training splits first and later decide to create an eval set by sampling from production logs, you might find that many of your production samples are already in training. The priority rule would force you to remove them from training, which could destabilize your fine-tuning process. The correct order is: define eval set, define dev set, then use remaining data for training.

**Holdout management with access controls** enforces separation at the infrastructure level. Eval data must be stored separately from training and dev data, ideally in a different storage bucket or database with separate credentials and network isolation. Access to eval data is read-only for automated evaluation jobs running on scheduled CI or release gates and completely restricted for engineers during development. Developers should not be able to browse eval data, query eval data, or use eval data for debugging. If they need to debug a specific failure, the failure details are copied into dev data with the original eval case ID redacted.

Eval datasets are versioned and locked. Once eval version 1.0 ships and is used for a release decision, it's immutable. You cannot add, remove, or modify cases without creating a new version. If you discover a contaminated case in eval 1.0, you don't edit eval 1.0 to remove it. You create eval 1.1 with the contaminated case removed and a changelog documenting what changed and why. This preserves reproducibility. If you need to compare a current eval run against a result from six months ago that used eval 1.0, you can still run eval 1.0 even though it's been superseded.

All access to eval data is audited with logs tracking who accessed what data at what time for what purpose. If an engineer accidentally gets read access to eval data through a permission misconfiguration, the audit log reveals it. If a scheduled eval job accesses eval data outside normal evaluation windows, the audit log flags anomalous access patterns. For long-lived products, implement time-gated rotation where holdout sets are periodically refreshed to prevent eval burnout from repeated iteration. The rotation schedule and holdout version history are logged in the registry.

**Prompt templates as versioned artifacts** means every prompt template gets a unique version identifier, typically a semantic version like 1.0.0 or a commit hash from version control if templates are stored in code repositories. Templates are registered in the dataset registry with the same controls as eval data: immutability once deployed, access controls, and audit logging. Before any template is deployed, run a deduplication check against eval examples. If the template contains few-shot examples and any of those examples appear in the eval set, that's contamination. Remove the example from the template or from eval, applying the same priority rule.

Lock status after deployment means once a template version is deployed to production, it's immutable for reproducibility. When you update a prompt, you create a new version, re-run deduplication to ensure the new version doesn't contaminate eval, and treat the old version as frozen. This prevents the common failure mode where someone updates a prompt to fix an eval failure, then re-runs eval on the same set to confirm the fix, unintentionally turning eval data into dev data because the prompt was optimized against eval results.

## Determinism and Replayability

Contamination prevention ensures your evaluation measures what you think it measures. Determinism and replayability ensure you can reproduce evaluation results exactly, which is essential for debugging regressions, comparing system versions, and auditing decisions.

**Fixed random seeds** are required for any operation involving randomness. Model sampling uses temperature, top-p, or top-k, all of which introduce randomness into text generation. Set an explicit seed for every eval run and log it with the results. Most model APIs support seed parameters that make sampling deterministic given the same input and model version. Use them. If the API doesn't support seeds or seeds aren't reliable across versions, set temperature to zero for evaluation runs to eliminate sampling randomness, even though zero temperature changes model behavior slightly compared to production settings.

Dataset sampling for subset evaluation requires a seed. If you're running eval on a random 20 percent sample of your full dataset, set a seed for the sampling operation so you get the same 20 percent every time. Retrieval tie-breaking when multiple chunks have identical scores requires deterministic ordering. Don't rely on database query result order, which may vary. Sort by a stable field like document ID as a secondary sort key. Tool order selection in agent workflows when multiple tools could apply requires a seed or deterministic ordering logic. If your agent can choose between Tool A and Tool B for a given step, and the choice is arbitrary, make it deterministically arbitrary by sorting tools alphabetically and choosing the first.

Without seeded randomness, you cannot reproduce a result, which means you cannot debug failures or validate that a fix worked. If your eval score dropped from 90 percent to 85 percent, you need to reproduce the exact run to identify which cases regressed. If every run produces slightly different results because of uncontrolled randomness, you can't tell whether the drop was real or noise.

**Tool mocks for agent eval replay** address the fact that agent evaluations are notoriously non-deterministic because external tools return different results over time. APIs change their response formats. Databases update with new records. External services have variable latency and availability, which affects timeout logic and retry behavior. The calendar API that returns "available slots for next week" on January 15 returns different results than the same query on January 22. The weather API that returns current conditions gives different answers every hour.

The fix is to record all tool calls and responses during the first evaluation run, store them as a tool response tape, and in replay mode, mock the tools to return recorded responses instead of calling live APIs. This allows you to compare agent decision-making and workflow logic independent of tool output variability. You're testing whether the agent makes the right decisions given specific tool responses, not whether external tools behave consistently.

Tool mocks have a second benefit: they make evaluation faster and cheaper. If your agent workflow makes 10 API calls and each call takes 500 milliseconds, that's five seconds per case. If your eval set has 1,000 cases, that's 5,000 seconds, over an hour of wall-clock time just waiting for API responses. Tool mocks return instantly. The same 1,000 cases might run in five minutes with mocks. For large eval sets or frequent eval runs, the time savings are substantial.

**Config version pinning** means every evaluation run must lock the entire system configuration: the model version or checkpoint identifier, including the specific API version or model deployment timestamp if the provider doesn't guarantee version stability; the prompt template version for every prompt in the system, with templates stored in version control and referenced by commit hash or semantic version; the retriever configuration including vector store snapshot ID, retrieval count K, reranking settings, and embedding model version; and the tool schema versions for every tool available to agent workflows, including input validation rules and output format specifications.

Log all versions in a single configuration file stored with the evaluation results. This file becomes the reproducibility manifest. If you need to reproduce an eval run six months later, you load the manifest and reconstruct the exact configuration. Without version pinning, you cannot determine whether a score change reflects a true regression, a model version change, a prompt template change, or a retrieval configuration drift. Every one of these changes can affect scores, and they can change silently if not pinned.

## Split Ratios and Rotation Strategies

The right split ratio depends on your data volume, fine-tuning strategy, and product maturity. These are practical defaults that work for most enterprise deployments, with modifications for edge cases.

For most enterprise datasets with fine-tuning, a reasonable default is 60 to 70 percent training and fine-tuning data, 15 to 20 percent dev and validation data, and 15 to 20 percent eval and test holdout. This ratio assumes you have enough data that a 15 to 20 percent eval set provides sufficient coverage across your slice dimensions and sufficient statistical power to detect regressions. If you have 5,000 examples total, a 20 percent holdout gives you 1,000 eval cases, which is enough for reliable metrics if the cases are well-distributed across tiers, languages, and difficulty levels.

If you're not fine-tuning and are only doing prompt engineering and retrieval tuning with off-the-shelf models, the training split is zero percent. You divide available data 50-50 between dev and eval, or 60-40 if you expect heavy iteration and want more dev data to burn through without touching the holdout. The key principle is that eval data never participates in any optimization loop, whether that's model training, prompt tuning, retrieval parameter adjustment, or tool schema refinement.

If your dataset is small, fewer than 500 examples total, standard train-dev-eval splits produce eval sets too small for reliable metrics. A 20 percent holdout from 400 examples gives you only 80 eval cases. That's not enough to measure accuracy with reasonable confidence intervals across multiple slices. The fix is k-fold cross-validation with rotating holdouts. Partition your data into five folds of roughly equal size. In iteration one, use folds 1 through 4 for dev and fold 5 for eval. Run your evaluation. In iteration two, rotate: use folds 1 through 3 plus fold 5 for dev and fold 4 for eval. Run evaluation again. Continue rotating through all five folds. Report metrics as the average across all folds with confidence intervals.

This approach maximizes data efficiency while maintaining holdout integrity within each iteration. The cost is that you can't make system changes between iterations without invalidating the cross-validation because once you've seen results from all folds, you've effectively used all data for development. If you need to iterate after completing cross-validation, you need fresh data, which brings you back to the dataset size problem. The real fix for small datasets is to generate more data through synthetic generation, expert authoring, or production sampling until you have at least 500 cases, preferably 1,000 or more.

Deduplication thresholds determine when two examples are considered duplicates. Exact hash match based on normalized input and expected output text means automatic rejection across splits with no human review required. If you hash an example and find the same hash in a different split, one copy must be removed according to the priority rule. Fuzzy near-match, defined as greater than 95 percent semantic similarity based on embedding distance, means flag for human review. A domain expert examines both cases and decides whether they're semantically different enough to keep in separate splits or similar enough that keeping both would constitute leakage because the model could interpolate from one to the other.

The 95 percent similarity threshold is calibrated for most enterprise use cases but may need adjustment. If your domain has many cases that are legitimately similar but test different edge conditions, you might raise the threshold to 98 percent to avoid false positives. If your domain has high variance and you're concerned about subtle leakage through paraphrasing, you might lower the threshold to 90 percent and accept more human review overhead.

Holdout rotation cadence addresses eval burnout for products that iterate frequently or over long time horizons. If your eval set is small, fewer than 1,000 cases, or your product evolves quickly with weekly or bi-weekly releases, the eval set risks burnout. You run eval, make changes, run eval again, make more changes, run eval again. After 20 iterations, you've effectively optimized against the eval set even if you didn't intend to. The eval set is no longer a holdout. It's dev data that you've been tuning against indirectly through accumulated iteration.

The fix is to rotate holdouts periodically. Example schedule: Q1 uses holdout A for eval and holdout sets B and C for dev. Engineers iterate freely against dev data. When it's time for a release, they run eval against holdout A, which they haven't seen during iteration. Q2 rotates to holdout B for eval, with A and C available for dev. Holdout A, which was used for Q1 release evaluation, is now burned and becomes dev data for Q2. Q3 rotates to holdout C for eval. Q4 refreshes all holdouts with new production data, retiring the old holdouts entirely to prevent accumulated leakage from a year of iteration.

This rotation strategy maintains holdout integrity for release gates while allowing continuous iteration. The cost is that you need three times as much eval data as you would with a single static holdout. If you need 500 eval cases per holdout and you're rotating three holdouts, you need 1,500 total cases allocated to the eval partition. This is why dataset size matters. Small datasets can't support rotation without cross-validation, which has its own limitations.

## Failure Modes and Diagnostics

When teams say "eval scores are suspiciously high," the first diagnostic is contamination. The model scores 95 percent or higher, but production quality doesn't match. The eval dataset likely leaked into training, fine-tuning, or prompt templates. The fix is to run a contamination audit: hash all training data, fine-tuning data, and prompt examples, then check for collisions with eval data. If you find significant overlap, rebuild the eval set from fresh production logs or expert-written cases that have never been used for training. Rerun evaluation on the clean holdout and expect scores to drop. That drop is the true signal.

When teams say "we can't reproduce last month's eval results," the problem is lack of determinism. Same model, same dataset, different scores. The root cause is usually missing random seed pinning, automatic model version updates where the team thought they were using the same model but the API silently served a newer version, or tool responses that changed because live APIs were called instead of mocked. The fix is to enforce config version pinning for all runs, use tool mocks for agent replay, and lock eval corpus snapshots for RAG systems so retrieval results are reproducible.

When teams say "we ran out of holdout data," it means the eval set has been burned through repeated iteration. The team has run eval so many times, making system changes based on results each time, that the eval data is effectively dev data now. There's no true holdout left. The fix is to implement holdout rotation so eval sets are refreshed periodically, expand dataset size with a target of 500 or more eval examples minimum to reduce the rate of burnout, and use separate datasets for dev iteration versus final release gates so the release gate eval remains uncontaminated.

When teams say "prompt changes broke eval comparability," it means after updating prompts, scores changed dramatically and the team can't tell if the model regressed or the prompt changed the task definition. The fix is to version all prompt templates and treat prompt changes as system changes that require evaluation on both old and new configurations. When the prompt changes, re-run eval on the previous model version with the new prompt to isolate the prompt impact from the model impact. This allows you to attribute score changes correctly and avoid false positives where you think the model regressed but actually the prompt redefined the task.

## Contamination Detection After the Fact

Prevention is ideal, but what if you suspect contamination has already occurred? Maybe you inherited a dataset from a previous team. Maybe your fine-tuning process changed six months ago and you're not sure if eval data leaked in. Maybe your eval scores are suspiciously high and you want to rule out contamination as the cause. Contamination detection after the fact is harder than prevention, but it's possible with the right analysis.

Start with hash-based overlap detection. Hash every example in your eval set and every example in your training, fine-tuning, and prompt template data. Check for exact matches. If you find exact matches, you have confirmed contamination. Remove the contaminated cases from eval or, better, rebuild the eval set entirely because if some cases leaked, others might have leaked through paraphrasing that hashes won't catch.

Run semantic similarity analysis for near-duplicates. Embed all eval cases and all training data using a sentence embedding model. For each eval case, find the nearest neighbors in the training data based on cosine similarity. If the top neighbor has similarity above 95 percent and isn't from a different split, you likely have contamination through paraphrasing. Review the top 100 highest-similarity pairs manually to confirm. If you find systematic contamination, the eval set is compromised.

Analyze score distributions for contamination signatures. If your model scores 95 percent or higher on eval but only 75 percent on production traffic, that's a red flag. If scores on eval are significantly higher than scores on a separate validation set created from fresh production data, that's another red flag. If scores improved dramatically after fine-tuning without corresponding prompt or architecture changes, contamination is a likely explanation. These signals aren't proof, but they justify deeper investigation.

Test the model on a clean holdout created after contamination could have occurred. Sample 200 to 500 cases from recent production logs or have domain experts write fresh cases that definitely weren't in any training, dev, or eval set. Run evaluation on this clean holdout. If scores drop significantly compared to your main eval set, your main eval set is likely contaminated. The clean holdout gives you the true performance estimate.

## What This Looks Like at Scale

Enterprise teams that run evaluation as a discipline maintain a dataset registry where every example is logged with split assignment, content hash, source provenance, creation timestamp, and access controls. The registry is the single source of truth for all data in the evaluation ecosystem. Before any data is used for training, fine-tuning, prompt engineering, or evaluation, it's registered. The registration process enforces deduplication, assigns splits, and logs provenance. No exceptions.

They run hash-based deduplication across train, dev, and eval splits before any data is used and after every dataset update. Deduplication isn't a one-time setup step. It's a continuous process that runs on every data ingestion, every dataset expansion, and every split rebalancing. Automated systems flag hash collisions and near-duplicates for review. Human reviewers make final decisions on ambiguous cases. The deduplication log is auditable and versioned so you can trace every decision.

They version and lock eval sets, treating them as immutable once deployed. Eval version 1.0 is locked. If you need to add cases, you create eval version 1.1 with the additions clearly marked. If you need to remove contaminated cases, you create eval version 2.0 with a changelog documenting what changed and why. Versioning ensures reproducibility and traceability. You can always go back and rerun an evaluation on a specific version to compare results across time.

They enforce deterministic replay by pinning random seeds, model versions, prompt versions, and tool response mocks for every evaluation run. Every eval run produces a configuration manifest listing every version number, every seed, every parameter setting that affects results. The manifest is stored with the results. If you need to reproduce a result six months later, you load the manifest and replay the exact configuration. This is essential for debugging regressions, validating fixes, and auditing decisions.

They implement holdout rotation for long-lived products where eval sets risk burnout from repeated iteration over months or years. The rotation schedule is planned in advance and documented. Engineers know which holdout is current and which holdouts are available for dev. When a holdout rotates, it's announced and the dataset registry is updated. Holdout rotation prevents eval burnout while maintaining continuous iteration velocity.

They treat contamination audits as a mandatory pre-release gate, running deduplication checks and overlap analysis before every major launch. The audit checks hash collisions, semantic similarity, and score distribution anomalies. It produces a contamination risk report that goes to the release review board. If contamination risk is above threshold, the release is blocked until the eval set is cleaned or rebuilt. This prevents contaminated evals from approving launches that will fail in production.

They track eval data provenance so they can trace any example back to its original source, creation date, and split assignment history. If a contaminated case is discovered, provenance tracking allows you to find all related cases from the same source batch and audit them for contamination. If a production incident reveals that an eval case didn't reflect real user behavior, provenance tracking allows you to trace it back to the synthetic generation prompt or expert authoring session that created it and fix the source process.

Splits and holdouts are the foundation of evaluation credibility. They ensure your metrics measure generalization, not memorization. They prevent the silent killer of contamination that produces inflated scores and fragile production deployments by optimizing against leaked test data. They enable reproducibility so you can debug regressions and validate fixes with confidence. And they maintain the integrity of your evaluation signal over time as your product, data ecosystem, and model landscape evolve through continuous iteration and improvement.

The next chapter covers version control and documentation practices that make your eval dataset a maintainable asset instead of an unmanageable liability.
