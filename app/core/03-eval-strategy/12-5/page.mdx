# 12.5 — Canary & Shadow Deployment

## Testing in the Real World

A major e-commerce company spent three months building a perfect evaluation dataset for their customer service chatbot. Five thousand carefully labeled conversations. Twenty quality metrics. Automated evaluation pipeline running on every commit. When they shipped the new model to production, it failed spectacularly within two hours.

What happened? The evaluation dataset was all English queries from US customers during business hours. Production traffic included Spanish queries, broken grammar from mobile typing, multi-turn conversations that wandered off-topic, and edge cases their team never imagined. Their test set covered maybe five percent of what production actually sees.

This is the fundamental problem with offline evaluation. You can only test what you know to test. Your curated datasets, no matter how carefully constructed, are a tiny sample of production reality. Real users are infinitely creative in ways they break your system.

**Canary deployment** and **shadow deployment** solve this by testing with real production traffic in controlled ways. Shadow deployments run your new version alongside the old one, processing real queries but showing users only the old results. Canary deployments route a small percentage of real traffic to the new version and show users the new results, while monitoring quality closely. Both techniques let you evaluate with the full complexity of production before committing to a full rollout.

This chapter explains how to use canary and shadow deployments to catch regressions that offline evaluation misses, when to use each technique, and how to build automated promotion and rollback systems that let you ship confidently.

---

## Why Offline Evaluation Is Never Enough

Your evaluation dataset is a snapshot. It represents what you knew to test when you built it. Production is a living, evolving stream of queries you never anticipated.

Consider a customer support bot. Your eval set has questions about account issues, billing, shipping, returns. All clearly stated, grammatically correct, focused on one topic. Then production happens:

A user types "my package says delivered but i never got it also can i change my password and why did you charge me twice last month" — three unrelated issues in one run-on sentence. Your eval set has nothing like this.

Another user asks "wheres my stuff" with zero context, expecting the bot to magically know which order they mean from their account history. Your eval set always includes explicit order numbers.

A third user switches languages mid-conversation, starts with a reasonable question then escalates to angry demands, then tests the bot with nonsense to see how it responds. Your eval set is polite and monolingual.

These are not edge cases. This is twenty percent of production traffic. You cannot predict this diversity when building static evaluation datasets.

**Test set drift** compounds the problem. The queries users send today are different from the queries they sent six months ago. Product launches change vocabulary. Seasonal events shift intent distribution. Viral social media posts create new slang. Your eval set becomes stale the moment you finalize it.

You need to evaluate on real production traffic. But you cannot just ship a new model and hope for the best. You need a controlled way to test with real queries before committing to a full rollout.

---

## Shadow Deployment: Safe Testing with Real Traffic

**Shadow deployment** runs your new model version alongside your current production version. Both versions receive the same real production traffic. Users see only the current version's responses. The new version's outputs go into a comparison pipeline but never reach users.

This is the safest way to test a new model with real production queries. There is zero user impact. If the new version hallucinates, crashes, or produces gibberish, users never see it. You catch the problem in your comparison metrics before any harm is done.

**Shadow deployment architecture** looks like this:

- Production request comes in
- Request is duplicated and sent to both current and shadow models
- Current model response is returned to the user immediately
- Shadow model response is captured for comparison
- Comparison metrics are calculated and logged
- If shadow quality is acceptable after sufficient volume, promote to canary

The shadow model runs asynchronously. It does not add latency to user requests. If the shadow model is slow or fails, the user never knows.

**What to measure in shadow mode:**

**Agreement rate** — what percentage of shadow outputs match or are equivalent to production outputs? For deterministic systems, this should be very high. For LLMs with temperature greater than zero, define "equivalent" carefully. Exact string match is too strict. Use semantic similarity or human-labeled equivalence.

**Quality score difference** — run both outputs through your automated quality rubric. Is the shadow model scoring higher, lower, or the same? If your production model scores seventy-five percent on average and shadow scores seventy-two percent, you have a regression.

**Latency difference** — is the shadow model faster or slower? Latency matters for user experience. A better model that takes twice as long may not be worth shipping.

**Cost difference** — if the shadow model uses a more expensive underlying model or more tokens, calculate the cost impact of a full rollout. A model that improves quality by two percent but doubles cost may not be the right trade.

**Error rate** — does the shadow model fail more often? Timeouts, exceptions, refusals to answer? Even if quality is higher when it succeeds, a model that fails ten percent of the time is not production-ready.

**Shadow duration** — run shadow mode long enough to capture diverse traffic. If you only shadow for one hour, you might miss the evening traffic spike, the weekend behavior shift, or the edge cases that happen once per thousand queries. Run shadow for at least twenty-four hours, ideally seven days, to capture full weekly patterns.

Shadow deployment tells you how the new model performs on real queries without any risk. It is the first gate before canary testing.

---

## Canary Deployment: Real Users, Real Feedback

**Canary deployment** routes a small percentage of production traffic to the new model version. Users in the canary group see the new model's responses. You monitor quality, satisfaction, and error rates in real time. If metrics drop below acceptable thresholds, you roll back immediately.

Canary deployment has real user impact, which makes it scarier than shadow but also more informative. Shadow mode cannot tell you if users actually like the new model. Canary mode gives you real satisfaction signals: thumbs up/down, conversation abandonment, follow-up clarification requests, customer support tickets.

**Canary traffic routing** is typically done by percentage or by user segment:

**Percentage routing** — one percent of all requests go to canary, ninety-nine percent to current production. Simple to implement. Random distribution across user base.

**User segment routing** — route canary to a specific user segment. Internal employees first, then beta users, then free tier, then paying customers. This lets you test on more forgiving populations before exposing your most valuable users.

**Session consistency** — once a user is in the canary group, keep them there for the entire session or longer. If a user gets canary on one request and production on the next, they experience inconsistent behavior, which is confusing.

**Canary population size** — start small. One percent is typical. For high-traffic systems, one percent might be thousands of requests per hour, which is enough to detect problems quickly. For lower-traffic systems, you might need five or ten percent to get statistical significance within a reasonable timeframe.

**What to monitor in canary mode:**

**Quality metrics** — run the same automated quality metrics you use in offline eval and shadow mode. Is canary quality statistically equivalent to production quality?

**User satisfaction** — thumbs up/down rates, conversation completion rates, user ratings. Is canary satisfaction equal to or better than production?

**Error rates** — timeouts, exceptions, refusals. Is canary failing more often than production?

**Latency** — P50, P95, P99 latency. Is canary slower? Latency regressions often matter more to users than small quality differences.

**Cost per request** — if canary costs significantly more, calculate the financial impact of a full rollout before promoting.

**Canary duration** — run canary for at least several hours, ideally twenty-four to seventy-two hours. You need enough volume to detect regressions with statistical confidence. If your canary is one percent and you get one thousand requests per hour, that is only ten canary requests per hour. You need hundreds or thousands of canary samples before you can confidently say quality is equivalent.

---

## Shadow vs Canary: When to Use Each

**Shadow deployment** is safer. No user impact. Use shadow first, always, before canary. Shadow mode catches obvious regressions: crashes, refusals, severe quality drops, unacceptable latency. If shadow looks bad, you never get to canary.

But shadow has limits. Shadow mode cannot tell you if users like the new model. Shadow mode cannot measure real satisfaction, engagement, conversion, or retention. Shadow mode gives you technical metrics but not business metrics.

**Canary deployment** has real user impact but gives real feedback. Canary tells you if users are happier, more engaged, more successful. Canary exposes you to reputational risk if the new model behaves badly, but it also gives you the truth about user experience.

**Typical progression:**

1. **Offline eval** — test on curated datasets. Fast feedback loop. Catches obvious regressions.
2. **Shadow deployment** — test on real production traffic with no user impact. Catches problems your test set missed.
3. **Canary deployment** — test on real users with real impact. Measure real satisfaction.
4. **Gradual rollout** — promote canary from one percent to five percent to twenty-five percent to one hundred percent, monitoring at each stage.

Each stage is a gate. If you fail at any stage, stop and debug. Do not proceed to the next stage with known regressions.

---

## Shadow Deployment Evaluation in Practice

You have a customer support chatbot. Current production model is GPT-4. You want to test GPT-5.1. You start with shadow deployment.

**Step one:** Duplicate all production traffic to the shadow model. Current model responses go to users. Shadow model responses go to a logging pipeline.

**Step two:** For each request, calculate comparison metrics:

- **Exact match rate** — what percent of shadow responses are identical to production responses? For your use case, this is fifteen percent. Most responses differ slightly in phrasing but convey the same information.
- **Semantic similarity** — use an embedding model to calculate cosine similarity between production and shadow responses. Average similarity is 0.92. High similarity is good; it means shadow is not wildly different.
- **Quality score** — run both responses through your automated rubric: relevance, accuracy, helpfulness, tone. Production average: seventy-eight percent. Shadow average: eighty-one percent. Shadow is slightly better.
- **Latency** — production P95 latency: 1.2 seconds. Shadow P95 latency: 1.4 seconds. Shadow is slightly slower but still acceptable.
- **Cost** — production: six cents per request. Shadow: eight cents per request. Shadow is thirty-three percent more expensive. This is a significant cost increase for a three percent quality improvement.

**Step three:** Sample divergent cases. Identify requests where production and shadow responses differ significantly. Review manually. Why do they differ? Is shadow better, worse, or just different?

You review one hundred divergent cases. In sixty cases, shadow is clearly better: more complete answers, better tone, more accurate. In thirty cases, shadow is equivalent but phrased differently. In ten cases, shadow is worse: misunderstands the question or gives incorrect information.

**Step four:** Decision. Shadow quality is slightly better on average. Latency is acceptable. Cost is higher but within budget. The ten regressions are concerning. You investigate those cases and find a pattern: shadow struggles with ambiguous questions that reference prior conversation turns. You decide to fix this issue before promoting to canary.

Shadow deployment gave you the signal you needed without impacting a single user. You discovered a regression pattern that did not appear in offline eval because your test set did not include multi-turn ambiguous queries.

---

## Canary Deployment Evaluation in Practice

You fixed the shadow regressions. Shadow metrics look good after another forty-eight hours of testing. You promote to canary at one percent traffic.

**Step one:** Route one percent of production traffic to the new model. Users in the canary group see the new model's responses.

**Step two:** Monitor canary metrics in real time. You have a dashboard that shows:

- **Thumbs up/down rate** — production: twelve percent thumbs up, two percent thumbs down. Canary: fourteen percent thumbs up, two percent thumbs down. Canary is slightly better.
- **Conversation completion rate** — production: seventy-five percent. Canary: seventy-six percent. Equivalent.
- **Follow-up question rate** — production: thirty percent of conversations have a follow-up clarification question. Canary: twenty-eight percent. Canary is slightly better; fewer users need to ask follow-ups.
- **Error rate** — production: 0.5 percent. Canary: 0.4 percent. Equivalent.
- **Latency** — production P95: 1.2 seconds. Canary P95: 1.4 seconds. Slightly slower but still within SLA.

**Step three:** Wait for statistical significance. At one percent traffic and one thousand requests per hour, you get ten canary requests per hour. After twenty-four hours, you have 240 canary samples. This is enough for directional confidence but not bulletproof statistical significance. You let it run for seventy-two hours, collecting 720 canary samples.

**Step four:** Promote to five percent. Metrics still look good. After another forty-eight hours at five percent, you have thousands of canary samples. No regressions detected.

**Step five:** Promote to twenty-five percent. After twenty-four hours, metrics remain stable. Promote to one hundred percent. Canary becomes production. Old model is retired.

The gradual rollout gave you multiple checkpoints to detect regressions before committing to a full deployment. If metrics had dropped at any stage, you would have rolled back.

---

## Canary Promotion Strategy: When to Move Forward

You do not want to wait forever in canary. But you also do not want to rush. What is the right promotion policy?

**Time-based gates:** Do not promote until the canary has been running for at least X hours. For low-traffic systems, this might be forty-eight hours. For high-traffic systems, twelve hours might be enough. The goal is to capture diverse traffic: different times of day, different user types, different query patterns.

**Volume-based gates:** Do not promote until the canary has processed at least Y requests. If you need statistical confidence in your metrics, you need sample size. Five hundred canary samples is a reasonable minimum for basic confidence. Five thousand is better.

**Quality-based gates:** Do not promote unless canary quality is statistically equivalent to or better than production. Define equivalence thresholds in advance. For example, canary is acceptable if quality score is within two percentage points of production and user satisfaction is within one percentage point.

**No-regression gates:** Do not promote if any critical metric has regressed significantly. Define critical metrics and regression thresholds in advance. For example, error rate above one percent is unacceptable, latency P95 above two seconds is unacceptable, user satisfaction below ten percent is unacceptable.

**Gradual ramp schedule:** Do not jump from one percent to one hundred percent. Use a staged rollout: one percent for twenty-four hours, then five percent for twenty-four hours, then twenty-five percent for twenty-four hours, then one hundred percent. Each stage is a gate. If any stage fails, stop and roll back.

**Weekend pauses:** Do not promote during low-traffic periods when you cannot detect regressions quickly. If you promote on Friday evening and the canary is bad, you might not have enough weekend traffic to notice before Monday. Wait until Monday morning to promote.

**Example promotion policy:**

- Shadow mode: minimum forty-eight hours, compare on ten thousand requests
- Canary one percent: minimum twenty-four hours, minimum five hundred canary requests, quality within two percent of production
- Canary five percent: minimum twenty-four hours, minimum two thousand canary requests, no critical metric regressions
- Canary twenty-five percent: minimum twelve hours, minimum five thousand canary requests
- Canary one hundred percent: promote if twenty-five percent stage passes all gates

This policy balances speed and safety. You can go from shadow to full production in five days if everything goes well. If problems appear at any stage, stop and debug.

---

## Automatic Rollback: Failing Fast Without Humans

Canary deployments only work if you can roll back quickly when things go wrong. Waiting for a human to notice a problem, investigate, and manually trigger a rollback takes too long. By the time a human acts, hundreds or thousands of users may have had a bad experience.

**Automatic rollback** means your system detects regressions and reverts to the previous version without human intervention. You define rollback triggers in advance. If any trigger fires, the canary is immediately stopped and traffic is routed back to the stable production version.

**Rollback triggers:**

**Error rate threshold** — if canary error rate exceeds X percent, roll back. For example, if error rate goes above two percent for more than five minutes, roll back. This catches crashes, timeouts, and model refusals.

**Latency threshold** — if canary P95 latency exceeds Y milliseconds, roll back. For example, if P95 goes above three seconds for more than five minutes, roll back. This catches performance regressions.

**Quality score threshold** — if canary average quality score drops below Z percent of production average, roll back. For example, if canary quality is more than five percent lower than production for more than ten minutes, roll back.

**User satisfaction threshold** — if canary thumbs-down rate exceeds W percent, roll back. For example, if thumbs-down rate goes above five percent for more than fifteen minutes, roll back.

**Cost threshold** — if canary cost per request exceeds budget limits, roll back. This prevents accidentally shipping a model that will blow your budget.

**Trigger evaluation frequency:** Check rollback triggers every one to five minutes. Do not react to single bad requests. Use a sliding window: if the metric is bad for N consecutive minutes, then roll back. This avoids false positives from random noise.

**Rollback speed:** Once a trigger fires, rollback should happen in seconds, not minutes. Traffic routing systems can switch from canary to production almost instantaneously. The canary version can be kept running in shadow mode for debugging even after rollback.

**Rollback alerts:** When automatic rollback happens, alert the team immediately. Someone needs to investigate why the canary failed. But the rollback does not wait for human investigation. It happens first, investigation comes second.

**Example:** Your canary deployment has been running for six hours at five percent traffic. Suddenly, error rate spikes from 0.5 percent to four percent. Your automatic rollback system detects this: error rate has been above two percent for five consecutive minutes. Rollback is triggered. Traffic is routed back to production. Total time from regression start to rollback: five minutes. Number of affected users: approximately five percent of five percent of six minutes of traffic, which is a tiny fraction of your user base. Without automatic rollback, this regression might have gone unnoticed for an hour or longer, affecting far more users.

Automatic rollback is essential for canary deployments at scale. You cannot expect humans to watch dashboards 24/7.

---

## Multi-Model Canary: Testing Model Upgrades Safely

The most common use case for canary deployment in AI systems is testing a new underlying model. OpenAI releases GPT-5. Anthropic releases Claude Opus 5. You want to upgrade. How do you know the new model is actually better for your use case?

**Multi-model canary** means running a canary deployment where the only difference is the underlying LLM. Your prompts, your architecture, your evaluation logic — all identical. Only the model changes.

This is the safest way to evaluate model upgrades. Vendor benchmarks tell you the average improvement across many tasks. But your task is not average. The new model might be better at reasoning but worse at following instructions. Better at English but worse at code. Better at creative writing but worse at structured output.

You cannot know without testing on your actual production traffic. Multi-model canary gives you real data.

**Step one:** Set up shadow deployment with the new model. Run both models on the same production traffic. Compare outputs. Does the new model improve quality on your metrics? Does it have different failure modes?

**Step two:** If shadow looks good, promote to canary at one percent. Monitor user satisfaction. Do users prefer the new model's outputs? Are they more or less engaged?

**Step three:** Gradually ramp if metrics are positive. If the new model is clearly better, you can accelerate the ramp. If metrics are mixed — better on some dimensions, worse on others — you need to decide if the tradeoff is worth it.

**Step four:** If canary fails, you have valuable information. The new model is not better for your use case despite vendor claims. Stay on the current model or investigate why the new model underperforms.

**Example:** A legal document analysis company wants to upgrade from GPT-4 to GPT-5.1. They run shadow mode for one week. GPT-5.1 produces longer, more detailed answers. Quality score is slightly higher. But latency is fifty percent worse and cost is double. They run canary at one percent. User satisfaction is the same. Users do not value the extra detail enough to offset the slower responses. They decide not to upgrade. The canary test saved them from a costly mistake.

Multi-model canary also applies to testing cheaper models. Can you replace GPT-4 with GPT-4-mini for some queries and save money? Run a canary test. If quality remains acceptable, the cost savings are real.

---

## 2026 Patterns: Tools and Platforms for Canary and Shadow

**Feature flag platforms** have evolved to support AI-specific use cases. LaunchDarkly and Split now offer LLM routing flags. You can define rules like "route ten percent of traffic to model B, ninety percent to model A, but route all internal users to model B." These platforms integrate with monitoring systems to provide automatic rollback when metrics degrade.

**Automated canary analysis** tools, inspired by Netflix's Kayenta, have been adapted for LLM evaluation. These systems automatically compare canary and production metrics using statistical tests. They tell you not just whether canary is different, but whether the difference is statistically significant and practically meaningful. They reduce the manual analysis burden.

**LangSmith and LangFuse** both support shadow mode natively. You can configure your LangChain application to log both production and shadow outputs for the same requests. The platform calculates comparison metrics and provides a dashboard for reviewing divergent cases.

**Progressive delivery for LLM applications** means combining feature flags, canary deployments, and monitoring into a unified workflow. You push a new model version. The system automatically starts shadow mode, waits for sufficient data, promotes to canary, monitors metrics, and either promotes or rolls back based on predefined policies. Human intervention is only required for final approval or when automatic rollback occurs.

**Model registries** like MLflow now support canary metadata. You can tag a model version as "shadow," "canary-1-percent," "canary-5-percent," or "production." Your serving infrastructure reads these tags and routes traffic accordingly. This makes canary deployment a first-class part of your model lifecycle management.

**Observability platforms** like Datadog and Honeycomb have added LLM-specific canary dashboards. These dashboards show side-by-side comparisons of production and canary metrics with automatic significance testing. They highlight when canary metrics deviate from production baselines.

The tooling has matured significantly. In 2023, teams built custom canary systems. In 2026, you can use off-the-shelf platforms and integrate them into your CI/CD pipeline with minimal custom code.

---

## Failure Modes and Blind Spots

**Insufficient sample size:** You run canary at one percent for two hours, see no problems, and promote to one hundred percent. But two hours of one percent traffic is only a few dozen samples. You do not have statistical power to detect regressions. Then the full rollout reveals a five percent drop in quality that was hidden by noise in the small canary sample. Always wait for sufficient volume before promoting.

**Selection bias in canary routing:** You route canary to internal users or beta testers who are more forgiving than your average user. Canary metrics look great. You promote to full production and discover that regular users hate the new model. Canary routing should be random or representative, not biased toward friendly users.

**Metric mismatch:** You monitor automated quality scores and error rates in canary, but you do not monitor user satisfaction. Automated metrics look fine. You promote to production and user satisfaction drops. The automated metrics did not capture what users care about. Always include user satisfaction signals in canary monitoring.

**Regression in rare cases:** Your canary runs for twenty-four hours and processes one thousand requests. Everything looks good. You promote to full production. Then you discover that the new model fails catastrophically on a specific query type that only happens once per thousand requests. Your canary sample size was too small to see this rare failure. Consider longer canary periods or stratified sampling to ensure rare cases are tested.

**Delayed regressions:** The canary looks good for the first twenty-four hours, then metrics start to degrade on day two. Maybe the model has a subtle context-handling bug that only manifests in multi-session conversations. Or maybe the traffic pattern changes and the model struggles with a different query distribution. Do not assume that early success means permanent success. Monitor canary for multiple days and across different traffic patterns.

**Cost estimation errors:** Your canary is one percent of traffic. Cost per request is double what production costs. You calculate the total cost increase and decide it is acceptable. But you forgot to account for traffic growth. By the time you promote to full production, your traffic has doubled, and the cost increase is now unaffordable. Always project cost increases against future traffic, not current traffic.

---

## Enterprise Expectations: Governance and Compliance

Large enterprises have strict change control processes. You cannot just ship a new model to production and see what happens. Canary and shadow deployments must fit into existing governance frameworks.

**Change approval:** Even shadow deployments may require approval because they use production data. Document what data the shadow model will see, how it will be evaluated, and when it will be deleted. Get sign-off from compliance and legal teams.

**Audit trails:** Every canary deployment decision must be logged. Who approved the canary? What metrics were monitored? When was the canary promoted or rolled back? Why? This audit trail is necessary for regulatory compliance and post-incident reviews.

**Rollback authority:** Define in advance who has authority to trigger manual rollback. In some organizations, only senior engineers or managers can stop a canary. In others, anyone on-call can trigger rollback if they see a problem. Automatic rollback reduces the need for human authority but does not eliminate it.

**Customer opt-out:** Some customers may not want to be part of canary testing, especially in regulated industries like finance or healthcare. Your canary routing system must respect customer preferences. Flag accounts that should always use the stable production version, never canary.

**Notification policies:** Do you notify users that they are in a canary group? For most consumer applications, no. For enterprise B2B applications, maybe. Some customers want to know when you are testing new features on them. Define notification policies in advance and honor customer agreements.

**Data residency:** If your shadow or canary model runs in a different cloud region than production, you might violate data residency requirements. Ensure that shadow and canary deployments respect the same data locality rules as production.

These governance requirements add friction but they are necessary for enterprise trust. Build them into your canary process from the start, not as an afterthought.

---

## Practical Template: Canary Deployment Checklist

**Pre-deployment**

- [ ] Shadow deployment complete, metrics reviewed, regressions resolved
- [ ] Canary promotion policy defined: traffic percentages, duration at each stage, quality gates
- [ ] Automatic rollback triggers defined: error rate, latency, quality, satisfaction thresholds
- [ ] Monitoring dashboard configured: production vs canary side-by-side metrics
- [ ] Rollback procedure tested: can we revert traffic in under one minute?
- [ ] Team alerted: everyone knows canary is starting, who is on-call for issues

**Canary at 1%**

- [ ] Traffic routing verified: exactly one percent of requests go to canary
- [ ] Metrics flowing: dashboard shows canary data in real time
- [ ] Error rate within threshold: canary errors not significantly higher than production
- [ ] Latency within threshold: P95 latency not significantly higher than production
- [ ] Quality within threshold: automated quality scores within acceptable range
- [ ] Duration gate met: canary has run for at least twenty-four hours
- [ ] Volume gate met: canary has processed at least five hundred requests

**Promote to 5%**

- [ ] All 1% gates passed
- [ ] No rollback triggers fired during 1% stage
- [ ] Sample divergent cases reviewed: no unexpected failure patterns

**Promote to 25%**

- [ ] All 5% gates passed
- [ ] User satisfaction metrics equivalent to or better than production
- [ ] Cost increase confirmed within budget

**Promote to 100%**

- [ ] All 25% gates passed
- [ ] Final manual approval from team lead or product owner
- [ ] Rollout complete: canary becomes production, old version retired

**Post-deployment**

- [ ] Monitor production metrics for next seventy-two hours: ensure no delayed regressions
- [ ] Document results: what worked, what did not, lessons learned
- [ ] Update runbooks: new production version documented

---
