# Chapter 2.1 — Task-Specific Definitions of "Good" (Chat / RAG / Agents / Voice)

When teams say "the model is good," they usually mean different things depending on the product.

In enterprise evals, we don't start with metrics. We start with a clear definition of "good" for each task type. That becomes your "contract" for building rubrics, datasets, automated tests, and release gates.

You'll see this idea across modern eval tooling too: curated datasets + task-specific evaluators + regression checks (OpenAI Evals, LangSmith evals, RAGAS metrics, etc.).

---

### The Core Idea: "Good" = Outcome + Constraints

For any AI feature, define "good" like this:

**Outcome:** what the user must get

**Constraints:** what must NOT happen (safety, privacy, policy, cost, latency, tone)

**Evidence:** what we can check in an eval (logs, citations, tool calls, transcripts, human scoring)

If you only define outcomes, you'll ship something that "sounds right" but breaks in production.

---

### A) "Good" for Chat (General Assistant / Support Bot)

#### Mechanics (How It Works)

Chat is mostly: interpret intent → decide what info is needed → respond clearly → stay safe.

#### What "Good" Means (Task Definition)

A "good" chat answer is:

- Directly answers the user's question
- Correct enough for the use case
- Easy to understand
- Honest about uncertainty
- Safe + policy-aligned
- Doesn't waste time (no fluff when the user wants speed)

#### Knobs & Defaults (What You Set)

- **Answer style:** short vs detailed, formal vs friendly
- **Ask-clarify behavior:** when to ask questions vs make best-effort assumptions
- **Uncertainty behavior:** when to say "I'm not sure" vs "Here's the best estimate"
- **Safety behavior:** when to refuse, when to redirect, when to give safer alternatives
- **Latency/cost targets:** "fast enough" matters in enterprise support

#### Failure Modes (Symptoms → Root Causes)

- Sounds confident but wrong → no grounding, poor knowledge boundaries, missing verification
- Over-explains → no "brevity" dimension in the definition of good
- Asks too many questions → "clarify-first" default too aggressive
- Too agreeable (sycophant behavior) → reward signals favor "pleasantness" over correctness/safety (this has been a real production lesson in the industry)

#### Debug Playbook

- Collect 10–30 real conversations per top user intent
- For each: label intent, what a great answer would contain, what would be unacceptable
- Add "hard cases": angry user, vague question, unsafe request, conflicting constraints

#### Enterprise Expectations

- A supportable definition: different reviewers score similarly
- A measurable definition: ties to ticket resolution, CSAT, escalations, handle time
- A regression-proof definition: you can say "this got worse" before customers do

#### Interview-Ready Talking Points

"Chat quality isn't one thing. I define 'good' per intent: accuracy, clarity, actionability, safety, and efficiency — then I build rubrics and datasets per intent."

---

### B) "Good" for RAG (Retrieval-Augmented Generation)

RAG is two systems glued together: retrieval + generation. "Good" must cover both.

#### Mechanics

User asks → system retrieves sources → model answers using sources → (optionally) cites them.

#### What "Good" Means (Task Definition)

A "good" RAG answer is:

- Relevant to the question (answers the right thing)
- Grounded in retrieved context (no invented facts)
- Uses the best parts of the sources (not random snippets)
- Admits when sources don't support an answer
- Cites/quotes appropriately when required
- Keeps scope tight (doesn't "freestyle" outside the documents)

This is why RAG evals commonly split into dimensions like answer relevancy and faithfulness/grounding.

#### Knobs & Defaults

**Retriever knobs:**
- chunk size, overlap
- top-k
- filters (time, permissions, tenant/org)
- reranking on/off
- query rewriting on/off

**Generator knobs:**
- "use only provided context"
- refusal/abstain behavior when context is missing
- citation format requirement
- verbosity level

#### Failure Modes

- Hallucination with high confidence → generator not constrained, weak grounding checks
- Right docs retrieved, wrong answer → model didn't use the evidence (prompt/format issue)
- Wrong docs retrieved, answer looks plausible → retrieval failure masked by fluent text
- Good for easy queries, fails on long-tail → coverage gap in dataset + taxonomy

#### Debug Playbook

- Check retrieval first (did we fetch the right evidence?)
- If retrieval is good, check instruction/prompt (does it force evidence use?)
- Add eval cases where the correct behavior is "I can't answer from these sources."
- Track: "how often do we answer anyway when we shouldn't?"

#### Enterprise Expectations

- Permission-safe retrieval (no data leaks across tenants)
- Auditability: "show me why the bot said that"
- Repeatability: same question should not randomly flip answers
- Clear "abstain" rules for compliance-heavy domains (health, finance, legal)

#### Interview-Ready Talking Points

"In RAG, 'good' must be defined across retrieval and generation. I score relevancy + grounding separately so I can tell whether regressions come from the retriever or the model."

---

### C) "Good" for Agents (Tool-Using, Multi-Step Workflows)

Agents are not just "chat." They are decision systems that take actions.

#### Mechanics

Interpret goal → plan → call tools (API/DB/browser) → handle errors → update state → finish.

#### What "Good" Means (Task Definition)

A "good" agent:

- Completes the user's goal (not just "talks about it")
- Uses tools correctly (right tool, right params, right sequence)
- Is safe with actions (no risky operations without confirmation)
- Is reliable under failure (timeouts, bad data, tool errors)
- Keeps clean state (doesn't lose context or repeat actions)
- Leaves an audit trail (what it did, why, results)

#### Knobs & Defaults

- Allowed tools + permissions
- Confirmation rules ("always ask before sending email / charging card / deleting")
- Retry rules (how many retries, backoff)
- Time budget (max steps, max tool calls)
- Memory rules (what can be stored, retention time)
- Escalation rules (when to hand off to a human)

#### Failure Modes

- Tool thrashing (calls tools in loops) → no stop conditions, poor planning
- Silent partial success → missing "done criteria"
- Repeated actions (duplicate charges/messages) → no idempotency keys, weak state
- Unsafe actions → missing confirmations and role-based permissions

#### Debug Playbook

- Require structured traces: plan → actions → results → final outcome
- Add "chaos tests": tool timeout, malformed API response, empty search results
- Add "must-confirm" tests: agent tries a risky action and should stop

#### Enterprise Expectations

- Permissioning, audit logs, and least-privilege by default
- "Break glass" controls and human approval for high-risk actions
- SLA thinking: success rate, time-to-complete, cost-per-task

#### Interview-Ready Talking Points

"For agents, 'good' is action correctness + safety + recoverability. I evaluate not only the final answer, but the tool calls, state handling, and confirmation behavior."

---

### D) "Good" for Voice AI (Calls, Real-Time Assistants)

Voice adds constraints that text doesn't: time, turn-taking, audio quality, and human comfort.

#### Mechanics

Audio in → speech-to-text → reasoning + policy → text-to-speech → audio out (fast).

#### What "Good" Means (Task Definition)

A "good" voice agent:

- Understands the caller (even with accents, noise, interruptions)
- Responds fast enough to feel natural
- Handles barge-in (caller interrupts) without breaking
- Stays calm and clear (tone matters more in voice)
- Never leaks private info (voice is high-risk for identity data)
- Escalates gracefully when unsure (handoff to human or fallback)

#### Knobs & Defaults

- Latency targets (end-to-end)
- Barge-in on/off, interruption policy
- "Ask to repeat" thresholds
- Voice style (speed, warmth, formality)
- Verification rules (identity checks before account actions)

#### Failure Modes

- Feels slow → pipeline latency, long prompts, too many tool calls
- Talks over the user → barge-in handling is wrong
- Mishears names/numbers → ASR weaknesses, no confirmation loop for critical fields
- Over-confident on low-quality audio → missing "confidence gating"

#### Debug Playbook

- Test with: noisy audio, speakerphone, fast talkers, interruptions, accents
- Add "critical fields" checklist: phone numbers, addresses, dates must be confirmed
- Keep transcripts + timestamps for every turn (so you can find where it went wrong)

#### Enterprise Expectations

- Compliance: call recording rules, consent, PII redaction, retention controls
- Monitoring: abandonment rate, transfer rate, time-to-resolution
- Clear escalation paths and safe "can't help with that" scripts

#### Interview-Ready Talking Points

"Voice quality is mostly latency + turn-taking + correct capture of critical info. I define 'good' with comfort and safety constraints, not only 'correct answers.'"

---

### A Practical Template You Can Reuse (Definition of "Good" Card)

Copy this into your notes for each feature:

- **Feature / Task:**
- **Primary user goal:**
- **User context (who / where / why):**
- **What a great output must include (3–7 bullets):**
- **What is unacceptable (3–7 bullets):**
- **Edge cases we must handle:**
- **When we must refuse / escalate:**
- **Evidence we log for audits (traces, citations, transcripts):**
- **Success signals (business):**
- **Failure signals (business):**
