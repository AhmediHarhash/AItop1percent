# 2.1 â€” Task-Specific Definitions of "Good" (Chat / RAG / Agents / Voice)

In late 2024, a healthcare technology company deployed a patient support chatbot to handle routine questions about insurance coverage and appointment scheduling. The product team celebrated when early metrics looked strong. Average response time was under three seconds. User engagement was high, with most conversations lasting multiple turns. The bot sounded helpful and friendly. But within six weeks, the Trust and Safety team flagged a pattern that made the entire deployment unviable. The bot was giving insurance coverage estimates that contradicted official policy documentation in roughly twelve percent of cases. Patients were booking appointments based on incorrect information about what their plans would cover. The company pulled the feature and spent four months rebuilding it.

The root cause was not a technical failure. The model worked exactly as designed. The problem was definitional. The team had optimized for engagement and tone without defining what "correct" meant for insurance questions. They had not distinguished between general health questions, where a friendly approximation was acceptable, and coverage questions, where precision was mandatory. They had built a single rubric that treated all outputs the same. When the rebuild launched, it used three separate definitions of "good" for three question types, each with explicit constraints around grounding, refusal behavior, and citation requirements. Coverage questions were held to a much stricter standard than appointment reminders. This is the lesson that separates production-ready evaluation from wishful thinking.

## The Core Contract: Outcome Plus Constraints

When you define "good" for an AI feature, you are writing a contract between your product and your users. That contract has two parts. The first part is the **outcome** the user must receive. The second part is the **constraints** that must hold even when delivering that outcome. Most teams define only the outcome. They say "the bot should answer questions accurately" or "the assistant should help users complete tasks." These are not definitions. They are aspirations. They give you no basis for building a rubric, no basis for choosing test cases, and no basis for deciding whether a release is safe.

A real definition specifies what the user must get and what must never happen. For a customer support bot, the outcome might be "resolve the user's issue without escalation." The constraints might include "never promise a refund without manager approval," "never ask for payment card details," and "never claim certainty about delivery dates when the order is still in processing." For a RAG system answering questions from internal documentation, the outcome might be "provide the relevant policy or procedure." The constraints might include "cite the source document," "refuse to answer if the documents do not contain the answer," and "never invent details not present in the sources."

The distinction matters because constraints define your failure modes. You can deliver the outcome and still fail the task if you violate a constraint. A support bot that resolves the issue by promising an unauthorized refund has failed. A RAG system that answers the question by inventing facts has failed. You cannot evaluate these failures unless you have defined the constraints in advance. You cannot build automated checks unless the constraints are explicit. You cannot communicate expectations to reviewers unless the contract is written down.

This is why evaluation starts with task definition, not with metrics. Metrics measure adherence to a definition. If the definition is vague, the metrics will be vague. If the definition omits constraints, the metrics will reward systems that violate those constraints. You must define "good" before you can measure it.

## What Makes a Task Definition Usable

A usable definition has three components. First, it describes the primary outcome in terms a domain expert can verify. Second, it lists the constraints that must hold, organized by category: safety, privacy, policy, cost, latency, tone, and scope. Third, it specifies what evidence you can collect to verify both the outcome and the constraints. This third component is what makes the definition testable.

For a chat assistant, the evidence might include the conversation transcript, the user's rating, and metadata about whether the bot asked clarifying questions or escalated to a human. For a RAG system, the evidence might include the retrieved documents, the citations in the response, and a log of whether the system refused to answer when the documents were insufficient. For an agent, the evidence might include the sequence of tool calls, the parameters passed to each tool, and the final state of the task. For a voice assistant, the evidence might include the audio transcript, timestamps for each turn, and confidence scores from the speech recognition system.

If you cannot specify what evidence you will collect, you cannot evaluate the system. If the evidence is not logged by default, you cannot investigate failures after the fact. If the evidence is logged but not structured, you cannot automate checks at scale. The task definition must include the instrumentation plan. This is not optional. It is part of the definition.

## Chat: The Baseline Task Type

Chat is the simplest task type to define because the interaction is stateless and the outcome is immediate. The user sends a message. The system interprets the intent, decides what information is needed, and responds. The response should answer the question, match the user's expectations for style and detail, and avoid unsafe or policy-violating content. This is the baseline contract for most conversational AI.

The outcome is straightforward. A good chat response answers the user's question correctly. It is easy to understand. It does not waste time with unnecessary details unless the user has asked for depth. It admits uncertainty when the system does not know the answer. It refuses inappropriate requests and redirects the user to safer alternatives when needed. These are not controversial requirements. Every team agrees on them in principle. The difficulty is making them concrete enough to score.

Correctness is the first dimension most teams underspecify. They say "the answer should be correct" without defining correct relative to what. Correct relative to the model's training data is not good enough. Training data contains errors, outdated information, and gaps. Correct relative to the user's expectations is closer, but expectations vary. The right baseline is correct relative to the use case. For a customer support bot, correctness means the answer matches the official support documentation and current product behavior. For a general assistant, correctness means the answer is factually accurate and does not contradict widely accepted sources. For a domain-specific assistant, correctness means the answer aligns with domain expertise and does not give advice outside the system's intended scope.

Clarity is the second dimension teams often neglect. They assume that if the model produces fluent text, the response is clear. Fluency and clarity are not the same. Fluency means the text reads smoothly. Clarity means the user can act on the information without confusion. A response can be fluent and still fail on clarity if it buries the key point in surrounding context, uses jargon the user does not understand, or provides multiple options without helping the user choose. Clarity is a function of structure, word choice, and relevance to the user's immediate need.

Safety is the third dimension and the one that requires the hardest gates. A chat response that answers the question correctly and clearly can still fail if it provides unsafe instructions, leaks private information, or bypasses policy guardrails. Safety is not a score on a scale. It is a binary check. You define the categories of unsafe content for your use case, and you refuse or redirect when the user's request falls into those categories. For customer support, unsafe content might include giving medical or legal advice, promising actions the company cannot deliver, or asking for sensitive information over an insecure channel. For a general assistant, unsafe content might include instructions for illegal activity, self-harm guidance, or generation of deceptive content.

The knobs you set for chat tasks include answer style, clarification behavior, uncertainty handling, and refusal behavior. Answer style determines whether the system defaults to short, direct responses or longer, more detailed explanations. Clarification behavior determines whether the system asks follow-up questions when the user's intent is ambiguous or makes a best-effort guess. Uncertainty handling determines when the system says "I don't know" versus "here's the best estimate I can give you." Refusal behavior determines when the system declines to answer versus when it provides a safer alternative.

Failure modes for chat systems cluster around a few common patterns. The system sounds confident but gives wrong answers because it has no mechanism for grounding its responses in verified sources. The system over-explains because the rubric rewards length and friendliness over brevity and directness. The system asks too many clarifying questions because the default is to seek perfect information before answering. The system is overly agreeable, prioritizing user satisfaction over correctness or safety. This last failure mode has been a recurring lesson across the industry. Models fine-tuned to maximize user ratings tend to avoid disagreement even when disagreement is the correct response. You cannot fix this without explicitly defining when disagreement is required.

## RAG: The Grounded Task Type

RAG systems introduce a second component to the task: retrieval. The outcome is still "answer the user's question," but now the answer must be grounded in retrieved sources. This changes the definition of "good" in two ways. First, correctness is no longer measured against general knowledge. It is measured against the specific documents the system retrieved. Second, you must define what happens when the retrieved documents do not contain the answer. The system must refuse to answer rather than invent information.

A good RAG response is relevant, grounded, and appropriately scoped. Relevance means the response answers the user's question using the best parts of the retrieved sources. Grounding means the response does not include claims unsupported by those sources. Scope means the response does not drift into topics outside the retrieved context. If the user asks about the company's return policy and the retrieved documents describe the policy for physical goods but not digital goods, the system should answer for physical goods and note that it cannot speak to digital goods. It should not invent a digital goods policy.

The constraints for RAG are stricter than for general chat because the system is explicitly positioned as a source-backed answering service. Users expect higher accuracy. Compliance and audit requirements often mandate that answers can be traced back to authoritative documents. If the system cannot cite its sources, it cannot be used in regulated environments. If the system mixes grounded and invented content, users cannot trust it. The failure mode is not just "wrong answer." It is "plausible-sounding answer that cannot be verified."

This is why RAG evaluations commonly split into two dimensions: answer relevancy and faithfulness. Relevancy measures whether the response addresses the user's question. Faithfulness measures whether the response is supported by the retrieved context. You need both. A response can be relevant but unfaithful if it answers the question by making up facts. A response can be faithful but irrelevant if it quotes the sources accurately but does not address what the user asked. The best RAG systems score high on both dimensions. The worst score high on relevancy and low on faithfulness, because they sound right but are not.

The knobs you set for RAG include retrieval parameters and generation constraints. Retrieval parameters control chunk size, overlap, top-k results, filters for permissions or time range, reranking, and query rewriting. Generation constraints control whether the model is instructed to use only the provided context, how it should behave when the context is insufficient, whether citations are required, and what format those citations should take. These knobs interact. If you retrieve too few documents, the model may lack the information it needs. If you retrieve too many, the model may struggle to identify the relevant parts. If you do not instruct the model to refuse when the context is missing, it will invent answers.

Failure modes for RAG cluster around retrieval failures and generation failures. Retrieval failures occur when the system fetches the wrong documents or fails to fetch the right ones. Generation failures occur when the system has the right documents but does not use them correctly. The most dangerous failure mode is the one that masks retrieval problems with fluent generation. The system retrieves irrelevant documents, but the model generates a plausible answer anyway, and the user has no way to know the answer is not grounded. This is why citation requirements are not optional in production RAG. They make the grounding explicit.

## Agents: The Action-Oriented Task Type

Agents are not conversational systems that talk about tasks. They are decision systems that complete tasks by taking actions in the world. The outcome is task completion. The constraints include action safety, state correctness, and recoverability. An agent that completes the task by taking an unsafe action has failed. An agent that loses track of state and repeats actions has failed. An agent that cannot recover from tool errors has failed. The bar is higher because the consequences of failure are higher.

A good agent completes the user's goal using the correct sequence of tool calls with the correct parameters. It confirms high-risk actions before executing them. It handles errors gracefully, retrying when appropriate and escalating when necessary. It maintains clean state and does not repeat actions unnecessarily. It logs enough information that you can audit what it did and why. These requirements are not aspirational. They are the minimum standard for deploying an agent that interacts with real systems.

The outcome is binary in most cases. Either the agent completed the task or it did not. Partial completion is a failure unless the agent explicitly communicated that the task was only partially completable and sought user input on how to proceed. If the user asked the agent to book a flight and the agent found flights but did not complete the booking, that is a failure. If the agent completed the booking but used the wrong dates, that is a failure. If the agent completed the booking correctly but also sent three duplicate confirmation emails because it did not track state properly, that is a failure. Agents have less room for error than chat systems because their errors have side effects.

The constraints focus on safety and reliability. Safety constraints include requiring confirmation before actions that spend money, delete data, or share information outside the organization. Reliability constraints include retry logic for transient errors, idempotency keys for actions that should not be repeated, and time budgets to prevent infinite loops. You must define these constraints before you deploy the agent. You cannot patch them in after users report problems. The problems will have already caused harm.

Failure modes for agents include tool thrashing, silent partial success, repeated actions, and unsafe execution. Tool thrashing occurs when the agent calls tools in loops without making progress, usually because it has no clear stopping condition or because it is retrying failed actions without changing its approach. Silent partial success occurs when the agent completes part of the task but does not recognize that the full task is incomplete. Repeated actions occur when the agent lacks state tracking or idempotency controls. Unsafe execution occurs when the agent bypasses confirmation requirements or operates outside its permission scope.

The evidence you log for agents must include the full execution trace: the user's goal, the plan the agent generated, the sequence of tool calls with parameters and results, any errors encountered, and the final outcome. Without this trace, you cannot debug failures. Without structured logging, you cannot automate analysis. The trace is the core artifact for agent evaluation. It is also the core artifact for auditing and compliance. If you cannot show what the agent did, you cannot use it in regulated environments.

## Voice: The Real-Time Task Type

Voice systems add constraints that text-based systems do not face. Latency must be low enough that the interaction feels natural. The system must handle interruptions and overlapping speech. It must accurately capture critical information like names, numbers, and addresses even when audio quality is poor. It must maintain appropriate tone and pacing. It must escalate gracefully when it cannot help. These constraints are not secondary considerations. They define whether the system is usable.

A good voice assistant understands the caller, responds quickly enough to feel conversational, handles barge-in without breaking, stays calm and clear, never leaks private information, and escalates when uncertain. Understanding accuracy depends on the quality of the speech-to-text system and the robustness of the intent recognition. Response speed depends on the latency of the entire pipeline: transcription, reasoning, generation, and text-to-speech. Barge-in handling depends on the system's ability to detect interruptions and gracefully stop speaking. Tone depends on the voice synthesis and the phrasing of responses. Privacy depends on verification flows and redaction of sensitive fields in logs.

The failure modes for voice are distinct from text-based systems. The system feels slow because the pipeline has too many steps or the prompts are too long. The system talks over the user because it does not detect interruptions quickly enough. The system mishears critical information like phone numbers or account identifiers because the ASR is weak or because the system does not confirm high-stakes fields. The system sounds overconfident even when the audio quality is poor because it does not gate responses on confidence scores.

The knobs you set for voice include latency targets, barge-in policies, repetition thresholds, voice style, and verification rules. Latency targets define the acceptable end-to-end response time. Barge-in policies define whether the system allows interruptions and how it responds when interrupted. Repetition thresholds define when the system asks the user to repeat themselves versus when it makes a best guess. Voice style defines the speed, warmth, and formality of the synthesized speech. Verification rules define when the system must confirm critical information before proceeding.

The evidence you log for voice must include transcripts with timestamps for every turn, confidence scores from the ASR, the sequence of intents recognized, and any verification or escalation events. You cannot debug voice failures without this data. You cannot evaluate turn-taking quality without timestamps. You cannot assess understanding accuracy without confidence scores. The logging requirements are more complex than for text-based systems because the input modality introduces additional failure modes.

## The Cross-Task Pattern

Across all task types, the pattern is the same. Define the outcome the user must receive. Define the constraints that must hold. Define the evidence you will collect to verify both. Make the definition concrete enough that different reviewers can apply it consistently. Make it specific enough that you can build automated checks. Make it strict enough that violating a constraint is a failure regardless of outcome quality.

When you define "good" this way, you create the foundation for everything that follows. Your rubrics score adherence to the definition. Your datasets test the edge cases the definition anticipates. Your automated evaluators check the constraints the definition specifies. Your release gates enforce the thresholds the definition requires. Without the definition, none of these artifacts have meaning. With the definition, they become a coherent system.

## Writing Your Task Definition: The Template

The process of writing a task definition follows a structured template that forces you to be concrete. You start with the feature name and the primary user goal. You describe who the users are, where they are using the system, and why they need this capability. This context matters because it determines what counts as success. A chat system for emergency services has different success criteria than a chat system for shopping recommendations. The user context defines the stakes.

Next, you list what a great output must include. This is not a wish list. It is a minimum bar. For a customer support bot, a great output must resolve the user's question, cite the relevant policy or documentation, use language the user understands, maintain appropriate tone, and complete within the target response time. For a RAG system in a legal environment, a great output must answer the question, cite specific documents with section references, refuse to answer when sources are insufficient, and avoid any claim not directly supported by those sources. For an agent booking travel, a great output must complete the booking with correct details, confirm high-cost actions, handle errors without losing state, and log the full interaction for audit purposes.

Then you list what is unacceptable. This is your constraint list. For the support bot, unacceptable outcomes include promising refunds without authorization, asking for payment information, giving medical or legal advice, leaking customer data in logs, and claiming certainty when the answer depends on case-specific details. For the RAG system, unacceptable outcomes include inventing citations, mixing grounded and ungrounded claims, providing legal advice when the role is limited to information retrieval, and failing to flag ambiguous queries. For the travel agent, unacceptable outcomes include booking without confirmation, retrying failed payments without user approval, storing unencrypted payment details, and proceeding when critical fields like departure date are ambiguous.

Edge cases come next. These are the scenarios that break most definitions. For chat systems, edge cases include adversarial users trying to extract training data, users asking the same question in different phrasings to test consistency, users combining multiple intents in one message, and users requesting actions outside the bot's scope. For RAG systems, edge cases include queries where the answer spans multiple documents, queries where documents contradict each other, queries phrased in ways the retrieval system does not handle well, and queries where the correct answer is "the policy has changed, these documents are outdated." For agents, edge cases include tool timeouts during multi-step workflows, partial failures where some actions succeed and others fail, concurrent requests that might create race conditions, and scenarios where the user changes their goal midway through execution.

Refusal and escalation rules define when the system must stop. For chat, refusal rules might include "refuse all requests for medical diagnoses," "refuse requests to generate content that impersonates real individuals," and "refuse requests that attempt to override safety filters." Escalation rules might include "escalate to human support when the user expresses frustration three times," "escalate when the question involves a complaint about a charge over one thousand dollars," and "escalate when the answer requires access to data the system cannot retrieve." For RAG, refusal rules might include "refuse when no retrieved documents are relevant," "refuse when retrieved documents contradict each other without clear authority," and "refuse when the question asks for predictions about future policy changes." For agents, refusal rules might include "refuse to execute any action that deletes user data without explicit confirmation using the exact resource name," "refuse to send messages to external parties without showing the user the exact content first," and "refuse to proceed when any required field is ambiguous."

The evidence section specifies what you log. For chat, you log the full conversation history, the intent classification for each turn, any clarification questions asked, the sources or knowledge base articles referenced, the response time for each turn, and any escalation or refusal events. For RAG, you log the query, the retrieved document IDs with relevance scores, the chunks actually used in the response, the citations generated, whether the system refused to answer, and the grounding score if you compute one. For agents, you log the user's goal as stated, the plan generated, each tool call with parameters and return values, any retries or error handling, the final state, and a success or failure flag with reason. For voice, you log the audio transcript with timestamps, confidence scores from ASR, the intent at each turn, any barge-in events, verification steps taken, and escalation events.

Business success signals tie the definition back to product goals. For a support bot, success signals include ticket resolution rate, average handle time, customer satisfaction scores, and reduction in escalations to human agents. For a RAG system, success signals include answer acceptance rate, citation click-through rate, time saved versus manual document search, and reduction in follow-up questions. For an agent, success signals include task completion rate, time to completion, cost per task, and error rate requiring human intervention. For a voice assistant, success signals include call completion rate, average call duration, transfer rate to human agents, and customer satisfaction specific to voice interactions.

Business failure signals define what happens when the system does not meet the definition. For the support bot, failure signals include increased escalation rate, decreased CSAT scores, increased average handle time, and increased repeat contact rate. For the RAG system, failure signals include decreased answer acceptance rate, increased time spent verifying answers, increased rate of users bypassing the system to contact experts directly, and compliance violations flagged in audits. For the agent, failure signals include decreased task completion rate, increased cost per task due to retries, increased error rate, and safety incidents requiring remediation. For the voice assistant, failure signals include increased abandon rate, increased transfer rate, decreased call resolution rate, and increased complaints about misunderstandings or poor experience.

This template forces you to think through every dimension of the task before you build anything. It prevents the pattern where teams build first, define success later, and discover six months in that they have been optimizing for the wrong thing. The definition is the contract. Everything else is implementation.

## Common Mistakes in Task Definition

The most common mistake is defining "good" in terms of model behavior rather than user outcomes. Teams say "the model should be helpful and harmless" instead of "the system should resolve the user's issue without violating these specific policies." Model behavior is an implementation detail. User outcomes are the contract. You can swap models, change prompts, or rewrite the entire system architecture as long as you continue to deliver the outcomes and respect the constraints.

The second most common mistake is omitting constraints. Teams define the positive outcome but not the negative boundaries. They say "answer user questions" without specifying "but never provide medical diagnoses, never promise refunds without authorization, never leak PII, and never claim certainty about case-specific outcomes." The omission creates a system that optimizes for user satisfaction without regard for safety, policy, or correctness. When failures happen, the team has no basis for calling them failures because the constraints were never defined.

The third most common mistake is defining success in unmeasurable terms. Teams say "provide a delightful experience" or "be helpful and friendly" without specifying what evidence would demonstrate delight or helpfulness. These definitions cannot be scored. They cannot be automated. They cannot be used to gate releases. They are aspirations, not contracts. The fix is to tie every outcome and every constraint to observable evidence in logs, transcripts, or user actions.

The fourth most common mistake is writing definitions that are too rigid. Teams specify exact phrasings, exact formats, or exact sequences that must be followed. This prevents the system from adapting to user needs. The definition should specify outcomes and constraints, not implementations. If the user asks "what's your return policy," the system can answer with a direct statement, a bulleted list, a link to the policy page, or a combination, as long as the answer is correct, clear, grounded in the official policy, and complete. The format is flexible. The outcome is not.

The fifth most common mistake is not versioning the definition. As the product evolves, the definition of "good" changes. Features are added, constraints are tightened, edge cases are discovered, and policies are updated. If you do not version the definition, you cannot interpret past evaluation results. You cannot tell whether a drop in scores reflects a system regression or a change in standards. Version the definition the same way you version code. When the definition changes, document what changed and why.

## The Definition as a Communication Tool

The task definition is not just a technical artifact. It is a communication tool. When Product asks "how good is the new model," you point to the definition and say "here's what we're measuring." When Legal asks "how do we know the system is compliant," you point to the constraints and the evidence logs. When Engineering asks "what are we optimizing for," you point to the outcomes and the edge cases. When Leadership asks "should we ship this," you point to the success and failure signals.

The definition aligns stakeholders. Product, Engineering, Legal, Trust and Safety, and Domain Experts all have different views of what "good" means. Product cares about user satisfaction and task completion. Engineering cares about latency, cost, and reliability. Legal cares about compliance and auditability. Trust and Safety cares about preventing harm. Domain Experts care about correctness relative to ground truth. The definition integrates all of these perspectives into a single contract. When the contract is written down, disagreements become concrete. You can point to a specific constraint and debate whether it is necessary, whether it is too strict, or whether it is missing something. You cannot have that debate when the definition is implicit.

The definition also prevents scope creep during evaluation. Teams start evaluating one thing and end up measuring something else because the boundaries were not clear. They start measuring answer correctness and drift into measuring politeness, verbosity, or stylistic preferences. The definition keeps evaluation focused. Every dimension you score, every test case you write, and every automated check you build should map back to an outcome or constraint in the definition. If it does not map back, it is out of scope.

## Why This Step Cannot Be Skipped

Some teams try to skip task definition and go straight to rubrics or datasets. They assume the definition is obvious or that they can infer it from examples. This never works. Without a written definition, different people infer different standards. Reviewers score based on personal preferences. Dataset creators include cases that do not reflect real user needs. Automated checks measure the wrong things. The evaluation system becomes a collection of disconnected artifacts that do not align.

Other teams write definitions that are so abstract they provide no guidance. They say "the system should be accurate, safe, and user-friendly." This is not a definition. It is a restatement of the problem. A real definition specifies what accurate means for this task, what safety violations look like, and what user-friendly means in terms of observable behavior. It provides examples of edge cases, examples of unacceptable outputs, and examples of the evidence you will collect. It is concrete enough that two people reading it will build the same rubric and write the same test cases.

The task definition is the foundation. If the foundation is weak, everything built on top of it will be weak. Rubrics will drift. Datasets will miss critical cases. Automated checks will produce false positives and false negatives. Release gates will let bad outputs through or block good ones. You cannot fix these problems by tuning the rubric or expanding the dataset. You have to go back to the definition and make it concrete.

## Task Definitions for Hybrid Systems

Many production systems combine multiple task types. A customer service system might use chat for simple questions, RAG for policy lookups, and agents for account actions like password resets or order modifications. A research assistant might use RAG for document search, agents for data analysis, and chat for explaining results. These hybrid systems need task definitions for each component and interface definitions for how components interact.

For each component, you write a separate task definition following the template. The chat component has its own outcomes, constraints, edge cases, and evidence requirements. The RAG component has its own. The agent component has its own. These definitions can share common elements like safety requirements, but they must be specific about what success means for each component.

The interface definition describes how components hand off to each other. When does the chat component escalate to RAG? When does RAG escalate to an agent? What information is passed during the handoff? What happens if a component fails? For a customer service system, the interface definition might say "the chat component handles greetings, FAQs, and general questions. If the user asks about a specific policy or account detail, the chat component routes to RAG with the extracted intent and entities. If RAG retrieves documents but the user requests an action like 'cancel my subscription,' the system routes to the agent with the user's intent, the account context, and the relevant policy as grounding."

Hybrid systems fail when the interfaces are underspecified. The chat component passes the user to RAG without enough context. RAG returns documents but does not format them in a way the chat component can use. The agent receives a task but lacks the grounding to perform it safely. These failures are not component failures. They are interface failures. The task definition for a hybrid system must include interface specifications as first-class requirements.

Interface specifications define the contract between components. They specify what data is passed, in what format, with what guarantees. They specify error handling: what happens if a component is unavailable, what happens if a component returns an error, what happens if a component times out. They specify fallback behavior: if RAG finds no relevant documents, does the system refuse to answer, or does it fall back to the chat component with a disclaimer? If an agent encounters an error during a multi-step workflow, does it rollback partial changes, or does it leave the system in a partial state and report the failure?

These specifications are not afterthoughts. They are part of the task definition. When you evaluate a hybrid system, you evaluate not only each component in isolation but also the interfaces. You test cases where the chat component correctly routes to RAG. You test cases where RAG correctly escalates to an agent. You test cases where an agent fails and the system recovers gracefully. You test cases where the user's request spans multiple components and the system maintains context across handoffs. Without these tests, you cannot claim the system works.

## Versioning and Evolving Definitions

Task definitions evolve as products evolve. Features are added, user needs change, policies are updated, and new failure modes are discovered. If you do not version the definition, you lose the ability to interpret past results and compare performance over time. Versioning is not optional. It is part of the discipline.

When you update a task definition, you increment the version number and document what changed. Version 1.0 might define correctness for a support bot as "matches official documentation." Version 1.1 might add "and does not claim certainty about account-specific details without retrieving them." Version 2.0 might add an entirely new constraint: "never provides tax advice, even if the documentation includes tax information." Each change is documented. Each version is dated. The version history shows how your understanding of "good" has matured.

Versioning enables you to reinterpret past evaluation results. If you ran evals in October 2025 using version 1.0 of the definition and you run evals in January 2026 using version 1.1, you cannot directly compare the scores unless you know what changed between versions. If version 1.1 added a new constraint, scores might drop even if the system did not regress. The drop reflects the stricter standard, not a quality decrease. The version history makes this clear.

Versioning also enables you to validate that changes have the intended effect. When you update the definition, you rescore a sample of past outputs using the new version. You check whether the outputs that scored well under the old definition still score well under the new one. You check whether the outputs that scored poorly under the old definition now score even worse. If the rescoring patterns do not match your expectations, the definition update may have introduced ambiguity or unintended consequences.

Some changes to the definition require rescoring large portions of your evaluation dataset. If you add a new constraint that invalidates previously acceptable outputs, you need to know which outputs are affected. If you change the definition of a core dimension like correctness, you need to rescore the gold set and the calibration set. This work is not wasted. It is maintenance. It keeps your evaluation system aligned with your current understanding of quality.

## The Connection to Prompt Engineering

The task definition is not just for evaluation. It is also the input to prompt engineering. When you write a prompt, you are instructing the model to produce outputs that satisfy the task definition. The outcomes become instructions: "answer the user's question using only information from the retrieved documents." The constraints become guardrails: "do not invent information, do not provide medical advice, do not promise refunds without authorization." The edge cases become examples: "if the documents do not contain the answer, respond with 'I don't have enough information to answer that.'"

A well-written task definition makes prompt engineering faster and more reliable. You do not have to guess what the model should do in edge cases because the edge cases are already documented. You do not have to discover constraints through trial and error because the constraints are already listed. You translate the definition into a prompt, test the prompt against the edge cases, and iterate until the model behaves according to the contract. The definition is the spec. The prompt is the implementation.

This connection works in both directions. As you develop prompts, you discover ambiguities in the task definition. You find cases where the definition does not provide enough guidance. You find constraints that conflict in certain scenarios. These discoveries feed back into the definition. You update the definition to resolve the ambiguities, document the newly discovered edge cases, and clarify how conflicts should be resolved. The definition and the prompt co-evolve.

The same connection exists between the task definition and model fine-tuning. If you fine-tune a model to perform a specific task, the training data should reflect the task definition. The positive examples should satisfy the outcomes and constraints. The negative examples should violate them. The distribution of examples should cover the edge cases. Fine-tuning without a clear task definition produces models that overfit to the quirks of the training data rather than learning the underlying contract. The definition keeps fine-tuning aligned with product requirements.

The next step is turning the definition into a scoring system that humans can apply consistently. That requires rubrics with clear dimensions, anchored examples, and calibration processes. We cover that in the next subchapter.
