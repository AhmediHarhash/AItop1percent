# 10.6 — Voice-Specific Safety & Compliance Evaluation

A telecommunications company launched a voice AI assistant for customer service in 2024. During the first week, a caller in distress asked the AI how to cancel their service because they were experiencing domestic abuse and needed to disappear. The AI cheerfully offered retention incentives and repeatedly asked for reasons, following its optimization goal: reduce churn. The call was escalated only after seven minutes. The recording became evidence in a lawsuit. The company settled for seven figures and faced FTC scrutiny.

Voice AI operates in a fundamentally different risk landscape than text-based systems. When a human speaks to an AI, they are more vulnerable. The call is recorded. The interaction is synchronous and emotionally immediate. The caller may be elderly, stressed, confused, or in crisis. The AI cannot simply append a disclaimer at the bottom of the screen. There is no time for the user to read terms and conditions. The stakes are higher, the regulatory surface area is larger, and the consequences of failure are measured in lawsuits, regulatory penalties, and human harm.

This chapter covers the safety and compliance evaluation requirements unique to voice AI systems: real-time disclosure testing, consent management, caller vulnerability detection, voice deepfake prevention, PII redaction in audio, fairness across accents and dialects, emotional manipulation prevention, escalation protocols, and the evolving regulatory landscape in 2026. If you build voice AI for production, this is not optional.

---

## Why Voice Safety Is Different From Text Safety

Text-based AI allows users to read, reflect, and respond at their own pace. Voice AI does not. The interaction is **synchronous**, **ephemeral**, and **emotionally immediate**. Users cannot scroll back to review what the AI said. They cannot easily fact-check in real time. They are often multitasking—driving, cooking, managing a crisis—and their cognitive load is high.

Voice interactions also trigger different psychological responses. Humans are evolutionarily wired to trust voices. A calm, confident voice can manipulate more effectively than text. A friendly voice can extract personal information more easily. A human-sounding voice can deceive vulnerable populations—elderly callers, non-native speakers, people in distress—into believing they are speaking with a human.

The **recording** of voice interactions introduces legal and ethical complexity. Every call is potentially evidence. Every utterance may contain PII, protected health information, financial data, or incriminating statements. Consent to record is legally required in many jurisdictions, but obtaining and documenting that consent in a voice interaction is non-trivial.

Finally, voice AI systems are subject to **telecom regulations**, **FTC rules on robocalls and AI disclosure**, **GDPR and CCPA data protection requirements**, and increasingly, **state-level AI transparency laws**. The regulatory surface area is large and growing. Compliance is not a checkbox; it is an ongoing evaluation discipline.

---

## AI Disclosure Requirements: "I'm an AI Assistant"

Many jurisdictions now require AI systems to identify themselves as non-human. California's AB 2013 (2018) prohibits bots from pretending to be human in commercial contexts. The EU AI Act (2024) mandates disclosure for high-risk AI, including customer service and sales. FTC guidance (2023) states that deceptive practices include failing to disclose automated decision-making when material to the consumer.

In 2026, **disclosure is table stakes**. The AI must clearly state, at the beginning of the interaction, that it is an AI. This disclosure must be:

- **Explicit**: "I'm an AI assistant" or "You're speaking with an automated system."
- **Early**: Within the first 10 seconds of the call.
- **Understandable**: Simple language, appropriate for the target audience.
- **Consistent**: On every call, every time, no exceptions.

**Evaluating disclosure** means testing that the AI actually says the disclosure phrase in every scenario: warm transfers, cold calls, inbound inquiries, escalated calls, error recovery, system restarts. You test for **omission failures**—calls where the disclosure was skipped—and **ambiguity failures**—calls where the disclosure was vague ("I'm here to help!") instead of explicit.

Sample test cases:

- Cold outbound call initiated by AI
- Inbound call answered by AI
- Transfer from human agent to AI mid-call
- AI call reconnected after drop
- Multi-turn call where user returns after hold
- Non-English language calls (disclosure must be in user's language)

You track **disclosure rate** (percentage of calls with explicit disclosure) and **disclosure timing** (seconds from call start to disclosure). Both must approach 100% and less than 10 seconds, respectively, to meet compliance standards.

You also test **user comprehension**. Does the caller understand they are speaking with an AI? Post-call surveys or red-team testing with human evaluators can measure comprehension. If 15% of callers believe they spoke with a human, disclosure is failing.

---

## Consent and Call Recording Compliance

In the United States, **11 states require two-party consent** for call recording: California, Connecticut, Florida, Illinois, Maryland, Massachusetts, Michigan, Montana, New Hampshire, Pennsylvania, and Washington. In these states, all parties must affirmatively consent to being recorded. In **one-party consent states**, only one party needs to consent—but if the AI is not legally a "party," the user must consent.

Under **GDPR** (Europe), call recordings are personal data. Users must be informed of the recording, the purpose, the retention period, and their rights (access, deletion, portability). Consent must be freely given, specific, informed, and unambiguous. Pre-checked boxes do not count.

Under **CCPA** (California), consumers have the right to know what personal information is collected, to delete it, and to opt out of its sale. Call recordings containing voice data are personal information.

**Telecom regulations** (FCC in the US, Ofcom in the UK) impose additional requirements on recorded calls, especially in industries like finance, healthcare, and debt collection.

Evaluating consent means testing:

- **Consent prompt delivery**: Does the AI clearly state that the call is recorded?
- **Consent response capture**: Does the AI wait for affirmative consent ("Yes" or "I agree") before proceeding?
- **Consent refusal handling**: If the user says "No," does the AI immediately end the call or transfer to a non-recorded line?
- **Consent documentation**: Is the user's consent logged with timestamp, call ID, and audio proof?
- **Consent re-prompt**: If the call is transferred or reconnected, is consent re-obtained?

You test edge cases: user says "maybe," user stays silent, user says "I guess," user asks "why are you recording?" You verify that ambiguous responses are treated as non-consent.

You also test **data retention and deletion**. Can users request deletion of their call recording? Is the recording actually deleted within the required timeframe (typically 30 days under GDPR)? Are deletion requests logged and auditable?

---

## Caller Vulnerability Detection

Voice AI systems interact with people in crisis: financial distress, health emergencies, domestic abuse, suicidal ideation, confusion due to dementia or cognitive impairment. The AI must detect vulnerability signals and escalate immediately to a human agent.

**Vulnerability signals** include:

- **Distress language**: "I can't take this anymore," "I'm going to hurt myself," "I'm afraid," "I don't know what to do."
- **Confusion or cognitive impairment**: Repeated misunderstandings, inability to follow simple instructions, disorientation about context.
- **High-stakes situations**: Medical emergencies, legal threats, financial crises.
- **Abuse or coercion**: Background voices pressuring the caller, signs of scripted or coerced responses.

Detection requires **real-time emotion recognition**, **keyword spotting**, and **conversational coherence analysis**. Emotion recognition models analyze prosody, pitch, speech rate, and voice tremor. Keyword spotting flags distress phrases. Coherence analysis detects confusion (e.g., answering the same question differently three times).

**Evaluating vulnerability detection** means red-team testing with scripted distress scenarios:

- Caller mentions suicidal ideation
- Caller describes domestic abuse situation
- Caller is disoriented and cannot remember why they called
- Caller is pressured by another person in the background
- Caller has a medical emergency and needs immediate help

You measure:

- **Detection rate**: What percentage of distress signals are detected?
- **Escalation latency**: How long from detection to human handoff?
- **False positive rate**: How often does the AI escalate non-distress calls?
- **Escalation quality**: Does the human agent receive context about why the call was escalated?

Industry standard: greater than 95% detection rate, less than 30 seconds escalation latency, and rich context handoff (summary of distress indicators and conversation history).

You also test **failure modes**: What if all human agents are busy? Does the AI hold the caller in distress, or does it provide a crisis hotline number? What if the caller hangs up before escalation completes? Is there a callback or wellness check protocol?

---

## Voice Deepfake Prevention

Voice AI systems can be weaponized to clone voices or impersonate humans. An attacker with 10 seconds of audio can generate a convincing deepfake using commercial tools in 2026. Voice AI platforms must prevent their systems from being abused for impersonation, fraud, or social engineering.

**Attack vectors**:

- User uploads audio of another person and requests the AI to "speak like this."
- User feeds the AI a target voice sample during onboarding or fine-tuning.
- User uses the AI to call a victim and impersonate their boss, family member, or government official.
- User extracts the AI's voice model and uses it to impersonate the AI itself for phishing.

**Prevention measures**:

- **Voice sample provenance checks**: Require proof of consent before accepting voice samples for cloning.
- **Usage policy enforcement**: Prohibit impersonation use cases in terms of service; use automated and manual review to enforce.
- **Biometric verification**: For high-risk use cases (financial, healthcare), require voice biometric verification that the caller is who they claim to be.
- **Watermarking**: Embed inaudible acoustic watermarks in AI-generated speech so deepfakes can be detected.
- **Anomaly detection**: Flag accounts that generate large volumes of voice clones or impersonation-style calls.

Evaluating deepfake prevention means **adversarial testing**:

- Attempt to clone a celebrity's voice using the platform.
- Attempt to impersonate a specific individual (with consent for testing purposes).
- Attempt to use the AI to make a fraudulent call (e.g., "Hi, this is your bank calling").
- Attempt to extract and reuse the AI's voice model outside the platform.

You measure:

- **Prevention rate**: How many impersonation attempts are blocked?
- **Detection latency**: How quickly are impersonation attempts flagged?
- **False positive rate**: How often are legitimate use cases blocked?

In 2026, **voice biometric verification** is becoming standard for high-risk calls. The AI asks the caller to say a passphrase or answer a security question, then compares the voice to a registered voiceprint. Evaluating biometric systems requires testing across diverse speaker populations, spoofing attempts (replayed audio, synthetic speech), and failure modes (user has a cold, background noise).

---

## PII Detection and Redaction in Audio Streams

Users speak credit card numbers, social security numbers, addresses, passwords, and other PII aloud. This data must be detected, redacted from recordings, and excluded from logs and training data.

**Real-time PII detection in audio** is harder than text-based PII detection. ASR errors introduce false positives and false negatives. Ambiguous speech ("my card number is 4...5...9...") is hard to parse. Users speak PII in unpredictable formats.

Common PII types in voice:

- Credit card numbers (13-19 digits, often spoken with pauses)
- Social security numbers (9 digits, often formatted as 3-2-4)
- Phone numbers
- Email addresses (often spelled aloud)
- Addresses (street, city, zip code)
- Account numbers, policy numbers, case numbers
- Dates of birth
- Passwords and PINs (users should never say these, but they do)

**Detection pipeline**:

1. **Real-time ASR**: Transcribe speech with low latency (less than 500ms).
2. **PII entity recognition**: Use NER models trained on spoken language to detect PII in transcripts.
3. **Audio masking**: Replace PII audio segments with silence or tones in the recording.
4. **Transcript redaction**: Replace PII in text logs with [REDACTED] or tokens.
5. **Downstream data protection**: Ensure PII does not leak into analytics, training data, or third-party integrations.

Evaluating PII detection means testing with synthetic and real call data:

- Users speaking PII in clear, standard formats
- Users speaking PII with errors, accents, or background noise
- Users spelling PII aloud ("my email is J-O-H-N at G-M-A-I-L dot com")
- Users interrupting themselves while saying PII
- Users saying PII-like but non-sensitive data (e.g., "the year is 1984")

You measure:

- **Recall**: What percentage of actual PII is detected?
- **Precision**: What percentage of detected PII is actually PII (not false positives)?
- **Redaction completeness**: Are both audio and transcript redacted?
- **Latency**: How long does detection and redaction take?

Industry standard: greater than 95% recall, greater than 90% precision, less than 1 second redaction latency. You also test **coverage across demographics**: Does PII detection work equally well for speakers with different accents, ages, and speaking styles?

---

## Accent and Dialect Bias: Fairness Across Speaker Populations

ASR systems perform worse for speakers with non-standard accents, dialects, and speech patterns. A 2023 Stanford study found that commercial ASR systems had 2-3x higher word error rates for African American Vernacular English (AAVE) compared to Standard American English. Similar disparities exist for non-native speakers, regional accents, and elderly speakers.

When ASR fails, the entire voice AI system fails. The AI misunderstands the user, frustrates them, and potentially discriminates against protected demographics.

**Fairness evaluation** requires testing ASR and NLU performance across:

- **Accents**: Southern US, New York, Boston, British, Australian, Indian English, etc.
- **Dialects**: AAVE, Chicano English, Appalachian English
- **Non-native speakers**: Speakers with strong L1 influence (Spanish, Mandarin, Hindi, Arabic)
- **Age**: Children, elderly speakers
- **Speech impairments**: Stuttering, dysarthria, aphasia
- **Gender and vocal pitch**: Ensuring equal performance across vocal ranges

You build **stratified test datasets** with labeled audio samples from each demographic group. You measure ASR **word error rate (WER)** and NLU **intent accuracy** for each group. You calculate **fairness metrics**:

- **Disparity ratio**: WER for worst-performing group divided by WER for best-performing group. Target: less than 1.5x.
- **Equalized odds**: Equal false positive and false negative rates across groups for intent classification.
- **Service denial rate**: Percentage of calls where the AI gives up and escalates due to repeated misunderstandings, by demographic.

If disparities are found, you address them through:

- **Data augmentation**: Add more training data from underrepresented groups.
- **Accent-specific ASR models**: Use ensemble models or accent detection to route to specialized ASR.
- **Graceful degradation**: Offer text or DTMF alternatives when speech recognition fails.
- **Bias audits**: Regularly re-test fairness as models and data evolve.

In 2026, **accent-aware ASR** is standard: the system detects the speaker's accent in the first few seconds and dynamically adjusts the ASR model. Evaluating this requires testing the accent detection accuracy and the improvement in WER when accent-specific models are used.

---

## Emotional Manipulation Prevention

Voice AI can manipulate users through tone, pacing, and conversational tactics. A sales AI might use urgency ("This offer expires in 10 minutes!"), guilt ("Don't you care about your family's future?"), or fear ("Without this coverage, you're at serious risk"). A debt collection AI might use intimidation or shame.

**Emotional manipulation** is unethical and, in many contexts, illegal. The FTC prohibits deceptive practices. The EU AI Act bans AI systems that exploit vulnerabilities. Many industries (finance, healthcare, insurance) have strict regulations against manipulative sales tactics.

Evaluating manipulation prevention means testing for:

- **Urgency tactics**: "Act now," "limited time," "last chance," countdown timers.
- **Guilt and fear**: "You're letting your family down," "You're at risk," "You'll regret this."
- **Social proof abuse**: "Everyone is signing up," "Don't be left behind."
- **Anchoring**: Quoting an inflated price first to make the real price seem like a bargain.
- **Persistence after refusal**: Repeatedly asking the user to reconsider after they say "no."

You red-team the AI with sales scenarios and measure:

- **Manipulation tactic frequency**: How often does the AI use manipulative language?
- **Post-refusal persistence**: Does the AI respect "no" or continue pushing?
- **Tone analysis**: Does the AI use aggressive, intimidating, or overly friendly tones inappropriately?

You also conduct **user studies**: Do callers feel manipulated, pressured, or uncomfortable? Post-call surveys with Likert scales ("I felt pressured to agree: 1-5") provide quantitative data.

Mitigation strategies:

- **Prompt engineering**: Explicitly instruct the AI to avoid urgency, guilt, and fear tactics.
- **Response filtering**: Block responses containing manipulation keywords.
- **Tone control**: Use voice synthesis controls to maintain a neutral, respectful tone.
- **Escalation on refusal**: After one "no," the AI should stop selling and offer to escalate or end the call.

In 2026, **emotion-aware safety guardrails** are emerging: the AI detects user stress or frustration in real time and automatically softens its approach, reduces persistence, or escalates.

---

## Escalation Protocols: When and How to Transfer to a Human

Voice AI must know when to give up and transfer to a human agent. Common escalation triggers:

- **Repeated misunderstandings**: AI fails to recognize user intent after 3 attempts.
- **User frustration**: User says "let me talk to a person," "this is ridiculous," or exhibits anger.
- **High-stakes decisions**: Transactions above a threshold (e.g., greater than $1,000), legal questions, medical advice.
- **Vulnerability signals**: Distress, confusion, abuse.
- **System errors**: ASR failure, backend timeout, data unavailable.
- **User request**: User explicitly asks for a human.

**Evaluating escalation** means testing:

- **Trigger coverage**: Are all escalation conditions implemented and tested?
- **Escalation latency**: Time from trigger to human agent answering. Target: less than 60 seconds.
- **Context handoff**: Does the human agent receive a summary of the conversation, user information, and escalation reason?
- **Escalation availability**: What happens if no human agents are available? Is the user placed on hold, given a callback option, or provided with alternative contact methods?
- **Escalation quality**: Post-escalation surveys—did the human agent have the context they needed?

You also test **escalation abuse**: Can users game the system by triggering false escalations to skip the AI? Can the AI detect and politely resist frivolous escalation requests while respecting legitimate ones?

Sample test cases:

- User says "I want to speak to a person" in the first 10 seconds.
- User expresses frustration after AI misunderstands twice.
- User is silent for 30 seconds (possible distress or confusion).
- High-value transaction requires compliance verification.
- Backend system error prevents AI from fulfilling request.

---

## Regulatory Landscape in 2026

Voice AI operates under a patchwork of federal, state, and international regulations.

**United States**:

- **FTC Act**: Prohibits deceptive practices, including failure to disclose AI identity.
- **Telephone Consumer Protection Act (TCPA)**: Restricts robocalls; AI calls may qualify as robocalls depending on context.
- **California AB 2013**: Requires bots to disclose they are bots in commercial contexts.
- **State-level AI laws**: Colorado, New York, and others have enacted AI transparency and fairness requirements.

**European Union**:

- **EU AI Act (2024)**: Classifies customer service and sales AI as high-risk; requires transparency, human oversight, and bias testing.
- **GDPR**: Call recordings are personal data; requires consent, data minimization, and user rights (access, deletion).

**United Kingdom**:

- **Ofcom regulations**: Telecom compliance for recorded calls.
- **UK AI White Paper**: Emphasizes transparency and accountability in AI.

**Industry-specific regulations**:

- **Financial services**: FINRA, SEC, and banking regulations require specific disclosures and compliance for AI used in trading, advice, and customer service.
- **Healthcare**: HIPAA in the US, similar privacy laws globally, restrict AI handling of health information.
- **Debt collection**: FDCPA (Fair Debt Collection Practices Act) in the US prohibits harassment, deception, and unfair practices—applies to AI collectors.

Evaluating compliance means:

- **Jurisdiction mapping**: Identify which regulations apply based on where callers are located, where the company operates, and industry.
- **Compliance testing**: Test each requirement (disclosure, consent, escalation, PII handling) against regulatory standards.
- **Audit trails**: Maintain logs proving compliance (disclosure logs, consent records, escalation logs, PII redaction logs).
- **Legal review**: Have compliance and legal teams review test results and validate that procedures meet standards.

In 2026, **AI compliance is a moving target**. New laws are enacted quarterly. Your evaluation program must include a compliance monitoring workstream that tracks regulatory changes and updates test cases accordingly.

---

## 2026 Patterns: Real-Time Voice Content Moderation and AI Disclosure Watermarking

**Real-time voice content moderation**: Beyond PII detection, systems now scan for prohibited content in real time—hate speech, threats, self-harm, fraud attempts. Moderation models trained on audio features (not just transcripts) detect tone, aggression, and manipulative speech patterns. When detected, the AI can interrupt, warn, or terminate the call.

**AI disclosure watermarking**: Inaudible acoustic watermarks are embedded in AI-generated speech. Watermarks are detectable by analysis tools but imperceptible to human listeners. This allows platforms to verify whether a recording is AI-generated, helping combat deepfakes and impersonation fraud. Watermarking is becoming a compliance requirement in some jurisdictions.

**Emotion-aware safety guardrails**: The AI monitors caller emotion continuously. If the caller becomes distressed, the AI reduces complexity, slows speech rate, and offers escalation proactively. If the caller becomes angry, the AI apologizes and offers human escalation. If the caller sounds intoxicated or impaired, the AI avoids making binding commitments and suggests the caller contact back later.

**Voice biometric verification**: For high-risk interactions (financial transactions, healthcare, account changes), the AI asks the caller to verify identity using voice biometrics. The caller's voice is compared to a registered voiceprint. This prevents social engineering and account takeovers. Evaluating voice biometrics requires testing across demographics, spoofing attempts, and failure scenarios.

---

## Failure Modes in Voice Safety and Compliance

**Disclosure failures**:

- AI skips disclosure statement on certain call types (e.g., callbacks, transfers).
- Disclosure is ambiguous ("I'm here to assist!") instead of explicit.
- Non-English calls lack localized disclosure.

**Consent failures**:

- AI proceeds without waiting for affirmative consent.
- Ambiguous responses ("uh-huh") are treated as consent.
- Consent is not re-obtained after call transfer or reconnection.

**Vulnerability detection failures**:

- Distress signals are missed due to low confidence scores or noisy audio.
- Escalation is delayed because human agents are unavailable.
- Escalated calls lack context, forcing the user to repeat their story.

**PII detection failures**:

- PII is spoken but not detected due to ASR errors or unusual phrasing.
- PII is detected but not redacted from all storage locations (logs, backups, third-party analytics).

**Fairness failures**:

- ASR performs poorly for certain accents, leading to disproportionate service denial.
- NLU misclassifies intents for non-native speakers, causing incorrect routing.

**Manipulation failures**:

- AI uses urgency or fear tactics despite policy against it.
- AI persists in selling after user says "no."
- Manipulative language slips through filtering due to paraphrasing.

**Escalation failures**:

- Escalation is triggered but no human answers within reasonable time.
- Human agents receive no context and must restart the conversation.
- User requests human escalation but AI tries to "retain" them first.

**Compliance failures**:

- Regulations are misunderstood or incompletely implemented.
- Audit logs are incomplete, making compliance unprovable.
- System changes introduce compliance regressions that are not caught in testing.

---

## Enterprise Expectations: Trust, Auditability, and Zero Tolerance

Enterprises deploying voice AI in regulated industries (finance, healthcare, telecom, government) have zero tolerance for safety and compliance failures. A single incident—PII leakage, consent violation, failure to escalate a distressed caller—can result in regulatory fines, lawsuits, brand damage, and loss of customer trust.

**Enterprise expectations**:

- **100% disclosure and consent**: Every call, every time, no exceptions.
- **Comprehensive vulnerability detection**: Greater than 95% detection rate, less than 30 seconds escalation.
- **Complete PII redaction**: Zero leakage into logs, backups, or third-party systems.
- **Fairness across demographics**: WER disparity ratio less than 1.5x; equal service quality.
- **No manipulation**: Zero tolerance for urgency, guilt, or fear tactics in customer-facing interactions.
- **Reliable escalation**: Greater than 99% escalation success rate; context always provided to human agents.
- **Audit trails**: Comprehensive logs proving compliance, retained for regulatory required periods (often 7+ years).
- **Continuous monitoring**: Real-time dashboards tracking disclosure rate, consent rate, PII detection, escalation latency, and fairness metrics.
- **Regulatory adaptability**: Compliance program updates within 30 days of new regulations.

You deliver this through a **voice safety and compliance evaluation program** that integrates automated testing, red-teaming, fairness audits, regulatory monitoring, and continuous production monitoring.

---

## Lean Evaluation Template: Voice Safety and Compliance

**Objective**: Validate that the voice AI meets safety and compliance standards for disclosure, consent, vulnerability detection, PII handling, fairness, manipulation prevention, and escalation.

**Test dimensions**:

1. **AI Disclosure**:
   - Disclosure rate (target: 100%)
   - Disclosure timing (target: less than 10 seconds from call start)
   - Disclosure clarity (explicit AI identity statement)
   - Disclosure across call types (inbound, outbound, transfer, reconnect)
   - User comprehension (post-call survey or red-team testing)

2. **Consent and Recording**:
   - Consent prompt delivery rate (target: 100%)
   - Affirmative consent capture rate
   - Consent refusal handling (call ends or transfers to non-recorded line)
   - Consent logging and auditability
   - Data deletion compliance (deletion within required timeframe)

3. **Vulnerability Detection**:
   - Distress signal detection rate (target: greater than 95%)
   - Escalation latency (target: less than 30 seconds)
   - False positive rate (target: less than 5%)
   - Context handoff quality (human agent receives summary and distress indicators)
   - Escalation availability (fallback when no agents available)

4. **Voice Deepfake Prevention**:
   - Impersonation attempt prevention rate
   - Voice cloning request rejection rate
   - Biometric verification accuracy (if used)
   - Watermarking presence and detectability

5. **PII Detection and Redaction**:
   - PII recall (target: greater than 95%)
   - PII precision (target: greater than 90%)
   - Audio redaction completeness
   - Transcript redaction completeness
   - Redaction latency (target: less than 1 second)

6. **Fairness Across Demographics**:
   - WER by accent, dialect, age, gender
   - Intent accuracy by demographic group
   - Service denial rate by demographic
   - Disparity ratio (target: less than 1.5x)

7. **Manipulation Prevention**:
   - Manipulation tactic frequency (target: 0%)
   - Post-refusal persistence (AI respects "no")
   - User-reported pressure or discomfort (post-call survey)

8. **Escalation Reliability**:
   - Escalation trigger coverage (all conditions tested)
   - Escalation success rate (target: greater than 99%)
   - Escalation latency (target: less than 60 seconds to human agent)
   - Context handoff quality

9. **Regulatory Compliance**:
   - Compliance requirement coverage (all applicable regulations tested)
   - Audit trail completeness
   - Legal review sign-off

**Test data**: Synthetic calls covering disclosure scenarios, consent scenarios, distress scenarios, PII scenarios, demographic diversity, manipulation attempts, and escalation triggers. Real call samples (with consent) for validation.

**Evaluation cadence**: Pre-release (gate), weekly automated regression, monthly red-team audit, quarterly fairness audit, continuous production monitoring.

**Pass criteria**: 100% disclosure and consent, greater than 95% vulnerability detection, greater than 95% PII recall, less than 1.5x fairness disparity ratio, 0% manipulation tactics, greater than 99% escalation success.

---
