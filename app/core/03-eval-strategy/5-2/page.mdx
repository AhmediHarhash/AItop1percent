# 5.2 — Dataset Formats: Structuring Examples for Chat, RAG, Agents, and Voice

In late 2024, a financial services company built an agent system to automate trade reconciliation workflows. The agent could query internal databases, invoke calculation tools, and escalate discrepancies to human reviewers. The team created an evaluation dataset of 400 examples to measure agent performance before launch. Each example stored the user's initial goal and the agent's final text response. When the agent produced a reasonable-sounding summary of the reconciliation results, the example was scored as correct. The system passed evaluation with 89% accuracy and was deployed to a pilot group of analysts in early 2025. Within two weeks, the pilot was paused. Analysts reported that the agent frequently queried the wrong databases, invoked tools with incorrect parameters, skipped required confirmation steps, and produced final summaries that contradicted the actual data it had retrieved. The evaluation dataset had measured whether the agent could write coherent text, but it hadn't measured whether the agent actually did the right things in the right order with the right parameters.

The failure wasn't a modeling problem. The root cause was dataset format. The team had stored only the final response text and ignored the tool trace—the sequence of tool calls, parameters, and responses that represented the agent's actual behavior. When you evaluate agents by scoring final outputs without validating the actions that produced those outputs, you're measuring the wrong thing. Your evaluation becomes disconnected from the behaviors that matter in production, and you pass systems that look good on paper but fail operationally.

## What a Dataset Example Must Contain to Be Useful

A good evaluation example is a self-contained test case that can be replayed reliably and scored consistently. At minimum, every example must include a stable identifier so you can track it across dataset versions, a task label from your taxonomy that defines what capability is being tested, the inputs required to execute the task, the expected behavior or outcome, the scoring hooks that define how correctness is measured, the slice metadata that identifies what population segment the example represents, and the version information that records what dataset version and policy version apply. If any of these elements is missing, the example becomes unreliable. You can't compare results across time if identifiers aren't stable. You can't stratify metrics by task or slice if metadata isn't recorded. You can't reproduce evaluation results if inputs or expected outputs aren't captured completely.

The principle is simple but violated constantly in practice. If a case can't be replayed reliably, it's not a good evaluation case. Reproducibility is not a nice-to-have feature. It's the foundation of regression testing, model comparison, and confidence in your metrics. When you change models, update prompts, or modify retrieval logic, you must be able to re-run evaluation on the same cases and compare results directly. If your dataset format doesn't support that, your evaluation program collapses into one-time measurements that tell you nothing about whether changes improve or degrade performance.

Reproducibility requires capturing not just the inputs and outputs but the full context that determines correctness. For chat, that means storing conversation history. For RAG, that means storing retrieved chunks. For agents, that means storing tool traces. For voice, that means storing audio and timing. Every channel has dependencies that must be frozen in the dataset to ensure that evaluation remains valid as your system evolves.

## Chat Format: Single-Turn and Multi-Turn Conversations

For chat systems, the correct format is a conversation array where each message includes role, content, timestamp, and optional attachments. The role field identifies whether the message came from the user, the assistant, the system prompt, or a tool response. The content field stores the actual message text. The timestamp is optional but useful for debugging latency or turn-taking issues. Attachments are necessary if your product supports image inputs, document uploads, or other non-text modalities.

The most common mistake in chat datasets is storing only the last user message and ignoring prior turns. If your product is multi-turn—if context from earlier messages influences the correctness of later responses—then your evaluation dataset must include full conversation windows that match production patterns. A question like "what about the other one" is unanswerable without the previous turn that established what "the other one" refers to. A query like "and in Spanish" assumes the assistant remembers what was just discussed. If your dataset strips away prior context, you're not measuring how the system performs in realistic multi-turn interactions. You're measuring how it performs on artificially isolated single-turn queries, and your metrics will be optimistic relative to real user experience.

This mistake is surprisingly common because single-turn examples are easier to create, easier to label, and easier to score. You don't need to track conversation state, manage context windows, or reason about turn-level dependencies. But that simplicity comes at the cost of validity. If your production system is multi-turn, your evaluation must be multi-turn. Otherwise, you're testing a different problem than the one you're solving.

The enterprise rule is straightforward. If the product is multi-turn, the dataset must be multi-turn. You store conversation windows of realistic length, including turns where the user clarifies, corrects, or extends prior requests. You don't truncate context to simplify evaluation. You test the system under the same context constraints it will face in production. That means capturing conversations that span three turns, five turns, or ten turns if that's how users actually interact with your system. It means including cases where the user changes topics mid-conversation, where the user corrects a previous statement, where the user asks a follow-up question that depends on the assistant's previous response.

You also capture system messages and tool responses if your product includes them in the conversation context. Some chat systems inject system-level prompts or instructions dynamically based on user state, permissions, or workflow stage. If those system messages influence the assistant's behavior, they must be included in the evaluation dataset. Similarly, if your assistant can invoke tools and inject tool responses into the conversation, those tool responses are part of the context that determines correctness, and they must be stored in the dataset.

## RAG Format: Queries, Retrieved Chunks, and Citation Constraints

For RAG systems, evaluation examples must include the user query, the conversation context if multi-turn, the exact retrieved chunks of text used by the system, the document identifiers for each chunk including document ID, title, URL or file path, and section heading, and the expected answer constraints including required facts, required citations if your product uses citations, and abstain rules if evidence is missing. Optionally, you also store retrieval metadata such as rank order, chunk boundaries, embedding version, and retriever version. This metadata is useful when diagnosing why retrieval results changed over time, but it's not required for basic evaluation.

The critical requirement is storing the retrieved context snapshot. The most common mistake in RAG datasets is storing "the documents" but not the exact retrieved chunks that were used at evaluation time. This makes the evaluation non-reproducible because retrieval changes over time. When you update your embedding model, reindex your document corpus, or modify your chunking strategy, the retriever returns different chunks for the same query. If your dataset doesn't capture the original chunks, you can't distinguish between model improvements and retrieval changes when metrics shift. You're comparing results under different conditions and calling it a controlled experiment.

This is not a hypothetical problem. In 2026, enterprises update their retrieval systems frequently. They switch from OpenAI embeddings to Cohere embeddings, from 512-token chunks to 1,024-token chunks, from lexical retrieval to hybrid retrieval with reranking. Each change alters what chunks the model sees, which alters what answers are possible, which alters evaluation scores. If you don't snapshot the retrieved context, you can't tell whether a metric drop reflects model degradation or simply that the new retriever returned harder chunks. You lose the ability to isolate variables and make controlled comparisons.

The enterprise rule is to store the retrieved context snapshot used for evaluation, even if the live retriever changes later. You freeze the retrieval results for each example so that re-running evaluation measures model performance on identical inputs. When you upgrade your retriever, you can choose to refresh the dataset with new retrieval results, but you do so deliberately and version the dataset accordingly. You never accidentally invalidate regression comparisons because retrieval silently changed underneath your evaluation.

For RAG cases, expected behavior is more nuanced than for chat. You specify whether the correct answer is "answer with evidence," "abstain because evidence is missing," or "refuse because the question is out of scope or prohibited." You also specify whether citations are required, and if so, which chunks must be cited to satisfy correctness. If your product uses inline citations, you validate that the response attributes facts to the correct source documents and doesn't hallucinate citations to documents that weren't retrieved. RAG evaluation is not just "did the model answer the question?" It's "did the model answer correctly using only the evidence provided, and did it abstain or cite correctly when required?"

Abstain conditions are particularly important for high-stakes RAG systems. In legal document review, clinical decision support, and compliance workflows, the system must explicitly refuse to answer when the retrieved evidence is insufficient or contradictory. A RAG system that always attempts to answer, even when evidence is missing, creates liability. Your evaluation dataset must include "must abstain" cases where the correct behavior is to say "I don't have enough information to answer that" or "the documents provided don't address this question." You validate that the system can distinguish between "I can answer this based on the evidence" and "I can't answer this safely."

Citation requirements vary by product. Some RAG systems include inline citations that attribute every factual claim to a source document. Others include a references list at the end of the response. Still others don't expose citations to users but log them internally for auditability. Whatever your citation model, your evaluation dataset must encode the citation requirements for each example and validate that citations are accurate, complete, and properly formatted. A citation is incorrect if it attributes a fact to the wrong document, if it references a document that wasn't retrieved, or if it claims support that the cited document doesn't provide.

## Agent Format: Tool Traces and State Transitions

For agent systems, evaluation examples must include the user goal, the list of allowed tools with their schemas, the tool trace that records every tool call, parameter set, and tool response in sequence, the final state that defines what "done" looks like, and the safety requirements including confirmations required, approvals required, and prohibited actions. You do not need to store intermediate assistant thoughts or reasoning steps unless your product exposes those to users or uses them for debugging. What matters for evaluation is the sequence of actions the agent took, whether those actions were correct, and whether the final state matches the goal.

The most common mistake in agent evaluation is scoring only the final text output and ignoring whether the agent actually did the right thing. An agent can produce a polished, confident-sounding summary of a completed task while having queried the wrong database, called tools with incorrect parameters, or skipped required confirmation steps. If you score only the final message, you pass systems that write well but act incorrectly. The enterprise rule is that agent datasets must be trace-first. You evaluate "did it do the right steps safely?" not just "did it say something nice?"

Tool traces must be stored with full fidelity. That means recording the tool name, the parameters passed, the response received, and the order in which tool calls were made. Order matters because some workflows require specific sequencing—you must authenticate before querying, you must validate before executing, you must confirm before committing. If your dataset stores only which tools were called but not the order, you can't detect sequencing errors. If your dataset stores only that tools were called but not what parameters were passed, you can't detect parameter errors. If your dataset stores only the successful tool calls but not the failed ones, you can't detect whether the agent handles errors gracefully.

Tool traces must include tool-failure scenarios. In production, tools time out, return partial results, throw permission errors, or fail silently. If your evaluation dataset includes only happy-path traces where every tool call succeeds, you're not testing how the agent handles adversity. You're measuring best-case performance and ignoring the failure modes that cause production incidents. You add synthetic or expert-written cases where tools fail, and you validate that the agent retries appropriately, escalates when necessary, and doesn't hallucinate results when a tool returns an error.

Final state validation is critical for transactional workflows. If the user goal is "cancel my subscription and delete my payment method," the correct final state is that both the subscription is canceled and the payment method is removed. If the agent cancels the subscription but leaves the payment method in place, the task is incomplete. You can't detect that failure by reading the agent's final message. You detect it by checking the final state against the goal requirements. Agent evaluation must validate outcomes, not just outputs.

Safety requirements must be encoded explicitly. For high-stakes agent workflows, certain actions require user confirmation before execution. Deleting data, making purchases, sending messages on behalf of the user, modifying permissions—these actions are irreversible or consequential, and agents must obtain explicit approval before proceeding. Your evaluation dataset must specify which actions require confirmation, and your scoring must validate that confirmations were obtained. An agent that executes a deletion without confirming fails the evaluation even if the deletion was technically correct.

## Voice Format: Audio, Transcripts, Timing, and Outcomes

For voice systems, evaluation examples must include an audio file reference or snippet, transcripts including both the automatic speech recognition output and a human reference transcript if possible, segmentation into speaker turns with timestamps, environment labels that identify noise level, accent or dialect, barge-in presence, and interruptions, expected dialogue behavior including confirmations for critical fields and escalation rules, and the final task outcome such as ticket created, appointment booked, or escalation triggered. Voice evaluation is multi-layered because correctness depends not just on what the system understood or said, but on how it behaved in real-time interaction.

The most common mistake in voice datasets is measuring only transcript accuracy while ignoring task success and user experience behavior. An ASR system can transcribe speech perfectly and still produce a terrible user experience if the dialog policy is wrong—if the assistant interrupts the user unnecessarily, fails to confirm critical information, or ignores correction attempts. Conversely, an ASR system can make transcription errors that don't affect task outcome because the dialog policy is robust to minor misunderstandings and confirms before acting.

The enterprise rule is that voice truth is multi-layer. You measure ASR accuracy by comparing the ASR output transcript to the gold reference transcript and computing word error rate or other ASR metrics. You measure dialogue policy by validating that the assistant confirmed critical fields, escalated when required, and allowed the user to correct mistakes. You measure task outcome by checking whether the final result matches the user's goal—was the appointment actually booked, was the ticket actually created, did the assistant capture the right information? You also measure latency and turn-taking behavior using the timestamp data, because users perceive the system as broken if it takes too long to respond or interrupts them mid-sentence even if the final transcript is perfect.

Voice datasets must store audio references and timestamps, not just text, because timing and prosody influence user experience in ways that text alone doesn't capture. A response that feels instant in text may feel sluggish in voice. A confirmation that reads as polite in text may sound curt or robotic in voice. You can't evaluate voice quality without listening to the audio or measuring timing, and your dataset format must support that.

Environment labels are critical for diagnosing performance issues. Voice systems perform differently in quiet office environments versus noisy call centers, with native speakers versus non-native speakers, with clear audio versus compressed or distorted audio. If your evaluation dataset doesn't label environment conditions, you can't stratify metrics by environment and you can't detect that performance degrades under specific conditions. You tag examples with noise level, accent, barge-in presence, and other environmental factors so you can measure performance separately for each condition.

Dialogue behavior evaluation is distinct from transcription accuracy. You validate that the assistant confirmed critical fields like dates, phone numbers, and addresses before proceeding. You validate that the assistant escalated to a human when the user requested it or when the task exceeded the assistant's capabilities. You validate that the assistant allowed the user to correct mistakes without starting over. These behaviors aren't captured in transcripts. They're captured in dialogue structure, which you represent by annotating expected confirmations, escalation triggers, and correction handling in your dataset.

## The Unified Schema Approach

Even if you have different input formats for different channels, you enforce a shared top-level structure so that cross-channel reporting and versioning remain consistent. Every example, regardless of channel, includes example ID, task ID, channel, risk tier, difficulty, slice metadata for language, customer tier, tenant, region, inputs in a channel-specific payload, expected behavior and constraints, scoring rubric hooks, provenance metadata recording source type, creator, and creation date, and version information including dataset version, policy version, retriever version, and tool schema version. This unified schema makes it possible to aggregate metrics across channels, detect dataset drift by slice or difficulty, and version datasets consistently as your system evolves.

The inputs field contains channel-specific payloads. For chat, it's a conversation array. For RAG, it's a conversation array plus retrieved chunks and document identifiers. For agents, it's a user goal, allowed tools, and tool trace. For voice, it's audio references, transcripts, and timestamps. The structure adapts to the channel, but the top-level schema remains consistent. This consistency is what enables CI integration, automated regression testing, and cross-channel quality monitoring. Without it, every channel becomes a siloed evaluation program with incompatible data formats and incompatible reporting.

The unified schema also simplifies versioning. When you update your dataset, you increment the dataset version and record what changed. When you update your policy or business rules, you increment the policy version and record which examples are affected. When you update your retriever or tool schemas, you increment those versions and record the compatibility requirements. Version tracking is what makes regression testing meaningful. Without it, you can't tell whether metric changes reflect model changes, data changes, or infrastructure changes.

## Reproducibility Defaults and Privacy Defaults

To ensure reproducibility, you store retrieval snapshots for RAG, tool traces for agents, audio references and timestamps for voice, and prompt or system configuration references if needed for replay. You version everything—datasets, policies, retrievers, tool schemas—so that you can reconstruct exactly what conditions applied when a given evaluation was run. Reproducibility is not automatic. It's a design choice that requires deliberate versioning and snapshotting of dependencies.

Reproducibility also requires determinism where possible. If your system includes randomness—sampling temperature, retrieval reranking, tool selection heuristics—you either fix random seeds for evaluation or you run multiple trials and aggregate results. You don't rely on single-shot evaluation for stochastic systems because metrics will vary across runs. You establish reproducibility protocols that account for non-determinism and ensure that observed metric changes reflect real performance shifts, not random variation.

To ensure privacy, you treat production data as high-risk and apply sanitization before storing it in evaluation datasets. You store only the minimum data needed for evaluation, you separate sensitive fields into locked storage with access controls, and you apply retention policies that purge old examples according to defined schedules. Privacy defaults are covered in depth in Chapter 5.5, but the principle applies at the format level. Your dataset schema must support redaction, anonymization, and access control so that sensitive data doesn't leak into evaluation pipelines, shared dashboards, or public repositories.

Privacy considerations vary by channel. For chat, you redact personally identifiable information like names, email addresses, phone numbers, and account identifiers. For RAG, you redact sensitive content from retrieved chunks and anonymize document identifiers. For agents, you redact tool parameters and responses that contain sensitive data. For voice, you strip speaker identity metadata and anonymize transcripts. The schema must support these redactions without breaking evaluation logic.

## Storage and Access Patterns for Multi-Channel Formats

Different channels have different storage requirements. Chat examples are small and fit comfortably in JSON files or database rows. RAG examples are larger because they include retrieved chunks, which can be kilobytes per example. Agent examples can be very large if tool traces are long or if tool responses include substantial data. Voice examples are enormous because they include audio files, which can be megabytes per example.

You don't store all channels in the same format. For chat and small RAG examples, you use structured storage like JSON files, relational databases, or document stores. For large RAG examples with many retrieved chunks, you store chunk references separately and join them at evaluation time to avoid duplicating chunk content across examples. For agent examples with large tool traces, you store traces in blob storage and reference them from the main example record. For voice examples, you store audio in object storage like S3 or GCS and store transcripts and metadata in structured storage with pointers to the audio.

This separation of concerns keeps your primary dataset storage efficient while supporting large payloads when needed. You don't load gigabytes of audio into memory just to filter examples by task ID or slice. You query structured metadata to identify relevant examples, then load channel-specific payloads only for examples you're actually evaluating. This access pattern is critical when datasets grow to tens of thousands of examples across multiple channels.

You also implement access controls that vary by data sensitivity. Chat examples that have been sanitized can be stored with broad read access for your evaluation team. RAG examples that include proprietary documents require restricted access limited to authorized personnel. Agent examples that include tool parameters with sensitive data require encryption at rest and audit logging for every access. Voice examples that include speaker audio require the strictest controls because voice is biometric data that can identify individuals. Your storage architecture must support fine-grained access control so sensitive data doesn't leak while non-sensitive data remains accessible for evaluation.

## Failure Modes and Diagnostic Signals

When you can't reproduce an evaluation result, the root causes are that retrieval context changed between runs, tool responses changed, configuration references are missing, or cases lack stable IDs and versioning. The fix is to snapshot retrieval chunks, record tool input-output pairs, version datasets and dependencies, and enforce stable identifiers for every example. Reproducibility failures are almost always format failures. You didn't capture enough information to replay the test case under identical conditions.

When agents look good in evaluation but fail in production, the root causes are that evaluation lacks tool-failure traces, evaluation doesn't validate state transitions, or only final responses are scored. The fix is to add tool-failure packs that simulate timeouts, errors, and partial results, to score traces and state outcomes in addition to final messages, and to validate safety requirements like confirmations and approvals. Agent evaluation must test behavior, not just language.

When voice systems pass ASR metrics but users still hate the experience, the root causes are no scoring for confirmations, no scoring for interruptions, and no latency or turn-taking evaluation. The fix is to add voice behavior rubrics that validate dialog policy and to add timing metrics that measure responsiveness and turn-taking. Voice quality is not reducible to transcription accuracy. It's a combination of understanding, behavior, and timing, and your dataset format must support evaluating all three.

When cross-channel comparisons produce inconsistent results, the root cause is incompatible data formats or missing metadata. Chat examples use one schema, RAG examples use another, agents use a third, and there's no shared structure that allows aggregation. The fix is to adopt the unified schema approach, where channel-specific payloads are nested under a consistent top-level structure that includes task ID, risk tier, difficulty, slices, and versions. Without that shared structure, cross-channel reporting becomes impossible.

When dataset loading becomes slow or memory-intensive, the root cause is that you're loading all channel-specific payloads into memory regardless of whether they're needed. Large RAG chunks, long agent traces, and audio files consume memory even when you're only filtering by metadata. The fix is to implement lazy loading where you query metadata first, identify relevant examples, and load channel-specific payloads only for examples being evaluated. You also separate large payloads into external storage and reference them by pointer rather than embedding them directly in example records.

When format validation fails intermittently, the root cause is schema heterogeneity—different examples were created under different schema versions and you're validating them all against the current schema. The fix is to implement schema-aware validation that checks each example against the schema version it declares, and to implement migration logic that transforms old examples to new schema versions when needed. You don't assume schema uniformity—you handle schema evolution explicitly.

## Format Evolution and Migration

Datasets outlive the systems they evaluate. Your format must evolve as your product adds capabilities, your evaluation requirements change, and your infrastructure matures. Format evolution without breaking backward compatibility is what separates maintainable evaluation programs from brittle ones.

You evolve formats by adding fields, not by removing or renaming fields. When you need to capture new information—a new metadata dimension, a new input modality, a new scoring requirement—you add it as an optional field with a default value. Existing examples that don't have the new field are interpreted as having the default. This preserves backward compatibility. Old examples remain valid and scoreable even as the schema grows.

When you must change a field's meaning or structure, you version the schema explicitly and implement migration logic. You assign a schema version to every example so your evaluation pipeline knows how to interpret it. You write migration code that transforms examples from old schema versions to new schema versions. You test migration on historical examples to ensure no information is lost. You never assume that all examples in your dataset conform to the current schema—you handle schema heterogeneity gracefully.

Format migrations are particularly important when moving from informal early-stage formats to production-grade formats. Many teams start with ad-hoc JSON structures, CSVs, or spreadsheets during prototyping. As evaluation becomes operational, they migrate to structured schemas with validation, versioning, and access controls. That migration must preserve historical examples so you don't lose the ability to compare current performance against baseline performance from early releases.

## Format Validation and Quality Controls

Invalid formats cause silent evaluation failures. If an example is missing required fields, if IDs aren't unique, if metadata is inconsistent, evaluation either crashes or produces unreliable results. Format validation catches these errors before they corrupt metrics.

You implement format validation as a pre-flight check that runs before evaluation begins. You validate that every example conforms to the expected schema, that required fields are present and non-empty, that IDs are unique within the dataset, that versions are consistent across examples, that metadata values are drawn from defined enumerations, and that channel-specific payloads match the declared channel type. Validation either passes and evaluation proceeds, or validation fails and you fix the dataset before re-running.

You also implement semantic validation that checks for common errors beyond schema conformance. You validate that conversation arrays aren't empty, that retrieved chunks match the declared document IDs, that tool traces include responses for every tool call, that audio references point to files that exist, and that expected behaviors are drawn from the defined set of valid behaviors. Semantic validation catches errors that schema validation misses because they're logically invalid even if structurally valid.

Quality controls extend beyond validation to sample-based review. You periodically sample examples from your dataset and manually review them for correctness, clarity, and representativeness. You check that production examples reflect real user behavior, that expert-written examples test the intended capability, that synthetic examples are natural and grounded, and that labels and expected outputs are accurate. Sample-based review catches errors that automated validation can't detect—cases where an example is technically valid but substantively wrong.

## Tooling and Infrastructure for Format Management

Managing dataset formats at scale requires tooling. You don't edit JSON files by hand or write one-off scripts to parse examples. You build or adopt libraries that abstract format handling, provide schema validation, support migration, and integrate with your evaluation pipeline.

In 2026, many enterprise teams use tools like Hugging Face Datasets for format management and distribution, Pydantic or JSON Schema for validation, DVC or Git LFS for versioning large datasets, and custom libraries for channel-specific parsing. The tooling choice matters less than the principles. You need versioned schemas, automated validation, migration support, and integration with CI so that format errors are caught early and don't propagate into production evaluation.

You also need tooling for dataset exploration and debugging. When an evaluation fails, you must be able to inspect the examples that failed, examine their inputs and expected outputs, and determine whether failure reflects model issues or format issues. You build or adopt viewers that render examples in human-readable formats, that highlight missing or inconsistent fields, and that let you compare examples across dataset versions. Without exploration tooling, debugging evaluation failures becomes guesswork.

## What Serious Teams Actually Do

Enterprise teams in 2026 maintain a unified schema with channel-specific payloads. They store replayable artifacts including retrieval snapshots, tool traces, and audio timestamps so that regression testing remains reliable as systems evolve. They keep datasets compatible with CI by ensuring deterministic replays for regression suites, and they enforce access control and auditing for sensitive fields so that production data doesn't leak into unauthorized contexts. They version datasets and dependencies explicitly, and they treat reproducibility as a first-class requirement, not an afterthought.

They also build format validation into their evaluation pipelines. Before running evaluation, they validate that every example conforms to the expected schema, that required fields are present, that IDs are unique, and that versions are consistent. Format validation catches errors early, before bad data corrupts metrics or breaks regression comparisons. They treat dataset formats as contracts that define what evaluation requires and what systems must provide.

They invest in format evolution infrastructure so datasets can grow without breaking. They implement schema versioning, migration logic, and backward compatibility so historical examples remain valid as formats evolve. They build tooling for format management, validation, and exploration so dataset quality remains high and debugging remains tractable. They treat formats as long-lived artifacts that outlive individual releases and must be maintained with the same rigor as production code.

They implement format governance that defines who can propose schema changes, what review process applies, and what migration requirements exist. They maintain format documentation that explains every field, every enum value, and every version change so new team members can understand the schema without archaeo logy. They run format audits quarterly to identify deprecated fields, inconsistent usage, or technical debt that should be addressed. They treat format management as continuous maintenance, not as set-and-forget infrastructure.

Your dataset format determines what you can measure and what you can't. If the format doesn't capture the inputs and outputs that define correctness for your channel, you'll measure proxies instead of truth, and your metrics will drift away from real performance. Format choices made early in development have long-term consequences because datasets outlive the systems they evaluate. A poorly chosen format locks you into limitations that become harder to escape as your dataset grows. A well-chosen format provides flexibility to evolve as requirements change without invalidating historical data or breaking regression comparisons. The time you invest in format design upfront pays dividends in maintainability, reproducibility, and confidence in your evaluation results over the lifetime of your product.

In the next subchapter, we'll examine dataset size—how many examples you actually need to get reliable metrics, and how to avoid both under-sampling and over-sampling as you scale your evaluation program.
