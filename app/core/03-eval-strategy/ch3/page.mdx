# Chapter 3 â€” Task Taxonomy & Coverage

### Plain English

You cannot evaluate what you have not categorized.

**What are the different kinds of tasks your AI system must handle, and do you have enough test cases for each?**

AI systems rarely do just one thing.

A customer support chatbot might:
- answer simple FAQs
- look up account information
- troubleshoot technical issues
- escalate to humans
- handle angry users

Each of these is a different task category requiring different evaluation logic.

A task taxonomy organizes these categories into a map.

Coverage ensures you have sufficient test cases for each category so your evals are not blind to entire classes of failures.

This chapter teaches you how to build task taxonomies, measure coverage, and maintain them as your system evolves.

---

### Why This Chapter Exists

Most eval failures happen in task categories you forgot to test.

Without a task taxonomy:
- you test only the obvious cases
- you miss edge cases and rare scenarios
- you have no systematic way to find gaps
- you cannot measure whether your eval set is balanced

A task taxonomy exists to:
- make the invisible visible
- organize test coverage systematically
- prevent blind spots
- align product roadmaps with eval coverage
- communicate scope clearly to stakeholders

In 2026, **task taxonomies are living documents**.

They evolve as your product evolves, and they drive dataset design.

---

### What a Task Taxonomy Actually Is (2026 Meaning)

A task taxonomy is **not**:
- a single list of features
- a product spec
- a static document written once

A task taxonomy **is**:
- a hierarchical map of task categories
- updated continuously as tasks change
- the foundation for dataset design
- used to measure coverage gaps
- tied directly to product capabilities

Technically, it defines:
- task categories and subcategories
- input/output patterns per category
- difficulty levels
- frequency or priority of each task
- which tasks require special evaluation logic

---

### Core Components of Task Taxonomy & Coverage

#### 1. Build a Task Taxonomy

Start by asking: **what are the distinct categories of things your system does?**

A taxonomy has multiple levels:

**Level 1: High-level capabilities**
- Conversational chat
- Information retrieval
- Task execution
- Creative generation

**Level 2: Task types**
- Conversational chat: greetings, clarifications, follow-ups, corrections
- Information retrieval: single-document lookup, multi-document synthesis, entity extraction
- Task execution: tool calls, multi-step workflows, error recovery

**Level 3: Subtask variations**
- Multi-step workflows: linear flows, conditional branches, loops, error recovery, partial success

Good taxonomies:
- are hierarchical
- cover all current capabilities
- include edge cases and errors
- distinguish task difficulty
- are agreed upon by product and engineering

Bad taxonomies:
- are too granular (100+ leaf categories)
- are too coarse (3 total categories)
- are organized by implementation, not behavior
- are never updated

---

#### 2. Coverage Map

A coverage map shows which tasks you have evaluated and which you have not.

Example coverage map:

Task Category: Information Retrieval
- Single-document lookup: 120 test cases
- Multi-document synthesis: 45 test cases
- Entity extraction: 30 test cases
- Missing information handling: 8 test cases (COVERAGE GAP)

Task Category: Tool Calling
- Single tool call: 200 test cases
- Multi-tool sequences: 15 test cases (COVERAGE GAP)
- Tool call errors: 5 test cases (CRITICAL GAP)

A coverage map allows you to:
- identify under-tested categories
- prioritize dataset expansion
- communicate risk to stakeholders
- track coverage over time

Without a coverage map, you have no idea where your blind spots are.

---

#### 3. Sampling Strategy

Not all tasks are equally important.

Your sampling strategy defines:
- how many test cases per task category
- how to balance common vs rare tasks
- how to represent difficulty levels

Common sampling approaches:

**Proportional sampling**: test cases match production frequency
- 80% common tasks, 15% uncommon, 5% rare

**Risk-weighted sampling**: test cases match failure impact
- 50% high-risk tasks, 30% medium-risk, 20% low-risk

**Uniform sampling**: equal test cases per category
- useful for catching blind spots in rare tasks

Most mature systems use **hybrid sampling**:
- proportional for common paths
- risk-weighted for safety-critical tasks
- uniform for ensuring no category is forgotten

Your sampling strategy is documented and reviewed, not accidental.

---

#### 4. Slice Strategy

Slicing cuts your taxonomy by additional dimensions.

Common slices:
- User type (new user, power user, enterprise admin)
- Input language (English, Spanish, code-switching)
- Input modality (text, voice, image)
- Conversation length (single-turn, short, long)
- Time sensitivity (urgent, routine)
- Domain (medical, legal, technical, casual)

Example slicing:

Task: Entity extraction
- Slice by language: English (50 cases), Spanish (30 cases), multilingual (10 cases)
- Slice by domain: medical (20 cases), legal (20 cases), general (50 cases)
- Slice by input quality: clean text (60 cases), OCR errors (20 cases), user typos (10 cases)

Slicing prevents:
- hidden biases in your eval set
- assuming one context generalizes to all contexts
- missing entire user populations

Without slicing, your evals may pass on demo data and fail in production.

---

#### 5. Keeping Taxonomy Updated

Task taxonomies rot.

Your product adds features, users behave unexpectedly, and edge cases emerge.

Best practices for maintaining taxonomies:

**Review cadence**: monthly or quarterly
**Triggers for updates**:
- new product features launch
- production failures reveal new task categories
- user research uncovers unmet needs

**Update process**:
- identify new task categories
- classify recent production failures
- add new categories to the taxonomy
- create test cases for new categories
- deprecate obsolete categories

Taxonomies are versioned and reviewed like code.

Old taxonomies are archived, not deleted.

---

#### 6. Practical Coverage Math

How many test cases do you need?

There is no universal answer, but here is practical math:

**Minimum viable coverage**:
- 10-20 cases per leaf task category
- 50-100 cases per high-risk category
- 5-10 cases per rare edge case

**Mature system coverage**:
- 50-200 cases per leaf task category
- 200-500 cases per high-risk category
- 20-50 cases per rare edge case

**Coverage targets**:
- 80% coverage of common tasks (by production volume)
- 100% coverage of safety-critical tasks
- 50% coverage of rare tasks

Coverage is never 100% complete.

The goal is **systematic coverage**, not exhaustive coverage.

---

#### 7. Agentic Task Decomposition

Agents decompose high-level goals into subtasks.

Example: "Book a flight to Tokyo"
- Search for flights
- Compare options
- Select best option
- Enter passenger details
- Complete payment
- Confirm booking

Agentic task taxonomies must include:
- task decomposition strategies
- subtask dependencies
- error recovery paths
- partial completion handling

Agentic evals require:
- end-to-end workflow tests
- subtask-level tests
- error injection tests
- rollback and retry tests

Agentic systems fail in ways single-turn systems do not.

Your taxonomy must reflect this complexity.

---

### Enterprise Perspective

Enterprises need:
- comprehensive coverage maps for audit trails
- risk-weighted sampling to protect high-value workflows
- taxonomies aligned across products and geographies
- clear accountability for coverage gaps

Task taxonomies allow:
- consistent evaluation across teams
- evidence-based risk assessment
- prioritization of eval investment
- alignment between product roadmaps and eval capacity

In regulated industries, coverage gaps are treated as compliance risks.

---

### Founder / Startup Perspective

Startups need:
- fast but systematic taxonomy creation
- coverage maps that highlight the biggest risks
- sampling strategies that maximize learning per test case
- taxonomies that evolve as the product pivots

Good taxonomies:
- clarify what you are building
- reveal blind spots before users find them
- allow you to prioritize dataset work
- create leverage with small teams

Startups that skip taxonomies discover blind spots only in production.

---

### Common Failure Modes

- No task taxonomy, just ad-hoc test cases
- Taxonomy is created once and never updated
- Coverage is accidental, not measured
- Sampling strategy is implicit, not documented
- Slicing by user type or language is forgotten
- Rare tasks are completely untested
- Agentic workflows are tested only end-to-end, not per subtask
- Coverage gaps are known but ignored

Recognizing these failures is the foundation of systematic evaluation.

---
