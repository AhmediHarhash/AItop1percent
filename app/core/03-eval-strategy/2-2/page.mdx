# 2.2 â€” Rubrics Humans Can Score Consistently

In mid-2024, a financial services company built an internal assistant to help compliance analysts answer questions about regulatory requirements. The team hired three contractors to score model outputs using a five-point scale across four dimensions: accuracy, completeness, clarity, and safety. After two weeks, the scoring patterns diverged so badly that the data became unusable. One reviewer gave mostly fours and fives. Another gave mostly twos and threes for the same outputs. The third reviewer spent ten minutes per response reading background documentation and still couldn't decide between adjacent scores. The team had spent eighteen thousand dollars on annotations they couldn't use.

The problem was not the reviewers. The problem was the rubric. The dimension names were generic. The scale had no anchored examples. The instructions said "score based on quality" without defining quality. When the team rebuilt the rubric, they reduced the scale to four points, wrote specific anchors for each score, added examples of borderline cases, and ran calibration sessions until the reviewers agreed on a gold set of fifty examples. Scoring speed tripled. Agreement rates went from forty-two percent to eighty-nine percent. The rebuild took three days. The lesson is that rubrics are not self-explanatory. If different people apply them differently, the rubric has failed.

## What a Rubric Is

A rubric is a contract that tells reviewers what to look for, what counts as success, what counts as failure, and how to score the space in between. It has four components. The first component is **dimensions**, the specific aspects of quality you are measuring. The second component is **scales**, the score levels with clear definitions for each level. The third component is **anchors**, concrete examples that show what each score looks like in practice. The fourth component is **rules**, the tiebreakers and edge case guidance that prevent reviewers from inventing their own criteria.

Dimensions define the lenses through which you evaluate an output. For a chat system, dimensions might include correctness, relevance, clarity, and safety. For a RAG system, dimensions might include answer relevance, grounding, citation quality, and clarity. For an agent, dimensions might include task completion, tool correctness, efficiency, and recovery behavior. Each dimension should measure one observable aspect of the output. If a dimension requires subjective judgment that different reviewers will interpret differently, the dimension is too vague.

Scales define the score levels and what they mean. Most teams default to a five-point scale because it feels familiar, but five-point scales are harder to use consistently than simpler scales. The difference between a three and a four is often unclear. Reviewers gravitate toward the middle to avoid seeming too harsh or too lenient. A four-point scale from zero to three removes the middle refuge and forces clearer distinctions. Zero is fail, one is weak, two is good, three is great. The meanings are unambiguous. Reviewers score faster and agree more often.

Anchors are examples that show what each score looks like. An anchor is not a definition. It is a specific instance. For a correctness dimension, the anchor for score zero might be "confidently states that the return policy allows refunds after ninety days when the actual policy is thirty days." The anchor for score two might be "correctly states the thirty-day return policy and notes that proof of purchase is required." The anchor for score three might be "correctly states the policy, includes the proof of purchase requirement, and notes the exception for defective items." Anchors turn abstract criteria into concrete standards.

Rules are the tiebreakers and hard gates that prevent edge cases from creating inconsistency. A rule might say "if the safety dimension fails, the overall score is fail regardless of other dimensions." Another rule might say "when a response is borderline between two scores, choose the lower score and note the reason in one sentence." Another rule might say "if the reviewer is uncertain whether a claim is grounded, mark it as ungrounded and flag for review." Rules close the gaps where subjective judgment would otherwise leak in.

## The Problem with Generic Dimensions

Most rubrics fail because the dimensions are too generic. Reviewers are told to score "helpfulness" or "quality" or "usefulness" without guidance on what those words mean in context. Helpfulness for a customer support bot is not the same as helpfulness for a research assistant. Quality for a chat system is not the same as quality for an agent. Usefulness varies by user goal. Generic dimensions invite reviewers to apply personal preferences. The rubric becomes a survey of opinions rather than a measurement of observable behavior.

The fix is to score observable behavior, not impressions. Instead of "was it helpful," ask "did it answer the user's question using only the allowed sources." Instead of "was it high quality," ask "did it provide accurate information with clear structure and no policy violations." Instead of "was it useful," ask "did it complete the task without requiring follow-up." Observable behavior can be verified. Different reviewers looking at the same output will see the same evidence. Impressions cannot be verified. Different reviewers will have different reactions.

This distinction matters most for the dimensions that determine whether an output is safe to ship. You cannot gate a release on helpfulness scores because helpfulness is subjective. You can gate a release on safety scores if safety is defined as "does not provide instructions for illegal activity, does not leak private information, does not bypass policy guardrails." The second definition is a checklist. The first is a feeling. Checklists scale. Feelings do not.

## The Universal Rubric Structure

There is a baseline rubric structure that works for most AI tasks. It has four core dimensions and optional extensions depending on task type. The core dimensions are correctness, relevance, clarity, and safety. Correctness measures whether the output is factually accurate and appropriate for the use case. Relevance measures whether the output addresses the user's actual goal. Clarity measures whether the output is structured and phrased in a way the user can act on. Safety is a hard gate that checks for policy violations, privacy leaks, and unsafe instructions.

Correctness uses a zero-to-three scale. Zero is fail: the output is wrong, misleading, or contradicts known facts. One is weak: the output is partly correct but has key errors or omissions. Two is good: the output is correct for the task with only minor gaps. Three is great: the output is fully correct and handles edge cases appropriately. The key is to define "correct" relative to the use case, not relative to general knowledge. For a support bot, correct means matching official documentation. For a domain assistant, correct means aligning with domain expertise.

Relevance uses a zero-to-three scale. Zero is fail: the output does not answer the question or goes off-topic. One is weak: the output partially addresses the goal but misses the main intent. Two is good: the output addresses the intent directly. Three is great: the output addresses the intent and anticipates the obvious next step without drifting. Relevance failures are common when the system misinterprets the user's goal or when it provides technically correct information that does not help the user.

Clarity uses a zero-to-three scale. Zero is fail: the output is confusing, hard to follow, or poorly structured. One is weak: the output has some clarity but is messy or too dense. Two is good: the output is clear, structured, and easy to act on. Three is great: the output is clear and formatted so well that the user can copy or follow it immediately. Clarity is a function of structure, word choice, and relevance to the user's immediate need. A correct, relevant answer can still fail on clarity if the user cannot extract the key information.

Safety is a pass-fail gate, not a score. Fail means the output provides unsafe instructions, leaks private information, includes disallowed content, or guides the user to bypass policy. Pass means the output handles these cases correctly and refuses or redirects when appropriate. Safety is binary because there is no acceptable middle ground. An output that is mostly safe but includes one privacy leak is a failure. An output that is completely safe but mediocre on other dimensions is still a pass on safety. The overall score depends on the aggregation rule, but safety failures always override other scores.

## Task-Specific Extensions

The universal structure covers most tasks, but some task types need additional dimensions. RAG systems need a grounding dimension that measures whether claims are supported by retrieved sources. Agents need dimensions for tool correctness and efficiency. Voice systems need dimensions for turn-taking and latency perception. These extensions follow the same pattern: define the dimension in terms of observable behavior, anchor it with examples, and integrate it into the scoring rules.

For RAG systems, the grounding dimension measures faithfulness to sources. Zero is fail: the output makes claims not supported by the retrieved documents. One is weak: the output has some grounding but mixes in unsupported claims. Two is good: the output is mostly grounded with only minor unsupported wording. Three is great: the output is fully grounded and cites sources correctly. The critical check is whether the reviewer can trace each claim back to a specific source. If the claim cannot be traced, it is ungrounded.

For agents, tool correctness measures whether the agent used tools appropriately. Zero is fail: the agent used the wrong tool, passed wrong parameters, or took an unsafe action. One is weak: the agent eventually reached correct tool use but with unnecessary retries or errors. Two is good: the agent used tools correctly with minor inefficiency. Three is great: the agent used a clean tool sequence with appropriate checks and idempotent behavior. Tool correctness is distinct from task completion. An agent can complete the task with messy tool usage or fail the task with correct tool usage but bad planning.

For agents, efficiency measures whether the agent minimized waste. Zero is fail: the agent looped unnecessarily or made excessive tool calls. One is weak: the agent was inefficient but completed the task. Two is good: the agent was reasonably efficient. Three is great: the agent used minimal steps with good defaults. Efficiency matters in production because wasted tool calls increase cost and latency. An agent that completes the task but burns ten times the necessary API calls is not production-ready.

For voice systems, turn-taking measures whether the agent handled interruptions and pacing correctly. Zero is fail: the agent talked over the user or ignored interruptions. One is weak: the agent handled interruptions inconsistently or awkwardly. Two is good: the agent handled turn-taking smoothly in most cases. Three is great: the agent handled barge-in gracefully and confirmed critical information naturally. Turn-taking cannot be scored from a transcript alone. It requires timestamps and, ideally, audio review. This makes voice rubrics more expensive to apply, but the dimension is not optional. Poor turn-taking breaks the user experience regardless of content quality.

## The Scoring Process

The scoring process determines whether the rubric is practical. A rubric that takes ten minutes per output is unusable at scale. A rubric that takes thirty seconds but produces inconsistent scores is also unusable. The target is two to three minutes per output with high agreement rates. Reaching that target requires clear instructions, fast-access anchors, and rules for common edge cases.

Reviewer instructions should fit on one page. Read the user request first. Score each dimension independently. Use the anchors, do not invent new criteria. When uncertain, choose the lower score and write the reason in one sentence. Do not penalize outputs for stylistic choices unless style affects clarity or safety. Do not reward outputs for politeness unless the task requires a specific tone. These instructions prevent the most common sources of drift: reviewers applying personal preferences, reviewers conflating dimensions, and reviewers inventing criteria not in the rubric.

The anchors should be embedded in the scoring interface, not in a separate document. Reviewers should not have to switch contexts to see examples. Each dimension should show the anchor examples inline. The examples should cover common cases and borderline cases. A borderline case is an output that sits between two score levels and could reasonably be scored either way. The anchor for borderline cases includes a tiebreaker rule: "when the output is borderline between two and three on clarity, score it as two if the user would need to reread any part to understand it."

The aggregation rule determines the overall score. The simplest rule is: if safety fails, the overall score is fail. Otherwise, the overall score is the average of the other dimensions, rounded down. Rounding down is conservative. It prevents borderline outputs from being scored as good when most dimensions are weak. Some teams use weighted averages, but weights add complexity and require justification. Start with equal weights. Add weighting only when you have strong evidence that one dimension matters more than others.

## Calibration: Making Consistency Repeatable

Calibration is the process of training reviewers to apply the rubric the same way. It is not optional. Without calibration, rubrics drift. Reviewers develop personal interpretations of the dimensions. Scores shift over time. Agreement rates drop. The data becomes noisy and eventually unusable. Calibration prevents this by creating a shared understanding of what each score means.

The calibration process has four steps. First, create a gold set of fifty examples: twenty easy cases, twenty normal cases, ten hard or edge cases. Second, have each reviewer score the gold set independently. Third, compare scores and discuss disagreements. Fourth, update the anchors and rules based on the patterns that caused disagreement. Repeat this cycle weekly until agreement rates stabilize above eighty-five percent, then move to monthly calibration to prevent drift.

The gold set is the shared reference that anchors the rubric. It should include examples of every score level for every dimension. It should include examples of common edge cases: outputs that are correct but unclear, outputs that are clear but wrong, outputs that are grounded but irrelevant, outputs that complete the task but use unsafe methods. The gold set is versioned with the rubric. When the rubric changes, the gold set is updated to reflect the new criteria.

Disagreements during calibration are not failures. They are signals that the rubric needs refinement. If two reviewers consistently disagree on a specific type of output, the rubric does not provide enough guidance for that case. The fix is to add an anchor or a rule. If reviewers agree on most cases but disagree on a few outliers, the fix is to add those outliers to the gold set with adjudicated scores. If reviewers disagree randomly, the dimension is too vague and needs to be rewritten in terms of observable behavior.

Adjudication is the process of resolving disagreements that calibration does not fix. When two reviewers score the same output differently, a senior reviewer adjudicates. The adjudication decision is recorded and added to the gold set. Adjudication rules handle systematic conflicts. If the safety score differs, the safety reviewer wins by default. If the correctness score differs by two or more points, escalate to a domain expert. If the disagreement is about interpretation of the rubric rather than the output, update the rubric. Adjudication is not a voting process. It is a refinement process.

## Preventing Rubric Rot

Rubrics decay over time. Reviewers get bored and start applying shortcuts. New types of outputs appear that the rubric does not cover. The task definition evolves but the rubric does not. The result is rubric rot: the rubric still exists, but it no longer measures what it was designed to measure. Preventing rot requires active maintenance.

Every time you see a new failure type, add it to the rubric. If the model starts making a new kind of error, add an anchor that shows what that error looks like and what score it should receive. If users start asking a new type of question, add examples of good and bad responses to that question type. If a policy changes, update the safety dimension to reflect the new requirements. The rubric is a living document. It evolves with the system.

Version the rubric explicitly. Use version numbers like v1, v1.1, v2. Record what changed in each version. When you update the rubric, rescore a sample of past outputs using the new version to verify that the changes produce the expected scoring patterns. Do not retroactively change scores on old outputs. The version history shows how your standards have evolved. Retroactive changes erase that history and make trends uninterpretable.

Run periodic audits of scorer behavior. Pull a random sample of scored outputs each month and check for drift. Look for patterns like "all recent scores are threes" or "correctness scores have dropped by half a point over the last six weeks." Drift is a signal that recalibration is needed or that the rubric no longer matches the task. Address drift immediately. Once drift becomes entrenched, it is hard to reverse without rescoring large datasets.

## Common Failure Modes and Fixes

Rubrics fail in predictable ways. Reviewers disagree because the dimensions are vague. Scoring is too slow because the rubric has too many dimensions or the instructions are too long. Scores drift because there is no calibration process. The rubric rewards style over substance because the dimensions are not tied to observable behavior. These problems are fixable.

If reviewers disagree, reduce the number of dimensions, simplify the wording, and add anchors. A rubric with four well-anchored dimensions will produce more consistent scores than a rubric with ten vague dimensions. If scoring is too slow, check the number of dimensions and the length of the instructions. The target is four to seven dimensions and one page of instructions. If scores drift, run calibration sessions and create a gold set. If the rubric rewards style, rewrite the dimensions to focus on observable outcomes and constraints rather than impressions.

The most insidious failure mode is gaming. Reviewers figure out what scores are expected and give those scores regardless of output quality. Models figure out what patterns the rubric rewards and produce those patterns regardless of task correctness. Gaming happens when the rubric is misaligned with the actual definition of "good." The fix is to tie the rubric back to the task definition from the previous subchapter. Every dimension should measure an outcome or constraint from that definition. If a dimension does not map to the definition, remove it.

## What the Rubric Enables

A well-designed rubric does more than produce scores. It creates a shared language for talking about quality. It makes training faster because new reviewers can see exactly what good looks like. It makes debugging faster because you can point to specific dimension scores and say "we are failing on grounding" or "we are weak on tool correctness." It makes automated evaluation possible because you can train a model to predict rubric scores or write code to check the constraints the rubric measures.

The rubric is also the foundation for release gates. You cannot set a quality threshold without a consistent way to measure quality. You cannot say "we require an average correctness score above 2.5" unless correctness is measured the same way every time. You cannot say "we allow zero safety failures" unless safety is a well-defined hard gate. The rubric turns subjective quality into objective criteria. That is what makes it usable for decisions.

When you build a rubric this way, you turn human review from an expensive bottlennel into a scalable quality signal. Reviewers score faster. Scores are consistent. The data is usable for training, debugging, and gating releases. The rubric becomes the bridge between your task definition and your evaluation infrastructure.

## Building Your First Rubric: The Practical Process

Building a rubric is not a solo activity. You need input from multiple perspectives: Product defines what matters to users, Domain Experts define what correctness means, Trust and Safety defines what safety violations look like, and reviewers who will actually use the rubric provide feedback on clarity and practicality. The process starts with the task definition from the previous subchapter and translates it into scoreable dimensions.

Begin by identifying the three to five most important aspects of quality for your task. These become your core dimensions. For most tasks, you will include correctness, relevance, and clarity. For tasks with high safety or compliance requirements, add safety as a hard gate. For RAG, add grounding. For agents, add tool correctness. For voice, add turn-taking. Do not add dimensions just because they seem important in the abstract. Add them only if failure on that dimension means the output is not acceptable.

For each dimension, write the zero-to-three scale definitions. Zero is fail. One is weak but not a complete failure. Two is good enough to ship. Three is excellent. The language should be specific to your task. For correctness in a support bot, zero might be "provides information that contradicts official documentation or policy," one might be "provides mostly correct information but omits critical details or includes minor errors," two might be "provides correct information with all critical details and only minor gaps in edge cases," and three might be "provides fully correct information, addresses edge cases appropriately, and anticipates likely follow-up questions."

Next, write anchors for each score level. Anchors are real or realistic examples of outputs at each level. For a support bot answering "what is your return policy," a score-zero anchor might be "you can return items anytime within 90 days" when the actual policy is 30 days. A score-two anchor might be "you can return items within 30 days with proof of purchase." A score-three anchor might be "you can return items within 30 days with proof of purchase, and we extend the return window to 60 days for defective items." The anchors show reviewers exactly what each score looks like.

Write the aggregation rule. The simplest rule is: if safety fails, the overall score is fail. Otherwise, average the other dimensions and round down. Write it explicitly. Do not assume reviewers will infer it. If you want to weight dimensions differently, specify the weights and justify them. If you want to use a different aggregation method, document it with examples.

Test the rubric with a small set of outputs. Have three reviewers score the same twenty outputs independently using the rubric. Calculate agreement rates. If agreement is below seventy-five percent, the rubric is too vague. Look for patterns in the disagreements. If reviewers disagree on specific types of outputs, add anchors for those cases. If reviewers disagree on the meaning of a dimension, rewrite it more concretely. If reviewers disagree randomly, the dimension may be measuring something subjective that cannot be made consistent.

Iterate until agreement is above eighty-five percent on the test set. This usually takes two to four rounds of refinement. Once agreement is stable, expand to a gold set of fifty examples and run formal calibration. The gold set should include easy cases where the correct score is obvious, normal cases that represent typical outputs, and hard cases where the score is debatable. The hard cases are where the rubric earns its value. If the rubric cannot produce consistent scores on hard cases, it will not scale.

## Handling Disagreements During Calibration

Disagreements are not errors. They are feedback. When two reviewers score the same output differently, you learn something about the rubric. If the disagreement is consistent across multiple outputs of the same type, the rubric lacks guidance for that type. If the disagreement is random, the dimension is too subjective. If the disagreement involves the boundary between two scores, you need a tiebreaker rule.

The calibration discussion is where the rubric gets refined. Gather the reviewers, show them the outputs where they disagreed, and ask them to explain their reasoning. Often, you will discover that they are interpreting the dimension differently. One reviewer thinks "clarity" means the response is well-written. Another thinks it means the user can act on the information immediately. Both are reasonable interpretations, but they lead to different scores. The fix is to make the dimension definition explicit: "clarity means the user can act on the information without needing to reread, look up terms, or seek additional context."

Sometimes the disagreement reveals a missing dimension. Reviewers agree on correctness and clarity but disagree on the overall score because some think the response is too long and others do not care about length. The fix is either to add an efficiency dimension if verbosity matters for your task, or to add a rule that says "do not penalize responses for length unless the extra content is irrelevant or confusing." The goal is to make all the implicit criteria explicit.

Sometimes the disagreement reveals ambiguity in the anchors. An anchor says "mostly correct" without defining what "mostly" means. One reviewer interprets it as "at least seventy-five percent correct." Another interprets it as "correct on the main point but wrong on details." The fix is to replace "mostly" with a concrete standard: "correct on the main claim but contains minor errors or omissions in supporting details."

Sometimes the disagreement is legitimate. The output genuinely sits on the boundary between two scores. When this happens, document it as a borderline case, add a tiebreaker rule, and include it in the gold set with the adjudicated score. The tiebreaker might be "when borderline, choose the lower score" or it might be "when borderline on clarity, score as two if the user would need to reread any part to understand it." The rule prevents future reviewers from agonizing over the same case.

## The Role of the Gold Set

The gold set is the shared reference that anchors the rubric over time. It is a versioned collection of fifty to one hundred examples with adjudicated scores. New reviewers are trained on the gold set. Calibration sessions use the gold set to check for drift. When the rubric is updated, the gold set is rescored to verify that the changes have the intended effect.

The gold set should represent the distribution of real outputs your system produces. If seventy percent of real outputs are straightforward and score two or three, seventy percent of the gold set should be straightforward. If ten percent of real outputs are edge cases that score zero or one, ten percent of the gold set should be edge cases. If five percent of real outputs trigger the safety gate, five percent of the gold set should trigger it. The gold set is not a showcase of perfect outputs. It is a representative sample that includes common failures.

The gold set evolves as the system evolves. When a new failure mode appears in production, add examples of that failure mode to the gold set with the correct scores. When the product adds a new feature, add examples of outputs using that feature. When policies change, update the gold set to reflect the new standards. The gold set is a living artifact. It grows with the system.

Version the gold set the same way you version the rubric. When you add or remove examples, increment the version number. When you rescore examples due to a rubric change, increment the version number. Keep the history. The version history shows how your standards have evolved and makes it possible to reinterpret past evaluation results in light of current standards.

## Scaling Rubrics Across Teams

When you have multiple teams evaluating different features or different parts of the same system, you need rubric consistency across teams. The worst case is when each team invents its own rubric with different dimension names, different scales, and different standards. Scores from different teams cannot be compared. You cannot roll up quality metrics across the organization. You cannot transfer reviewers between teams without retraining.

The fix is to define a universal rubric structure that all teams use as a baseline. The universal structure includes the core dimensions that apply to most tasks: correctness, relevance, clarity, and safety. Each team extends the universal structure with task-specific dimensions as needed. RAG teams add grounding. Agent teams add tool correctness and efficiency. Voice teams add turn-taking. But the core dimensions remain consistent.

The universal structure also defines the scale: zero to three, with zero as fail, one as weak, two as good, and three as great. It defines the aggregation rule: safety failures override all other scores. It defines the calibration process: gold set of fifty examples, agreement threshold of eighty-five percent, monthly calibration to prevent drift. These standards make it possible to compare scores across teams and to move reviewers between projects without starting from scratch.

Shared anchors help. If multiple teams are evaluating chat systems, they can share anchors for the core dimensions even if their task-specific requirements differ. A support bot team and a sales assistant team both need anchors for correctness, relevance, and clarity. The specific examples will differ, but the structure is the same. Sharing anchors reduces duplicated work and increases consistency.

Centralized calibration helps. Run quarterly calibration sessions where reviewers from different teams score the same set of cross-functional examples. This prevents drift at the organizational level. It also surfaces differences in interpretation early, before they become entrenched. If reviewers from two teams consistently score safety differently, you need to align on the safety standard across the organization.

## Rubrics for Model-Graded Evaluation

Once you have a stable human rubric, you can train a model to predict rubric scores. This is the bridge from human evaluation to automated evaluation. You collect a few hundred to a few thousand examples scored by humans using the rubric. You use those scores as training data to fine-tune a model or to build a prompt that instructs a model to score outputs the same way humans do.

Model-graded evaluation is not a replacement for human evaluation. It is a scaling mechanism. Human evaluation defines the standard. Model-graded evaluation applies that standard at higher volume and lower cost. You use human evaluation to build the rubric, create the gold set, and validate that the model-graded scores align with human scores. You use model-graded evaluation to score thousands or tens of thousands of outputs during development and regression testing.

The quality of model-graded evaluation depends entirely on the quality of the rubric. If the rubric is vague, the model will learn the vagueness and produce inconsistent scores. If the rubric conflates multiple concepts in one dimension, the model will conflate them too. If the rubric drifts over time, the model will learn the drift and amplify it. The rubric must be clean before you can automate it.

Validating model-graded scores is not optional. You cannot assume the model is scoring correctly just because it was trained on human scores. You must sample model-graded outputs regularly and have humans rescore them. You calculate agreement between the model and the humans. If agreement drops below eighty percent, you investigate. The model may have learned spurious patterns. The rubric may have drifted. The distribution of outputs may have changed. You fix the problem, retrain or reprompt the model, and validate again.

Model-graded evaluation works best for dimensions that are objective and well-anchored. Correctness can be model-graded if you have clear anchors and enough training data. Grounding can be model-graded if you log the retrieved documents and the model can check whether claims are supported. Safety can be model-graded if the safety categories are explicit and you have many examples of each. Clarity is harder to model-grade because it depends on subtle aspects of structure and phrasing. Efficiency for agents can be model-graded if you log the tool calls and define what counts as wasteful.

## The Rubric as a Product Artifact

The rubric is not just an internal tool. It is a product artifact that defines what your system promises to deliver. When you ship a feature, you are implicitly promising that it meets the standards in your rubric. When you set a release gate at "average correctness above 2.5," you are promising users that the system is correct in the way your rubric defines correctness. When you allow zero safety failures, you are promising that the system will not violate the safety constraints in your rubric.

This means the rubric must reflect real user needs and real business requirements. It cannot be an academic exercise. It cannot define standards that are impossibly high or irrelevant to users. It must balance quality, cost, and speed. A rubric that requires every output to score three on every dimension is not practical. A rubric that allows scores of one on correctness is not acceptable for most production systems. The rubric defines the bar. The bar must be high enough to protect users and the business, and achievable enough to ship.

The rubric also defines your regression tests. When you update the model, change the prompt, or modify the retrieval system, you rescore a sample of outputs using the rubric. If scores drop, you have a regression. If scores stay the same or improve, you have validated the change. The rubric makes regressions measurable and makes improvements verifiable. Without the rubric, you are flying blind.

When you build a rubric this way, you turn human review from an expensive bottleneck into a scalable quality signal. Reviewers score faster. Scores are consistent. The data is usable for training, debugging, and gating releases. The rubric becomes the bridge between your task definition and your evaluation infrastructure. In the next subchapter, we cover how to turn those scores into datasets that systematically test your system's behavior.
