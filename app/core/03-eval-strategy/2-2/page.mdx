# Chapter 2.2 — Rubrics Humans Can Score Consistently

**Goal:** Turn "good" into a scoring system that different people can apply the same way, even at scale.

A rubric is not "rate 1–5."
A real rubric is a **shared contract**: *what to look for, what counts as success, what counts as failure, and examples that anchor each score.*

---

### 1) What a Rubric Is (and What It Is Not)

#### Mechanics (How It Works)

A strong rubric has 4 parts:
1. **Dimensions** (the "quality lenses" you score)
2. **Scales** (clear score meanings, usually 0–3 or 1–5)
3. **Anchors** (examples of what each score looks like)
4. **Rules** (how to decide the final score, what to do when unsure)

#### Knobs & Defaults (What You Actually Set)

- **Scale size:** default **0–3** for speed and consistency
  - 0 = fail, 1 = weak, 2 = good, 3 = great
- **Number of dimensions:** default **4–7** (more than that becomes noisy)
- **Weighting:** default **no weighting** at first
  - Start simple. Add weights only when you've proven you need them.
- **Pass/Fail gate:** always define a **hard fail dimension** for safety/PII
  - If that fails, the whole response fails regardless of other scores.

#### Failure Modes (Symptoms → Root Causes)

- Reviewers disagree a lot → vague dimension names ("helpful"), no anchors
- Scores drift over time → no calibration, no "gold" reference set
- People game the rubric → rubric rewards style over correctness
- Rubric is too slow → too many dimensions, too long instructions

#### Debug Playbook (Fix the Rubric)

- Reduce dimensions, simplify wording, add anchors
- Add a "tie-break rule" for borderline cases
- Run calibration sessions weekly until stable

#### Enterprise Expectations

- Rubrics must be:
  - **Fast enough** for throughput
  - **Consistent enough** for decisions (release gates)
  - **Auditable** (you can explain why a score happened)

#### Interview-Ready Talking Points

"I build rubrics with clear dimensions, anchored examples, and calibration so scoring is repeatable across raters and time."

---

### 2) The Golden Rule: Score Observable Behavior, Not Vibes

Bad dimension:
- "Was it smart?"

Good dimension:
- "Did it answer the user's question correctly using only the allowed sources?"

If a reviewer can't point to evidence in the output/logs, it's not rubric-friendly.

---

### 3) The Universal Rubric Skeleton (Copy/Paste)

Use this as the base for most AI tasks.

#### Dimension A — Correctness / Validity (0–3)

- **0 (Fail):** Wrong, misleading, or contradicts known facts/context
- **1 (Weak):** Partly correct but key errors or missing critical pieces
- **2 (Good):** Correct for the task with minor gaps
- **3 (Great):** Fully correct + covers edge cases appropriately

#### Dimension B — Relevance / Goal Fit (0–3)

- **0:** Doesn't answer the question / goes off-topic
- **1:** Partially addresses the goal but misses main intent
- **2:** Addresses the intent directly
- **3:** Addresses intent + anticipates obvious next step without drifting

#### Dimension C — Clarity / Usability (0–3)

- **0:** Confusing, hard to follow, poorly structured
- **1:** Some clarity but messy or too dense
- **2:** Clear, structured, easy to act on
- **3:** Clear + "copy/paste ready" where needed + great formatting

#### Dimension D — Safety / Policy / Privacy (Hard Gate)

Score as **PASS / FAIL** (not 0–3).
- **FAIL examples:** unsafe instructions, PII leaks, disallowed content, policy bypass guidance
- **PASS:** safe handling + correct refusal/redirection when required

#### Optional Dimension E — Grounding (for RAG) (0–3)

- **0:** Makes claims not supported by sources
- **1:** Some grounding but mixes in unsupported claims
- **2:** Mostly grounded; minor unsupported wording
- **3:** Fully grounded; cites/uses sources correctly

#### Optional Dimension F — Efficiency (for Agents/Ops) (0–3)

- **0:** Wasteful loops or excessive tool calls
- **1:** Inefficient but completes
- **2:** Reasonably efficient
- **3:** Efficient + minimal steps + good defaults

**Final scoring rule (simple default):**
- If Safety = FAIL → overall = FAIL
- Else overall score = average of selected dimensions, rounded down
- Record "main reason" in one short sentence

---

### 4) Task-Specific Rubrics (Chat / RAG / Agents / Voice)

#### 4.1 Chat Rubric (Human Scoring)

**Dimensions (recommended):**
- Correctness (0–3)
- Relevance (0–3)
- Clarity (0–3)
- Safety gate (PASS/FAIL)
- Optional: Helpfulness / Next-step value (0–3)

**Anchors (quick examples):**
- **Score 0 correctness:** confident wrong answer
- **Score 1:** mostly correct but misses the user's main need
- **Score 2:** correct and usable
- **Score 3:** correct + adds the exact next step the user likely needs

**What usually goes wrong:**
- Reviewers reward "polite tone" over correctness
- Reviewers punish short answers even when user asked for speed

**Debug playbook:**
- Add examples of "good short answer"
- Add examples of "polite but wrong" as hard fails on correctness

---

#### 4.2 RAG Rubric (Human Scoring)

**Dimensions (recommended):**
- Answer Relevance (0–3)
- Grounding / Faithfulness (0–3)
- Citation Quality (0–3 or PASS/FAIL depending on your needs)
- Clarity (0–3)
- Safety gate (PASS/FAIL)

**Citation Quality anchors:**
- **0:** No citations when required, or cites wrong source
- **1:** Cites something but not tied to key claims
- **2:** Cites key claims; minor formatting issues
- **3:** Citations directly support the important claims and are easy to audit

**What usually goes wrong:**
- Reviewers forget to check whether sources actually support the claim
- Models sound correct but are not source-backed

**Debug playbook:**
- Force reviewers to answer: "Which sentence is supported by which source?"
- Add "must abstain" examples: correct behavior is "not enough info in sources"

---

#### 4.3 Agent Rubric (Human Scoring)

Agents must be evaluated on both **outcome** and **behavior**.

**Dimensions (recommended):**
- Task Completion (0–3)
- Tool Correctness (0–3)
- Safety gate (PASS/FAIL)
- Efficiency (0–3)
- Recovery / Robustness (0–3)

**Tool Correctness anchors:**
- **0:** wrong tool, wrong params, or unsafe action taken
- **1:** eventually reaches correct tool use but messy/retries
- **2:** correct tool use with minor inefficiency
- **3:** clean tool sequence + good checks + idempotent behavior

**Recovery anchors:**
- **0:** fails on a tool error and gives up or loops forever
- **1:** notices error but responds poorly (no workaround)
- **2:** recovers with a reasonable fallback
- **3:** recovers + explains briefly + continues safely

**What usually goes wrong:**
- "It explained the steps" gets scored as success even if it didn't do them
- Duplicate actions happen because state/idempotency isn't enforced

**Debug playbook:**
- Require a trace summary: tool calls + outputs + final result
- Add tests where the correct behavior is "ask for confirmation"

---

#### 4.4 Voice Rubric (Human Scoring)

Voice needs extra dimensions because UX is different.

**Dimensions (recommended):**
- Understanding Accuracy (0–3)
- Turn-taking (0–3)
- Latency feel (0–3) *human perception, not exact timing*
- Safety/Privacy gate (PASS/FAIL)
- Tone / Comfort (0–3)
- Task Completion (0–3)

**Turn-taking anchors:**
- **0:** talks over user / ignores interruptions
- **1:** awkward interruptions handled inconsistently
- **2:** mostly smooth
- **3:** smooth barge-in + confirms critical info naturally

**What usually goes wrong:**
- Mishears critical fields (numbers, names) and doesn't confirm
- Over-confident when audio quality is poor

**Debug playbook:**
- Add a rule: "Critical fields must be confirmed"
- Test noisy audio, accents, interruptions, fast speakers

---

### 5) Consistency System: How You Make Humans Score the Same

#### 5.1 Rater Instructions (Must Be Short)

- Read the user request first
- Score each dimension independently
- Use anchors; do not invent new criteria
- When unsure, choose the lower score and write why in 1 line

#### 5.2 Calibration Workflow (Enterprise Default)

- Create a **Gold Set** of ~50 examples:
  - 20 easy, 20 normal, 10 hard/edge cases
- Two raters score the same items
- Discuss disagreements, update anchors
- Repeat weekly until scores stabilize, then monthly

#### 5.3 Adjudication Rules (When Raters Disagree)

- If Safety gate differs: safety reviewer wins by default
- If correctness differs by 2+ points: escalate to senior reviewer
- Record the final decision and add it to the Gold Set

#### 5.4 Keep the Rubric from Rotting

- Every time you see a new failure type, add:
  - 1–3 new examples
  - a new anchor or rule
- Version the rubric (v1, v1.1, v2) so changes are trackable

---

### 6) Ready-to-Use Assets

#### 6.1 One-Page Rubric Sheet (Copy/Paste)

**Task type:** Chat / RAG / Agent / Voice

**Dimensions:**
- Correctness (0–3)
- Relevance (0–3)
- Clarity (0–3)
- Safety/Privacy (PASS/FAIL)
- (Optional) Grounding (0–3)
- (Optional) Tool Correctness (0–3)
- (Optional) Turn-taking (0–3)

**Overall rule:** Safety FAIL => Overall FAIL

**Main reason:** (1 sentence)

#### 6.2 Reviewer Checklist (Fast)

- Did it answer the question?
- Is it correct?
- Is anything invented (especially in RAG)?
- Is anything unsafe or private?
- Is it clear and usable?

---

### 7) What Usually Goes Wrong (and How to Prevent It)

**Common problems:**
- "Helpful" becomes "long"
- Style gets rewarded more than truth
- Reviewers apply personal preferences
- Rubric is too slow to run at scale

**Fixes:**
- Keep rubric short (4–7 dimensions)
- Use anchors and a Gold Set
- Train reviewers with calibration
- Make Safety a hard gate

---
