# Chapter 2.3 — Good vs Bad Patterns (Examples)

**What we're doing here:**
Now that we can *define* "good" (2.1) and *score* it (2.2), we need examples people can learn from fast.
This chapter gives **repeatable patterns**: what strong outputs look like, what weak outputs look like, and why.

**How to use this chapter in real teams:**
- Put these examples in your "Gold Set" for calibrating reviewers
- Use them in onboarding for new raters and new engineers
- Use them as "unit tests" for prompts, tools, and policies

---

### 1) Chat Patterns (General Assistant)

#### Pattern: Direct + Structured Answer

**Good:**
- Starts with the answer (or the next action)
- Uses short steps or bullets
- Matches the user's requested style (short vs detailed)

**Bad:**
- Long preamble
- Explains the world before answering
- Adds unnecessary assumptions

**Why it matters:**
- Users judge quality by "Did it help me now?" not "Was it fancy?"

**Mini example:**
User: "Write a short LinkedIn message to a recruiter asking for the JD."
- Good: 4–6 lines, clear ask, friendly tone, no fluff
- Bad: 3 paragraphs about your background before asking

---

#### Pattern: Honest Uncertainty (Without Being Useless)

**Good:**
- States what is known vs unknown
- Offers a safe next step: "Here's how to verify"
- Gives best-effort help without pretending certainty

**Bad:**
- Confident guessing
- Or the opposite: "I can't help" when you clearly can

**What usually goes wrong:**
- Models learn "confidence sounds good"
- Reviewers reward confident tone even when wrong

---

#### Pattern: Clarify Only When It Truly Blocks Progress

**Good:**
- Makes a reasonable assumption and proceeds
- Asks 1–2 key questions only if necessary

**Bad:**
- Asks 5 questions before doing anything
- Or never asks and outputs something unusable

**Debug tip:**
- Track "clarification rate" and whether clarifications were actually needed

---

### 2) RAG Patterns (Retrieval + Grounded Answers)

#### Pattern: "Answer Only from Sources" Behavior

**Good:**
- Uses only retrieved context
- Clearly separates: "What sources say" vs "What we don't know"
- Cites key claims when required

**Bad:**
- Adds external facts not in the documents
- Uses citations that do not support the claim (fake grounding)

**Mini example:**
Question: "What is our refund policy for annual plans?"
- Good: quotes/paraphrases exact policy language and cites it
- Bad: invents "30-day refund" because it's common elsewhere

---

#### Pattern: Correct Abstain When Evidence Is Missing

**Good:**
- "I don't have enough info in the provided documents to answer."
- Suggests what doc/field is needed, or offers to search again with a better query

**Bad:**
- Makes up the policy anyway
- Or blames the user: "Your question is unclear" when it's not

**What usually goes wrong:**
- Teams forget to include "must abstain" cases in eval datasets — so the model never learns that abstaining can be "correct."

---

#### Pattern: Retrieval Failure Diagnosis (Don't Blame the Model Too Early)

**Good:**
- When answer is bad, checks:
  - Did we retrieve the right chunks?
  - Is chunking too big or too small?
  - Do we need reranking?
  - Are filters blocking relevant docs?

**Bad:**
- Prompt-tweaks forever while retrieval is the real issue

---

### 3) Agent Patterns (Tools + Multi-Step)

#### Pattern: Plan → Act → Verify → Finish

**Good:**
- Brief plan (1–3 steps)
- Executes tools
- Verifies outputs
- Declares done with a clear result

**Bad:**
- Endless planning, no execution
- Executes but never verifies
- Stops mid-way without saying what happened

**Mini example:**
Task: "Find the latest invoice and summarize line items."
- Good: searches → opens invoice → extracts items → summarizes → links/IDs
- Bad: explains how to find invoices but doesn't do it

---

#### Pattern: Safe Confirmation Before Irreversible Actions

**Good:**
- "I'm about to send an email / delete data / charge a card. Confirm?"
- Uses least privilege
- Uses idempotency keys for actions that can be duplicated

**Bad:**
- Takes action without user confirmation
- Repeats action due to retries and causes duplicates

**What usually goes wrong:**
- Tool retries without idempotency
- State not stored (agent forgets it already sent something)

---

#### Pattern: Graceful Tool Failure Recovery

**Good:**
- Detects tool error
- Retries with backoff (limited)
- Switches to fallback approach
- If blocked, escalates clearly

**Bad:**
- Loops tool calls
- Pretends it succeeded
- Dumps raw error logs without interpretation

---

### 4) Voice Patterns (Real-Time Calls)

#### Pattern: Confirm Critical Fields (Always)

Critical fields: names, phone numbers, dates, addresses, amounts.

**Good:**
- "Just to confirm, that's 0-1-2-… correct?"
- Uses natural confirmation, not robotic repetition

**Bad:**
- Assumes it heard correctly
- Doesn't confirm when audio is noisy

**Why it matters:**
- Voice errors are high-impact and feel "creepy" when wrong

---

#### Pattern: Smooth Turn-Taking (Barge-In)

**Good:**
- Stops speaking when user interrupts
- Responds to the interruption, not the old plan

**Bad:**
- Talks over user
- Ignores interruption and continues script

---

#### Pattern: Fast, Calm, Human Pacing

**Good:**
- Short sentences
- Small pauses
- Doesn't read long paragraphs

**Bad:**
- Long monologues
- Slow pipeline responses that feel like awkward silence

---

### 5) "Gold Examples" Pack (Ready to Build Your Calibration Set)

Create a shared folder/page with:
- 10 examples of "great"
- 10 examples of "acceptable"
- 10 examples of "fail"

For each example, store:
- The user request
- The system output
- The rubric scores
- 1–2 sentences: "why this is scored that way"

This becomes your reviewer training kit and your regression baseline.

---

### 6) What Usually Goes Wrong (Across All Task Types)

- Teams confuse "sounds good" with "is correct"
- No abstain cases, so the model learns to guess
- Reviewers score based on style preferences
- Agents are judged on text quality instead of action correctness
- RAG bugs get misdiagnosed as model bugs

---

### 7) Debug Playbook (Quick)

When quality drops:
1. Identify task type: Chat, RAG, Agent, Voice
2. Check the main failure dimension:
   - Correctness? Relevance? Grounding? Tool correctness? Turn-taking?
3. Localize:
   - Retrieval vs generation (RAG)
   - Tool layer vs planning layer (agents)
   - ASR vs reasoning vs TTS (voice)
4. Add 3–10 new gold examples that represent the failure
5. Add them to regression tests

---
