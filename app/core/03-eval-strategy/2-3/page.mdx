# 2.3 â€” Good vs Bad Patterns (Examples)

In early 2025, a healthcare technology company deployed an AI assistant to help 400 clinical staff draft patient communication. The prompt engineering team had spent three months building elaborate instructions. The rubrics were detailed. The acceptance criteria were agreed upon. But within two weeks, the nursing team stopped using it entirely. The reason surfaced in a retrospective: nobody could explain what "good" looked like without showing real examples. The rubric said "clear, helpful, appropriate tone" but every nurse interpreted that differently. One nurse thought formality signaled professionalism. Another thought warmth mattered most. A third prioritized brevity above all. The evaluation scores were all over the place, and the model learned nothing coherent. The root cause was not the rubric itself. The root cause was the absence of concrete, shared reference examples that calibrated human judgment before scoring began.

This is the role of patterns. You can define quality in the abstract, and you can score it with rubrics, but you cannot calibrate reviewers or train models without showing them what good and bad actually look like in practice. Patterns are the shared reference set that turns subjective judgment into consistent evaluation. They are the examples you put in front of every new reviewer, every new engineer, every new stakeholder to say: this is what we mean by good, this is what we mean by bad, and this is why. They become your unit tests for prompts, your calibration set for raters, your regression baseline for model updates. Without them, evaluation becomes opinion. With them, it becomes repeatable craft.

The challenge is that most teams collect patterns reactively, after failures accumulate. They wait until quality drops, then scramble to identify what went wrong, then document the failure mode as an example. This approach is backwards. You need patterns before you start evaluating, not after. You need them during the requirements phase when you define what success looks like. You need them during the rubric design phase when you calibrate what each score means. You need them during reviewer training when you teach people how to apply the rubric consistently. And you need them during regression testing when you verify that changes did not break existing quality. Patterns are not documentation. They are the foundation of your evaluation system.

## Chat Patterns: Direct, Structured, Honest

The most common AI task is conversational assistance. The user asks a question or makes a request, and the model responds. Good chat outputs share three characteristics: they start with the answer or the next action, they match the user's requested style and level of detail, and they handle uncertainty honestly without becoming useless. Bad chat outputs do the opposite. They open with preambles. They explain the world before helping. They add unnecessary assumptions or hedge so much they provide no value. The difference is not stylistic preference. The difference is whether the output helps the user make progress now.

Start with the answer. If the user asks how to do something, the first sentence should tell them what to do or ask one clarifying question if the request is genuinely ambiguous. If the user asks for a draft, the first paragraph should be the draft, not an explanation of what a draft is. If the user asks for a decision, the first line should state the recommendation and the reasoning can follow. This is not about being curt. This is about respecting the user's time and cognitive load. Users judge quality by whether the output helped them in the moment, not by whether it sounded impressive.

The failure mode of starting with preambles is pervasive across models. Claude Opus 4.5 and GPT-5 both have a tendency to open with context-setting paragraphs when the user asked for action. A user requests a LinkedIn outreach message, and the model responds with three sentences about the importance of personalization before drafting anything. This is trained behavior. The models were fine-tuned on examples where reviewers rewarded thoroughness and explanation. The fix is to create gold examples that demonstrate frontloading the answer, and to penalize preambles explicitly in your rubric. You want the first sentence to either answer the question, state the next step, or ask one clarifying question if truly needed. Nothing else.

Structure matters for scannability. If the answer involves steps, use short numbered steps or prose enumeration. If the answer involves options, present them clearly with brief explanations. If the answer involves a decision, state the recommendation first and then the rationale. The goal is to let the user extract what they need without reading every word. Bad outputs bury the answer in paragraphs of setup. Good outputs surface the answer immediately and provide depth only where the user needs it. This principle applies whether the user requested a short answer or a detailed one. Short answers should still be structured. Detailed answers should still lead with the core point.

Testing structure is straightforward. Take a sample of outputs and measure how many lines or paragraphs a user must read before encountering the actionable content. If the answer is in the third paragraph, the output is poorly structured. If the answer is in the first sentence, the structure is good. You can instrument this by having reviewers mark where the actionable content starts, and then calculating the median position across outputs. If the median is beyond the first paragraph, you have a structure problem. The fix is to add examples that demonstrate answer-first structure and to train the model on those examples explicitly.

Uncertainty must be handled honestly but constructively. When the model does not have enough information to answer confidently, it should say so, state what is known versus unknown, and offer a safe next step such as how to verify the answer or where to find the missing information. This is honest uncertainty. Bad uncertainty is either confident guessing, where the model invents details and presents them with certainty, or useless hedging, where the model says it cannot help when it clearly can provide some value. The correct behavior is to give best-effort help while being transparent about the confidence level.

For example, if a user asks for a LinkedIn message to a recruiter, and the model does not know the user's background, the good response is to draft a message with placeholders and note that the user should personalize specific details. The bad response is to refuse entirely or to invent a fake background and present it as fact. The good pattern balances helpfulness with honesty. The message is usable immediately, the placeholders are clearly marked, and the user knows exactly what to customize. This pattern applies across domains. Medical advice with uncertainty should state what is known, what is not, and recommend consulting a professional for the unknowns. Legal advice with uncertainty should provide general principles and recommend consulting a lawyer for specifics. Financial advice with uncertainty should explain the variables and recommend professional guidance for personalized decisions.

Clarification should be used sparingly and only when it truly blocks progress. If the model can make a reasonable assumption and proceed, it should do so and state the assumption. If the ambiguity is genuinely high-impact, the model should ask one or two key questions, not five. The failure mode here is over-asking, where the model interrogates the user instead of helping, or under-asking, where the model proceeds with an assumption that makes the output unusable. The debug signal is clarification rate: if more than twenty percent of responses ask clarifying questions, something is miscalibrated. Track whether those clarifications were actually necessary by checking if the user's answer changed the output meaningfully. If not, the model should have proceeded with a default assumption.

The pattern library for chat should include at least ten examples of each scenario: answer-first structure, honest uncertainty, constructive clarification, and appropriate tone matching. Each example should show the user input, the model output, and a one-sentence explanation of why it is good or bad. These examples are what you use to train reviewers, to regression-test prompt changes, and to fine-tune the model if you go that route. Without concrete examples, your rubric definitions will drift as different reviewers interpret them differently.

## RAG Patterns: Grounding, Abstaining, Diagnosing

Retrieval-augmented generation introduces a new quality dimension: grounding. The model must answer only from the retrieved sources, not from its training data. Good RAG outputs clearly separate what the sources say from what is unknown. They cite key claims when required by policy. They abstain cleanly when the evidence is missing. Bad RAG outputs add external facts not present in the documents, use citations that do not support the claim, or invent details because the model was trained to sound helpful even when it should admit ignorance.

Answer only from sources. This is the core contract of RAG. If the user asks what the refund policy is, and the retrieved documents contain the policy, the model should quote or paraphrase the exact language and cite the source. If the retrieved documents do not contain the policy, the model must not invent one. The failure mode is hallucinated grounding, where the model generates a plausible-sounding policy, appends a citation, and presents it as fact. This destroys trust faster than any other error because it looks authoritative. The fix is to penalize unsupported claims heavily in evaluation and to include abstain-required cases in the training data so the model learns that saying "I do not have enough information" is sometimes the correct answer.

Testing grounding requires instrumentation. You need to log what documents were retrieved, what chunks were passed to the model, and what claims appeared in the final output. Then you need a reviewer or an automated check to verify that each claim is supported by the retrieved chunks. If a claim appears in the output but not in the chunks, that is a grounding failure. If a citation is attached to a claim that does not appear in the cited source, that is a grounding failure. If the model says "according to the documentation" but the statement is from the model's training data rather than the retrieved documents, that is a grounding failure. You cannot test grounding without this instrumentation. Grounding is not a property you can judge by reading the output alone. You must compare the output to the sources.

Abstaining correctly when evidence is missing is a learnable skill. The model should say clearly that it does not have the information in the provided documents, suggest what document or field is needed, and offer to search again with a better query if that capability exists. This is constructive abstention. Bad abstention is either refusing to help at all or blaming the user by saying the question is unclear when the question is perfectly clear but the retrieval failed. The root cause of poor abstention behavior is usually that teams forget to include must-abstain cases in their evaluation datasets. If every example in the training set has a correct answer present in the sources, the model never learns that abstaining can be scored as good. You must include cases where the correct behavior is to say "I cannot answer this from the provided context" and score those responses as perfect.

The abstain pattern should be encoded explicitly. When documents do not contain the answer, the good response template is: state what was searched, state that the answer was not found, suggest what document or query might help, and offer to try again if the user provides more context. This four-part structure gives the user transparency, maintains trust, and keeps the conversation moving forward. The bad pattern is to either hallucinate an answer or to say "I cannot help" without any guidance on next steps. Both failures make the system feel broken. The good pattern makes the system feel honest and collaborative.

Retrieval failure must be diagnosed separately from generation failure. When a RAG answer is bad, the first question is whether the right chunks were retrieved. If the answer is in the corpus but the retrieval layer did not surface it, no amount of prompt tuning will fix the problem. Check whether chunking is too large or too small. Check whether the embedding model understands domain-specific terminology. Check whether filters are blocking relevant documents. Check whether reranking would help. This diagnostic step is often skipped, and teams waste weeks tweaking prompts while the real issue is that the retrieval layer is fundamentally broken. The pattern is: retrieval first, generation second. If retrieval is wrong, generation cannot save you.

Diagnosing retrieval requires logging the full pipeline. For every query, log the user question, the query sent to the retrieval system, the documents returned, the chunks selected, and the final generated answer. Then when quality drops, you can trace backwards. If the query was malformed, the problem is query generation. If the query was good but the wrong documents were returned, the problem is retrieval ranking. If the right documents were returned but the wrong chunks were selected, the problem is chunking or reranking. If the right chunks were selected but the answer was still wrong, the problem is generation. This level of visibility is what separates teams that debug RAG failures in hours from teams that debug them in weeks.

## Agent Patterns: Planning, Confirmation, Recovery

Agents combine language models with tools and multi-step execution. Good agent outputs follow a predictable flow: brief plan, tool execution, verification, clear finish. Bad agent outputs fall into one of three traps: endless planning with no action, execution without verification, or stopping mid-task without communicating status. The quality bar here is not the sophistication of the plan. The quality bar is whether the agent completed the task correctly and communicated what happened.

Plan, act, verify, finish. The plan should be one to three steps, not a ten-step dissertation. The user does not need to see the agent's entire reasoning process unless debugging is enabled. The agent should execute the necessary tools, verify that the outputs make sense, and then declare done with a clear result. For example, if the task is to find the latest invoice and summarize line items, the good pattern is: search for invoices, open the most recent one, extract line items, summarize them, provide the invoice ID or link. The bad pattern is to explain how one might find invoices without actually doing it, or to retrieve the invoice but stop before summarizing, or to summarize without verifying that the invoice is actually the latest.

The failure mode of over-planning is common in agentic systems built on Claude and GPT-4. The models generate elaborate multi-step plans with contingencies and edge cases, then never execute them. This happens because the models were trained to show their reasoning, and reviewers rewarded detailed explanations. The fix is to create examples where the agent plans briefly, acts immediately, and only shows reasoning if explicitly requested by the user. The goal is task completion, not demonstration of planning capability.

Confirmation before irreversible actions is not optional. If the agent is about to send an email, delete data, charge a payment card, or modify production state, it must ask for explicit user confirmation first. This is a hard rule. The agent should state clearly what it is about to do, show the key details such as recipient or amount, and wait for a yes or no. The failure mode is acting without confirmation, which can cause real damage. The second failure mode is acting multiple times due to retries without idempotency, which causes duplicate emails, duplicate charges, or cascading errors. The fix is to use idempotency keys for any action that can be duplicated and to store state so the agent remembers it already took the action.

The confirmation pattern should be standardized across all high-risk actions. The template is: state the action type, show the key parameters, ask for confirmation, wait for response, execute only if confirmed, log the confirmation and the result. This structure prevents accidental execution and provides an audit trail. It also gives the user control, which is critical for trust in agentic systems. Users tolerate agents that ask for confirmation. Users do not tolerate agents that take irreversible actions autonomously.

Graceful tool failure recovery separates production-ready agents from prototypes. Tools fail. APIs return errors. Rate limits are hit. The agent must detect the error, retry with exponential backoff up to a limit, switch to a fallback approach if the primary tool is unavailable, and escalate clearly if the task cannot be completed. Bad recovery is looping the same tool call forever, pretending the tool succeeded when it failed, or dumping raw error logs into the user-facing response. Good recovery is transparent: the agent says what went wrong, what it tried, and what the user should do next. This requires instrumentation. You need logs that show tool calls, tool results, retry counts, and fallback triggers. Without instrumentation, debugging agent failures is guesswork.

The recovery pattern library should include examples of each common failure scenario: tool timeout, tool error, missing permissions, rate limit hit, invalid parameters, unexpected response format. For each scenario, the pattern should show how the agent detects the failure, what retry logic it applies, what fallback it uses, and how it communicates the outcome to the user. These patterns become the regression tests for your agent system. Every time you change the tool layer or the planning layer, you run these patterns through the system and verify that recovery still works correctly.

## Voice Patterns: Confirmation, Turn-Taking, Pacing

Voice interfaces introduce latency, transcription errors, and real-time interaction dynamics that text interfaces do not have. Good voice outputs confirm critical fields, handle interruptions smoothly, and maintain natural pacing. Bad voice outputs assume transcription is always correct, talk over the user, and produce long monologues that feel robotic. Voice errors are higher-impact than text errors because they happen in real time and cannot be easily corrected by scrolling back.

Confirm critical fields always. Names, phone numbers, dates, addresses, dollar amounts. If the user provides one of these, the agent must confirm it explicitly. The confirmation should be natural, not robotic repetition. For example, instead of repeating the phone number exactly as heard, the agent should say "just to confirm, that is 555-0123, correct?" and wait for acknowledgment. The failure mode is assuming the transcription was correct when it was not, which leads to wrong data being stored. This is especially damaging in healthcare, finance, and logistics where a single digit error can cause real harm. The pattern is simple: detect critical field types, confirm them, and do not proceed until confirmed.

Smooth turn-taking means the agent stops speaking when the user interrupts. This is called **barge-in**. The agent must detect the interruption, stop mid-sentence if necessary, and respond to what the user just said rather than continuing the old plan. Bad turn-taking is talking over the user or ignoring the interruption and finishing the scripted response. This feels dismissive and breaks the conversational contract. The technical implementation requires low-latency interrupt detection and the ability to cancel in-progress text-to-speech. If the platform does not support this, the agent will feel unresponsive no matter how good the reasoning is.

Fast, calm, human pacing is the goal. Sentences should be short. Pauses should be small but present. The agent should not read long paragraphs aloud. If the response is complex, the agent should break it into chunks and check in with the user between chunks. The failure mode is long monologues that make the user wait through information they do not need, or slow pipeline latency that creates awkward silence. Track the time between user input and agent first word. If it is longer than 800 milliseconds, the interaction will feel slow. Track the length of uninterrupted agent speech. If it is longer than twenty seconds, the user's attention will drift. Voice is the only modality where pacing is part of the quality rubric.

The pacing pattern should be tested with real users, not just internal reviewers. Internal reviewers are patient and forgiving. Real users hang up if the agent is too slow or talks too long. The pattern library for voice should include examples of good pacing with short sentences and natural pauses, bad pacing with long monologues, good confirmation with natural language, bad confirmation with robotic repetition, good turn-taking with barge-in support, and bad turn-taking with ignored interruptions. These examples become the gold standard for voice quality.

## Gold Examples: The Calibration Set

Once you understand the patterns, you need to encode them in a shared reference set. This is your **Gold Set**, and it serves three purposes: reviewer calibration, regression testing, and model training. The structure is simple. Collect ten examples of great outputs, ten examples of acceptable outputs, and ten examples of fail outputs. For each example, store the user request, the system output, the rubric scores, and one to two sentences explaining why it was scored that way. This becomes your reviewer training kit and your baseline for detecting regressions when you update the model or the prompt.

Great examples are not edge cases. They are central examples that demonstrate the ideal behavior for common requests. They should cover the most frequent task types your system handles: answering questions, drafting text, retrieving information, executing actions. Each great example should score well on all rubric dimensions and should illustrate one or more of the patterns described earlier in this chapter. For instance, a great chat example might show a direct answer with clean structure and honest uncertainty handling. A great RAG example might show correct grounding with proper citation and clean abstention when evidence is missing. A great agent example might show plan-act-verify-finish with confirmation before an irreversible action.

Acceptable examples are outputs that meet the minimum bar but have minor flaws. They are correct and safe but not polished. They might be slightly verbose, or they might not cite sources as cleanly as the great examples, or they might ask one unnecessary clarifying question. Acceptable examples help reviewers understand where the line is between pass and fail. They prevent reviewers from being overly harsh and rejecting outputs that are good enough for production. They also help model trainers understand which improvements are high-priority versus nice-to-have.

Fail examples are outputs that violate a core quality dimension. They might be factually wrong, or unsafe, or completely irrelevant to the user request, or so poorly structured that they are unusable. Fail examples should cover the most common failure modes for each task type: confident hallucination in RAG, refusal to help when help is safe in chat, missing confirmation before irreversible actions in agents, missing field confirmation in voice. Each fail example should include a note explaining which rubric dimension failed and why. This turns the fail set into a diagnostic tool. When quality drops in production, you compare new outputs to the fail set to identify which failure mode is recurring.

The Gold Set must be versioned and reviewed by multiple people. Do not let one person create the entire set. Have at least three reviewers score each example independently, discuss disagreements, and reach consensus. This calibration process is where your team learns to apply the rubric consistently. If reviewers disagree on more than twenty percent of examples, your rubric is not clear enough and you need to add more detail or more anchor examples. Once the set is stable, use it as regression tests. Every time you update the model, the prompt, or the retrieval pipeline, run the Gold Set through the system and check that the scores have not degraded. If they have, investigate before deploying.

## What Usually Goes Wrong

The most common failure across all task types is confusing "sounds good" with "is correct." Reviewers score based on fluency, formality, or stylistic preference rather than correctness, relevance, grounding, and safety. This is especially damaging in RAG systems, where a confident, well-written hallucination will score higher than a correctly grounded answer that is less polished. The fix is to train reviewers explicitly on this distinction and to add rubric penalties for unsupported claims even if the writing quality is high.

The second common failure is not including abstain cases in the evaluation dataset. If every example has a correct answer present in the sources, the model learns that it should always generate an answer. It never learns that abstaining is sometimes the right move. This leads to confident guessing, which destroys trust. The fix is to deliberately include cases where the correct behavior is to say "I do not have enough information" and to score those responses as perfect. At least ten to fifteen percent of your evaluation set should be abstain-required cases.

The third common failure is judging agents on text quality instead of action correctness. The agent might produce a beautifully formatted response explaining what it plans to do, but if it does not actually execute the tools and complete the task, the output is a failure. Reviewers trained on chat systems will sometimes score agent outputs highly because the explanation is clear, even though the task was not done. The fix is to add a hard requirement in the agent rubric: did the task get completed correctly, yes or no. If no, the output fails regardless of how well it is explained.

The fourth common failure is misdiagnosing RAG bugs as model bugs. When a RAG answer is wrong, teams immediately start tweaking the prompt or the model parameters. They do not check whether the retrieval layer surfaced the right chunks. They do not check whether the chunking strategy makes sense for the corpus. They do not check whether the embedding model understands the domain terminology. This leads to weeks of wasted prompt engineering while the real issue is that the retrieval is fundamentally broken. The fix is to instrument the retrieval layer so you can see what was retrieved for every query, and to always check retrieval quality before adjusting generation.

## Debug Playbook

When quality drops, follow this sequence. First, identify the task type: chat, RAG, agent, or voice. Each task type has different failure modes and different diagnostic paths. Second, check the main failure dimension using the rubric categories: correctness, relevance, grounding, tool correctness, turn-taking, safety. This narrows the search space. Third, localize the failure within the system architecture. For RAG, check retrieval versus generation. For agents, check tool execution versus planning logic. For voice, check transcription versus reasoning versus speech synthesis. Fourth, add three to ten new gold examples that represent the failure mode you just identified. This prevents regressions. Fifth, add those examples to your regression test suite and verify that future updates do not reintroduce the same failure.

Localization is the step most teams skip. They see a bad output and immediately try to fix it with a prompt change. They do not isolate which layer caused the failure. For RAG, log what chunks were retrieved and whether the answer was present in those chunks. If the answer was not retrieved, the problem is retrieval, not generation. For agents, log which tools were called, what they returned, and whether the agent used those results correctly. If the tool returned an error and the agent ignored it, the problem is error handling, not planning. For voice, log the transcription, the reasoning trace, and the synthesized speech. If the transcription was wrong, the problem is acoustic model tuning or noise handling, not the reasoning layer. Proper instrumentation makes localization trivial. Without it, debugging is random iteration.

## Pattern Versioning and Evolution

Your pattern library is not static. It must evolve as your system changes, as new failure modes emerge, and as user expectations shift. Every pattern should be versioned with a date and a changelog. When you update a pattern, document why the change was made and what triggered it. This creates an institutional memory that prevents teams from repeating past mistakes when team members turn over or when the system scales to new use cases.

Pattern evolution follows a predictable cycle. You launch with an initial set of patterns based on requirements analysis and early prototyping. Within the first month, production traffic surfaces failure modes that were not represented in the initial set. You add patterns for those failures and update the rubric to penalize them. Within the first quarter, you accumulate enough data to identify which patterns are rare edge cases and which are common modes that deserve more training examples. You prune the rare patterns from the core set and expand the common patterns with more variations. Within the first year, user behavior changes, model capabilities improve, or business requirements shift, and you need to update the pattern library to reflect the new reality.

The versioning structure should include the pattern ID, the creation date, the last update date, the task type, the quality dimensions it addresses, the example count, and the status. Status can be active, deprecated, or candidate. Active patterns are in the production evaluation set. Deprecated patterns are no longer used but retained for historical reference. Candidate patterns are under review and not yet in production. This structure lets you track which patterns are being used, which are obsolete, and which are being considered for addition.

Pattern review should happen quarterly at minimum, monthly for high-velocity teams. The review process involves analyzing recent production failures, checking whether existing patterns cover those failures, identifying gaps, proposing new patterns, and voting on which candidates to promote to active status. The review should be cross-functional. Include product managers who understand user needs, engineers who understand system constraints, domain experts who understand correctness requirements, and trust and safety leads who understand risk. A pattern library built by one person or one function will have blind spots.

## Integration with Continuous Evaluation

Patterns are not just for human review. They become the test cases for continuous evaluation pipelines that run on every code change, every prompt update, and every model swap. When you update your RAG chunking strategy, you run the RAG pattern set through the system and compare the scores to the baseline. If grounding degrades, you investigate before deploying. When you update the agent confirmation logic, you run the agent pattern set and verify that high-risk actions still trigger confirmation prompts. When you update the voice speech synthesis, you run the voice pattern set and verify that pacing has not degraded.

The integration structure is straightforward. Your evaluation pipeline takes a pattern set as input, runs each example through the system, scores the output using the rubric, and compares the scores to the baseline for that pattern version. If the score delta exceeds a threshold, the pipeline flags the change as risky and requires manual review. If the scores improve, the pipeline updates the baseline and approves the change. If the scores stay flat, the change is neutral and can proceed. This automated gate prevents regressions that would otherwise go unnoticed until users complain.

The pattern-to-pipeline integration must be fast. If running the full pattern set takes hours, engineers will skip it or run it only before major releases. The target is under five minutes for the core pattern set, under thirty minutes for the full set. This requires parallelization, caching, and prioritization. Run the most critical patterns first. If those pass, run the rest. If those fail, stop and surface the failure immediately. This gives engineers tight feedback loops and prevents them from stacking multiple changes before discovering that one of them broke quality.

## Cross-Task Pattern Reuse

Some patterns apply across task types. The principle of honest uncertainty applies to chat, RAG, agents, and voice. The principle of confirmation before irreversible actions applies to agents and voice. The principle of constructive refusal applies everywhere. These cross-task patterns should be marked explicitly in the library so reviewers understand that the same standard applies regardless of context. This reduces cognitive load and improves consistency.

Cross-task patterns also help with transfer learning when you add new task types. If you already have a well-calibrated uncertainty pattern for chat, you can adapt it for a new RAG use case by changing the domain but keeping the structure. If you already have a well-calibrated confirmation pattern for agents, you can adapt it for voice by adjusting for latency constraints but keeping the confirmation requirement. This accelerates pattern development for new use cases and prevents teams from reinventing patterns that already exist elsewhere in the system.

The reuse structure should include tags that mark which patterns are task-specific and which are cross-task. Task-specific patterns are labeled with chat, RAG, agent, or voice. Cross-task patterns are labeled with uncertainty, refusal, safety, grounding, or structure. This taxonomy makes it easy to find relevant patterns when building evaluation sets for new features. If you are adding a new RAG feature, you filter for RAG patterns and for cross-task patterns tagged with grounding and uncertainty. This gives you the starting point for your evaluation set without building it from scratch.

## Pattern Authorship and Ownership

Every pattern in your library should have an owner. This is the person or team responsible for keeping the pattern current, for proposing updates when failure modes change, and for reviewing whether the pattern still reflects production reality. Without ownership, pattern libraries decay. Patterns that were accurate six months ago become stale as user behavior shifts, as model capabilities improve, or as business requirements evolve. Stale patterns are worse than no patterns because they calibrate reviewers to the wrong standard.

Ownership assignment follows domain boundaries. Product managers own patterns related to user experience and task completion. Engineering owns patterns related to system behavior and error handling. Domain experts own patterns related to correctness in specialized fields like medicine, law, or finance. Trust and safety owns patterns related to refusal and harmful content. This division ensures that the people with the most context are responsible for the patterns in their domain.

The owner's responsibilities include quarterly pattern review, updating examples when failure modes change, proposing new patterns when gaps are identified, and archiving patterns that are no longer relevant. The owner also trains reviewers on their domain patterns and resolves disputes when reviewers disagree on scoring. This distributed ownership scales better than centralized ownership because it distributes the maintenance burden and ensures domain expertise is applied to pattern curation.

Ownership must be tracked in metadata. Each pattern includes an owner field with the person's name or team identifier, a contact method, and a last-review date. If the last-review date is more than six months old, the pattern is flagged for review. If the owner leaves the team, the pattern is flagged for reassignment. This prevents orphaned patterns from accumulating in the library without anyone responsible for maintaining them.

## Pattern Discovery and Search

As your library grows to hundreds of patterns, discovery becomes a challenge. Reviewers need to find relevant patterns quickly when calibrating on a new task type or when diagnosing a production failure. Search capability is essential. The library should support search by task type, by quality dimension, by failure mode, by keyword, and by example content. This lets reviewers find patterns that match the scenario they are evaluating.

The search interface should return ranked results with snippets showing why each pattern matched. If a reviewer searches for "RAG grounding failure," the results should show patterns tagged with RAG and grounding, with snippets highlighting the relevant sections. The ranking should prioritize active patterns over deprecated ones, recently updated patterns over stale ones, and patterns with more examples over patterns with fewer examples. This surfaces the most relevant and most reliable patterns first.

Pattern discovery also happens through recommendation. When a reviewer scores an output, the system should suggest patterns that match the task type and failure modes present in that output. If the output is a RAG response with a potential grounding issue, the system suggests grounding patterns to help the reviewer calibrate their judgment. If the output is an agent response with a potential confirmation issue, the system suggests confirmation patterns. This contextual recommendation reduces the cognitive load on reviewers and improves scoring consistency.

The recommendation engine requires tagging infrastructure. Each pattern is tagged with task type, quality dimensions, failure modes, and key concepts. When an output is submitted for review, the system extracts features such as task type from metadata and potential failure modes from automated checks. It then matches those features against pattern tags and returns the top matches. The reviewer can accept the suggestions or search manually if the suggestions are not relevant.

## Measuring Pattern Coverage and Gaps

Your pattern library should cover the most common scenarios your system encounters, but you cannot anticipate every edge case. Pattern coverage is the percentage of production outputs that match at least one pattern in your library. You measure this by sampling production outputs, attempting to match each output to a pattern, and calculating the match rate. If coverage is below eighty percent, you have gaps in your library that need to be filled.

Gap identification is the inverse of coverage measurement. For every production output that does not match a pattern, log the task type, the quality dimensions, and any failure modes present. Cluster these unmatched outputs to identify recurring scenarios that are not represented in the library. If you find a cluster of fifty RAG outputs with a specific retrieval failure pattern, that cluster becomes a candidate for a new pattern. If you find singleton edge cases that appear once and never again, those do not warrant patterns.

The gap-filling process should be prioritized by frequency and impact. A gap that appears in five percent of production traffic and causes user-visible failures is high-priority. A gap that appears in 0.1 percent of traffic and causes minor quality degradation is low-priority. Track the gap backlog and address high-priority gaps in the next pattern review cycle. This ensures the library evolves to cover the scenarios that matter most to production quality.

Coverage measurement also reveals over-represented patterns. If you have twenty patterns for a scenario that appears in one percent of traffic, you are over-investing in that scenario. Some of those patterns may be redundant or overly specific. Consolidate them into a smaller set of representative examples. This keeps the library manageable and prevents reviewers from being overwhelmed by too many similar patterns.

Your next step is to define output quality dimensions for specific task types, which Chapter 2.4 addresses by covering uncertainty, refusal, and safety scoring when the model cannot or should not answer confidently.
