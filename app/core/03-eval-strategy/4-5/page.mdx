# 4.5 â€” Preventing Label Rot: Versioning and Change Control

In November 2024, a fintech company's fraud detection team noticed something strange. Their model's precision had supposedly improved from 91% to 94% over three months, but fraud complaints from customers had tripled. The evaluation dashboard showed steady improvement. The support tickets showed steady deterioration. The head of Trust and Safety demanded an audit.

The root cause emerged within hours. Six months earlier, the team had built a gold set of 800 labeled fraud cases. In March 2024, the company updated its refund policy to be more customer-friendly, automatically approving refunds under fifty dollars without fraud review. In June, they launched a new account tier with different transaction limits. In September, they integrated a new payment processor with different error codes. The evaluation system was still scoring the model against the original gold set from November 2023, but the world had changed. Half the gold labels were now wrong. The model was being punished for following the new rules and rewarded for following the old ones. The dashboard showed improvement because the model had learned to game labels that no longer reflected reality. This is label rot, and it turns your evaluation system into a beautifully presented lie.

## The Mechanics of Label Rot

Label rot is a change control failure disguised as a data quality problem. It happens when the environment changes but your ground truth doesn't. The labels you carefully created six months ago were correct then, but policies shifted, products evolved, tools changed, and your labels stayed frozen. Now they're measuring the wrong thing, and nobody noticed because the metrics still updated every week with confident precision.

You see label rot in five patterns. Old gold examples no longer match current policy, so your regression suite fails when the model produces correct outputs under the new rules. A customer service chatbot correctly follows the updated refund policy allowing automatic approvals under fifty dollars, but your gold set still expects the old seventy-two-hour review period. The model passes the real test, which is serving users correctly, but fails your evaluation, which is testing obsolete rules. This creates a devastating trust problem: engineering stops believing your metrics because they see correct behavior marked as failures.

Rater disagreement increases over time because different team members remember different versions of the standards, and nobody knows which version is canonical. The person who labeled examples in January remembers the strict interpretation of compliance rules. The person who started in June learned the relaxed interpretation that legal approved in April. Both are applying what they believe to be correct, but they're working from different mental models. Inter-rater agreement decays from eighty-five percent to sixty percent over six months, and you can't figure out why until you realize half your team is using outdated criteria.

Your regression suite starts failing for what engineering calls "no reason" because the failures aren't bugs, they're correct behavior under updated policies. Product shipped a feature that changes the conversation flow. The model adapted and now handles the new flow correctly. But your regression suite expects the old flow, so it marks the new behavior as wrong. Engineering investigates, finds nothing broken, closes the ticket as "evaluation error," and your evaluation loses credibility. This happens three times and engineering stops taking your regression failures seriously. Now when real bugs appear, they get ignored.

The model looks worse after a policy update even though it's behaving correctly, because you're still scoring it against pre-update expectations. Legal changes the criteria for flagging suspicious transactions to reduce false positives. The model correctly implements the new criteria and flags fewer transactions. Your evaluation shows precision dropping because your gold set still uses the old criteria, where more transactions should have been flagged. Leadership sees declining performance metrics and questions whether the model is regressing. You spend two weeks proving the model is fine and the evaluation is stale.

Different teams label the same case differently because they're working from different documentation versions, different training materials, different implicit assumptions about what correct means. The product team labels based on the latest product requirements doc. The evaluation team labels based on documentation from three months ago. The compliance team labels based on their interpretation of regulatory guidance. All three teams think they're right, and without a single source of truth, the conflict never resolves. Cases get relabeled repeatedly as different teams override each other's judgments.

The underlying problem is not that raters are sloppy or that your taxonomy is ambiguous. The problem is that you treat labels as static artifacts when they're actually living dependencies on an ever-changing system. Every time you update a policy, launch a feature, change an API response format, reorganize your documentation, or rotate in new raters, you create a potential divergence between your ground truth and reality. Without change control, divergence is inevitable. With enough time, your evaluation becomes unmoored from the system it's supposed to measure. You're flying blind with instruments that show steady altitude while you're actually descending.

## Root Causes: Why Labels Rot

Policy and documentation changes are the most common trigger. Your refund policy updates to comply with new regulations. Your SLA definitions change because you launched premium support tiers. Your compliance rules shift because legal reviewed an incident. Every policy change potentially invalidates labels that depend on that policy. If your ground truth says "approve refunds for damaged goods within thirty days" and legal changes the window to fourteen days, every label based on the old window is now wrong. If you don't track which labels depend on which policies, you can't know what to update when policies change.

Product changes create their own form of rot. You ship a new feature that changes how users interact with the system. You modify a workflow that changes the sequence of steps users take. You introduce new plan tiers that change what actions are available to which users. You change default behavior that affects what users expect. Each of these changes can invalidate labels that assumed the old product behavior. If your ground truth for a chatbot task assumes a certain UI flow, and you redesign the UI, labels based on that assumption are now testing the wrong thing.

Tool and API changes hit agent systems especially hard. You update a tool and it returns a different response schema. New error codes appear. Latency characteristics change. Partial failures become possible where they weren't before. If your ground truth expects the old tool behavior, the agent is now being scored against obsolete expectations. A concrete example: your agent calls a search API, and the API starts returning a new field for relevance scores. Your ground truth didn't account for this field, so the agent ignores it, and you mark that as correct. But the optimal behavior is now to use the relevance scores. Your label is technically still accurate for the old API contract but misleading for the new one.

RAG corpus changes create silent rot. You reorganize documentation to improve user clarity. You change your chunking strategy to improve retrieval. You update the retrieval pipeline to use a better embedding model. All of these improve the RAG system, but they can invalidate ground truth that assumed the old corpus structure. If your ground truth says "the answer is in document A, paragraph three" and you reorganized the docs, that reference is now wrong. If your ground truth expects certain chunks to be retrieved and your chunking strategy changed, the expected retrieval set is now different.

Human drift is the subtlest cause. New raters join the team without experiencing the original context that informed early labeling decisions. Raters develop different interpretations of ambiguous rubric language over time. Standards slowly shift as the team forgets why certain edge cases were labeled certain ways. Someone who labeled examples in January has a different mental model than someone who started in June, and neither of them matches the assumptions from the original dataset creation in the previous year. This isn't malice or incompetence. It's the natural entropy of human systems. Without active calibration and version control, understanding decays.

## Change Control Levels and Approval Gates

Version control is not optional. You must version your taxonomy, rubric, ground truth, datasets, and gold sets. Taxonomy version tracks what tasks exist and how they're defined. When you add a new task type or redefine an existing one, that's a taxonomy change. Rubric version tracks how you score each task. When you change what constitutes a passing score or add a new scoring dimension, that's a rubric change. Ground truth version tracks what the correct answers are for each test case. When you relabel an example or update the expected output, that's a ground truth change. Dataset version tracks which examples are included in each evaluation set. When you add cases, remove cases, or change the slice distribution, that's a dataset change. Gold set version tracks your stable regression suite. When you update your regression tests, that's a gold set change.

Every time you make a substantive change to any of these, you increment the version and record what changed, who approved it, why it changed, and when it became effective. This isn't just good practice, it's how you maintain comparability over time. If your model scores eighty-five percent on gold set version 2.3 and ninety percent on version 2.4, you need to know what changed between versions to understand whether that's genuine improvement or just easier tests. Without version history, you can't make that determination.

Change approval levels prevent chaos. Not all changes carry the same risk, so they shouldn't require the same approval process. A typo fix in a rubric description carries very different risk than changing correctness criteria for a compliance workflow. Define three levels to match risk to approval requirements.

Level one changes are minor: wording clarifications that don't change meaning, small rubric fixes that resolve obvious ambiguities, formatting improvements that make labels easier to read. These require approval from the eval owner only. They're low-risk adjustments that improve clarity without shifting what you're measuring. An example: you notice the rubric says "the response must be polite" but doesn't define polite, so you add "polite means no profanity, no insults, no dismissive language." That's a clarification, not a change in standards, so it's level one.

Level two changes are moderate: adding new tasks to cover gaps in your taxonomy, splitting a task into subtasks to improve granularity, adding new slices to your coverage map to test underrepresented scenarios, expanding your gold set with new edge cases that weren't previously tested. These require approval from both the eval owner and the product owner because they change what you're measuring, even if they don't change the fundamental correctness criteria. An example: you add a new task type for handling multi-turn clarification dialogues because users frequently ask follow-up questions. That's expanding your scope, which affects what the model is optimized for, so Product needs to confirm this aligns with product priorities.

Level three changes are major: policy changes that affect correctness criteria, changes to Tier 3 workflows where failure has serious legal or financial consequences, updates to safety rules that affect what content is allowed, compliance requirement changes driven by new regulations. These require approval from the eval owner, the domain owner such as Legal or Security or Finance, and leadership sign-off. These changes affect business risk and regulatory exposure, so they need multi-stakeholder consensus. An example: legal updates the policy on what medical claims can be made in product descriptions to comply with new FDA guidance. This affects correctness criteria for a regulated workflow, so Legal must approve the label updates, leadership must be informed of the compliance change, and the eval owner coordinates the implementation.

Refresh cadence establishes the operational rhythm for preventing drift. This isn't something you do when you remember or when things seem broken. It's a scheduled process that runs regardless of whether you think you need it. Weekly drift review catches emerging problems early, before they become systematic. You examine unknown intents that the taxonomy doesn't cover, user complaints that suggest the model is failing in ways your evaluation doesn't test, edge cases the model handled poorly, and cases where raters disagreed strongly. This is your early warning system for coverage gaps and ambiguous criteria.

Monthly gold set audit is where you sample fifty to two hundred gold items, confirm they still match current policy and documentation, and relabel only when necessary. The sampling should be stratified: pull examples from each task type, each tier, each major slice. The goal isn't to find reasons to change things. The goal is to verify that your ground truth still reflects the system you're measuring. Most months, you'll find that ninety-five percent of your gold set is still accurate. But that five percent matters. Those are the cases where policy changed or product evolved, and catching them monthly prevents them from accumulating into systematic rot.

Monthly rubric calibration brings your raters together to score the same examples and discuss discrepancies. You don't just measure inter-rater agreement, you actively improve it. When two raters disagree, you discuss why, refine the rubric to be clearer, and re-score to confirm the clarification worked. This prevents human drift and builds shared understanding. Teams that do this monthly maintain inter-rater agreement above eighty percent. Teams that skip it see agreement decay to sixty percent or lower within six months.

Per-release updates to your regression suite ensure that every deployment is tested against current expectations, not stale ones. Before you ship a new version, you review whether any changes in this release affect ground truth. New features, modified workflows, updated policies, changed tool integrations. If any of these happened, you update your regression tests to reflect the new reality before you gate the release on passing those tests.

Per-policy-change ground truth updates are non-negotiable. The moment a policy changes, not a week later, not at the next monthly review, but immediately, you trigger a review of all labels that depend on that policy. You identify the affected examples, assess whether they need relabeling, update them if necessary, and version the change. This prevents the scenario where the model correctly follows new policy but gets marked wrong by evaluation using old policy.

## The Label Change Control Pipeline

Serious teams treat label updates like code changes. There's a trigger, an impact analysis, a controlled update process, a re-baselining step, and a publication with full audit trail. This isn't bureaucracy. It's how you maintain trustworthy evaluation over months and years. Without this process, label changes happen haphazardly, nobody knows what's current, and your evaluation drifts until it measures nothing meaningful.

Triggers include policy changes, incidents that revealed gaps in your ground truth, major updates to models or prompts or tools, repeated rater disagreement on specific task types, and expansion into new markets or languages. Each trigger initiates a formal change process. You don't just update a label because someone thinks it might be wrong. You document why the change is being considered and who initiated it. Policy change triggers are usually the most straightforward: legal sends an email saying the refund policy changed, you initiate a label review. Incident triggers are more subtle: a model made a wrong decision that slipped through evaluation, you investigate why your ground truth didn't catch it, you find a gap, you initiate a change to fill that gap.

Repeated rater disagreement triggers are your signal that the rubric is ambiguous. If the same task type consistently produces disagreement, that's not rater error, it's rubric insufficiency. You initiate a change to clarify the criteria. Major model or tool updates can change optimal behavior in ways that invalidate old ground truth. If you switch from GPT-4 to GPT-5 and the new model has different capabilities, some of your expected behaviors might need updating. If you update a tool and it returns different data, your agent ground truth might expect the old data format.

Impact analysis happens before you change anything. You answer four questions. Which tasks are impacted by this change? Not all tasks, usually a subset. If the refund policy changed, that affects refund-related tasks but not greeting tasks. Which slices of your dataset are impacted? Maybe only cases involving refunds over a certain amount, or only cases in certain regions. Which gold items are impacted? You need the specific item IDs so you can relabel precisely the right examples. Do you need to relabel examples, or can you just update scoring rules? Sometimes the examples are still valid, you just need to change what counts as correct.

Impact analysis prevents cascading errors where you fix one thing and break three others. It also prevents unnecessary work where you relabel your entire dataset when only a small slice was affected. Concrete example: the refund policy changed the approval threshold from one hundred dollars to fifty dollars. Impact analysis reveals this affects the refund task type, the high-value-transaction slice, and forty-three specific gold items. You relabel those forty-three items. You don't touch the other seven hundred fifty-seven items in your gold set because they're unaffected.

Controlled updates mean you only change what's necessary. You revise ground truth rules to reflect the new policy or product behavior. The rules document gets updated with the new criteria and versioned. You update rubrics and anchors so raters know how to apply the new rules. If the old rubric said "approve refunds under one hundred dollars" and the new policy says fifty dollars, you update the rubric to match. You update the anchors to show examples of correct behavior under the new threshold.

You relabel impacted items, and only impacted items. You don't relabel the whole dataset because that's expensive, time-consuming, and introduces new errors through rater mistakes. You add new regression tests to cover the cases that triggered this update so you don't regress in the future. If this change was triggered by an incident, you add the incident case to your regression suite. If it was triggered by a policy change, you add edge cases around the new policy boundaries.

Re-baselining acknowledges that truth changed. After a major update, you can't directly compare metrics from before the change to metrics after the change because you changed what correct means. If you changed the refund threshold from one hundred to fifty dollars, and precision dropped from ninety-two percent to eighty-seven percent, that doesn't necessarily mean the model got worse. It might mean the model is still optimized for the old threshold and needs retraining, or it might mean the new threshold is genuinely harder to get right.

You record old-versus-new comparability notes so future you understands why the trendline shows a discontinuity. You write "on March 15, 2025, we updated ground truth to reflect the new refund policy lowering the auto-approval threshold from one hundred to fifty dollars, metrics before and after this date are not directly comparable." You define a new benchmark baseline so you can track trends going forward. The March 15 score becomes the new baseline, and you measure improvement from there.

This is not admitting defeat. This is maintaining intellectual honesty about what your metrics mean. Executives who understand evaluation appreciate this transparency. Executives who don't might push back and demand continuous trendlines. Your job is to explain that a trendline crossing a truth change is meaningless, and re-baselining preserves the ability to measure real improvement going forward.

Publication and audit close the loop. You store what changed, who approved it, why it changed, the effective date, and the list of affected items. This creates an audit trail for compliance, for debugging, and for onboarding new team members. Six months from now when someone asks "why did our precision drop in March?" you can point to the policy change, the label update, the approval chain, and the comparability notes. Without this documentation, that question turns into a week-long archaeology project trying to reconstruct what happened. With it, you answer in five minutes.

## Keeping Trendlines Meaningful When Truth Changes

Leadership loves trendlines. Executives want to see that the model is improving quarter over quarter. Investors want evidence of progress. Product managers want proof that their initiatives are working. But truth changes break comparisons. If you relabel your dataset to match a new policy, your model's score on the new labels isn't comparable to its score on the old labels. The model didn't change, the measurement did. How do you maintain meaningful long-term tracking when the definition of correct keeps shifting?

Keep a stable core benchmark. Maintain a set of tasks and test cases that almost never change, regardless of policy updates or product evolution. General assistant behaviors like greeting users politely, handling clarification requests appropriately, refusing harmful instructions. These behaviors are timeless: they were correct a year ago, they're correct now, they'll be correct a year from now. Stable FAQ patterns where the answers don't depend on time-sensitive policies. If your product has core functionality that hasn't changed in two years, those tasks form part of your stable core. Timeless safety cases like not generating instructions for illegal activity, not producing hate speech, not leaking sensitive data. These correctness criteria don't shift with policy updates.

These stable tasks form your longitudinal benchmark. Even when policies change and you relabel twenty percent of your evaluation set, you can still track performance on the stable eighty percent to see genuine model improvement over time. When executives ask "is the model actually getting better or are we just measuring differently?" you can point to the stable core and show trend over eighteen months with no definition changes. This gives you a clean signal of real improvement separate from measurement shifts.

The stable core should represent at least fifty percent of your evaluation, ideally more. If policy changes invalidate most of your ground truth every quarter, your measurements become too unstable to track progress. You need enough stable ground to anchor your trendlines. In practice, this means designing your taxonomy to separate policy-dependent tasks from policy-independent tasks, and ensuring the policy-independent tasks get adequate coverage.

Allow re-baselining after major policy changes. If a policy change affects twenty percent or more of your evaluation, you acknowledge that metrics before and after the change aren't directly comparable. You set a new baseline date and add a note in your reporting dashboard: metrics before March 15, 2025 were scored under the old refund policy, metrics after that date use the updated policy. You show two trendlines: one for the stable core that runs continuously, one for the full evaluation that re-baselines at the policy change.

This is normal in enterprise systems outside of AI. Financial reporting does this when accounting standards change: companies restate historical results under the new standard or clearly mark the discontinuity. Clinical trials do this when endpoints are modified: they analyze pre-change and post-change cohorts separately. Evaluation systems should do it too. The alternative is to pretend nothing changed and present a misleading continuous trendline that conflates genuine improvement with measurement shifts.

Some executives resist re-baselining because they want a simple story: "performance improved thirty percent over two years." Re-baselining complicates that story. Your job is to explain that the alternative is a false story. If you changed what correct means halfway through those two years, the thirty percent improvement might be twenty percent genuine improvement plus ten percent from easier tests. That's not transparency, it's deception. Re-baselining lets you say "performance improved twenty percent on stable tasks, and we updated our standards to match new policy, here's the new baseline." That's a true story.

Store old truth instead of deleting it. Never delete old labels. Mark them as superseded, deprecated, or replaced by version X.Y. This protects you in three ways. Audits may require you to show what you were measuring six months ago. If a regulator asks "why did your model approve this transaction in September?" and your ground truth has changed since September, you need the September version to reconstruct what the model was evaluated against at that time.

Debugging may require you to understand why the model behaved a certain way under old policies. If users report that the model's behavior changed in October, and you updated ground truth in October, you need the old labels to verify whether this is the model changing or your expectations changing. Root cause analysis after incidents may require reconstructing what the evaluation was testing at a specific point in time. If an incident occurred because your evaluation had a gap, you need the historical ground truth to prove the gap existed and show when you fixed it.

Deleted labels make all of this impossible. You're left saying "I think we were testing X back then, but I'm not sure." Versioned labels with full history make it straightforward: you pull up version 2.3 from September, see exactly what was being tested, and answer the question definitively. Storage is cheap. Inability to answer audit questions is expensive. Keep everything.

## Failure Modes and Fixes

Your regression suite constantly fails after policy updates. The root cause is that the suite includes policy-dependent examples, but there's no version tie between the test cases and the policy effective date. Every time policy changes, the suite fails because it's testing against obsolete expectations. Engineering sees fifty test failures, investigates, finds the model behavior is correct under the new policy, and starts ignoring regression failures entirely. This destroys the value of your regression suite.

The fix is to tag policy-dependent tests with the policy name and version. Every test case that depends on a policy gets metadata: policy name, policy version, effective date. When policy changes, these tests automatically route to a relabel workflow. You don't wait for failures to occur. The moment policy updates, you query for all tests tagged with that policy, review them, update the expected behavior to match the new policy, increment the version, and document the change. Now your regression suite stays synchronized with your policies, and failures represent real regressions, not stale expectations.

Rater disagreement keeps increasing over time. The root cause is rubric drift, where the written rubric stays the same but raters develop different interpretations. The rubric says "responses should be helpful" but doesn't define helpful precisely, so each rater develops their own interpretation. Over six months, those interpretations diverge. Rater A thinks helpful means comprehensive, Rater B thinks helpful means concise. Both believe they're following the rubric correctly.

Anchors haven't been updated to cover new edge cases that emerged after the original rubric was written, so raters invent their own standards for those cases. The rules for when to assume missing information versus clarify versus abstain are unclear, so different raters apply different heuristics. Some raters assume unless explicitly stated otherwise, others clarify whenever there's any ambiguity. These aren't rater errors, they're rubric gaps.

The fix is monthly calibration sessions where raters score the same examples and discuss disagreements. You don't just measure inter-rater agreement, you actively improve it. When Rater A and Rater B disagree on whether a response is helpful, you have them explain their reasoning, identify where the rubric is ambiguous, refine the definition of helpful to be more specific, and re-score to confirm everyone now agrees. You expand your anchor set to cover the edge cases that caused confusion, adding examples that clearly illustrate how to handle the previously ambiguous scenarios. You tighten your ambiguity guidance, referencing the framework from subchapter 4.2 on when to assume versus clarify. Calibration isn't a one-time training session at onboarding. It's a recurring operating rhythm that prevents drift.

Teams argue about what's correct. The root cause is that there's no source-of-truth registry, no adjudication process, and no formal change control. Engineering thinks one thing is correct based on the product spec they received three months ago. Product thinks something else is correct based on user research conducted last month. Legal thinks a third thing is correct based on their interpretation of regulatory requirements. All three teams are acting in good faith, working from what they believe to be authoritative sources, but those sources conflict.

Without a registry and adjudication process, these conflicts never resolve. They just cycle: Engineering labels a case, Product relabels it differently, Legal relabels it a third way, Engineering sees the change and reverts it, repeat forever. Each team wastes time relitigating the same decisions, trust erodes, and the evaluation becomes unreliable because its ground truth is constantly changing based on whoever touched it last.

The fix is to enforce the truth registry from subchapter 4.3, the adjudication process from subchapter 4.4, and the versioning system described here. When there's disagreement, you don't let teams override each other's labels. You escalate to adjudication. The adjudicator, who has authority to make binding decisions, reviews the conflicting sources, consults stakeholders if needed, makes a definitive ruling on what's correct, and records that ruling in the truth registry. The decision gets versioned, so everyone knows this is the current canonical answer. Now there's one source of truth, not three conflicting ones, and everyone knows where to find it.

Metrics improved but customers got worse. This is the nightmare scenario. Your dashboard shows the model getting better quarter over quarter. Precision is up, recall is up, task success rate is up. But support tickets are increasing, user satisfaction scores are dropping, and customer complaints are rising. The evaluation says everything is fine. The users say everything is broken. One of them is lying, and it's not the users.

The root cause is that the dataset became easier through silent changes. Someone removed hard examples to reduce rater disagreement, which made the metrics look better because the model performs better on easier cases. Labels shifted to be more lenient to make the model look better, either consciously because someone wanted to hit their OKRs or unconsciously through rater drift toward leniency. The coverage map drifted so you're no longer testing the tasks users actually care about. You're testing the tasks that are easy to evaluate, which aren't the same as the tasks that matter for user satisfaction.

The fix has three parts. First, freeze your regression suite so it can't be silently watered down. The regression suite is append-only: you can add new hard cases, but you can't remove existing cases without explicit approval and documentation. This prevents silent deletion of hard examples. Second, track dataset difficulty mix over time and alert if it shifts more than ten percent. If your dataset is forty percent hard cases, thirty percent medium, thirty percent easy, and over three months it shifts to twenty percent hard, thirty percent medium, fifty percent easy, that's a red flag. The shift might be legitimate if product focus changed, but it needs to be intentional and documented, not silent.

Third, audit label distributions over time and investigate if the proportion of positive labels increases without a corresponding product change. If the model's pass rate goes from seventy percent to eighty-five percent but you didn't retrain the model or change the product, something is wrong. Either the dataset got easier or the labels got more lenient. Neither is acceptable without justification. These checks catch silent dataset drift before it decouples your metrics from reality and lets you ship a model that looks great on paper but fails in production.

## Monthly Label Rot Prevention Checks

Run these checks on a fixed monthly schedule. Gold set audit: sample fifty to two hundred gold items, confirm they still match current policy and documentation, relabel only when necessary. The goal isn't to find reasons to change things. The goal is to verify that your ground truth still reflects the system you're measuring. Rater drift check: have the same set of examples labeled by different raters, look for growing disagreement patterns. If disagreement is increasing over time, your rubric needs clarification or your calibration rhythm needs to increase. Policy dependency scan: list all tasks that are tied to specific policies like refund rules, SLA definitions, compliance requirements. Ensure each policy has a version recorded. When policies update, you know exactly which labels to review.

Dataset change log review confirms that no silent replacement of evaluation sets occurred. Every addition, removal, or modification to your datasets should be logged. If items disappeared and nobody can explain why, you have a process failure. If the slice distribution shifted and it wasn't intentional, you have drift. The change log makes these problems visible. Regression stability check runs your regression suite three times and confirms stability per the rule from subchapter 3.6. If the suite gives different results across runs, something is non-deterministic and your labels might be ambiguous or your model might be temperature-sensitive. Either way, you need to investigate before trusting the results.

## Enterprise Expectations

Serious teams maintain a formal change process with pull-request-style review for label changes. When someone wants to update ground truth, they open a change request with the justification, the impact analysis, and the proposed new labels. Reviewers from the affected domains comment and approve. This creates accountability and prevents unilateral changes that break other teams' assumptions. For Tier 3 workflows, approvals by domain owners are required. If you're changing fraud detection labels, the head of Trust and Safety approves. If you're changing compliance labels, Legal approves. If you're changing financial projections, Finance approves.

Enterprise teams maintain an audit trail for compliance. Regulators and auditors expect you to explain why your model made certain decisions at certain times. If your labels changed between the time of the decision and the time of the audit, you need to show what the labels were when the decision was made. Without version history, you can't reconstruct this. With proper versioning and audit logs, you can show exactly what the model was trained on and evaluated against at any point in time.

Policy effective date tracking in ground truth is mandatory. Every label that depends on a policy should be tagged with the policy name and the effective date range. This makes impact analysis trivial when policies change. You query for all labels tagged with the old policy version, review them, and update as needed. Without this tagging, you have to manually search through examples trying to remember which ones depend on which policies.

Rater calibration is a fixed operating rhythm, not a one-time event. Monthly or quarterly calibration sessions are scheduled on the calendar. Attendance is mandatory for anyone who labels data. You score examples together, discuss disagreements, update rubrics based on what you learn. This prevents drift and builds shared understanding. Teams that skip calibration see inter-rater agreement decay by fifteen to twenty percentage points over six months.

Keep old labels for audits and root cause analysis. When an incident happens, you need to understand what the evaluation was testing at the time. When a regulator asks about a decision from eight months ago, you need to show what ground truth was in effect. When you're debugging why the model regressed on a task, you need to compare current labels to historical labels. Deleted labels make all of this impossible. Versioned labels with full history make it routine.

## Ready-to-Use Change Request Template

Every label change should follow a standard format. Request ID identifies this change uniquely. Requested by names the person initiating the change. Date records when the request was opened. Change level indicates whether this is level one, two, or three, which determines the approval requirements. Trigger explains why this change is needed: policy change, incident, drift, release, user feedback. Impacted tasks lists which task types are affected. Impacted slices lists which data slices need review. Impacted gold items lists specific examples that need relabeling, with item IDs. Proposed changes describes what will change in concrete terms. Reasoning explains why this change is correct and necessary. Approvers lists who needs to sign off based on the change level. Effective date indicates when this change takes effect. Rollback plan describes how to revert if the change causes problems.

For policy-tied labels, store additional metadata. Policy name identifies which policy this label depends on. Policy version records the specific version of the policy. Effective date indicates when this version became active. Source link or location points to the canonical policy document. Owner names who's responsible for this policy. Notes capture any context or edge cases that future maintainers need to understand.

This isn't paperwork for its own sake. It's how you maintain trustworthy evaluation at enterprise scale over years. Without version control and change management, your evaluation system becomes a beautiful dashboard reporting meaningless numbers.

In the next subchapter, we'll examine how to build ground truth for open-ended and creative tasks, where there's no single correct answer and traditional labeling approaches break down completely.
