# Chapter 4.5 — Preventing Label Rot (Versioning + Change Control)

**What we're doing here:**
"Label rot" happens when your previously-correct labels slowly become wrong or inconsistent because the world changed:
- policies changed
- product behavior changed
- tools/APIs changed
- docs changed
- raters changed standards over time

If you don't prevent label rot, your evaluation becomes a fake mirror:
- your metrics look stable,
- but they no longer measure what matters.

**Enterprise outcome:**
A controlled system where labels remain trustworthy over months and years, with audit trails and safe updates.

---

## 1) Mechanics: what label rot looks like in production

Label rot usually shows up as:

- **Old gold examples** no longer match current policy
- **Raters disagree more over time**
- **Regression suite starts failing "for no reason"**
- **Model looks "worse" after a policy update**
- **Different teams label the same case differently**

Label rot is not a rater problem only. It's a **change control problem**.

---

## 2) The root causes (why labels rot)

### 2.1 Policy/document changes
Refund policy updated, SLA updated, compliance rules updated.

### 2.2 Product changes
New feature, new flow, new UI, new plan tiers, new default behavior.

### 2.3 Tool and API changes (agents)
Tool outputs change shape, new error codes, different latency, partial failures.

### 2.4 RAG corpus changes
Docs reorganized, chunking changed, retrieval pipeline updated.

### 2.5 Human drift
New raters, inconsistent training, "standards slowly shift."

---

## 3) Knobs & defaults (what you actually set)

### 3.1 Version everything (non-negotiable)
You must version:
- taxonomy version (what tasks exist)
- rubric version (how you score)
- ground truth version (what is correct)
- dataset version (what examples are included)
- gold set version (your truth anchors)

### 3.2 Change approval levels (simple)
Define 3 levels:

- **Level 1 (minor):** wording clarifications, small rubric fixes — Approval: Eval owner
- **Level 2 (moderate):** new tasks, split tasks, new slices — Approval: Eval owner + product owner
- **Level 3 (major):** policy changes, Tier 3 workflows, safety rules — Approval: Eval owner + domain owner (legal/security/finance) + leadership sign-off

### 3.3 Refresh cadence (recommended)
- Weekly: drift review (unknown intents, complaints)
- Monthly: gold set audit + rubric calibration
- Per release: regression suite update + gating review
- Per policy change: mandatory ground truth update

---

## 4) The core system: "Label change control" pipeline

A real enterprise setup treats label updates like code changes.

### Step 1 — Trigger
Triggers include:
- policy change
- incident
- major model/prompt/tool update
- repeated rater disagreement
- new market/language expansion

### Step 2 — Impact analysis
Before changing anything, answer:
- Which tasks are impacted?
- Which slices are impacted?
- Which gold items are impacted?
- Do we need to re-label, or just update scoring rules?

### Step 3 — Controlled updates
Only update what is necessary:
- revise ground truth rules
- update rubrics/anchors
- relabel impacted items
- add new regression tests

### Step 4 — Re-baseline
After changes, you must define:
- "Old vs new" comparability notes
- a new benchmark baseline for trendlines

### Step 5 — Publish + audit
Store:
- what changed
- who approved
- why it changed
- effective date
- affected items list

---

## 5) How to keep trendlines meaningful (even when truth changes)

Leadership loves trendlines, but truth changes can break comparisons.

Use these 3 techniques:

### 5.1 Keep a stable "core benchmark"
Maintain a set of tasks and cases that almost never change:
- general assistant behaviors
- stable FAQ patterns
- timeless safety cases

This preserves long-term tracking.

### 5.2 Allow "re-baselining" after major policy changes
If policy changes, you may need:
- a new baseline date
- a note: "metrics before X date are not comparable"

This is normal in enterprise systems.

### 5.3 Store "old truth" instead of deleting it
Never delete old labels.
Mark them as:
- superseded
- deprecated
- replaced by version vX.Y

This protects audits and debugging.

---

## 6) Failure modes (symptoms + root causes)

### 6.1 "Regression suite constantly fails after policy updates"
Root causes:
- suite includes policy-dependent examples
- no version tie to policy effective date

Fix:
- tag policy-dependent tests
- auto-route them to relabel workflow on policy change

---

### 6.2 "Rater disagreement keeps increasing"
Root causes:
- rubric drift
- anchors not updated
- unclear "assume vs clarify vs abstain" rules

Fix:
- monthly calibration
- expand anchors
- tighten ambiguity guidance (4.2)

---

### 6.3 "Teams argue about what's correct"
Root causes:
- no source-of-truth registry (4.3)
- no adjudication (4.4)
- no formal change control

Fix:
- enforce truth registry + adjudication + versioning

---

### 6.4 "Metrics improved but customers got worse"
Root causes:
- dataset became easier (silent change)
- labels shifted to be more lenient
- coverage map drifted

Fix:
- freeze regression suite
- track dataset difficulty mix
- audit label distributions over time

---

## 7) Debug playbook: label rot prevention checks (monthly)

Run these checks monthly:

1. **Gold set audit** — sample 50–200 gold items, confirm they still match policy/docs, relabel only when necessary
2. **Rater drift check** — same set labeled by different raters, look for growing disagreement patterns
3. **Policy dependency scan** — list tasks tied to policies (refunds, SLAs, compliance), ensure policy versions are recorded
4. **Dataset change log review** — confirm no silent replacement of eval sets, confirm slice distribution hasn't drifted unintentionally
5. **Regression stability check** — run regression suite 3 times, confirm stability (3.6 rule)

---

## 8) Enterprise expectations (what serious teams do)

- Maintain a formal change process:
  - PR-style review for label changes
  - approvals by domain owners for Tier 3
- Maintain an audit trail for compliance
- Maintain "policy effective date" tracking in ground truth
- Maintain rater calibration as a fixed operating rhythm
- Keep old labels for audits and root-cause analysis

---

## 9) Ready-to-use templates

### 9.1 Label change request (copy/paste)

**Request ID:**
**Requested by:**
**Date:**
**Change level:** 1 / 2 / 3
**Trigger:** policy change / incident / drift / release
**Impacted tasks:**
**Impacted slices:**
**Impacted gold items:**
**Proposed changes:**
**Reasoning:**
**Approvers:**
**Effective date:**
**Rollback plan:** (if needed)

### 9.2 "Policy-tied label" metadata (copy/paste)

**Policy name:**
**Policy version:**
**Effective date:**
**Source link/location:**
**Owner:**
**Notes:**

---
