# 3.5 â€” Keeping Taxonomy Updated (Drift and New Intents)

In October 2025, a customer support platform serving financial services companies ran into a problem that looked like a mystery. Their eval scores held steady at 89% for three months straight, even as their quarterly business review showed escalation rates climbing 22% and customer satisfaction scores dropping by eleven points. The product team celebrated the stable eval metrics. The support operations team watched the fire spread.

What happened? The taxonomy they built in Q2 2025 reflected the product as it existed then. But by Q4, two major features had launched, user behavior had shifted to mobile-first interactions with different phrasing patterns, and regulatory changes in the EU AI Act enforcement phase had introduced new compliance questions that accounted for 18% of incoming volume. The eval program was measuring the old world while the product lived in the new one. Their taxonomy had become a museum, preserving a moment in time while reality moved on without it. The stable scores weren't proof of quality. They were proof of irrelevance.

This is the drift problem, and it kills more eval programs than any technical mistake. Your task taxonomy and coverage map are not artifacts you build once and archive. They are living systems that must evolve with your product, your users, and your model behavior. This chapter shows you how enterprise teams keep taxonomies alive so your eval program stays relevant instead of slowly becoming fake.

## The Three Forms of Drift You Must Plan For

Drift comes in three distinct forms, and each one requires different detection mechanisms and different response patterns. Understanding which type of drift you're facing determines how you respond.

**Intent drift** happens when users start asking new things or phrasing existing requests differently. A healthcare chatbot built in early 2025 might have included detailed coverage for COVID-19 testing procedures, vaccine appointment booking, and isolation guidance. By late 2025, that traffic dropped to background levels while mental health support requests, long COVID symptom tracking, and health insurance navigation queries surged. The user needs evolved. The taxonomy didn't. New features often trigger intent drift. When a SaaS company launches mobile app access, users who previously asked "how do I export my data" start asking "how do I share this screen with my team" because the mobile context enables different workflows. Seasonal patterns create intent drift too. Tax preparation software sees completely different intent distributions in January versus July. Language evolution drives intent drift when users adopt new terminology, whether from cultural trends, industry jargon shifts, or product marketing that introduces new vocabulary.

**Data drift** happens when the information your system relies on changes shape or content. A RAG system built against a knowledge base in March 2025 might face data drift when that knowledge base gets reorganized in July, changing how information chunks, how metadata is structured, and which documents get retrieved for which queries. Policy document updates create data drift. A healthcare system relying on treatment protocols must adapt when clinical guidelines change, when formulary restrictions update, or when prior authorization requirements shift. API response changes trigger data drift in agent systems. If your agent calls a calendar API that returns availability in fifteen-minute increments in version one but switches to thirty-minute increments in version two, your evaluation data that assumes fifteen-minute granularity no longer reflects reality. Even corpus growth creates drift. A support bot trained on 500 articles might behave very differently when the knowledge base expands to 5,000 articles, because retrieval patterns change, because chunking strategies need adjustment, and because the same query now surfaces different context.

**Model drift** happens when system behavior changes even though you think nothing changed. The most obvious form is when you explicitly update the model, swap from GPT-5 to Claude Opus 4.5, or upgrade from Llama 4.0 to Llama 4 Scout. But subtler forms catch teams off guard. Prompt changes trigger model drift, whether you're refining instructions for clarity, adding new constraints for safety, or adjusting tone guidance for brand voice. Tool configuration changes create drift in agent systems. Adding a new tool, removing a deprecated API, or changing parameter schemas all shift how the model plans and executes. Safety rule updates introduce drift when you tighten content filters, expand PII detection patterns, or adjust moderation thresholds. Vendor updates can cause drift you never asked for. In 2024 and 2025, teams discovered that model providers occasionally update deployed models with bug fixes or safety improvements, and those updates can change output style, reasoning patterns, or refusal behavior without any action on your part.

The key insight here is that a taxonomy which doesn't evolve becomes a measurement of the past, not the present. Your eval scores will look stable and reassuring while reality diverges. You'll miss new failure modes. You'll over-invest in testing intents that no longer matter. You'll under-invest in covering the risks that actually threaten your users today.

This divergence happens gradually, which makes it insidious. In month one, your taxonomy might cover 95% of real user intents. By month three, coverage drops to 88% as new features launch and user behavior shifts. By month six, coverage is at 79% and your eval program is measuring a product that no longer exists. Each month the gap widens. Each month your confidence in eval results should decline. But if you're only looking at eval scores without comparing them to production signals, you won't notice the drift until a crisis forces you to confront it. A major incident, a user exodus, a compliance audit, or a competitor launch that exposes your quality gaps. By then, catching up requires weeks of emergency taxonomy work instead of hours of routine maintenance.

## The Taxonomy Update Loop

Enterprise teams treat taxonomy maintenance as a product management discipline, not a one-time documentation task. They run a continuous loop with clear cadences, clear triggers, and clear ownership.

The loop starts with **collecting signals** about what changed in production. You gather data from multiple sources because no single signal catches everything. Production logs show you which intents are growing in volume and which are declining. Unknown or unclassified query buckets reveal new user needs your taxonomy never anticipated. Escalation logs and low satisfaction conversations tell you where the system is struggling with real user requests. Support ticket clusters highlight repeated failure patterns. Product release notes tell you what features launched and therefore what new tasks users can now attempt. Compliance and policy updates signal new requirements your system must handle. User research and customer feedback sessions surface emergent needs before they show up in volume.

From those signals, you **detect new intents and shifts** in the distribution. This is pattern recognition work. You look for query clusters that don't fit existing taxonomy nodes. You identify phrasing changes where users ask for the same outcome but use new vocabulary. You spot new edge cases that existing tasks didn't anticipate. You notice volume shifts where a previously rare intent becomes common or a previously dominant intent fades to background noise.

Pattern detection requires looking at both what's appearing and what's disappearing. A surge in queries about a specific feature might indicate a new intent worth adding to your taxonomy. But a decline in queries about an old feature might indicate that users have moved to a different workflow, which means your taxonomy is over-invested in coverage that no longer matters. Both signals matter. The first tells you where to expand. The second tells you where to prune or deprecate. Effective drift detection tracks both growth and decline, both emergence and obsolescence.

You also watch for subtle shifts in how users phrase requests. Early in a product's life, users might ask "how do I export my data?" A year later, after marketing campaigns and user education, they might ask "where's the download button?" The intent is identical, but the vocabulary changed. Your taxonomy doesn't need a new task, but your eval examples need to include the new phrasing patterns or your coverage will drift toward testing queries nobody actually uses anymore.

Once you've identified what changed, you **update the taxonomy** itself. This requires discipline because bad taxonomy updates break trendlines and make historical comparisons impossible. You add new leaf tasks for genuinely new intents. You split existing tasks when you discover they were conflating multiple distinct user needs. You mark tasks as deprecated when they're no longer relevant but preserve them for historical tracking. You adjust risk tier assignments when business context changes. You update slice definitions when new customer segments, new languages, or new channels enter the product.

After updating the taxonomy, you **update the coverage map and datasets** to reflect the new structure. This means creating eval cases for new tasks, expanding coverage in areas that grew in risk or volume, and potentially reducing investment in areas that declined in importance. You refresh datasets to match current production distributions. You ensure difficulty mixes still reflect real user challenges. You validate that slices still represent your actual user base.

Next, you **add regression tests** for any new critical paths, any incidents that occurred, and any high-risk tasks that entered the taxonomy. These regression tests form your "never break this again" suite that runs in CI before every release.

Before finalizing changes, you **review and sign off** with stakeholders. Product confirms the new tasks match user needs. Engineering confirms the new coverage is feasible. Legal or compliance confirms new risk tiers are appropriate. Trust and safety confirms new safety-critical tasks are identified.

Finally, you **version everything**. The taxonomy gets a version number. The coverage map gets aligned to that taxonomy version. Datasets get tagged to indicate which taxonomy version they were built against. This versioning makes historical comparisons possible and makes rollback feasible if a change causes problems.

This loop isn't optional maintenance. It's the mechanism that keeps your eval program honest. Without it, your metrics become fiction.

## Cadences and Triggers for Taxonomy Updates

Enterprise teams don't wait for catastrophic drift before updating taxonomies. They run regular review cycles and respond to specific triggers.

**Weekly reviews** take thirty to sixty minutes and focus on fast-moving signals. You pull the top unknown or unclassified items from production logs, typically the top 100 to 500 examples. You review escalations and low satisfaction interactions from the past week. You check for safety incidents, PII exposures, or near-miss events. You scan for new feature releases that might introduce new tasks. For each signal, you decide whether it maps to an existing task or requires a new taxonomy node. If it's new and significant, you add five to twenty eval cases to your datasets immediately, mark the gap in your coverage map, and schedule fuller expansion for the monthly review. If it represents a critical risk, you add regression tests before the next release.

**Monthly reviews** take one to two hours and focus on strategic alignment. You examine the top twenty tasks by traffic volume and confirm they still represent real user behavior. You review the top twenty tasks by risk tier and validate that risk assignments still match business context. You identify the worst-performing slices and determine whether poor performance reflects real quality issues or misaligned datasets. You check dataset freshness and decide which eval sets need expansion or refresh. You review taxonomy structure to catch sprawl, where too many hyper-specific tasks dilute focus, or conflation, where one task is trying to cover too many distinct intents. You confirm ownership assignments are still correct as team structures shift.

**Per-release reviews** happen before every significant deployment and focus on change impact. You review what changed in the release: prompt updates, model swaps, tool additions or removals, policy changes, safety rule adjustments. You run your regression suite and compare performance across all major slices. You add new tests for workflows the release introduced. You verify that any incidents from the previous release cycle have been converted into regression tests and coverage expansions.

Beyond scheduled reviews, certain **triggers** demand immediate taxonomy updates regardless of cadence. A new feature launch always requires taxonomy expansion because new features create new tasks, new risk profiles, and new failure modes. A production incident, especially one that reached users or violated compliance requirements, always generates new eval cases and often reveals taxonomy gaps. A safety or PII exposure immediately triggers review of whether that risk category was adequately covered and whether risk tiers need adjustment. A top enterprise tenant reporting a new workflow or a previously unseen failure pattern triggers immediate investigation because enterprise customers often represent significant revenue concentration.

Incident-driven taxonomy updates deserve special attention because they represent the highest-value signal you can get: real failures that reached real users. When an incident occurs, the taxonomy update workflow should be automatic and mandatory. You add the incident case to your eval dataset. You verify whether the incident revealed a gap in your taxonomy, a task that should exist but doesn't, or a risk tier that was set too low. You add regression tests so the specific failure never recurs. You check whether similar patterns might exist elsewhere in your product that the incident didn't touch but that share the same vulnerability. This incident response discipline turns every production failure into a permanent improvement in your eval program, ensuring you don't fail the same way twice.

The discipline here is treating taxonomy maintenance as ongoing product work, not sporadic firefighting. Teams that wait until drift becomes obvious have already lost weeks or months of meaningful eval signal. By the time your metrics diverge noticeably from user experience, you've already shipped multiple releases with degraded quality that your eval program failed to catch. The cost of reactive taxonomy updates is measured in user churn, support burden, and engineering time spent debugging problems that should never have reached production.

Proactive taxonomy maintenance, by contrast, catches drift early when it's cheap to fix. A weekly review that identifies a new intent and adds twenty eval cases prevents that intent from becoming a blind spot that persists for months. A monthly review that spots a risk tier mismatch prevents shipping a feature with inadequate testing. A per-release review that validates taxonomy coverage prevents regressions from reaching users. The time investment is modest, typically two to four hours per month for a small to mid-sized product, and the return is an eval program that remains trustworthy as your product evolves.

## Detecting New Intents with Simple, Reliable Methods

You don't need sophisticated clustering algorithms or advanced analytics to find new intents. Enterprise teams rely on simple methods that work reliably and require minimal tooling.

The **top unknown bucket** approach is the most straightforward. In your intent classification system, whether it's a human review process, a lightweight classifier, or a model-based routing layer, you maintain a label for UNKNOWN or OTHER. Any query that doesn't clearly map to an existing taxonomy task gets tagged with this label. Weekly, you inspect the top 100 to 500 examples in this bucket, sorted by frequency or by severity if you have signals like escalation or low satisfaction attached. For each cluster of similar examples, you decide whether to map them to an existing task with clearer documentation or examples, or whether to create a new leaf task because they represent a genuinely new user intent. This method requires almost no infrastructure. You're manually reviewing a small set of examples, applying human judgment, and making taxonomy decisions. It scales surprisingly well because you're only reviewing the head of the unknown distribution, not every edge case.

**Complaint-driven discovery** mines your richest signal source: the interactions where users were unhappy. You examine escalations where the system handed off to a human agent. You review low customer satisfaction conversations where users rated the interaction poorly. You look at agent fallback events where the system explicitly said it couldn't help. You analyze repeated user rephrases, the pattern where a user asks a question, gets an answer, and then says "no, I mean..." These moments reveal both missing intents and misunderstood intents. A missing intent shows up when users ask for something your taxonomy never anticipated. A misunderstood intent shows up when your system routes the query to the wrong task or retrieves the wrong context. Both problems require taxonomy updates: the first needs a new node, the second needs clearer boundaries between existing nodes.

**Product change triggers** create a formal handoff between product development and eval program maintenance. Every new feature must declare, before it ships, what new intents it creates, what new risk tiers it introduces, what new slices it affects (new customer segments, new languages, new channels), and what new tests are required. This declaration becomes part of the feature specification, reviewed in design and confirmed before launch. The eval team uses this declaration to expand the taxonomy, update the coverage map, and build eval datasets before the feature reaches production. No feature ships without an eval plan. This discipline prevents the common failure mode where features launch, users adopt them, problems emerge, and only then does the eval team scramble to add coverage.

A practical implementation of product change triggers is a lightweight template that product managers fill out during feature design. The template asks: what user intents does this feature enable that didn't exist before? What existing intents does it modify? What's the risk tier of this feature based on potential failure impact? What user segments will have access? What languages or regions are included in the launch? What are three to five test scenarios that would catch a regression in this feature? This template takes ten to fifteen minutes to complete and prevents months of eval debt.

These methods work because they're grounded in observable user behavior, not speculation. You're reacting to what users actually do, what actually fails, and what your product actually ships. You're not predicting what might matter. You're measuring what demonstrably does matter based on production evidence.

## Updating Taxonomy Without Breaking Historical Comparisons

The tension in taxonomy maintenance is that you need to evolve the structure to stay relevant, but changes can break your ability to compare performance over time. If you rename a task, your month-over-month trendline breaks. If you split a task into two, you can't easily compare the new structure to the old results. Enterprise teams follow disciplined change rules to preserve comparability while allowing necessary evolution.

**Safe change rules** prioritize stability for core tasks while allowing growth at the edges. You avoid renaming tasks casually because renaming breaks trendlines and creates confusion. If a task name is genuinely misleading or outdated, you rename it but document the change with a migration note that explicitly states the old name, the new name, the date of the change, and the reason. More often, you prefer to add new leaf tasks rather than modify existing ones. If you discover that your "password reset" task actually covers two distinct intents, one for forgotten passwords and one for security-driven forced resets, you split the task into two new children: "password reset (user-initiated)" and "password reset (security-forced)". You mark the original task as deprecated but keep it in the taxonomy for historical tracking. Your new eval runs measure the two child tasks. Your historical data remains attached to the parent task. You maintain comparability by tracking the parent task's aggregate performance as the sum of the two children.

When a task becomes obsolete because a feature was removed, a workflow changed, or user behavior shifted away from that intent entirely, you **deprecate** rather than delete. You mark the task as DEPRECATED in your taxonomy with a deprecation date and a reason. You keep the task in your taxonomy structure so historical results remain interpretable. You remove it from active coverage requirements after a grace period, typically one to three months, so you're not investing new effort in building eval cases for irrelevant tasks. But you keep the task definition and its historical data accessible for trend analysis and incident investigation.

**Migration notes** are mandatory whenever you split, merge, rename, or deprecate tasks. A migration note records what changed, why it changed, when it changed, and how old data maps to new tasks. For a split, you note that task A was split into tasks B and C because user research revealed two distinct intents, and you provide best-effort guidance on how to reclassify old examples. For a merge, you note that tasks X and Y were merged into task Z because they represented the same user intent with superficial phrasing differences. For a rename, you note the old name and new name with the rationale. These notes live in your taxonomy documentation, in your version history, and ideally in structured metadata so tooling can surface them automatically when someone is analyzing trends.

The discipline here is recognizing that your taxonomy serves two masters: current measurement needs and historical comparability. You can evolve the structure, but you must do so in a way that preserves the ability to understand how quality changed over time.

## Common Failure Modes and How to Fix Them

Three failure modes appear repeatedly in production eval programs, each with clear symptoms and clear fixes.

**Failure mode one: you keep missing new problems.** Symptoms include production incidents that involve user intents your eval program never tested, escalation patterns that don't appear in your coverage map, and user complaints about failures your metrics say shouldn't be happening. Root causes are typically that taxonomy reviews are rare or informal, that you don't have an unknown bucket review process, and that incidents don't trigger taxonomy updates. When an incident happens, you fix the immediate bug but you don't add the incident case to your regression suite or expand your taxonomy to cover the gap the incident revealed. The fix is instituting weekly drift reviews where you inspect unknown buckets, escalations, and low satisfaction interactions, and implementing an incident workflow that forces taxonomy and eval updates before the incident is marked resolved.

**Failure mode two: you can't compare month to month performance.** Symptoms include trendlines that break whenever you update the taxonomy, historical data that no longer maps to current task definitions, and teams arguing about whether quality improved or declined because the measurement changed. Root causes are tasks being renamed or restructured constantly without migration notes, datasets being replaced wholesale rather than versioned and evolved, and lack of a stable regression suite that serves as a consistent anchor. The fix is maintaining a core set of stable tasks that almost never change, versioning your taxonomy with explicit version numbers and migration notes, and keeping a regression suite that runs consistently across releases so you always have at least one stable measurement to anchor comparisons.

**Failure mode three: the taxonomy exploded into hundreds of hyper-specific tasks.** Symptoms include a taxonomy with 300 or 500 leaf tasks, difficulty assigning new queries to the right task because boundaries are unclear, and teams spending more time managing the taxonomy than using it. Root causes are lack of standards for what constitutes a leaf task, people adding extremely specific tasks for every minor variation, and no review process that challenges whether a proposed new task is actually distinct from existing tasks. The fix is defining clear standards for leaf task granularity, typically "testable and meaningful" where a task is large enough that multiple examples can share it and small enough that pass or fail has business significance. You push variations into slices, difficulty tags, or simply multiple examples within the same task rather than creating separate tasks. You institute a review gate where new task proposals must be justified and approved by an eval owner or working group that enforces consistency.

Taxonomy sprawl creates several secondary problems beyond just management burden. When you have hundreds of hyper-specific tasks, you dilute your eval coverage. Each task gets only a handful of examples because you can't afford to build comprehensive datasets for 500 tasks. Your metrics become noisy because small sample sizes produce unstable measurements. Your regression suite becomes unwieldy, taking hours to run and producing hundreds of pass-fail signals that are impossible to interpret. Your team loses the ability to reason about quality because no one can hold 500 task definitions in their head. The taxonomy becomes a bureaucratic artifact that everyone resents rather than a useful tool that guides evaluation strategy.

The right taxonomy size for most products is twenty to eighty leaf tasks, occasionally reaching 150 for very complex products with many distinct workflows. This range is large enough to capture meaningful distinctions in user intent and small enough to remain manageable, well-tested, and comprehensible. When you're tempted to add task 151, ask whether it represents a genuinely new user intent or whether it's a variation of an existing task that should be captured through examples, slices, or difficulty tags.

These failures are organizational, not technical. They emerge from lack of process discipline, not from wrong algorithms or insufficient tooling.

## The Taxonomy Maintenance Standard Operating Procedure

Enterprise teams run taxonomy maintenance as a repeatable SOP with clear rhythms and clear outputs.

**Weekly maintenance** takes thirty to sixty minutes and focuses on fast signal processing. You pull the top unknown or unclassified items from the past week, the top low satisfaction or escalated interactions, the top safety or PII near-miss events. For each cluster, you decide whether it maps to an existing task or requires a new leaf task. If it's new and represents meaningful volume or risk, you add five to twenty eval examples immediately. You update your coverage map to note any gaps you identified. If the issue is critical, you add regression tests before the next release. You document any new tasks or changes in your taxonomy tracker so the monthly review has context.

The weekly cadence is fast enough to catch emerging patterns before they become systemic problems but not so frequent that it becomes burdensome. Doing this daily would create noise and fatigue. Doing it monthly would miss time-sensitive issues and let drift accumulate. Weekly hits the right balance for most teams.

**Monthly maintenance** takes one to two hours and focuses on strategic review. You examine the top twenty tasks by traffic volume and confirm they still represent real user behavior, not artifacts of outdated routing logic or stale documentation. You review the top twenty tasks by risk tier and validate that risk assignments reflect current business context, regulatory requirements, and failure impact. You identify the worst-performing slices, whether by language, customer segment, channel, or difficulty level, and determine whether poor performance is a real quality issue requiring system improvement or a dataset issue requiring refresh. You confirm dataset freshness by sampling recent production examples and checking whether your eval sets still look like real usage. You review ownership assignments and update them as team structures change.

Monthly reviews also surface structural issues that weekly reviews miss. You look for taxonomy sprawl, where the number of leaf tasks has grown beyond what's manageable. You check for conflation, where a single task is trying to cover too many distinct intents and should be split. You verify that difficulty distributions across your datasets still match production, because drift in difficulty is as problematic as drift in intent distribution. You review your unknown bucket trends over the past month to identify systematic gaps rather than one-off anomalies. These structural reviews prevent slow degradation in taxonomy quality that happens when weekly maintenance focuses only on immediate signals.

**Per-release maintenance** happens before every significant deployment. You review all changes in the release: prompt updates, model swaps, tool additions or removals, policy changes, safety rule adjustments, new features. You run your full regression suite and compare results to the prior release across all major slices. You add new tests for any workflows the release introduced. You verify that any incidents from the previous cycle have been converted into regression tests and reflected in the taxonomy. You confirm that the release won't degrade performance on critical tasks before it ships.

Beyond these scheduled rhythms, certain events trigger immediate updates. A production incident always triggers a taxonomy review and regression test addition. A new feature launch always triggers taxonomy expansion and coverage map updates. A top enterprise customer reporting a new workflow or failure pattern always triggers investigation and potential taxonomy addition.

This SOP turns taxonomy maintenance from ad hoc firefighting into predictable, manageable work. Teams budget the time, assign ownership, and track completion. Taxonomy drift becomes a managed risk instead of a slow-motion crisis.

The key to making this SOP sustainable is treating it as non-negotiable infrastructure work, like security updates or dependency patches. You don't skip weekly drift reviews because you're busy shipping features. You don't defer monthly taxonomy reviews because they're inconvenient. You don't skip per-release validation because you're confident nothing important changed. These maintenance cycles are what keep your eval program honest, and skipping them accumulates eval debt that eventually becomes impossible to pay down.

Teams sometimes ask whether they can automate taxonomy maintenance with clustering algorithms, LLM-based intent detection, or automated drift monitoring. The answer is that automation can assist with signal collection and pattern detection, but it can't replace human judgment about what constitutes a meaningful new intent, what risk tier a task belongs to, or how to structure the taxonomy for clarity and usability. Automation can flag that 500 queries in the past week don't map cleanly to existing tasks. It can't decide whether those 500 queries represent a new leaf task, a variation of an existing task, or noise that should be filtered out. That judgment requires understanding user needs, product strategy, and business priorities. It's product management work, not algorithm work.

## Enterprise Expectations for Taxonomy as a Product Artifact

Serious teams treat the taxonomy like any other product artifact: owned, versioned, reviewed, and tied to releases.

**Ownership** is explicit. Each major branch of the taxonomy has a designated owner, typically someone from product or domain expertise, who is responsible for keeping that branch current, relevant, and accurate. An eval owner or small working group reviews and approves changes to ensure consistency, prevent sprawl, and maintain standards. This ownership structure prevents the tragedy of the commons where everyone assumes someone else is maintaining the taxonomy. Clear ownership also creates accountability. When drift is detected, you know who to talk to. When a new feature needs taxonomy coverage, you know who reviews the proposal. When an incident reveals a taxonomy gap, you know who ensures it gets addressed. Without explicit ownership, taxonomy maintenance becomes nobody's job, which means it doesn't get done.

**Versioning** is rigorous. The taxonomy has a version number, typically semantic versioning like v1.0, v1.1, v1.2. The coverage map version is aligned with the taxonomy version. Datasets are tagged to indicate which taxonomy version they were built against. This versioning makes it possible to reproduce eval runs, understand historical trends, and roll back if a taxonomy change causes problems. Version numbers increment when you add or remove tasks, when you split or merge tasks, when you change risk tier assignments, or when you restructure task hierarchies. Minor updates like adding examples to existing tasks or clarifying task descriptions don't require version increments. This versioning discipline creates an audit trail that explains how your eval strategy evolved alongside your product.

**Review processes** are formal. New task proposals go through a lightweight review where an eval owner confirms the task is distinct, meaningful, and appropriately scoped. Significant restructuring, like splitting a major task or changing risk tiers, goes through broader stakeholder review involving product, engineering, legal, and compliance as appropriate. Changes are documented in migration notes that explain what changed, why, and how old data maps to new structure. This review discipline prevents taxonomy chaos where different team members make conflicting changes, where task boundaries become unclear, or where the taxonomy evolves in directions that don't serve the eval program's needs. The review gate adds minimal friction, typically five to fifteen minutes per proposal, while preventing hours of cleanup work later.

**Tie to releases** is explicit. Before a release ships, the taxonomy and coverage map are reviewed to confirm they reflect what's actually in the release. Regression tests tied to the taxonomy run in CI and must pass before deployment. If the release introduces new workflows or changes existing ones, the taxonomy is updated to match before the release is marked complete. This release gate ensures that your eval program evolves in lockstep with your product, that new features ship with appropriate test coverage, and that changes to existing features trigger updates to the eval cases that test them. Without this discipline, your product and your eval program drift apart, creating the gap where quality regressions hide.

**Incident integration** is automatic. Every production incident produces several outputs: a root cause analysis, a fix to the system, a new set of eval cases that would have caught the incident, regression tests so the incident never happens again, and a taxonomy update if the incident revealed a gap in coverage. These outputs are tracked in the incident workflow and verified during incident closure.

The incident integration workflow typically works like this: when an incident is filed, the incident owner identifies whether the failure was covered by existing taxonomy or represents a gap. If it's a gap, a taxonomy change request is filed as part of incident resolution. If it's covered but the eval cases didn't catch it, the dataset is reviewed to understand why. Maybe the eval examples didn't include the specific edge case that triggered the incident. Maybe the success criteria were too loose. Maybe the test was marked as non-blocking when it should have blocked the release. Either way, the eval program is updated to catch that failure mode in the future, and the incident isn't closed until those updates are verified.

**Permanent core benchmark** is maintained. Serious teams keep a stable set of core tasks, core datasets, and core regression tests that almost never change. This benchmark serves as a consistent reference point across months and years, allowing long-term trend analysis even as the broader taxonomy evolves. When you compare a release in January 2026 to a release in January 2025, the core benchmark tells you whether fundamental quality improved or declined, independent of taxonomy changes.

The core benchmark typically represents 10% to 20% of your total eval coverage but covers the most critical, highest-volume, and most stable aspects of your product. These are the tasks that existed from day one and will exist as long as the product exists. For a customer support bot, the core benchmark might include checking account status, resetting passwords, explaining billing charges, and escalating to human support. These fundamental capabilities anchor your quality measurement even as you add features, enter new markets, or evolve your taxonomy to reflect changing user needs.

This level of discipline transforms the taxonomy from a static document into a living system that evolves with your product while maintaining the rigor needed for meaningful measurement. It's the difference between an eval program that slowly becomes irrelevant and one that remains a trusted source of truth about quality. Teams that neglect taxonomy maintenance watch their eval scores become decorative numbers that bear no relationship to user experience. Teams that maintain taxonomies rigorously have eval programs that catch problems before users report them, that guide product decisions with trustworthy data, and that provide early warning when quality degrades.

The next subchapter addresses a question teams often ask once they've built comprehensive coverage: how do you decide how many test cases are enough without turning the problem into complex statistics?
