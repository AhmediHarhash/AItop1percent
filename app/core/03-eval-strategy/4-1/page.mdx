# 4.1 â€” What Counts as Ground Truth Per Task Type

In late 2025, a healthcare technology company spent four months building an evaluation framework for their clinical documentation assistant. They'd hired two contractors to label 800 test cases, written detailed rubrics, and built a dashboard tracking fourteen metrics. When they ran their first full eval, the results looked excellent: 91% accuracy, 88% helpfulness scores, strong marks across every dimension. Engineering celebrated. Product approved the pilot launch. Then nurses started using it.

Within three weeks, the system was pulled from two hospital floors. The failures weren't subtle. The AI confidently summarized patient histories that contradicted the source notes. It invented medication dosages when records were incomplete. It generated perfectly formatted discharge instructions that contained medically dangerous advice. When the team went back to their evaluation data, they found the problem immediately: their ground truth was designed for chat tasks, not medical RAG tasks. They'd been measuring whether outputs sounded helpful and clear, not whether they were evidence-backed and safe to act on. Their 91% accuracy was measuring the wrong kind of correctness entirely.

This is the foundational mistake in enterprise evaluation design. Ground truth is not a universal concept you define once and apply everywhere. What counts as correct depends entirely on the task architecture you're evaluating. A chat assistant, a retrieval-augmented generation system, an agentic workflow, and a voice interface all require fundamentally different definitions of correctness. If you treat them the same way, your evals will be precise measurements of the wrong thing, and you won't discover the error until production failures force you to look closer.

## The Task Architecture Determines Truth Structure

Most teams approach ground truth design by asking "what's the right answer?" This is the wrong question. The correct question is "what does correctness mean for this task architecture?" The distinction matters because different AI task types have different success criteria, different failure modes, and different relationships between outputs and truth.

For a chat task, ground truth is often a pattern of acceptable responses. There's no single correct way to explain a concept or draft an email. Correctness means the response includes required elements, avoids prohibited content, matches the appropriate tone, and serves the user's goal. You're measuring fitness for purpose, not matching a reference string. If your ground truth expects word-for-word alignment, you'll penalize good outputs that use different phrasing. If your ground truth is too loose, you won't catch responses that sound helpful but miss critical requirements.

For RAG tasks, ground truth must be evidence-backed. The correct answer isn't just factually accurate in the abstract; it must be supported by the retrieved documents provided to the system. This creates a fundamentally different truth structure. A statement can be true in the world but wrong for your RAG system if it's not present in your knowledge base. More importantly, when evidence is missing or insufficient, the correct behavior is abstention, not speculation. "I can't find this in the provided sources" is not a failure; it's often the correct ground truth. Teams that don't design for evidence-backed truth end up measuring fluency and confidence instead of grounded accuracy, which is exactly what happened to the healthcare company.

Agent tasks require behavioral truth. You're not just evaluating the final text output; you're evaluating the sequence of tool calls, the parameters passed to each tool, the handling of tool failures, the respect for safety gates, and the final state of the system. An agent that produces the right outcome through incorrect tool usage is not correct. An agent that calls tools redundantly or skips required confirmation steps is not correct, even if the user gets what they asked for. Ground truth for agents must specify correct traces, not just correct endpoints. If you only measure outcomes, you'll approve agents that work through brittle or unsafe paths.

Voice interfaces add multiple truth layers simultaneously. You need transcript-level truth to measure whether the system heard correctly. You need dialogue-level truth to measure turn-taking, barge-in handling, and confirmation behavior. You need task-level truth to measure whether the user's goal was achieved. You need safety-level truth to ensure no PII was leaked and critical fields were confirmed through the required protocol. A voice system can succeed at the transcript layer and fail at the task layer. It can succeed at the task layer and fail at the safety layer. Ground truth must account for all dimensions, not collapse them into a single correctness judgment.

## Single-Answer Truth and When to Use It

The simplest form of ground truth is single-answer truth: one question, one correct response, no acceptable variations. This appears most often in factual lookup tasks, API documentation queries, policy classification, and deterministic computations. "What is the endpoint for user authentication?" has one correct answer. "Which tier of service includes priority support?" has one correct answer, assuming your service tiers are clearly defined and the question provides enough context.

Single-answer truth is easy to implement and easy to score. You can use exact string matching, normalized matching, or semantic similarity with a high threshold. False positives are rare. Rater agreement is near-perfect. Metrics are interpretable. This is why teams default to single-answer truth even when their task doesn't fit the structure. It feels clean and definitive.

The problems emerge when reality changes. Single-answer truth goes stale fast. If your API endpoints change, your policy tiers change, your product features change, or your documentation updates, every affected test case becomes incorrect. Without version control and ownership, your ground truth degrades silently. You'll start seeing eval scores drop not because your model regressed, but because your truth is outdated. Worse, if you don't notice the staleness, you'll tune your system toward obsolete answers, creating a model that's optimized for yesterday's reality.

A logistics company experienced this in 2024 when they updated their shipping zones. Their customer service RAG system had ground truth built around the old zone definitions. When customers asked about shipping costs, the system correctly retrieved information from the updated knowledge base and provided answers based on the new zones. The evaluation framework scored these answers as incorrect because the ground truth still referenced the old zones. For three weeks, the team believed their system was hallucinating shipping information, when in fact it was correct and the ground truth was stale. They wasted engineering time investigating a phantom regression until someone thought to check whether the ground truth had been updated when the zones changed.

Single-answer truth also collapses under ambiguity. If the question is "What's our refund policy?" and your policy has different rules for different product categories, time windows, and customer tiers, there is no single answer. If your ground truth picks one scenario and treats it as universal, you'll penalize correct answers that apply to different scenarios. If your ground truth says "it depends" without specifying what it depends on, you've created a vague reference that can't be scored consistently.

The temptation with single-answer truth is to expand it to cover more ground than it should. Teams start with genuinely singular answers like API endpoints, then gradually add questions that have contextual variation. "What's the standard contract term?" might have one answer for most customers but different answers for enterprise customers or international markets. If you encode the standard answer as the single truth, you'll penalize correct outputs that account for these variations. The correct approach is to either make the question more specific or switch to multi-answer truth that acknowledges the variation.

You should use single-answer truth only when the answer space is genuinely singular, the answer is stable over time, and you've implemented version control tied to your source of truth. For everything else, you need multi-answer truth or behavioral truth.

## Multi-Answer Truth and Acceptable Variation

Most production AI tasks allow multiple correct outputs. Writing tasks, summarization tasks, creative generation, tone-adjusted responses, and explanation tasks all have answer spaces with legitimate variation. "Write a polite email declining this meeting" can be done well in dozens of ways. "Explain how authentication works" can be answered at different technical levels, with different examples, in different orders. "Suggest three marketing ideas for this product" has an enormous solution space.

If you define ground truth as a single reference output and score based on similarity to that reference, you'll systematically penalize good responses that differ in structure or phrasing. Your rubrics will reward outputs that mimic your reference style, even when other styles are equally valid. Your evals will be measuring model obedience, not output quality. This is one of the most common eval design mistakes in 2026 enterprise systems.

Multi-answer truth solves this by defining correctness as membership in an acceptable set. Instead of one reference output, you provide multiple anchor examples that span the acceptable range: one short and direct, one detailed and thorough, one formal in tone, one conversational. You define required elements that must appear in any correct output: key facts, mandatory disclaimers, necessary context. You define forbidden elements that disqualify an output: fabricated claims, overpromises, prohibited advice, unsafe instructions. An output is correct if it includes required elements, avoids forbidden elements, and falls within the acceptable range demonstrated by your anchors.

This structure supports scoring through checklists, rubrics, or LLM-as-judge evaluation. You can score each required element as present or absent. You can use a rubric with dimensions like completeness, tone alignment, and factual accuracy. You can ask an LLM judge to compare the output against your anchor set and rate it on a scale. The key is that correctness is defined by criteria and boundaries, not by proximity to a single ideal.

The power of multi-answer truth is that it captures what actually matters. For a customer service response, what matters is that the customer's question was answered, the tone was appropriate, required disclosures were included, and prohibited promises weren't made. The specific words used, the order of information, and stylistic choices are negotiable as long as the core criteria are met. This aligns evaluation with real product requirements rather than arbitrary stylistic preferences.

The failure mode is under-specification. If your acceptable set is too broad, your rubric too vague, or your required elements too sparse, you won't catch bad outputs. If you define correctness as "be helpful and clear," you've created a truth structure that depends entirely on rater judgment, which means your scores will vary by rater, by day, and by mood. You need enough structure to ensure consistency without so much rigidity that you penalize valid variation. This balance comes from anchors: concrete examples that define the edges of acceptable. One anchor showing the minimum acceptable output, one showing an ideal output, and one showing a common failure mode gives raters the calibration they need.

A marketing technology company learned this lesson when they evaluated subject line generation for email campaigns. Initially, they defined ground truth as a single reference subject line per campaign and scored based on semantic similarity. Their evals showed poor performance, but when they tested the outputs manually, many of the "failures" were actually good subject lines that used different hooks or angles than the reference. They switched to multi-answer truth with three to five acceptable subject lines per campaign, plus criteria for forbidden elements like spam trigger words and misleading claims. Their eval scores immediately improved, not because the model changed, but because they were finally measuring quality correctly rather than measuring adherence to arbitrary reference text.

## Behavioral Truth for Safety and Control

The most important form of ground truth in high-risk enterprise systems is behavioral truth. Correctness is defined not by what the system says, but by what the system does. This applies to refusal behaviors, abstention decisions, confirmation protocols, escalation triggers, and tool usage boundaries. Behavioral truth is how you encode safety, compliance, and operational discipline into your evaluation framework.

Behavioral truth appears whenever the correct response is a meta-action rather than a direct answer. When a user asks a question your system can't answer safely, the correct behavior is abstention: "I can't answer this based on the provided information." When a user asks for something that violates policy, the correct behavior is refusal: "I can't help with that, but here's what I can do instead." When an agent needs to execute a high-risk action, the correct behavior is confirmation: "Before I delete these files, please confirm you want to proceed." When a voice system hears a critical field like a credit card number or medical record number, the correct behavior is read-back confirmation: "I heard 4-2-3-7. Is that correct?"

These behaviors are not optional niceties. They're the control surface for risk. If your ground truth doesn't include behavioral expectations, you can't measure whether your system respects them. If you only evaluate the final text output, you'll approve systems that skip confirmations, ignore abstention triggers, and proceed confidently through unsafe paths. This is exactly what happened to the healthcare documentation system. Their ground truth measured output quality, not evidence discipline. The system learned to generate confident, well-formatted answers in all cases, including cases where the correct behavior was refusing to generate an answer at all.

The healthcare company's failure was instructive because it revealed how behavioral truth interacts with training incentives. Their evaluation dataset contained 800 cases, and in 790 of them, the correct behavior was generating a complete answer. Only 10 cases required abstention because evidence was missing. The system learned that generating output was almost always correct, so it generated output in all cases, including the 10 where abstention was required. When they audited production failures, they found that evidence-limited scenarios occurred in about 18% of real usage, far more than the 1.25% represented in their eval dataset. They'd trained and evaluated a system that worked for the common case but failed systematically on a critical minority case.

Behavioral truth requires you to define correct behavior for edge cases explicitly. When retrieved documents contradict each other, what should the system do? When a tool call fails, should the agent retry, fall back to a different tool, or escalate to a human? When a user provides ambiguous input, should the system ask for clarification, proceed with stated assumptions, or refuse to proceed? Each of these scenarios needs a labeled correct behavior, not just a correct output. You're building a specification for how your system should navigate uncertainty, risk, and failure, not just how it should perform when everything works.

Document contradiction is a particularly common scenario that teams handle poorly. If one retrieved document says the policy allows something and another says it doesn't, the system needs to surface the conflict, not pick one source and ignore the other. The correct output might be "I found conflicting information in our documentation: Document A states X, but Document B states Y. I recommend confirming with your manager" or "Our policy documentation appears inconsistent on this point; I'm escalating this to Policy Review." What's not correct is silently choosing one source, averaging the two positions, or fabricating a reconciliation that isn't supported by either document.

The most neglected form of behavioral truth is correct abstention. Teams spend enormous effort defining what a correct answer looks like and almost no effort defining when the system should refuse to answer. This creates models that are rewarded for always producing output, even when silence or escalation is the correct response. In regulated industries, in high-stakes decision support, and in any system touching PII or financial actions, abstention correctness is not optional. Your ground truth must include a set of test cases where the correct answer is "I don't know," "I can't determine this from available information," "This requires human review," or "I need more input to proceed safely." If these cases aren't in your ground truth, your system won't learn them, and your evals won't measure them.

Building abstention ground truth means deliberately creating test cases where the correct answer isn't in your knowledge base, or where the question touches compliance boundaries the system shouldn't adjudicate, or where multiple reasonable interpretations exist and choosing wrong has consequences. A financial services RAG system needs test cases asking about products that were discontinued, about scenarios that require individualized advice, and about regulatory questions that vary by jurisdiction. The correct behavior for all of these is some form of abstention or escalation, not a confident answer based on general knowledge or outdated information.

## Trace Truth for Agentic Systems

Agentic systems require a new category of ground truth: trace truth. You're not just evaluating the outcome; you're evaluating the path. An agent that accomplishes the task through an incorrect sequence of tool calls, with redundant API requests, with missing error handling, or by violating safety constraints has not succeeded, even if the user sees the result they wanted. Trace truth makes the intermediate steps part of the correctness criteria.

Trace truth includes tool selection correctness. Did the agent choose the right tool for the task? If multiple tools could work, did it choose the most efficient or most reliable option? Did it call tools in the correct order, respecting dependencies between steps? If a task requires reading a file before editing it, an agent that edits first and reads later is incorrect, even if the final file state is correct. Tool selection errors compound. An agent that starts down the wrong tool path often has to backtrack, retry, or abandon the task entirely.

The subtlety here is that tool selection reveals the agent's understanding of the task structure. An agent that reaches for a search tool when it already has the necessary context in memory is wasting API calls and adding latency. An agent that tries to update a database record before verifying the record exists is setting itself up for failure. An agent that calls an expensive LLM tool when a simple deterministic function would suffice is burning budget unnecessarily. These aren't just efficiency problems; they're correctness problems. The path the agent takes demonstrates whether it understands the domain, the available capabilities, and the optimal strategy for the task.

Trace truth includes parameter correctness. Did the agent pass the right arguments to each tool? Did it extract parameters from user input correctly, or did it fabricate values? Did it respect parameter constraints like maximum file size, allowed characters, or required formats? An agent that calls the right tool with the wrong parameters is not close to correct; it's fully incorrect. Parameter errors cause tool failures, data corruption, and user frustration. They're not minor deviations; they're architectural failures.

Parameter extraction is where many agents fail silently. The user says "schedule a meeting with the team next Tuesday at 2pm" and the agent extracts "next Tuesday" but interprets it as the wrong week because it doesn't account for timezone context or current date properly. It books the meeting, the tool accepts the parameters, and the agent reports success. Only when Tuesday arrives and the meeting doesn't happen does anyone notice the error. Your ground truth must specify not just that the agent called the scheduling tool, but that it extracted the date correctly, converted it to the appropriate format, included all required participants, and handled timezone conversions properly.

Trace truth includes error handling and retry behavior. When a tool call fails, did the agent detect the failure? Did it interpret the error correctly? Did it retry with corrected parameters, fall back to an alternative tool, or escalate to a human? Did it retry indefinitely, creating an infinite loop? Did it ignore the error and proceed as if the tool had succeeded, leading to downstream failures? Error handling is where agents reveal their robustness. A system that works perfectly in the happy path but collapses under tool failures is not production-ready. Your ground truth must include failure scenarios and specify correct recovery behavior.

The distinction between transient and permanent failures matters enormously. If a tool returns a 503 service unavailable error, retrying after a brief delay is appropriate. If it returns a 400 bad request error, retrying with the same parameters is pointless; the agent needs to fix the parameters or try a different approach. If it returns a 403 forbidden error, the agent doesn't have permission and should escalate rather than retry. Your trace truth needs to encode these distinctions. An agent that retries a 400 error five times is not showing persistence; it's showing poor error comprehension.

Trace truth includes safety and idempotency checks. Did the agent request confirmation before executing destructive actions? Did it verify preconditions before proceeding? Did it avoid calling the same tool redundantly with identical parameters? Did it respect rate limits and concurrency constraints? An agent that books a flight twice because it didn't check whether the first booking succeeded is incorrect. An agent that deletes a file without confirmation is incorrect, even if the user eventually wanted the file deleted. These are not edge cases; they're core correctness criteria.

Idempotency is particularly subtle. Some tools are naturally idempotent: calling them multiple times with the same parameters produces the same result. Other tools are not: each call creates a new record, charges a new transaction, or sends a new message. An agent that doesn't track which tools it has already called can easily create duplicate actions. Your ground truth should include test cases where the agent encounters ambiguity mid-task, backtracks, and needs to avoid re-executing tools that already succeeded. If the agent creates two calendar events instead of one, it failed, even if both events are individually correct.

Building trace truth requires you to log the full execution path and label it step by step. You need the sequence of tool calls, the parameters for each call, the results returned, the errors encountered, and the agent's decision at each branching point. You need to define correct and incorrect paths through the task space. You need to specify when retries are appropriate and when they're wasteful. This is more labor-intensive than labeling final outputs, but it's the only way to evaluate agentic behavior correctly. If you skip trace truth and only evaluate outcomes, you'll deploy agents that succeed through luck, not discipline.

Labeling trace truth also requires domain expertise. You need someone who understands not just what the correct outcome is, but what the correct process is. In a financial services agent, the compliance officer might specify that certain checks must happen before certain actions, regardless of whether skipping them would still produce the correct outcome. In a healthcare agent, clinical protocols might require specific confirmation steps even when the agent is highly confident. These process requirements become part of your trace truth, and they're non-negotiable. An agent that violates process but achieves the outcome is still wrong.

## Voice Truth and Multi-Layer Correctness

Voice interfaces require ground truth at four layers simultaneously: transcription, dialogue management, task completion, and safety compliance. A voice system can fail at any layer independently, and each layer requires its own correctness criteria. Collapsing all four into a single pass-fail judgment hides where the system is weak and prevents targeted improvement.

Transcription truth measures whether the system heard the user correctly. This is the foundation layer. If the transcript is wrong, everything downstream is built on a misunderstanding. Ground truth for transcription is the correct text representation of the audio input. You measure word error rate, phoneme accuracy, and handling of accents, background noise, and crosstalk. Transcription errors are often recoverable if the system detects uncertainty and asks for confirmation, but undetected transcription errors cascade into task failures.

The subtlety is that perfect transcription accuracy isn't always required. If the user says "I need to schedule a doctor's appointment" and the system transcribes it as "I need to schedule a doctor appointment," the missing possessive doesn't affect task completion. But if the user says "cancel my appointment for Tuesday" and the system hears "schedule my appointment for Tuesday," that's a critical transcription failure that leads to the opposite action. Your ground truth needs to distinguish transcription errors that affect task outcome from those that don't, and weight them accordingly.

Dialogue truth measures whether the system managed the conversation correctly. Did it handle turn-taking, or did it interrupt the user mid-sentence? Did it respond to barge-ins appropriately, or did it ignore user corrections? Did it confirm critical fields like numbers, dates, and names, or did it proceed with unverified values? Did it pace the conversation appropriately, or did it rush through confirmations? Dialogue correctness is about the interaction protocol, not the content. A system that gets the task done but violates conversational norms will frustrate users and erode trust.

Confirmation protocol is particularly critical for voice because users can't see what the system understood. If a user schedules a meeting for "2:00 PM" and the system hears "2:30 PM" but doesn't read back the time, the error won't be caught until the meeting time arrives. Voice ground truth must specify when read-back confirmation is required, what format it should take, and how the system should handle corrections. "I've scheduled your appointment for 2:30 PM on Tuesday, March 4th. Is that correct?" is not optional politeness; it's mandatory error detection.

Task truth measures whether the user's goal was accomplished. Did the system book the appointment, submit the ticket, update the record, or retrieve the information the user requested? This is the outcome layer. Task truth often aligns with ground truth from other modalities: for a voice RAG system, task truth means the answer was evidence-backed; for a voice agent, task truth means the tool trace was correct. The difference is that voice tasks often require multi-turn dialogues to gather enough information, so task truth includes whether the system asked the right follow-up questions.

Multi-turn task completion introduces complexity because you're evaluating a conversation flow, not a single response. The system might gather information correctly across three turns but fail on the fourth. Your ground truth needs to specify the acceptable dialogue paths to task completion, including how many turns are reasonable, what information should be gathered when, and how the system should recover if the user provides information out of order.

Safety truth measures whether the system respected privacy, security, and compliance requirements. Did it avoid speaking PII aloud in shared spaces? Did it verify identity before disclosing sensitive information? Did it confirm destructive actions like cancellations or deletions? Did it escalate to a human when required by policy? Safety truth is non-negotiable in regulated industries. A voice system that completes tasks efficiently but leaks patient information or confirms financial transactions without verification is not correct; it's a compliance liability.

A healthcare provider learned this when they deployed a voice assistant for patient check-in. The system would confirm appointments by speaking patient names and appointment reasons aloud in the waiting room, violating HIPAA privacy requirements. Their evaluation framework had measured task completion and dialogue quality but not privacy compliance. They had to pull the system and rebuild it to use visual confirmation on a screen rather than verbal confirmation in shared spaces. If their ground truth had included safety layer testing, they would have caught this before deployment.

Defining ground truth for voice means labeling all four layers for each test case. You need the reference transcript, the expected dialogue behavior, the correct task outcome, and the required safety checks. This is more complex than single-modality truth, but it's unavoidable. Voice systems operate in higher-risk contexts, often with vulnerable users, and often in domains where errors have immediate consequences. If your ground truth only measures task completion, you'll approve systems that work but violate safety protocols. If your ground truth only measures safety, you'll approve systems that are cautious but ineffective. You need both, plus the conversational quality that determines whether users will tolerate the system.

## Versioning Ground Truth and Preventing Staleness

Ground truth is not static. Your product changes, your policies update, your knowledge base evolves, and your user needs shift. If your ground truth doesn't track these changes, it becomes a historical artifact that measures your system against obsolete requirements. Stale ground truth causes eval scores to drift, makes regression detection impossible, and leads teams to optimize for the wrong target.

Versioning ground truth means treating it as a controlled artifact with the same discipline you apply to code or configuration. Every ground truth dataset needs a version number, a creation date, a last-updated timestamp, and a changelog. When your product releases a new feature, your ground truth must be updated to reflect the new correct behavior. When your policy changes, your ground truth must be updated to reflect the new rules. When your knowledge base is refreshed, your ground truth must be updated to match the new source documents. This coupling is not optional; it's the only way to keep evals meaningful.

The most common versioning failure is orphaned ground truth. A team creates a ground truth dataset, uses it for three months, then moves on to other priorities. Six months later, the product has changed substantially, but the ground truth hasn't. New features aren't covered, old behaviors are still expected, and deprecated functionality is still tested. The eval scores start declining, not because the model regressed, but because the model is being measured against outdated standards. Teams waste weeks investigating phantom regressions before they realize the ground truth is stale.

This happened to a SaaS company in early 2025. They built a comprehensive ground truth dataset for their document processing assistant in Q1, evaluated rigorously, and achieved strong baseline performance. By Q4, their eval scores had dropped fifteen percentage points. Engineering assumed the model had degraded and began investigating training data contamination, prompt drift, and API changes. After two weeks of investigation, they discovered the real cause: their product had added three new document types, changed the formatting rules for two existing types, and updated the compliance requirements for financial documents. None of these changes had been reflected in ground truth. The model was actually performing better than before, but it was being measured against obsolete standards that penalized correct behavior on the new document types.

The second most common failure is inconsistent versioning across ground truth subsets. You have separate datasets for chat, RAG, and agent tasks. You update the chat ground truth when the product changes, but you forget to update the RAG and agent datasets. Now your eval results are internally inconsistent. The chat eval shows improvement, the RAG eval shows regression, and the agent eval is unchanged, but the real cause is that only one dataset is current. You can't compare results across task types because they're measuring against different versions of correct behavior.

Cross-dataset version skew creates particularly insidious problems when you're trying to diagnose performance issues. You see that chat quality improved but RAG quality declined, and you hypothesize that the model is trading retrieval accuracy for conversational fluency. You adjust prompts to rebalance, but the problem persists. Only later do you realize the RAG dataset was testing against a knowledge base version from three months ago, and the "regression" was actually the model correctly citing updated information that the stale ground truth didn't recognize.

Preventing staleness requires ownership and automation. Every ground truth dataset needs a named owner responsible for keeping it current. That owner reviews ground truth whenever the product, policy, or knowledge base changes. Ideally, ground truth updates are part of your release checklist: before deploying a new feature, update the ground truth to include test cases for that feature. Before updating your knowledge base, update the ground truth to reflect the new information. This discipline is easier to maintain when ground truth is stored in version control, reviewed through pull requests, and updated alongside the code it evaluates.

Some teams automate staleness detection by tracking the last-modified dates of source materials. If your knowledge base was updated thirty days ago but your ground truth was last updated ninety days ago, the system flags a potential version mismatch. If your product release notes mention new features but your ground truth changelog doesn't reference them, the system alerts the ground truth owner. These automated checks don't eliminate the need for human review, but they catch the most common cases where ground truth drifts out of sync with reality.

## Designing Ground Truth Correctly From the Start

Most teams don't design ground truth; they inherit it. Someone creates a dataset, labels it quickly, and it becomes the de facto standard. Six months later, the team discovers the ground truth was poorly structured, the labels were inconsistent, and the coverage was thin. Fixing it requires re-labeling hundreds or thousands of cases, which nobody has time for, so the team lives with flawed ground truth and compensates by adding more metrics, more dashboards, and more process. The correct approach is to design ground truth deliberately before you start labeling.

Start by identifying your task type. Is this chat, RAG, agent, or voice? Does it fit cleanly into one category, or does it span multiple? A RAG agent combines retrieval and tool execution, which means your ground truth needs evidence-backing criteria and trace correctness criteria. A voice RAG system needs transcription truth, dialogue truth, and evidence-backing truth. Map your task architecture first, then design truth to match.

Next, choose your truth structure. Do you need single-answer truth, multi-answer truth, behavioral truth, or trace truth? Most tasks need a combination. Even if your task allows multiple correct outputs, you probably still need behavioral truth for edge cases like missing information or policy violations. Define your required elements, your forbidden elements, your acceptable variation boundaries, and your behavioral expectations. Write these down before you create any test cases.

Then create anchors. For each major scenario, label three examples: a great output, an acceptable output, and a failure. The great output shows what good looks like. The acceptable output shows the minimum bar. The failure shows a common mistake. These anchors calibrate your labeling process and give raters concrete reference points. Without anchors, raters will invent their own standards, which leads to inconsistency, disagreement, and noisy metrics.

Finally, version it from day one. Even if you only have 50 test cases, treat them as version 1.0. Assign an owner. Link the ground truth to the product version, policy version, or knowledge base version it represents. Add a changelog. When you update the ground truth, document what changed and why. This discipline costs almost nothing at the start and saves enormous time later when you're trying to understand why eval scores shifted.

Ground truth design is not a one-time task. It's an ongoing practice that evolves with your system. The teams that treat it seriously, version it properly, and keep it aligned with their product are the teams whose evals remain meaningful over time. The teams that treat it casually, label it quickly, and never revisit it are the teams whose evals drift into irrelevance, leaving them blind to real regressions and chasing phantom improvements.

## What Comes Next

Once you've defined what counts as correct for your task type, the next challenge is handling the cases where correctness is ambiguous or multiple answers are valid. Chapter 4.2 explains how to design ground truth and scoring when the user's request is unclear, the answer depends on context, or multiple interpretations are reasonable without penalizing good outputs or rewarding dangerous guessing.
