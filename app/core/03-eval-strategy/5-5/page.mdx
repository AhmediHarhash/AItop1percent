# Chapter 5.5 — Privacy, Sanitization & Data Governance

**What we're doing here:**
Imagine you build a great eval dataset from production logs. You share eval results with leadership. Buried in a sample output is a real customer's name, email, and medical question.

That's not a hypothetical. It happens when teams treat privacy as "legal's problem." In evaluation, privacy is **your** problem — because PII in eval sets creates liability, bias, and compliance risk all at once.

This chapter covers how to find PII, remove it safely, control who sees what, and stay compliant with the regulations that matter in 2026.

---

## 1) Why privacy is an evaluation problem

**Liability.** If PII leaks into eval reports, demo outputs, or shared datasets, you've violated data protection laws. GDPR, CCPA, HIPAA — each carries real penalties and customer trust damage.

**Bias.** PII is not random. Names correlate with ethnicity. Zip codes correlate with income and race. Medical history reveals disability status. If your eval set contains these, your model may overfit to those correlations. Sanitization is not just privacy — it's fairness engineering.

**Compliance blockers.** In 2026, enterprise procurement requires SOC 2 Type II, GDPR processing records, and EU AI Act conformity assessments for high-risk systems. If your evaluation pipeline can't prove data governance, you can't sell to regulated customers.

---

## 2) What PII looks like in eval datasets

### Direct identifiers (obvious PII)
Names, email addresses, phone numbers, government IDs (SSN, passport), financial data (credit card numbers, account numbers), and health data (diagnoses, medications, lab results).

### Quasi-identifiers (the sneaky kind)
Combinations of seemingly harmless fields that uniquely identify someone. "VP of Engineering in Boulder, started January 2025" is often enough to identify one person. Research shows that three or more quasi-identifiers can re-identify over 80% of people in "anonymized" datasets. Simple redaction is not enough.

### Behavioral traces
Multi-turn conversation sequences that reveal identity over time. Tool call parameters in agent logs (database queries with customer IDs). Voice biometrics — voiceprints are biometric PII under GDPR and Illinois BIPA.

### Channel-specific patterns
- **Chat:** Users mention names and contact info in free text. Multi-turn dialogs accumulate identity signals.
- **RAG:** Retrieved documents contain PII (contracts, emails, customer records). Queries reveal internal knowledge.
- **Agents:** Tool parameters carry PII — things like `lookup_customer(email="real@person.com")`.
- **Voice:** Audio contains voiceprints. Transcripts contain direct PII plus speech patterns. Background audio may capture third parties.

---

## 3) The sanitization pipeline

Think of sanitization as a three-step assembly line: **detect → redact → validate.**

### Step 1: Detection (finding PII)

In 2026, best practice is to combine three methods:

**Named entity recognition (NER)** — models like spaCy or commercial APIs (AWS Comprehend, Google DLP) that detect names, locations, organizations. Fast and good at common entities, but misses context-specific PII.

**Regex patterns** — pattern-matching for structured PII like emails, phone numbers, SSNs, credit cards. Precise for known formats, but misses obfuscated variants ("five five five, one two three four").

**LLM-assisted scanning** — pass text through an LLM and ask it to identify all PII with type, span, and confidence. Catches context-dependent PII that rules miss (nicknames, implied identifiers). Slower, but catches edge cases.

**Default: run all three.** Regex for structured PII (high precision), NER for entity-level (speed plus recall), LLM for ambiguous cases (catch what the others miss).

### Step 2: Redaction (what to do with detected PII)

**Replacement** (recommended for most eval datasets): swap PII with realistic fake values. "Sarah Johnson" becomes "Jane Doe." Preserves sentence structure and realism. Be careful not to introduce bias in replacements — don't always swap Hispanic names with Anglo names.

**Redaction** (safest option): replace with placeholders like `[NAME]` or `[EMAIL]`. Simple and safe, but loses signal for multi-turn consistency.

**Generalization** (for quasi-identifiers): replace specific values with categories. "30-year-old from Boulder" becomes "30-year-old from [CITY_100K-250K]." Reduces re-identification risk while preserving distributional signal.

### Step 3: Validation (always verify)

Sanitization is fallible. Always:
1. Re-run PII detection on sanitized output (should find zero direct identifiers)
2. Human spot-check 1–2% of sanitized examples for missed PII
3. Test re-identification: could a reviewer identify the original user from the sanitized version?
4. Utility check: does the sanitized text still make sense for evaluation?

---

## 4) Compliance frameworks (2026 landscape)

You don't need to be a lawyer, but you need to know what each framework cares about:

**GDPR (EU):** Legal basis for processing (evaluation typically falls under legitimate interest — document it). Data minimization. Purpose limitation. Right to erasure — if a user requests deletion, you must purge their data from eval sets. Retention limits.

**CCPA/CPRA (California):** Right to know what data is collected. Right to delete. Right to opt out of "sale" (broadly defined). If your eval dataset contains California users' production logs, support deletion requests.

**HIPAA (healthcare):** Protected Health Information must be de-identified using Safe Harbor (remove 18 identifier types) or Expert Determination (statistical proof of low re-identification risk). Business Associate Agreements required for third-party eval vendors.

**SOC 2 Type II:** Access controls, audit logs, data retention policies, incident response. Your SOC 2 audit will review eval dataset governance. Expect auditors to spot-check access logs and sanitization validation.

**EU AI Act (in effect 2026):** High-risk AI systems need training data governance (relevant, representative, free of errors), logging and traceability, human oversight of evaluation, and conformity assessment. Your eval pipeline itself becomes a compliance artifact.

**Default legal basis:** Most enterprise eval pipelines rely on legitimate interest, not explicit consent. But document your legal basis in writing and get legal counsel to review it.

---

## 5) Access control & retention

**Access tiers (least-privilege principle):**
- **Raw production logs** — data engineering team only, 30–90 day retention, auto-delete after
- **Sanitized eval sets** — eval team plus annotators, 1-year retention
- **Gold eval sets** — eval team plus approved researchers, 3-year retention, anonymized
- **Aggregate reports** — broadly accessible, no PII, indefinite retention

**Audit logging:** Log every access to eval datasets — who, when, what action, which dataset version, success or failure. Retain logs for 1–3 years. You'll need these for SOC 2 audits and incident investigation.

**Right to erasure (GDPR Article 17):** If a user requests deletion, you need to find all datasets containing their data and delete within 30 days. This requires lineage tracking — a mapping from user IDs to dataset versions. If you can't locate user data across your eval sets, you can't honor deletion requests. Build this mapping early.

---

## 6) Knobs & defaults

**PII detection confidence threshold:** Default 0.75 (balanced). Use 0.50 for aggressive detection (safer, more false positives). Use 0.90 for conservative detection (fewer false positives, may miss PII). Prefer over-detection — a false positive is inconvenient, a missed PII is a compliance violation.

**Redaction strategy:** Use replacement for direct identifiers (names, emails → synthetic values). Use generalization for quasi-identifiers (rare job titles → role categories). Use session ID hashing for behavioral traces (preserve multi-turn linkage without real user IDs).

**Retention windows:** Raw logs 90 days, sanitized eval sets 1 year, gold sets 3 years, audit logs 3 years, aggregate reports indefinite.

---

## 7) Failure modes

**"PII leaked into an eval report shared with leadership."**
Sanitization pipeline missed PII, or report pulled from raw logs instead of sanitized set. Fix: enforce policy that all shared outputs come from sanitized datasets only. Add automated PII scan to report generation.

**"Over-redaction destroyed signal — examples are unreadable."**
Too-aggressive detection (low threshold) plus redaction instead of replacement. Fix: increase threshold, switch to synthetic replacement, add utility check after sanitization.

**"GDPR deletion request can't be fulfilled — we don't know which datasets have this user's data."**
No lineage tracking. Fix: implement a user_id-to-dataset_version mapping table. Use pseudonymization (reversible hash) during sanitization so you can trace deletion requests.

**"SOC 2 audit failed — no proof of who accessed eval datasets."**
Audit logging not enabled on storage. Fix: enable audit logging on all eval dataset buckets, set retention to 3 years, run quarterly access reviews.

---

## 8) Enterprise expectations

- They treat eval datasets as **regulated data assets** with the same rigor as production databases
- They implement **multi-layer detection** (NER + regex + LLM) — not just regex
- They use **replacement over redaction** to preserve semantic signal
- They maintain **lineage tracking** (user_id → dataset_version) for deletion requests
- They enforce **role-based access** with audit logs for every dataset access
- They document **legal basis and retention policies** for GDPR, CCPA, HIPAA, and SOC 2
- They run **quarterly audits** of sanitization quality
- They separate raw, sanitized, and gold datasets with different access and retention rules

---
