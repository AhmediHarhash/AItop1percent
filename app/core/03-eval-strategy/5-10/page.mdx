# Chapter 5.10 — Multilingual & Cross-Cultural Dataset Construction

**What we're doing here:**
Your English eval set is solid — 2,000 cases, balanced difficulty, clean labels. Then someone asks: "Does this work in Spanish?" You grab 200 translated cases, run eval, scores look fine. Ship it.

Two weeks later, your Spanish-speaking users are furious. The model gives technically correct answers that sound robotic, misses cultural context, and handles formal/informal address wrong. Your "translated" eval set didn't catch any of this because translation doesn't equal localization.

Building multilingual eval datasets is not a translation project. It's a cultural adaptation project with translation as one component.

---

## 1) Why translation alone fails

Direct translation preserves meaning but destroys signal. Here's what goes wrong:

**Phrasing patterns don't transfer.** English users ask "Can I get a refund?" — casual, direct. Japanese users wrap the same intent in three layers of politeness. If your Japanese eval cases sound like translated English, you're testing the model on queries no real Japanese user would send.

**Cultural expectations differ.** A German user expects precision and completeness. A Brazilian user expects warmth and flexibility. "Good quality" means different things — your rubric needs to reflect that.

**Difficulty changes across languages.** A query that's "easy" in English might be "hard" in Korean because of honorific complexity. Word order differences make some retrieval tasks harder in SOV languages. Your difficulty labels need per-language calibration.

**Edge cases are language-specific.** Code-switching (mixing languages mid-sentence) is normal in multilingual markets. Dialectal variation matters — Latin American Spanish vs European Spanish. Script variations (simplified vs traditional Chinese) affect retrieval.

---

## 2) The three-layer approach

### Layer 1: Translated core (baseline coverage)
Take your strongest English eval cases — the ones testing core capabilities — and professionally translate them. This gives you baseline coverage across languages. But treat these as your floor, not your ceiling.

**Translation quality matters.** Machine translation introduces artifacts. Professional translators miss domain jargon. Back-translation validation helps — translate to target language, translate back to English, compare meaning preservation. If back-translation diverges significantly, the forward translation probably lost something.

### Layer 2: Locally authored cases (cultural realism)
Have native speakers in each market create eval cases from scratch. These reflect real user patterns — how people in that culture actually phrase requests, what they expect, what frustrates them.

This is where you catch things translation misses: culturally specific scenarios (asking about local holidays, local regulations, local customs), natural phrasing patterns, expected tone and formality, and common cultural misunderstandings the model might make.

### Layer 3: Cross-cultural edge cases (stress testing)
Build cases that specifically test cultural boundaries: code-switching between languages, culturally sensitive topics that require different handling by region, queries where the "correct" answer depends on the user's cultural context, and formality/politeness calibration.

---

## 3) Rubric adaptation

Your English rubric won't work across languages without adaptation.

**Tone expectations vary.** "Helpful" in American English means friendly and casual. "Helpful" in Japanese means respectful and thorough. Your rubric's tone dimension needs per-language anchors. Write explicit examples of what a 5/5 tone score looks like in each language.

**Completeness expectations vary.** Some cultures prefer concise answers. Others expect comprehensive context-setting before the answer. Your "appropriate length" rubric dimension needs cultural calibration.

**Formality is a scoring dimension.** In languages with formal/informal registers (French tu/vous, Japanese keigo, Korean speech levels), getting the register wrong is a quality failure. English rubrics don't have this dimension — you need to add it for languages where it matters.

**What to keep universal:** Factual accuracy, safety, and task completion are culture-independent. A wrong answer is wrong in every language. These dimensions stay the same.

---

## 4) Building the dataset practically

**Start with your top 3-5 languages by user volume.** Don't try to cover 20 languages at once. Build depth before breadth.

**Per-language minimum:** 200 eval cases per language (100 translated core + 100 locally authored). For your primary language, maintain your full set. For secondary languages, 200 is minimum viable — enough for per-slice analysis if you keep difficulty balanced.

**Native speaker involvement is non-negotiable.** At minimum: native speakers review all translated cases, native speakers write the locally authored cases, and native speakers validate rubric adaptations. "Fluent" is not enough — cultural intuition requires native-level understanding.

**Synthetic generation for multilingual coverage.** Use frontier models to generate cases in target languages. But validate harder than you would for English — LLMs are stronger in English, and their non-English output can be fluent but culturally off. Have native speakers review at least 30% of synthetic multilingual cases (vs 20% for English).

---

## 5) Knobs & defaults

**Translation validation:** Back-translation check on 100% of translated cases. Native speaker review on 30% minimum.

**Locally authored ratio:** At least 50% of non-English cases should be locally authored (not translated). Higher for markets where cultural distance from English is large (East Asian, Middle Eastern markets).

**Difficulty recalibration:** Run your difficulty heuristics (Ch 5.3) per language. Don't assume English difficulty labels transfer.

**Rubric adaptation cadence:** When entering a new language market, adapt rubric before building the dataset. Revisit rubric quarterly based on native speaker feedback.

**Cross-cultural edge cases:** 10-15% of each non-English dataset should be cross-cultural stress tests (code-switching, cultural sensitivity, formality edge cases).

---

## 6) Failure modes

**"Scores look the same across all languages."**
Your eval cases are too similar — probably all translations from English. Real users in different languages have different patterns. Fix: increase locally authored ratio, add language-specific edge cases.

**"Native speakers say the model sounds 'off' but scores are high."**
Rubric doesn't capture cultural expectations. Tone, formality, or cultural sensitivity dimensions are missing. Fix: adapt rubric with native speaker input, add culture-specific scoring dimensions.

**"We can't find annotators for all our languages."**
Common for smaller languages. Fix: prioritize languages by user volume. For lower-volume languages, use LLM-as-Judge with native speaker calibration on a sample. Accept that smaller-market eval sets will be less robust — document the limitation.

**"Translation quality varies wildly across languages."**
Some translators are better than others. Fix: back-translation validation catches the worst problems. Pair every translator with a domain-expert native reviewer. Track translation quality per translator and per language.

---

## 7) Enterprise expectations

- They maintain separate eval datasets per language, not just translated copies
- They involve native speakers in dataset creation, rubric adaptation, and ongoing QA
- They track quality metrics per language independently — no averaging across languages
- They have language-specific rubric dimensions (formality, cultural sensitivity) where needed
- They recalibrate difficulty labels per language rather than assuming English labels transfer
- They treat multilingual coverage as a product requirement, not an afterthought

---
