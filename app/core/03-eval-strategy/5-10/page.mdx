# 5.10 â€” Multilingual and Cross-Cultural Dataset Construction

In late 2024, a fintech startup spent six weeks building a Spanish-language evaluation dataset for their customer support agent. They started with their English eval set, which had 1800 cases and was well-balanced across intents, difficulty levels, and channels. They hired a professional translation service to translate the entire set into Spanish. The translation was fluent and grammatically correct. They ran their evaluation, and the model scored 89 percent on Spanish cases versus 92 percent on English cases. The three-point gap seemed reasonable for a secondary language. They shipped Spanish support.

Within two weeks, they received 200 complaints from Spanish-speaking users. The model's responses were technically correct but sounded robotic and unnatural. It used overly formal language when users expected casual conversation. It failed to recognize common colloquialisms and regional expressions. It handled the formal usted form exclusively when many users preferred the informal tu. The evaluation had missed all of this because the translated eval cases themselves were unnatural. They reflected English communication patterns translated into Spanish words, not how Spanish speakers actually communicate. The difficulty labels were wrong as well. A query labeled normal in English might be hard in Spanish because of grammatical complexity or cultural context that does not exist in English. A query labeled hard in English might be normal in Spanish because the source of difficulty did not transfer across languages.

The root cause was treating multilingual evaluation as a translation problem rather than a localization and cultural adaptation problem. Translation preserves literal meaning. It does not preserve communication patterns, cultural expectations, or linguistic difficulty. Building multilingual eval datasets requires understanding how different cultures phrase requests, what quality means in different cultural contexts, and how difficulty manifests differently across languages. This subchapter covers how to build evaluation datasets that work across languages and cultures without falling into the translation trap.

## Why Translation Alone Fails

Direct translation preserves semantic content but destroys evaluative signal. The translated eval cases test whether the model can handle translated input, not whether it can handle real users from that culture. This distinction matters because real users do not send translated English queries. They send queries that reflect their language's structure, their culture's communication norms, and their region's expectations.

Phrasing patterns do not transfer across languages. English users ask "Can I get a refund?" which is casual and direct. Japanese users wrap the same intent in multiple layers of politeness and indirection. They might say something closer to "I was wondering if it might perhaps be possible to discuss the matter of a potential refund." The literal meaning is the same, but the communication style is completely different. If your Japanese eval cases sound like translated English, you are testing the model on queries no real Japanese user would send. The model might handle translated cases well and still fail on real Japanese communication patterns.

Cultural expectations differ in ways that affect quality assessment. A German user expects precision, completeness, and formal structure in responses. A Brazilian user expects warmth, flexibility, and personal connection. What constitutes a "good quality" response is culturally dependent. Your English rubric might score a response as excellent because it is concise and professional. That same response might score poorly in a culture that values elaboration and warmth. If you use the same rubric across all languages without cultural adaptation, you are measuring quality against the wrong standard.

Difficulty changes across languages in non-obvious ways. A query that is easy in English might be hard in Korean because of honorific complexity. Korean has multiple speech levels, and choosing the wrong level is a significant quality failure. English has no equivalent. A query about product features might be straightforward in English but difficult in German because German compound nouns create ambiguity about which product the user means. Word order differences make some retrieval tasks harder in subject-object-verb languages than in subject-verb-object languages. Your difficulty labels need per-language calibration, not direct transfer from English.

Edge cases are language-specific and do not appear in translated datasets. Code-switching, where users mix languages mid-sentence, is common in multilingual markets. A user in Miami might ask a question in English with Spanish words mixed in. A user in Singapore might switch between English, Mandarin, and Malay within a single query. This is normal behavior in those markets, but it will not appear in translated eval cases unless you deliberately create it. Dialectal variation matters as well. Latin American Spanish differs from European Spanish in vocabulary, grammar, and cultural references. Simplified Chinese and traditional Chinese are different scripts with different user bases and expectations. A translated eval set does not capture these distinctions.

## The Three-Layer Approach to Multilingual Datasets

Building a multilingual eval set is not a single activity. It is three distinct activities with different goals, methods, and quality standards. Organizations that treat multilingual evaluation as "translate the English set" fail on all three layers. Organizations that understand the three layers build eval sets that actually measure multilingual quality.

Layer one is translated core cases that establish baseline coverage. Take your strongest English eval cases, the ones testing fundamental capabilities that should work in any language, and professionally translate them. This gives you baseline coverage across languages. It ensures that core functionalities like intent recognition, factual accuracy, and safety boundaries work in the target language. These are the cases where language is mostly a surface feature. The underlying capability being tested is language-independent. Can the model extract the right information from context? Can the model follow policy rules? Can the model detect when a request violates safety boundaries? These capabilities should work regardless of language. Translated core cases test whether they do.

But treat these as your floor, not your ceiling. Translated cases tell you whether the model can handle translated input. They do not tell you whether it can handle real users. They test a language-independent capability through a language-specific interface. They do not test language-dependent capabilities like tone, register, cultural appropriateness, and natural phrasing. You need layers two and three for that.

Professional translation quality matters for this layer. Machine translation introduces artifacts. A model trained on large amounts of machine-translated data might be better at machine translation patterns than human translation patterns, which skews your evaluation. You would measure how well the model handles machine translation, not how well it handles human language. Professional translators miss domain jargon unless you provide glossaries. A translator who does not understand your product will translate billing as accounts payable, subscription as subscription with wrong connotations, cancel as terminate when discontinue is more accurate. Provide a glossary of domain terms with approved translations. This ensures consistency and accuracy.

Back-translation validation helps catch the worst problems. Translate from English to Spanish, then translate the Spanish back to English using a different translator who has not seen the original. Compare the back-translation to the original. If the meaning diverges significantly, the forward translation probably lost something critical. This catches major errors but not subtle cultural mismatches. A back-translation might preserve meaning while using unnatural phrasing in the forward language. Budget back-translation review for 100 percent of translated core cases. The cost is manageable because you are only translating core cases, not your entire eval set. For a 1000-case English eval set, you might translate 100 core cases per language. That is 100 forward translations and 100 back translations per language. Expensive but tractable.

Layer two is locally authored cases that capture cultural realism. Have native speakers in each market create eval cases from scratch. Do not give them English cases to adapt. Give them your taxonomy, your difficulty framework, examples of what you are testing, and instructions to write cases that reflect how real users in their market would interact with your system. These cases reflect real user patterns: how people in that culture actually phrase requests, what they expect from responses, what frustrates them, what delights them. This is where you catch things translation misses.

Culturally specific scenarios appear here. Asking about local holidays that do not exist in other markets. Asking about local regulations that differ from country to country. Asking about local customs that shape how users interact with products. Asking about local products, brands, and services that are culturally specific. These scenarios cannot be translated from English because they do not exist in English context. They must be authored locally by people who understand the market.

Natural phrasing patterns appear here. The level of formality users expect when interacting with a business or service. The degree of indirection in requests. English tends toward directness. Many Asian languages tend toward indirection. The use of colloquialisms and slang that vary by region even within the same language. Mexican Spanish uses different colloquialisms than Spanish Spanish. Brazilian Portuguese uses different slang than European Portuguese. Expected tone and style appear here. Do users expect warm, personal responses or professional, formal ones? Do they expect concise answers or thorough explanations? These expectations vary by culture and are not captured in translated cases.

Common cultural misunderstandings appear here. Scenarios where a literal translation of policy would offend users or violate cultural norms. Scenarios where a response correct in one culture is inappropriate in another. A friendly, informal response that works in the United States might seem unprofessional in Germany. A formal, thorough response that works in Japan might seem cold and bureaucratic in Brazil. Locally authored cases test whether the model adapts to these cultural norms or defaults to a single standard that may be inappropriate.

Locally authored cases should constitute at least 50 percent of your non-English eval sets. For languages with significant cultural distance from English, like Japanese, Arabic, or Mandarin, aim for 70 percent locally authored. Cultural distance correlates with the need for local authoring. Japanese communication patterns differ dramatically from English patterns. Politeness systems are complex. Indirection is expected. Context dependence is higher. You cannot test Japanese quality with translated English cases. You need Japanese-authored cases. For languages closer to English in structure and culture, like German or Dutch, 50 percent local authoring may suffice. But still necessary. Even closely related languages have cultural differences that affect quality expectations.

The key is that locally authored cases provide the ground truth about real user behavior. Translated cases provide coverage volume and test language-independent capabilities. You need both. A dataset with only translated cases tests whether your model can handle translated input. A dataset with only locally authored cases might miss coverage gaps because local authors naturally focus on patterns they see most often. The combination gives you both breadth and realism.

Layer three is cross-cultural edge cases that stress test cultural boundaries. These cases test scenarios where the model must navigate cultural differences explicitly. This is the hardest layer to build and the most commonly skipped. It requires people who understand multiple cultures deeply enough to know where boundaries lie. Most teams do not have this expertise in-house. But skipping this layer means missing the failures that cause the most harm: cultural insensitivity, inappropriate responses to sensitive topics, failure to adapt to user context.

Code-switching between languages appears here. A user asks a question in English and Spanish mixed together. The model must handle both languages within a single query without getting confused. Culturally sensitive topics that require different handling by region appear here. A question about alcohol consumption might be routine in one culture and offensive in another. The model must adapt its response based on cultural context. If it cannot infer context, it must default to neutral responses that work across cultures. Queries where the "correct" answer depends on the user's cultural context appear here. What counts as polite, what counts as complete, what counts as helpful: all culturally dependent. The model must have some way to infer or adapt to user culture. Formality and politeness calibration appears here. A user sends a formal query expecting a formal response. The model must match the user's register. A user sends a casual query. The model must not respond with stiff formality that creates distance.

This layer should be 10 to 15 percent of each non-English eval set. These cases are difficult to create because they require deep cultural knowledge and an understanding of where cultures diverge. But they are essential for Tier 2 and Tier 3 applications where cultural missteps create real harm. A Tier 1 application providing weather information might tolerate occasional cultural awkwardness. Users forgive tone issues when the factual content is correct. A Tier 3 application providing healthcare advice serving diverse markets cannot tolerate cultural missteps. An inappropriate response to a sensitive health topic can cause real harm and destroy user trust permanently.

## Rubric Adaptation for Cultural Contexts

Your English rubric will not work across languages without adaptation. The dimensions you measure and the standards you apply must reflect cultural expectations. This is not optional cultural sensitivity. This is measurement validity. If your rubric measures quality according to English-American cultural norms and applies those norms to all languages, you are not measuring quality. You are measuring cultural conformity to American standards. Users in other cultures will have different quality expectations. If you do not measure against those expectations, your eval scores will not predict user satisfaction.

Tone expectations vary dramatically across cultures. "Helpful" in American English means friendly, casual, and encouraging. A helpful response uses first person, shows enthusiasm, offers proactive suggestions. "Helpful" in Japanese means respectful, thorough, and appropriately humble. A helpful response uses appropriate keigo, provides comprehensive context, avoids presumption. If you apply American tone standards to Japanese responses, you will penalize Japanese-appropriate helpfulness as being too formal or too verbose. If you apply Japanese tone standards to American responses, you will penalize American-appropriate helpfulness as being too casual or too brief. Your rubric's tone dimension needs per-language anchors that define what excellent tone means for that culture.

Write explicit examples of what a five-out-of-five tone score looks like in each language. Show what excellent tone sounds like for that culture. Do not assume that translating an English example produces a culturally appropriate example in the target language. Have native speakers write examples from scratch. These examples become your scoring anchors. When annotators score tone, they compare the model's response to these culturally appropriate examples, not to translated American examples. This ensures you are measuring tone appropriateness for the user's culture, not tone conformity to American norms.

Completeness expectations vary across cultures. Some cultures prefer concise answers that get to the point quickly. American business culture values efficiency. A complete answer provides the necessary information without elaboration. British culture values even more brevity. A complete answer is as short as possible while remaining polite. German and Japanese cultures value thoroughness. A complete answer provides context, explains reasoning, addresses potential follow-up questions. A response that scores five out of five for completeness in American English might score three out of five in German for being insufficiently thorough. A response that scores five out of five in German might score three out of five in American English for being unnecessarily verbose. Your "appropriate length" rubric dimension needs cultural calibration. Define what complete means for each culture. Provide concrete examples of responses that are too short, appropriately complete, and excessively verbose for that culture. Train annotators on these examples. Make it clear that completeness is culturally relative, not absolute.

Formality is a scoring dimension in many languages but not in English. English has lost most formal registers. You can signal formality through word choice and sentence structure, but there is no grammatical formality system. In languages with formal and informal registers, formality is grammatically encoded. Getting the register wrong is not a stylistic preference. It is a linguistic error that can offend users or create social distance. French has tu and vous for second person. Using tu when vous is expected is rude. Using vous when tu is expected is cold. Japanese has multiple levels of keigo: sonkeigo for respectful language toward superiors, kenjougo for humble language about oneself, teineigo for polite language in general. Choosing the wrong level signals the wrong social relationship. Korean has seven speech levels ranging from highly formal to highly informal. Spanish has tu and usted. German has du and Sie. In these languages, your rubric must include formality as an explicit dimension. The model must match the user's register or default to an appropriate level based on context. A response that uses informal language when formal is expected scores poorly even if the factual content is correct. A response that uses overly formal language when informal is expected scores poorly for creating unnecessary distance. English rubrics do not have this dimension because English lacks formal registers. You must add it for languages where it matters. Define the formality dimension clearly: what register is expected, what register the model used, whether they match, whether the mismatch is appropriate or inappropriate given context.

Cultural appropriateness is a new dimension for most rubrics. It captures whether the response respects cultural norms beyond language. Does the response assume cultural context that may not apply to the user? Does it reference holidays, customs, or norms specific to one culture and inappropriate for others? Does it handle culturally sensitive topics with appropriate care? This dimension is hardest to define because it requires understanding cultural boundaries that vary by region, religion, and individual background. Start narrow. Define cultural appropriateness for the specific cultural issues most likely to arise in your domain. For a food delivery app, cultural appropriateness includes respecting dietary restrictions and preferences that vary by culture and religion. For a healthcare app, cultural appropriateness includes respecting cultural norms around discussing body, illness, and family. For a financial app, cultural appropriateness includes respecting cultural norms around debt, interest, and family financial obligations. Do not try to define universal cultural appropriateness. Define domain-specific cultural appropriateness for the markets you serve.

What to keep universal: factual accuracy, safety, and task completion are culture-independent. A wrong answer is wrong in every language. If the model states an incorrect fact, that is a failure regardless of culture. Factual errors do not become acceptable in different cultural contexts. A safety violation is a violation regardless of culture. If the model generates harmful content, that is a failure in every language. Your safety policy should be consistent across languages, though implementation may require cultural adaptation in how you communicate policy boundaries. A failure to complete the requested task is a failure everywhere. If the user asks for account information and the model provides pricing information instead, that is task failure in every culture. These dimensions stay the same across languages. You measure them the same way. The standards do not change. What changes is tone, completeness, formality, and cultural appropriateness. These are the dimensions that require cultural adaptation because they depend on user expectations that vary by culture.

## Building the Dataset Practically

Start with your top three to five languages by user volume. Do not try to cover 20 languages at once. Build depth before breadth. Pick the languages that matter most for your user base. If 80 percent of your non-English users speak Spanish, prioritize Spanish. If your users are distributed across ten languages with no clear leader, prioritize the languages where quality failures create the highest risk. A healthcare application serving diverse markets might prioritize languages based on patient population size and regulatory requirements. An e-commerce application might prioritize languages based on revenue. For Tier 3 applications, regulatory requirements may dictate which languages you must support regardless of user volume. The EU AI Act requires certain applications to support all official EU languages for high-risk systems. Know your regulatory obligations before setting priorities.

Per-language minimum is 200 eval cases. This breaks down as 100 translated core cases and 100 locally authored cases. For your primary language, likely English, maintain your full eval set of 1000 to 2000 cases or more. For secondary languages, 200 is minimum viable. It is enough for per-slice analysis if you keep difficulty balanced. You can detect major quality differences between languages. You can identify which intents or difficulty levels cause problems. You cannot do fine-grained coverage mapping with only 200 cases, but you can run meaningful evaluation. If a language represents more than 20 percent of your user base, increase the target to 500 cases. If a language is mission-critical for regulatory or business reasons, match your English eval set size.

The build sequence matters. Start by translating your 100 strongest English cases. These should be cases that test core capabilities independent of cultural context: factual question answering, basic intent recognition, safety boundaries, task completion. Avoid cases heavy in cultural nuance or idiom for this initial translation. Run back-translation validation on all 100. Fix issues caught by back-translation. Then send to native reviewers. They will catch 30 to 50 percent that need revision even after back-translation passed. Make revisions. Now you have 100 validated translated core cases.

Next, engage native speakers to author 100 cases from scratch. Give them your task taxonomy, your difficulty framework, and examples of your English eval cases as reference. But do not ask them to translate or adapt English cases. Ask them to write cases that reflect how real users in their culture would interact with your system. Provide clear guidance on what you are testing: intents, edge cases, difficulty levels, channels. Review the first ten cases together to ensure they understand the goals. Iterate on guidance based on what they produce. This catches misunderstandings early before they author 100 cases in the wrong direction.

As native speakers author cases, have them propose expected outputs and rubric scores. Do not impose English-authored expected outputs. They know what a good response looks like in their culture. You do not. Review their expected outputs for consistency with your product policy and safety standards. Policy must be consistent across languages. Tone and style can vary. After they author 100 cases, validate a sample with a second native reviewer. This catches cases where the first author's judgment was idiosyncratic. Aim for 20 percent second-reviewer validation. Resolve disagreements through discussion, not by defaulting to English standards.

Finally, create 20 to 30 cross-cultural edge cases. These require collaboration between native speakers and your evaluation team. The native speaker knows what cultural boundaries exist. Your evaluation team knows what failure modes to test. Work together to design cases that probe those boundaries. This is the hardest and most valuable part of multilingual dataset construction. It cannot be outsourced or automated. It requires deep expertise on both sides.

Native speaker involvement is non-negotiable at three points in the process. First, native speakers must review all translated cases. They catch cultural mismatches, awkward phrasing, and register problems that professional translators miss. Plan for 30 to 50 percent of translated cases to need revision after native review. This is normal. Professional translation optimizes for meaning preservation, not for natural user language. Native reviewers optimize for realism. Second, native speakers must write the locally authored cases. Fluent is not enough. You need native-level cultural intuition to write cases that reflect real user patterns. A non-native speaker who learned the language academically will produce cases that sound like textbook examples, not like real users. Third, native speakers must validate rubric adaptations. They tell you whether your tone anchors make sense, whether your completeness standards match cultural expectations, and whether you have missed important dimensions like formality.

Finding qualified native speakers is harder than it sounds. You need people who are native speakers, understand your domain, understand evaluation methodology, and have time to contribute. This is a small pool. For a healthcare application, you need native speakers with clinical knowledge. For a legal application, you need native speakers with legal expertise. For a financial application, you need native speakers who understand financial concepts and regulations in their market. These are not generic translators. They are domain experts who happen to be native speakers of your target language. Many organizations solve this by hiring in-market contractors or partnering with localization agencies that provide domain experts. Budget for this. Native speaker time is expensive, especially for domain expertise. A native Spanish speaker who understands financial services might cost 100 to 200 dollars per hour. For 100 locally authored cases at 15 minutes per case, that is 25 hours and 2500 to 5000 dollars. Plus review time. Plus rubric adaptation time. Budget 5000 to 10000 dollars per language for a high-quality 200-case eval set. This is not optional expense. It is the minimum cost of doing multilingual evaluation correctly.

If you cannot afford native experts for all languages, prioritize. Use native experts for your top three languages. For lower-priority languages, use fluent speakers with native expert spot-checking. Accept that lower-priority language eval sets will be less robust. Document this clearly. Do not claim the same evaluation rigor across all languages when you have not invested equally.

Synthetic generation for multilingual coverage works but requires higher validation rates. Use frontier models to generate cases in target languages. GPT-5, Claude Opus 4.5, and Gemini 2 are reasonably strong in major languages like Spanish, French, German, Mandarin, and Japanese. They can generate grammatically correct, semantically appropriate cases. But validate harder than you would for English. LLMs are strongest in English. Their non-English output can be fluent but culturally off. They might use unnatural phrasing, choose the wrong register, or miss cultural context. Have native speakers review at least 30 percent of synthetic multilingual cases versus 20 percent for English. For languages where the model is known to be weaker, like Arabic, Korean, or smaller European languages, review 50 percent or more. For low-resource languages where frontier models have limited training data, synthetic generation is not recommended. The outputs will be too unreliable to be useful even with validation.

Track the ratio of translated, locally authored, and synthetic cases per language. For each language, aim for 30 percent translated, 50 percent locally authored, 20 percent synthetic. This balances coverage, realism, and efficiency. If any category exceeds 60 percent, you have a balance problem. Too much translation means weak cultural adaptation. Too much local authoring with no translation means you are not testing cross-language consistency on core capabilities. Too much synthetic means you are not grounding evaluation in real user patterns.

## Difficulty Recalibration Per Language

Do not assume English difficulty labels transfer to other languages. Run your difficulty heuristics from Chapter 5.3 separately for each language. A case labeled normal in English might be hard in Korean because of grammatical complexity. Korean has multiple speech levels and honorific systems. Choosing the right register requires understanding social context that is not explicit in the query. This makes Korean cases inherently more complex than equivalent English cases. A case labeled hard in English might be normal in Spanish because the source of difficulty, maybe ambiguous pronouns, resolves cleanly in Spanish grammar. Spanish grammar disambiguates subjects and objects in ways English does not. What is ambiguous in English may be clear in Spanish. Difficulty is not language-invariant. It depends on linguistic features, cultural context, and how the specific language handles the request.

Recalibrate difficulty using both heuristics and empirical model performance. Heuristics give you a first pass. Run token count, syntactic complexity, and ambiguity measures per language. Compare to English. If Korean cases labeled normal have double the token count of English normal cases, your labels may be wrong. Or Korean may genuinely require more tokens to express the same intent due to linguistic structure. Investigate by looking at native production data. If real Korean users also use double the tokens for similar requests, the label might be correct. If real users are more concise, your eval cases are unrealistic. Empirical performance gives you ground truth. Run your model on both English and target language cases labeled the same difficulty. If the model scores 90 percent on English normal cases and 70 percent on Spanish normal cases, either the Spanish cases are mislabeled as normal or the model is weaker in Spanish. Investigate which. Have native speakers review the Spanish failures. If failures are due to mislabeled difficulty, adjust labels. If failures are due to model weakness, document the gap and decide whether to improve the model or adjust expectations.

For languages you are just adding, start with conservative difficulty labels. Label cases as hard if you are unsure. It is easier to downgrade a case from hard to normal after seeing model performance than to discover a normal case is actually hard and have skewed metrics. Recalibrate quarterly for the first year as you gather more data. After that, annual recalibration suffices unless you change models or the language significantly. Track difficulty distribution per language. If your English eval set is 50 percent normal, 30 percent hard, 20 percent easy, and your Spanish eval set is 20 percent normal, 70 percent hard, 10 percent easy, something is wrong. Either your Spanish cases are systematically mislabeled, or you are testing different capabilities in Spanish than English. Investigate and rebalance.

Some languages have difficulty factors that do not exist in English. Grammatical gender affects difficulty in languages like French, Spanish, and German. The model must track gender agreement across long contexts. Tone systems affect difficulty in Mandarin and Vietnamese. The model must distinguish words that differ only in tone. Script complexity affects difficulty in languages like Arabic and Thai where diacritics change meaning. The model must handle text that may be written with or without diacritics. These language-specific factors mean that your difficulty framework from Chapter 5.3 may need additional dimensions for certain languages. Add these dimensions when they materially affect evaluation. Do not add them as academic exercises. Only add dimensions that predict model performance and user satisfaction.

## Cross-Cultural Edge Cases

Build cases that specifically test cultural boundaries. These are hard to create but essential for international products. They require collaboration between native speakers who understand cultural nuance and evaluation experts who understand what to test. Most organizations under-invest in this layer because it is difficult and time-consuming. This is a mistake. Cultural edge cases catch failures that destroy user trust in ways normal cases never reveal.

Code-switching is common in multilingual markets and completely absent from translated eval sets. A user in Singapore might switch between English, Mandarin, and Malay within a single query. This is not a user error. It is normal communication in multilingual contexts. A user in California might mix English and Spanish. A user in India might mix English and Hindi. Your model must handle this gracefully. It should recognize both languages, understand the intent, and respond appropriately. Test this explicitly. Create eval cases where users mix languages mid-sentence. Score based on whether the model handles both languages correctly and responds in the appropriate language or asks for clarification when ambiguous. The hardest cases are where code-switching changes meaning. A word that exists in both languages but means different things. The model must use context to determine which language the user intended.

Culturally sensitive topics require different handling by region. A question about alcohol consumption might be routine in France and sensitive in Saudi Arabia. A question about pork might be routine in Germany and offensive in Israel. A question about religious holidays might be straightforward in the United States and complex in Israel where multiple religious calendars coexist. Your model must adapt its response based on the user's cultural context. If you can infer the user's region or culture from metadata like IP address, account settings, or language choice, test that the model adjusts its responses appropriately. If you cannot infer culture, test that the model defaults to culturally neutral responses that do not assume the user's background. This is harder than it sounds. Many topics that seem neutral in one culture carry strong cultural associations in others.

Queries where the correct answer depends on cultural context are the hardest to evaluate. What counts as polite varies. In American culture, directness is polite. In Japanese culture, indirectness is polite. What counts as complete varies. In German culture, thoroughness is complete. In British culture, brevity is complete. What counts as helpful varies. In American culture, proactive suggestions are helpful. In some Asian cultures, unsolicited advice is presumptuous. Your eval cases must test whether the model adapts to cultural context rather than defaulting to a single standard. This requires metadata tagging: this case is from a German user expecting formal, thorough responses. This case is from a Brazilian user expecting warm, flexible responses. Score based on whether the model matches expectations for that context. This means your expected outputs are context-dependent. The same query from different cultural contexts has different correct responses.

Formality and politeness calibration cases test whether the model matches user register. A user sends a formal query using formal grammar and vocabulary. The model must respond formally. A user sends a casual query with slang. The model must respond casually without being overly stiff. This is language-specific. In English, the distinction is subtle. You can get it wrong and most users will not notice. In Japanese or Korean, the distinction is explicit and critical. Getting it wrong is a major quality failure that offends users. Test both directions: formal query requiring formal response, casual query requiring casual response. Also test ambiguous cases where the model must infer appropriate formality from context. A business query sent in casual language: should the model mirror the casual tone or default to business formality? There is no universal answer. It depends on company culture and user expectations. Your eval cases should test that the model makes reasonable choices and errs on the side of formality when unsure.

Regional variation within languages creates additional edge cases. Spanish spoken in Mexico differs from Spanish spoken in Spain in vocabulary, grammar, and cultural references. Portuguese in Brazil differs from Portuguese in Portugal. English in the United States differs from English in the United Kingdom, Australia, and India. These are not just accent differences. They are meaningful linguistic and cultural differences. If your system serves multiple regions for the same language, test regional adaptation. Create eval cases with region-specific vocabulary and cultural references. Score based on whether the model recognizes the region and responds appropriately. Do not penalize the model for using a different regional variant than the user if both are comprehensible. But do penalize the model for using region-specific references that are wrong for the user's region. If a user in Mexico asks about phone plans and the model references Spain-specific carriers, that is a failure even though both are Spanish.

Measurement units and formatting conventions vary by region and create subtle edge cases. Date formats differ: MM/DD/YYYY in the United States, DD/MM/YYYY in most of Europe, YYYY-MM-DD in East Asia. Temperature units differ: Fahrenheit in the United States, Celsius everywhere else. Currency formats differ: dollar sign before the number in English, after the number in French. Decimal separators differ: period in English, comma in European languages. These seem like minor formatting issues, but they cause real user confusion when wrong. Test that your model uses the conventions appropriate for the user's region or language. If you cannot infer region, test that the model states units explicitly to avoid ambiguity.

## Knobs and Defaults

Translation validation starts with back-translation check on 100 percent of translated cases. Use a different translator for back-translation than forward translation. This catches errors the original translator might not see. Then native speaker review on 30 percent minimum, focusing on cases where back-translation diverged significantly or where cultural context matters most. For domain-specific content like medical, legal, or financial, review 50 percent. Domain errors are more costly than general errors.

Locally authored ratio should be at least 50 percent of non-English cases. This is a floor, not a target. Higher for markets where cultural distance from English is large. Aim for 70 percent locally authored for East Asian languages, Arabic, and other high-distance languages. For languages where cultural norms around communication differ dramatically from English, local authoring becomes the primary source of cases, with translation serving only to cover baseline capabilities. Track this ratio per language. If you find yourself translating 80 percent of cases for all languages, you are not adapting to cultural differences. You are measuring translated English.

Difficulty recalibration must happen per language using both heuristics and empirical performance. Run difficulty heuristics from Chapter 5.3 separately for each language. Token count, syntactic complexity, ambiguity measures: all language-dependent. Compare empirical model performance across languages at the same labeled difficulty. If normal-difficulty English cases have 90 percent pass rate and normal-difficulty Korean cases have 70 percent pass rate, investigate. Either Korean cases are mislabeled, or the model is weaker in Korean, or Korean genuinely has higher linguistic complexity for the same conceptual difficulty. Have native speakers review and determine which. Adjust labels based on actual performance, not assumptions. Rebalance difficulty distribution per language quarterly for the first year, then annually.

Rubric adaptation happens once when entering a new language market, then revisited quarterly based on native speaker feedback and production performance. Work with native speakers to define what each rubric dimension means in their culture. Helpful, professional, complete, concise: these are not universal concepts. They have culturally specific meanings. Write concrete examples for each score level in each language. Do not translate English examples. Have native speakers write examples that reflect their cultural norms. After initial adaptation, revisit quarterly. Production user feedback will reveal whether your rubric is capturing what matters. If native users complain about tone but tone scores are high, your rubric is wrong. Adapt it.

Cross-cultural edge cases should be 10 to 15 percent of each non-English dataset. Start with ten cases per language if resources are limited. Focus on the highest-risk cultural boundaries for your domain. Expand over time as you learn what matters. Native speaker validation rate is 30 percent for translated cases, 20 percent for locally authored cases if the author is a trusted expert, 50 percent if the author is new or you are entering a new domain. For synthetic cases, validate 30 percent or higher. Synthetic multilingual cases are less reliable than synthetic English cases. They require higher validation.

Budget three to six months to build a high-quality 200-case eval set for a new language, including translation, local authoring, validation, and rubric adaptation. This assumes you have native speakers available and your domain is moderately complex. For highly specialized domains like medical or legal, budget six to twelve months. For simpler domains like e-commerce, three to four months may suffice. Do not rush. A poorly built multilingual eval set is worse than no eval set. It gives false confidence. Budget 5000 to 10000 dollars per language for a 200-case set with proper native involvement. Scale up for larger eval sets or more complex domains. This is not optional cost. It is the price of international quality assurance.

## Failure Modes and Fixes

Scores look the same across all languages. This suggests your eval cases are too similar, probably all translations from English. Real users in different languages have different patterns, different difficulty distributions, and different quality expectations. If your model scores 88 percent across English, Spanish, French, and German with less than two points of variation, either your model is miraculously consistent or your eval sets are not testing language-specific capabilities. Fix by increasing locally authored ratio and adding language-specific edge cases. Run embedding-based clustering across languages. Embed all eval cases regardless of language. If all languages cluster together, your cases are not capturing real linguistic diversity. They are testing translated English, not actual language use. Regenerate with more locally authored and cross-cultural edge cases. Aim for at least 15 percent score variation across languages. Some variation comes from model strength differences. Some comes from genuine linguistic difficulty differences. Both are real signals.

Native speakers say the model sounds off but scores are high. Your rubric does not capture cultural expectations. The model is producing outputs that are technically correct but culturally inappropriate. Tone is wrong. Formality is wrong. Completeness does not match expectations. Phrasing sounds unnatural. Fix by adapting rubric with native speaker input. Add culture-specific scoring dimensions. Provide concrete examples of what good looks like in that culture. Do not assume English anchors translate. If your English rubric says helpful tone means friendly and encouraging, that may not be what helpful means in Japanese or German. Work with native speakers to define helpful, professional, complete, and concise for each culture. Update rubric scoring to reflect these definitions. Rerun evaluation with the updated rubric. Scores will likely drop, which is correct. The model was not performing as well as scores suggested. Now you have accurate measurement.

You cannot find annotators for all your languages. This is common for smaller languages or specialized domains. You need native speakers with domain expertise. For low-volume languages, the pool is tiny. For specialized domains like healthcare or law, the pool is tiny even in major languages. Fix by prioritizing languages by user volume and risk. Focus native expert resources on languages where quality failures create the highest impact. For lower-volume languages, use LLM-as-Judge with native speaker calibration on a sample. Have a native speaker review and correct 50 LLM-judged cases. Use their corrections to calibrate LLM-Judge prompts. Rerun on the full set. Spot-check another 20 cases to verify calibration held. Accept that smaller-market eval sets will be less robust. Document the limitation clearly. Do not pretend you have the same confidence in eval quality across all languages. Be transparent about where coverage is weak and where you are using LLM-Judge instead of human judgment.

Translation quality varies wildly across languages. Some translators are better than others. Some languages are harder to translate than others. Some domains have specialized terminology that generic translators miss. You receive translated cases that are grammatically correct but semantically wrong or culturally inappropriate. Fix by using back-translation validation to catch the worst problems. Pair every translator with a domain-expert native reviewer. Track translation quality per translator and per language. If a translator consistently produces cases that fail native review, replace them. For difficult languages, invest more in native authoring and less in translation. If translated cases keep failing validation at rates above 40 percent, stop translating and author locally instead. Translation is only cost-effective if it produces acceptable quality at lower cost than authoring. If you spend more time fixing bad translations than you would spend authoring from scratch, translation is not saving you money.

The model performs much worse in some languages than others. Spanish scores 85 percent, English scores 92 percent, German scores 78 percent. This could mean the model is weaker in those languages, or your eval cases are mislabeled or unrealistic, or both. Investigate by comparing model outputs to expected outputs. Are the model's responses wrong, or are the expected outputs wrong? Have native speakers review a sample of failures. If many failures are actually acceptable responses that do not match expected outputs, your expected outputs are bad. Fix by having native speakers rewrite expected outputs to reflect what a good response actually looks like. If most failures are genuinely wrong, the model is weak in that language. Document the gap. Decide whether to improve the model, adjust user expectations, or limit language support. For weak languages, you might need more training data, not just better evaluation. But you need accurate evaluation to know how weak the model actually is. Do not dismiss low scores as eval problems without investigation. Also do not assume low scores are model problems without investigation. Both happen.

Locally authored cases do not match your intent taxonomy. Native authors write cases that reflect real user patterns, which is good. But those patterns do not map cleanly to your taxonomy, which was designed based on English usage. Your taxonomy has a returns intent and an exchanges intent as separate categories. In the Spanish market, users do not distinguish these. They use one word for both concepts. Your German eval cases have four different intents that all map to the same English intent because German has finer-grained distinctions. Fix by accepting that taxonomies are culturally dependent. Either maintain language-specific taxonomies or use a coarser-grained taxonomy that works across languages. Do not force native authors to write unrealistic cases that fit an English-centric taxonomy. Better to adjust the taxonomy than to compromise realism. Track cross-language intent mapping. Document where languages differ in how they carve up intent space. This informs product design as well as evaluation.

Cross-cultural edge cases are too hard to create and you skip them. Your eval sets cover baseline capabilities and locally authored realism but miss cultural boundaries. This is common. Cross-cultural cases are difficult to design and require deep cultural knowledge that most teams lack. Fix by starting small. Create ten cross-cultural edge cases per language, not 30. Focus on the highest-risk cultural boundaries for your domain. If your product deals with financial transactions, test culturally sensitive topics around debt, interest, and family obligations. If your product deals with health, test culturally sensitive topics around body, diet, and end-of-life. You cannot test every cultural boundary. Prioritize the ones where mistakes cause harm or destroy trust. Partner with cultural consultants if your team lacks in-house expertise. This is specialized knowledge. Do not guess.

## Enterprise Expectations

Organizations doing this well maintain separate eval datasets per language, not just translated copies. They involve native speakers in dataset creation, rubric adaptation, and ongoing quality assurance. They track quality metrics per language independently. They do not average across languages. A model might be excellent in English and poor in Spanish. Averaging hides this. They have language-specific rubric dimensions like formality and cultural sensitivity where needed. They recalibrate difficulty labels per language rather than assuming English labels transfer. They treat multilingual coverage as a product requirement, not an afterthought. Multilingual support is a first-class feature, and multilingual evaluation receives the same rigor as English evaluation. They budget appropriately for native speaker involvement, treating it as essential rather than optional. They version multilingual datasets separately, tracking when each language's eval set was last updated and what changes were made. They run cross-language consistency checks to ensure policy and safety standards are uniform while allowing cultural adaptation on tone and style.

The next subchapter addresses dataset evolution and maintenance, ensuring your eval sets remain current as products, users, models, and standards change over time.
