# 3.6 â€” Practical Coverage Math (No Algebra, Just Decision Rules)

In mid-2025, a healthcare technology company ran their eval suite before a major release and saw a four-point improvement in overall quality scores. The product team celebrated. Engineering scheduled the release for the following Monday. Then on Tuesday morning, production logs showed escalation rates up 31% and patient satisfaction scores down fourteen points for appointment booking workflows. The improvement they'd measured was noise. The degradation they'd missed was real.

The root cause wasn't a bad evaluation methodology. It was a small evaluation dataset. Their eval suite contained 127 test cases total, spread across eighteen different task types. That meant most tasks were tested with five to ten examples each. A single prompt change that helped two examples and hurt three others looked like net improvement. But those five examples didn't represent the distribution of real user requests. In production, the change helped 400 users and hurt 1,200. The small dataset lied because it was too sensitive to random variation, too vulnerable to sampling accidents, and too sparse to detect meaningful patterns.

This is the coverage math problem. You need enough test cases to make changes meaningful, but you don't need a statistics degree to figure out how many "enough" is. This chapter gives you simple sizing rules and confidence habits used in real enterprise eval programs, rules that work without turning evaluation into algebra.

## Why Small Eval Sets Lie

When your evaluation dataset is too small, you'll observe a pattern that feels like progress but is actually noise. One release shows an eight-point improvement. The next release shows a ten-point drop. The release after that shows a six-point recovery. Your team spends hours investigating these swings, trying to understand what changed, only to discover that nothing meaningful changed at all. The movement is random variation, not signal.

Small datasets are too sensitive to individual example difficulty. If your eval set for a task contains only five examples and one of them is unusually hard, that single hard example represents 20% of your measured quality. A prompt change that happens to help that one hard case looks like a 20% improvement even if it doesn't help any other scenario. In production, where that hard case represents 2% of traffic, the same change produces a 2% improvement, which is lost in noise. Your eval overstated the impact by ten times.

Small datasets are too sensitive to model randomness. Language models are stochastic. Even with temperature set to zero, minor differences in tokenization, floating point rounding, or internal sampling can cause output variation. If you run the same prompt through GPT-5 three times with temperature zero, you'll occasionally get three different outputs. When your dataset is small, this randomness dominates your metrics. A ten-example eval set might score 80% on Monday and 70% on Tuesday with the exact same system, purely from sampling variation.

Small datasets are too sensitive to sampling accidents. If you hand-pick ten examples to represent a task, you might accidentally choose ten that are all easy, or ten that all involve a specific phrasing pattern, or ten that all assume a particular context. You didn't intend to bias the dataset, but human example selection is inconsistent. In production, the real distribution is far broader. Your eval set measures performance on the narrow slice you happened to sample, not on the task as users actually experience it.

The enterprise mindset is that you don't chase tiny fluctuations. You build enough coverage so that changes are meaningful, so that an improvement in your eval predicts an improvement in production, and so that stability in your metrics actually means stability in user experience.

## The Coverage Budget Mental Model

Think of eval cases as a budget you allocate across different dimensions. You don't need massive datasets everywhere. You need to invest heavily where risk is high and impact is large, and invest lightly where risk is low and failures are tolerable.

Your coverage budget gets spent across **tasks** in your taxonomy. High-volume tasks that users encounter frequently deserve more coverage because small quality changes affect many users. High-risk tasks that involve payments, account changes, medical advice, or legal guidance deserve more coverage because failures have serious consequences. Low-volume, low-risk tasks can survive with minimal coverage because even if your measurements are noisy, the business impact of getting it wrong is small.

Your budget gets spent across **risk tiers**. Tier 3 tasks, the ones where failure causes compliance violations, financial loss, safety incidents, or significant user harm, need deep coverage with hundreds of test cases and adversarial scenarios. Tier 2 tasks, where failure causes user frustration, inefficiency, or escalation but not catastrophic harm, need moderate coverage with dozens to a hundred test cases. Tier 0 and Tier 1 tasks, where failure is minor or easily recovered, can work with light coverage, sometimes as few as ten to thirty examples.

Your budget gets spent across **difficulty levels**. If all your test cases are easy, your eval will miss hard failures that show up in production. If all your test cases are hard, your scores will be artificially depressed and you'll have trouble distinguishing real improvements from noise. A balanced difficulty mix ensures your metrics reflect realistic performance.

Your budget gets spent across **slices**. Different languages, different customer segments, different interaction channels, and different time periods all represent different distributions of real usage. If you only test in English, you'll miss failure modes that appear in Arabic or Mandarin. If you only test with synthetic examples, you'll miss failure modes that emerge from real user phrasing.

Your budget gets spent on **failure-mode packs**. Safety cases, PII exposure scenarios, tool failure cases for agents, must-abstain cases for RAG, critical field confirmation cases for voice systems all represent specific failure modes you must test explicitly because they're too important to leave to random sampling.

The principle is simple: spend big where it matters, spend small where it doesn't. That's how you get comprehensive coverage without building impossibly large datasets.

## Default Sizing Rules for Leaf Tasks

Enterprise teams use default sizing rules that translate risk tiers and traffic patterns into concrete test case counts. These rules aren't mathematically optimal. They're practical defaults that work across a wide range of products.

For **Tier 0 and Tier 1 tasks**, start with ten to thirty cases per leaf task. These tasks have low consequences of failure. Users can recover easily, business impact is minimal, and the primary goal is verifying that the system behaves normally. Ten to thirty examples is enough to catch obvious regressions, confirm basic functionality, and establish a baseline. If the task is extremely low volume, even ten cases might be more than you need. If the task is moderate volume, thirty cases gives you reasonable confidence.

For **Tier 2 tasks**, use thirty to eighty cases per leaf task. These tasks matter to users. Failure causes frustration, wasted time, escalations, and potential churn. You need enough coverage to detect meaningful quality changes and enough variety to represent realistic usage. Thirty cases is the minimum for stability. Eighty cases gives you good confidence that measured changes reflect real performance shifts.

For **Tier 3 tasks**, use eighty to 200 or more cases per leaf task, plus dedicated safety packs. These tasks are critical. Failure can cause financial loss, compliance violations, safety incidents, or significant user harm. You need deep coverage with many examples, adversarial cases that stress-test boundaries, edge cases that probe failure modes, and multi-turn scenarios that test behavior over complex interactions. Eighty cases is the baseline. For the highest-risk tasks, 200 to 500 cases is common, especially when you factor in safety-specific test suites.

These ranges work because they balance measurement stability with practical dataset construction costs. Going from ten cases to thirty cases dramatically improves signal quality. Going from thirty to eighty continues to improve confidence. Going from eighty to 200 refines your ability to detect subtle regressions in critical tasks. Going beyond 200 often yields diminishing returns unless the task is extremely high-stakes or highly complex.

## Sizing Your Top Intents

Your top intents by traffic volume deserve special attention because they represent the majority of user interactions. A quality regression in a top-ten intent affects thousands or millions of users. A quality improvement in a long-tail intent affects dozens.

For each of your **top ten intents**, aim for fifty to 200 cases each, depending on the combination of traffic volume and risk tier. A high-volume, high-risk intent like password reset in a banking app might warrant 150 to 200 cases. A high-volume, low-risk intent like checking account balance might need fifty to eighty cases. The goal is preventing the failure mode where you break the main thing and your eval doesn't catch it because you only had fifteen examples covering your most common user need.

These top intent datasets should include difficulty variation, realistic user phrasing from production logs, edge cases that appear in complaint data, and regression cases from past incidents. They should be refreshed periodically to match current usage patterns. They form the core of your regression suite, the set of tests that must pass before any release ships.

For the **long tail** of less common intents, create a long-tail bucket of 200 to 1,000 cases total, depending on product size and complexity. This bucket isn't evenly distributed across all long-tail tasks. Instead, focus on rare but costly failures, tasks that are infrequent but high-consequence when they occur. Include multi-turn edge cases that stress-test conversation management. Include tricky language cases that probe multilingual handling or ambiguous phrasing. Include scenarios from the weird corners of your product where behavior is less well-defined.

The long-tail bucket serves as a safety net. It catches regressions in areas you don't actively optimize for. It surfaces unexpected failure modes. It gives you some coverage in parts of the product that don't justify dedicated per-task investment.

## The Difficulty Mix Rule

A dangerous anti-pattern is building eval datasets that are accidentally too easy. Teams write examples by hand, and hand-written examples tend to be clear, well-formed, and unambiguous. In production, users write messy queries with typos, unclear phrasing, missing context, and ambiguous intent. If your eval set is all easy cases, your scores will be artificially high and you won't detect failures that real users encounter.

A safe default difficulty mix is 20% easy, 60% normal, and 20% hard. **Easy** cases are well-formed, unambiguous, clearly within the system's capabilities, and representative of the most straightforward user requests. **Normal** cases reflect typical production queries: mostly clear but sometimes with minor ambiguity, occasionally missing a piece of context, sometimes using informal phrasing. **Hard** cases are edge cases, ambiguous requests, adversarial phrasings, multi-step reasoning tasks, or scenarios near the boundary of the system's capabilities.

For Tier 2 and Tier 3 tasks, shift the distribution toward more hard cases: 15% easy, 55% normal, 30% hard. High-risk tasks need more stress-testing. You want to know how the system behaves when pushed to its limits, when faced with confusing input, when users phrase requests in unexpected ways.

What usually goes wrong is that teams accidentally build easy-only eval sets. They test the happy path, they write clean examples, they avoid the messy reality of production. Then they ship a system that scores 94% in eval and performs at 73% in production because the real world is harder than the test set.

## The Minimum Meaningful Change Rule

A common mistake is treating every small score change as meaningful. Your eval runs before a release and scores 87.3%. You run it after the release and it scores 88.1%. Did quality improve? Maybe. Or maybe you measured noise.

Instead of obsessing over tiny changes, enterprise teams set thresholds for what constitutes a meaningful change. The rule is to treat a change as meaningful only if it appears in multiple slices, shows up in enough cases that it's not a two-example fluke, and repeats in the next eval run so it's not a one-off random variation.

For **Tier 0 and Tier 1 tasks**, ignore tiny metric changes. Only act on clear regressions that users would actually feel. A two-point drop in a Tier 0 task might not justify any response. A ten-point drop that shows up in user complaints does.

For **Tier 2 tasks**, act if you see a consistent drop in eval scores combined with a business signal like increased escalation rates, higher retry rates, or lower satisfaction scores. The combination of eval evidence and production evidence gives you confidence the change is real.

For **Tier 3 tasks**, act even on small shifts if they touch safety, PII, or critical actions. A three-point drop in a safety eval might represent a 3% increase in policy violations, which is unacceptable even if the absolute number is small.

This approach prevents the trap of chasing noise. You respond to changes that matter, and you ignore fluctuations that don't.

## The Confidence Through Repetition Trick

One of the simplest ways to check whether your eval suite is stable is to run it multiple times and see if results vary. If you run the same eval suite three times with the same system and get wildly different scores, one of three problems exists: your suite is too small, your system is too unstable, or your sampling settings are too random.

A simple enterprise practice is to run the same suite **three times** before making release decisions. If results vary significantly, don't trust the score. Instead, fix the suite by adding more examples, or stabilize the system by reducing temperature or fixing non-determinism, or adjust sampling to ensure consistency.

This is one of the easiest stability checks that requires no statistics background. If your measurements are repeatable, you can trust them. If they're not, you can't.

## Regression Suite Sizing

Your regression suite is not your full evaluation dataset. It's the focused subset that must pass in continuous integration before any code ships. Regression suites need to be fast enough to run in CI, comprehensive enough to catch breaking changes, and stable enough that failures indicate real problems rather than flaky tests.

For a **small product**, 150 to 400 regression tests is typical. This covers top intents, critical paths, past incident cases, and major failure modes without creating a test suite that takes hours to run.

For a **mid-sized product**, 400 to 1,500 regression tests is common. This scale supports multiple channels, multiple languages, several customer segments, and a richer set of features and workflows.

For a **large enterprise product**, 1,500 to 5,000 or more regression tests is normal. These products serve diverse user bases, operate in multiple languages and regulatory contexts, handle complex multi-step workflows, and carry significant business risk.

What goes into the regression suite is not arbitrary. You always include top intents, the head of your traffic distribution. You always include Tier 2 and Tier 3 tasks because failures there have business consequences. You always include past incident cases, the examples that caught real bugs in production. You always include safety red cases, examples that must trigger refusals or content filtering. For agent systems, you include tool failure cases that test error handling. For RAG systems, you include must-abstain cases where the system should decline to answer because it lacks information. For voice systems, you include critical-field confirmation cases where specific values must be validated before proceeding.

The regression suite is your last line of defense before production. It needs to be comprehensive enough to catch regressions but focused enough to provide fast feedback.

## Slice Sizing Rules

You don't need full duplication of your eval dataset across every slice. That would create impossibly large datasets. Instead, you ensure each important slice has minimum viable coverage.

For every important slice, whether it's a language like Arabic or Spanish, a customer segment like enterprise tenants, or a channel like voice versus chat, apply a slice minimum based on risk tier. For **Tier 0 and Tier 1 tasks**, twenty to fifty cases per slice gives you basic confidence. For **Tier 2 tasks**, fifty to 150 cases per slice ensures you can detect meaningful quality differences across slices. For **Tier 3 tasks**, 150 to 300 or more cases per slice provides the depth needed to validate critical functionality in each context.

For top enterprise tenants, maintain a dedicated "top customers pack" even if it's only thirty to 100 cases for each major customer. Enterprise customers often represent massive revenue concentration. A single enterprise tenant can be worth more than your entire long-tail user base combined. Dedicated per-tenant eval coverage is cheap insurance against alienating a key account.

Slice sizing is about recognizing that quality varies across contexts. A system that performs beautifully in English might fail badly in Arabic due to right-to-left text handling, script differences, or cultural context mismatches. A system that works well for small business customers might struggle with enterprise workflows that involve complex approval chains, compliance requirements, or multi-user coordination. Slices let you measure and manage that variation.

## The Safety Math Rule

Safety and privacy are not like other quality dimensions. You don't average safety performance with task accuracy. You don't accept "mostly safe" as good enough. A system that correctly answers 97% of questions but leaks PII in 3% of interactions is not a 97% quality system. It's a compliance disaster.

Enterprise teams maintain a dedicated safety suite with hundreds to thousands of adversarial prompts, scaled to product risk. A consumer chatbot might test against 200 to 500 adversarial prompts covering common jailbreak attempts, policy violations, and harmful content generation. An enterprise system handling medical records might test against 2,000 to 5,000 cases covering HIPAA scenarios, PII exposure risks, and clinical safety constraints.

Safety suites include multilingual variants because safety failures often emerge in non-English languages where safety training data is sparser. They include realistic phrasing, not just obvious bad prompts like "tell me how to build a bomb," but subtler attempts that embed harmful requests in legitimate-looking contexts.

The release gate for safety is binary. **Any safety or PII failure blocks the release** for gated systems where the risk is unacceptable. For lower-risk systems, you might tolerate a small failure rate but you track it explicitly and you set hard thresholds. A system that fails 0.5% of safety cases might be acceptable. A system that fails 5% is not.

## Common Coverage Problems and Fixes

Three problems appear repeatedly in production eval programs, each with clear symptoms and clear fixes.

**Problem one: you have 500 tests but still miss bugs.** Symptoms include production incidents that should have been caught by your eval suite, user complaints about issues your metrics say don't exist, and regressions that slip through despite comprehensive testing. The likely causes are that your tests are duplicated variants of the same easy pattern, or you're missing entire categories like tool failures, abstain cases, or edge slices. The fix is adding "hard packs" that explicitly test difficult cases, adding incident-based cases from every production failure, and enforcing difficulty mix rules so your dataset includes realistic challenges.

**Problem two: quality looks worse after you improved the system.** Symptoms include eval scores dropping after a release you're confident made things better, user metrics improving while eval metrics decline, and confusion about whether to trust the eval or the production data. The likely causes are that your sampling changed, so you're measuring a different distribution than before, or your new eval data is harder, which is actually good but makes scores look worse. The fix is keeping a stable regression suite that doesn't change, tracking difficulty labels so you can compare fairly across dataset updates, and anchoring comparisons to production metrics when eval metrics diverge.

**Problem three: eval scores don't match user complaints.** Symptoms include high eval scores alongside high escalation rates, stable metrics alongside declining satisfaction scores, and a disconnect between what your measurements say and what your users experience. The likely causes are that you're not sampling production data, so your eval reflects an idealized distribution that doesn't match reality, or you don't have business metrics in the loop, so you're measuring technical correctness without measuring user value. The fix is aligning your eval datasets with production slices, sampling real user queries to ensure representativeness, and integrating business scorecards that tie eval results to user satisfaction, escalation rates, and other outcome metrics.

These problems aren't unsolvable. They're symptoms of misalignment between what you measure and what matters. Fixing them is about adjusting your coverage to reflect reality.

## Quick-Start Rules Summary

If you need a concrete starting point, use these defaults and adjust based on what you learn.

For Tier 0 and Tier 1 tasks, start with ten to thirty cases per leaf task. For Tier 2 tasks, use thirty to eighty cases per leaf task. For Tier 3 tasks, use eighty to 200 or more cases per leaf task, plus dedicated safety suites. Set your difficulty mix to 20% easy, 60% normal, 20% hard, and shift to 15% easy, 55% normal, 30% hard for high-risk tasks. Give your top ten intents fifty to 200 cases each. Create a long-tail bucket of 200 to 1,000 cases covering rare but costly failures. Size your regression suite to 150 to 5,000 tests depending on product scale. Run your suite three times to check stability before trusting scores. Maintain a dedicated safety and PII suite where any failure blocks the release for gated systems. Keep hard packs that explicitly test tool failures, abstain cases, and critical field validations. Maintain a top customers pack for enterprise tenants. Convert every production incident into regression tests and expanded coverage.

These rules aren't perfect. They're practical defaults that work across many products and many teams. You'll adjust them as you learn what matters in your specific context. But they give you a concrete starting point that's far better than guessing.

The value of these concrete numbers is that they remove paralysis. Teams often get stuck debating how many test cases are enough, whether 50 is better than 80, whether they should invest in more coverage for a specific task. Having default sizing rules means you can make a decision quickly, start building eval coverage, and refine based on what you learn. You can always add more cases later if you discover the coverage is too sparse. You can always consolidate if you discover you over-invested. The worst outcome is spending weeks debating optimal dataset sizes while shipping releases with no eval coverage at all.

The next subchapter tackles a challenge that's becoming increasingly important in 2026 as autonomous agents move from demos to production: how do you evaluate composite workflows where a single user goal decomposes into multiple dependent steps, each of which can succeed or fail independently?

