# 3.1 — Build a Task Taxonomy (Intents, Skills, Channels)

In September 2025, a healthcare technology company spent four months building what they called a "comprehensive evaluation suite" for their clinical documentation assistant. They ran 2,400 test cases, achieved a 91% pass rate, and shipped to production across 18 hospital systems. Within two weeks, they received escalations from seven hospitals about the same problem: the assistant was hallucinating medication dosage details when users asked follow-up questions about previously discussed prescriptions. The eval team reviewed their test coverage and discovered that not a single one of their 2,400 cases tested multi-turn medication queries. They had tested medication extraction, they had tested dosage formatting, they had tested prescription summarization. But they had never classified "multi-turn clinical clarification with medication context" as a distinct task. The taxonomy they built had 11 task categories, all organized by document type. They never organized by intent, by conversational pattern, or by risk. When the post-mortem report landed on the CEO's desk, the conclusion was stark: they tested what was easy to measure, not what was dangerous to get wrong.

This is the foundational mistake that kills eval programs before they begin. Teams build evaluation datasets without first building a clear, structured map of what their system actually does. They organize tests around implementation details, around UI surfaces, around whatever data was easiest to collect. They end up with "great coverage" of trivial cases and zero coverage of the interactions that actually matter. A task taxonomy is not an academic exercise. It is the architecture that determines whether your evaluation program catches real problems or generates false confidence.

## What a Task Taxonomy Is and Why You Need One

A **task taxonomy** is a structured, versioned inventory of every distinct thing your AI system must handle, organized by intent, skill, channel, risk level, and complexity. It is the authoritative source of truth that answers the question: what does this system actually do? Not what features does it have, not what endpoints does it expose, not what models does it use. What tasks must it execute correctly for users to trust it and for your organization to avoid harm? The taxonomy is the foundation on which you build every other component of your evaluation program: your datasets, your metrics, your gates, your monitoring, and your incident response process.

You build this taxonomy so you can do five things that are otherwise impossible. First, you can construct evaluation datasets that are representative, not convenient. Without a taxonomy, teams build eval sets by pulling whatever data is easiest to collect: recent user logs, examples that domain experts happen to remember, test cases copied from public benchmarks. This produces datasets that are biased toward what is frequent, well-documented, or easy to label. The taxonomy forces you to ask: what are all the tasks we must handle, not just the tasks we happen to have data for? It makes missing coverage visible before you invest in building a dataset.

Second, you can track performance regressions by task type, not just overall averages that hide catastrophic failures in specific scenarios. When you deploy a new model or change a prompt template, your aggregate accuracy might stay flat or even improve. But that aggregate hides the fact that Tier 3 medical disambiguation tasks regressed by 15 percentage points while Tier 0 creative writing tasks improved by 20 percentage points. Without a taxonomy, you celebrate the overall improvement and ship the regression. With a taxonomy, you catch the regression immediately because you slice your eval results by task type, and the Tier 3 decline is flagged as a blocker.

Third, you can assign clear ownership for each task category so that when something breaks, the responsible team knows immediately. When a production incident surfaces, the first question is always: who owns this? Without a taxonomy, the answer is vague: "the AI team" or "the product team." With a taxonomy, the answer is specific: "the agent workflows team owns all C3 tasks in the refund processing category, and they are already investigating." This clarity eliminates the delay between incident detection and incident response. Every hour spent figuring out who should investigate is an hour that the bug continues affecting users.

Fourth, you can communicate risk to leadership and legal in terms they understand: not "the model scores 87% on our eval set" but "we have strong coverage on Tier 2 account management tasks and weak coverage on Tier 3 medical disambiguation tasks." Leadership does not care about model architectures or eval methodologies. They care about risk exposure. Is the system safe to ship? What are the chances of a regulatory incident? What are the chances of a user harm event? The taxonomy translates technical eval results into risk language that executives and legal teams can act on. It is the Rosetta Stone between engineering and leadership.

Fifth, you can prevent the spread-too-thin anti-pattern where teams add new product features without adding corresponding evaluation coverage. Every feature launch is a promise to users that the feature works. But if you do not have eval coverage for the feature, you have no evidence to back up that promise. The taxonomy makes feature launches legible: when product proposes a new feature, you check whether it represents a new task or a variant of an existing task. If it is new, you add it to the taxonomy and block launch until eval coverage exists. If it is a variant, you check whether the existing eval set covers the variant, and you add examples if it does not. This discipline prevents the gap between what you ship and what you test from growing over time.

The taxonomy is not a description of your current eval set. It is a description of reality. If your system handles a task in production, that task belongs in the taxonomy, whether you test it today or not. The taxonomy makes gaps visible. That visibility is the entire point. A mature eval program is not one where every task has perfect coverage. It is one where you know exactly which tasks have coverage and which do not, and you make conscious, risk-informed decisions about which gaps to close first. The taxonomy is the tool that enables those decisions.

Taxonomies also serve as communication bridges across org boundaries. When you tell a security team "we need to test authentication flows," they hear one thing. When you tell them "we need coverage for three Tier 3 tasks: password reset via agent, two-factor authentication in voice, and account recovery in chat," you trigger a different conversation. The taxonomy gives everyone a shared vocabulary. It prevents the miscommunication that happens when engineering, product, QA, and legal are all using different mental models of what the system does. The taxonomy forces those mental models to converge into a single, documented structure that everyone references.

## The Five Layers of Task Classification

A complete taxonomy has five dimensions. You classify each task along all five axes because each axis reveals different kinds of risk and different testing requirements. The first dimension is product surface, which most teams call channel. This answers: where does the interaction happen? Is this a chat assistant, a retrieval-augmented generation system, an agent that uses tools, a voice interface, a multimodal system that interprets screenshots or form fields? Channel matters because each channel introduces different failure modes. A chat assistant might generate a plausible but incorrect answer. A RAG system might cite a source that does not support the claim. An agent might call the wrong tool or fail to recover from a timeout. A voice system might mishear a critical field like a medication name. You cannot write meaningful test cases until you know which channel you are testing.

Channel classification also determines your testing infrastructure. Chat tasks can be evaluated with text-to-text comparison. RAG tasks require citation verification: does the generated answer have support in the retrieved documents, and is the citation accurate? Agent tasks require tool call verification: did the agent call the right tools in the right sequence, and did it handle errors correctly? Voice tasks require audio input testing and speech recognition accuracy measurement. Multimodal tasks require visual understanding evaluation: did the system correctly interpret the image or screenshot? Each channel needs different evaluation tooling, and your taxonomy must make channel explicit so that your eval infrastructure can route tasks to the appropriate evaluators.

Channel distinctions also surface when you add a new product surface. If you launch a chat assistant and later add voice support, you cannot assume that your chat eval set covers voice. Voice introduces failure modes that do not exist in chat: misheard words, background noise, varied speaking rates, accent variations. Your taxonomy must distinguish chat tasks from voice tasks so that when you add voice, you know exactly which tasks need new test coverage and which can reuse existing coverage with minor adaptations. This clarity prevents the common mistake of assuming that a feature working in one channel means it works in all channels.

The second dimension is user intent, which answers: what does the user want to accomplish? This is not the same as what the system does. Intent is the user's goal. Common intents include asking a question, summarizing content, drafting a document, retrieving policy or knowledge, troubleshooting a problem, completing a workflow like booking an appointment or processing a refund, escalating to a human agent, or reporting an issue or complaint. Your product will have domain-specific intents. A legal research assistant has intents like "find precedent cases" or "compare contract clauses." A customer support assistant has intents like "check order status" or "explain return policy." A code generation assistant has intents like "implement function from spec" or "debug failing test." The key principle is that intent must be defined from the user's perspective, not from the system's internal architecture.

Intent classification is harder than it looks because users rarely state their intent explicitly. A user who types "I need to update my address" has the intent "modify account information," but they phrased it as a need statement, not a command. A user who types "why was I charged twice" has the intent "dispute a charge" or "troubleshoot a billing issue," but they phrased it as a question. Your taxonomy must define intents in canonical form, and your system must map messy user inputs to those canonical intents. This mapping is part of what you evaluate. If your system misclassifies intent, it will execute the wrong task, and the user experience fails even if the system executes that wrong task perfectly. Intent classification errors are a common failure mode, and they surface when you test multi-turn conversations where the user's stated intent evolves across turns.

Intent also serves as a stability layer. Your system architecture will change: you will swap models, rewrite prompts, refactor your retrieval pipeline. But user intents remain stable. Users will always want to "check order status" or "dispute a charge" regardless of whether you are using GPT-4 or Claude 4, regardless of whether you use RAG or fine-tuning. When you organize your taxonomy by intent, you create a stable reference that survives implementation changes. This stability is why intent is a first-class dimension, not a secondary attribute.

The third dimension is required skills, which answers: what must the system do to satisfy the intent? This is where you map capabilities. Common skills include instruction following, reasoning, grounding in sources for RAG systems, tool use for agents, data extraction for structured outputs, personalization within allowed privacy constraints, safety handling such as refusal or redirection, tone control to match brand voice, and multi-turn memory or state management. A single user intent often requires multiple skills. When a user says "book me an appointment with the same doctor I saw last month," the system must demonstrate multi-turn memory, tool use to query availability, data extraction to parse the appointment slot, and tone control to confirm in a natural, reassuring way. If your eval set does not test the combination of skills required for real intents, your eval set does not reflect reality.

Skill classification is where most taxonomies reveal whether the team understands their system deeply or superficially. A superficial taxonomy lists skills like "uses GPT-4" or "has a vector database." Those are implementation details, not skills. A deep taxonomy lists skills like "synthesizes information from conflicting sources" or "recovers gracefully when a tool returns an error." The difference is that deep skills are observable behaviors that you can test, while superficial skills are architectural choices that tell you nothing about whether the system works. When you classify skills, ask: can I write a test case that passes if the system has this skill and fails if it does not? If the answer is no, you have not defined a skill. You have named a component.

Skill requirements also interact. A task that requires both multi-turn memory and tool use is harder to test than a task that requires only one. A task that requires both reasoning and safety handling introduces tradeoffs: the system must be smart enough to solve the problem but cautious enough to refuse when the request is unsafe. Your taxonomy must capture these interactions because they are where subtle bugs hide. A system might pass tests for reasoning alone and pass tests for safety alone, but fail when a task requires both simultaneously. If your taxonomy does not identify multi-skill tasks as distinct from single-skill tasks, you will not catch these integration failures.

Skills also change as models evolve. GPT-4 in 2024 required careful prompting to follow multi-step instructions. GPT-5.1 and Claude Opus 4 in 2025 and 2026 handle multi-step instructions more naturally. If your taxonomy labels tasks by skill instead of by implementation, you can measure how model improvements change your coverage needs. A task that was C3 because it required complex orchestration might become C2 with a better model. The taxonomy helps you track these shifts systematically.

The fourth dimension is risk tier, which answers: how dangerous is it if the system gets this wrong? You use a simple four-tier model because anything more granular creates classification paralysis. **Tier 0** is low risk: creative brainstorming, casual writing assistance, exploratory questions where the user knows to verify the answer. **Tier 1** is medium risk: how-to guidance, general troubleshooting, content summarization where errors are inconvenient but not harmful. **Tier 2** is high risk: account actions, payment processing, business-critical decisions, any task where an error costs money or damages user trust. **Tier 3** is regulated or critical: medical advice, legal guidance, financial recommendations, child safety decisions, identity verification, or any task that handles personally identifiable information under regulatory constraints like HIPAA, GDPR, or the EU AI Act. Risk tier determines gate severity, approval workflows, and monitoring requirements. Tier 3 tasks require human-in-the-loop verification or hard constraints that prevent the system from acting autonomously. You cannot make these decisions without a risk classification.

Risk tiers are not static. They change as regulations evolve and as your user base changes. A task that was Tier 1 in 2024 might be Tier 3 in 2026 if new regulations reclassify it. The EU AI Act, which came into full enforcement in 2025, reclassified many tasks that touch hiring, credit decisions, and law enforcement as high-risk, which maps to Tier 3 in this framework. If your taxonomy does not include a process for reviewing risk classifications annually, you will miss these regulatory shifts, and you will discover that you are out of compliance when an auditor asks to see your risk assessments.

Risk classification also depends on user context. A task that is Tier 1 for an internal tool might be Tier 2 or Tier 3 for a customer-facing tool. A task that is Tier 1 for an adult user might be Tier 3 for a minor. Your taxonomy must account for these context-dependent risk variations, either by creating separate leaf tasks for different user contexts or by adding context flags to each task. The wrong approach is to average the risk: a task that is sometimes Tier 3 must always be treated as Tier 3, because you cannot predict when the high-risk context will occur.

Risk tiers also drive resource allocation. Tier 3 tasks require dedicated red-team exercises, adversarial testing, human reviewers, and ongoing monitoring. Tier 0 tasks require basic regression tests and light monitoring. If you do not classify risk, you cannot allocate resources appropriately, and you either over-invest in low-risk tasks or under-invest in high-risk tasks. Both are inefficiencies that the taxonomy prevents by making risk explicit and measurable.

The fifth dimension is complexity tier, which answers: how hard is this task for the system to execute? Complexity drives your testing strategy because harder tasks require more edge-case coverage and more sophisticated evaluation methods. **C0 tasks** are single-turn and obvious: the user asks a simple factual question, the system retrieves a clear answer from a knowledge base. **C1 tasks** require multi-turn clarification: the user asks an ambiguous question, the system must ask for more information before answering. **C2 tasks** require retrieval or structured reasoning: the system must search multiple sources, reconcile conflicting information, or apply domain-specific rules. **C3 tasks** require tools, state, and error recovery: the system must orchestrate multiple tool calls, manage conversation state across turns, and handle failures like timeouts, missing data, or user corrections. Complexity is not the same as risk. A C3 task might be Tier 1 if it is exploratory. A C0 task might be Tier 3 if it handles regulated data. You classify both dimensions independently.

Complexity tier tells you how many test cases you need and how sophisticated your evaluation methods must be. C0 tasks can be tested with 20 examples and simple pass or fail criteria. C3 tasks need 50 or more examples, and they need multi-step evaluation criteria: did the agent call the right tools, in the right order, with the right parameters, and did it handle errors correctly at each step? If any step fails, the task fails, even if the final output looks plausible. Evaluating C3 tasks requires trace-level instrumentation: you need to log every tool call, every state transition, and every decision point, so that when a test fails, you can identify which step broke and why.

Complexity also determines how you handle model upgrades. When you switch from GPT-4 to GPT-5.1 or from Claude Opus 4.5 to Claude 4, C0 tasks usually improve or stay the same. C3 tasks might improve in some dimensions and regress in others, because the new model has different strengths and weaknesses in multi-step reasoning, tool use, and error recovery. Your taxonomy must flag C3 tasks as high-risk during model transitions, even if they are Tier 1 from a safety perspective, because complexity makes regressions harder to predict and harder to debug.

Complexity tier also interacts with organizational readiness. C3 tasks require engineers who understand distributed systems, error handling, and state management. If your team has expertise in prompt engineering but not in agent orchestration, your C3 tasks are higher risk than the complexity tier alone suggests. The taxonomy makes this skill gap visible so that you can decide whether to invest in training, hire specialists, or reduce reliance on C3 tasks until the team matures.

Complexity tier also determines your debugging strategy. When a C0 task fails, you check the input, the output, and the model response. Debugging is straightforward. When a C3 task fails, you need to trace through tool calls, state transitions, error handling branches, and recovery logic. Debugging requires sophisticated instrumentation and logging. If your observability infrastructure is not built for C3 debugging, you will struggle to diagnose failures quickly. The taxonomy makes this requirement explicit so that you can invest in the right tooling before you ship C3 tasks to production. Teams that skip this investment ship C3 tasks with C0-level observability, and when things break, they cannot diagnose the root cause without days of manual log analysis.

## How to Structure the Taxonomy for Production Use

The taxonomy must be usable by your entire organization, not just the eval team. That means it must be concise, consistent, and versioned. The most effective taxonomies in enterprise production have between 30 and 80 leaf tasks. Fewer than 30 and you are collapsing distinct tasks into vague buckets that hide failures. More than 80 and you create classification overhead that slows down dataset construction and reporting. Each leaf task must meet three criteria: it must be testable, meaning you can write eval cases for it and measure success; it must be owned, meaning a specific team or person is responsible for its performance; and it must be tied to a dataset slice, meaning you can filter your eval results to see performance for that task alone.

The testability criterion is the most commonly violated. Teams create task categories like "handle complex queries" or "provide personalized responses." These are not testable because they are too vague. What makes a query complex? What counts as personalization? Without clear definitions, different evaluators will apply different standards, and your eval results will be unreliable. Testable tasks have clear success criteria: "Retrieve the correct refund policy document and cite the relevant section" is testable. "Help the user with refunds" is not. When you review your taxonomy, apply the test: can two evaluators independently assess the same output and agree on whether it passes? If not, the task definition is too vague.

The ownership criterion prevents the diffusion of responsibility. When a task fails in production, someone must be accountable for investigating and fixing it. If the taxonomy does not assign ownership, the responsible team is ambiguous, and the bug languishes. Ownership is usually at the branch level, not the leaf level. A product team might own all tasks related to account management. An engineering team might own all tasks in the agent channel. The taxonomy makes these assignments explicit. When you onboard a new team member, you hand them the taxonomy and say: these are the tasks you own. That clarity eliminates confusion and speeds up incident response.

Ownership also creates healthy tension. When you assign a team to own a set of tasks, they care about the quality of those tasks because failures reflect on them. Teams start asking for better eval coverage before they ship features because they know they will be held accountable for regressions. This accountability loop is how organizations move from reactive quality management to proactive quality management. The taxonomy makes accountability visible and measurable, and visibility drives behavior change.

You use risk tier and channel as first-class labels in every report, every scorecard, and every regression alert. When you tell a product manager that "overall accuracy is 89%," you communicate nothing actionable. When you tell them that "Tier 2 account modification tasks in the agent channel are at 76%, down from 84% last week," you trigger the right conversation. Leadership needs to see risk. Engineers need to see channel. Domain experts need to see intent. Your taxonomy structure must support all three views. This multi-dimensional slicing is only possible if you tag every task with all five dimensions at taxonomy creation time, not retroactively. Retrofitting tags onto an existing taxonomy is slow and error-prone. Building them in from the start is fast and reliable.

You enforce a consistent naming convention to prevent chaos as the taxonomy grows. The format that works best is verb-object-constraint. "Retrieve refund policy" is better than "refund help." "Summarize clinical note for insurance submission" is better than "clinical summarization." Each task name should include the channel, risk tier, and complexity tier in metadata, even if not in the name itself. For example, "Retrieve refund policy (RAG, Tier 2, C2)" tells evaluators exactly what they are testing and what standards apply. Naming consistency also makes the taxonomy searchable. When an engineer sees a bug report about refund policy retrieval, they can search the taxonomy for "refund policy" and immediately find the relevant task, the owner, the eval dataset, and the historical performance trends.

Naming conventions also prevent duplication. Without a consistent format, different teams might create "check order status," "verify order status," and "get order status" as three separate tasks when they all refer to the same user intent. Duplication fragments your eval coverage and makes performance tracking unreliable. The naming convention forces teams to check whether a task already exists before adding a new one, and it makes duplicates easy to spot during reviews. Duplication also creates confusion during incident response: when a bug surfaces, multiple teams might think they own it because multiple similar tasks exist, and the resulting coordination overhead delays the fix. The naming convention eliminates this ambiguity by ensuring that each user intent maps to exactly one canonical task name.

## Common Failure Modes and How They Surface

The most common failure mode is shallow taxonomy syndrome, where teams create only five or six task categories and call it complete. A taxonomy with categories like "question answering," "summarization," and "general help" is not a taxonomy. It is a vague gesture at structure. When you run evals against these categories, you get meaningless averages. High-risk tasks are buried inside those averages. Rare but critical edge cases never appear because they are statistically insignificant in the overall mix. When something breaks in production, you cannot pinpoint the failure because your taxonomy has no resolution. Shallow taxonomies feel efficient because they are easy to create and easy to maintain. But they are false efficiency. They save time on taxonomy construction and cost weeks on debugging production incidents because you cannot isolate the failure to a specific task type.

The second failure mode is implementation-centric taxonomy, where tasks are organized by internal system architecture instead of user-facing behavior. Teams build categories like "vector search tasks," "tool invocation tasks," and "prompt chaining tasks." These categories describe how the system works, not what it does for users. The problem is that a single user intent often spans multiple implementation layers. When you organize by implementation, you cannot evaluate whether the end-to-end task succeeds. You evaluate components in isolation and miss integration failures. This failure mode is common among engineering-led teams who think about the system in terms of its internal components. The fix is to involve product and domain experts in taxonomy construction so that the user perspective is represented, not just the engineering perspective.

The third failure mode is missing risk stratification, where the taxonomy lists tasks but does not label their risk tiers. When leadership asks "what are we good at and what are we bad at," you show them a flat list of scores. They cannot distinguish between a 78% pass rate on Tier 0 creative tasks and a 78% pass rate on Tier 3 medical disambiguation. One is acceptable, the other is a liability. Without risk labels, you cannot set appropriate gates, you cannot prioritize bug fixes, and you cannot communicate exposure to legal or compliance teams. This failure mode happens when the taxonomy is built by engineers or QA without input from legal, compliance, or domain experts who understand which tasks carry regulatory or reputational risk. The fix is to make risk classification a required step in taxonomy construction, and to assign a legal or compliance reviewer to validate the classifications.

The fourth failure mode is missing channel segmentation, where tasks are defined without specifying the interaction surface. "Retrieve policy information" means something completely different in a chat assistant versus a RAG system versus a voice agent. In chat, failure might mean a generic response. In RAG, failure might mean citing a source that contradicts the claim. In voice, failure might mean misinterpreting the policy name and returning the wrong document. If your taxonomy does not distinguish these, your eval dataset will not either, and you will miss channel-specific regressions. This failure mode surfaces when you add a new channel to an existing product. You copy the task definitions from the original channel without re-examining whether those definitions still apply. They usually do not. Each channel introduces unique failure modes, and the taxonomy must capture that uniqueness.

The fifth failure mode is taxonomy abandonment, where the team builds a taxonomy, uses it for a few weeks, and then stops updating it as the product evolves. Six months later, the taxonomy is obsolete. It describes a system that no longer exists. New features are not represented. Deprecated features are still listed. Risk classifications are out of date. The taxonomy becomes a historical artifact instead of a living tool. Abandonment happens when taxonomy maintenance is not integrated into the product development process. The fix is to make taxonomy updates a required step in your feature launch checklist, and to assign ownership of taxonomy maintenance to a specific person or team, not to the amorphous "AI team."

A sixth failure mode is over-granularity, where teams create 200 or 300 leaf tasks because they classify every minor variation as a distinct task. The taxonomy becomes unwieldy. No one can remember what all the tasks are. Reporting requires scrolling through pages of细 categories. The overhead of classification exceeds the value it provides. Over-granularity happens when teams confuse the taxonomy with test case documentation. The taxonomy defines tasks at a strategic level. Test cases cover variations within each task. If your taxonomy has a task for every possible phrasing of a user request, you have gone too far. The fix is to consolidate tasks that share the same intent, channel, risk tier, and complexity tier, and to capture variations in your test cases, not in your taxonomy.

## How to Build the Taxonomy from Production Data

You do not build the taxonomy by brainstorming in a conference room. You build it from real user interactions. Start by pulling the top 1,000 user interactions from production logs. If you do not have production traffic yet, pull from beta testing, from internal dogfooding, from user research sessions. If you have nothing, you start with your product requirements and acceptance criteria, but you commit to rebuilding from real data as soon as it exists. The taxonomy built from hypothetical use cases will diverge from reality the moment real users touch your system, and that divergence is where failures hide.

The most common mistake at this stage is pulling a random sample of interactions and calling it representative. A random sample gives you what is frequent, not what is important. If 80% of your traffic is simple questions and 5% is high-risk account modifications, a random sample gives you a taxonomy dominated by simple questions. You will build eval coverage that reflects frequency, and you will miss the rare, high-stakes interactions that cause incidents. Instead, you stratify your sample. Pull the top 500 interactions by volume, then separately pull 200 interactions from each high-risk category, then pull 100 interactions from each known failure mode. This stratified sample ensures that your taxonomy reflects both what users do most often and what you cannot afford to get wrong.

You cluster those 1,000 interactions into 10 to 20 intent groups. Use whatever method is fastest: manual review, embeddings-based clustering, an LLM-assisted grouping tool. Speed matters more than perfection at this stage. Each intent group represents a user goal: "check account balance," "dispute a charge," "reschedule an appointment," "explain a diagnosis code," "retrieve compliance guidelines." The intent groups are your first draft. You review this draft with your product team and your domain experts to catch gaps. Product knows what features exist that might not yet have traffic. Domain experts know what edge cases exist that might not yet have surfaced. This review step typically surfaces another 3 to 5 intent groups that were missing from the data-driven clustering.

Next, you expand each intent group into leaf tasks by splitting along the five dimensions. "Check account balance" might split into: "check account balance in chat, Tier 1, C0," "check account balance in voice, Tier 2, C1," and "check account balance via agent with follow-up transaction, Tier 2, C3." Not every combination will exist in your product, but every combination that does exist must be represented. The leaf tasks are what you actually test. This expansion step is where you discover hidden complexity. An intent that seemed simple in aggregate reveals that it has high-risk variants and low-risk variants, voice-specific challenges and agent-specific challenges. Each variant becomes a leaf task because each variant has different success criteria and different failure modes.

You tag each leaf task with its channel, risk tier, and complexity tier. You assign an owner, which is usually the product or engineering team responsible for that feature area. You record whether a dataset slice exists for that task, and if so, how many examples it contains. This metadata turns the taxonomy from a static document into a live operations tool. The metadata also reveals organizational gaps. If a task has no clear owner, that is a signal that responsibility is ambiguous, and ambiguous responsibility is how tasks fall through the cracks. If a task has an owner but no dataset, that is a signal that the owner has not prioritized evaluation, and you escalate that gap to leadership.

You establish a version control rule: the taxonomy is a versioned artifact, just like your codebase. When you add a new product feature, you add the corresponding tasks to the taxonomy before you ship. When you discover a new edge case in production, you add it as a refinement to an existing task or as a new leaf task if it represents a distinct interaction pattern. When you run a post-incident review, one of the action items is always "update the taxonomy to reflect the gap we missed." This discipline prevents taxonomy drift, where the document becomes stale and disconnected from the system it describes. Taxonomy drift is how teams end up with "comprehensive" eval coverage that misses the features that shipped in the last six months.

You version the taxonomy explicitly: version 1.0 at launch, version 1.1 when you add support for a new channel, version 2.0 when you restructure the task hierarchy after learning from six months of production data. Each version is tagged with a date and a changelog that explains what changed and why. This version history is valuable when you need to understand why a task was classified the way it was, or when you need to roll back a taxonomy change that introduced confusion. It also makes the taxonomy auditable, which matters when legal or compliance asks "how did you decide what to test."

Version history also enables trend analysis. You can track how many tasks were added per quarter, which teams contributed the most tasks, and whether task additions correlate with incident rates. If you add five new tasks in a quarter but incidents stay flat, you are learning and adapting. If you add zero tasks but incidents spike, you are missing something fundamental about how users interact with your system. Versioning turns the taxonomy into a time series that reveals organizational learning velocity.

## What Enterprise Teams Do with the Taxonomy

Professional teams maintain the taxonomy as a shared document, usually in a wiki, a spreadsheet, or a configuration file that feeds into their evaluation pipeline. It is never hidden in someone's head or trapped in a slide deck. It is accessible to product, engineering, QA, legal, and leadership. It is the single source of truth for the question: what does this system do? The format matters less than the accessibility. A well-structured spreadsheet with clear column headers works better than a beautifully formatted wiki page that no one can find or edit. The best taxonomies are the ones that get used daily, not the ones that look impressive in a readout.

They add a change control rule: no new product feature ships without corresponding taxonomy updates. This prevents the common failure mode where a new feature launches, performs poorly, and no one notices because the eval set does not cover it. The taxonomy is the gate. If a task is not in the taxonomy, it is not tested, and if it is not tested, it does not ship to production. This rule sounds obvious, but it requires enforcement. Product teams move fast, and they will ship features without waiting for eval coverage if you let them. The change control rule must be backed by a process: the product manager submits a taxonomy update as part of the feature spec, the eval team reviews and approves it, and the feature is added to the release checklist only after the taxonomy update is merged.

They slice every scorecard by task, tier, and channel. When they review eval results in a weekly meeting, they do not start with an overall accuracy number. They start with Tier 3 tasks, then Tier 2, then Tier 1. They review tasks that have regressed since the last release. They review tasks that have weak dataset coverage. They review tasks that have no dataset coverage at all, and they prioritize building that coverage based on risk and user impact. This top-down review by risk tier is the discipline that prevents the common mistake of celebrating an overall accuracy improvement while missing that a critical task regressed by 10 percentage points.

They use the taxonomy to assign ownership and accountability. When a Tier 2 agent task fails in production, the responsible team knows immediately because the taxonomy maps that task to them. There is no ambiguity, no finger-pointing, no "we thought another team was handling that." Ownership is explicit and documented. This ownership model also surfaces organizational problems. If the same team owns 30 of your 50 leaf tasks, you have a bottleneck. If a task has changed owners three times in six months, you have a leadership problem. The taxonomy makes these patterns visible.

They track taxonomy completeness as a key metric. Completeness is the percentage of production tasks that have adequate eval coverage. A task has adequate coverage if it meets the minimum standards for its tier: at least 20 examples for Tier 0-1 tasks, at least 30 examples for Tier 2 tasks, at least 50 examples plus red-team cases for Tier 3 tasks. Leadership reviews completeness monthly, and any task with zero coverage is flagged as a gap that must be closed before the next major release. Completeness is a lagging indicator: it tells you where you are, not where you are going. The leading indicator is velocity: how many tasks moved from zero coverage to some coverage this month? If velocity is zero, your eval program is stagnant. If velocity is positive but slower than your feature release velocity, you are falling behind, and eventually a gap will cause an incident.

They also track taxonomy stability, which measures how often you restructure the task hierarchy. High stability means you got the structure right early and you are just adding leaf tasks as the product grows. Low stability means you are constantly reorganizing, which suggests that the original structure did not reflect the domain. Some instability is expected in the first six months as you learn from production data. But if you are still restructuring after a year, you have a conceptual problem. Either the taxonomy is too fine-grained and you are collapsing categories, or it is too coarse and you are discovering that your broad categories hide important distinctions. Stability is a signal of maturity.

## Taxonomy as the Contract Between Teams

The taxonomy is not just a technical artifact. It is a contract between product, engineering, QA, legal, and domain experts. Product commits to defining what the system must do in user-facing terms. Engineering commits to building capabilities that handle those tasks reliably. QA commits to testing each task against defined success criteria. Legal and compliance commit to classifying risk tiers accurately and reviewing high-risk tasks before launch. Domain experts commit to providing the ground truth and edge-case knowledge needed to build representative datasets. When all these teams contribute to the taxonomy and agree on its structure, you have alignment. When one team dominates taxonomy construction, you have gaps.

The contract model also creates accountability. When a task fails in production, the taxonomy tells you which team is responsible for fixing it, which team is responsible for preventing recurrence through improved tests, and which team is responsible for updating the taxonomy to reflect what you learned. This clarity speeds up incident response and prevents the blame-shifting that wastes time and damages morale. The taxonomy is the shared ground truth that all teams reference when things go wrong.

The taxonomy also serves as onboarding documentation. When a new engineer joins the team, you hand them the taxonomy and say: this is what our system does, this is how we categorize tasks, this is how we assign risk, and this is where your work fits into the overall picture. Without a taxonomy, onboarding is ad hoc. With a taxonomy, onboarding is systematic. New team members can contribute productively within days instead of weeks because they understand the structure of the system and the structure of the evaluation program.

The taxonomy also functions as a forcing function for cross-functional collaboration. When product proposes a feature, engineering must classify its complexity tier. When engineering implements a feature, legal must classify its risk tier. When QA builds tests, domain experts must validate that the test cases reflect real user needs. This collaboration happens naturally because the taxonomy requires input from all these perspectives. Teams that skip this collaboration produce incomplete taxonomies, and incomplete taxonomies produce incomplete testing. The taxonomy makes collaboration a requirement, not an option, and that structural forcing function is what prevents the siloed thinking that causes gaps between what teams believe they shipped and what they actually shipped.

## How the Taxonomy Evolves as Your System Matures

The taxonomy you build at launch is not the taxonomy you will have two years later. As your system matures, the taxonomy evolves in predictable ways. In the first six months, you add leaf tasks rapidly as you discover edge cases and user behaviors you did not anticipate. Your initial taxonomy might have 30 tasks. Six months later, it has 55 tasks because you learned that users do things you never designed for, and each of those new behaviors needs evaluation coverage. This rapid growth phase is healthy. It means you are learning from production data and closing gaps. The velocity of task additions is a signal of how much you are learning. If you add zero tasks in the first six months, either your initial taxonomy was perfect, which is unlikely, or you are not paying attention to production data, which is likely.

After the first year, growth slows. You are still adding tasks, but at a slower rate, and most new tasks are refinements of existing tasks rather than entirely new categories. Instead of adding new leaf tasks, you split existing tasks into more granular variants when you discover that a single task actually has multiple failure modes that require different testing strategies. For example, "retrieve policy information" might split into "retrieve policy when source is clear" and "retrieve policy when multiple conflicting sources exist." This refinement phase is where the taxonomy becomes precise enough to guide targeted debugging and root cause analysis. Refinement is a sign of deepening expertise. It means your team understands the nuances of the domain well enough to distinguish variants that previously looked the same.

After two years, the taxonomy stabilizes. New tasks are rare, and most changes are updates to risk tiers or complexity tiers as you improve the system's capabilities or as regulatory requirements change. A task that was Tier 2 might become Tier 3 if a new regulation reclassifies it as high-risk. A task that was C3 might become C2 if you improve your agent's error recovery mechanisms. This stability phase is a sign of maturity. It means your taxonomy reflects the domain accurately, and your evaluation program is no longer chasing new failure modes every week. Stability does not mean stagnation. It means you have captured the core structure of the problem space, and changes are incremental rather than foundational.

The taxonomy also evolves in response to model changes. When you upgrade from GPT-4 to GPT-5.1, or from Claude Opus 4.5 to Claude 4, certain tasks become easier or harder. Tasks that required complex prompting might become simple. Tasks that were reliable might start failing because the new model has different biases or refusal patterns. The taxonomy itself does not change, but the coverage requirements change. Tasks that were stable might need increased monitoring. Tasks that improve might allow you to reduce redundant test cases. The taxonomy is the stable reference point that lets you measure how model changes affect your system's capabilities. Without the taxonomy, model upgrades are a black box: you hope things improve, but you have no systematic way to verify what changed and whether the changes are acceptable.

## Quick Check for Taxonomy Readiness

You know your taxonomy is ready when you can answer these questions without hesitation. First, which tasks are we best at? If you cannot list the top five tasks by performance, your taxonomy does not have enough resolution. Second, which high-risk tasks have weak coverage? If you cannot filter your eval results by risk tier and see the gaps, you have not integrated risk classification. Third, who owns each task category? If the answer is "the AI team," your ownership model is too coarse. Fourth, what new tasks did we add in the last release? If you do not know, you do not have change control. Fifth, what happens if we deprecate a model or change a prompt template? If you cannot predict which tasks will be affected, your taxonomy is not connected to your system architecture.

If you struggle with any of these questions, you do not have a taxonomy. You have a list. The taxonomy is the structure that makes these questions answerable, and answering them is what separates evaluation programs that prevent harm from evaluation programs that provide false assurance. The readiness check is not a one-time gate. You run it quarterly to ensure that the taxonomy remains useful as your system grows. If the taxonomy stops being useful, you do not abandon the concept. You fix the taxonomy.

The readiness check also serves as a calibration tool for new team members. When someone joins, you give them the taxonomy and ask them these questions. Their ability to answer reveals how well they understand the system and the evaluation program. If they struggle, you know where to focus their onboarding. If they answer easily, you know they have internalized the structure and can contribute immediately. This onboarding function is undervalued but critical: new team members who understand the taxonomy contribute high-quality work faster than those who must infer the system structure from ad hoc conversations and scattered documentation.

The taxonomy is the foundation, but it is not sufficient on its own. A taxonomy without corresponding eval coverage is just a wish list. The coverage map, which we build in the next subchapter, takes the taxonomy you have created and maps it to the tests you have actually built, making gaps visible and forcing prioritization of the risks that matter most.
