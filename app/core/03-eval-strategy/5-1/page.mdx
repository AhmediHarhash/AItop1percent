# 5.1 — Data Sources: Production Logs, Synthetic Cases, and Expert-Written Examples

In mid-2025, a healthcare technology company launched a clinical documentation assistant for physician workflows. The product used a small evaluation dataset of 200 examples, all written by the product team during feature development. The examples were clean, well-formed, and carefully crafted to test the assistant's ability to extract symptoms, medications, and diagnosis codes from physician notes. The system performed beautifully in testing, achieving 94% accuracy on the curated dataset. When the company deployed the assistant to twelve hospital systems in November 2025, physician complaints started arriving within days. The assistant failed on shorthand notations, struggled with dictated voice-to-text errors, couldn't handle multi-lingual patient names, and broke on copy-pasted lab results with formatting artifacts. The evaluation dataset had measured performance on an idealized version of the problem that bore little resemblance to how physicians actually documented patient encounters in production environments.

The failure wasn't a modeling problem or a prompt engineering mistake. The root cause was dataset composition. The team had built their entire evaluation program on a single source type—expert-written cases created in a controlled environment—and completely ignored the reality of production traffic. When you measure performance only on clean, carefully crafted examples, you build a system optimized for a world that doesn't exist. Your evaluation becomes a vanity metric that tells you nothing about how the system will behave when real users interact with it using real language in real contexts with real constraints.

## The Three Source Types and What They Actually Measure

Every evaluation dataset pulls from one or more of three fundamental sources. Production logs are captures of real user interactions with your system, sampled and stored from live traffic. Expert-written cases are examples crafted deliberately by domain experts, product teams, or trust and safety specialists to test specific capabilities, edge cases, or policy boundaries. Synthetic cases are examples generated programmatically, often using language models or rule-based systems, to fill coverage gaps, stress-test adversarial scenarios, or scale to languages and slices where production data is sparse.

Each source type has inherent strengths and inherent weaknesses. Production logs give you realism—the actual phrasing users employ, the actual distribution of tasks and intents, the actual long-tail variations and failure modes that occur in the wild. They show you what people really ask for, how they recover from errors, and what happens when they're confused or frustrated. Production logs are your ground truth for user behavior. But they're also noisy, biased toward current product capabilities, contaminated with sensitive data, and often incomplete or ambiguous. Users don't always know what they want, and their queries reflect confusion as much as intent.

Expert-written cases give you precision and control. When you need to test compliance boundaries, safety refusals, high-stakes workflows, or policy adherence, you can't rely on hoping the right scenario shows up in production logs. Experts can craft cases that isolate specific capabilities, encode regulatory requirements, or test "must abstain" conditions where the system should explicitly refuse to answer. Expert-written cases are critical for tier 2 and tier 3 workflows where errors have legal, financial, or safety consequences. But expert-written cases are expensive to create, limited in volume, and often written in a cleaner, more formal style than real users employ. Experts encode their own preferences and assumptions, and the resulting dataset can become unrepresentative of actual user behavior.

Synthetic cases give you scale and coverage. You can generate thousands of multilingual variants, adversarial paraphrases, jailbreak attempts, or edge cases that would take months to encounter in production. Synthetic data lets you test systematically across slices, simulate rare but dangerous scenarios, and fill gaps where production data is sparse or unavailable. In 2026, synthetic generation has become sophisticated enough that well-constructed synthetic cases are indistinguishable from real user queries for many task types. But synthetic data can also be unnatural, repetitive, or overly stylized. It can overfit to the generator's patterns, miss real business constraints, or create scenarios that are technically valid but pragmatically irrelevant. Synthetic datasets can become "too easy" if the generator produces only well-formed queries, or "too weird" if the generator optimizes for edge cases that never occur in practice.

## Production Logs: Realism and Its Costs

Production logs are the foundation of representative evaluation. When you sample real user interactions, you capture the authentic distribution of tasks, the authentic phrasing patterns, the authentic error recovery attempts, and the authentic context windows that define how users actually interact with your system. Production logs reveal the queries you didn't anticipate during design, the edge cases you didn't think to test, and the failure modes you didn't know existed. They force your evaluation to confront reality rather than idealized assumptions.

But production logs come with significant operational costs. First, they contain personally identifiable information, sensitive business data, and potentially regulated content that creates legal and reputational risk if mishandled. You can't sample production logs casually and store them in plaintext evaluation datasets without violating privacy policies, user consent agreements, and regulatory requirements. Second, production logs are noisy. Users make typos, submit incomplete queries, ask ambiguous questions, and provide contradictory context. The logs capture not just user intent but user confusion, frustration, and experimentation. Third, production logs are biased toward current product capabilities. If your system doesn't support a particular workflow, users won't ask for it, and it won't appear in your logs. Production logs tell you what users are doing today, not what they would do if your system were more capable.

Fourth, production logs can contain contamination that compromises evaluation validity. If users copy-paste text from your documentation, repeat examples from your marketing materials, or quote your own prompts back to you, those interactions aren't representative of organic use. They're artifacts of how users learn to interact with your system, not authentic expressions of intent. Fifth, production logs require labeling. The raw logs don't come with ground truth labels, correct answers, or success criteria. You must annotate them, either manually by human reviewers or programmatically using heuristics, before they become useful evaluation cases. That labeling process introduces delay, cost, and potential labeling errors.

Despite these costs, production logs remain essential for enterprise evaluation. The alternative is to evaluate on data that doesn't represent real user behavior, which guarantees that your metrics will diverge from production performance. The solution is not to avoid production logs but to build the infrastructure to sample, sanitize, label, and version them safely. You implement sampling strategies that capture representative traffic without collecting excessive volume. You apply anonymization and redaction pipelines that remove sensitive content before logs enter evaluation datasets. You establish consent and retention policies that comply with GDPR, HIPAA, and other regulatory requirements. You version production samples so you can detect distribution drift over time.

## Expert-Written Cases: Precision and Its Limits

Expert-written cases are how you test scenarios that don't occur frequently in production but matter critically when they do. When you need to validate that your system correctly refuses prohibited requests, correctly abstains when evidence is missing, correctly escalates when a workflow requires human review, or correctly applies complex business rules under edge conditions, you can't wait for those scenarios to appear organically in production logs. You commission experts to write them deliberately.

Expert-written cases are particularly critical for high-stakes workflows. In compliance, financial operations, clinical decision support, and legal document review, errors have consequences that production logs can't capture. A system that accidentally approves a fraudulent transaction once in ten thousand cases might show 99.99% accuracy in aggregate metrics, but that single failure can cost your organization millions of dollars and trigger regulatory enforcement action. Expert-written cases let you test those critical failure modes explicitly, in isolation, with clear correctness criteria.

But expert-written cases have systematic limitations. Experts write in a different register than real users. They use complete sentences, correct grammar, precise terminology, and explicit context. Real users write fragments, make typos, use colloquial language, and provide implicit context that assumes shared understanding. When your evaluation dataset is dominated by expert-written cases, your metrics become optimistic because the test data is cleaner and easier than real traffic. The system performs well on well-formed inputs but struggles when users deviate from expected patterns.

Experts also encode their own biases and assumptions. When a domain expert writes an evaluation case, they include the context they consider relevant and omit the context they consider irrelevant. But users don't know what context is relevant. They provide too much, too little, or the wrong information, and your system must handle that gracefully. Expert-written cases tend to include exactly the information needed to answer the question, which doesn't reflect how real interactions unfold. Experts also prioritize scenarios they find interesting or challenging, which may not align with scenarios that occur frequently or matter most to users.

The fix is not to abandon expert-written cases but to use them strategically. You commission expert-written cases for critical workflows, policy boundaries, and safety requirements where precision matters more than realism. You pair expert-written cases with paraphrase variants, incomplete versions, and multi-turn extensions that inject realistic noise without distorting correctness criteria. You track the proportion of expert-written cases in your dataset and ensure it doesn't grow so large that it drowns out production signal. You treat expert-written cases as a tool for targeted testing, not as a replacement for representative evaluation.

## Synthetic Cases: Scale and Its Tradeoffs

Synthetic generation has become a standard tool in enterprise evaluation pipelines. In 2026, you can use GPT-5, Claude Opus 4.5, or Llama 4 to generate thousands of evaluation cases in hours. You can create multilingual variants by prompting the generator to translate examples into Spanish, Mandarin, Arabic, or Hindi. You can create adversarial variants by prompting the generator to paraphrase queries in ways that attempt to bypass safety filters or confuse the system. You can create coverage variants by prompting the generator to explore combinations of task types, slice attributes, and difficulty levels that would take months to sample from production logs.

Synthetic generation solves two critical problems. First, it fills gaps in production coverage. If your production logs are 95% English but your product must support ten languages, you can't evaluate multilingual performance using only production data. You generate synthetic cases in underrepresented languages and validate that the system behaves correctly. Second, synthetic generation accelerates adversarial testing. If you're testing refusal boundaries, jailbreak resistance, or prompt injection defenses, you can't wait for adversaries to attack your production system. You generate synthetic adversarial cases that probe those boundaries systematically.

But synthetic data introduces new risks. Synthetic cases can be unnatural in ways that make them easier or harder than real queries. If the generator produces only grammatically perfect, contextually complete queries, your evaluation becomes optimistic. If the generator produces bizarre edge cases that would never occur in practice, your evaluation becomes pessimistic. Synthetic data can also overfit to the generator's style. If you generate 1,000 synthetic cases using the same prompt template, the cases will share structural patterns, vocabulary choices, and phrasing conventions that don't reflect real user diversity. Your dataset becomes redundant rather than representative.

Synthetic data can also miss business constraints that real users navigate implicitly. A synthetic case might ask for information that's publicly available but treated as confidential in your organization's context. A synthetic case might request a workflow that's technically possible but prohibited by policy. A synthetic case might assume capabilities your system doesn't have or ignore capabilities your system does have. These mismatches arise because the generator doesn't have deep context about your product, your users, or your constraints.

The fix is to apply synthetic control. You cap the proportion of near-duplicate patterns by clustering synthetic cases and sampling diverse representatives. You ensure multiple writing styles by varying the generation prompts, using different models, or post-processing synthetic outputs to inject realistic noise. You ground synthetic generation in real failure modes by seeding the generator with examples of past incidents, escalations, and user complaints. You label synthetic cases explicitly with source metadata so you can stratify metrics by source type and detect when synthetic data is distorting aggregate results. You treat synthetic cases as a supplement to production data, not a replacement for it.

## Why Hybrid Datasets Are Not Optional in 2026

The enterprise default in 2026 is to build hybrid datasets that combine all three source types in deliberate proportions. You sample production logs for realism and distribution alignment, you commission expert-written cases for high-stakes workflows and policy boundaries, and you generate synthetic cases to fill coverage gaps and stress-test adversarial robustness. The specific mix depends on your risk tier, your use case, and your access to production data.

For general-purpose assistants handling primarily tier 0 and tier 1 workflows—questions, summaries, recommendations with low consequence—a typical dataset composition is 50 to 70 percent production logs, 20 to 30 percent synthetic cases, and 10 to 20 percent expert-written cases. The production logs anchor the dataset to real user behavior, the synthetic cases fill long-tail gaps and test edge cases, and the expert-written cases validate specific capabilities or policy adherence that might not surface frequently in production.

For high-stakes assistants handling tier 2 and tier 3 workflows—compliance, financial operations, clinical decision support, legal document review—the composition shifts significantly. You use 30 to 50 percent production logs, carefully sanitized to remove sensitive data, 20 to 30 percent synthetic cases focused on adversarial scenarios and coverage, and 20 to 40 percent expert-written cases that encode policy requirements, safety boundaries, and "must abstain" conditions. The higher the risk tier, the more you rely on expert-written and synthetic cases to ensure you've tested every scenario that matters, not just the scenarios that happen to occur in production.

For agent systems that use tools and take actions, your dataset must include production traces where possible, sanitized to remove sensitive parameters and outputs, expert-written workflow cases that validate multi-step task completion and confirmation requirements, and synthetic tool-failure packs that simulate timeouts, permission errors, partial results, and cascading failures. Agents fail in complex ways that don't show up in single-turn question answering, and your dataset must reflect that complexity.

For RAG systems, you need production queries paired with real document snapshots, expert-written "must abstain" cases where the evidence doesn't support an answer, expert-written "must cite" cases that validate citation accuracy, and synthetic "retrieval trap" cases where similar-sounding but incorrect documents are placed in the retrieval context to test whether the system can distinguish relevant from irrelevant evidence. RAG evaluation is not just about whether the model can answer questions—it's about whether the system can abstain when evidence is missing and cite correctly when evidence is present.

For voice systems, you use real call snippets where permitted and safe, scripted scenarios for coverage of dialog flows and confirmation requirements, and synthetic variants that test robustness to noise, accents, interruptions, and barge-in conditions. Voice datasets must include audio, transcripts, and timing metadata to evaluate not just what the system says but how it behaves in real-time interaction.

## Metadata is How You Prevent Silent Dataset Drift

Every example in your dataset must be tagged with source metadata that records where it came from, when it was added, and what conditions it represents. At minimum, you track source type—production, expert-written, or synthetic—along with channel, risk tier, task ID from your taxonomy, slice labels for language, region, customer tier, and tenant, difficulty label ranging from easy to adversarial, and versioning information including date added and dataset version. This metadata is not optional documentation. It's how you detect and diagnose silent dataset drift, where the distribution of examples in your evaluation set diverges from the distribution of examples in production traffic.

If your production traffic shifts toward multilingual queries but your dataset remains 90 percent English, your evaluation results will become increasingly disconnected from real performance. If your production traffic starts including more tier 2 workflows but your dataset remains dominated by tier 0 examples, you'll underestimate risk and overestimate readiness. If your expert-written cases grow to dominate the dataset, your metrics will become optimistic because expert-written cases are cleaner and easier than real user queries. Metadata lets you track these shifts and rebalance your dataset before your evaluation program loses predictive power.

You also tag difficulty explicitly. Easy cases are straightforward, unambiguous, and require only basic capabilities. Normal cases reflect typical production complexity. Hard cases are legitimately challenging but realistic. Adversarial cases are deliberately constructed to test robustness, safety boundaries, or jailbreak resistance. If your dataset is entirely easy and normal cases, you're measuring performance on the easiest slice of the problem and ignoring the long tail where failures actually occur. If your dataset is entirely adversarial, you're measuring robustness but not typical performance, and your metrics will be pessimistic relative to real user experience.

## Realism Enforcement and Synthetic Control

If you use a large proportion of expert-written cases, you must enforce realism by adding paraphrase variants that mimic real user phrasing, including incomplete or short queries, and incorporating messy multi-turn context where prior turns influence the current query. Expert-written cases tend to be well-formed, grammatically correct, and contextually complete. Real users write sentence fragments, make typos, use slang, ask ambiguous questions, and provide incomplete context. If your evaluation dataset doesn't reflect that messiness, you'll overestimate performance on real traffic.

But you never distort policy facts to add realism. If an expert-written case is testing whether the system correctly refuses a prohibited request, you don't paraphrase the refusal boundary into ambiguity just to make the query sound more natural. Realism enforcement applies to phrasing and structure, not to ground truth or policy requirements.

For synthetic cases, you enforce synthetic control by capping near-duplicate patterns, ensuring multiple writing styles, and grounding synthetic generation in real failure modes rather than fantasy edge cases. Synthetic generators—especially language models—can produce highly similar outputs if you sample repeatedly from the same prompt or template. If 40 percent of your synthetic cases are minor variations on the same phrasing pattern, you're not adding coverage, you're adding redundancy. You also ensure that synthetic cases reflect real business constraints and real user intents, not hypothetical scenarios that would never occur in production. Synthetic cases should fill gaps and stress-test boundaries, not introduce noise that dilutes signal in your evaluation metrics.

## The Cost-Quality Tradeoff in Source Selection

Different source types have different costs. Production logs require infrastructure to capture, store, sanitize, and label. Expert-written cases require specialized human time. Synthetic cases require generation compute and quality review. You must balance cost against quality when deciding source mix.

Production logs are expensive to sanitize but cheap to collect at scale once infrastructure is in place. The fixed cost is building the sampling pipeline, the anonymization logic, and the labeling workflow. Once built, marginal cost per example is low. You pay upfront for infrastructure and ongoing costs for storage and labeling, but you get high-fidelity representation of real user behavior. The ROI is strongest when production volume is high and when realism is critical for your use case.

Expert-written cases are expensive per example because they require specialized knowledge and careful review. A domain expert might spend 30 minutes writing and validating a single high-stakes evaluation case. At that rate, building a 500-example expert-written dataset costs 250 hours of expert time. For tier 2 and tier 3 workflows, that cost is justified because expert-written cases test scenarios that production logs rarely capture and that synthetic generation can't reliably produce. For tier 0 and tier 1 workflows, expert-written cases are cost-prohibitive at scale and should be reserved for critical edge cases.

Synthetic cases are cheap to generate at scale but require careful quality control. You can generate 1,000 synthetic cases in minutes using GPT-5 or Claude Opus 4.5. But quality review takes time, and bad synthetic cases corrupt metrics worse than having no cases at all. The cost model for synthetic data is cheap generation plus non-trivial review. You optimize by improving generation quality through better prompts, better seeding, and better filtering, which reduces review burden.

The practical implication is that you adjust source mix based on available budget and risk tolerance. If you have limited budget and low risk, you maximize production logs and synthetic cases, using expert-written cases only for critical edge cases. If you have higher budget and high risk, you invest in expert-written cases for comprehensive policy coverage and use production and synthetic data to supplement. You don't treat source mix as a fixed ratio—you treat it as a budget allocation problem where cost, risk, and coverage determine the optimal mix.

## Failure Modes and Diagnostic Signals

When your evaluation results are consistently too optimistic—metrics look strong but production performance is weak—the root cause is almost always an overreliance on expert-written clean text combined with insufficient production data and insufficient adversarial packs. Your dataset is easier than real traffic, and your metrics are measuring performance on an idealized problem. The fix is to increase the proportion of production logs and long-tail packs, add difficulty mixing as described in Chapter 3, and introduce adversarial safety packs that test refusal boundaries and jailbreak robustness.

When your evaluation results are consistently too pessimistic—metrics look weak but production performance is acceptable—the root cause is typically that synthetic cases are unrealistically hard or unnatural, or the dataset is overweighted with edge cases that rarely occur in production. Your metrics are dominated by scenarios that don't reflect real user distribution. The fix is to rebalance with realistic production distribution, label synthetic difficulty carefully so you can stratify metrics by difficulty level, and ensure that your core dataset reflects typical user behavior, with adversarial cases tracked separately.

When you can't use production logs because of privacy constraints, the root cause is either the absence of an anonymization pipeline or unclear consent and retention rules. Production logs are high-risk data, and treating them carelessly creates legal and reputational exposure. The fix is to build a safe sanitization process that removes personally identifiable information, redacts sensitive entities, and anonymizes user identifiers, and to use expert-written proxies that mimic production patterns until your sanitization pipeline is ready. You never compromise on privacy to get data, but you also don't accept that production data is permanently off-limits without exploring sanitization options.

When expert-written cases don't generalize to production behavior, the root cause is that experts encode assumptions that real users don't share. Experts provide complete context, use precise terminology, and structure queries logically. Real users don't. The fix is to pair expert-written cases with paraphrase variants that inject realistic noise—sentence fragments, typos, ambiguous pronouns, implicit context. You preserve the correctness criteria from the expert-written case while making the phrasing more representative of real user behavior.

When synthetic cases feel artificial or repetitive, the root cause is generation overfitting to prompt templates or insufficient diversity in the generator configuration. The fix is to use multiple models, vary generation prompts, post-process synthetic outputs to inject realistic variation, and cluster synthetic cases to detect and remove near-duplicates. You also seed generation with real production examples so synthetic cases are grounded in real user patterns rather than the model's default style.

## Source Selection Operationally: What You Actually Do

When you build a new evaluation dataset, you make three concrete decisions. First, you determine what proportion of your dataset will come from each source type based on your risk tier and use case. Second, you establish sampling criteria that define how you select examples from each source. Third, you implement quality controls that ensure examples from each source meet minimum standards before entering the dataset.

For production logs, you implement stratified sampling that ensures representation across slices. You don't sample uniformly from all production traffic because that overweights high-volume tasks and underweights rare but important tasks. Instead, you sample proportionally within each task category, each language, each customer tier, and each risk tier. If your taxonomy includes 50 task types and one task represents 40% of traffic, you don't let that task dominate your dataset. You cap its representation at 10 or 15% and allocate the remaining capacity to other tasks. Stratified sampling ensures coverage without letting high-volume tasks drown out everything else.

You also filter production logs before sampling. You remove bot traffic, test queries, duplicate submissions, and interactions that failed before reaching your model. These examples don't represent real user behavior—they represent noise. You filter examples where the user abandoned the session immediately, where the query was empty or malformed, or where the system encountered an infrastructure error. You're sampling production logs to capture real user intent and real system behavior, not to capture everything that hit your servers.

For expert-written cases, you establish clear commissioning criteria. You define what workflows need expert coverage, what policy boundaries need testing, what edge cases matter for your risk tier. You provide experts with templates, examples, and correctness criteria so they understand what "good" looks like. You review expert-written cases before they enter the dataset to ensure they test the intended capability, include necessary context, and avoid encoding expert biases that don't reflect real user behavior. You also version expert-written cases and track who wrote them so you can audit quality and identify systematic issues.

For synthetic cases, you implement generation protocols that control quality and diversity. You use multiple models or multiple prompts to avoid overfitting to a single generator's style. You cluster synthetic outputs and sample diverse representatives to avoid redundancy. You validate that synthetic cases are grounded in real failure modes by seeding generation with examples of past incidents, escalations, and user complaints. You label synthetic cases explicitly so you can track their proportion in your dataset and detect when synthetic data is distorting metrics.

## Balancing Coverage and Realism

The tension in dataset construction is between coverage and realism. Production logs give you realism but limited coverage. Expert-written and synthetic cases give you coverage but limited realism. Your dataset must balance these competing goals without sacrificing either.

You balance by treating production logs as the anchor. Whatever proportion of your dataset comes from production defines the baseline distribution you're trying to match. If production logs show that 60% of queries are tier 0, 30% are tier 1, and 10% are tier 2, that distribution becomes your target. When you add expert-written and synthetic cases, you add them in ways that preserve or deliberately adjust that distribution based on risk.

For high-volume, low-risk tasks, you rely primarily on production logs and supplement with synthetic cases to fill long-tail gaps. You don't commission expert-written cases for every variation of a tier 0 question because the cost doesn't justify the benefit. For low-volume, high-risk tasks, you invert the ratio. You commission expert-written cases to ensure comprehensive coverage of policy boundaries and edge cases, you generate synthetic adversarial cases to stress-test robustness, and you supplement with production logs where available.

You also balance coverage across slices. If your production traffic is 80% English but your product supports ten languages, you don't build an 80% English evaluation dataset. You deliberately oversample or generate synthetic cases in underrepresented languages to ensure you can measure performance across all supported languages. You're balancing production realism with the operational need to measure performance in segments that matter even if they're not yet high-volume.

## Versioning and Locking Datasets

Datasets evolve over time as you add new examples, retire outdated examples, and rebalance source proportions. Without versioning, you can't compare evaluation results across time because you don't know whether metric changes reflect model changes or dataset changes. Versioning is how you maintain reproducibility and enable regression testing.

You version datasets by assigning a version identifier whenever you make changes. A version identifier can be a timestamp, a sequential number, or a semantic version like 2.1. You record what changed—examples added, examples removed, examples modified—and why the change was made. You store multiple versions of the dataset so you can re-run evaluation on older versions if needed. You lock datasets used for regression testing so they remain stable across releases.

Locking means freezing a dataset version and treating it as immutable. Once a dataset is locked, you don't add examples, remove examples, or modify examples. You use the locked dataset to gate releases, compare models, and track performance trends over time. If you need to add new examples or change source proportions, you create a new version and lock that version separately. You maintain both versions so you can compare results on the stable locked dataset and on the updated dataset.

You also version the dependencies that determine evaluation behavior. If your dataset includes RAG examples, you version the retrieval system that generated the retrieved chunks. If your dataset includes agent examples, you version the tool schemas that define valid tool calls. If your dataset includes policy-dependent examples, you version the policy document that defines correct behavior. Versioning dependencies is what makes reproducibility possible. Without it, you can't reconstruct the conditions under which a given evaluation was run.

## The Dataset Construction Workflow

Building a dataset correctly starts from your taxonomy and coverage map, developed in Chapter 3. You know what tasks and intents you need to cover, what slices matter, and what risk tiers apply. You choose the dataset purpose—regression gate for blocking bad releases, broad quality tracking for monitoring trends, or safety red-team for adversarial testing—and that purpose determines your source mix and difficulty distribution. You select sources by combining production samples, expert-written packs, and synthetic gap fillers in proportions appropriate to your risk tier and use case. You tag everything with source metadata, slice labels, and difficulty. You balance the dataset across risk tiers, difficulty levels, and slices so no single segment dominates and distorts your metrics. You add anchor items—gold cases that define correct behavior and incident cases that capture past failures—to ensure your dataset tests the scenarios that have historically mattered. You version the dataset and lock it so that regression comparisons remain valid over time.

## What Serious Teams Actually Do

Enterprise teams in 2026 maintain multiple datasets, not one. They have a regression suite that remains stable over time and serves as the gate for release decisions. They have a monitoring set that is refreshed regularly with production samples to ensure alignment with current traffic patterns. They have a safety suite composed primarily of adversarial and expert-written cases that test refusal boundaries, jailbreak resistance, and policy adherence under stress. They have tenant-specific packs for top enterprise customers whose workflows or policy requirements differ from the general population. Each dataset serves a distinct purpose, and no single dataset is expected to serve all purposes simultaneously.

They treat production logs as regulated data with strict access control, anonymization pipelines, and retention rules. Production data is logged with consent, stored with encryption, accessed only by authorized personnel, and purged according to defined schedules. They don't sample production logs casually or store them indefinitely without review. They also track dataset drift and label drift with versioning, so when metrics shift over time, they can determine whether the shift reflects model changes, dataset changes, or traffic changes.

They implement source governance processes that define who can commission expert-written cases, who can generate synthetic cases, who can sample production logs, and what quality standards apply to each source type. They audit source composition quarterly to ensure proportions remain appropriate as traffic and requirements evolve. They review synthetic generation protocols annually to ensure generation quality keeps pace with model improvements. They treat dataset construction as an ongoing operational process, not a one-time project completed during initial development.

Your evaluation is only as good as the data you measure it against. If your dataset is unrepresentative, noisy, or biased, every metric you compute will be unreliable, and every decision you make based on those metrics will be wrong. Dataset construction is not a one-time activity completed during initial development. It's an ongoing process that evolves as your product changes, as your traffic changes, and as your understanding of what matters deepens. You refresh production samples to track distribution drift. You commission new expert-written cases as policy requirements change. You generate new synthetic cases as threat models evolve. You retire outdated examples that no longer reflect current product behavior or current user needs. The dataset you use for evaluation in month one won't be the dataset you use in month twelve, and that's not a failure of planning—it's a sign that your evaluation program is adapting to reality.

In the next subchapter, we'll examine dataset formats—how to structure examples for chat, RAG, agents, and voice so that evaluation remains reproducible and meaningful as your system evolves.
