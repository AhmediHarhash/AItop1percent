# Chapter 5 — Dataset Design & Engineering

### Plain English

Your evaluation is only as good as your dataset.

**What data do you use to evaluate your AI system, and how do you engineer it to be reliable, balanced, and representative?**

A dataset is not just a pile of examples.

It is an engineered artifact with:
- clear sourcing strategy
- balanced coverage across task types
- controlled difficulty distribution
- proper splits to prevent overfitting
- privacy protection and sanitization
- de-duplication logic
- quality assurance checks
- versioning and lineage tracking

Bad datasets produce misleading evals.

Good datasets are the foundation of trustworthy AI systems.

This chapter teaches you how to source data, format it correctly, balance difficulty, create holdout sets, handle privacy, prevent contamination, version datasets, and maintain them over time.

---

### Why This Chapter Exists

Most eval failures are dataset failures.

Common dataset problems:
- data is too easy (evals pass, production fails)
- data is too hard (evals fail, production works)
- data is biased toward one task type (blind spots in coverage)
- data is contaminated (model saw it during training)
- data leaks across train/test splits (overfitting)
- data contains sensitive information (privacy violations)
- data is outdated (label rot)

This chapter exists to:
- make dataset design systematic, not ad-hoc
- prevent data quality issues before they break evals
- align dataset strategy with eval strategy
- create reproducible, auditable datasets
- prevent contamination and overfitting

In 2026, **dataset engineering is infrastructure work**.

It is not a one-time annotation project.

---

### What Dataset Design Actually Is (2026 Meaning)

Dataset design is **not**:
- collecting random examples
- using benchmark datasets without customization
- a one-time annotation sprint

Dataset design **is**:
- a systematic process for sourcing, formatting, and maintaining eval data
- aligned with your task taxonomy and coverage strategy
- engineered for balance, difficulty, and representativeness
- versioned and auditable like code
- continuously updated as your system evolves

Technically, it includes:
- data sourcing strategy
- format and schema design
- balancing and difficulty control
- split strategy (train, validation, test, holdout)
- privacy and sanitization pipelines
- de-duplication and contamination prevention
- quality assurance processes
- versioning, lineage, and registry systems

---

### Core Components of Dataset Design & Engineering

#### 1. Data Sources

Where does your eval data come from?

Common sources:

**Production logs**:
- Real user queries and interactions
- Pros: highly representative
- Cons: requires privacy scrubbing, may lack ground truth

**Human-authored examples**:
- Domain experts write examples
- Pros: high quality, controlled difficulty
- Cons: expensive, may not match real usage

**Synthetic generation**:
- Use models to generate test cases
- Pros: fast, scalable
- Cons: may lack realism, can inherit model biases

**Public benchmarks**:
- Pre-existing datasets (e.g., MMLU, HumanEval)
- Pros: easy to use, allows comparison
- Cons: may not match your task, risk of contamination

**User research**:
- Collect examples from user interviews
- Pros: captures real user intent
- Cons: small sample size

**Adversarial red-teaming**:
- Security team writes adversarial examples
- Pros: finds edge cases and safety failures
- Cons: not representative of typical usage

Best practice: **use a mix of sources**.

Production logs for representativeness, human-authored for edge cases, synthetic for scale.

---

#### 2. Dataset Formats

Datasets must have a consistent format.

Minimum required fields:
- Unique ID
- Input (user query, context, etc.)
- Expected output or ground truth
- Task category (from your taxonomy)
- Metadata (difficulty, source, language, etc.)

Example format (JSON):

```
{
  "id": "eval-12345",
  "task_category": "rag_multidoc_synthesis",
  "input": {
    "query": "What is the company's revenue growth?",
    "documents": ["doc1.txt", "doc2.txt"]
  },
  "ground_truth": "Revenue grew 15% in Q4 2025",
  "acceptable_variations": ["15% growth", "increased by 15%"],
  "metadata": {
    "difficulty": "medium",
    "source": "production_logs",
    "language": "en",
    "created_at": "2025-12-01"
  }
}
```

Format consistency allows:
- automated evaluation pipelines
- reproducible results
- easy versioning and comparison

Without a schema, datasets become unmaintainable.

---

#### 3. Balancing & Difficulty

Datasets must be balanced across:
- task categories (per your taxonomy)
- difficulty levels (easy, medium, hard)
- user types (novice, expert)
- languages and modalities

**Balancing by task category**:
- Use your coverage map to define target distribution
- Example: 40% FAQ, 30% RAG, 20% agent tasks, 10% edge cases

**Balancing by difficulty**:
- Easy: typical cases with clear answers
- Medium: requires reasoning or multi-step logic
- Hard: ambiguous, adversarial, or rare cases

Recommended distribution:
- 50% easy
- 30% medium
- 20% hard

**Why balance matters**:
- Unbalanced datasets hide failures
- If 90% of examples are easy, a mediocre model scores well
- If 90% of examples are hard, a good model looks bad

Balance is not always proportional to production frequency.

Sometimes you oversample rare but critical cases.

---

#### 4. Splits & Holdouts

Datasets are split to prevent overfitting.

Common splits:

**Development set (dev)**:
- Used during active development
- Can be looked at frequently
- Typically 200-1000 examples

**Validation set (val)**:
- Used for tuning and prompt iteration
- Looked at occasionally
- Typically 500-2000 examples

**Test set (test)**:
- Used for final evaluation before release
- Rarely looked at
- Typically 500-5000 examples

**Holdout set (holdout)**:
- Locked away, only used for critical decisions
- Never used for tuning
- Typically 200-1000 examples

**Why splits matter**:
- If you tune on the same data you evaluate on, you overfit
- Models "memorize" examples they see repeatedly
- Holdout sets are your insurance against overfitting

Best practice:
- Use dev set for daily work
- Use test set for pre-release checks
- Use holdout set for major version releases or audits

Holdout sets are locked with version control and access restrictions.

---

#### 5. Privacy & Sanitization

Production logs often contain sensitive information.

Common sensitive data:
- personally identifiable information (PII): names, emails, phone numbers
- credentials: passwords, API keys
- financial information: credit card numbers, account balances
- health information: medical records, diagnoses
- proprietary data: trade secrets, confidential business info

Sanitization strategies:

**Redaction**:
- Replace sensitive entities with placeholders
- Example: "My email is john@example.com" becomes "My email is EMAIL_PLACEHOLDER"

**Synthetic replacement**:
- Replace sensitive entities with realistic but fake data
- Example: "My email is john@example.com" becomes "My email is alex@test.org"

**Hashing**:
- Hash sensitive values for consistency
- Example: "john@example.com" becomes "USER_7a3f9b2c"

**Manual review**:
- Human reviewers check for sensitive data
- Required for high-risk domains (healthcare, finance)

Sanitization is required by:
- GDPR (Europe)
- CCPA (California)
- HIPAA (healthcare)
- PCI-DSS (payments)
- SOC 2 and enterprise contracts

Skipping sanitization is a legal and reputational risk.

---

#### 6. De-duplication & Near-Duplicate Control

Duplicates skew evaluation results.

If your dataset has 10 copies of the same example, that example is over-represented.

**Exact duplicate detection**:
- Hash inputs and check for collisions
- Remove exact duplicates

**Near-duplicate detection**:
- Use embedding similarity
- Remove examples with cosine similarity above a threshold (e.g., 0.95)

**Why de-duplication matters**:
- Prevents train/test leakage
- Prevents over-representation of specific patterns
- Improves dataset diversity

De-duplication is automated and run on every dataset version.

---

#### 7. Dataset QA

Datasets require quality assurance.

Common QA checks:

**Schema validation**:
- All required fields are present
- Field types are correct

**Ground truth validation**:
- Ground truth is not empty
- Ground truth matches expected format

**Coverage validation**:
- All task categories are represented
- Difficulty distribution matches target

**Duplicate detection**:
- No exact duplicates
- No near-duplicates

**Privacy validation**:
- No PII detected
- Sanitization rules applied correctly

**Inter-annotator agreement**:
- Multiple annotators label a sample
- Measure agreement (Cohen's kappa)
- Flag examples with low agreement

QA is automated and runs on every dataset change.

Failed QA checks block dataset releases.

---

#### 8. Versioning, Lineage, and Registry

Datasets are versioned like code.

**Versioning**:
- Each dataset has a version number (e.g., v1.2.3)
- Changes are tracked in a changelog
- Old versions are archived, not deleted

**Lineage tracking**:
- Record where each example came from
- Track transformations (sanitization, synthetic generation)
- Link to source data (production logs, annotation batches)

**Dataset registry**:
- Centralized catalog of all datasets
- Metadata: version, size, task coverage, creation date
- Access controls: who can read, who can modify

Example registry entry:

```
Dataset: customer_support_eval
Version: 2.1.0
Size: 3500 examples
Task categories: FAQ (40%), RAG (30%), escalation (20%), edge cases (10%)
Created: 2025-11-15
Source: production logs (60%), human-authored (25%), synthetic (15%)
Privacy: sanitized, reviewed
Last validated: 2025-12-01
```

Without versioning, you cannot reproduce eval results.

Without a registry, teams cannot find or reuse datasets.

---

#### 9. Synthetic Data Generation

Synthetic data scales dataset creation.

Common synthetic generation techniques:

**Paraphrasing**:
- Take existing examples and rephrase them
- Use models to generate variations

**Templating**:
- Define templates with variables
- Generate examples by filling in variables
- Example: "What is the capital of COUNTRY?" → generate for all countries

**Model-generated**:
- Use a strong model to generate input-output pairs
- Human reviewers validate and correct
- Faster than pure human authoring

**Adversarial generation**:
- Use models to generate challenging or adversarial examples
- Red-teaming and jailbreak attempts

**Data augmentation**:
- Add noise, typos, or formatting variations
- Test robustness to input quality

Synthetic data is **not a replacement** for real data.

It is a **complement** that scales coverage.

Best practice: **always validate synthetic data with human review**.

---

#### 10. Multilingual & Cross-Cultural Datasets

AI systems increasingly serve global users.

Multilingual dataset challenges:
- Different languages have different eval requirements
- Direct translation often loses nuance
- Cultural context matters (e.g., date formats, politeness norms)

Multilingual dataset strategies:

**Native annotation**:
- Native speakers write examples in their language
- More authentic than translation

**Translation with review**:
- Translate existing examples
- Native speakers review and correct

**Cross-cultural validation**:
- Test examples across cultures
- Ensure examples are appropriate and unbiased

**Language-specific ground truth**:
- Ground truth may differ by language
- Example: "What is soccer?" → "football" in most languages, "soccer" in US English

Multilingual datasets are **not optional** for global products.

---

#### 11. Dataset Evolution & Maintenance

Datasets are living artifacts.

They must evolve as:
- your product changes
- user behavior shifts
- new failure modes emerge

**Maintenance cadence**:
- Monthly: add new examples from production failures
- Quarterly: review coverage and balance
- Annually: re-annotate samples to check for label rot

**Triggers for updates**:
- New features launch
- Eval results no longer match production performance
- User complaints reveal eval blind spots

**Deprecation policy**:
- Mark outdated examples as deprecated
- Archive instead of deleting
- Document why examples were deprecated

Datasets that are not maintained become misleading.

---

### Enterprise Perspective

Enterprises need:
- centralized dataset registries
- strict privacy and sanitization controls
- version control and audit trails
- consistent formats across teams

Dataset engineering allows:
- reproducible evaluation
- compliance with privacy regulations
- consistent quality standards
- reuse of datasets across projects

In regulated industries, dataset lineage is audited.

It is treated as critical infrastructure.

---

### Founder / Startup Perspective

Startups need:
- fast dataset creation without sacrificing quality
- clear sourcing strategies
- balance between human-authored and synthetic data
- processes that scale as the team grows

Good dataset practices:
- clarify what you are evaluating
- reduce time spent on re-annotation
- allow you to delegate dataset work
- create institutional knowledge even with turnover

Early-stage teams that skip dataset engineering spend weeks debugging misleading eval results.

---

### Common Failure Modes

- No systematic sourcing strategy
- Using only one data source (e.g., only production logs)
- No format schema or consistency
- Unbalanced datasets (90% easy examples)
- No holdout sets (overfitting)
- No privacy sanitization (legal risk)
- No de-duplication (contamination)
- No dataset QA (garbage in, garbage out)
- No versioning (cannot reproduce results)
- Datasets are created once and never updated (label rot)
- No cross-cultural or multilingual validation

Recognizing these failures is the first step to building reliable datasets.

---
