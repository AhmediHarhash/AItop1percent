# Chapter 8.8 — Human-in-the-Loop Agent Evaluation

A VP of Engineering once told me: "We built the most autonomous agent in our space. It can search the web, write code, deploy infrastructure, send emails — all without asking permission. We gave it to our first customer. Within an hour, it had sent 147 emails to their entire engineering org, apologizing for an outage that never happened. The agent thought a test alert was real. It escalated. Hard."

The company rolled back the deployment. They added what they called "safety rails" — a human had to approve any action that touched external systems. The agent could still plan, search, and reason autonomously. But before it sent an email, deployed code, or modified infrastructure, it stopped and asked: "Should I do this?"

The irony: the agent became more valuable, not less. Customers trusted it more because they could intervene before mistakes became disasters. The company learned a counterintuitive truth that now defines agent evaluation in 2026.

The more autonomous your agent, the more critical human oversight becomes.

---

## The Autonomy Paradox

You'd think it would be the opposite. You'd think: "If my agent is smart enough to handle complex tasks autonomously, why do I need humans watching it?"

Here's why.

When a chatbot generates a bad response, the user reads it, recognizes it's wrong, and ignores it. The blast radius is one conversation. The failure mode is obvious and immediate.

When an agent generates a bad action plan, it doesn't just return text. It executes. It calls APIs. It moves money. It sends messages. It modifies databases. The blast radius can be your entire customer base, your production infrastructure, or your company's reputation. The failure mode might not be obvious until it's too late to stop.

This is why **human-in-the-loop evaluation** matters more for agents than for any other AI system. You're not just evaluating whether the agent produced good outputs. You're evaluating whether humans can effectively oversee, intervene in, and collaborate with an autonomous system that has real-world consequences.

In 2026, 60% of organizations restrict agent access to sensitive tools without human oversight. This isn't because the agents aren't capable. It's because the stakes are too high to trust automation alone, no matter how good it gets.

Your job is to evaluate both the agent and the human-agent partnership. Does the agent know when to stop and ask? Do humans get the context they need to approve or reject? Is the handoff smooth enough that humans trust it? These questions define whether your agent succeeds in production.

---

## The 60% Statistic: Why Organizations Gate Agent Actions

By 2026, most enterprise agents don't run fully autonomously in production. They run in **supervised autonomy** mode — the agent plans and proposes, humans approve or reject before execution.

Here's what the data shows:

- 60% of organizations require human approval before agents can access customer data, financial systems, or external communication tools
- 42% require approval for any action that modifies production state
- 28% require approval for every single agent action, treating agents as recommendation engines rather than autonomous executors

Why such conservative adoption?

Three reasons.

**1. Mistakes compound**

A chatbot that hallucinates can be corrected in the next message. An agent that executes a hallucinated plan can cause cascading failures. One wrong API call can trigger alerts, page engineers, confuse customers, or violate compliance policies.

Example: an agent helping with customer support sees a ticket about a billing issue. It decides the right action is to refund the customer $500. It calls the refund API. The customer's issue was actually about a $50 charge, not $500. The agent misread the context. You just lost $450 and created a new support ticket explaining the error.

Human approval would have caught this in three seconds: "Wait, this refund amount seems wrong."

**2. Explainability gaps**

Agents operate through long chains of reasoning: retrieve context, plan actions, execute tools, observe results, replan. When something goes wrong, it's often hard to trace why the agent made a particular decision.

Humans in the loop act as **checkpoints**. Before the agent commits to an action, a human reviews the plan and asks: "Does this make sense? Did the agent misunderstand something?" This forces the agent to surface its reasoning at critical moments, making failures easier to diagnose and prevent.

**3. Trust calibration**

Users don't trust systems they can't stop. If an agent runs fully autonomously and makes a mistake, the user feels powerless. If the agent asks for approval before sensitive actions, the user feels in control — even if they approve 95% of the time.

This is psychological as much as technical. **Supervised autonomy** isn't just about catching mistakes. It's about building user trust by demonstrating that the agent respects human judgment.

In production, this means: even if your agent is accurate 98% of the time, you still need human oversight for the 2% where mistakes are catastrophic.

---

## Approval Gates: Evaluating When the Agent Stops and Waits

An **approval gate** is a point in the agent's workflow where execution pauses until a human grants permission to proceed.

Example gates:
- Before sending an email to a customer
- Before deploying code to production
- Before executing a financial transaction
- Before modifying a shared database
- Before accessing sensitive user data

Your job in evaluation is to test: does the agent correctly identify when it needs approval, and does it stop and wait in the right way?

### What "Correct Stopping" Looks Like

The agent should stop when:
- The action is high-risk (external communication, state modification, financial impact)
- The agent's confidence is low (multiple valid plans, ambiguous intent, missing information)
- The context suggests the user might want control (first-time action, unusual request, sensitive topic)

The agent should NOT stop when:
- The action is low-risk (internal search, reading data, computing a result)
- The agent's confidence is high (clear intent, deterministic outcome, well-understood domain)
- Stopping would break user flow (mid-calculation, mid-retrieval, routine operation)

The failure modes:
- **Over-gating**: the agent asks for approval on every trivial action, making it unusable
- **Under-gating**: the agent executes high-risk actions without approval, causing disasters

Evaluating this requires testing the agent across a spectrum of scenarios:

**Low-risk actions** (should NOT trigger approval):
- Searching internal documentation
- Retrieving user's own data
- Computing a summary or answer

**Medium-risk actions** (should trigger approval IF confidence is low):
- Generating a draft email (approval needed before sending)
- Proposing a code change (approval needed before deploying)
- Suggesting a database query (approval needed before executing)

**High-risk actions** (should ALWAYS trigger approval):
- Sending external messages
- Modifying production systems
- Accessing other users' data
- Making financial transactions

You build an eval set covering all three categories. For each scenario, you test: did the agent stop when it should have? Did it proceed when it shouldn't have stopped? Did it explain why it needed approval?

### The Approval Prompt Problem

When the agent hits a gate, what does it show the human?

A bad approval prompt looks like this:
"Approve action: execute_sql(query='UPDATE users SET balance = 0 WHERE id = 12345')"

This is technically accurate but gives the human no context. Why is the agent setting this user's balance to zero? What was the original request? Is this expected?

A good approval prompt looks like this:
"The user requested: 'Cancel my subscription and refund my remaining balance.'
I found the user has $45.00 remaining. I plan to:
1. Set subscription status to 'canceled'
2. Issue a $45.00 refund
3. Update balance to $0.00

Approve this refund?"

This explains the intent, shows the reasoning, and makes the action reviewable.

Your eval should test: do approval prompts contain enough context for a human to make an informed decision? Can a human who wasn't involved in the original conversation understand what the agent is about to do and why?

Measure this with human raters: show them 100 approval prompts (without the preceding conversation) and ask: "Can you tell what the agent is trying to do and whether it's appropriate?" If raters can't answer with 80% confidence, your prompts need more context.

---

## Escalation Quality: Testing How the Agent Hands Off to Humans

Sometimes the agent doesn't just need approval. It needs to **escalate** — hand the entire task off to a human because it can't proceed.

Escalation happens when:
- The agent doesn't know how to solve the problem (novel request, missing capabilities, ambiguous intent)
- The agent hit an error it can't recover from (API failure, permission denied, unexpected response)
- The user explicitly asked for a human ("Can I talk to someone?")

Evaluating escalation quality means testing: when the agent escalates, does the human who receives the handoff have what they need to help the user?

### The Escalation Context Test

Good escalation provides:
- What the user originally asked for
- What the agent tried to do
- What went wrong or why the agent can't proceed
- What information the agent gathered (context, search results, partial answers)
- What the agent recommends the human do next

Bad escalation looks like this:
"I couldn't complete this task. Escalating to human support."

The human support agent now has to start from scratch: re-read the conversation, re-ask the user for context, re-attempt the same failed actions.

Good escalation looks like this:
"The user asked me to update their shipping address for order #78234. I found the order, but it's already shipped (status: 'in transit'). I can't modify the address at this stage. I'm escalating to a support agent who can contact the carrier or arrange a re-delivery. User's preferred new address: 123 Main St, Austin TX."

The human support agent can immediately pick up where the agent left off.

### How to Evaluate This

Build an eval set of 50-100 scenarios where escalation is appropriate:
- Requests outside the agent's scope (policy questions only a human can answer)
- Error states the agent can't recover from (payment gateway timeout, missing permissions)
- Ambiguous or contradictory user requests (user asks for two incompatible actions)

For each scenario, run the agent and force an escalation. Then show the escalation message to a human reviewer (without the conversation history) and ask:
- Can you understand what the user wanted?
- Can you understand what the agent tried?
- Can you understand why it failed?
- Do you have enough information to help the user, or do you need to ask the user to repeat themselves?

Score escalation quality on a scale:
- **Excellent** (5/5): human can immediately help the user, no re-questioning needed
- **Good** (4/5): human has most context, might need to clarify one detail
- **Adequate** (3/5): human understands the situation but needs to re-gather some information
- **Poor** (2/5): human has to start over, agent provided minimal context
- **Failure** (1/5): escalation message is confusing or misleading, makes the human's job harder

Target: 80% of escalations should score 4 or 5. If you're below 60%, your agent isn't providing enough context.

---

## Human Evaluation of Trajectories: Reviewing Full Action Traces

With chatbots, you evaluate outputs — single pieces of text. With agents, you evaluate **trajectories** — sequences of actions, observations, and decisions.

A trajectory might look like this:
1. Agent receives user request: "Find me the cheapest flight to Boston next week."
2. Agent calls search_flights(destination='Boston', dates='next week') → returns 15 results
3. Agent filters results by price, selects top 3
4. Agent calls get_flight_details(flight_id=...) for each of the 3 flights
5. Agent compares details, picks the cheapest: $287 on Tuesday
6. Agent returns: "The cheapest flight to Boston next week is $287 on Tuesday, departing at 9am."

To evaluate this, you don't just look at the final output. You review the entire sequence: did the agent search correctly? Did it filter appropriately? Did it call the right APIs? Did it compare accurately?

### What Humans Look For in Trajectory Review

**1. Plan quality**

Did the agent choose a reasonable sequence of actions? Was there a more efficient path? Did it make unnecessary API calls?

Example: the agent could have called search_flights with a "sort by price" parameter instead of retrieving all results and filtering manually. A human reviewer marks this as "inefficient but correct."

**2. Error recovery**

If an API call failed, did the agent retry? Did it try an alternative approach? Or did it give up immediately?

Example: the first search_flights call timed out. The agent retried with a narrower date range. This is good error recovery.

**3. Context use**

Did the agent use available context correctly? Did it ignore relevant information? Did it hallucinate facts?

Example: the user said "next week," which means April 8-14. The agent searched for April 15-21. It misunderstood relative dates. A human reviewer marks this as "incorrect context parsing."

**4. Tool correctness**

Did the agent call APIs with valid parameters? Did it handle responses correctly? Did it misinterpret API output?

Example: the API returned prices in cents (28700 = $287.00). The agent displayed it as $28,700. Tool output parsing error.

**5. Stopping conditions**

Did the agent know when to stop? Or did it keep searching/refining unnecessarily? Or did it stop too early and return incomplete results?

Example: the agent found 3 flights, compared them, and returned the cheapest. This is appropriate. If it had kept searching through all 15 results one by one, that would be inefficient. If it had returned only the first result without comparing, that would be premature.

### How to Structure Human Trajectory Review

You can't have humans review every agent interaction in production. That's too expensive. Instead, you sample strategically:

**Random sampling** (5% of all trajectories): gives you a baseline view of agent performance across all interactions.

**Edge case sampling** (100% of long trajectories): if an agent takes greater than 10 steps to complete a task, review it. Long trajectories often indicate confusion or error recovery failures.

**Failure sampling** (100% of trajectories ending in escalation or error): understand why the agent couldn't complete the task.

**High-stakes sampling** (100% of trajectories involving sensitive actions): before the agent sends an email, modifies data, or makes a financial transaction, a human reviews the plan.

For each trajectory, the reviewer fills out a structured form:
- Overall success: Did the agent complete the task correctly? (Yes / Partial / No)
- Plan quality: Was the action sequence appropriate? (Efficient / Acceptable / Inefficient)
- Tool use: Did the agent call APIs correctly? (Correct / Minor errors / Major errors)
- Context handling: Did the agent use available information appropriately? (Good / Adequate / Poor)
- Error recovery: If errors occurred, did the agent recover well? (Good / Adequate / Failed)
- Recommendation: (Approve / Approve with feedback / Reject / Escalate for review)

This structured review generates data you can aggregate: "87% of trajectories are rated 'efficient,' 62% have no tool errors, 91% handle context well."

---

## Combining Automated and Human Agent Eval

You can't review every trajectory manually. You need **hybrid evaluation** (as discussed in Chapter 6.8): automated checks catch the obvious issues, humans review the ambiguous and high-stakes cases.

### Automated Checks for Agents

**Tool correctness**: did the agent call APIs with valid parameters? Did it handle responses according to schema?

This is deterministic. You log every tool call, validate parameters against API specs, check that responses match expected formats. No human needed — this is schema validation and type checking.

**Efficiency**: did the agent complete the task in a reasonable number of steps?

Set thresholds: tasks of type X should complete in fewer than 8 steps. If an agent takes 15 steps, flag it for human review. The agent might be stuck in a loop, exploring irrelevant paths, or recovering from errors.

**Grounding**: did the agent's final answer reference information from the tools it called?

Use LLM-as-a-judge (Chapter 7): "Based on the tool outputs, is the agent's final answer grounded in the retrieved information?" This catches hallucination without requiring humans to read every tool response.

**Safety**: did the agent attempt any actions that violate policy?

Deterministic checks: did the agent try to access data it doesn't have permissions for? Did it call sensitive APIs without approval? Flag these immediately.

### Human Review for Agents

**Plan quality**: is the action sequence reasonable and efficient?

This requires judgment. Automated systems can detect obviously inefficient plans (10+ redundant API calls) but can't judge whether a 5-step plan is better than a 3-step plan. Humans review a sample of plans and rate them.

**Edge cases**: did the agent handle unusual requests correctly?

Automated evals work well on common tasks. On rare or novel requests, humans need to judge whether the agent adapted appropriately.

**Escalation appropriateness**: when the agent escalated, was that the right call?

Automated systems can detect that escalation occurred. Humans judge whether it was necessary (agent couldn't proceed) or unnecessary (agent gave up too early).

**Approval decisions**: when the agent asked for approval, should it have been granted?

Humans make the actual approve/reject decision in production. In eval, you test: how often do humans approve? If humans reject 40% of approval requests, your agent's judgment is poorly calibrated.

### The 2026 Hybrid Pattern

In production, the pattern looks like this:

- **100% automated monitoring** of tool correctness, safety violations, schema compliance
- **10% human review** of trajectories, stratified by risk and length (random sample + high-stakes + long trajectories)
- **100% human approval** for high-risk actions (external communication, state modification, financial transactions)
- **100% human review** of escalations (did the agent hand off appropriately?)

This gives you scale (automated checks catch most issues) and quality (humans catch judgment failures).

The feedback loop: when humans reject an approval request or rate a trajectory poorly, log the case. Use it to improve the agent's planning, tool use, or escalation logic. This is active learning for agents (Chapter 7.3): human corrections teach the agent to make better decisions.

---

## Human Override Evaluation: When Humans Reject the Agent's Decision

In a supervised autonomy model, humans can override the agent's proposed actions. The agent says "Send this email." The human says "No, don't send that."

**Override evaluation** tracks: how often do humans override, and were they right to do so?

### The Two Types of Overrides

**1. Necessary overrides** (agent was wrong)

The agent planned to send an email to the wrong person, or with incorrect information, or at the wrong time. The human caught the mistake and stopped it. This is the system working as intended.

Example: agent drafts an email saying "Your refund of $500 has been processed." The human notices the refund amount should be $50. They override, correct the email, and send the updated version.

**2. Unnecessary overrides** (agent was right, human was cautious or wrong)

The agent's plan was correct, but the human rejected it anyway — either because they didn't understand the context, or they were risk-averse, or they preferred a different approach.

Example: agent drafts an email saying "Your refund of $50 has been processed." The human thinks "That seems low, let me double-check." They review the user's account, confirm $50 is correct, and approve. The override added no value; it was just caution.

### Why This Distinction Matters

If 100% of overrides are necessary, your agent's judgment is miscalibrated — it's proposing bad actions too often. You need to improve the agent's planning or confidence thresholds.

If 40% of overrides are unnecessary, your human reviewers are overly cautious — they're rejecting good actions, slowing down workflows, and reducing trust in the agent. You need to improve the approval prompts (give humans more context) or train reviewers to trust the agent on routine cases.

### How to Measure This

For every human override, log:
- What the agent proposed
- Why the human rejected it
- What the human did instead

Then, have a second reviewer (or an automated judge) evaluate:
- Was the agent's original proposal correct? (Yes / No / Ambiguous)
- Was the human's override necessary? (Yes / No / Debatable)

Aggregate the data:
- **Agent error rate**: percentage of necessary overrides out of all approval requests
- **Human caution rate**: percentage of unnecessary overrides out of all overrides
- **Override rate**: percentage of approval requests that humans reject

Example results:
- Agent proposed 1,000 actions
- Humans rejected 150 (15% override rate)
- Of those 150, 100 were necessary (agent was wrong), 50 were unnecessary (human was cautious)
- Agent error rate: 100 / 1,000 = 10%
- Human caution rate: 50 / 150 = 33%

Targets vary by risk, but a reasonable benchmark:
- Agent error rate less than 5% (95% of proposals are correct)
- Override rate between 5-15% (humans intervene when needed but don't block everything)
- Human caution rate less than 30% (most overrides are justified)

If agent error rate is above 10%, improve agent planning. If human caution rate is above 40%, improve approval context and reviewer training.

---

## Trust Calibration: Applying the Right Level of Oversight

Too much human oversight makes the agent slow and expensive. Too little makes it risky. **Trust calibration** is about finding the right balance.

### The Oversight Spectrum

**Level 0: No oversight** (full autonomy)

The agent executes all actions without approval. Fast, cheap, high-risk. Only appropriate for low-stakes tasks in controlled environments (internal tools, sandboxed testing).

**Level 1: Approval for high-risk actions** (supervised autonomy)

The agent executes routine actions autonomously but asks for approval before sensitive actions (external communication, state modification, financial transactions). Balances speed and safety. This is the 2026 standard for production agents.

**Level 2: Approval for all actions** (recommendation mode)

The agent proposes every action, humans approve each one. Slow, expensive, very safe. Appropriate for high-stakes domains (healthcare, legal, finance) or early-stage agents that aren't yet trusted.

**Level 3: Human executes, agent advises** (decision support)

The agent doesn't execute anything. It provides recommendations, humans make decisions and execute manually. Not really an "agent" at this point — it's a chatbot with tool access.

### How to Choose the Right Level

The right oversight level depends on:

**Agent accuracy**: if your agent makes mistakes on 15% of tasks, you need Level 2 (approval for everything). If it makes mistakes on 2% of tasks, Level 1 (approval for high-risk only) is sufficient.

**Task risk**: if a mistake costs $10 and annoys one user, Level 1 is fine. If a mistake costs $10,000 or exposes sensitive data, you need Level 2 or even Level 3.

**User trust**: if users are new to agents and skeptical, start with Level 2 (high oversight), then graduate to Level 1 as trust builds.

**Regulatory requirements**: some industries (healthcare, finance) mandate human oversight on certain actions regardless of agent accuracy.

### Evaluating Whether Your Oversight Level Is Right

You want the minimum oversight that keeps risk acceptable. Measure:

**Incident rate**: how often does the agent cause a problem that requires human intervention to fix?

If incident rate is greater than 1% of tasks, you need more oversight (move from Level 1 to Level 2). If incident rate is less than 0.1%, you might be over-supervising (consider moving from Level 2 to Level 1).

**Approval friction**: how often do humans approve without changes?

If humans approve 98% of agent proposals immediately, your oversight is probably unnecessary (the agent is already getting it right). If humans approve only 60%, your agent isn't ready for autonomy (keep oversight high and improve the agent).

**Task latency**: how much does human approval slow down task completion?

If the agent completes tasks in 10 seconds autonomously but 2 hours with human approval (because humans are busy), you're paying a 720x latency penalty. Evaluate whether that penalty is justified by the risk reduction.

The 2026 best practice: **adaptive oversight** based on agent confidence. If the agent is highly confident in its plan (based on past success on similar tasks), require less oversight. If the agent is uncertain or attempting a novel action, require more oversight. This dynamically adjusts the oversight level to match the risk.

---

## Evaluation of the Human-Agent Team: The Combination Matters

You can have a mediocre agent with excellent human oversight that outperforms a strong agent with poor oversight.

Example: Agent A is 95% accurate but provides poor context in approval prompts. Humans approve blindly because they don't understand what the agent is doing. Effective accuracy: 95%.

Agent B is 85% accurate but provides excellent context in approval prompts. Humans catch most of the 15% errors and fix them before execution. Effective accuracy: 97%.

Agent B is less capable but performs better in production because the **human-agent team** is well-designed.

### What Makes a Good Human-Agent Team

**1. Clear division of labor**

The agent handles routine, high-volume, deterministic tasks. Humans handle edge cases, ambiguous situations, and high-stakes decisions. Neither is trying to do the other's job.

**2. Effective communication**

The agent explains its reasoning clearly. Humans understand what the agent is trying to do and why. Approval prompts, escalation messages, and error reports are concise and informative.

**3. Trust calibration**

Humans trust the agent on tasks it handles well and intervene on tasks it struggles with. The agent escalates when uncertain rather than guessing. Both sides know their limits.

**4. Feedback loops**

Human corrections improve the agent over time. When humans override a decision, the agent learns from that override. When humans approve a decision, the agent gains confidence on similar tasks.

### How to Evaluate Team Performance

Test the human-agent team on a suite of 100 realistic tasks. Measure:

**Task success rate**: what percentage of tasks are completed correctly end-to-end?

Compare this to agent-alone success rate (no human oversight) and human-alone success rate (no agent assistance). The team should outperform both.

**Task efficiency**: how long does the team take to complete tasks, compared to agent-alone or human-alone?

The team should be faster than human-alone (agent handles routine steps) but might be slower than agent-alone (human approval adds latency).

**Error detection rate**: what percentage of agent errors do humans catch before execution?

This measures oversight quality. Target: humans catch greater than 80% of errors flagged for approval.

**False rejection rate**: what percentage of correct agent decisions do humans incorrectly reject?

This measures over-caution. Target: less than 10% of approvals are unnecessary rejections.

**User satisfaction**: do users prefer working with the human-agent team vs. agent-alone or human-alone?

Collect feedback: "Did you feel in control? Did the agent help or slow you down? Would you use this again?"

The best human-agent teams score high on success rate and user satisfaction while keeping efficiency reasonable (not 10x slower than agent-alone).

---

## Building Human Review Interfaces for Agent Traces

If you're asking humans to review agent trajectories, the interface matters. A bad interface makes review slow, frustrating, and error-prone.

### What to Show Reviewers

**1. The user's original request**

Displayed prominently at the top. Reviewers need to know what the agent is trying to accomplish.

**2. The agent's plan**

A high-level summary of the action sequence: "I will search for flights, filter by price, and return the top 3 results."

**3. The action trace**

A step-by-step log of what the agent did:
- Step 1: Called search_flights(destination='Boston', dates='next week') → 15 results
- Step 2: Filtered by price → top 3 results
- Step 3: Called get_details for each → retrieved details
- Step 4: Compared prices → selected cheapest

Each step shows the tool called, the parameters, and the result (or error).

**4. The agent's final output**

What the agent plans to return to the user or what action it plans to execute.

**5. Confidence indicators**

If the agent has low confidence or flagged an ambiguity, show that: "I'm uncertain whether 'next week' means April 8-14 or April 15-21. I assumed April 8-14."

**6. Contextual information**

Any relevant user data, past interactions, or system state that informed the agent's decisions.

### How to Make Traces Reviewable

**Collapse long tool outputs**: if an API returned 100 results, show a summary ("Returned 100 results, showing top 5") with an option to expand.

**Highlight errors and warnings**: if a tool call failed or returned an unexpected result, mark it visually (red text, warning icon).

**Provide quick actions**: "Approve," "Reject," "Edit and approve," "Escalate to senior reviewer." Let reviewers act without writing essays.

**Show reasoning**: if the agent used chain-of-thought planning (Chapter 8.4), show the reasoning trace: "I chose to search by price because the user said 'cheapest.'"

**Contextual help**: if the reviewer hovers over a tool call, show a tooltip explaining what that tool does and what the parameters mean.

### Example Review Interface

```
User Request: "Find me the cheapest flight to Boston next week."

Agent Plan: Search flights, filter by price, return the cheapest option.

--- Action Trace ---

Step 1: search_flights(destination='Boston', date_range='2026-04-08 to 2026-04-14')
Result: 15 flights found (show details)

Step 2: sort_by_price()
Result: Top 3 flights: $287, $312, $340

Step 3: get_flight_details(flight_id='FL287')
Result: Departs 9am Tuesday, arrives 12pm, nonstop (show full details)

Step 4: return_to_user()
Output: "The cheapest flight is $287 on Tuesday at 9am."

--- Review ---

Agent Confidence: High (similar requests completed successfully 42 times)
Errors: None
Warnings: None

[Approve] [Reject] [Edit] [Escalate]
```

This interface gives the reviewer everything they need in a scannable format. They can approve in 10 seconds or dig into details if something looks wrong.

---

## 2026 Patterns: Confidence-Based Gating, Adaptive Thresholds, Human-in-the-Loop Platforms

By 2026, human-in-the-loop evaluation has matured into sophisticated platforms and patterns.

### Confidence-Based Approval Gating

Instead of gating every action of a certain type, gate based on the agent's confidence.

The agent estimates: "I'm 95% confident this email is correct." If confidence is greater than 90%, send without approval. If confidence is between 70-90%, request approval. If confidence is less than 70%, escalate to a human to draft the email manually.

This **adaptive gating** reduces approval overhead on routine tasks while maintaining safety on uncertain ones.

Implementation: the agent tracks success rate on past similar tasks. If it's completed 50 similar requests with 98% success rate, confidence is high. If it's only completed 3 similar requests, confidence is low.

### Adaptive Approval Thresholds

The confidence threshold isn't static. It adapts based on context.

During business hours when human reviewers are available, lower the threshold (require approval at 80% confidence). During nights and weekends when reviewers are offline, raise the threshold (only auto-approve at 95% confidence).

During the first week of a new agent deployment, use a conservative threshold (90%). After a month with no incidents, relax it to 80%.

For high-stakes users (enterprise customers, VIPs), use a stricter threshold. For low-stakes users (free tier, internal testing), use a looser threshold.

This **risk-aware gating** balances speed and safety dynamically.

### Human-in-the-Loop Platforms

Several platforms have emerged to make human oversight scalable:

**Agent review dashboards**: centralized interfaces where reviewers see pending approval requests, prioritized by risk and urgency.

**Escalation routing systems**: automatically route agent escalations to the right human based on task type, urgency, and reviewer expertise (technical issues to engineers, policy questions to support leads).

**Feedback collection tools**: when humans override an agent decision, they explain why ("agent used wrong tone," "agent miscalculated price"). This feeds back into agent training.

**A/B testing frameworks for oversight**: test different approval thresholds, different prompt formats, different review interfaces, and measure which combinations produce the best team performance.

**Audit trails**: log every approval request, every override, every escalation, with full context (user request, agent plan, human decision, outcome). This supports compliance, debugging, and continuous improvement.

The 2026 playbook: don't build human-in-the-loop oversight from scratch. Use a platform that handles routing, logging, and feedback collection, and focus your engineering effort on agent improvement.

---

## Failure Modes and How to Detect Them

Human-in-the-loop systems fail in predictable ways.

### Failure Mode 1: Approval Fatigue

Humans see 200 approval requests per day. 95% are routine and correct. They start rubber-stamping approvals without reading them. They miss the 5% that are actually wrong.

**Detection**: track approval latency. If humans are approving in less than 5 seconds consistently, they're not reading the requests.

**Fix**: reduce approval volume by raising confidence thresholds, or add "attention check" cases (inject obvious errors and verify humans catch them).

### Failure Mode 2: Context Overload

The approval prompt contains 10 paragraphs of background, 5 API responses, and 3 alternative plans. Humans don't read it because it's too much. They guess.

**Detection**: track rejection rate. If rejection rate is less than 5% on a task where agent accuracy is 85%, humans aren't reviewing carefully.

**Fix**: simplify approval prompts. Show only the essential context: what the agent is about to do, why, and what could go wrong.

### Failure Mode 3: Trust Miscalibration

Humans don't trust the agent even when it's correct. They reject 40% of proposals, most of which were actually right. The agent becomes unusable because every action is blocked.

**Detection**: track unnecessary override rate (measure using second reviewers or automated analysis).

**Fix**: improve approval prompts to show the agent's reasoning and past success rate. Train reviewers on when to trust the agent.

### Failure Mode 4: Escalation Loops

The agent escalates to a human. The human doesn't have enough context. They send it back to the agent for more information. The agent still can't provide the context. It escalates again. The task is stuck in a loop.

**Detection**: track escalation re-escalation rate (how often does an escalated task get escalated again?).

**Fix**: improve escalation context (Chapter 8.5) so humans have what they need to resolve the issue on the first attempt.

### Failure Mode 5: Delayed Oversight

The agent executes 1,000 actions per day. Humans review them... 3 days later. By the time they notice an error, the damage is done (emails sent, money moved, data modified).

**Detection**: track time-to-review for different action types.

**Fix**: require approval before execution for high-risk actions, not after. Use asynchronous review only for low-risk actions where mistakes are easily reversible.

---

## Enterprise Expectations: What Leadership Needs to See

When you present human-in-the-loop evaluation to executives, they'll ask:

**"Why do we need humans if the agent is autonomous?"**

Answer: "Autonomy and oversight aren't opposites. The agent handles 90% of tasks autonomously. Humans oversee the 10% that are high-risk, ambiguous, or novel. This prevents costly mistakes while keeping the system fast and scalable. 60% of companies in our space use this model for production agents."

**"How much does human oversight cost?"**

Answer: "Currently, we route 12% of agent actions to human approval. Each approval takes 30 seconds on average. At 10,000 actions per day, that's 1,200 approvals = 10 hours of human time = $300/day. This is 8% of the cost of having humans do all the work manually. As the agent improves, this percentage will decrease."

**"What happens if humans aren't available to approve?"**

Answer: "We have tiered routing. High-risk actions require synchronous approval (agent waits for human response). Medium-risk actions use asynchronous approval (agent queues the request, executes after approval within 1 hour). Low-risk actions proceed autonomously and are audited later. If the approval queue exceeds capacity, we raise the confidence threshold temporarily to reduce volume."

**"How do we know humans are catching errors?"**

Answer: "We inject 'attention check' cases into the approval queue — obvious errors that humans should catch. If reviewers miss these checks, we flag them for retraining. We also track necessary vs. unnecessary overrides to measure whether humans are adding value."

**"Can we reduce human oversight over time?"**

Answer: "Yes. As the agent's accuracy improves and we build confidence through data, we can lower the approval threshold. Our goal is to move from 12% approval rate to 5% over the next 6 months. We'll validate this with A/B testing: route 50% of traffic through lower thresholds, measure incident rates, and roll out if safe."

**"What's our incident rate with this oversight model?"**

Answer: "With current oversight, we have 0.3% incident rate (3 issues per 1,000 agent actions). Without oversight, historical data shows 8% incident rate. Oversight reduces incidents by 96% while only slowing task completion by 15% on average."

This framing shows oversight as a cost-effective risk mitigation strategy, not a lack of trust in the agent.

---

## Template: Human Review Rubric for Agent Trajectories

```yaml
Trajectory Review Rubric

Reviewer: [Name]
Date: [Date]
Agent Version: [v2.3]
Task ID: [12345]

--- Context ---
User Request: [Display user's original request]
Agent Plan: [High-level summary of action sequence]
Outcome: [Task completed / Escalated / Error]

--- Evaluation ---

1. Task Success
   Did the agent complete the user's request correctly?
   [ ] Yes - fully correct
   [ ] Partial - mostly correct, minor issues
   [ ] No - incorrect or incomplete

2. Plan Quality
   Was the action sequence efficient and appropriate?
   [ ] Efficient - minimal steps, no waste
   [ ] Acceptable - reasonable approach
   [ ] Inefficient - unnecessary steps or poor strategy

3. Tool Use
   Did the agent call APIs correctly with valid parameters?
   [ ] Correct - all tool calls appropriate and accurate
   [ ] Minor errors - small mistakes in parameters or handling
   [ ] Major errors - wrong tools, invalid params, misinterpreted results

4. Context Handling
   Did the agent use available information appropriately?
   [ ] Good - used all relevant context correctly
   [ ] Adequate - used most context, minor gaps
   [ ] Poor - ignored context or misunderstood key information

5. Error Recovery (if errors occurred)
   Did the agent handle errors appropriately?
   [ ] Good - retried effectively, found alternatives
   [ ] Adequate - attempted recovery, partially successful
   [ ] Poor - gave up quickly or made situation worse
   [ ] N/A - no errors occurred

6. Escalation Decision (if escalated)
   Was escalation appropriate?
   [ ] Yes - agent correctly identified it couldn't proceed
   [ ] Debatable - escalation was cautious but possibly unnecessary
   [ ] No - agent could have completed the task
   [ ] N/A - task completed without escalation

7. Approval Requests (if any)
   Did approval requests contain sufficient context?
   [ ] Excellent - clear explanation, easy to review
   [ ] Good - adequate context, minor ambiguity
   [ ] Poor - unclear, missing information
   [ ] N/A - no approval requests

8. Overall Recommendation
   [ ] Approve - trajectory is good, agent performed well
   [ ] Approve with feedback - trajectory is acceptable, but note areas for improvement
   [ ] Flag for training - trajectory shows specific issues that should improve agent
   [ ] Escalate - trajectory reveals serious issues requiring immediate attention

9. Notes
   [Free text: explain ratings, highlight specific issues or successes]
```

This rubric provides structure for human reviewers and generates data you can aggregate across many trajectories to identify patterns.

---
