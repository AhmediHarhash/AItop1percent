# Cost Management & Budget Allocation

I once watched a PM present their Q3 eval roadmap to leadership. Beautiful deck. Ambitious test coverage. Plans to scale human review from 500 cases a month to 5,000. The VP of Engineering stopped them halfway through and asked one question: "What's this going to cost?"

Silence. The PM hadn't done the math. They thought of human evaluation as a quality activity, not a budget line item. The VP did some napkin arithmetic right there in the meeting — 5,000 cases, three raters each, $2 per rating, plus platform fees and management overhead. That's roughly $35,000 per month. $420,000 annually. For one product's eval program.

The roadmap got approved, but with a caveat: "Show me the ROI or show me how you'll cut this in half by next quarter."

That's the reality of human evaluation. It's the single most expensive component of your eval infrastructure. More expensive than your test data pipeline. More expensive than your AI judge costs. More expensive than the engineering time to build the system. And if you don't budget for it properly, you either under-invest and miss critical issues, or you over-invest and get your program cut.

Let me walk you through how to budget, optimize, and justify the spend.

---

## Why This Matters

Human evaluation costs real money. Not cloud credits. Not GPU cycles you can optimize. Actual cash going to actual people reviewing actual outputs. And unlike infrastructure costs that drop over time, human costs are sticky. They scale linearly with volume. They don't get cheaper just because you're doing more of it.

Most teams underbudget for human eval because they think of it like testing. "We'll just have our QA team review some outputs." But human eval isn't testing. It's research. It's judgment. It requires trained raters, quality control, tooling, management overhead, and ongoing calibration. The true cost is three to five times what naive estimates suggest.

And yet, the alternative is worse. Skipping human eval means shipping regressions you can't catch with automated metrics. It means trusting AI judges without ground truth calibration. It means production incidents that cost far more than the eval program you skipped.

The companies that win at AI evaluation treat human eval as a strategic investment, not a cost center. They build rigorous cost models, optimize without cutting quality, and clearly articulate ROI to leadership. That's what we're learning here.

---

## The True Cost of Human Evaluation

Let's start with honesty: human eval is expensive because quality judgment is expensive. You're not buying mechanical clicks. You're buying cognitive labor from people who understand your domain, your users, and what "good" looks like.

**The cost components:**

**Rater compensation** — The actual payment to the people doing the evaluation. This varies wildly by sourcing model. Internal employees cost $50-150 per hour fully loaded (salary, benefits, overhead). Crowdworkers cost $12-25 per hour on platforms like Scale or Surge. Domain experts cost $75-300 per hour depending on specialization. College students or community reviewers cost $15-20 per hour. The compensation sets your floor.

**Platform fees** — If you're using a managed platform (Scale AI, Surge, Labelbox), they charge 20-40% on top of rater compensation for quality management, tooling, and support. A $20/hour rater actually costs you $28/hour after platform fees. If you're building in-house tooling, you skip the platform fee but absorb engineering and maintenance costs instead.

**Management overhead** — Someone has to define tasks, write guidelines, train raters, review quality, handle edge cases, and iterate on the process. For every 10 hours of rater time, budget 2-3 hours of manager or researcher time. That's expensive senior labor.

**Training time** — New raters take 2-6 hours of paid training before they're productive. You're paying for that training, but getting no labels. If your rater churn is 30% per quarter (typical for crowdwork), you're constantly retraining. Training cost is often 5-10% of total program cost.

**Quality assurance** — You need redundant ratings (2-3 raters per case), gold standard checks, and expert review of disputed cases. That multiplies your base cost. A single rating might cost $2, but getting a high-confidence label with QA might cost $5-7.

**Tooling and infrastructure** — Either you're paying platform fees or you're building custom evaluation UIs, data pipelines, analytics dashboards, and reporting systems. Tooling cost is front-loaded but amortizes over time. Budget $50,000-150,000 for initial buildout if going custom, or 25-40% ongoing fees if using a platform.

The math: if you need 10,000 labels per month, each label requires two raters at $2 per rating (after platform fees), plus 10% QA overhead, your monthly cost is roughly $22,000. Annually, that's $264,000. And that assumes your task is simple. Complex tasks cost more.

---

## Cost Per Label by Task Type

Not all eval tasks cost the same. The complexity of the judgment drives the time required, which drives the cost. Here's the hierarchy from cheapest to most expensive:

**Simple binary classification** — "Is this output safe or unsafe?" "Does this answer the question, yes or no?" These take 10-20 seconds per case. Cost: $0.50-1.00 per label (with redundancy). This is your cheapest option. Use it wherever you can reduce complex judgments to binary decisions.

**Likert scale ratings** — "Rate the helpfulness from 1-5." "How fluent is this response?" These take 30-60 seconds per case because raters have to calibrate what a "3" means versus a "4." Cost: $1.50-2.50 per label. Still affordable, but the inter-rater reliability is lower, so you often need three raters instead of two.

**Pairwise comparison** — "Which response is better, A or B?" This is the gold standard for preference evaluation. Takes 45-90 seconds per case because raters have to read both outputs and make a comparative judgment. Cost: $2.00-3.50 per comparison. Doubles your cost versus single ratings, but the signal is cleaner.

**Multi-dimensional rubric** — "Rate this response on accuracy, completeness, tone, and safety, each on a 1-5 scale." Now you're asking for four independent judgments, each requiring cognitive load. Takes 2-4 minutes per case. Cost: $4.00-7.00 per label. Use this sparingly, for high-stakes evaluations or complex outputs like long-form generation.

**Expert domain review** — "Is this medical advice clinically accurate?" "Does this legal summary correctly interpret the statute?" You need subject matter experts, not generalist crowdworkers. Takes 5-15 minutes per case. Cost: $10-40 per label, depending on domain rarity. This is where costs explode, but there's no substitute for expertise.

**Qualitative open-ended feedback** — "Explain what's wrong with this output and suggest how to fix it." Now you're paying for thoughtful written responses, not just clicks. Takes 5-10 minutes. Cost: $8-15 per response. High value for development eval, unsustainable for production monitoring at scale.

The trap: using expensive task formats when cheaper ones would suffice. If you can answer your eval question with binary labels instead of Likert scales, you cut your cost in half. If you can use pairwise comparison instead of multi-dimensional rubrics, you cut it by 60%. Match the task complexity to the question you're answering, not to the most sophisticated format you can imagine.

---

## Building a Cost Model

You need a simple formula to forecast spend and make trade-offs. Here's the model I use:

**Total cost = (labels needed) × (cost per label) × (raters per case) × (QA overhead multiplier)**

Let's break that down:

**Labels needed** — How many eval cases are you running per month? For development eval, you might run 500-2,000 cases per experiment. For release gates, maybe 1,000-5,000 cases per release. For production monitoring, 500-10,000 cases per month depending on traffic sampling rate. For red teaming, 200-1,000 adversarial cases per quarter. Add it all up. If you're running continuous eval, your monthly label volume might be 5,000-20,000.

**Cost per label** — From the breakdown above. Simple binary is $0.50-1.00. Pairwise comparison is $2.00-3.50. Expert review is $10-40. Use the midpoint for planning, then adjust based on actuals.

**Raters per case** — For low-stakes eval, you might use one rater (no redundancy). For medium-stakes, two raters with tie-breaking if they disagree. For high-stakes (release gates, safety review), three raters with majority vote or expert adjudication. More raters means higher confidence but multiplies cost.

**QA overhead multiplier** — This captures the hidden costs. Training, gold standard checks, manager review, tooling maintenance. Typically 1.1x to 1.3x your base cost. If you're doing 10% of cases as gold standard checks with expert review, that adds cost. If raters need recalibration every month, that adds cost. Use 1.2x as a default.

Example: you need 10,000 labels per month, pairwise comparisons at $2.50 per label, two raters per case, 1.2x QA overhead.

Cost = 10,000 × $2.50 × 2 × 1.2 = $60,000 per month.

That's $720,000 annually. Now you know what you're signing up for.

---

## Budget Allocation Across Eval Types

Not all eval types deserve equal budget. You allocate based on where human judgment adds the most value and where the stakes are highest.

**Development eval (30-40% of budget)** — This is your research and iteration loop. You're running experiments, comparing prompt variants, testing new model versions. High volume, medium stakes. You can tolerate slightly lower quality here because you're looking for directional signal, not absolute truth. Use cheaper formats (binary, Likert) and fewer raters (one or two).

**Release gates (25-35% of budget)** — This is your safety net before shipping. High stakes, moderate volume. You're evaluating release candidates, running regression suites, checking edge cases. You need high confidence here, so use redundant raters (two or three) and higher-quality formats (pairwise, rubrics). This is where you catch showstopper issues.

**Production monitoring (15-25% of budget)** — This is your ongoing surveillance. You're sampling live traffic, spot-checking outputs, validating that quality hasn't degraded. Medium volume, high stakes. Use smart sampling (focus on low-confidence cases or user-reported issues) and automated pre-filtering to reduce the human review load. Reserve expensive expert review for anomalies.

**Red teaming and adversarial eval (10-15% of budget)** — This is your proactive risk discovery. Lower volume, but requires creativity and domain expertise. Budget for expert raters and open-ended qualitative feedback. This is expensive per case, but you're not doing thousands of cases. You're doing hundreds of high-value adversarial probes.

**Calibration and ground truth creation (5-10% of budget)** — This is your foundation. Building gold standard datasets, running inter-rater reliability studies, validating that your AI judges align with human judgment. Low volume, very high stakes. Use expert raters and consensus processes. This data is reused for months or years, so invest in quality.

The mistake: spreading budget evenly across all eval types. That's like funding every project equally regardless of impact. Allocate strategically. If you're in a mature product phase, shift more budget to production monitoring. If you're in rapid iteration, shift more to development eval. If you're entering a new vertical, shift more to red teaming.

---

## Optimizing Without Cutting Quality

The goal isn't to minimize cost. It's to maximize value per dollar. Here's how you optimize without compromising the signal:

**Smart sampling** — Don't review everything. Use stratified sampling to cover edge cases and high-risk segments. Sample more heavily from low-confidence model outputs, from new user cohorts, from domains where you've seen past issues. Sample lightly from high-confidence, well-calibrated domains. This can cut review volume by 50-70% while maintaining coverage of risk.

**Tiered review** — Match the cost of review to the risk of the output. Low-risk outputs (simple FAQ answers) get one crowdworker rater. Medium-risk outputs (product recommendations) get two crowdworkers with majority vote. High-risk outputs (medical advice, financial guidance) get expert review. Don't pay expert rates to review everything.

**Automation assist** — Use AI judges to pre-filter and triage. Run automated evals first (Chapter 7), then send only the edge cases and disagreements to human review. If your AI judge agrees with your heuristic checks 95% of the time, you only need human review on the 5% where they diverge. This can reduce human review volume by 80-90%.

**Hybrid workflows** — Combine cheap high-volume labels with expensive low-volume expert review. Use crowdworkers to label 5,000 cases, then have an expert review a 200-case sample to calibrate and catch systematic errors. The crowdworker labels cost $10,000, the expert calibration costs $4,000, but the total is far less than having the expert review all 5,000 cases ($80,000).

**Reuse ground truth** — Once you've paid for high-quality human labels on a dataset, reuse it. Use it to train AI judges. Use it to validate new metrics. Use it to onboard new raters. The marginal cost of reuse is zero, so amortize the initial investment over multiple use cases.

**Template and guideline refinement** — The clearer your guidelines, the faster raters work and the fewer errors they make. Invest up front in refining task instructions, providing examples, and building intuitive UIs. A well-designed task takes 30 seconds instead of 60 seconds. That halves your cost.

**Batch and pipeline efficiently** — Don't send cases to raters one at a time. Batch them into coherent sets (same domain, same task type) so raters can stay in context and work faster. Automate data prep and result aggregation so you're not paying people to wrangle spreadsheets.

The key: these optimizations require thought and engineering. They're not free. But the ROI is massive. A month of engineering time to build smart sampling or automation assist can save $100,000+ annually in human review costs.

---

## The ROI Argument

At some point, you'll have to justify the spend to leadership. Here's the argument that works:

**What does a missed regression cost?** If you ship a quality regression and 5% of users get bad outputs for a week, what's the damage? Lost revenue, support tickets, churn, brand damage. For a B2C product with 1M weekly users, a bad week might cost $50,000 in support load and $200,000 in churn. For a B2B product, one bad customer experience might cost a $500,000 contract renewal.

**What does human eval cost?** If your monthly human eval budget is $30,000, you're spending $360,000 annually. That's one medium-sized regression prevented. If your eval program catches three regressions per year (very conservative), you're at 3x ROI. If it catches ten, you're at 10x ROI.

The math gets even better when you consider velocity. Without reliable eval, every release is risky. You slow down. You ship less. You miss market opportunities. Reliable eval lets you ship faster with confidence. The value isn't just in preventing bad outcomes, it's in enabling more frequent good outcomes.

**The comparison:** What's the alternative to human eval? Relying only on automated metrics and AI judges. That's cheaper, but less reliable. How much less reliable? If your AI judge catches 80% of issues and your human eval catches 95% of issues, the difference is 15 percentage points of risk. On a product with $10M annual revenue, 15% more risk might mean $1.5M in expected loss. Paying $360,000 to avoid $1.5M in loss is a good trade.

Frame it this way: human eval isn't a cost. It's insurance. You're paying a known amount to avoid an unknown but potentially catastrophic loss. Insurance is rational when the cost is small relative to the risk.

And if leadership pushes back, offer a pilot. "Let's run human eval on 20% of releases for one quarter and measure how many issues we catch that automated eval missed. Then we'll decide if it's worth scaling." Usually, the pilot shows value and the program gets funded.

---

## Reducing Cost Over Time

The per-label cost of human eval is dropping. Not because raters are getting cheaper (they're not), but because AI judges are getting better and reducing the volume of human review required.

**The 2026 reality:** You still need human eval, but you need less of it. Instead of reviewing 10,000 cases per month, you review 2,000. Instead of using human eval for volume scoring, you use it for calibration and edge case validation.

**The shift:**

**From volume review to calibration** — Instead of having humans label everything, you have humans label a small high-quality sample, then use that to train and validate AI judges. The humans do 500 cases, the AI judge does 50,000 cases. The human cost drops 100x, but the quality signal stays high because the AI judge is well-calibrated.

**From broad coverage to focused risk domains** — Instead of reviewing all output types equally, you focus human eval on the highest-risk areas: safety, edge cases, new domains, adversarial inputs. The bulk of routine eval shifts to automation. Human review becomes the expert layer, not the base layer.

**From continuous review to episodic audits** — Instead of reviewing every release, you review quarterly or when risk signals fire. You run a 1,000-case human audit, validate that your automated evals are still accurate, then trust automation until the next audit. Human eval becomes your calibration checkpoint, not your daily workflow.

**From labeling to feedback and iteration** — Instead of paying humans just to click "good" or "bad," you pay them to explain why and suggest improvements. You do fewer cases but extract more value per case. Qualitative feedback is expensive per case, but if one insight saves a month of engineering time, the ROI is huge.

The goal isn't to eliminate human eval. It's to use it strategically. Think of human eval as your ground truth layer. You don't need infinite ground truth. You need enough to validate that everything else is working.

Five years ago, you might have needed 50,000 human labels per quarter. Today, you might need 5,000. In three years, you might need 1,000 high-quality expert labels plus smart automation for the rest. The per-label cost stays the same, but the total program cost drops because volume drops.

---

## Hidden Costs Nobody Warns You About

When you budget for human eval, you budget for labels. But there are other costs that sneak up on you:

**Rater turnover and retraining** — If your rater churn is 30% per quarter (typical for crowdwork), you're retraining 30% of your pool every three months. Each new rater costs 3-5 hours of paid training. If you have 20 active raters, you're retraining 6 per quarter, costing 18-30 hours of training time. At $20/hour, that's $360-600 per quarter just on turnover. Multiply by four quarters, that's $1,440-2,400 annually in pure retraining cost. And that assumes low turnover. Some teams see 50% quarterly churn.

**Quality drift and recalibration** — Even trained raters drift over time. Their interpretation of "helpful" or "accurate" shifts. You need monthly or quarterly recalibration sessions where raters review gold standard cases and realign. Budget 1-2 hours per rater per quarter. For a 20-rater pool, that's 40-80 hours annually at $20-50/hour, costing $800-4,000.

**Scope creep** — You start by evaluating one output type (chatbot responses). Then product expands into image generation, so you need to evaluate images. Then voice, so you need to evaluate audio. Each new modality requires new guidelines, new training, new tooling. Eval coverage expands 3x over two years, and so does your cost. Budget for growth.

**Tooling maintenance** — If you built custom eval UIs, someone has to maintain them. Fix bugs, add features, handle rater support requests. Budget 10-20% of one engineer's time ongoing. That's $20,000-40,000 annually in engineering cost that doesn't show up in your "human eval budget" but is essential to the program.

**Data pipeline overhead** — Getting eval cases into the review tool and getting labels back into your data warehouse isn't free. Someone has to build and maintain the ETL, handle schema changes, debug missing data. Budget another 10% of an engineer's time. That's another $20,000 annually.

**Management and coordination** — Someone has to run the program. Define tasks, review quality, communicate with raters, report results to stakeholders. For a small program, that's 20% of one person's time. For a large program, it's one or two full-time roles. Budget $30,000-150,000 annually depending on scale.

Add it all up: if your direct label cost is $300,000 per year, your total program cost might be $400,000-450,000 when you include these hidden costs. That's a 30-50% overhead multiplier. Don't get surprised by it. Budget for it up front.

---

## Vendor Negotiation and SLA Structures

If you're working with a managed eval platform (Scale AI, Surge, Labelbox, CloudFactory), you're negotiating a contract. Here's what matters:

**Volume discounts** — Platforms charge less per label at higher volumes. If you commit to 100,000 labels per year instead of 10,000, you might get 20-30% off. But volume commitments are risky if your needs fluctuate. Negotiate tiered pricing instead: $X per label for first 50,000, $Y per label for next 50,000. That way you get discounts as you scale without over-committing.

**Quality guarantees** — Platforms should offer minimum accuracy SLAs (e.g., 95% agreement with gold standard). If they miss the SLA, you get credits or refunds. Make sure the SLA is measured on your data, not theirs. Ask how they handle edge cases and how quickly they remediate quality issues.

**Turnaround time** — Standard turnaround is 24-48 hours. If you need faster (4-hour or same-day), you pay a premium (often 1.5x-2x). Negotiate different tiers: standard pricing for routine eval, expedited pricing for release-blocking issues. Don't pay expedite fees for everything.

**Rater qualifications** — Platforms have different rater pools. Generalist crowdworkers are cheap. Domain experts are expensive. Make sure the contract specifies rater qualifications (e.g., "native English speakers," "college-educated," "medical professionals for clinical eval"). Audit rater quality in the first month to ensure they meet spec.

**Data security and privacy** — If you're evaluating customer data or proprietary content, the contract needs data protection clauses. Who has access to your data? How is it stored? When is it deleted? For enterprise or healthcare contexts, you may need BAAs (Business Associate Agreements) or GDPR compliance guarantees.

**Penalty clauses** — What happens if the platform delivers late or misses quality SLAs? Make sure there are financial penalties, not just apologies. If a missed SLA delays your release and costs you $50,000 in lost revenue, the platform should refund more than the $2,000 you paid for the eval job.

**Flexibility and scaling** — Can you ramp up or down on short notice? Can you pause the contract if priorities shift? Avoid rigid annual commitments unless you're very confident in volume. Tech priorities change fast.

The biggest mistake: signing a contract without running a paid pilot. Always do a 1,000-label pilot before committing to 100,000 labels. The pilot reveals quality issues, turnaround problems, and whether their tooling actually fits your workflow. It costs $2,000-5,000 and saves you from a bad $200,000 annual commitment.

---

## Annual Budgeting Cycle

Human eval costs aren't static. They grow as your product scales, as eval coverage expands, as you move into new domains. You need an annual budgeting process:

**Q4 planning (for next fiscal year):**

**Forecast eval volume** — Based on product roadmap, how many releases will you ship? How much traffic will you monitor? How many experiments will you run? Each of those has an eval volume estimate. Add them up. If you did 80,000 labels this year and product is growing 50%, plan for 120,000 labels next year.

**Estimate cost per label** — Based on your current mix of task types and sourcing models. If you're shifting toward more expert review, cost per label will rise. If you're shifting toward automation assist, it will fall. Use historical data and adjust for known changes.

**Calculate total cost** — Volume × cost per label × raters per case × overhead. Then add management, tooling, and hidden costs. If direct labels cost $360,000 and overhead is 40%, budget $500,000 total.

**Build in contingency** — Eval needs are unpredictable. A new product launch might require an extra 20,000 labels. A safety incident might trigger emergency red teaming. Budget 15-20% contingency. If your base estimate is $500,000, request $575,000-600,000.

**Justify to finance** — Show ROI. Show what eval caught this year. Show the cost of what would have happened without it. Show benchmarks from peer companies. Show how the cost scales sublinearly with product growth (because of automation and optimization).

**Q1 execution:**

Lock in vendor contracts or hire internal raters. Make sure tooling is ready. Train the team. Run calibration exercises.

**Q2-Q3 monitoring:**

Track actual spend versus budget monthly. If you're trending over, identify why (scope creep? higher cost per label? more volume?) and adjust. If you're trending under, consider reallocating to higher-value eval areas.

**Q4 retrospective:**

What did you learn this year? Did you over-budget or under-budget? Did the cost model hold? What changed? Feed that into next year's planning.

The goal: no surprises. Finance hates surprises. If you budgeted $500,000 and spent $750,000, you lose credibility and get cut next year. If you budgeted $500,000 and spent $400,000 but can show you optimized intelligently, you look good.

---

## Enterprise Expectations

If you're working in a large organization, human eval budget gets scrutiny. Here's what leadership expects:

**Clear cost-benefit justification** — Not just "we need this for quality." Quantify the risk you're mitigating, the regressions you've prevented, the velocity you've enabled. Use numbers.

**Benchmarking** — "Is $500,000 per year reasonable for human eval?" Leadership doesn't know. You have to provide context. If peer companies spend 2-5% of product engineering budget on eval, and your product engineering budget is $15M, then $300,000-750,000 is industry-standard. Show the benchmark.

**Optimization over time** — Leadership expects costs to improve as you mature. If you spent $400,000 last year and $600,000 this year, you need to explain why. Is it product growth (justifiable) or inefficiency (not justifiable)? Show how you're driving cost per label down even as total spend increases.

**Trade-off transparency** — "If we cut the budget by 30%, what risk do we take on?" Be honest. Maybe you cut production monitoring coverage from 10,000 cases to 5,000, and you miss 20% of regressions. Or maybe you shift from expert review to crowdworkers, and quality drops 5 percentage points. Make the trade-off explicit so leadership can make an informed decision.

**Long-term strategy** — Show the glide path. "This year we need heavy human eval. Next year we'll have calibrated AI judges and reduce human review by 40%. The year after, we'll shift to episodic audits and reduce another 30%." Leadership likes a plan, not a static cost.

**Governance and accountability** — Who owns the budget? Who approves scope changes? How do you prevent runaway spending? Have a clear decision framework: routine eval is pre-approved, new eval types or volume spikes require VP sign-off.

In enterprise, the cost isn't the issue. It's the lack of clarity around the cost. If you can articulate what you're spending, why, what you're getting, and how it's improving, you'll get the budget. If you just ask for money without justification, you won't.

---

## The 2026 Cost Landscape

Here's where we are as of early 2026:

**Per-label costs are dropping** — Five years ago, a pairwise comparison cost $5-8. Today it's $2-4. Platforms are more efficient, tooling is better, rater pools are larger. Expect another 20-30% drop over the next three years.

**Total spend is increasing** — Even though per-label costs are falling, companies are doing more eval. Broader coverage, more domains, higher frequency. So the overall budget is growing, just not as fast as volume.

**Hybrid approaches are standard** — Nobody does pure human eval at scale anymore. Everyone uses AI judges for volume, human eval for calibration and edge cases. The companies spending the most on human eval are the ones with the most mature hybrid systems, because they've realized where human judgment is irreplaceable.

**Specialization and tiering** — The market is splitting. Cheap generalist crowdwork for simple tasks (binary, Likert). Expensive expert networks for domain-specific review (legal, medical, technical). Mid-tier platforms for general-purpose quality eval. Match your sourcing to your task.

**Automation is table stakes** — Pre-filtering, smart sampling, AI-assisted triage — these aren't nice-to-haves anymore. They're expected. If you're not using automation to reduce human review volume, you're overspending by 3x-5x compared to peers.

The message: budget for human eval like it's a strategic capability, not a tactical expense. The companies that underfund it ship worse products and move slower. The companies that overfund it waste money they could allocate elsewhere. The companies that fund it right and optimize continuously win.

---
