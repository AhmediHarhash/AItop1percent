# 15.12 — Control Plane Hardening — From Red Team Findings to Fixes

Finding vulnerabilities is the easy part. A competent red team can enumerate misconfigurations, demonstrate exploits, and produce a report full of critical findings in a week. The hard part is what happens after. Most organizations receive red team reports, acknowledge the findings, create tickets, and then watch those tickets age in the backlog while the vulnerabilities remain open. Six months later, the same red team finds the same vulnerabilities. A year later, an attacker finds them too. The gap between finding and fixing is where most AI infrastructure compromises actually happen — not because the organization was unaware, but because awareness did not translate into action.

This subchapter is about closing that gap. It is about the process, the prioritization framework, and the architectural patterns that turn red team findings into durable defenses. If you run a red team engagement and nothing changes afterward, you paid for a very expensive report that sits in a SharePoint folder.

## Translating Findings into Actionable Hardening

A red team report is written for the red team. It describes attack chains, exploitation techniques, and proof-of-concept details that make sense to security practitioners. The people who need to act on the report — infrastructure engineers, platform teams, DevOps leads, engineering managers — speak a different language. They think in terms of deployments, configurations, service-level objectives, and change management windows. The first step in hardening is translation.

Every finding must be converted into a specific configuration change, policy update, or architectural modification. "The model serving pods have excessive privileges" is a finding. "Set the securityContext in the model-serving Deployment to disallow privilege escalation, drop all capabilities except NET_BIND_SERVICE, and set the container to run as a non-root user with UID 1000" is a fix. The gap between the two is where hardening stalls. If the fix is not specific enough for the infrastructure team to implement without security expertise, it will not be implemented.

Create a hardening runbook for each finding category. For container security findings, the runbook specifies the exact pod security context fields to change, the admission policy rules to add, and the validation tests to confirm the fix works. For network segmentation findings, the runbook specifies the network policy resources to create, the firewall rules to add, and the connectivity tests to verify isolation. For secrets management findings, the runbook specifies the rotation procedure, the vault configuration changes, and the monitoring queries to detect future exposure. Specificity is the difference between a report that gathers dust and a report that hardens infrastructure.

## The VOICE Prioritization Framework

Not all findings are equally urgent, and infrastructure teams cannot fix everything simultaneously. A prioritization framework ensures that the most dangerous vulnerabilities are fixed first while less critical findings are tracked and addressed on a realistic timeline. The **VOICE framework** — standing for Value, Observability, Impact, Complexity, and Exposure — provides a structured approach to ranking control plane vulnerabilities.

**Value** measures what the attacker gains from exploiting the vulnerability. A finding that grants access to production model weights has higher value than one that reveals the model serving framework version. A finding that enables model replacement has higher value than one that enables model fingerprinting. Rank value on a five-point scale where one is informational reconnaissance and five is full compromise of model integrity or data confidentiality.

**Observability** measures whether existing monitoring would detect exploitation. A vulnerability that can be exploited without generating any log entry, alert, or metric anomaly is more dangerous than one that triggers an alert on first attempt. If your logging infrastructure would not notice the attack, the attacker has unlimited time to exploit it. Rank observability inversely — one means the attack would be immediately detected, five means it would generate no observable signal.

**Impact** measures the blast radius. A vulnerability in a single non-production endpoint has limited impact. A vulnerability in the control plane that affects every model serving pod in the cluster has maximum impact. Consider how many systems, users, and data assets are affected if the vulnerability is exploited. Consider cascading effects — does exploiting this vulnerability enable access to additional systems?

**Complexity** measures the difficulty of remediation. A fix that requires a single configuration change deployable through the existing CI/CD pipeline is low complexity. A fix that requires re-architecting the network topology, rewriting the deployment pipeline, and migrating to a new secrets management system is high complexity. Rank complexity so that low-complexity, high-impact fixes are prioritized — these are the quick wins that reduce risk fastest.

**Exposure** measures the accessibility of the vulnerability. A finding that requires internal network access to exploit is less exposed than one exploitable from the public internet. A finding that requires Kubernetes API access is less exposed than one exploitable from any container in the cluster. Consider who can reach the vulnerable component and how many trust boundaries the attacker must cross.

Multiply the scores. The highest products are the highest priority. A vulnerability that is high-value, invisible to monitoring, broad in impact, simple to fix, and publicly exposed is the first thing you fix on Monday morning. A vulnerability that is low-value, well-monitored, narrow in impact, complex to fix, and accessible only from a privileged internal position goes into the quarterly hardening roadmap.

## Defense-in-Depth for AI Infrastructure

No single control prevents all attacks. Defense-in-depth applies multiple independent security controls so that the failure of any one control does not result in compromise. For AI infrastructure, defense-in-depth spans five layers: network, orchestration, container, application, and model.

At the **network layer**, enforce encryption on all traffic — internal and external. Deploy network policies that restrict pod-to-pod communication to only the paths required by the architecture. Segment the network so that the model serving tier, the training tier, the data storage tier, and the development tier are in separate zones with controlled ingress and egress. Monitor for anomalous traffic patterns: unexpected connections between zones, unusually large data transfers, connections to external addresses not on the approved list.

At the **orchestration layer**, enforce pod security standards that prevent privileged containers, host path mounts, and host network access unless explicitly exempted through a reviewed exception process. Deploy admission controllers that validate every deployment against security policies before it is applied to the cluster. Use RBAC with the principle of least privilege — service accounts should have exactly the permissions they need and no more. Rotate service account tokens and Kubernetes secrets on a schedule. Audit every API server request and alert on actions that indicate privilege escalation.

At the **container layer**, use minimal base images that contain only the software required to run the workload. Scan every image for known vulnerabilities before it enters the approved registry. Enforce image signing so that only images built by the approved pipeline can be deployed. Use read-only root filesystems where the workload permits. Drop all Linux capabilities that the workload does not explicitly need. Run containers as non-root users.

At the **application layer**, enforce authentication and authorization on every model endpoint — no exceptions for internal endpoints, no exceptions for development environments. Implement rate limiting that resists the bypass techniques described in earlier subchapters. Filter model outputs for credentials, PII, and other sensitive data before they reach the user. Log every inference request with enough detail to support forensic investigation without logging the content of sensitive queries.

At the **model layer**, monitor for behavioral drift that might indicate model replacement or poisoning. Validate model integrity through cryptographic checksums before loading. Enforce that model serving nodes can only read models from the registry, never write. Test safety controls continuously — if the safety classifier stops blocking known-bad inputs, that is an alert condition regardless of the cause.

The strength of defense-in-depth is redundancy. An attacker who bypasses network segmentation still faces admission controller validation. An attacker who escapes a container still faces RBAC restrictions on the Kubernetes API. An attacker who replaces a model still faces integrity validation at load time. No layer is impervious. Every layer reduces the probability that an attacker achieves their objective.

## Immutable Infrastructure for Model Serving

**Immutable infrastructure** is the practice of never modifying deployed resources in place. Instead of patching a running container, you build a new container image with the fix and replace the old container entirely. Instead of editing a running pod's configuration, you update the manifest in version control and deploy a new pod.

For AI model serving, immutability eliminates entire categories of attack. If the container's root filesystem is read-only, an attacker who gains code execution inside the container cannot install tools, drop backdoors, or modify the serving framework's behavior. If deployments are always created from version-controlled manifests, an attacker cannot quietly modify a running deployment's security context. If model weights are loaded from a verified checksum at startup, an attacker cannot replace the model in a running pod without triggering a checksum failure.

The implementation requires discipline. Every change — model update, configuration change, dependency patch — flows through the CI/CD pipeline. No engineer SSHs into a model serving pod. No manual kubectl edit commands are allowed against production deployments. The pipeline builds the image, signs it, pushes it to the registry, deploys it through a manifest change, and validates the deployment through automated health checks. The running infrastructure is a reflection of the version-controlled source, nothing more.

Immutability also simplifies forensics. If a running pod differs from its expected state — unexpected files, modified binaries, additional network connections — the deviation is immediately suspicious because it should be impossible. Immutable infrastructure turns every unauthorized change into a detection signal.

Red teams validate immutability by attempting to modify running infrastructure. Can you write files to the container filesystem? Can you modify environment variables in a running pod? Can you change a deployment's configuration through the Kubernetes API without triggering an alert? Can you replace a model file on disk after it has been loaded? Every successful modification is an immutability failure that needs architectural remediation.

## Audit Logging and Tamper Detection

You cannot investigate what you did not record. **Audit logging** for AI infrastructure must capture every action that changes the security state of the system: who deployed what, when, and from where. But logging is only useful if the logs are trustworthy. An attacker who compromises the infrastructure will attempt to modify or delete logs that record their activity. **Tamper detection** ensures that log modification is detectable.

The minimum audit surface for AI control plane security includes every Kubernetes API server request, every container image pull, every secret access, every model registry read and write, every network policy change, every admission webhook decision, and every model endpoint configuration change. Each log entry must include the identity of the actor, the timestamp, the action, the target resource, and the result. For model-specific operations, add the model identifier, the model version or checksum, and the source of the model artifact.

Log integrity requires architectural separation. Logs must be written to a system that the infrastructure being logged cannot modify. If your Kubernetes cluster writes audit logs to a storage bucket, the cluster's service account must have append-only access to that bucket — it can write new log entries but cannot modify or delete existing ones. Cloud providers offer immutable storage tiers for this purpose: S3 Object Lock, Azure Immutable Blob Storage, and Google Cloud retention policies. Use them. If an attacker compromises the cluster and can also modify the audit logs, the logs are worthless for incident response.

Tamper detection adds a verification layer. Hash each log entry and chain the hashes so that modifying any entry changes the hash of every subsequent entry. This is the same principle that blockchain uses, applied to security logging. Periodically verify the hash chain by computing the expected hash from the log entries and comparing it to the stored hash. Any discrepancy indicates tampering. Some log management platforms provide tamper detection natively; for others, you build it as a post-processing step.

Real-time alerting on critical audit events is the operational layer. When someone deploys a new container image that was not built by the approved pipeline, the alert fires immediately — not at the next morning's log review. When someone accesses a secret that is not used by any current deployment, the alert fires. When someone modifies a network policy to allow traffic between previously isolated zones, the alert fires. The speed of detection determines the attacker's dwell time, and dwell time determines the damage.

## The Control Plane Security Maturity Model

Not every organization starts at the same level. A maturity model provides a roadmap from baseline controls to advanced posture, allowing teams to assess where they are and plan where they need to be.

**Level one: Visibility.** You know what AI infrastructure exists. You have an inventory of every model endpoint, every GPU instance, every container image, every service account. Shadow AI systems have been discovered and cataloged. This is the foundation — you cannot secure what you do not know exists. Most organizations believe they are past this level. Most are wrong. The shadow AI audit typically reveals 30 to 50 percent more AI infrastructure than the official inventory.

**Level two: Baseline controls.** Standard security controls are applied to AI infrastructure. Containers run as non-root. Network policies restrict pod-to-pod communication. Secrets are stored in a vault, not in environment variables or manifests. Images are scanned for known vulnerabilities. Authentication is required on every endpoint. This is where the low-hanging fruit is harvested — the misconfigurations and missing controls that the first red team engagement reveals.

**Level three: AI-specific hardening.** Controls are tailored to the unique properties of AI workloads. GPU container security contexts are reviewed and minimized. Model integrity validation is enforced at load time. System prompts and tool definitions are protected from extraction. Rate limiting resists known bypass techniques. The model registry enforces read-only access from serving infrastructure. This level requires understanding not just Kubernetes security or network security, but the specific attack patterns that target AI systems.

**Level four: Continuous validation.** Red teaming is not an annual engagement — it is a continuous process. Automated security tests run against every deployment. Model behavior is monitored for drift that might indicate compromise. Audit logs are analyzed in real time for indicators of attack. The security team can detect and respond to a control plane compromise within hours, not weeks. Chaos engineering principles are applied to security controls — teams deliberately disable defenses to verify that monitoring detects the gap.

**Level five: Adaptive defense.** The security posture evolves in response to the threat landscape. New attack techniques discovered in the wild are tested against the infrastructure within days. Red team methodologies are updated quarterly to incorporate emerging threats. The organization contributes to the broader AI security community through responsible disclosure and shared threat intelligence. This level is aspirational for most organizations in 2026, but it defines the direction of travel.

Assess your current level honestly. Most organizations in 2026 are between level one and level two — they have incomplete visibility and basic controls with significant gaps. The red team report tells you exactly where the gaps are. The hardening roadmap tells you how to close them. The maturity model tells you what comes next.

## Sustaining the Hardened State

Hardening is not a project with a completion date. It is an ongoing discipline that must survive personnel turnover, organizational change, and the relentless pressure to ship features faster. The team that hardens the control plane today is not the team that maintains it in eighteen months. The engineer who understood why a particular network policy exists moves to another company. The new engineer, under deadline pressure, removes the policy because it blocks a new integration. The vulnerability reopens silently.

Three practices sustain hardening over time. First, **encode controls in infrastructure-as-code** that lives in version control and deploys through the same pipeline as application code. A network policy in a YAML file in a Git repository is durable. A network policy applied manually through kubectl is temporary. Second, **automate compliance verification** so that every deployment is validated against the security baseline. If someone removes a security control from a manifest, the CI/CD pipeline rejects the change before it reaches the cluster. Third, **run periodic red team assessments** that specifically re-test previously remediated findings. If a fix has regressed, the red team discovers it before an attacker does.

The control plane is the brain of your AI infrastructure. It decides what runs, where it runs, and with what permissions. A compromised control plane does not just affect one model or one endpoint — it affects everything the control plane manages. Hardening the control plane is not a security project. It is the security project, because every other defense depends on the integrity of the infrastructure that deploys and operates it.

The next chapter shifts from infrastructure defense to a different kind of adversarial skill: detection evasion, where red teams learn to operate without triggering the monitoring, alerting, and behavioral analysis systems that defenders rely on to catch attacks in progress.