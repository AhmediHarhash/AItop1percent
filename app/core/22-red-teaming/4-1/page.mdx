# 4.1 — The Model-Layer Threat Model: Attacks Below the Prompt

Most teams think about AI security at the prompt layer. Can a user trick the model into ignoring instructions? Can they extract training data through clever phrasing? Can they bypass content filters with creative encoding? These are real threats, and Section 21 covered them comprehensively. But they are only one layer of the attack surface.

Below the prompt layer sits the model itself — the weights that encode learned patterns, the embeddings that represent semantic meaning, the gradient flows that reveal internal structure, the fine-tuning processes that adapt behavior. This layer is invisible to most users. It requires technical sophistication to attack. But when an attacker has the knowledge and access to operate at this level, prompt-layer defenses become irrelevant. You can have perfect input filtering, flawless output monitoring, and comprehensive safety fine-tuning — and still be vulnerable to attacks that manipulate the mathematical substrate of the model itself.

Understanding the model-layer threat model is essential for comprehensive AI security. This subchapter defines what the model layer is, who can attack it, and why it requires fundamentally different defensive thinking than prompt-layer security.

## What the Model Layer Is

The model layer encompasses everything that happens beneath user-facing inputs and outputs. It includes the weight matrices that define the model's behavior, the embedding spaces that represent tokens and concepts as vectors, the attention mechanisms that determine information flow, the activation patterns that emerge during inference, and the gradient signals that flow during training and fine-tuning.

At the prompt layer, an attacker interacts with the model as a black box. They send text, they receive text. They can observe behavior — what the model says, how it responds to different inputs — but they cannot directly manipulate the internal machinery. At the model layer, the attacker operates on that machinery itself. They might perturb embeddings to change semantic meaning. They might craft adversarial examples that exploit gradient properties. They might poison fine-tuning data to corrupt weight updates. They might extract sensitive information by analyzing logit distributions or gradient flows.

The model layer is not one thing. It is a collection of attack surfaces that most teams never defend because they assume the model is a trusted component. Prompt injection assumes the model works correctly and tries to misuse it. Model-layer attacks assume the model itself can be compromised, manipulated, or exploited through mathematical properties that have nothing to do with what the user types.

## Why Model-Layer Attacks Are Different

Prompt-layer attacks are constrained by what the model allows. If the model has been trained to refuse harmful requests, the attacker must find ways to phrase the request so the refusal mechanism does not trigger. If the model has been fine-tuned to ignore system prompt overrides, the attacker must craft inputs that appear benign until the refusal logic is bypassed. The model's learned behavior is the barrier the attacker must overcome.

Model-layer attacks bypass learned behavior entirely. An adversarial example does not convince the model to behave differently — it exploits the model's sensitivity to small input perturbations that are invisible to humans but catastrophic to the model's classification boundary. An embedding space attack does not trick the model into retrieving the wrong documents — it manipulates the vector representation so that semantically unrelated content becomes mathematically similar. A gradient-based extraction attack does not persuade the model to reveal training data — it uses the gradient signal itself to reconstruct information the model has memorized.

These attacks operate on mathematical properties, not semantic ones. A prompt filter that blocks the word "ignore previous instructions" does nothing against an adversarial token sequence that causes the classifier to misfire. A content moderation system that flags toxic language does nothing against an embedding collision attack that makes harmful content appear benign in vector space. Prompt-layer defenses assume the model is a reliable judge of its own inputs. Model-layer attacks exploit the fact that the model is a high-dimensional mathematical function with exploitable properties.

## The Threat Model for Model Access Levels

Not every attacker has the same level of model access. The threat model changes dramatically based on what the attacker can observe and manipulate.

**API-only access** is the most restricted level. The attacker can send inputs and receive outputs, but they cannot observe internal model state. They cannot see embeddings, activations, gradients, or logits. They can only infer model behavior from repeated queries. Even at this level, certain model-layer attacks are possible. Transfer attacks — adversarial examples crafted on one model and applied to another — can work if the target model shares architectural similarities with publicly available models. Timing side-channels can leak information about internal computation. Repeated queries with slight variations can probe decision boundaries and extract information about the model's learned patterns.

**Logit-level access** gives the attacker probability distributions over the model's output vocabulary, not just the top token. Many API providers expose this through parameters like temperature, top-k, or logprobs. This access is significantly more dangerous. An attacker can use logit distributions to extract training data more efficiently than with black-box access. They can detect when the model has memorized specific sequences by observing unnaturally high probabilities for continuation tokens. They can use gradient estimation techniques to approximate model behavior without direct gradient access.

**Embedding-level access** allows the attacker to observe or manipulate the vector representations the model uses internally. This is common in RAG systems where embeddings are exposed for retrieval, in multimodal systems where embeddings bridge modalities, and in fine-tuning workflows where teams inspect embeddings for debugging. Embedding access enables collision attacks, poisoning attacks, and extraction attacks that exploit the geometry of the embedding space. An attacker who can inject adversarial embeddings into a retrieval database can manipulate what documents the model sees. An attacker who can observe embeddings can infer sensitive information about the content being embedded.

**Gradient-level access** is the most dangerous access level short of full weight access. It occurs during fine-tuning, when gradients flow backward through the model to update weights. It also occurs in research settings where teams inspect gradients for interpretability. Gradient access enables direct extraction of training data through gradient inversion attacks. It enables precise crafting of adversarial examples using gradient-based optimization. It enables poisoning attacks that target specific weight updates. Most teams who expose gradient-level access do not realize they have opened a critical attack surface.

**Full weight access** means the attacker can inspect or modify the model's parameters directly. This is the scenario when a team downloads an open-weight model from an untrusted source, when a malicious insider has filesystem access to model checkpoints, or when supply chain attacks compromise the model distribution process. With weight access, every defense implemented at the prompt layer is bypassable. The attacker can reverse-engineer safety fine-tuning, extract memorized data, plant backdoors, or modify behavior in ways that are undetectable through normal inference.

## The Gradient as Attack Surface

Gradients are how models learn. During training, gradients indicate how to adjust weights to reduce loss. During fine-tuning, gradients propagate the signal from task-specific data back through the model. Most teams think of gradients as an internal implementation detail — something that happens during training and then disappears during inference.

But gradients are also an information channel. When you compute a gradient with respect to a specific input, you learn how the model's output would change if that input changed slightly. This reveals structure. An attacker who can query gradients can reconstruct aspects of the training data by inverting the gradient signal. They can craft adversarial examples far more efficiently than with black-box methods. They can identify which weights are most sensitive to specific inputs, enabling targeted poisoning attacks during fine-tuning.

The gradient as attack surface is most relevant in fine-tuning scenarios. When you fine-tune a model on proprietary data, gradients computed on that data encode information about the data itself. If an attacker can observe those gradients — through a compromised fine-tuning service, through a malicious co-training participant in federated learning, through side-channel access to gradient checkpoints — they can extract information the model was supposed to keep private. Differential privacy during training limits this attack by adding noise to gradients, but most teams do not use DP-SGD because of the performance cost. Without it, gradients are an open channel.

## Embedding Spaces as Vulnerabilities

Embeddings map discrete tokens or concepts into continuous vector spaces where semantic similarity corresponds to geometric proximity. "King" and "queen" are close in embedding space. "King" and "database" are far apart. This property is what makes embeddings useful for retrieval, classification, and semantic reasoning.

It is also what makes embeddings an attack surface. If an attacker can manipulate embeddings, they can manipulate semantic relationships. An adversarial embedding can make unrelated content appear similar, causing a retrieval system to return irrelevant or malicious documents. An embedding collision attack can make harmful content cluster with benign content, bypassing classifiers that rely on embedding-based similarity. An embedding poisoning attack can corrupt the embedding database so that legitimate queries return attacker-controlled results.

Embedding attacks are especially dangerous in RAG systems, where the model's knowledge base is represented as embeddings in a vector database. Most teams assume the embedding space is a trusted component. They filter inputs and outputs, but they do not validate embeddings. An attacker who can inject adversarial embeddings into the database can manipulate retrieval without ever touching the prompt. The model receives the wrong context, generates the wrong output, and the team never sees the attack because it happened in vector space, not in text space.

## Model-Layer vs Prompt-Layer Attacks

The distinction between model-layer and prompt-layer attacks is not just academic. It determines what defenses work.

Prompt-layer attacks are detectable through input and output monitoring. You can log prompts, classify them for malicious intent, and block or sanitize them before they reach the model. You can monitor outputs for policy violations and refuse to return harmful content. These defenses assume the model itself is trustworthy — that it will correctly interpret benign inputs as benign and only produce harmful outputs when tricked by adversarial phrasing.

Model-layer attacks bypass this assumption. An adversarial example does not contain malicious phrasing. It looks like a normal input to a human reviewer. The attack is in the token-level perturbations that cause the model's internal activations to land on the wrong side of a decision boundary. A prompt filter sees nothing wrong. The model misbehaves anyway.

An embedding poisoning attack does not involve a harmful prompt. The attacker injects adversarial embeddings into the retrieval database. When a legitimate user asks a benign question, the model retrieves poisoned context and generates an incorrect or harmful answer. The user's prompt is clean. The model's output might pass content filters if the poisoning is subtle. The attack is invisible at the prompt layer because it happened in embedding space.

This means you cannot defend the model layer with prompt-layer tools. You need different instrumentation, different monitoring, different testing. You need to validate embeddings, not just text. You need to detect distributional shifts in activations, not just policy violations in outputs. You need to test for adversarial robustness, not just prompt injection resilience.

## Who Has Model-Layer Access

The answer depends on your deployment model. In a fully black-box API scenario, the only party with model-layer access is the model provider. The customer has API-only access. They cannot see embeddings, gradients, or weights. The provider is responsible for model-layer security, and the customer can do nothing to defend against model-layer attacks that originate from compromised weights or poisoned fine-tuning.

In a fine-tuning scenario, the customer gains gradient-level access, even if they do not realize it. When you fine-tune a model through an API like OpenAI's fine-tuning endpoint, gradients are computed on your data. If the provider logs those gradients, they could theoretically reconstruct aspects of your training data. If a malicious insider at the provider has access to gradient checkpoints, they have an extraction vector. Most customers do not consider this when uploading proprietary data for fine-tuning.

In a self-hosted open-weight model scenario, you have full weight access — but so does anyone else who can access your model files. If your model checkpoint is stored on a shared filesystem without encryption, any colleague with access can inspect weights, extract data, or plant backdoors. If you download a model from an untrusted source, you have no guarantee the weights have not been tampered with. Supply chain attacks at the model layer are not theoretical. They happened to traditional software for decades. They will happen to AI models.

In a RAG scenario, anyone who can write to your vector database has embedding-level access. If your annotation team can upload documents that get embedded and indexed, they can inject adversarial embeddings. If your database is exposed to an API without authentication, external attackers can poison it. Most teams secure their prompt API but leave their embedding database unsecured because they think of it as an internal data store, not an attack surface.

## What This Means for Defense

Defending the model layer requires visibility and control at the model layer. You cannot rely on prompt filtering or output moderation. You need to monitor embeddings for distributional anomalies. You need to validate that fine-tuning updates do not introduce unexpected behavior shifts. You need to test for adversarial robustness, not just prompt injection resilience. You need to secure model weights with the same rigor you secure API keys and database credentials.

Most teams have none of this. They have comprehensive prompt-layer defenses and zero model-layer visibility. They would not detect an adversarial example attack. They would not notice embedding poisoning. They would not catch a gradient-based extraction attempt. They assume the model is a black box that can be trusted as long as the prompts are clean.

That assumption is wrong. The model layer is an attack surface. Attackers who understand this have access to techniques that bypass every prompt-layer defense you have built. The question is not whether model-layer attacks are possible. The question is whether you are testing for them, monitoring for them, and designing your system to limit the access levels that enable them.

The next subchapter covers the most well-known model-layer attack: adversarial examples. These are inputs that look normal to humans but cause models to catastrophically misclassify. They exploit the geometry of decision boundaries in high-dimensional space. And they work.
