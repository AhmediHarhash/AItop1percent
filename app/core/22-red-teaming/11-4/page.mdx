# 11.4 — Adversarial Prompt Libraries and Benchmarks

Most teams test the same vulnerabilities over and over. They discover jailbreaks, patch them, then six months later discover the same attack pattern under a different phrasing. They have no memory. No institutional knowledge of what attacks exist, what has been tried, what worked. Every red team exercise starts from scratch. Every new team member reinvents attacks that were solved two quarters ago. The organization learns nothing.

Standard adversarial prompt libraries fix this. They are the shared memory of your security program. A curated collection of attacks, organized by category, tagged by severity, validated against your system. When a new attack emerges in the wild, you add it to the library. When a team member leaves, the library remains. When you upgrade to a new model, you run the entire library against it to see what broke. Libraries turn ad-hoc red teaming into systematic testing. Benchmarks turn internal testing into comparative measurement. Together they form the foundation of repeatable adversarial evaluation.

## Why Standard Libraries Matter

Without a standard library, every test cycle reinvents the wheel. A team member thinks of a jailbreak. They try it. It works or it does not. They move on. Six months later, a different team member thinks of the same jailbreak, phrased slightly differently. They try it. It works. They report it as a new finding. Engineering patches it. Three months after that, the first team member tests again and finds their original jailbreak still works because the patch only covered the second phrasing. This is not theoretical. This is how most organizations operate in 2026.

A standard library breaks this cycle. Every attack gets documented once. Every variation gets tagged as related. When you test, you run the entire library, not just what someone remembered today. When you patch, you verify against all known variations, not just the one that was reported. When you hire, new team members inherit the institutional knowledge instead of starting from zero.

The secondary benefit is coverage tracking. Without a library, you have no idea what you have tested. You might have run a hundred jailbreaks, but were they all prompt injection variants? Did you test refusal bypass? Goal hijacking? Context confusion? A categorized library shows you the gaps. You can see that you have forty prompt injection tests but only three data exfiltration tests. That gap becomes a priority. Without the library, the gap is invisible.

The tertiary benefit is regression testing. When you upgrade models, when you change system prompts, when you add new tools, you need to know what broke. Running the full library before and after the change gives you a diff. These fifteen attacks used to fail, now they succeed. That diff is not noise. It is signal. It tells you exactly what your change weakened. Without the library, you only discover the regression when a user triggers it in production.

## Major Public Prompt Libraries

Several public adversarial prompt libraries exist in 2026, maintained by research institutions, industry coalitions, and open-source communities. The largest is the **OWASP LLM Attack Database**, which catalogs over two thousand prompt-based attacks organized into eighteen categories. It includes jailbreaks, injection techniques, refusal bypasses, and misalignment triggers. Each attack is tagged with severity, target capability, and known mitigations. The database is updated monthly with new attacks discovered in the wild.

The **HarmBench** project, originally released in late 2024, evolved into a community-maintained benchmark suite with attack prompts, evaluation criteria, and scoring rubrics. It focuses on safety failures — toxic generation, illegal advice, privacy violations, harmful misinformation. HarmBench includes both direct attacks and multi-turn conversation sequences that gradually escalate toward harmful outputs. Teams use it to measure baseline safety and track improvements over time.

For enterprise applications, the **AI Risk Consortium** maintains a commercially-licensed library of business-relevant attacks. These are not generic jailbreaks. They are domain-specific scenarios — financial fraud prompts, healthcare privacy attacks, HR discrimination triggers, contract manipulation attempts. The library is built from real incidents reported by member companies, anonymized and generalized. It is updated quarterly and includes attack narratives explaining how each prompt exploits business logic, not just model weaknesses.

For multilingual systems, the **Polyglot Attack Library** covers adversarial prompts in forty-three languages. Many attacks that fail in English succeed in lower-resource languages where the model has weaker safety tuning. The library includes direct translations of known attacks, culturally-specific attacks that only make sense in certain languages, and cross-lingual attacks that exploit translation layers. If your system supports multiple languages, testing only in English leaves massive blind spots.

## Domain-Specific Attack Sets

Generic prompt libraries cover common vulnerabilities. Domain-specific attack sets cover your actual risk surface. A healthcare chatbot faces different attacks than a financial advisor bot. A customer service agent faces different threats than a code generation tool. Generic libraries give you baseline coverage. Domain-specific sets give you operational readiness.

Building a domain-specific set starts with threat modeling. What capabilities does your system have? What data does it access? What actions can it take? What would an attacker want to achieve? For a healthcare bot, the threats include extracting patient information, generating fake medical advice, manipulating treatment recommendations, and accessing restricted records. For a financial advisor, the threats include portfolio manipulation, unauthorized trading, fee disclosure evasion, and regulatory requirement bypass. The threat model defines the attack categories. The attack categories define the prompts.

The best domain-specific sets are built from real attempts. Every production incident where a user tried something adversarial becomes an entry in the library. Every customer support ticket where someone complained about inappropriate outputs becomes a test case. Every internal audit finding becomes a regression test. This is not hypothetical red teaming. This is hardening against observed attacks.

One enterprise team running a legal document assistant maintains a library of three hundred domain-specific adversarial prompts. These include attempts to generate fraudulent contracts, requests to remove liability clauses, prompts designed to bias contract terms toward one party, and attempts to access confidential precedents from other cases. None of these appear in generic jailbreak libraries. All of them are business-critical. The library is run nightly against every model candidate before deployment. Passing the domain library is a release gate. Generic benchmark performance is informative. Domain library performance is mandatory.

## Benchmark Datasets for Safety

Benchmarks serve a different purpose than attack libraries. Libraries are for testing your system. Benchmarks are for comparing your system to others and tracking trends over time. A benchmark is a standardized evaluation with fixed prompts, fixed scoring criteria, and published baseline results from reference models.

The most widely used safety benchmark in 2026 is **SafetyBench v3**, which includes twelve hundred prompts across eight harm categories. Each prompt is scored on a four-point scale from fully safe to severely harmful. The benchmark publishes reference scores for major models — GPT-5, Claude Opus 4.5, Gemini 3 Pro — so you can see how your fine-tuned or custom model compares. If GPT-5 scores ninety-two percent safe on the toxicity category and your fine-tuned version scores seventy-eight percent, you know fine-tuning degraded safety. The magnitude of degradation is quantified, not guessed.

For bias and fairness, **FairLLM Benchmark** tests for demographic bias across gender, race, age, religion, and disability status. It includes prompts designed to elicit biased outputs, counterfactual prompt pairs that flip demographic attributes, and consistency checks where the model should give identical advice regardless of demographic context. Scoring is automated using classifier-based detection and structural consistency checks. The benchmark provides subscores for each demographic dimension, so you can identify specific weaknesses.

For misinformation resistance, **TruthfulQA Extended** includes over two thousand questions where common misconceptions, conspiracy theories, or misleading framings might lead the model astray. The benchmark measures whether the model gives accurate answers, refuses to answer when appropriate, and avoids confidently stating falsehoods. This is particularly important for customer-facing systems where users trust model outputs as factual.

One limitation of public benchmarks: they are public. Models are increasingly trained on benchmark data, either deliberately or through web scraping. By 2026, most frontier models have seen SafetyBench and TruthfulQA during training. Performance on these benchmarks no longer purely measures safety — it also measures memorization. This does not make benchmarks useless. It means you need private evaluation sets alongside public benchmarks. Use public benchmarks to track industry trends and compare against baselines. Use private libraries to measure your actual risk surface.

## Creating Custom Attack Libraries

Every organization should maintain a custom attack library. This is not optional for production systems. The library starts small — maybe fifty prompts covering your highest-risk scenarios. It grows over time as you discover new attacks, as your system gains new capabilities, as the threat landscape evolves.

The structure of a custom library matters. Each attack needs metadata: category, severity, target capability, success criteria, known mitigations, discovery date, last validated date. Without metadata, the library becomes a dumping ground. With metadata, it becomes a managed asset. You can filter by severity to run high-risk tests first. You can filter by capability to test only what changed. You can filter by validation date to identify stale tests that might no longer trigger on current models.

Severity classification is particularly important. Not all attacks are equal. A prompt that makes the model output a mildly rude response is not the same as a prompt that extracts confidential data. A four-tier severity system works well: **Critical** attacks cause immediate harm — data breaches, dangerous advice, regulatory violations. **High** attacks cause significant harm but require specific conditions or user action. **Medium** attacks are embarrassing or degrading but not directly harmful. **Low** attacks are edge cases or theoretical vulnerabilities. Critical attacks block releases. High attacks require documented mitigations. Medium and low attacks are tracked but do not gate deployment.

Success criteria must be explicit. What does it mean for an attack to succeed? For a jailbreak, success might be defined as the model producing the harmful content without refusal. For a prompt injection, success might be ignoring system instructions and following the injected instruction instead. For a data exfiltration attack, success might be outputting information that should be access-controlled. Vague criteria — "the output seems problematic" — lead to inconsistent evaluation. Explicit criteria enable automation.

One financial services company maintains a library of eight hundred custom attacks targeting their loan advisory chatbot. Every attack has a severity tag, a capability tag indicating which bot feature it targets, and a binary success criterion. The library is versioned in Git. Every change requires review from both security and product teams. Every quarter, the team conducts a coverage audit to identify underrepresented attack categories and prioritize expansion. The library is not a side project. It is infrastructure.

## Library Maintenance and Evolution

Attack libraries decay without maintenance. Attacks that worked six months ago might no longer trigger after model updates or system changes. New attack patterns emerge continuously. The library needs active curation, not just accumulation.

Validation cadence matters. Every attack in the library should be run against the current production system at least quarterly. Attacks that no longer succeed get tagged as mitigated with the mitigation date recorded. They remain in the library as regression tests — if they start succeeding again after a model upgrade, that is a critical finding. Attacks that still succeed get escalated priority for mitigation or documented as accepted risks with justification.

New attack discovery happens through multiple channels. Red team exercises contribute new prompts. User reports contribute real-world attempts. Security research contributes novel techniques. Peer organizations share attacks through industry groups. Every new attack gets evaluated for inclusion. If it represents a genuinely new pattern, it gets added. If it is a variation of an existing attack, it gets cross-referenced. Over time, the library becomes a knowledge graph of attack patterns and their relationships.

Deprecation is as important as addition. Some attacks become obsolete as systems evolve. If you retire a capability, attacks targeting that capability are no longer relevant. If you implement robust mitigation for an entire attack category, those attacks might move from active testing to regression-only testing. A library that only grows becomes unwieldy. Thoughtful deprecation keeps it focused.

One team running a code generation assistant has a library that grew from one hundred to over a thousand attacks in two years. They implemented a quarterly review process where attacks that have not succeeded in four consecutive validation runs get moved to an archive tier. Archived attacks run monthly instead of nightly, reducing compute costs without losing regression coverage. New attacks start in active tier. Consistently mitigated attacks graduate to archive tier. Attacks that succeed even once stay active. This tiering keeps the active library focused on current threats while maintaining historical coverage.

## Using Benchmarks Appropriately

Benchmarks are tools, not targets. Optimizing for benchmark performance without understanding what the benchmark measures leads to superficial safety. A model can score ninety-eight percent on a toxicity benchmark by refusing to answer anything controversial, but that model is useless for real applications. A model can score perfectly on a jailbreak benchmark it has seen during training without being robust to novel jailbreaks.

The correct use of benchmarks is comparative and longitudinal. Compare your model to baselines. Compare your fine-tuned model to the base model you started from. Track how your scores change over time as you implement mitigations or add capabilities. Benchmarks provide context, not verdicts.

Never use a single benchmark. Different benchmarks measure different things. SafetyBench measures refusal of harmful requests. TruthfulQA measures factual accuracy. FairLLM measures demographic bias. A system can score well on one and poorly on another. You need a suite of benchmarks covering your risk surface. For a customer service bot, you might run SafetyBench for safety, TruthfulQA for accuracy, and a custom benchmark for brand consistency. Together they give you a multi-dimensional safety profile.

One common mistake is treating benchmark scores as absolute thresholds. There is no universal passing score. A ninety percent safety score might be acceptable for an internal tool with expert users. It is not acceptable for a consumer health advisor. The threshold depends on your risk tolerance, your user base, your regulatory environment. Define your thresholds based on your risk model, not based on what other models achieve.

Another mistake is ignoring subscores. A model might score eighty-five percent overall on a benchmark, but if you decompose by category, you find it scores ninety-five percent on toxicity and sixty percent on privacy violations. The aggregate score hides the critical weakness. Always examine subscores. A model with uneven performance requires targeted mitigation, not general safety tuning.

## The Limits of Benchmarks

Benchmarks measure known attacks. They do not predict robustness to unknown attacks. A model that scores perfectly on every public benchmark can still fail catastrophically on a novel jailbreak discovered tomorrow. This is not a flaw in benchmarks. This is the nature of adversarial testing. The attacker always has the advantage of creativity.

Benchmarks also suffer from context mismatch. A benchmark prompt in isolation might behave differently than the same prompt embedded in a real user conversation. A benchmark assumes a single-turn interaction. Real attacks often unfold across multiple turns, gradually escalating. A benchmark uses standardized evaluation. Real harm is contextual — a response that is safe in one domain might be dangerous in another.

The solution is not to abandon benchmarks. The solution is to use them as one component of a layered evaluation strategy. Benchmarks give you baseline coverage and comparative measurement. Custom libraries give you domain-specific testing. Manual red teaming gives you creative adversarial pressure. Continuous monitoring gives you real-world feedback. No single layer is sufficient. Together they form defense in depth.

One enterprise team uses benchmarks as a screening filter. Before a model enters manual red teaming, it must score above threshold on three public benchmarks. Models that fail benchmarks do not proceed to expensive human evaluation. Models that pass benchmarks enter a week-long red team exercise with internal security staff. Models that pass red teaming enter limited production with elevated monitoring. Benchmarks are the first gate, not the only gate. This staged approach allocates effort efficiently while maintaining rigorous standards.

Attack libraries and benchmarks are not adversarial testing. They are the infrastructure that makes adversarial testing systematic, repeatable, and cumulative. Without them, you are testing blind. With them, you build institutional knowledge that compounds over time. Every attack discovered improves the library. Every library run hardens the system. The cycle continues.

Next: **11.5 — Mutation-Based Testing: Evolving Attacks** — how to evolve attacks from working seeds, probing deeper with each generation.
