# 15.10 — Network-Level Attacks on AI Endpoints

Why do most AI security programs ignore the network? Because the network is old. It is solved. Every organization has firewalls, TLS, DNS, load balancers — the infrastructure that has protected web applications for two decades. The assumption is that AI endpoints are just another API behind the same network controls. This assumption fails in three ways. First, AI endpoints have different traffic patterns — long-running connections, streaming responses, large payloads — that break assumptions baked into traditional network security tools. Second, AI systems expose internal endpoints that were never designed for external access but become reachable through architectural drift. Third, the value of what crosses the network is different. A compromised web API leaks user data. A compromised model endpoint leaks the model itself, its system prompt, its tool configurations, and every piece of data it processes. The network is not solved for AI. It is the layer most teams forget to test.

## API Endpoint Discovery and Enumeration

Before an attacker can exploit a model endpoint, they must find it. **Endpoint enumeration** is the first phase of any network-level attack, and AI systems are surprisingly discoverable.

Start with DNS. AI teams name their endpoints predictably. Subdomains like "api-ml," "inference," "model-serve," "llm-gateway," "ai-prod," "embedding," and "completion" appear in internal and sometimes external DNS records. A DNS brute-force scan against common AI-related subdomain patterns reveals endpoints that the security team may not have inventoried. Certificate Transparency logs — public records of every TLS certificate issued — are another gold mine. If your team provisioned a certificate for inference.internal.company.com, that hostname appears in CT logs regardless of whether it resolves publicly.

Beyond DNS, AI endpoints leak through documentation, error messages, and client-side code. Swagger and OpenAPI specification files, often hosted at predictable paths, describe every endpoint including internal ones that were not intended for external consumption. Mobile and desktop applications that call model APIs embed endpoint URLs in their binaries. Browser developer tools reveal API calls to model serving infrastructure. A red teamer with access to the organization's mobile app can extract every model endpoint the app communicates with in under an hour.

Cloud provider metadata adds another discovery vector. If you compromise any instance in the cloud environment, the cloud provider's API can enumerate every load balancer, every API gateway, and every network endpoint in the account. Internal service discovery systems — Consul, CoreDNS, Kubernetes service records — provide a complete map of every model serving endpoint in the cluster. An attacker who gains access to the Kubernetes API server can list every service, every ingress rule, and every endpoint slice. The model endpoints that your team deployed "internally" are one API query away from full enumeration.

## Man-in-the-Middle Attacks on Model Traffic

A **man-in-the-middle attack** places the attacker between the client and the model endpoint, allowing them to observe, modify, or redirect traffic. In 2026, most external-facing model APIs use TLS, which theoretically prevents interception. But "most" and "external-facing" are doing a lot of work in that sentence.

Internal model traffic is the primary MITM target. In many architectures, the traffic between the application backend and the model serving layer traverses the internal network without TLS. The reasoning is familiar: "It is internal traffic. The network is trusted. TLS adds latency." For model inference, where every millisecond of latency affects user experience, teams are especially reluctant to add encryption overhead on internal paths. The result is plaintext model traffic — system prompts, user queries, model responses, tool call parameters — flowing across the internal network in the clear.

An attacker who achieves any foothold on the internal network — a compromised workstation, a vulnerable internal service, a rogue container — can use standard network sniffing tools to capture this traffic. ARP spoofing, DNS poisoning, or switch port mirroring provide the interception point. The attacker sees everything: the system prompt that defines the model's behavior, the user inputs that may contain sensitive data, the model responses that may contain confidential analysis, and the tool call parameters that may contain API keys and credentials.

The more sophisticated MITM attack modifies the traffic in transit. The attacker intercepts the system prompt flowing from the application to the model and injects additional instructions — a form of prompt injection delivered at the network layer rather than the application layer. The model receives the modified system prompt and follows the injected instructions. The application never detects the modification because it trusts the internal network path. This attack is especially effective against architectures where the system prompt is sent with every request rather than cached at the model serving layer, because every request creates a new injection opportunity.

Red teams test for internal MITM vulnerability by deploying a network tap or proxy on the path between the application and the model serving layer. If the traffic is readable — if you can see system prompts and model responses in cleartext — the finding is critical regardless of what other controls exist. The network is not a trust boundary. Encrypt everything, internal and external, or accept that anyone on the network can read and modify your model's behavior.

## DNS Rebinding Against Model Endpoints

**DNS rebinding** is a technique that bypasses network access controls by exploiting how browsers and HTTP clients resolve domain names. The attacker controls a domain with a short-lived DNS record. The first DNS resolution returns the attacker's server IP. After the client establishes a session, the DNS record changes to point to an internal IP — the address of your model endpoint. The client, believing it is still communicating with the attacker's domain, sends requests to the internal endpoint. Same-origin policy, which normally prevents cross-origin requests, does not block this because the domain has not changed — only its IP resolution has.

DNS rebinding became urgently relevant to AI infrastructure in 2025 when researchers demonstrated the attack against Model Context Protocol servers. MCP servers, which expose tool capabilities to AI agents over HTTP, frequently run on localhost without authentication. The attack flow is clean: the victim visits a page controlled by the attacker, the page makes requests to the attacker's domain, the DNS record flips to 127.0.0.1, and the requests now reach the local MCP server. The attacker can invoke any tool the MCP server exposes — read files, query databases, execute commands — through the victim's browser. CVE-2025-66416 was assigned to the MCP Python SDK for failing to enable DNS rebinding protection by default.

The Ollama vulnerability disclosed in 2024 as CVE-2024-28224 demonstrated the same pattern against local model serving. Ollama, one of the most popular tools for running models locally, listens on localhost by default and initially had no DNS rebinding protection. A malicious webpage could send inference requests to the local Ollama instance, extracting model responses or manipulating the model through the victim's browser.

The pattern generalizes. Any model serving endpoint that listens on localhost or an internal IP without authentication is vulnerable to DNS rebinding if a user on the same machine visits a malicious webpage. This includes Jupyter notebooks running model inference, local vLLM or text-generation-inference instances, development model servers, and any internal tool that exposes an HTTP API. Red teams should catalog every model-related endpoint that listens on localhost or internal addresses and test DNS rebinding against each one. The fix is straightforward — validate the Host header on every request and reject requests where the Host does not match the expected value — but it must be applied to every endpoint, not just the ones facing the internet.

## Rate Limiting and Throttling Bypass

Rate limiting on model endpoints serves two purposes: it prevents abuse, and it controls cost. Model inference is expensive. An attacker who bypasses rate limiting can run thousands of queries against your model, racking up compute costs, extracting training data through membership inference attacks, or brute-forcing the model's behavior through systematic prompt variation. Teams that implement rate limiting often implement it incorrectly, and the bypasses are well-known.

The most common bypass is **identity rotation**. If rate limiting is keyed to an API key or user account, the attacker creates multiple accounts and rotates between them. If rate limiting is keyed to IP address, the attacker rotates through a pool of proxy IPs. Cloud proxy services provide thousands of rotating IP addresses for dollars per month. If rate limiting is keyed to a session token, the attacker creates new sessions for each batch of requests.

**Header manipulation** bypasses rate limiters that trust forwarded headers. If the rate limiter reads the client IP from the X-Forwarded-For header — common when the model endpoint sits behind a reverse proxy — the attacker sets a different X-Forwarded-For value on each request. Each request appears to come from a different client. The rate limiter, trusting the header, applies no throttling.

**Endpoint variation** bypasses rate limiters that are applied per-path. If the rate limit is on the main completion endpoint, the attacker discovers an alternate path — a batch endpoint, a streaming endpoint, an older API version, an internal endpoint accessible through path traversal — that serves the same model without the same rate limit. API versioning is a frequent source of this bypass. The v2 endpoint has rate limiting. The v1 endpoint, which still works but was supposed to be deprecated, does not.

**Slowloris-style attacks** bypass rate limiters that count completed requests. The attacker opens many connections to the model endpoint and sends requests very slowly — fast enough to keep the connection alive, slow enough to never complete a request. The rate limiter counts zero completed requests while the model serving infrastructure dedicates resources to each open connection. For model endpoints that use streaming responses, this is especially effective because the server holds the connection open for the entire generation duration.

Red teams test rate limiting by attempting each bypass technique and documenting which ones succeed. The goal is not to generate a denial of service — it is to demonstrate that an attacker can send requests at a rate the organization did not intend to allow. Every request above the intended rate is compute cost the organization cannot control and data exposure they cannot monitor.

## Model Endpoint Fingerprinting

**Endpoint fingerprinting** identifies what model is running behind an API endpoint, what framework serves it, and what version of each component is deployed. This intelligence guides the attacker's subsequent actions. A known model version has known vulnerabilities. A known serving framework has known exploits. A known inference library has known configuration defaults that can be exploited.

The simplest fingerprinting technique is response analysis. Different models produce different distributions of tokens, different response formatting patterns, and different error messages. GPT-5 and Claude Opus 4.6 produce subtly different prose styles, punctuation preferences, and structural patterns. A systematic comparison of model responses against known model signatures can identify the model family with high confidence. Academic researchers have demonstrated model fingerprinting accuracy above 90 percent using fewer than fifty queries.

Error messages are more direct. A request that triggers an error often reveals the serving framework, the model name, and sometimes the version. Sending an oversized input might return an error that mentions "vLLM" or "Triton Inference Server" or "TGI." Sending a malformed request might return a Python traceback that reveals the library stack. Sending a request with an unsupported parameter might return an error that lists the supported parameters, revealing the API specification.

HTTP response headers leak framework information. The Server header, the X-Powered-By header, custom headers added by the serving framework, and the response Content-Type all provide identification signals. Timing analysis adds another dimension — different serving frameworks and models have characteristic latency profiles based on input length, batch size, and generation length. A fingerprinting tool that sends a calibrated set of inputs and measures the response timing can distinguish between serving frameworks with high accuracy.

For the red team, fingerprinting is the reconnaissance that makes subsequent attacks efficient. Knowing that the target runs vLLM on NVIDIA Triton behind an NGINX reverse proxy tells you exactly which vulnerabilities to test, which configuration defaults to exploit, and which attack techniques are most likely to succeed.

## Network Segmentation Testing for AI Infrastructure

Network segmentation is the practice of dividing the network into zones with controlled communication between them. For AI infrastructure, proper segmentation means the model serving layer cannot reach the training data store, the inference endpoint cannot reach the secrets manager directly, and the public-facing API gateway cannot reach the internal model registry. In theory. In practice, AI infrastructure sprawls across network boundaries because the data flows are complex and the teams that build them prioritize functionality over isolation.

Red teams test segmentation by starting from one zone and attempting to reach resources in other zones. From the public API gateway, can you reach internal model endpoints directly, bypassing the gateway's authentication and rate limiting? From a model serving pod, can you reach the training data bucket, the model registry, the secrets manager, or the Kubernetes API server? From a development notebook server, can you reach production model endpoints? Each successful cross-zone access is a segmentation failure.

The test methodology is systematic. Map every network zone that contains AI infrastructure: public-facing, API gateway, model serving, training, data storage, secrets management, monitoring, and development. For each zone, enumerate the resources within it and the intended communication paths to other zones. Then test whether unintended paths exist. Use network scanning tools from within each zone to discover accessible ports in other zones. Attempt to establish TCP connections to services in adjacent zones. Test whether DNS resolution works across zone boundaries — can a process in the model serving zone resolve hostnames in the training zone?

AI infrastructure creates unique segmentation challenges because the data flow is bidirectional and dynamic. The model serving layer needs to pull model weights from the registry. The training layer needs to push model weights to the registry. The evaluation layer needs to read from the data store and write to the results database. The monitoring layer needs to read from every other layer. Each of these legitimate flows requires a network path, and each network path is a potential lateral movement route for an attacker. The segmentation design must balance legitimate access with isolation, and the red team's job is to find every place where the balance tipped toward convenience.

The most dangerous segmentation failures in AI infrastructure involve the model registry. If the model serving layer has write access to the model registry — perhaps because the same service account is used for both pulling and pushing models — an attacker who compromises a serving node can push a poisoned model to the registry. The next deployment pulls the poisoned model into production. Network segmentation should enforce that model serving nodes can only read from the registry, never write. Red teams verify this by attempting to push an artifact to the registry from a serving node. If the push succeeds, the segmentation is broken in the most consequential way possible.

The network is the connective tissue of your AI infrastructure. Every connection that exists is a path an attacker can walk. Every connection that should not exist is a door you forgot to lock. Red teams walk every path and try every door.

The next subchapter moves from network-level concerns to the orchestration layer, examining how Kubernetes, containers, and service meshes introduce their own attack surfaces that are amplified by the unique demands of AI workloads.