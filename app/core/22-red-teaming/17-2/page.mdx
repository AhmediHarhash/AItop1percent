# 17.2 — Red-to-Blue Feedback Loops — Turning Findings into Detections

The report is the enemy.

That sounds extreme, but it is the core truth of purple teaming. The traditional red team deliverable is a report — a detailed document describing attack techniques, severity ratings, remediation recommendations, and sometimes proof-of-concept demonstrations. Reports are valuable for executive communication, audit compliance, and historical documentation. They are terrible for defensive operations. A report is a static artifact. A detection is a living defense. The moment a red team finding becomes a report instead of a detection, the clock starts ticking on how long that attack path remains open and unmonitored. In most organizations, the clock runs for months. In some, it runs forever.

## The Handoff Problem

The gap between offensive findings and defensive detections is not a technology problem. It is a workflow problem. Red teams and blue teams speak different languages, use different tools, operate on different timelines, and optimize for different outcomes. The red team's job is done when they demonstrate that an attack works. The blue team's job starts when they have something to detect. The space between "this attack works" and "we can detect this attack" is where most organizations lose the thread.

Consider what happens in a typical organization after a red team engagement against an AI system. The red team demonstrates that a customer support chatbot can be manipulated through indirect prompt injection embedded in support ticket attachments. The injected instructions cause the model to query the internal customer database with attacker-specified parameters and include the results in its response. The red team documents this in a report, assigns it a high severity rating, and recommends input sanitization for attachment processing and output monitoring for anomalous database queries.

The report lands on the blue team's desk. The blue team reads it, understands the attack conceptually, and creates a ticket: "Implement input sanitization for ticket attachments." Engineering picks up the ticket four weeks later. They add a text preprocessing step that strips known injection patterns from attachments. The blue team marks the finding as remediated. Nobody builds a detection for anomalous database queries triggered by the chatbot. Nobody monitors for the behavioral pattern of the model querying customer records in unusual volume or scope. Nobody tests whether the input sanitization actually blocks the specific injection technique the red team used. The defense is a patch, not a detection. The next time an attacker finds a different injection vector that achieves the same outcome, the monitoring pipeline is blind to it.

## The Tight Feedback Loop

The purple team alternative replaces the report-then-remediate workflow with a same-session feedback loop. The red team executes an attack. The blue team observes it in real time. Together they answer four questions immediately.

What telemetry did this attack produce? Every attack leaves traces, even if traditional tools do not capture them. Prompt injection through a ticket attachment produces a log entry showing the attachment was processed, followed by a model response that includes a tool call to the customer database with parameters that differ from the normal query pattern. The tool call log shows the anomalous parameters. The model response log shows data in the output that the user did not request. These are signals. The question is whether anyone is watching for them.

Can we detect this attack with existing monitoring? The purple team checks the current alerting configuration. Is there a rule that fires when the model makes database queries with unusual parameters after processing an attachment? In most cases, the answer is no — because nobody knew to write that rule until the red team demonstrated the attack. The gap between what the red team found and what the blue team monitors is the detection gap, and closing it is the purple team's primary function.

What detection should we build? The red team describes the attack pattern in terms the blue team can operationalize. Not "prompt injection through attachments" — that is too abstract to detect. Instead: "within sixty seconds of attachment processing, the model issues a database query where the customer ID does not match the ticket owner, or the query returns more than five records, or the query accesses fields that are not part of the standard support lookup." This level of specificity turns an attack description into a detection rule.

Does the detection actually work? The red team replays the attack. The detection fires. They adjust the rule to reduce false positives — the attachment processing latency varies, so they widen the window from sixty seconds to ninety. They test again. They run ten legitimate ticket interactions to confirm the detection does not fire on normal traffic. They adjust thresholds. They replay the attack one more time. It fires cleanly with zero false positives across the test set. The detection is ready for deployment.

This entire cycle — attack, observe, design detection, test, iterate, deploy — takes hours, not months. It is the fundamental operational unit of purple teaming.

## The Finding-to-Detection Document Format

The handoff between red and blue needs a standardized format that is richer than a report finding but more actionable than a vulnerability description. The **Attack-Detection Card** is the document that bridges the gap. Each card captures one attack technique and its corresponding detection in a format both sides can work from.

The card contains five sections. The attack description names the technique, describes the preconditions, and specifies the exact steps to reproduce it. The telemetry fingerprint lists every observable signal the attack produces — which logs, which fields, which values, which timing patterns. The detection logic defines the rule or correlation that would catch this attack, expressed in terms the SIEM or monitoring platform can consume. The validation record documents the test results: how many times the attack was replayed, how many times the detection fired correctly, what the false positive rate was against normal traffic, and what tuning was applied. The regression flag marks whether this card is included in the automated regression suite that replays attacks periodically to confirm detections still work.

Teams that adopt this format typically maintain a library of one hundred to three hundred attack-detection cards after their first year of purple teaming operations. Each card represents a closed loop: an attack that was found, a detection that was built, and a validation that was performed. The library is the purple team's primary deliverable — not a stack of reports, but a growing catalog of deployed defenses with proven effectiveness.

## Regression Testing — Ensuring Old Attacks Stay Detected

Detections rot. A detection built against one model version may miss the same attack against the next version because the model's behavior changed in ways that alter the telemetry fingerprint. A detection that fires on unusual database query parameters may stop working after the engineering team modifies the query interface and the parameter names change. A detection that monitors tool call frequency may need recalibration after a latency optimization that changes the typical call volume. If you build a detection and never test it again, you are building security theater that eventually becomes a blind spot.

Purple teams solve this with automated regression testing. The attack-detection card library is not just documentation — it is a test suite. On a regular cadence, typically weekly or after every model or system change, the regression suite replays attack techniques from the library against the current system and verifies that the corresponding detections still fire. Any detection that fails regression gets flagged for immediate review and repair. The red team re-executes the attack manually to determine whether the technique still works and, if so, what the new telemetry fingerprint looks like. The detection is updated or rebuilt. The card is updated. The loop closes.

Regression testing catches a category of failure that is invisible without it: **silent detection decay**. The detection still exists in your SIEM configuration. It has not been deleted or modified. But the signals it watches for no longer appear because the underlying system changed. Without regression testing, you discover this decay the hard way — when an attacker uses the technique in production and no alert fires. With regression testing, you discover it during the next scheduled test cycle and fix it before it matters.

A well-run AI purple team runs regression against its full detection library every two weeks and runs targeted regression within forty-eight hours of any model update, prompt change, or tool integration modification. This cadence keeps detections aligned with the system as it evolves, which is the entire point of purple teaming over traditional periodic assessments.

## Common Failure Modes in Feedback Loops

Three failure modes kill feedback loops even in organizations that intend to practice purple teaming.

The first is **findings without telemetry**. The red team documents an attack but nobody identifies what observable signals the attack produces. The finding is real, but the blue team cannot build a detection because they do not know what to look for. This happens when the red team lacks visibility into the system's logging and telemetry, which means the red team needs access to the same dashboards, log streams, and monitoring tools that the blue team uses. Purple teaming requires shared tooling, not shared meetings.

The second is **detections without validation**. The blue team builds a detection rule based on the attack description, but nobody confirms it works. The rule is deployed, the card is filed, and everyone assumes the detection is active. Two months later, an attacker uses the technique and the detection does not fire because the rule had a logic error, or the log format changed, or the threshold was set too conservatively. Every detection must be tested with an actual attack replay before it counts as deployed.

The third is **one-shot validation without regression**. The detection worked when it was built. It was never tested again. Over time, system changes caused the detection to drift into uselessness. This is the silent detection decay problem, and the only solution is periodic regression. Teams that skip regression are not doing purple teaming. They are doing a slightly more collaborative version of red-then-blue.

## Measuring Feedback Loop Health

The purple team tracks three metrics to assess whether its feedback loop is healthy. **Detection coverage ratio** is the percentage of red team findings that have a corresponding deployed and validated detection. A healthy purple team maintains coverage above 85 percent. Findings below 100 percent represent techniques where detection is technically infeasible — some attacks genuinely produce no distinguishable telemetry — but those should be rare exceptions with documented justification.

**Mean time to detection deployment** is the average elapsed time from attack demonstration to deployed detection in production. As discussed in the previous subchapter, elite teams target under forty-eight hours for critical findings. Organizations tracking this metric for the first time often discover their baseline is measured in weeks, which is itself a valuable finding.

**Regression pass rate** is the percentage of detection cards that pass their most recent regression test. A healthy purple team maintains a pass rate above 90 percent. A pass rate below 80 percent indicates that system changes are outpacing detection maintenance, and the team needs to either increase regression frequency or invest more capacity in detection upkeep.

These three metrics — coverage, speed, and durability — tell you whether your purple team is closing the loop, closing it fast, and keeping it closed. Any team that tracks all three is doing real purple teaming. Any team that tracks none of them has a collaboration initiative, not an operational function.

Deploying detections is only half the battle. When a detection fires in production, the security operations center needs to know what it means and what to do about it. The next subchapter covers SOC playbooks for AI-specific incidents — the response procedures that turn a fired alert into effective containment.
