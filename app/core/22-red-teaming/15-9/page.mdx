# 15.9 — GPU and Compute Infrastructure Attacks

Every model you serve runs on hardware. That hardware has memory, buses, scheduling queues, and thermal characteristics. And every one of those physical properties leaks information if you know how to listen. Most security teams treat the GPU as a black box — they secure the application layer, the network, the orchestration platform, and they stop there. The GPU is just "compute." It runs the math. It does not need its own threat model. This assumption is wrong, and in multi-tenant cloud environments where your inference workloads share silicon with unknown neighbors, it is dangerously wrong.

In January 2024, Trail of Bits disclosed **LeftoverLocals**, a vulnerability affecting AMD, Apple, and Qualcomm GPUs that allowed a malicious process running on the same GPU to read local memory left behind by a previous kernel execution. On an AMD Radeon RX 7900 XT running llama.cpp, the researchers recovered up to 181 megabytes of data per query — enough to reconstruct the full text of an LLM's response with high accuracy. The vulnerability existed because GPU local memory was not cleared between kernel executions. One process finished, another started, and the second process could read whatever the first had left behind. This is not exotic. This is how shared hardware works when nobody designs isolation into the memory lifecycle.

## The Multi-Tenant GPU Problem

When you run model inference on a cloud GPU instance, you assume your workload is isolated. In dedicated instances, it mostly is — you rent the entire GPU, and no other tenant's code runs on the same silicon. But dedicated GPU instances are expensive. To reduce cost, cloud providers and internal platforms increasingly offer shared GPU environments where multiple workloads share the same physical GPU through time-slicing, multi-instance GPU partitioning, or virtual GPU technologies.

NVIDIA's Multi-Instance GPU, introduced with the A100 and extended through the H100 and B200 architectures, partitions a single physical GPU into up to seven isolated instances. Each instance gets its own memory, cache, and streaming multiprocessors. MIG provides hardware-level isolation that is significantly stronger than software time-slicing. But MIG is not the default. It requires explicit configuration. Many shared GPU environments still use time-slicing, where workloads alternate on the same GPU cores and share the same memory hierarchy. In time-sliced environments, the isolation depends entirely on the GPU driver and the operating system clearing memory between context switches. LeftoverLocals proved that this clearing does not always happen.

The red team implication is direct. If your organization runs model inference on shared GPU infrastructure — whether in a public cloud, an internal multi-tenant cluster, or a managed inference platform — you need to test whether one tenant's data is accessible to another. The test is straightforward in concept: deploy a listening process on the same GPU as the target workload, run inference on the target, and check whether the listener can recover any data from GPU memory. The execution requires GPU programming knowledge, but the principle is the same as any shared-resource side-channel attack.

## GPU Memory Residuals and Data Exfiltration

GPU memory is allocated and deallocated constantly during inference. Input tensors, intermediate activations, attention key-value caches, and output logits all occupy GPU memory for the duration of a request and are supposed to be freed afterward. But "freed" in GPU memory management typically means "marked as available for reuse," not "overwritten with zeros." The data persists in memory until another allocation overwrites it.

This creates a window. Between the time a memory region is freed and the time it is reused, the data it contained is accessible to any process that can read that memory region. In a single-tenant environment, the only process that could read it is another part of your own application — no meaningful risk. In a multi-tenant environment, another tenant's process might be allocated the same memory region and find your data waiting there.

The severity depends on what was in memory. If your model processes medical records, the residual memory might contain patient data. If your model handles financial transactions, the residuals might contain account numbers. If your model uses a system prompt with embedded credentials, the key-value cache in GPU memory might contain those credentials in a recoverable form. The LeftoverLocals researchers demonstrated that the recovered data was not random noise — it was structured, interpretable, and sufficient to reconstruct the model's output.

Red teams testing for GPU memory residuals should focus on the gap between allocation and clearing. Deploy a workload that allocates GPU memory immediately after the target workload frees it, then inspect the contents. If you find structured data — tensors with non-random values, text fragments, recognizable patterns — the isolation is broken. Report the finding with the specific GPU partitioning mode, the driver version, and the time window between free and reallocation. Vendors need this specificity to reproduce and fix the issue.

## Side-Channel Attacks on GPU Inference

Beyond direct memory access, GPUs leak information through side channels — observable properties of computation that correlate with the data being processed. The three primary GPU side channels are timing, power consumption, and electromagnetic emissions.

**Timing side channels** exploit the fact that different inputs cause different execution times. In model inference, the time to process a request varies based on input length, sequence complexity, and which execution paths the model takes. An attacker who can measure the inference latency of your model endpoint — which is often exposed through the API response time — can infer properties of the input or output. For transformer models, the attention computation time correlates with sequence length. If your model serves responses of varying sensitivity levels, and sensitive responses tend to be longer, the timing alone reveals the sensitivity classification.

**Power side channels** are more hardware-dependent. GPU power consumption varies with the computation being performed. A floating-point multiplication consumes different power than an addition. A memory access to a cached address consumes different power than a cache miss. In physical-access scenarios — such as a colocation facility where the attacker has hardware adjacent to the target — power monitoring can reveal computation patterns. In cloud environments, the attacker does not have direct power measurement, but shared power delivery infrastructure can create observable correlations. This is an active area of academic research, and while practical cloud-based GPU power side channels have not been demonstrated at scale, the theoretical foundation is well-established.

**Contention-based side channels** are the most practical threat in cloud environments. When two workloads share GPU resources — cache, memory bus, interconnect bandwidth — one workload's activity affects the other's performance. The attacker runs a workload designed to be sensitive to contention on a specific shared resource, then observes how the target workload's activity modulates the attacker's performance. By carefully designing the attacker workload, it is possible to infer properties of the target's computation based on the contention patterns. University of Texas researchers demonstrated in 2025 that contention-based side channels on GPU shared memory could distinguish between different model architectures running on the same GPU, which has direct implications for model fingerprinting and intellectual property theft.

## Compute Resource Hijacking

Not every GPU attack is about stealing data. Some are about stealing the GPU itself. **Compute hijacking** — using your GPU infrastructure for unauthorized purposes — is one of the fastest-growing attack categories in cloud environments. The most common form is cryptomining, where an attacker deploys a cryptocurrency miner on your GPU instances and bills the electricity to you.

The economics make this attractive. A single NVIDIA H100 instance costs between twenty-five and forty dollars per hour on major cloud providers. An attacker who compromises ten GPU instances and runs them for a week generates compute bills between forty-two thousand and sixty-seven thousand dollars. Microsoft documented cryptojacking campaigns that incurred compute charges exceeding three hundred thousand dollars against compromised cloud accounts. The attacker bears no cost. You receive the bill.

GPU cryptomining is harder to detect than CPU cryptomining because AI workloads legitimately consume 100 percent of GPU resources. A CPU cryptominer stands out because most servers do not sustain 100 percent CPU usage. A GPU cryptominer looks like normal AI inference — high GPU utilization, high memory usage, high power consumption. The difference is invisible at the hardware metrics level.

Detection requires looking at what the GPU is doing, not how busy it is. Monitor for processes that were not deployed through your standard deployment pipeline. Track which container images are running on GPU instances and verify them against your approved registry. Watch for network connections to mining pool addresses. Check whether the workload's GPU memory access patterns match the expected inference profile — mining uses the GPU very differently from transformer inference, and the memory allocation patterns diverge. Some organizations deploy runtime behavioral analysis tools that fingerprint expected GPU workload patterns and alert on deviations.

The more sophisticated threat is unauthorized model training. An attacker who compromises your GPU cluster does not just mine cryptocurrency — they train their own models on your hardware. This is more difficult to detect than cryptomining because the GPU usage patterns of unauthorized training look similar to authorized training. The attacker provisions their training job through the same scheduler, uses the same GPU drivers, and generates the same telemetry. The only difference is that the training job is not in your project queue and the resulting model weights are exfiltrated to an external destination. Red teams test for this by checking whether arbitrary workloads can be scheduled on the GPU cluster without going through the approved workflow, and whether the network allows model weight files to be transferred to unapproved destinations.

## Cloud GPU Instance Metadata Attacks

Every cloud GPU instance has an instance metadata service — a local HTTP endpoint that provides information about the instance, including its identity, network configuration, and attached credentials. Instance metadata services are the most commonly exploited entry point in cloud compromises because they are accessible from within the instance without authentication and they provide credentials that often have broad permissions.

For GPU instances, the metadata service is particularly dangerous because GPU instances tend to have elevated permissions. They need access to model registries to pull model weights. They need access to storage buckets to read training data. They need access to secrets managers to retrieve API keys. These permissions are attached to the instance's service account and available through the metadata service. An attacker who achieves code execution on a GPU instance — through a vulnerability in the model serving framework, a container escape, or a compromised model artifact — can query the metadata service and obtain credentials for all of these resources.

The metadata service is accessible at a well-known URL on all major cloud providers. Version 1 of the AWS Instance Metadata Service required no authentication — a simple HTTP GET returned the credentials. Version 2 added a session token requirement, but many organizations have not enforced IMDSv2 on their GPU instances because the migration requires updating all software that queries the metadata service. Google Cloud and Azure have their own metadata endpoints with varying levels of protection. Red teams should always attempt metadata service access from within any compromised container or instance, because the credentials obtained from metadata often provide the lateral movement path to the rest of the infrastructure.

In AI-specific contexts, the metadata credentials often grant access to the model registry. With model registry access, the attacker can download proprietary model weights — stealing intellectual property — or upload poisoned model weights that will be served in the next deployment. The metadata service, designed as a convenience feature, becomes the bridge from compute compromise to model supply chain compromise.

## Testing Methodology for Compute Infrastructure

A systematic red team assessment of GPU and compute infrastructure covers five layers.

The first layer is **tenant isolation testing**. If the infrastructure is multi-tenant, test whether one tenant can observe, access, or influence another tenant's workload. Deploy processes that attempt to read GPU memory after other workloads complete. Measure whether one workload's performance correlates with another workload's activity. Test the boundaries of whatever partitioning technology is in use — MIG, vGPU, time-slicing — by attempting cross-partition access.

The second layer is **privilege escalation from inference**. Start with the access level of a model serving process and attempt to escalate. Can the inference container access the host filesystem? Can it communicate with the Kubernetes API server? Can it query the instance metadata service? Can it access other containers on the same node? The NVIDIA Container Toolkit vulnerability CVE-2024-0132, discovered by Wiz and assigned a CVSS score of 9.0, demonstrated that a specifically crafted container image could escape the container and access the host filesystem. Wiz found that 33 percent of cloud environments had a vulnerable version installed. A follow-up in 2025 revealed the initial patch was incomplete, and a new CVE, CVE-2025-23266, was assigned for the bypass. This is not theoretical. Container escapes from GPU workloads are actively exploitable.

The third layer is **resource hijacking detection**. Attempt to deploy unauthorized workloads on the GPU cluster. Test whether the scheduler enforces workload provenance — does it verify that the container image comes from the approved registry, that the deployment was initiated by an authorized pipeline, that the workload matches a registered project? If you can schedule a cryptominer on the cluster through the same API that schedules legitimate inference, the cluster lacks workload provenance controls.

The fourth layer is **metadata and credential harvesting**. From within a GPU instance or container, attempt to access the cloud metadata service. Document what credentials are available and what resources they grant access to. Test whether those credentials allow access to model registries, training data stores, secrets managers, or other GPU instances. Map the blast radius of a single instance compromise.

The fifth layer is **data residual analysis**. After workloads complete, examine GPU memory for residual data. This requires GPU programming tools and access to the GPU's memory management interfaces. The goal is not to demonstrate a specific exploit but to assess whether the environment's memory lifecycle clears sensitive data reliably. If residuals are recoverable, the finding applies to every workload that has ever run on that hardware.

Every GPU is a computer within a computer. It has its own memory, its own execution units, its own scheduling, and its own attack surface. The red team that stops at the container boundary misses the vulnerabilities that live in the silicon beneath it.

The next subchapter moves from the compute layer to the network layer, examining how attackers discover, fingerprint, and exploit the network paths that connect your model endpoints to the world.