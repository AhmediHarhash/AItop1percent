# 12.10 — Threat Intelligence for AI Systems

Threat intelligence informs where to focus. Every AI system faces infinite possible attacks—you cannot test them all. Threat intelligence answers: which attacks are actually being attempted in the wild? Which vulnerabilities are actively exploited? Which attack techniques are spreading through underground forums? Which nation-state actors are developing AI-specific capabilities? Without intelligence, red teaming is guesswork. With intelligence, red teaming becomes targeted, prioritized, and aligned with real-world threats.

Traditional threat intelligence for software vulnerabilities is well-established. CVE databases, vendor advisories, threat feeds from security companies. AI threat intelligence in 2026 is younger but rapidly maturing. The sources are different, the indicators are different, the sharing mechanisms are different—but the fundamental purpose is the same. You need to know what adversaries are doing so you can test your defenses against actual attacks, not hypothetical ones.

## What Threat Intelligence Means for AI

AI threat intelligence spans multiple layers. Infrastructure-level threats target the compute, network, and storage underlying AI systems—these look like traditional IT security threats. Application-level threats exploit AI-specific attack surfaces—prompt injection, data extraction, model poisoning. Adversarial threats target model behavior directly—jailbreaks, alignment manipulation, adversarial examples. Each layer requires different intelligence sources and different defensive responses.

The challenge is that AI threats evolve faster than traditional software threats. A new jailbreak technique can spread through social media in hours and be deployed against production systems the same day. Defensive patches that work Monday might fail Tuesday after attackers iterate. Threat intelligence for AI needs to operate at this velocity to remain relevant.

Priority shifts based on your system's threat model. A customer service chatbot primarily faces prompt injection and content filter bypass attempts. A code generation tool faces malicious output generation and training data extraction. A recommendation engine faces model poisoning through adversarial inputs. Generic threat intelligence helps—everyone should know about major new attack classes—but tailored intelligence filtered for your specific attack surface delivers the most value.

## Sources of AI Threat Intelligence

Public research is a primary source. Academic papers, conference presentations, and preprints describe new attack techniques often months before they appear in production exploitation. A team that monitors ML security conferences—NeurIPS, ICML, ICLR, USENIX Security—and relevant arXiv categories gets advance warning of emerging threats. The gap between publication and widespread exploitation varies, but it is rarely zero. That gap is your window to test defenses and harden systems.

Social media and forums surface practical exploitation techniques. Security researchers share jailbreak prompts on Twitter, Discord servers discuss data extraction methods, Reddit threads explore alignment bypasses. These sources are noisier than academic research but faster and closer to what attackers actually use. Monitoring requires filtering—most discussions are theoretical or low-quality—but signal exists. Several companies in 2026 employ analysts who do nothing but monitor social channels for AI security discussions and escalate credible threats to red teams.

Bug bounty submissions from your own program provide tailored threat intelligence. Every submission—even invalid ones—reveals what attackers are attempting. Patterns in submissions indicate which attack classes are trending, which techniques are spreading, which vulnerabilities researchers believe might exist. If you receive 40 prompt injection submissions in one month versus your typical 10, something shifted in the threat landscape. Maybe a new technique was published. Maybe a competitor's vulnerability disclosure made researchers think similar issues exist in your system. The volume and focus of submissions is intelligence.

## Attack Pattern Databases

Structured databases of AI attack patterns help teams catalog and track threats. MITRE ATLAS—Adversarial Threat Landscape for AI Systems—launched in 2024 and by 2026 catalogs over 200 AI-specific attack techniques across 14 tactics. Techniques are mapped to real-world incidents where available, which helps teams understand not just theoretical attacks but actual exploitation patterns.

Internal attack catalogs complement public databases. As your red team discovers vulnerabilities or as bug bounty researchers report issues, cataloging them systematically creates institutional knowledge. What attacks have we seen? Which succeeded? Which were mitigated and how? Which remain theoretical but concerning? Over time, this catalog becomes your organization's threat intelligence database, tailored to your specific systems and prioritized by your actual experiences.

Sharing attack patterns across teams prevents repeated discoveries. If your chatbot team found a prompt injection variant that bypasses filters, does your code generation team know? Does your recommendation team? Internal threat intelligence sharing ensures that lessons learned in one product inform security testing in others. This requires tooling—a shared wiki, a threat database, regular security briefings—but the alternative is rediscovering the same vulnerabilities across different systems.

## Industry Sharing Groups

Formal and informal industry sharing groups exchange threat intelligence among organizations. Information Sharing and Analysis Centers—ISACs—exist for many industries. Financial services, healthcare, energy, retail. Some ISACs began including AI threat intelligence in 2025 as members deployed more AI systems and encountered security incidents.

AI-specific sharing groups emerged as the threat landscape matured. The AI Security Consortium, formed in late 2024 by major AI providers and enterprise adopters, shares anonymized incident reports, attack techniques, and defensive measures. Participation requires reciprocity—you receive intelligence but must also contribute findings. The value is collective: no single organization sees the full threat landscape, but pooled intelligence provides broader visibility.

Trust and legal frameworks enable sharing. Participants need confidence that shared intelligence will not be misused, disclosed publicly without consent, or weaponized against contributors. Legal agreements—NDAs, information sharing agreements, antitrust protections—provide the framework. Technical measures—anonymization, aggregation, controlled access—protect sensitive details. When these elements work, sharing groups significantly enhance every participant's threat intelligence capability.

## Vendor Threat Feeds

Commercial threat intelligence vendors now offer AI-specific feeds. These aggregate data from multiple sources—honeypots, researcher networks, dark web monitoring, client telemetry—and deliver structured intelligence on emerging threats, active campaigns, and exploited vulnerabilities.

Vendor feeds vary in quality and relevance. Some focus on infrastructure threats that happen to target AI systems—DDoS against inference APIs, credential theft for model access. These are useful but generic. Others focus on AI-specific threats—new jailbreak patterns, training data extraction techniques, model poisoning indicators. These are more valuable but harder to operationalize because detection and mitigation often require custom development.

Operationalizing threat feeds requires integration with existing security tooling. Feeds that deliver indicators of compromise—specific prompt patterns, API abuse signatures, suspicious query sequences—can feed directly into monitoring systems. Feeds that deliver strategic intelligence—new attack classes, adversary capabilities, technique trends—inform red team planning and defensive roadmaps but do not automate directly. Both types have value. The key is understanding what you are receiving and how to use it.

## Building Internal Threat Knowledge

Your own production telemetry is the richest source of threat intelligence about your specific systems. What are users actually doing? Which prompts trigger safety filters most often? Which API patterns correlate with abuse? Which retrieval queries produce unexpected results? Production data reveals adversarial probing, exploitation attempts, and emergent attack patterns that external intelligence sources cannot see.

Honeypots and decoy systems attract and reveal attacker behavior. Deploy a deliberately vulnerable test endpoint alongside production systems. Advertise it subtly—documentation, developer forums, conference talks. Monitor what happens. Attackers who discover the honeypot will attempt exploitation, revealing their techniques, tools, and objectives. Honeypot telemetry becomes threat intelligence that informs defenses for production systems. A SaaS AI platform ran honeypots that mimicked three different model architectures and observed distinct attack patterns against each, which shaped their defensive strategies for actual deployments.

Post-incident analysis generates threat intelligence for future defense. After every security incident—successful attack, near-miss, or red team exercise—conduct structured analysis. What techniques were used? What vulnerabilities were exploited? What detections failed? What worked? Document findings systematically and share them across security, engineering, and operations teams. Incident reports become case studies that inform future red teaming priorities and defensive investments.

## Operationalizing Intelligence

Threat intelligence without action is expensive noise. Operationalizing intelligence means connecting what you learn about threats to what you test, monitor, and defend against.

Intelligence-driven red teaming prioritizes adversarial testing based on current threats. If intelligence indicates that a new prompt injection technique is spreading, red teams add that technique to their testing repertoire immediately. If a specific jailbreak pattern is being discussed in researcher forums, it becomes a test case. If industry sharing groups report attacks on similar systems, those attacks inform your testing priorities. Red teaming evolves continuously in response to threat landscape changes instead of running the same tests quarter after quarter.

Defensive roadmaps align with intelligence about emerging threats. If intelligence suggests that multi-turn jailbreaks are becoming more sophisticated, that informs investment in conversation-level safety monitoring. If training data extraction techniques are advancing, that drives prioritization of differential privacy or output filtering. Threat intelligence shapes not just testing but also engineering priorities.

Detection engineering uses intelligence to build monitors for known attack patterns. If intelligence identifies specific prompt structures used in injection attacks, those structures become detection signatures. If specific API call sequences indicate abuse, those sequences become monitoring rules. Intelligence translates into detection logic that runs continuously in production, identifying threats in real time rather than waiting for incidents.

## Intelligence-Driven Red Teaming

The best red teams in 2026 do not operate in a vacuum. They consume threat intelligence continuously and adjust their testing approach in response. This creates a feedback loop: intelligence informs testing, testing discovers vulnerabilities, vulnerability analysis generates more intelligence.

Weekly intelligence reviews ensure red teams stay current. The team spends 30 to 60 minutes reviewing new academic papers, industry disclosures, vendor intelligence feeds, bug bounty submissions, and production telemetry. They identify new techniques to test, emerging attack classes to explore, and trending threats that warrant prioritization. This review directly shapes the next week's testing plan.

Scenario-based exercises driven by intelligence simulate realistic attacks. Instead of generic "try to break the system" testing, red teams execute scenarios based on real adversary tactics. If intelligence indicates that financial fraud actors are using prompt injection to manipulate AI assistants, the red team simulates that attack chain against your financial AI product. If nation-state APT groups are developing AI data extraction capabilities, the red team tests whether your system is vulnerable to those techniques.

Collaboration between intelligence analysts and red teams creates force multiplication. Analysts understand the threat landscape broadly but may not know how to exploit specific vulnerabilities. Red teamers understand exploitation deeply but may not track the broader threat landscape. When they work together—analysts identifying threats and red teamers testing them—you get both breadth and depth. Some organizations embed threat intelligence analysts directly in red team operations for this reason.

Threat intelligence transforms red teaming from periodic security theater into continuous, adversary-informed defense. You stop testing what you think might be vulnerable and start testing what adversaries are actually targeting. You stop building defenses against hypothetical attacks and start hardening against real ones. The investment in intelligence capability—people, tools, processes, industry relationships—pays off in more effective security testing and better-prioritized defensive engineering.

Next, we examine the red team maturity model—how red teaming capability evolves over time and how to assess and improve your organization's maturity.
