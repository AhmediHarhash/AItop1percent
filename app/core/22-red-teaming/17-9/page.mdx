# 17.9 — AI Security CTFs and Training Exercises

Why do most purple teams plateau after their first year? Not because they run out of attack techniques — the adversarial landscape generates new ones faster than any team can implement them. They plateau because the team stops growing. The people doing the work in month fourteen are using the same mental models they developed in month three. New hires learn the team's existing playbook but never challenge it. The red team operator who discovered a novel multi-turn extraction chain last year now runs the same chain against every new model deployment because it reliably produces findings. The blue team analyst who built the first injection classifier never learns to think like the attacker who will evade it. Skills ossify. Creativity dies. And when a genuinely novel attack technique emerges — one that does not fit any existing pattern — the team cannot see it because they have been training on their own past work, not on the frontier of what is possible. Capture-the-flag competitions and structured training exercises are the antidote. They force practitioners out of their operational comfort zone, expose them to attack techniques and defensive approaches they would never encounter in their own systems, and build the adversarial creativity that separates a functional purple team from an exceptional one.

## What Makes AI Security CTFs Different from Traditional Security CTFs

Traditional cybersecurity CTFs have operated for decades on a stable model: exploit a binary vulnerability, capture a flag string, submit it for points. The challenges have well-defined boundaries. A buffer overflow either works or it does not. A SQL injection either returns the database contents or it fails. The attacker's success is binary and verifiable.

AI security CTFs operate in a fundamentally different space because the attack surface is semantic, not syntactic. Prompt injection does not exploit a parsing bug in deterministic code. It exploits the model's tendency to follow instructions regardless of their source. Jailbreaking does not bypass an access control check. It persuades the model to reinterpret its constraints. Data extraction does not read a file from disk. It coaxes the model into revealing information it was instructed to protect. These attacks succeed on a spectrum rather than a binary — the model might partially reveal a secret, or follow an injected instruction in spirit but not in letter, or produce output that technically complies with its safety guidelines while clearly violating their intent. This semantic ambiguity makes AI CTFs harder to design, harder to score, and more valuable as training exercises because they force participants to think like the model thinks, not like a machine executes.

By 2026, the AI CTF ecosystem has matured from a handful of proof-of-concept games into a structured competitive landscape with platforms, scoring frameworks, and corporate adoption. What started with Lakera's Gandalf — a deceptively simple game where you persuade an LLM to reveal a secret password through progressively hardened defenses — has expanded into multi-category competitions that test the full spectrum of AI adversarial skills.

## The AI CTF Challenge Landscape

The challenge categories in a modern AI security CTF reflect the real attack surface of production AI systems, and understanding them helps you design internal exercises that build the specific skills your team needs.

**Prompt injection challenges** are the foundational category. The participant faces a model with instructions to perform a specific task and must craft inputs that override those instructions. Difficulty scales across levels: basic direct injection at level one, where the model has no defenses; keyword-filtered injection at mid-levels, where obvious override phrases are blocked; and defense-in-depth injection at advanced levels, where input classifiers, output filters, and behavioral constraints all operate simultaneously. Lakera's Gandalf pioneered this progressive difficulty model with seven levels that mirror real-world defense maturation, and the approach has become standard across the ecosystem. The educational value is not in solving any single level — it is in experiencing the escalation from trivial to genuinely difficult and developing the creative persistence that real adversarial work demands.

**Secret extraction challenges** task participants with extracting a specific piece of information — a password, a flag string, a system prompt fragment — from a defended model. These differ from injection challenges because the objective is information retrieval, not behavior modification. The model might be perfectly willing to follow its instructions while still leaking the secret through indirect channels: describing the secret's characteristics without stating it directly, encoding it in the first letters of each sentence, or revealing it when asked to translate its instructions into another language. Wiz's Prompt Airlines CTF took this approach by tasking participants with tricking an airline chatbot into granting a free ticket — a realistic scenario that combines extraction with practical impact.

**Tool abuse challenges** give the model access to simulated tools — databases, APIs, file systems — and task participants with making the model use those tools in unintended ways. Retrieve records the user should not access. Call an API with parameters that exceed authorized scope. Chain multiple tool calls to escalate from low-privilege to high-privilege access. These challenges build the specific skills needed for testing agentic AI systems where tool use is the primary attack surface.

**Jailbreaking challenges** focus on bypassing the model's safety alignment. Produce content the model is instructed to refuse. Make the model adopt a persona that ignores its constraints. Convince the model that its safety guidelines do not apply in the current context. HackAPrompt, run by the AI security research community, built an entire competition around this category with thousands of participants submitting hundreds of thousands of prompts, revealing that approximately 1.3 percent of submissions successfully extracted the target across all difficulty levels — a number that sounds small until you realize it represents thousands of successful bypasses discovered by crowd-sourced creativity.

**Agent hijacking challenges** are the most recent category, emerging as agentic AI deployments proliferate. The participant interacts with an AI agent that has a defined workflow — book a meeting, research a topic, process a document — and must redirect the agent to perform an entirely different task without the agent's orchestration system detecting the deviation. These challenges build skills directly relevant to the agent security testing covered earlier in this section.

## Designing Internal AI Security CTF Events

Public competitions build individual skills. Internal CTFs build team capability. The distinction matters because your purple team's effectiveness depends not just on what each person can do individually but on how they collaborate, communicate findings, and translate discoveries into defensive improvements.

An effective internal AI CTF runs against your own systems — not generic challenge platforms — using your actual models, your actual prompts, your actual tools, and your actual defenses. This means participants discover vulnerabilities that are directly relevant to your production security posture, and every finding from the exercise feeds immediately into your remediation backlog.

Design the event around three to five challenge tracks that map to your system's actual attack surface. If your production system uses RAG, include a retrieval poisoning track. If it has tool access, include a tool abuse track. If it serves multiple tenants, include a cross-tenant isolation track. Generic challenges from public platforms teach generic skills. Challenges built on your own architecture teach your team where your own defenses fail.

Set the duration based on team size and challenge complexity. A half-day event with three to five challenges works for teams of four to eight people. A full-day event with eight to twelve challenges suits larger teams or cross-functional participation where developers, security engineers, and ML engineers all compete. Multi-day events risk participant fatigue — the creativity that makes CTF challenges valuable degrades sharply after six to eight hours of concentrated adversarial work.

## Scoring and Difficulty Calibration

Scoring AI CTFs is harder than scoring traditional CTFs because success is often partial and subjective. A prompt injection that causes the model to reveal 60 percent of its system prompt is a partial success. A jailbreak that produces content that is close to but not exactly the target output requires judgment. A tool abuse that accesses an adjacent scope rather than the target scope is a near-miss that still demonstrates a vulnerability.

The most effective scoring model uses tiered points with partial credit. Full points for complete objective achievement — the flag is extracted verbatim, the model performs the exact unauthorized action, the safety bypass produces the exact target content. Partial points for demonstrating the vulnerability without achieving the full objective — the model acknowledges the secret's existence without revealing it, the model attempts the unauthorized action but is blocked by a secondary control, the safety bypass produces related but not targeted content. No points for attempts that produce no observable impact on model behavior. This tiered approach rewards creative problem-solving and prevents the frustration of all-or-nothing scoring where participants who discover real vulnerabilities receive zero credit because they fell short of the exact flag.

Difficulty calibration follows a pyramid structure. Approximately 40 percent of challenges should be solvable by any participant with basic AI security knowledge — these build confidence and engagement. Another 35 percent should require intermediate skills — knowledge of encoding tricks, multi-turn strategies, and indirect approaches. The remaining 25 percent should challenge even experienced practitioners — novel defense architectures, multi-layered protections, and challenges that require combining techniques in ways the designer did not anticipate. If more than 80 percent of challenges are solved by the end of the event, the calibration was too easy. If fewer than 30 percent are solved, it was too hard. The sweet spot is 50 to 70 percent solve rate across all challenges, with the hardest challenges having a 5 to 15 percent solve rate.

## Real Platforms and Communities

The public AI CTF ecosystem provides both training grounds for individual practitioners and inspiration for internal event design.

Lakera's Gandalf remains the most widely used entry point, with its progressive difficulty model serving as effective onboarding for anyone new to prompt injection. The game's seven levels teach escalating attack sophistication from naive direct injection through encoding-based evasion to advanced multi-turn extraction. By 2026, Gandalf has expanded beyond its original password-extraction format to include agent-specific challenges where participants must hijack an AI agent's workflow through its tool-use interface.

AI Goat, an open-source project, provides vulnerable LLM challenges that run entirely locally — no cloud accounts, no API costs, no data leaving your network. This makes it particularly valuable for organizations with strict data handling requirements who cannot use cloud-hosted CTF platforms for training.

DEF CON's AI Village has hosted competitive AI security events since 2023, growing from an experimental side event into one of the conference's most attended villages. The 2025 AI Cyber Challenge, co-sponsored by DARPA with 8.5 million dollars in prizes, demonstrated the scale that AI security competitions have reached. These events produce public writeups and technique catalogs that serve as curriculum material for internal training programs.

The Generative AI Red Teaming Challenge, organized by Humane Intelligence, takes a different approach by focusing on bias, safety, and societal harms rather than purely technical exploitation. These broader challenges build a different skill set — the ability to identify harmful outputs that are technically correct but socially damaging, a skill increasingly relevant as regulatory frameworks like the EU AI Act require testing for bias and discrimination.

## CTFs as Hiring and Skill Assessment Tools

A candidate's CTF performance reveals capabilities that interviews and resume reviews cannot assess. Can they think creatively under constraints? Do they persist through failed attempts or abandon the approach? Can they articulate why a technique works, not just that it works? Do they discover techniques the challenge designer did not anticipate?

Several organizations by 2026 use AI CTF challenges as a formal component of their security hiring process. The format typically involves a take-home challenge set with three to five problems of escalating difficulty, followed by a live session where the candidate walks through their approach, explains the techniques they tried (including the ones that failed), and discusses how they would translate their findings into defensive recommendations. The live walkthrough matters more than the solve rate because it reveals the candidate's analytical depth. A candidate who solves two challenges and can explain the defensive implications of each finding is more valuable than a candidate who solves four challenges through trial and error but cannot articulate why their techniques worked.

For existing team members, periodic CTF performance tracks skill development over time. A practitioner who solves only basic injection challenges in their first quarter but progresses to tool abuse and agent hijacking challenges by their third quarter is developing the skills the team needs. A practitioner whose solve profile has not changed in six months may need targeted training in the specific areas where their skills have plateaued.

## From CTF Findings to Production Hardening

The most common failure in CTF programs is treating them as entertainment rather than intelligence. The team has a fun day. People learn some tricks. Everyone goes back to their regular work. Nothing changes in the production defense stack. This turns a potentially high-value exercise into an expensive team-building event.

The bridge from CTF to production hardening is a structured findings review that occurs within forty-eight hours of the event. Every successful challenge solution represents a technique that worked against a defense. If the CTF challenges were built on your own systems — as recommended above — every solution is a confirmed vulnerability. The review catalogs each technique, maps it to the defense it bypassed, and generates either a detection engineering ticket (build a detection for this technique) or a mitigation engineering ticket (harden the defense that this technique bypassed).

Track the conversion rate: what percentage of CTF findings result in deployed production improvements within thirty days? A healthy program converts 60 to 80 percent. Below 40 percent, the CTF is generating findings faster than the organization absorbs them, which means either the remediation pipeline needs more capacity or the findings are not being prioritized correctly.

The best CTF programs create a flywheel. Findings from one event drive defensive improvements. The next event tests whether those improvements hold. New techniques that bypass the improved defenses drive the next round of improvements. Over multiple cycles, the defense stack hardens against an increasingly sophisticated set of attacks, and the team develops the adversarial creativity and defensive engineering skills that cannot be learned from documentation alone.

CTF exercises build individual and team skills. But skills alone do not make a purple team effective. The organizational structure, cadence, tooling, and communication protocols that govern how those skills are applied day to day determine whether the team produces real security outcomes or just impressive exercise reports. The next subchapter covers the purple team operating model — the organizational machinery that turns adversarial talent into operational defense.