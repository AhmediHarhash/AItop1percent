# 8.1 — Why Agent Red-Teaming Is Different

The security team had tested everything. They ran thousands of adversarial prompts against the customer service agent. They verified tool-calling permissions. They validated every API endpoint the agent could access. They tested rate limits, input sanitization, and output filtering. Every component passed. In September 2025, they deployed the agent to production. Within four days, a user discovered they could manipulate the agent into making unauthorized refunds by structuring a conversation across three sessions, using the agent's memory of previous interactions to gradually shift its interpretation of company policy. No single message triggered a filter. The attack existed in the trajectory, not in any individual turn.

Component testing finds component failures. Agent red-teaming requires testing the behavior that emerges when components combine over time.

## The Autonomy Difference

A tool-calling system executes one function and stops. You test whether the function call is correct and whether the function itself is secure. An agent plans a sequence of actions, evaluates outcomes, adjusts strategy, and continues until it reaches a goal. Testing an agent means testing not just what it can do in one step, but what it will do across ten steps, fifty steps, or a hundred steps when pursuing an objective.

The attack surface is no longer a list of tools. It is the space of all possible action sequences the agent might execute. An attacker does not need to exploit a single vulnerability. They manipulate the planning process itself. They redirect the goal. They corrupt the memory that informs future decisions. They introduce inputs that cause the agent to choose a harmful trajectory even though every individual action looks legitimate.

Traditional red teaming asks: can I make the system produce a bad output? Agent red teaming asks: can I make the system take a bad sequence of actions? The shift is fundamental. Outputs are discrete and testable. Trajectories are combinatorial and emergent.

## Emergent Behavior as Vulnerability

Agents exhibit behavior that does not exist in their individual components. A model that refuses harmful requests can still power an agent that achieves harmful outcomes by breaking a task into benign-looking steps. A tool that is perfectly safe in isolation becomes dangerous when called in a specific sequence. A memory system that stores conversation context becomes a vulnerability when an attacker can poison that context across sessions.

You cannot test emergent behavior by testing components. A healthcare agent might have perfect access controls on every patient data API, but still leak information by combining outputs from multiple legitimate queries in ways the component designers never anticipated. A financial agent might correctly verify every transaction, but execute a sequence of small transfers that accomplishes fraud through aggregation. A code-writing agent might produce secure code in every function, but generate an insecure system when the functions interact.

Emergence means the failure mode is not in the code, the prompt, or the model. It is in the interaction over time. Red teaming agents requires simulating those interactions before users discover them.

## The Planning and Execution Gap

Agents plan. That is what makes them agents. They decompose a high-level goal into sub-goals, generate a strategy, and execute actions to achieve those sub-goals. Planning introduces a new attack surface: the gap between what the agent thinks it is doing and what it actually accomplishes.

An attacker can manipulate the planning process without touching the execution. They provide inputs that cause the agent to form an incorrect world model. The agent believes it is solving the user's problem, but the manipulated world model causes it to choose actions that serve the attacker. The execution is flawless. The plan is compromised.

Testing execution is standard software engineering. You verify that when the agent calls a function, the function does what it should. Testing planning requires adversarial simulation. You craft scenarios where the agent's model of the world diverges from reality, then observe whether the agent detects the divergence or follows the plan into failure.

A customer service agent believes it is helping a user troubleshoot an account issue. The user has structured their questions to make the agent believe the account is locked due to a billing error. The agent's plan: verify account status, check billing history, issue a refund if the billing error is confirmed. The execution is correct. The plan is based on false premises introduced by the attacker. The refund happens. The company loses money. No component failed. The trajectory failed.

## Why Component Testing Misses Agent Failures

You test the language model and it passes all safety benchmarks. You test the tool-calling layer and every tool executes correctly. You test the memory system and it stores and retrieves accurately. You test the planning module and it generates valid plans. You deploy the agent and it fails catastrophically because the failure exists in the interaction between components across multiple turns.

Component testing assumes you can decompose the system, test each part, and compose the results to understand the whole. This works for deterministic systems with no feedback loops. It fails for agents. Agents have internal state that evolves based on their own outputs. The output of step five depends on the result of step two, which depended on the memory from the previous session, which was influenced by an input the attacker provided six messages ago.

The failure is not in the components. It is in the state transitions. An agent with perfect components can still enter a bad state because the sequence of states was not tested. Red teaming agents means testing state space, not component space.

## Multi-Step Attack Surfaces

Single-turn attacks target one output. Multi-step attacks target trajectories. An attacker does not try to make the agent produce a harmful response in one message. They guide the agent through a series of interactions, each one slightly shifting the agent's context, memory, or world model, until the agent is in a state where it will take the action the attacker wants.

This is the boiling-frog attack. No single step triggers defenses. The temperature increases gradually. By the time the agent reaches the harmful action, it is operating in a context where that action seems justified based on the accumulated conversation history.

Testing for multi-step attacks requires generating adversarial trajectories, not adversarial prompts. You start with a goal the agent should refuse, then work backward to construct a conversation that leads the agent to accept it. You test whether the agent's refusal mechanisms remain active across turns, or whether they can be eroded through repeated interactions.

A hiring agent should never disclose salary information to unauthorized users. In a single turn, it refuses. Across ten turns, an attacker establishes themselves as a hiring manager, asks the agent to help "prepare a competitive offer," requests "recent salary ranges for similar candidates," and receives the information. Each turn looked like a legitimate HR workflow query. The trajectory was an information disclosure attack.

## Persistence as Vulnerability

Agents persist. They maintain memory across sessions. They build a model of the user and the task over time. Persistence is what makes them useful. It is also what makes them attackable in ways that stateless systems are not.

An attacker who can influence the agent's memory can influence all future interactions. Poisoning the context in session one affects behavior in session ten. This creates an attack vector that does not exist in single-turn systems: the sleeper attack. The malicious input is introduced early, stored in memory, and activated later when the right trigger condition emerges.

Testing persistence-based attacks requires long-horizon simulation. You introduce adversarial context in session one, run the agent through normal interactions for weeks, then check whether the poisoned context affects decisions in session fifty. You verify that the agent's memory does not accumulate corrupted beliefs that change its behavior over time.

A legal research agent accumulates memory of user preferences and past queries. An attacker submits queries designed to make the agent believe the user has a specific interpretation of contract law. Weeks later, when the user asks a genuine question about contract interpretation, the agent's answer is biased by the poisoned memory. The user trusts the answer because the agent has been reliable for weeks. The attack succeeds because it operated across time.

## Agent Autonomy Gradients

Not all agents are equally autonomous. A customer service agent that suggests responses for a human to approve is less autonomous than a financial agent that executes trades without confirmation. The autonomy gradient determines the risk profile and the red teaming strategy.

Low-autonomy agents require testing for harmful suggestions. You verify that the agent does not recommend actions that a human would incorrectly approve. High-autonomy agents require testing for harmful actions. You verify that the agent does not take actions that cause damage before a human can intervene.

The gradient also affects recovery. A low-autonomy agent can be stopped by the human in the loop. A high-autonomy agent can complete a harmful trajectory before anyone notices. Red teaming high-autonomy agents requires testing not just whether attacks are possible, but how quickly they can be detected and stopped once initiated.

You test a low-autonomy coding agent by verifying it does not suggest insecure code. You test a high-autonomy deployment agent by verifying it cannot be tricked into deploying to production without proper gates, because by the time a human reviews the suggestion, the deployment is already running.

## Red Team Implications

Agent red teaming is trajectory testing, not output testing. You design attacks that span multiple turns, sessions, or even weeks. You test the agent's behavior under adversarial context accumulation. You verify that planning cannot be hijacked, that memory cannot be poisoned, and that the agent stops when it should rather than continuing until it causes damage.

Your red team writes attack scenarios that unfold over time. They simulate patient adversaries who establish trust before exploiting it. They test whether the agent's world model can be manipulated into diverging from reality. They verify that the agent cannot be redirected from its goals through gradual conversational pressure.

Component testing is necessary but not sufficient. You need it to ensure the tools are safe, the model is aligned, and the memory is accurate. But agent failures emerge in the composition, and composition failures only appear in trajectories. If you only test components, you deploy an agent that passes all tests and fails in production because the failure mode was never tested.

The shift from output testing to trajectory testing is the shift from red teaming models to red teaming agents. The attack surface is time.

---

Next: 8.2 — The Agent Threat Model: Autonomy as Attack Multiplier
