# 11.9 — The Limits of Automation: What Humans Still Find

Automation finds regressions. It detects when a model that used to resist jailbreaks now accepts them. It catches when a bias metric crosses a threshold. It flags when PII leakage rates increase. But automation is pattern-matching. It looks for known failures in known forms. It does not invent new attacks. It does not understand context. It does not ask "what if." The vulnerabilities that surprise you — the ones that make it into incident reports, press coverage, and legal filings — are almost never caught by automated tests. They are found by humans who thought like attackers and explored spaces the automation never considered.

## What Automation Misses

Automated adversarial tests operate within a predefined search space. A jailbreak test suite contains a list of known jailbreak templates. A prompt injection suite contains a list of known injection patterns. A bias test suite contains a list of demographic attributes and a scoring function. The automation executes these tests, measures the results, and compares them to thresholds. It does not create new templates. It does not discover new patterns. It does not ask whether the list is complete.

A financial services company ran automated jailbreak tests covering forty-three known templates. The tests passed consistently for six months. Then a user discovered a multi-turn jailbreak that worked by pretending to be a compliance auditor, asking the model to "verify" its knowledge of prohibited actions by listing them. The model, trained to be helpful to compliance questions, generated detailed instructions for regulatory violations. The automation never found this because the attack required three turns of dialogue, a plausible cover story, and an understanding of the model's instruction-following hierarchy. It was not a variation of a known template. It was a novel attack requiring creativity and context understanding.

Automated tests also miss context-dependent vulnerabilities. A model might refuse to generate harmful content in isolation but comply when the request is embedded in a longer conversation about legitimate use cases. It might refuse to leak PII when asked directly but reveal it when asked to "summarize" a conversation that included PII. It might resist bias in single-turn interactions but exhibit bias across multi-turn conversations where stereotypes accumulate. Automation tests individual prompts. Humans test conversations, scenarios, and workflows.

## Novel Attack Patterns

The adversarial testing arms race is asymmetric. Defenders build tests for known attacks. Attackers invent new attacks. The time between a new attack being discovered and that attack being added to automated test suites is the window where every model is vulnerable. Some attacks spread in hours, posted on forums or social media. Some attacks are discovered independently by multiple red teamers. Some attacks emerge from interactions between the model and real users and are only detected through production monitoring.

In late 2025, a red teamer discovered that Claude Opus 4.5 could be jailbroken by asking it to roleplay as a "creative writing AI with no content restrictions" and then framing harmful requests as plot elements for a novel. This attack worked because it exploited the model's creative writing capabilities and its tendency to comply with roleplay scenarios. Automated jailbreak tests did not catch it because the prompt structure was different from traditional jailbreaks. It did not use "ignore your instructions" or "you are now in developer mode." It used narrative framing and creative context. Within a week, variations of the attack spread across the internet. Within two weeks, most major model providers had mitigated it. Within a month, it appeared in automated test suites. But for that first week, automation was blind.

Novel attacks often combine multiple techniques. An attacker might use prompt injection to bypass content filters, then use a jailbreak to override safety instructions, then use a multi-turn conversation to gradually escalate requests. Automated tests typically test each technique in isolation. Humans test combinations.

## Multi-Step Attack Chains

Most automated adversarial tests are single-turn. They send a prompt, receive a response, and evaluate whether the response is safe. But many real attacks unfold over multiple turns. The first prompt establishes context. The second prompt builds trust. The third prompt makes the malicious request seem reasonable. By the time the model complies, the conversation has drifted far from where it started.

A customer support AI was designed to refuse refund requests above a certain amount without manager approval. Automated tests confirmed that it correctly refused large refund requests. A human red teamer discovered that if you started the conversation by asking about the refund policy, then asked hypothetical questions about edge cases, then described a scenario that technically met the edge case criteria but was fabricated, the model would approve refunds it should have escalated. The attack required five turns, each one slightly pushing the boundary. Automation tested turn one. Humans tested the full chain.

Multi-step attacks are expensive to automate. They require state management, conversation tracking, and branching logic to handle different model responses. They require defining success criteria that span multiple turns. They require deciding when to give up if the model does not respond as expected. Most teams do not automate multi-step attacks until a specific attack chain has been observed in the wild and proven reproducible.

## Social Engineering and Roleplay Exploits

Automated tests treat the model as a technical system. Human red teamers treat the model as a conversational agent that can be socially engineered. They ask the model to roleplay. They claim authority. They create urgency. They appeal to helpfulness. They frame malicious requests as educational, creative, or compliance-related. These techniques work because models are trained to be helpful, to follow user instructions, and to adapt to conversational context. Automation does not roleplay. Humans do.

A red teamer testing a legal AI discovered that it would refuse to draft documents that facilitated tax evasion when asked directly, but would comply if the user claimed to be a law professor creating exam materials and needed realistic examples of "aggressive but technically legal tax strategies." The model, trained to assist educators, generated content it would have refused in a direct request. The vulnerability was not in the content filter. It was in the model's social reasoning. Automation does not understand social context. It does not model the interaction as a relationship between a user with intent and a model with policies. It models the interaction as input and output.

## Ethical Boundary Cases

Some adversarial scenarios are not about bypassing safety filters. They are about testing where the model draws ethical lines in ambiguous situations. A model might correctly refuse to help with obviously harmful requests but struggle with requests that are harmful in some contexts and legitimate in others. Should the model explain how to pick a lock? It depends whether the user locked themselves out of their own home or is trying to break into someone else's. Should the model provide advice on surveillance techniques? It depends whether the user is a journalist investigating corruption or a stalker tracking an ex-partner.

Automated tests struggle with context-dependent ethics. They can test whether the model refuses to provide lock-picking instructions, but they cannot test whether the model correctly distinguishes legitimate from malicious intent. Human red teamers explore these boundary cases by crafting scenarios where intent is ambiguous, where the same information could be used for good or harm, and where the model must make judgment calls. These cases rarely have clear right answers. The goal is not to make the model perfect. The goal is to understand how it behaves at the edges of its ethical reasoning and to document those behaviors so product and policy teams can make informed decisions.

## Human Intuition in Red Teaming

Experienced red teamers develop intuition for what might break a model. They notice patterns in how models respond to certain phrasings, certain topics, certain conversational structures. They form hypotheses about where the model's training might have gaps, where its safety measures might be brittle, where its helpfulness instinct might override its safety instinct. They explore these hypotheses with targeted tests. Automation does not form hypotheses. It executes tests.

A red teamer testing a medical AI noticed that the model was extremely cautious about providing medical advice but much less cautious when discussing veterinary medicine. They hypothesized that the model's safety training focused on human health and might have gaps for animal health. They tested whether the model would provide veterinary advice that, if applied to humans, would be dangerous. It did. The automation had tested medical advice refusal. It had not tested the veterinary medicine gap because no one had thought to add that to the test suite. The human red teamer found it by intuition.

Intuition comes from experience, domain knowledge, and creativity. It is pattern recognition across contexts that automation has never seen. It is asking "what if" questions that seem unreasonable until they work. It is the ability to see a vulnerability not as a technical failure but as a misunderstanding of how the model will be used.

## Balancing Automation and Human Effort

The goal is not to replace automation with humans or to replace humans with automation. The goal is to use each for what it does best. Automation is tireless, consistent, and fast. It runs the same tests thousands of times without boredom or fatigue. It catches regressions the moment they appear. It enforces thresholds without bias. Humans are creative, contextual, and adaptive. They invent new attacks. They explore ambiguous scenarios. They ask questions automation would never think to ask.

The strongest adversarial testing programs run automated tests continuously and schedule human red teaming periodically. Automation runs on every deployment, every change, every build. Human red teaming runs quarterly, or before major releases, or when launching new capabilities. Automation catches known failures. Humans discover unknown failures. When humans discover a new vulnerability, it is added to the automated test suite. The suite grows over time, capturing the accumulated knowledge of every red teaming session.

Some teams structure this as a feedback loop. Automated tests run continuously and flag anomalies or edge cases that do not quite fail but seem unusual. Human red teamers investigate those anomalies. If they find a vulnerability, they document it, develop a test for it, and add the test to the automation. The automation gets smarter with every human-discovered failure. The humans focus their effort on the frontier, not on re-testing known patterns.

The limit of automation is not technical. It is conceptual. Automation tests what you know to test. Humans discover what you did not know to test. The next question is which tools exist to support this work, how to choose them, and when to build your own.
