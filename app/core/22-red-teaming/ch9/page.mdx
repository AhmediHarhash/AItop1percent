# Chapter 9 — Social Engineering and Harm Amplification

AI systems can be weaponized for manipulation. Beyond technical vulnerabilities, red teams must test what happens when attackers use your AI against your users. The question is not just whether your model can be jailbroken to produce harmful content — the question is whether it can be used as a tool to manipulate, deceive, and harm at scale. Social engineering through AI is fundamentally different from traditional attacks because it combines human psychology with computational scale and personalization. This chapter covers social engineering through AI, harm amplification, the difficult line between helpful and harmful, and how to test for manipulation potential before attackers discover it.

---

- 9.1 — AI as Social Engineering Amplifier
- 9.2 — Persuasion and Influence Attacks
- 9.3 — Deception Generation: Using AI to Lie Convincingly
- 9.4 — Impersonation Attacks: AI Pretending to Be Human
- 9.5 — Phishing Assistance: AI Helping Craft Attacks
- 9.6 — Radicalization and Harmful Influence
- 9.7 — Bias Exploitation and Discrimination Amplification
- 9.8 — Psychological Harm Edge Cases: Testing for Vulnerable Users
- 9.9 — Adversarial Harm Modeling: Predicting Societal Impact
- 9.10 — Testing for Manipulation Potential
- 9.11 — Content Policy Enforcement
- 9.12 — The Line Between Helpful and Harmful

---

*The attacker does not need to break your model. They only need to use it exactly as designed — pointed at the wrong target, for the wrong purpose. The next subchapter begins with the amplification problem.*
