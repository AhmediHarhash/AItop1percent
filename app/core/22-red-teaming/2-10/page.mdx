# 2.10 — Multi-Agent Attack Surfaces: When Agents Talk to Agents

Most teams think about securing a single agent. They test the prompts, validate the outputs, restrict the tool access, and deploy. Then they add a second agent. Then a third. Suddenly the agents are communicating — passing data between each other, coordinating actions, making decisions based on what other agents reported. The security model that worked for one agent collapses. Multi-agent systems do not just add attack surface linearly. They multiply it. A vulnerability in one agent becomes a vulnerability in every agent it communicates with. An attacker who compromises one agent gains a foothold to compromise the entire system.

A logistics company in November 2025 built a multi-agent system where one agent handled customer inquiries, another managed inventory, and a third coordinated shipping. The agents communicated via an internal message queue. An attacker compromised the customer inquiry agent through prompt injection and used it to send malicious messages to the inventory agent. The inventory agent trusted messages from the customer agent because they were internal communications. The malicious messages convinced the inventory agent to mark high-value items as damaged and unsellable. The shipping agent, seeing the inventory updates, canceled shipments. The company lost $190,000 in a week before they detected the pattern. No single agent had a critical flaw. The vulnerability was in the trust model between agents.

## Why Multi-Agent Systems Are Harder to Secure

Single-agent security assumes one decision boundary. You validate inputs, you constrain outputs, you monitor behavior. Multi-agent security has as many decision boundaries as you have agents — and those boundaries interact. An input that seems safe to one agent becomes dangerous when passed to another agent with different permissions or different context.

Multi-agent systems create trust propagation chains. Agent A trusts the user. Agent B trusts Agent A. Agent C trusts Agent B. An attacker who manipulates the user input can compromise Agent A, which then compromises Agent B, which then compromises Agent C. Each agent thinks it is processing trusted data because it came from another agent, not from the original untrusted source. The trust assumption cascades through the system.

Multi-agent systems also create emergent behaviors that are not present in any single agent. Two agents might individually behave safely, but when they interact, they produce outputs or take actions that neither was designed to produce alone. An attacker can exploit these emergent behaviors by crafting inputs that cause agents to interact in unexpected ways — triggering edge cases that were never tested because they only occur when multiple agents are involved.

## Trust Propagation Between Agents

The fundamental vulnerability in multi-agent systems is trust propagation. Agents treat communications from other agents as trusted, even when the data originated from an untrusted source. This creates a privilege escalation path. An attacker does not need to compromise a high-privilege agent directly. They compromise a low-privilege agent and use it to send malicious data to higher-privilege agents.

A financial services company in December 2025 had a three-agent system: a front-end agent that handled customer requests, a risk assessment agent that evaluated transactions, and an execution agent that processed approved transactions. The front-end agent had no direct access to transaction processing. The execution agent only accepted commands from the risk assessment agent. An attacker used prompt injection to manipulate the front-end agent into sending a crafted message to the risk assessment agent. The message was formatted to look like a legitimate transaction summary but included instructions that overrode the risk assessment logic. The risk assessment agent, trusting data from the front-end agent, approved a fraudulent transaction. The execution agent, trusting the risk assessment agent, processed it.

Trust propagation is difficult to prevent because agents need to communicate. The solution is not to eliminate trust — it is to make trust explicit and conditional. An agent should not trust another agent's output simply because it came from that agent. It should validate that the output is consistent with what that agent is authorized to produce, that the output passes sanity checks, and that the output does not contain anomalies.

## Poisoning Agent Communication Channels

Agent communication channels are attack vectors. If agents communicate via message queues, shared databases, API calls, or file systems, an attacker who gains access to those channels can inject malicious messages that appear to come from legitimate agents.

Message queue poisoning works by injecting messages into the queue that agents consume. If Agent A sends messages to a queue and Agent B reads from that queue, an attacker who can write to the queue can send messages that Agent B will process as if they came from Agent A. The attacker does not need to compromise Agent A. They just need access to the communication channel.

A supply chain company in October 2025 used a message queue for agent communication. The queue had authentication — only authorized services could write to it. But one of those authorized services was a monitoring dashboard that logged agent communications. An attacker compromised the dashboard through an unrelated vulnerability and used its queue access to inject fake messages. The downstream agents processed the fake messages because they appeared in the legitimate queue. The attack was not discovered for three days because the messages were formatted correctly and did not trigger obvious errors.

Communication channel security requires end-to-end authentication. It is not enough for the channel to be authenticated. Each message must be authenticated so that the receiving agent can verify which agent sent it. This typically requires message signing — each agent signs its messages with a cryptographic key, and receiving agents verify the signature before processing the message. Without message signing, an attacker with channel access can impersonate any agent.

## Cascade Failures from Compromised Agents

When one agent in a multi-agent system is compromised, the compromise can cascade. The compromised agent sends malicious data to other agents. Those agents act on the malicious data and produce incorrect outputs. Those outputs propagate to additional agents. The failure spreads through the system until either a human intervenes or the entire system is degraded.

Cascade failures are particularly dangerous in systems where agents operate autonomously without human oversight. A single compromised agent can trigger a chain reaction that affects dozens of downstream processes before anyone notices. The time to detection becomes the critical variable. The longer the compromised agent operates undetected, the more damage it causes.

A manufacturing company in January 2026 had a multi-agent system managing production schedules. One agent monitored supply inventory, another optimized production runs, and a third scheduled shipping. An attacker compromised the inventory agent and fed it false data indicating a shortage of critical materials. The production agent, seeing the shortage, rescheduled production to conserve materials. The shipping agent, seeing the production changes, delayed shipments. The cascade affected three weeks of operations before the company realized the inventory data was wrong. The compromise of one agent triggered decisions across the entire production pipeline.

Preventing cascade failures requires circuit breakers — mechanisms that detect when an agent is producing anomalous outputs and stop the propagation. If an agent suddenly reports a massive inventory shortage, that should trigger a validation step before downstream agents act on it. If an agent requests an unusual number of high-risk actions, that should pause the workflow and request human review. Circuit breakers add friction, but they contain damage.

## Privilege Escalation Across Agent Boundaries

Multi-agent systems often have hierarchical privilege models. Low-privilege agents handle untrusted user input. High-privilege agents perform sensitive actions. The security assumption is that untrusted input cannot reach high-privilege agents without passing through validation layers. This assumption breaks when agents communicate.

An attacker can use a low-privilege agent as a stepping stone to a high-privilege agent by crafting inputs that the low-privilege agent passes to the high-privilege agent in a form that bypasses validation. The low-privilege agent is not compromised in the traditional sense — it is operating as designed. The attack is in what it sends to the next agent.

A legal services company in September 2025 had an agent that handled client intake and another that accessed privileged legal databases. The intake agent could ask the database agent for publicly available case summaries. It could not request confidential client information. An attacker used prompt injection to manipulate the intake agent into sending a database query that looked like a request for a case summary but was actually a request for confidential records. The database agent processed the query because it came from an authorized agent. The privilege escalation happened because the database agent did not validate that the requesting agent was authorized for that specific query — it only checked that the requesting agent was authorized to make queries in general.

Privilege escalation prevention requires fine-grained authorization checks at every agent boundary. It is not enough to authenticate the requesting agent. You must validate that the requesting agent is authorized for the specific action being requested. This requires maintaining an authorization model that defines what each agent can request from each other agent — not just a binary allowed or denied, but a detailed policy that specifies permissible actions.

## Coordinated Multi-Agent Attacks

The most sophisticated multi-agent attacks involve compromising multiple agents simultaneously to achieve an objective that no single agent could accomplish alone. An attacker identifies agents with complementary permissions, compromises each one through different attack vectors, and coordinates their actions to achieve the attacker's goal.

A real estate platform in August 2025 had agents for property search, mortgage pre-approval, and document generation. An attacker compromised the search agent to manipulate property listings, the mortgage agent to approve inflated loan amounts, and the document agent to generate contracts with fraudulent terms. Each agent's individual behavior was within normal parameters. The coordination was the attack. The system had monitoring for anomalous behavior in each agent but no monitoring for coordinated anomalies across agents.

Detecting coordinated multi-agent attacks requires correlation analysis. You must track not just what each agent is doing but the relationships between agent actions. If Agent A's outputs consistently trigger specific behaviors in Agent B, and those behaviors align with an attack pattern, that is a signal. This requires centralized logging and analysis across all agents — you cannot detect coordinated attacks by monitoring agents in isolation.

## Testing Multi-Agent Security

Testing multi-agent security means simulating attacks that exploit agent interactions. Start with single-agent compromise scenarios — inject prompts into one agent and observe how the compromise propagates. Measure how many agents are affected, how long it takes for the system to detect the issue, and what damage occurs before containment.

Test trust boundaries by sending malicious data from one agent to another. If Agent A is compromised, what can it convince Agent B to do? Can it escalate privileges? Can it access data it should not have? Can it trigger actions that should require higher authorization?

Test communication channels by injecting malicious messages directly into the channels. If you write to the message queue, API, or shared database that agents use to communicate, do the receiving agents validate message authenticity? Do they detect anomalies? Do they reject malformed or suspicious messages?

Test cascade scenarios by introducing failures in one agent and measuring how far the failure propagates. If one agent starts producing incorrect outputs, do downstream agents detect the errors? Do they halt processing? Do they request validation? Or do they blindly trust the upstream agent and amplify the error?

Test coordinated attacks by compromising multiple agents and attempting to achieve objectives that require coordination. Can you manipulate property listings and approvals simultaneously? Can you alter both inventory data and shipping schedules? The goal is to find whether your monitoring detects coordinated anomalies or only isolated ones.

## Defense Patterns for Agent Communication

Defending multi-agent systems requires defense in depth at every communication boundary. First, authenticate every message. Use cryptographic signatures so that receiving agents can verify which agent sent a message. This prevents attackers from injecting messages that appear to come from legitimate agents.

Second, validate message content against expected schemas and constraints. If Agent A is supposed to send status updates, Agent B should validate that incoming messages match the status update schema and contain plausible values. If a message claims to be a status update but includes unusual fields or out-of-range values, reject it.

Third, implement least-privilege communication. Agents should only be able to send specific types of messages to specific other agents. Agent A should not be able to send arbitrary messages to Agent B — it should only be able to send the message types that Agent B is designed to receive from Agent A. This limits the attack surface of each agent boundary.

Fourth, add anomaly detection at the communication layer. Monitor message frequency, message size, message content patterns. If Agent A suddenly starts sending ten times as many messages to Agent B, that is a signal. If messages that are normally 200 bytes suddenly grow to 20,000 bytes, that is a signal. If message content includes unusual patterns or keywords, that is a signal.

Fifth, implement circuit breakers that halt propagation when anomalies are detected. If Agent B receives a message from Agent A that fails validation or triggers anomaly detection, Agent B should not process it. Instead, it should log the anomaly, alert human operators, and request verification. The cost of a false positive — a halted workflow — is lower than the cost of a false negative — a compromised system.

Multi-agent systems are the frontier of AI security. They create vulnerabilities that do not exist in single-agent systems. When those multi-agent systems also serve multiple customers from shared infrastructure, the attack surface becomes even more complex.
