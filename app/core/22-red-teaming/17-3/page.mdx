# 17.3 — SOC Playbooks for AI-Specific Incidents

Why did the SOC analyst ignore the alert?

Because it said "anomalous model behavior detected" and the playbook told them to check the firewall logs, escalate to the network team, and run a malware scan. None of those steps made sense for an AI incident. The analyst spent forty-five minutes following a procedure designed for credential theft before concluding that the alert was a false positive and closing the ticket. Meanwhile, an attacker was extracting customer data through a prompt injection chain that the analyst had never been trained to recognize, let alone contain. The alert was real. The playbook was useless. The analyst did exactly what they were told to do, and the breach continued uninterrupted.

This is the core problem with AI security operations in 2026. Most SOCs have mature playbooks for network intrusions, malware infections, phishing compromises, and credential abuse. Almost none have playbooks for prompt injection, system prompt extraction, tool invocation abuse, safety filter bypass, or cross-tenant data leakage through model outputs. When an AI-specific alert fires, the analyst either follows an irrelevant traditional playbook or exercises their best judgment with no guidance. Neither option produces an effective response.

## Why Traditional Playbooks Do Not Transfer

SOC playbooks encode institutional knowledge about how to respond to specific incident types. A phishing playbook tells the analyst to isolate the affected endpoint, check for credential harvesting, review email gateway logs for similar messages, block the sender domain, and notify affected users. Every step makes sense because the playbook was designed for the specific mechanics of phishing attacks — the attack vector (email), the impact mechanism (credential theft), and the containment method (endpoint isolation and domain blocking) are well understood.

AI incidents have fundamentally different mechanics. The attack vector is natural language input, not network packets or malicious attachments. The impact mechanism is model behavior manipulation, not code execution or credential theft. The containment method is not endpoint isolation — you cannot isolate a model that is serving thousands of concurrent users — but rather input filtering, prompt modification, tool access revocation, or session termination. Every step in a traditional playbook maps to infrastructure and tools that have no relevance to AI incidents.

Consider the specific mismatch. A traditional data exfiltration playbook tells the analyst to check network DLP alerts, review outbound traffic logs, identify the destination IP, and block the exfiltration channel. AI data exfiltration happens through the model's normal response mechanism. There is no outbound connection to block. The data leaves through the same HTTPS response that delivers every legitimate answer. DLP tools that inspect network traffic see nothing abnormal because the traffic pattern — user sends request, system returns response — is identical to normal operation. The exfiltration is invisible to every tool the SOC analyst has been trained to use.

## The Six Core AI Incident Types

AI security incidents fall into six categories that SOC teams need dedicated playbooks for. Each has different detection signals, different investigation procedures, and different containment actions.

**Prompt injection detected.** This is the most common AI incident type. The detection fires when input monitoring identifies an instruction override attempt — a user or external data source attempting to redirect the model's behavior. Investigation requires examining the prompt log to determine what instructions were injected, whether the model followed them, and what actions resulted. Containment depends on severity: low-severity injections where the model refused the instructions may only require logging and monitoring. High-severity injections where the model followed malicious instructions and took action — called a tool, returned restricted data, modified a record — require immediate session termination, rollback of any actions the model took, and review of all sessions from the same user or IP address for related activity.

**System prompt extraction attempted.** The detection fires when the model's response contains fragments of system prompt content that should not be visible to users. Investigation requires comparing the model's output against known system prompt content to determine how much was leaked, then reviewing the conversation history to identify the extraction technique. Containment involves resetting the session, reviewing whether the leaked information reveals tool schemas, routing logic, or security controls that an attacker could exploit in subsequent attacks, and updating the system prompt to harden against the specific extraction technique.

**Anomalous tool invocation patterns.** This alert fires when the model calls tools with parameters, frequency, or sequences that deviate from established baselines. A customer service bot that normally queries one customer record per session suddenly queries fifteen. A research assistant that typically searches public databases suddenly attempts to access an internal knowledge base it has never called before. Investigation requires tracing the conversation that preceded the tool calls to determine whether the user directed the model to take unusual actions or whether the model's behavior was manipulated through injection. Containment may require revoking the model's access to specific tools, rate-limiting tool calls, or terminating sessions that trigger sustained anomalous patterns.

**Cross-tenant data access.** In multi-tenant AI systems, this alert fires when a model response contains data that belongs to a different tenant than the requesting user. Investigation is critical and urgent — it requires identifying exactly what data was exposed, which tenant it belongs to, and the mechanism that caused the leakage. Common causes include shared retrieval indices that do not enforce tenant isolation, conversation memory that crosses session boundaries, or tool integrations that do not properly scope queries to the requesting tenant. Containment requires immediate session termination, audit of all recent sessions for the same tenant, potential notification of the affected data owner, and escalation to engineering for root cause analysis.

**Safety filter bypass.** The detection fires when a model produces output that violates safety policies despite active filtering. Investigation determines whether the bypass was intentional, meaning an attacker deliberately circumvented safety controls, or incidental, meaning the model produced unsafe content through normal interaction without user manipulation. The distinction matters because intentional bypass indicates an adversarial actor who may be probing for additional weaknesses, while incidental bypass indicates a control gap that needs engineering attention. Containment for intentional bypass includes session termination, user flagging, and investigation of all sessions from the same source. Containment for incidental bypass includes logging the failure for the safety engineering team and monitoring for recurrence.

**Model behavior drift.** This is the slowest-burning and hardest-to-contain incident type. The alert fires when baseline behavioral metrics — response patterns, tool call distributions, safety filter trigger rates, or output characteristic measurements — shift beyond established thresholds without a corresponding deployment change. Drift may indicate data poisoning, model degradation, or adversarial manipulation that is subtle enough to avoid per-request detection but visible in aggregate statistics. Investigation requires comparing current behavioral baselines against historical benchmarks, reviewing recent data pipeline changes, and analyzing whether the drift correlates with specific user segments, input types, or time periods. Containment may require reverting to a previous model version, suspending specific features, or increasing monitoring granularity until root cause is established.

## Escalation Paths and Severity Classification

AI incidents need their own severity classification that maps to escalation paths the SOC team can execute. Trying to force AI incidents into existing severity frameworks designed for network security produces consistent misclassification. A prompt injection that the model refused is operationally trivial, but a traditional severity framework might rate it high because "injection" sounds dangerous. A subtle behavioral drift that indicates memory poisoning is operationally critical, but a traditional framework might rate it low because no single request looks anomalous.

The AI-specific severity framework uses four levels. **Critical** means confirmed unauthorized data access, successful cross-tenant leakage, or active exploitation of an attack chain that is producing ongoing damage. Response time: immediate. Escalation: AI engineering lead, security leadership, and legal within fifteen minutes. **High** means successful prompt injection where the model followed malicious instructions and took action, confirmed system prompt extraction that reveals exploitable information, or persistent anomalous tool invocation that has not yet been contained. Response time: under thirty minutes. Escalation: AI engineering team and security operations lead. **Medium** means prompt injection attempts that the model partially followed but did not produce confirmed data exposure, safety filter bypasses that produced policy-violating output without data impact, or anomalous patterns that require investigation but have not confirmed an active attack. Response time: under two hours. Escalation: security operations team with AI engineering on standby. **Low** means prompt injection attempts that the model fully refused, system prompt probing that did not extract meaningful content, or minor behavioral anomalies within expected variance. Response time: next business day. Escalation: documented for trend analysis, no immediate action required.

## Containment Actions Specific to AI Systems

Traditional containment relies on network isolation, endpoint quarantine, and credential revocation. AI containment requires different actions that the SOC team must be authorized and trained to execute.

**Session termination** ends the current conversation and invalidates the session token. This stops an active attack chain but does not prevent the attacker from starting a new session. It is the first containment action for any high or critical severity incident.

**Memory purge** clears the model's stored context for a specific user or session. This is critical when the attack involves memory poisoning — the attacker has injected persistent instructions into the model's conversation memory that would survive session termination. Without a memory purge, the attacker's influence persists even after the session is killed.

**Tool access revocation** removes the model's ability to call specific external tools. If the attack involves anomalous tool invocation — the model querying databases it should not access or calling APIs with manipulated parameters — revoking tool access stops the exploitation path. This is a targeted action: revoke the specific tool being abused, not all tools, unless the attack chain demonstrates lateral movement across multiple integrations.

**Rate limiting** reduces the number of requests or tool calls a specific user, IP address, or tenant can make within a time window. This is useful for attacks that rely on volume — systematic data extraction, brute-force prompt injection testing, or enumeration of tool capabilities. Rate limiting slows the attacker without disrupting legitimate users.

**Model rollback** reverts the production model to a previous known-good version. This is the nuclear option for behavioral drift incidents where the current model's behavior cannot be trusted. It is disruptive — reverting a model may remove legitimate improvements — but it is sometimes the only way to restore confidence in system behavior while root cause analysis proceeds.

## Building Playbooks from Purple Team Findings

The most effective AI SOC playbooks are not written from theory. They are built from purple team exercises. When the purple team executes an attack technique and observes what signals it produces, the detection side of that exercise naturally feeds into a playbook. The detection tells the analyst when to act. The playbook tells them how to act. The purple team's attack-detect-validate cycle produces both simultaneously.

For each detection the purple team deploys, the playbook appendix answers five questions the analyst will face when the alert fires. What does this alert mean in plain language? What should I look at first? What questions do I need to answer to determine severity? What containment actions are available to me? When do I escalate and to whom? These five questions, answered specifically for each AI detection, give the analyst a clear path from alert to action without requiring AI security expertise that most SOC analysts do not yet have.

Organizations running mature purple teams update their AI playbooks after every detection cycle. The playbook library grows alongside the detection library — each new attack-detection card produces a corresponding playbook entry. After twelve months of purple teaming, a typical team has playbooks covering fifty to eighty AI-specific incident scenarios, each tested against real attack replays and refined based on analyst feedback from tabletop exercises.

Playbooks tell analysts what to do when a detection fires. But detections only fire when the right telemetry reaches the right platform. The next subchapter covers the infrastructure challenge of getting AI system telemetry into SIEM platforms where detection rules can act on it.
