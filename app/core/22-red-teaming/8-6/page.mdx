# 8.6 — Multi-Agent Attacks: When Agents Attack Each Other

Why do agents trust each other? When Agent A receives a message from Agent B, does it validate the source, verify the content, or check for malicious intent? Usually not. Multi-agent systems operate on implicit trust. Agents assume messages from other agents are legitimate. This assumption creates a massive attack surface. An attacker who can control one agent, or who can impersonate an agent, can manipulate the entire system.

In January 2026, a logistics company deployed a multi-agent system to manage warehouse operations. One agent handled inventory tracking. Another managed order fulfillment. A third coordinated shipping. The agents communicated via a shared message bus. When the fulfillment agent needed inventory data, it sent a request to the inventory agent. When the shipping agent needed fulfillment status, it queried the fulfillment agent. The system worked efficiently for three months. Then an attacker gained access to the message bus—not by compromising an agent, but by exploiting a weak authentication mechanism on the bus itself. The attacker injected messages pretending to be the inventory agent, telling the fulfillment agent that high-demand items were out of stock. The fulfillment agent stopped processing orders for those items. The shipping agent, seeing no fulfillment activity, marked shipments as delayed. Customer complaints spiked. The company lost $1.2 million in revenue over four days before the attack was discovered. The agents were not compromised. The communication channel was.

Multi-agent attacks exploit the relationships between agents. The attacker does not need to break every agent. They just need to break one, or break the communication between them, or inject themselves into the conversation. The agents do the rest.

## The Multi-Agent Threat Landscape

Multi-agent systems create three new attack vectors that do not exist in single-agent systems: agent impersonation, message injection, and trust relationship exploitation. Each vector allows attackers to manipulate the system without directly compromising any individual agent's logic.

Agent impersonation means the attacker pretends to be a legitimate agent. If agents identify each other by name, service endpoint, or API key, and those identifiers are not cryptographically secured, an attacker can forge them. The attacker sends messages that appear to come from Agent A but actually come from the attacker's controlled system. Other agents process these messages as if they were legitimate.

Message injection means the attacker inserts malicious messages into the communication channel between agents. The agents are not compromised. The messages are crafted to look like normal inter-agent communication. The attacker does not impersonate a specific agent—they simply add extra messages to the queue. Agents process these messages alongside legitimate ones, and the attacker's inputs influence agent behavior.

Trust relationship exploitation means the attacker manipulates the trust assumptions between agents. If Agent A trusts all messages from Agent B without validation, the attacker only needs to compromise Agent B or impersonate it. Agent A will accept malicious instructions because it assumes Agent B is trustworthy. The attacker leverages the trust relationship as a force multiplier.

## Agent-to-Agent Manipulation

Agents influence each other through messages, commands, and shared state. An attacker who can inject messages can manipulate agents indirectly. Instead of attacking Agent A directly, the attacker sends a message to Agent B that causes Agent B to send a malicious message to Agent A. The attack is laundered through the intermediate agent.

A monitoring agent detects anomalies and alerts an incident response agent. The response agent takes corrective actions based on the alerts. An attacker injects false alerts into the monitoring agent, making it believe an incident is occurring. The monitoring agent, operating correctly, sends an alert to the response agent. The response agent, also operating correctly, executes a response—shutting down a service, revoking credentials, or quarantining a system. The attacker caused a denial-of-service attack by making one agent lie to another.

This attack is particularly effective because both agents are functioning as designed. The monitoring agent is doing its job: reporting anomalies. The response agent is doing its job: responding to incidents. The vulnerability is in the trust between them. The response agent does not verify that the monitoring agent's alert is based on real data. It trusts the monitoring agent implicitly.

## Trust Relationship Exploitation in Practice

Multi-agent systems often have hierarchical trust structures. A coordinator agent sends tasks to worker agents. The workers trust the coordinator. A supervisor agent monitors worker agents and reports to a management agent. The management agent trusts the supervisor. These trust relationships are design features, but they become vulnerabilities when trust is absolute and unverified.

An attacker who compromises the coordinator agent can send arbitrary tasks to all worker agents. The workers do not question the coordinator's authority. They execute the tasks. If the coordinator is compromised or impersonated, the attacker controls the entire workforce.

An attacker who compromises the supervisor agent can feed false data to the management agent. The management agent makes decisions based on that data—allocating resources, adjusting priorities, escalating issues. The attacker influences high-level decisions by poisoning the data at the supervisor level.

The logistics attack succeeded because the fulfillment agent trusted messages from the inventory agent without verification. The attacker did not need to compromise the fulfillment agent's logic. They just needed to send messages that looked like they came from the inventory agent. The fulfillment agent had no mechanism to verify message authenticity. It assumed any message on the inventory channel was legitimate.

## Message Injection Between Agents

Message injection attacks target the communication layer. If agents communicate via a shared queue, message bus, or API, and that layer lacks strong authentication, attackers can insert messages. The messages are indistinguishable from legitimate inter-agent communication.

The shared message bus used by the logistics company required a simple API key for access. The API key was intended to prevent random internet users from accessing the bus, but it was not tied to specific agents. Any system with the API key could publish messages to any channel. The attacker obtained the API key—possibly through a leaked configuration file, possibly through social engineering, possibly through a separate breach—and used it to publish messages to the inventory channel.

The fulfillment agent received these messages and processed them as if they came from the inventory agent. The messages had the correct structure, the correct field names, and plausible values. They just happened to be false. The agent had no way to know.

This vulnerability exists in any multi-agent system where agents trust the communication channel more than they trust the message content. If the channel is secure, the agents assume the messages are secure. But channel security and message authenticity are not the same. A secure channel prevents eavesdropping and tampering. It does not prevent an authorized attacker from publishing malicious messages.

## Coordination Protocol Attacks

Multi-agent systems often use coordination protocols—handshakes, acknowledgments, consensus mechanisms—to ensure agents agree on state and actions. Attackers exploit these protocols by participating in them maliciously.

A distributed task assignment system uses a consensus protocol. Three agents vote on which agent should handle a task. The majority wins. An attacker compromises one agent and makes it vote for a malicious choice. If the attacker can influence one other agent—through misinformation, timing manipulation, or indirect influence—they can control the outcome. Two out of three votes are enough to force the system into a state the attacker chooses.

A resource allocation system uses a token-passing protocol. Agents pass a token to signal which agent can access a shared resource. An attacker intercepts the token or creates a duplicate token. Multiple agents now believe they have exclusive access to the resource. Conflicts, data corruption, or deadlocks result.

These attacks are subtle because the protocols themselves are not broken. The agents follow the protocol correctly. The attacker simply participates in the protocol with malicious intent, and the protocol has no way to distinguish legitimate participation from malicious participation.

## Testing Multi-Agent Systems

Testing multi-agent systems for vulnerabilities requires simulating a hostile agent. You introduce an agent into the system that behaves maliciously—sending false data, refusing to cooperate, impersonating other agents, injecting unexpected messages. Then you observe how the legitimate agents respond.

One test is the malicious participant test. You add a rogue agent to the system that sends plausible but incorrect data. Does the system detect the discrepancy? Do other agents validate the data? Or do they trust it and propagate the error?

Another test is the impersonation test. You create a fake agent that claims to be a legitimate agent. You give it the same name, endpoint, or identifier. Does the system allow the fake agent to participate? Do other agents detect the impersonation? Or do they process messages from the fake agent as if it were real?

A third test is the message injection test. You insert messages into the communication channel without going through an agent. The messages have valid structure but malicious content. Do agents validate message sources? Do they check message integrity? Or do they blindly process whatever appears on the channel?

A fourth test is the trust chain test. You compromise one low-privilege agent and use it to send messages to higher-privilege agents. Can you escalate privileges by manipulating trust relationships? Can you cause a high-privilege agent to take actions it would not take based on direct user input?

## Isolation Between Agents

The fundamental defense against multi-agent attacks is isolation. Agents should not trust each other by default. They should verify messages, authenticate sources, and validate data. Isolation means assuming that any agent could be compromised or impersonated and designing defenses accordingly.

Message authentication means every message is cryptographically signed by the sender. The receiver verifies the signature before processing the message. If the signature is invalid or missing, the message is rejected. This prevents impersonation and injection. An attacker cannot forge a valid signature without the sender's private key.

Data validation means agents do not trust the content of messages even if the source is authenticated. If Agent A receives inventory data from Agent B, Agent A checks whether the data is plausible. Does it match recent historical trends? Is it consistent with other data sources? If the data is anomalous, Agent A escalates to a human or requests confirmation.

Rate limiting and anomaly detection protect against flooding attacks. If Agent A typically receives five messages per minute from Agent B, and suddenly receives 500 messages per minute, that is suspicious. The system flags the anomaly and pauses message processing until a human investigates.

Role-based access control limits what each agent can request from other agents. Agent A can query inventory data but cannot modify it. Agent B can send alerts but cannot execute responses. Agent C can execute responses but only after approval from a human. This limits the damage an attacker can do by compromising or impersonating a single agent.

## Defense Patterns for Multi-Agent Systems

Multi-agent systems require defense-in-depth. You cannot rely on a single security mechanism. You need multiple layers: secure communication channels, authenticated messages, validated data, monitored behavior, and isolated execution.

Secure communication channels mean agents communicate over encrypted, authenticated connections. The channel itself is protected against eavesdropping and tampering. But channel security is not enough. You also need message-level security.

Authenticated messages mean every message is signed by the sender and verified by the receiver. This prevents impersonation and injection. Even if an attacker gains access to the communication channel, they cannot forge messages from other agents.

Validated data means agents do not blindly trust message content. They apply sanity checks, consistency checks, and cross-validation. If data is implausible or inconsistent, it is flagged and investigated.

Monitored behavior means the system logs all inter-agent communication and analyzes it for anomalies. Unusual message patterns, unexpected sources, or suspicious content trigger alerts. Humans investigate before damage spreads.

Isolated execution means agents run in separate security contexts. If one agent is compromised, it cannot directly access other agents' memory, credentials, or internal state. The attacker must exploit the communication layer, which is monitored and controlled.

## The Trust Boundary Problem

The hardest problem in multi-agent security is defining trust boundaries. In a single-agent system, the boundary is clear: the agent trusts its own code and nothing else. In a multi-agent system, agents must trust each other to some degree—otherwise they cannot coordinate. But absolute trust is dangerous. Partial trust is complicated.

The answer is context-dependent trust. Agent A trusts Agent B for certain types of information but not others. Agent A trusts inventory data from Agent B because Agent B is the inventory specialist. But Agent A does not trust security policy decisions from Agent B because that is outside Agent B's domain. Trust is scoped by role and function.

This requires agents to have explicit trust policies. When Agent A receives a message from Agent B, it checks: is this message type something Agent B is authorized to send? Is the content plausible given Agent B's role? If not, the message is rejected or escalated.

Implementing these policies requires careful design. Agents need to know what other agents are responsible for, what data they can provide, and what actions they can request. This metadata must be correct, up-to-date, and tamper-proof. If an attacker can modify trust policies, they can authorize themselves to do anything.

The next vulnerability lies in the persistence of agent state. Agents remember previous interactions, and attackers poison that memory to influence future behavior, creating compromises that persist across sessions and survive restarts.
