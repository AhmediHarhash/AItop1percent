# 11.10 — Tools and Platforms for Automated Red Teaming

The tooling ecosystem for automated adversarial testing matured rapidly between 2024 and 2026. What started as research prototypes and internal scripts evolved into production-grade platforms with comprehensive attack libraries, CI/CD integrations, and enterprise support. Some tools specialize in specific attack categories. Some tools cover the full adversarial testing lifecycle. Some tools are open source. Some are commercial. Understanding the landscape lets you choose tools that match your threat model, your technical stack, and your team's capabilities — and it helps you recognize when existing tools are insufficient and you need to build your own.

## The Tool Landscape in 2026

Adversarial testing tools fall into four categories. **Attack generation tools** produce adversarial inputs: jailbreaks, prompt injections, toxic content, biased prompts. **Evaluation frameworks** run those attacks against your model and measure the results. **Orchestration platforms** manage test suites, track results over time, and integrate with CI/CD pipelines. **Specialized toolkits** focus on narrow domains: NLP robustness, vision model attacks, or adversarial perturbations. Most comprehensive adversarial testing programs use tools from multiple categories.

The maturity varies widely. Some tools are maintained by large organizations with dedicated teams and regular releases. Some tools are research projects that have not been updated in months. Some tools have extensive documentation and active communities. Some tools have a README and a handful of examples. The right tool is not always the most popular one. The right tool is the one that covers your attack surface, integrates with your stack, and is maintained well enough that you trust it in production.

## Garak: Comprehensive LLM Vulnerability Scanning

Garak, originally developed by NVIDIA and now maintained as an open-source project, is the most comprehensive LLM vulnerability scanner as of 2026. It tests for a wide range of vulnerabilities: jailbreaks, prompt injections, bias, toxicity, PII leakage, hallucination, and encoding-based attacks. It includes over 200 built-in probes covering known attack patterns and supports custom probe development.

Garak's strength is breadth. A single command runs a full vulnerability scan across dozens of attack categories. It outputs structured JSON results that can be parsed by CI/CD systems or fed into dashboards. It supports multiple LLM backends: OpenAI, Anthropic, Hugging Face models, vLLM-hosted models, and custom API endpoints. It runs locally or in cloud environments. It is designed for security teams who need to test a model against the full known threat landscape without manually implementing each attack type.

The limitation is that Garak is general-purpose. It does not know your specific use case, your domain-specific risks, or your custom safety policies. Its probes test for common vulnerabilities. If your application has unique risks — medical advice in rare disease contexts, legal reasoning about emerging regulations, or financial advice about novel instruments — Garak's default probes will not cover them. You extend it by writing custom probes, which requires understanding Garak's plugin architecture and Python development.

Teams use Garak as a baseline. Run it on every model update to catch regressions in known vulnerability categories. When it finds a failure, investigate whether the failure is a real risk or a false positive given your use case. Add custom probes for domain-specific risks. Use Garak's results as one input into your overall adversarial testing strategy, not as the complete strategy.

## PyRIT: Microsoft's Purple Team Framework

PyRIT, the Python Risk Identification Toolkit from Microsoft, is a framework for building adversarial testing workflows. Where Garak is a scanner, PyRIT is a toolkit. It provides primitives for generating adversarial prompts, managing multi-turn conversations, scoring model responses, and orchestrating complex attack scenarios. It is designed for red teams and security researchers who need flexibility to test novel attacks, not just run predefined test suites.

PyRIT's core concept is the attack orchestrator. You define a goal — "make the model reveal PII" or "bypass content filters" — and PyRIT generates attack prompts, sends them to the model, evaluates the responses, and iterates. It supports both manual prompt crafting and LLM-assisted attack generation, where one LLM generates adversarial prompts and another LLM evaluates whether the attack succeeded. This lets you automate exploratory red teaming, not just regression testing.

The framework includes a library of attack techniques: jailbreaks, crescendo attacks where prompts gradually escalate, multi-turn social engineering, and encoding-based attacks. It supports scoring functions for toxicity, bias, hallucination, and custom safety policies. It integrates with Azure AI Studio for teams running on Azure, but it also works with non-Azure models through custom adapters.

PyRIT's strength is flexibility. If you need to test a novel attack chain or a domain-specific vulnerability that no off-the-shelf tool covers, PyRIT gives you the building blocks to implement it. The limitation is that flexibility requires effort. PyRIT does not come with a turnkey test suite you can run on day one. You build your test suite using PyRIT's primitives. Teams use PyRIT when they have dedicated security researchers who are comfortable writing code and when their adversarial testing needs go beyond what scanners like Garak provide.

## Promptfoo: Developer-Friendly Eval and Red Team Tool

Promptfoo started as a prompt evaluation tool for developers and added red teaming capabilities in 2025. It is designed for engineering teams who treat adversarial testing as part of the development workflow, not as a separate security function. It runs locally, integrates with version control, and uses configuration files to define test cases. It is less comprehensive than Garak and less flexible than PyRIT, but it is easier to adopt and faster to get value from.

Promptfoo's red team mode includes built-in test suites for jailbreaks, prompt injections, PII leakage, and harmful content generation. You point it at your model or your prompt template, run the test suite, and get a report showing which attacks succeeded. It supports custom test cases defined in YAML or JSON, making it accessible to teams who are not comfortable writing Python. It generates HTML reports that developers can review locally, and it outputs JSON for CI/CD integration.

The tool is particularly strong for prompt engineering workflows. If your team iterates on prompt templates frequently, Promptfoo lets you test each iteration for both task performance and adversarial robustness. It runs regression tests to ensure that a prompt change that improves quality does not introduce a safety vulnerability. It is not as deep as Garak or as flexible as PyRIT, but it is faster to integrate into developer workflows.

Teams use Promptfoo when they want adversarial testing to be part of the prompt development process, not a separate audit stage. It works best for applications where prompts are versioned in code, where developers run tests locally before committing, and where the adversarial threat model is relatively standard.

## Adversarial Robustness Toolbox: ML Model Security

The Adversarial Robustness Toolbox, originally developed by IBM Research, is a Python library for testing machine learning models against adversarial attacks. It predates the LLM era and was originally designed for vision models, but it has been extended to support NLP models and multimodal systems. It includes attacks like FGSM, PGD, and adversarial perturbations that are more relevant to image classifiers than to text generation, but it also supports textual attacks like synonym substitution, character-level perturbations, and semantic similarity attacks.

ART's strength is depth in adversarial machine learning research. If your model is a classifier — spam detection, content moderation, sentiment analysis — ART provides attacks and defenses that are well-grounded in academic literature. It includes methods for generating adversarial examples, training robust models, and certifying model robustness. It is less useful for generative models or conversational AI, where the attack surface is less about misclassifying inputs and more about generating harmful outputs.

Teams use ART when they have traditional ML models in production alongside LLMs, when they need to test models that classify or rank rather than generate, or when they are implementing defenses like adversarial training and want a library with proven implementations.

## TextAttack: NLP-Focused Adversarial Framework

TextAttack is a framework for adversarial attacks on NLP models, developed by researchers and widely used in academic settings. It focuses on attacks that perturb input text to cause misclassification: changing words to synonyms, inserting typos, rearranging sentence structure. It includes recipes for well-known attacks like TextFooler, BERT-Attack, and BAE, and it supports custom attack development.

TextAttack is best suited for testing NLP classifiers: sentiment analysis, toxicity detection, named entity recognition. It is less directly applicable to generative models, though some teams use it to test whether prompt perturbations can bypass content filters. If your application includes a classifier that gates access to a generative model — for example, a toxicity classifier that filters user input before sending it to the LLM — TextAttack is a strong choice for testing whether an attacker can craft inputs that evade the classifier.

The tool is open source, well-documented, and has an active research community. It integrates with Hugging Face models. Teams use it when they need to test NLP classifiers as part of their adversarial testing strategy, not as a replacement for LLM-specific red teaming tools.

## Custom Tool Development: When and Why

No tool covers every attack surface. Every application has domain-specific risks, custom safety policies, or unique threat models that generic tools do not address. Custom tool development is not about reinventing the wheel. It is about extending existing tools or building new capabilities for gaps that matter.

Common reasons to build custom tools: your application processes structured data formats that standard tools do not test, your safety policies include domain-specific constraints that require custom scoring functions, your threat model includes multi-step workflows that require orchestration logic, or your integration requirements — specific CI/CD platforms, specific logging systems, specific compliance frameworks — are not supported by existing tools.

The pattern is to start with open-source tools and extend them. Garak supports custom probes. PyRIT supports custom attack orchestrators. Promptfoo supports custom test cases. Most teams build custom extensions before they build fully custom tools. A fintech company used PyRIT as the orchestration layer and built custom attack generators for financial domain vulnerabilities: prompts that tested whether the model would provide advice that violated securities regulations, prompts that tested whether the model leaked transaction patterns, prompts that tested bias in lending advice. The custom generators used PyRIT's infrastructure for running tests, scoring responses, and logging results.

When existing tools cannot be extended to meet your needs, building a custom tool is justified. A healthcare company built a custom adversarial testing framework that tested medical AI systems against clinical scenarios with known edge cases: rare diseases, drug interactions, contraindications. The tool generated adversarial medical queries, sent them to the model, and evaluated responses using a combination of rule-based checks and clinician review. No existing tool covered this domain deeply enough.

## Tool Selection Criteria

Choosing the right tool starts with your adversarial testing goals. Are you running regression tests on every deployment, or are you conducting periodic exploratory red teaming? Are you testing for known vulnerabilities, or are you discovering new attack patterns? Are you testing a single model, or are you testing a system with multiple models, classifiers, and retrieval components?

Evaluate tools on coverage, integration, maintenance, and customization. Coverage: does the tool test the attack categories that matter for your application? If you are primarily concerned with jailbreaks and prompt injections, a tool with deep coverage in those areas is more valuable than a tool with shallow coverage across twenty categories. Integration: does the tool work with your model hosting setup, your CI/CD platform, your logging infrastructure? A tool that requires major integration work is less valuable than a tool that fits into your existing stack. Maintenance: is the tool actively maintained, or is it a research artifact that has not been updated in a year? A tool with regular releases, active issues, and a responsive community is safer to depend on. Customization: can you extend the tool to cover domain-specific risks, or are you limited to its default test suite?

Most teams use multiple tools. Garak for broad vulnerability scanning. PyRIT for exploratory red teaming. Promptfoo for developer-facing regression tests. TextAttack for NLP classifier testing. Custom scripts for domain-specific vulnerabilities. The tools are complementary. Each one covers part of the attack surface. Together they provide defense in depth.

The adversarial testing tool ecosystem will continue evolving. New attacks will emerge. New tools will be built. Existing tools will add new capabilities. The principle stays constant: automated adversarial testing is not about running one perfect tool. It is about assembling a suite of tools, tests, and processes that continuously find vulnerabilities before your users do. The next chapter explores how to sustain this work over time — not as a one-time audit, but as a continuous red teaming practice embedded in your operating model.
