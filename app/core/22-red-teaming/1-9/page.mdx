# 1.9 — The Red Team Charter: Scope, Rules of Engagement, Boundaries

The difference between a red team and an attacker is a charter. Without one, red teaming is indistinguishable from hacking — unauthorized access, unclear boundaries, legal risk, and findings that leadership might ignore because no one agreed in advance what the red team was allowed to do. With a charter, red teaming becomes a structured, authorized, legally protected exercise with defined goals, clear communication paths, and findings that carry weight because everyone signed off on the process before it began.

A red team charter is the contract between the red team and the organization. It answers the questions that must be settled before testing starts: What systems are in scope? What actions are permitted? What data can be accessed? Who receives findings? What happens if something breaks? How are legal and ethical boundaries enforced? Without written answers to all of these, you are improvising adversarial testing in a legal and organizational minefield. With written answers, you have a framework that protects the red team, protects the organization, and ensures findings lead to fixes instead of finger-pointing.

## What a Red Team Charter Defines and Why It Matters

The charter is documentation and authorization. It specifies exactly what the red team is being asked to do, exactly what resources they can use, and exactly how their work will be used. It is signed by leadership — typically the CISO, CTO, or VP Engineering — which makes it an official directive, not a rogue security exercise. This signature matters. It protects red teamers from internal blowback if they find embarrassing vulnerabilities. It protects the organization from legal risk if adversarial testing accidentally causes damage. It creates accountability for acting on findings.

A charter defines scope: which AI systems, which environments, which attack vectors are in-scope versus out-of-scope. It defines rules of engagement: what methods are allowed, what data can be accessed, what actions require escalation. It defines boundaries: ethical limits, legal constraints, and off-limits areas that must not be touched even if technically vulnerable. It defines logistics: timeline, resources, access credentials, points of contact. It defines deliverables: what gets reported, in what format, to whom, and by when.

Why it matters is simple. Without a charter, a red team finding that the AI system can be tricked into leaking internal data becomes a political mess. Did they have permission to test data leakage? Were they supposed to access that environment? Should they have stopped when they realized the data was sensitive? Who decides whether this finding is valid or out-of-bounds? With a charter, the answers are already written down. The red team was authorized. The test was in-scope. The finding is valid. Now the organization's only job is to fix it.

A charter also creates psychological safety for red teamers. Their job is to find and report vulnerabilities, including embarrassing ones, including ones that implicate decisions made by senior engineers or leadership. If the charter does not protect them from retaliation, they will self-censor. They will file the safe findings and bury the ones that might get them in trouble. A charter that explicitly authorizes adversarial testing, specifies that findings will not result in blame, and guarantees that red teamers report to someone outside the product delivery chain removes that fear.

## Scope Definition: What's In Bounds, What's Off Limits

Scope is the boundary between authorized testing and unauthorized hacking. It must be specific. "Test the AI assistant" is not scope. "Test the customer-facing AI assistant deployed at assistant-dot-example-dot-com in the staging environment, including all prompt injection vectors, jailbreak attempts, tool misuse, and data exfiltration scenarios, excluding denial-of-service attacks and any testing against the production environment" is scope.

In-scope typically includes the target AI system, the environments designated for testing — staging, pre-production, dedicated red team sandboxes — and the attack vectors the organization wants validated. For an AI product, this usually means adversarial prompts, indirect prompt injection via user-uploaded content, tool misuse if the agent uses tools, training data extraction attempts, safety filter bypasses, and boundary violations where the system should refuse but might not.

Out-of-scope typically includes production environments unless explicitly authorized, attacks that cause service degradation or denial-of-service, testing against third-party systems the AI integrates with unless you have written permission from those third parties, social engineering of employees, and physical security testing. For AI systems specifically, testing against live user data is almost always out-of-scope unless the charter specifies a narrow exception with strong data handling controls.

Edge cases must be clarified in advance. Is the red team allowed to test whether the system leaks training data if doing so might expose proprietary data the company considers confidential? Are they allowed to test whether the agent can be tricked into sending emails if those emails would actually send, or must they stop before the final action executes? Are they allowed to test prompt injection via image inputs if the system accepts images, or is multimodal testing out-of-scope for this engagement?

A financial AI company running a red team exercise in early 2026 learned the cost of ambiguous scope the hard way. The charter said "test all adversarial prompt vectors" but did not specify which environments. The red team assumed staging was in-scope and production was not. They tested staging. They also tested a pre-production environment they assumed was staging but was actually a live mirror used for load testing with sanitized production data. They found a prompt injection that leaked account identifiers. The data was sanitized — no real user harm — but the test triggered alerts, involved the incident response team, and created confusion about whether the red team had gone out of scope. The ambiguity cost three days of meetings to untangle. A charter that explicitly listed in-scope environments by URL would have prevented it.

Scope should be written, not verbal. It should list specific systems, URLs, environments, API endpoints, and attack classes. It should be reviewed by Legal, Security, and Engineering before the red team starts. Ambiguity protects no one. Clarity protects everyone.

## Rules of Engagement: Ethical Boundaries, Legal Considerations, Data Handling

Rules of engagement define how the red team conducts testing, not just what they test. These rules ensure that adversarial testing stays ethical, legal, and aligned with the organization's values. They cover data handling, responsible disclosure, escalation protocols, and constraints on testing methods.

Data handling is first. If the red team discovers real user data, proprietary company information, or sensitive internal content during testing, the charter must specify what they do with it. The standard rule is: document that the data was accessible, note the method used to access it, and do not exfiltrate, store, or distribute the data itself. The finding is "the system can be tricked into leaking user email addresses via this prompt pattern," not a CSV file of actual user emails. If the red team needs to prove the finding, they provide one or two examples with sufficient detail to confirm the vulnerability, not the entire dataset.

For AI systems, this is especially important because many vulnerabilities involve the model outputting training data or confidential context. A finding that the model memorized and can be prompted to recite internal documents is valid and critical. A red team report that includes the full text of those internal documents is a secondary data leak. The rule is: prove the vulnerability exists, minimize exposure, report the method, and let Engineering validate the full scope internally.

Ethical boundaries address the limits of adversarial creativity. Red teamers are encouraged to think like attackers, but they are not authorized to act like attackers in ways that cause real harm. Testing whether the AI can be tricked into generating illegal content is in-scope. Actually distributing that content is not. Testing whether the agent can be made to exfiltrate data is in-scope. Exfiltrating the data to an external server the red team controls without telling anyone is not. The charter makes this explicit: you are authorized to find vulnerabilities and document them, not to exploit them for real-world gain or harm.

Legal considerations vary by jurisdiction and industry. In the United States, the Computer Fraud and Abuse Act makes unauthorized access to computer systems a federal crime. The charter is the authorization that makes red teaming lawful. Without it, even well-intentioned security testing can create legal risk. In the EU, GDPR creates constraints around accessing personal data, even in testing. The charter should confirm that Legal has reviewed the scope and rules, that testing complies with relevant laws, and that the organization indemnifies the red team for actions taken within the charter's bounds.

If the red team is external, the charter often takes the form of a formal statement of work or addendum to a consulting agreement. It serves as both technical scope and legal protection. If the red team is internal, the charter is an internal document signed by leadership, often stored alongside security policies and incident response plans.

## Authorization and Documentation Requirements

Authorization must be explicit and traceable. The charter should include signatures from whoever has the authority to authorize security testing against the target systems — typically the CISO, CTO, or product owner. For external red teams, this authorization is part of the contract. For internal red teams, it is an email thread, a signed memo, or an entry in a governance system that proves leadership knew about and approved the exercise.

Documentation requirements specify what the red team must produce and when. The minimum is a written findings report that includes every identified vulnerability, severity ratings, reproduction steps, and remediation recommendations. Many charters also require interim updates — a daily or weekly summary during the engagement so Engineering is not blindsided by critical findings on the last day.

For high-risk systems or regulatory contexts, the charter might require additional documentation: a methodology statement explaining how testing was conducted, a scope confirmation that lists everything tested, evidence artifacts like screenshots or request logs, and a sign-off report after fixes are validated. This documentation serves multiple purposes. Engineering uses it to fix issues. Compliance teams use it to satisfy audit requirements. Leadership uses it to report to the board or to customers asking for security validation.

The charter also specifies documentation security. Red team reports contain detailed descriptions of vulnerabilities, which makes them sensitive documents. They should be stored securely, access-controlled to a need-to-know basis, and never transmitted via unencrypted email. If a vulnerability is critical enough that public disclosure would create immediate risk, the charter might include a clause requiring all documentation to be handled as confidential until the issue is patched.

## Communication Protocols: Who Gets Findings, Escalation Paths

The charter defines who receives findings, in what format, and under what circumstances. The default path is: red team reports findings to a designated security lead or product owner, who triages them, assigns them to Engineering, and tracks remediation. This works for most findings. But not all findings are equal. Some require immediate escalation.

Escalation paths cover the edge cases. If the red team finds a critical vulnerability — one that could lead to significant data breach, regulatory violation, or user harm if exploited — the charter specifies who gets notified immediately. This might be the CISO, the CTO, the incident response lead, or all three. It might trigger an emergency fix process that pauses other work. The point is that the red team does not have to guess whether a finding is "critical enough" to bypass normal channels. The charter defines the criteria and the escalation contact.

Communication protocols also cover timing. When do findings get reported? Some charters require daily summaries so Engineering can start fixing issues during the engagement, not after it ends. Some require a final report only, on the assumption that daily updates would distract from testing. The right answer depends on engagement length and organizational preference. A two-day sprint benefits from real-time communication. A four-week engagement benefits from weekly updates and a comprehensive final report.

The charter also clarifies what happens if the red team finds something that might indicate an active security incident — evidence that an attacker has already exploited a vulnerability, signs of compromise, or anomalies that suggest the system is behaving in ways that cannot be explained by adversarial testing alone. In these cases, the red team escalates immediately to the incident response team, and testing might pause until the organization determines whether there is an active threat.

For external red teams, communication protocols are especially important because they prevent misunderstandings about who owns findings and what the organization commits to doing with them. The charter typically includes a clause that the organization will acknowledge findings, provide a timeline for remediation, and notify the red team when fixes are deployed. This is the foundation of responsible disclosure. The red team reports privately and waits. The organization fixes and confirms. Only then — if at all — does the finding become public.

## Time-Boxing and Resource Constraints

Red team engagements are time-boxed by necessity. You cannot test indefinitely. The charter specifies the duration: two weeks, four weeks, a three-day sprint. It also specifies effort: full-time testing, part-time testing, or time-bound bursts like a structured hackathon. Time-boxing forces prioritization. The red team focuses on the most critical attack vectors first, knowing they might not have time to test everything.

Resource constraints are also defined. Does the red team get access to internal documentation, architecture diagrams, source code? Or are they testing black-box with no inside knowledge? Both approaches are valid. White-box testing with full documentation finds deeper architectural issues. Black-box testing simulates how an external attacker with no inside information would approach the system. The charter specifies which model applies.

The charter also defines what happens if testing runs over time or uncovers issues that require extended investigation. If the engagement is contracted for two weeks but the red team finds a critical vulnerability on day thirteen that requires another week to fully document, does the timeline extend? Does the scope narrow? The charter should anticipate this and specify whether extensions are possible, who approves them, and how they are billed if the red team is external.

## The Difference Between Red Teaming and Malicious Hacking

The difference is intent, authorization, and disclosure. Red teamers are authorized to test, their intent is to improve security, and they disclose findings responsibly to the organization. Malicious hackers are unauthorized, their intent is harm or gain, and they do not disclose — they exploit.

But from a technical standpoint, the actions can look identical. A red teamer crafting a jailbreak prompt uses the same techniques a malicious actor would. A red teamer testing for training data extraction uses the same methods an attacker would. The difference is entirely in the charter: the red teamer has written permission, a defined scope, and an obligation to report findings. The attacker has none of those.

This distinction matters legally and culturally. Legally, the charter is what makes red teaming lawful. Culturally, it is what makes red team findings credible and actionable instead of threatening. If a security researcher approaches your company and says "I found a vulnerability in your AI," your response depends entirely on whether they acted within responsible disclosure norms. If they reported it privately, gave you time to fix it, and did not exploit it, they are acting like a red teamer and you owe them a serious response. If they published it publicly without notice, exfiltrated data, or demanded payment, they are acting like an attacker and your response involves Legal and law enforcement.

The charter formalizes the cooperative relationship. It says: we are asking you to attack our system, within these bounds, so we can fix it before someone attacks it without bounds. That agreement is what makes red teaming possible.

## Sample Charter Elements for AI Red Teams

A minimal red team charter for an AI system includes the following elements:

**Engagement summary**: One paragraph describing the goal — "assess adversarial robustness of the customer support AI assistant prior to public launch."

**Scope**: Specific systems, environments, and URLs. "In-scope: staging deployment at staging-assistant-dot-example-dot-com, API endpoints at api-staging-dot-example-dot-com. Out-of-scope: production environments, third-party integrations, denial-of-service testing."

**Attack vectors authorized**: "Adversarial prompting, jailbreak attempts, indirect prompt injection via user inputs, tool misuse, boundary testing of refusal mechanisms, training data extraction attempts. Not authorized: social engineering of employees, physical access testing."

**Timeline and effort**: "Two-week engagement, full-time testing by a team of two red teamers, between March 1 and March 15, 2026."

**Data handling rules**: "Document vulnerabilities without exfiltrating user data. Provide minimal examples to prove findings. Do not store, distribute, or further process any sensitive data accessed during testing."

**Reporting and escalation**: "Daily email summaries to security-lead@example-dot-com. Final written report delivered by March 18. Critical findings escalate immediately to CISO and CTO via Slack incident channel."

**Authorization**: "Approved by Jane Doe, CISO, and John Smith, VP Engineering, on February 20, 2026. Digital signatures attached."

**Legal and ethical boundaries**: "Testing conducted in compliance with all applicable laws. Red team indemnified for actions within scope. Any findings suggesting active compromise escalated immediately to incident response."

This is the skeleton. More complex engagements might add sections on access credentials, tool usage, third-party integrations, customer notification requirements, or regulatory alignment. But even a minimal charter with these elements is vastly better than starting a red team exercise with a handshake and a verbal "go break stuff."

The charter is not bureaucracy. It is the foundation that makes adversarial testing safe, legal, credible, and actionable. Without it, red teaming is just chaos with good intentions. With it, red teaming becomes the structured discipline that finds and fixes vulnerabilities before adversaries do.

The next question is where the field stands today — how mature adversarial testing practices are in 2026, what tools exist, what gaps remain, and where red teaming for AI is headed.

