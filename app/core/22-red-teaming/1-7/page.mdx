# 1.7 — When to Red Team: Timing in the Development Lifecycle

The most expensive red team engagement is the one that happens after launch. Not because external firms charge more for production systems — though they do — but because every finding now requires live system changes, user communication, and potential regulatory disclosure. The second-most expensive red team engagement is the one that never happens at all.

Timing determines whether red teaming is a quality gate or a damage control exercise. Get it right and vulnerabilities die in staging. Get it wrong and they live in production until a customer, researcher, or regulator finds them first. The question is not whether to red team. The question is when, how often, and with what intensity for your risk profile.

## Pre-Launch Red Teaming: Before Any Production Deployment

The non-negotiable baseline is this: no AI system reaches production without at least one red team assessment. Not a quick internal test. Not a few adversarial prompts from the team that built it. A structured red team exercise with fresh eyes, documented methodology, and a findings report that goes to leadership before the deploy button gets pressed.

For a Risk Tier 1 system — high-stakes, regulated, user-facing — this means a formal engagement weeks before launch. You bring in external red teamers if you can afford it, or dedicate internal security and ML engineers who had no part in building the system. You give them two to four weeks depending on system complexity. They get full documentation, API access, and permission to break things in staging. The finding threshold is zero critical vulnerabilities and fewer than five high-severity issues unmitigated. Anything worse delays launch.

For Risk Tier 2 and 3 systems, the timeline compresses but the requirement holds. You might run a one-week internal red team sprint. You might structure it as a hackathon where engineers who didn't build the prompt architecture spend a day trying to break it. The format flexes. The gate does not. If you ship without testing adversarial resilience, you are hoping your users are kind and your attackers are lazy. Neither assumption survives contact with the internet.

The most common mistake is treating pre-launch red teaming as a checklist item that happens the week before release. You find twelve critical issues. Leadership asks how long to fix them. Engineering says three weeks. Launch is in five days. Now you're choosing between shipping a system you know is vulnerable or missing a business deadline. This is why mature teams run preliminary red teaming during development — lightweight adversarial testing in monthly sprints — so the pre-launch formal assessment finds refinements, not catastrophes.

## Post-Major-Change Red Teaming: After Model Updates, Prompt Changes, Tool Additions

Every significant change to an AI system resets part of its attack surface. You swap from Claude Opus 4.5 to GPT-5.2 and the jailbreak techniques that failed before might work now. You add a tool that lets the agent query internal databases and you just gave attackers a new vector. You update the system prompt to improve tone and you accidentally removed a safety instruction that was blocking indirect prompt injection.

Post-major-change red teaming targets the delta. You do not need to retest the entire system every time you change a parameter. You need to test the components that changed and the interactions those changes created. If you updated the model, you retest jailbreaks, refusals, and boundary enforcement. If you added a tool, you test tool misuse, parameter injection, and privilege escalation through that tool. If you changed the prompt, you test whether safety instructions still hold under adversarial input.

A financial services company running a contract analysis agent learned this the expensive way in late 2025. They upgraded from a mid-2025 model to a November 2025 release for better accuracy. The upgrade improved contract extraction by four percentage points. It also introduced a regression where the model would, under specific prompt patterns, output its internal reasoning about why it was refusing a request — and that reasoning included fragments of the original system prompt. An external researcher found it in three days. The company had run extensive pre-launch red teaming nine months earlier but skipped it for the model upgrade because "it's just a model swap, same architecture, same prompts." They were wrong.

The rule: any change that touches the model, the prompt architecture, the tool set, or the data pipeline triggers targeted red teaming before that change reaches production. The scope scales with risk. A parameter tweak might get an hour of adversarial testing from the team. A full model replacement gets a multi-day formal exercise. But nothing ships untested.

## Continuous Red Teaming: Ongoing as Part of Release Process

Mature teams do not red team once before launch and then hope for the best. They build adversarial testing into the release pipeline the same way they build unit tests and integration tests. Every significant commit gets some level of adversarial validation before it merges. Every sprint includes time allocated for security-focused testing. Every quarter includes a formal red team week where the sole objective is to break the system.

Continuous red teaming works at multiple timescales. Daily, automated adversarial test suites run against every build — known jailbreaks, boundary violations, refusal bypasses. These catch regressions fast. Weekly, engineers dedicate a few hours to manual adversarial exploration — creative attacks, novel injection patterns, edge cases the automated suite missed. Monthly, a rotating cast of engineers from other teams spend a day red teaming systems they did not build. Quarterly, you run a structured exercise that simulates a motivated attacker with time and creativity.

The key is integration. Red teaming is not a separate phase that happens "later." It is embedded in the development cadence. Your CI/CD pipeline includes adversarial test gates. Your sprint planning includes red team hours. Your roadmap includes quarterly security sprints. When adversarial testing is continuous, you catch vulnerabilities when they are single-commit fixes, not architectural rewrites.

A healthcare AI company running a clinical documentation assistant adopted continuous red teaming in early 2025 after a pre-launch red team found twenty-three critical issues two weeks before their target launch. The findings delayed release by six weeks and cost them a partnership deadline. The CTO's response was not to skip red teaming next time. It was to make it continuous so findings arrive in small batches throughout development instead of all at once at the worst possible moment. A year later, their quarterly formal red teams find an average of three medium-severity issues. Daily and weekly testing catches another dozen minor issues per month. None of them block releases because continuous testing prevents accumulation.

## Event-Triggered Red Teaming: After Incidents, Before Enterprise Onboarding

Certain events mandate immediate red team reassessment even if you are already on a regular cadence. The first is any security incident or near-miss. If a user found a jailbreak that reached production, you red team immediately to find what else you missed. If an internal QA test found a prompt injection that should have been caught earlier, you reassess your testing coverage. Incidents are signals that your adversarial testing has gaps. The response is not just to patch the specific issue. It is to red team the entire class of vulnerabilities that issue represents.

The second trigger is enterprise customer onboarding, especially for regulated industries. A Fortune 500 financial institution is not going to deploy your AI assistant without third-party security validation. A hospital system is not going to integrate your clinical summarization tool without evidence that adversarial testing happened and passed. These customers often require recent red team reports — "recent" meaning within the last ninety days — as part of their procurement process. If your last formal red team was nine months ago, you are either running a new one or losing the deal.

The third trigger is regulatory inquiry or audit. The EU AI Act's high-risk system requirements include adversarial robustness testing. If you are in scope and regulators ask for evidence, "we tested it internally a year ago" does not meet the standard. You need documented, recent, ideally third-party red team assessments. The same applies to HIPAA audits for healthcare AI, SOX compliance for financial AI, and any sector where regulatory scrutiny is increasing.

Event-triggered red teaming is unplanned by definition, which means you need budget and team capacity held in reserve. Mature organizations allocate ten to fifteen percent of their annual security budget to reactive red teaming. They maintain relationships with external firms who can start an engagement within two weeks. They have internal runbooks for spinning up an emergency red team sprint. Because when the trigger event happens, the clock is already running.

## The Cost of Red Teaming Too Late vs. Too Early

Red teaming too late means finding critical vulnerabilities when fixing them is maximally disruptive. You are in production. Users depend on the system. Changing the architecture now affects live traffic. Updating the prompt now requires regression testing against months of real-world usage patterns. Rolling back a tool integration now means telling enterprise customers a feature is being removed. The technical cost is high. The organizational cost — missed deadlines, broken trust, emergency all-hands meetings — is higher.

A legal tech company found this in mid-2025. They built a contract review assistant, launched it to three hundred law firms, and ran their first formal red team assessment four months post-launch. The red team found that specific adversarial clauses in uploaded contracts could cause the agent to misrepresent key terms — a catastrophic failure mode for a legal product. Fixing it required rewriting the extraction logic, updating the prompt architecture, and revalidating against every contract type in their test corpus. The fix took seven weeks. During that time, they had to quietly warn existing customers, pause new sales, and disclose the issue to their Series B investors. The total cost — engineering time, lost revenue, reputational damage — exceeded two million dollars. A pre-launch red team would have cost twenty thousand dollars and two weeks.

Red teaming too early — before the system is stable enough to meaningfully test — wastes red team time on findings that will be obsolete by launch. If you bring in external red teamers when your prompt architecture is still changing daily, half their findings will be irrelevant by the time they deliver the report. If you run a formal exercise before you have integrated all planned tools, you are only testing part of the attack surface.

The timing sweet spot is late enough that the system is feature-complete and architecturally stable, early enough that findings can be fixed without disrupting launch timelines. For most teams, this means red teaming begins when the system enters staging and feature freeze is two to four weeks away. Large, complex systems might need multiple rounds — an early-stage red team during alpha to catch architectural issues, a pre-launch red team during beta to catch refinement-level vulnerabilities.

## Integration with CI/CD and Release Gates

Red teaming integrates with your release process the same way security scanning, performance testing, and compliance checks integrate. You define gates: specific red team criteria that must pass before a build can promote from staging to production. You automate what can be automated and require manual validation for what cannot.

A basic gate structure looks like this. Every commit to the main branch triggers an automated adversarial test suite — thirty to fifty known attack patterns across jailbreaks, injections, boundary violations, refusals. If any test fails, the build does not deploy. Weekly, a human red teamer runs a structured manual test session and files findings in the same ticketing system as bugs. Critical and high-severity findings block the next release. Medium-severity findings require mitigation plans. Low-severity findings get prioritized into the backlog.

More mature setups include tiered gates. Automated adversarial tests run on every pull request. Manual adversarial spot-checks run on every staging deploy. Formal red team exercises — internal or external — run before every major release or quarterly, whichever is more frequent. The findings feed into a security dashboard that leadership reviews weekly. If the trend is increasing critical findings over time, you slow down feature development and invest in foundational security work. If the trend is flat or decreasing, you are keeping pace with the adversarial threat.

The integration is technical and cultural. Technical: your CI/CD tooling must support adversarial test suites, your issue tracker must categorize security findings, your deploy scripts must respect red team gates. Cultural: your engineering team must treat red team failures the same way they treat broken tests — as blocks, not suggestions. Your leadership must accept that red team gates will occasionally delay releases and view that delay as the system working, not as friction to be minimized.

## How Frequency Should Scale with Risk Tier

Risk Tier 1 systems — high-stakes, regulated, user-facing decisions with significant consequences — require the highest red team cadence. Pre-launch formal red team of at least two weeks. Post-major-change targeted red teaming for every significant update. Quarterly formal red team exercises even if nothing changed. Continuous automated adversarial testing in CI/CD. Annual third-party red team assessments for regulatory and enterprise customer requirements. This is the cost of operating in high-risk domains. You do not get to skip it.

Risk Tier 2 systems — moderate stakes, less regulatory pressure, some tolerance for occasional errors — can reduce frequency but not eliminate it. Pre-launch red team of one week. Post-major-change testing for model swaps and tool additions, skippable for minor prompt updates. Semi-annual formal red team exercises. Continuous automated testing. Third-party assessment every eighteen to twenty-four months or when major enterprise deals require it.

Risk Tier 3 systems — low stakes, internal tools, experimental features — need baseline red teaming but can operate with lighter cadence. Pre-launch red team of a few days or a structured internal sprint. Post-major-change testing for significant updates, judgment call for minor ones. Annual formal red team exercise. Automated testing for known vulnerability classes. Third-party assessment optional unless customer contracts require it.

The frequency scales with risk, but the floor is not zero. Even a low-stakes internal tool can become a security liability if it leaks data, gets repurposed into a higher-stakes context, or becomes a pivot point for attackers targeting your infrastructure. The adversarial mindset does not distinguish between "important" and "unimportant" systems. Attackers exploit whatever is weakest. Your red team cadence must ensure nothing is weak enough to be the easiest path in.

The next question is who performs this testing — and whether internal teams, external experts, or a hybrid model delivers the adversarial rigor your system requires.

