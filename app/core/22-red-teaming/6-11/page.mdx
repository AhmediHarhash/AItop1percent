# 6.11 — Defending Against Extraction: What Works

Most teams approach data extraction defense with a single technique — prompt filtering, output sanitization, or training data removal — and assume the problem is solved. The problem is never solved with one layer. Extraction attacks are multi-stage, adaptive, and relentless. Defense must match that reality. No single technique prevents all leakage. Understanding what works, what fails, and how to combine defenses is the difference between theoretical protection and actual resilience.

## The Defense Challenge

Defending against extraction is harder than attacking. An attacker needs one successful prompt. A defender needs every prompt to fail safely. The attacker iterates freely. The defender must maintain functionality, user experience, and performance while blocking adversarial behavior. The attacker has time. The defender has deadlines. This asymmetry is permanent. Defense planning starts with accepting that perfect prevention is impossible. The goal is not zero leakage — it is making extraction so difficult, so detectable, and so limited that it becomes impractical.

The second challenge is the **accuracy-privacy tradeoff**. The more a model knows, the better it performs. The better it performs, the more it has memorized. Memorization enables extraction. You can train a model on scrubbed, sanitized data with zero sensitive information — and it will leak nothing, because it knows nothing. It will also fail to solve real user problems. A medical AI that has never seen real patient data cannot diagnose real patients. A financial AI that has never seen real transaction patterns cannot detect real fraud. Privacy and utility are in tension. Every defense technique that reduces leakage also risks reducing capability.

The third challenge is **detection difficulty**. Many extraction attacks look identical to legitimate use. "What are common symptoms for diabetes?" is a valid medical query. "What are the symptoms documented for patient ID 572849?" is an extraction attempt. The model cannot always tell the difference. Neither can automated filters. Some leakage is obvious — exact social security numbers, verbatim training data. Some leakage is subtle — stylistic mimicry, high-confidence completions of sensitive prefixes, aggregated information that reveals individual records. Defending the obvious is table stakes. Defending the subtle is where most systems fail.

## Training Data Sanitization

The first defense layer is removing sensitive data before training. If the model never sees PII, PHI, financial records, or credentials, it cannot leak them. Sanitization is conceptually simple and operationally complex. It requires identifying every sensitive pattern, scrubbing it correctly, and validating that scrubbing did not destroy the data's utility.

**Pattern-based scrubbing** uses regular expressions and named entity recognition to find and remove known sensitive patterns: social security numbers, credit card numbers, email addresses, phone numbers, addresses, dates of birth. This works for structured PII. It fails for unstructured or contextual information. A document that says "the patient lives in the blue house at the end of Oak Street" contains no PII by pattern, but it might uniquely identify someone in a small town. Scrubbing patterns catches the easy cases. It misses the hard ones.

**Synthetic data replacement** swaps real data with realistic but fake data. Real names become synthetic names. Real addresses become plausible but non-existent addresses. Real transaction amounts become statistically similar but randomized amounts. The model trains on data that looks real but cannot be traced to actual individuals. Synthetic replacement preserves statistical properties, which preserves model utility. But it is not foolproof. If synthetic data is generated poorly, patterns from real data leak through. If the same synthetic record is used in multiple contexts, it becomes a unique identifier. If real and synthetic data are mixed, the model learns to distinguish them.

**Differential privacy in training** adds noise during the training process to mathematically bound what the model can memorize about any individual record. DP-SGD, the most common technique, clips gradients and adds calibrated noise at each training step. The privacy budget — epsilon — controls the trade-off between privacy and accuracy. Lower epsilon means stronger privacy and lower model performance. Higher epsilon means weaker privacy and better performance. DP training is the gold standard for provable privacy guarantees, but it requires expertise, computational overhead, and careful tuning. Most teams do not use it because the accuracy cost is too high for their use case.

The reality is that sanitization is necessary but insufficient. Even perfectly sanitized training data does not prevent leakage if the model is later fine-tuned on unsanitized data, if retrieval systems pull unsanitized documents, or if users inject sensitive data into prompts that the model echoes back.

## Fine-Tuning for Privacy

Fine-tuning introduces new leakage risk. The base model might be clean, but fine-tuning on customer data, domain-specific documents, or user-generated content creates memorization opportunities. Defending against fine-tuning leakage requires controlling what data is used, how it is formatted, and how the model is updated.

The first step is **fine-tuning data review**. Do not fine-tune on data you have not inspected. Automated scrubbing is a start, but manual review catches edge cases. Review a sample of fine-tuning data for PII, credentials, proprietary information, and unintended context. If you are fine-tuning on customer support tickets, check for tickets that include passwords, account details, or sensitive customer complaints. If you are fine-tuning on legal documents, check for privileged attorney-client communications. The review does not need to cover every record, but it needs to cover enough to catch patterns.

The second step is **low-rank adaptation instead of full fine-tuning**. LoRA and similar parameter-efficient fine-tuning methods update a small percentage of the model's weights. This reduces memorization capacity. A full fine-tune can overfit on training data and memorize verbatim. A LoRA fine-tune with rank 8 or 16 has limited capacity to store exact records. It learns patterns and style, not rote memorization. LoRA does not eliminate leakage, but it makes extraction harder. For high-risk use cases, parameter-efficient methods are the safer default.

The third step is **regularization and early stopping**. Overfitting increases memorization. Regularization techniques — dropout, weight decay, label smoothing — reduce overfitting. Early stopping prevents the model from training so long that it memorizes the training set. Monitor validation loss during fine-tuning. When validation loss stops improving or starts increasing, stop. A model that generalizes well memorizes less. A model that memorizes well generalizes poorly. Optimize for generalization.

The fourth step is **differential privacy in fine-tuning**. Just as DP-SGD can be applied to base model training, it can be applied to fine-tuning. The privacy budget for fine-tuning is separate from the base model's budget. Fine-tuning with DP adds noise to gradient updates, limiting what the model can learn about individual fine-tuning examples. This is especially important in scenarios where fine-tuning data is highly sensitive — medical records, financial transactions, personal communications. The accuracy cost is still significant, but the privacy guarantee is mathematically provable.

## Output Filtering and Detection

Even if the model memorized sensitive data, you can prevent leakage by filtering outputs before they reach users. Output filtering is the last line of defense. It catches extraction attempts that bypassed all upstream protections.

**Pattern-based output filtering** scans model outputs for known sensitive patterns: social security numbers, credit card numbers, API keys, email addresses, phone numbers. If a pattern is detected, the output is blocked and replaced with a generic refusal. This is fast, deterministic, and effective for structured PII. It fails for unstructured information and adversarial obfuscation. An attacker who requests "the patient's social security number but with spaces between each digit" might bypass regex-based filters. Pattern-based filtering is necessary but not sufficient.

**Semantic detection** uses a secondary model to classify whether the primary model's output contains sensitive information. The detector model is trained on examples of safe and unsafe outputs. It evaluates the primary model's response and assigns a risk score. High-risk outputs are blocked. Medium-risk outputs trigger human review. Low-risk outputs are delivered. Semantic detection catches leakage that pattern matching misses: indirect references, paraphrased sensitive information, aggregated data that reveals individuals. But semantic detectors are themselves models, which means they have false positives and false negatives. Tune thresholds to your risk tolerance.

**Confidence thresholding** blocks outputs where the model is suspiciously confident. If the model generates a response with 99 percent confidence on a question it should not be able to answer, that is a memorization signal. Legitimate responses to novel questions have moderate confidence. Memorized responses have high confidence. This technique is crude — confidence does not always correlate with memorization — but it catches some extraction attempts that other methods miss. Combine it with other filters, not as a standalone defense.

**Rate limiting and anomaly detection** stop extraction at scale. An attacker probing for leakage will submit dozens or hundreds of similar prompts in a short time. A legitimate user does not. Rate limit requests per user, per session, or per IP. If a user exceeds the limit, throttle responses or require reauthentication. If a user submits ten prompts in one minute that all contain the phrase "retrieve user data," flag the session for review. Rate limiting does not prevent extraction, but it slows it down and increases detection likelihood.

## Access Control and Authentication

Many extraction attacks succeed because the attacker has more access than they should. Strengthening access control reduces attack surface.

**Role-based access control** ensures users only access data appropriate to their role. An admin might legitimately query user records. A standard user should not. The model's behavior must respect the user's role, not just the user's ability to form a valid prompt. This requires passing authenticated user context to the model and validating permissions before generating responses. If your system treats all users as equivalent, every user can attempt every extraction vector.

**Session-based access control** isolates user sessions. A user's session should not have access to another user's data, conversation history, or retrieval context. Session isolation prevents cross-user leakage and limits the blast radius of a compromised session. If an attacker hijacks a session, they gain access to that session's data, not the entire system's.

**API key and credential management** prevents extraction via stolen or leaked credentials. Rotate API keys regularly. Revoke keys that are no longer needed. Monitor for keys used from unexpected locations, at unexpected times, or with unusual request patterns. An API key used from a new country, submitting 500 requests in an hour, all asking for user data, is likely compromised. Detect and revoke immediately.

**Multi-factor authentication for sensitive operations** adds a layer of defense for high-risk queries. If a user requests access to financial records, personal health information, or administrative data, require MFA confirmation. This stops automated extraction scripts and raises the cost of manual attacks. MFA does not prevent leakage, but it makes bulk extraction impractical.

## Monitoring and Alerting

Defense is not static. It is continuous monitoring, detection, and response. Monitoring turns extraction attempts from silent successes into visible incidents.

**Prompt logging** records every user prompt and model response. Logs must include timestamps, user IDs, session IDs, roles, and metadata. When an extraction incident occurs, logs are the only way to determine scope, duration, and impact. Without logs, incident response is guesswork. Logs must be retained long enough to support forensic investigation — 90 days minimum, one year for high-risk systems.

**Anomaly detection** flags unusual behavior: users requesting data outside their normal patterns, sessions with abnormally high request rates, prompts that match known extraction templates. Anomalies are not always attacks, but attacks are almost always anomalies. Tune detection thresholds to balance false positives and false negatives. Too many false positives and your security team ignores alerts. Too few and real attacks slip through.

**Canary monitoring** continuously probes for planted sensitive data. If a canary appears in model output, trigger an immediate alert. Canaries provide definitive evidence of leakage. Their detection is never a false positive. Canary alerts should wake people up. Literally. They indicate that defenses failed and sensitive data is accessible.

**Alerting workflows** route detected incidents to the right teams. Prompt anomalies go to security. Canary detections go to engineering and incident response. Access control violations go to compliance. Alerts must be actionable: what was detected, where, when, by whom, and what to do next. Generic alerts — "security event detected" — are noise. Specific alerts — "user ID 8473 submitted 14 prompts in 90 seconds requesting cross-tenant data" — enable response.

## The Defense-in-Depth Approach

No single technique prevents all extraction. Defense requires layers. Training data sanitization reduces what the model knows. Fine-tuning controls limit what it learns. Output filtering catches what it tries to say. Access control limits who can ask. Monitoring detects when defenses fail. Each layer is imperfect. Together, they make extraction difficult enough that most attackers move to easier targets.

**Layer one: data governance**. Remove sensitive data before training. Sanitize fine-tuning data. Use differential privacy when privacy is critical.

**Layer two: model architecture**. Use parameter-efficient fine-tuning to limit memorization. Apply regularization to reduce overfitting. Avoid training to convergence on sensitive data.

**Layer three: access control**. Authenticate users. Enforce role-based permissions. Isolate sessions. Require MFA for high-risk operations.

**Layer four: output filtering**. Scan outputs for PII patterns. Use semantic detectors for unstructured leakage. Apply confidence thresholding for memorization signals.

**Layer five: monitoring and response**. Log all interactions. Detect anomalies. Monitor canaries. Alert on violations. Respond within minutes, not days.

Defense-in-depth is not paranoia. It is the acknowledgment that every defense has bypass techniques. The goal is not to build an impenetrable wall. It is to build enough walls that an attacker cannot bypass them all without detection.

## What Does Not Work

Some defenses look effective but fail under adversarial pressure. Knowing what does not work prevents wasted effort.

**Hoping attackers do not try.** They will. Extraction is too easy and too valuable. Assume every system is being probed, continuously.

**Relying on model refusals alone.** Refusals can be bypassed with jailbreaks, role-play, or multi-turn manipulation. Refusals are a UX layer, not a security layer.

**Trusting that fine-tuning is safe because the base model is safe.** Fine-tuning introduces new memorization risk. Every fine-tune is a new attack surface.

**Assuming retrieval access control is enough.** If the model receives retrieved documents, it can leak their contents even if the user should not have access. Filter what the model receives, not just what the user requests.

**Ignoring partial leakage.** An attacker who extracts 20 percent of a patient record still violated privacy. Partial leakage is still leakage.

**Treating extraction as a one-time security audit.** Extraction risk evolves with every model update, data change, and feature release. Defense is continuous.

## The Realistic Standard

Defend against extraction with the assumption that motivated attackers will probe your system for months. Build multiple independent defense layers. Monitor continuously. Respond to anomalies within minutes. Accept that some leakage might still occur, but make it detectable, limited, and prosecutable. Perfect defense is impossible. Disciplined, layered, monitored defense is achievable.

In 6.12, we examine privacy-preserving techniques that add mathematical guarantees to extraction defense.