# 10.3 — Rules of Engagement: Ethical and Legal Boundaries

Most teams think the hard part is finding vulnerabilities. They are wrong. The hard part is finding vulnerabilities without crossing legal, ethical, or organizational boundaries that turn testing into harm. Red teaming operates in a space where the technical goal — break the system — conflicts with ethical constraints — do no harm. Rules of engagement are what keep that tension from collapsing into actual damage.

## Why Rules Matter More for AI Than for Traditional Systems

In traditional penetration testing, the rules are well-established. You do not attack production without permission. You do not access real user data. You do not destroy data or disrupt services. You document everything and disclose responsibly. These rules still apply to AI red teaming — but they are not sufficient.

AI systems introduce new ethical boundaries that traditional security testing never had to address. When you test a chatbot for jailbreaking, you are generating hateful, violent, or disturbing content to see if the model will reproduce it. When you test for training data extraction, you are attempting to exfiltrate information that might include real user conversations or proprietary data. When you test a code generation model, you are trying to make it produce malicious code that could, if executed, cause harm. When you test an agent with tool access, you are trying to trick it into taking actions it should not take — actions that might have real consequences if the test environment is not perfectly isolated.

The difference is that AI testing does not just probe technical boundaries — it probes content boundaries, behavioral boundaries, and harm boundaries. You cannot test whether a model will refuse hate speech without generating hate speech. You cannot test extraction attacks without attempting extraction. The adversarial inputs you create during testing are, in some cases, indistinguishable from the attacks you are trying to prevent.

That means rules of engagement for AI red teaming must address not just what systems you can test and how, but what content you can generate, how you handle harmful outputs, who can access test data, and what happens if testing accidentally creates harm. Without those rules, red teaming becomes the thing it is supposed to prevent.

## Legal Boundaries Keep You Out of Court

The first rule category is legal boundaries: what you are legally permitted to do during testing. This seems obvious but is often underspecified. If your AI system uses a third-party model API — OpenAI, Anthropic, Google, Mistral — you are bound by that provider's terms of service. Most provider terms explicitly prohibit adversarial testing, jailbreaking, or systematic attempts to bypass content filters unless you have written permission. Running a red team engagement against a third-party API without that permission is a contract violation and, depending on jurisdiction and severity, could expose you to legal liability.

If you are testing a third-party API, get explicit written permission from the provider before you start. Some providers have formal red teaming programs and will grant permission if you follow their process. Some providers prohibit it entirely. Some operate in a gray zone where the terms say no but enforcement is inconsistent. Do not assume that "we are just testing our own system" protects you if your system relies on a third-party model — you are still sending adversarial inputs to someone else's service, and they get to decide whether that is allowed.

For systems you fully control — self-hosted models, models you have fine-tuned, infrastructure you operate — legal boundaries are simpler but still exist. If your testing involves accessing production data, you must comply with data protection regulations. GDPR requires lawful basis for processing personal data — "we are red teaming our system" is not automatically a lawful basis. HIPAA prohibits unauthorized access to protected health information even for testing purposes unless the access is documented and approved under your security rule policies. If your system operates in a regulated industry — healthcare, finance, education — your red team must operate within the access and testing rules that govern that industry.

Legal boundaries must be documented before testing starts. Your engagement charter should include a section that explicitly states: what legal permissions have been obtained, what regulatory constraints apply, what access is authorized, and what activities are prohibited. If the red team violates these boundaries, the organization can face fines, lawsuits, or regulatory action. The red team themselves can face personal liability if they knowingly exceed authorization.

If you are unsure whether your planned testing is legally permissible, ask Legal before you start. The cost of a one-hour legal review is negligible compared to the cost of defending against a terms-of-service lawsuit or a regulatory enforcement action.

## Ethical Boundaries Define What Harm Is Acceptable

Legal boundaries tell you what you cannot do without breaking the law. Ethical boundaries tell you what you should not do even if it is technically legal. Red teaming often requires generating content or testing behaviors that would, in any other context, be harmful. The ethical question is: under what conditions is that harm acceptable, and how do you minimize it?

The core ethical principle is proportionality. The harm you create during testing must be proportional to the harm you are trying to prevent. If you are testing a customer support chatbot for jailbreaking, generating a dozen adversarial prompts is proportional. Generating ten thousand prompts including graphic violence, hate speech, and illegal content is disproportionate unless the system's risk profile justifies it. If you are testing a medical AI for data extraction, attempting to extract synthetic patient records is proportional. Attempting to extract real patient records when synthetic data would suffice is not.

Proportionality requires judgment. It is not a formula. But it is a necessary constraint. The red team must be able to justify why the adversarial inputs they generated and the methods they used were necessary to achieve the engagement's objectives. If they cannot — if testing was more invasive, more harmful, or more extreme than the risk justified — the testing crossed an ethical boundary.

A second ethical principle is minimization. Generate the minimum amount of harmful content necessary to validate the vulnerability. If you can demonstrate a jailbreak with five prompts, you do not need fifty. If you can prove data extraction with one successful attack, you do not need to extract the entire knowledge base. Minimization reduces the risk that testing content leaks, gets misused, or causes harm to the people conducting the testing.

Red team members who spend weeks generating hate speech, violent content, or disturbing scenarios experience psychological harm. That harm is real and must be minimized. Rotate team members through different testing tasks. Limit exposure to harmful content. Provide access to mental health support if needed. The goal of red teaming is to make systems safer — not to traumatize the people doing the testing.

## Data Handling Rules Protect Information Generated During Testing

Red team engagements generate sensitive data. Adversarial prompts that successfully jailbreak a model. Extracted training data that might contain PII. Logs of model outputs that include harmful content. Proof-of-concept exploits that demonstrate vulnerabilities. All of this data must be handled with care.

Data handling rules define who can access test data, how it must be stored, how long it is retained, and when it must be deleted. These rules should be explicit and enforced. Test data should be stored in access-controlled environments, not on personal laptops. Adversarial prompts and harmful outputs should be encrypted at rest and in transit. Access should be limited to red team members, engagement stakeholders, and anyone else with a documented need to know.

Retention policies prevent test data from lingering indefinitely. Once the engagement is complete and findings are documented, raw test data should be deleted unless there is a specific reason to retain it. If remediation requires reproducing a vulnerability, retain the minimal data necessary to do so — usually just the adversarial prompt and the context needed to trigger the issue. Do not retain entire logs, full conversation histories, or complete datasets.

For engagements involving regulated data, data handling rules must comply with GDPR, HIPAA, or other applicable regulations. If your testing involves personal data, that data must be handled as personal data — with lawful basis, with access controls, with breach notification obligations. If your testing involves protected health information, it must be handled under HIPAA rules even though it is test data.

Data handling rules also address what happens if test data is accidentally leaked or accessed by unauthorized parties. Define incident response procedures. If a red team member's laptop containing adversarial prompts is lost or stolen, what is the response? If logs containing harmful outputs are accidentally committed to a public GitHub repository, what is the process for removal and notification? These are not hypothetical scenarios — they happen. Plan for them.

## Disclosure Protocols Define When and How You Report Findings

Rules of engagement must specify how findings are disclosed, to whom, and on what timeline. For internal red team engagements, disclosure is straightforward: findings go to the engagement stakeholders, documented in a final report, with severity classifications and remediation timelines. But even internal engagements can raise disclosure questions. If you discover a critical vulnerability that could be actively exploited right now, do you finish the engagement or report immediately?

The rule should be: critical vulnerabilities are disclosed immediately, regardless of engagement timeline. If the red team discovers a prompt injection path that exposes all customer data, you do not wait two weeks until the final report. You stop testing, document the vulnerability, and report it to Engineering and Security immediately so it can be patched. Immediate disclosure protocols should define who gets notified, how urgently, and what actions are expected.

For external red team engagements — where a third-party team tests your system — disclosure rules are even more important. You must define whether findings can be disclosed publicly, shared with other clients, or retained for the red team's internal research. Most engagements include confidentiality agreements: findings belong to the organization that hired the red team and cannot be disclosed without permission. But those agreements must be explicit and signed before testing starts.

If your red team discovers vulnerabilities in a third-party component — a model provider's API, a retrieval system you did not build, an open-source library — disclosure rules must address responsible disclosure. You cannot ethically discover a critical vulnerability in someone else's system and stay silent. But you also cannot disclose publicly without giving the vendor time to fix it. Define a responsible disclosure timeline — typically 90 days — and coordinate with Legal before contacting external vendors.

Disclosure protocols prevent misunderstandings. The red team knows what they can and cannot share. Stakeholders know when to expect findings. Vendors know they will be notified before public disclosure. Everyone operates under the same expectations.

## Emergency Stop Conditions Allow You to Halt Testing Safely

Not all red team engagements go according to plan. Sometimes you discover something so severe, so unexpected, or so dangerous that continuing to test would be irresponsible. Emergency stop conditions define when testing must halt immediately.

Emergency stop conditions include discovering active exploitation of a vulnerability you just found — if the red team sees evidence that an attacker is already using the technique they were testing, you stop and report. They include accidental harm to production systems — if a test unexpectedly crashes a service, triggers a cascade failure, or impacts real users, you stop. They include legal or ethical boundary violations — if the red team realizes they have exceeded their authorization or accessed data they should not have accessed, you stop.

Emergency stop conditions also include harm to red team members. If testing is causing psychological distress that exceeds acceptable levels, you stop. If a team member accidentally exposes themselves to illegal content during testing — child exploitation material, classified information — you stop and follow incident response procedures.

Define stop conditions in advance. The red team should know that if they encounter any of these scenarios, they are authorized and expected to halt testing immediately, document what happened, and escalate. Stakeholders should know that stop conditions might mean the engagement ends early or changes scope — and that is acceptable because safety comes first.

Emergency stops should be rare. If they happen frequently, your engagement planning was insufficient. But they must be available as an option. Red teaming involves controlled risk. Emergency stop conditions are the safety valve that prevents controlled risk from becoming uncontrolled harm.

## Evidence Preservation Supports Remediation and Compliance

Red team findings are only useful if they can be reproduced, validated, and fixed. That requires evidence. Rules of engagement must define what evidence the red team collects, how it is documented, and how it is preserved.

Evidence for AI vulnerabilities typically includes the adversarial input, the model's response, the context or conversation history if relevant, and any metadata needed to reproduce the issue — model version, system configuration, timestamps. For complex exploits, evidence might include multi-step attack sequences, tool call logs, or retrieved documents that enabled the attack.

Evidence must be sufficient for Engineering to reproduce the issue without requiring the red team to be present. A finding documented as "the model sometimes generates harmful content" is not actionable. A finding documented as "when prompted with the following input in the context of a conversation that includes this prior exchange, the model generates content that violates policy X" is actionable.

Evidence preservation also supports compliance and audit requirements. If your organization is subject to regulatory oversight, you may be required to document security testing activities and findings. If you face litigation or a regulatory investigation, evidence from red team engagements can demonstrate that you were proactively testing for and addressing vulnerabilities. Preserve evidence in a way that supports these uses — with timestamps, access logs, and chain of custody documentation.

But evidence preservation has limits. You do not need to preserve every adversarial prompt that did not work. You do not need to keep full logs of every test. Preserve what is necessary to support remediation, compliance, and future reference — and delete the rest according to your data retention policies.

## Post-Engagement Obligations Extend Beyond the Final Report

Rules of engagement do not end when the red team delivers their final report. Post-engagement obligations define what happens next. Who is responsible for remediation? What is the timeline for fixes? How will fixes be validated? What happens to test data after the engagement ends?

Remediation responsibility should be clear. Findings go to Engineering. Engineering prioritizes them based on severity. Critical findings are addressed within days. High-severity findings within weeks. Medium and low findings are backlogged or accepted as risks. The red team is not responsible for fixing vulnerabilities — but they may be asked to validate that fixes work, which requires availability for follow-up testing.

Test data should be deleted or archived according to retention policies. Red team access to production systems should be revoked. Any temporary accounts, test datasets, or isolated environments created for the engagement should be decommissioned. If external red teamers were involved, their access should be audited to confirm it has been fully removed.

Post-engagement obligations also include debriefs. The red team and stakeholders meet to discuss what was learned, what surprised them, what worked well, and what should change for the next engagement. Lessons from one engagement inform planning for the next. This debrief is where you surface process issues, scope problems, or communication gaps that affected the engagement's usefulness.

If findings are severe or numerous, post-engagement obligations might include a follow-up engagement to validate remediations and test for regression. This is common for high-stakes systems — you run an engagement, fix the findings, and then run a second engagement to confirm the fixes worked and did not introduce new issues.

Document post-engagement obligations in your engagement charter. They are part of the engagement plan, not an afterthought. When everyone knows what happens after the final report, the transition from testing to remediation is smooth and findings actually get fixed.

Rules of engagement are not bureaucratic overhead. They are the structure that allows red teaming to be effective, ethical, and legal. Without rules, red teams either hold back — testing less aggressively than they should because they fear crossing a boundary — or overstep — causing harm that could have been prevented with clear guidance. With rules, red teams operate with confidence. They know what they are authorized to do, what methods are acceptable, what data they can access, and what happens if they find something critical.

The best rules of engagement are simple, explicit, and agreed upon by everyone involved. Write them down. Review them with Legal. Train the red team on them. Reference them when questions arise during testing. When the engagement ends, your rules of engagement are what allows you to confidently say: we tested aggressively, we stayed within bounds, and the findings we delivered are actionable and ethically sound.

Next: assembling the team with the skills and perspectives needed to find what generic testing misses.
