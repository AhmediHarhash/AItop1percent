# 12.9 — Community and Crowdsourced Red Teaming

The team had a formal bug bounty program. They paid well, triaged fast, and maintained good relationships with security researchers. But they noticed a pattern: most bounty submissions targeted the same attack surfaces using variations of known techniques. Novel attack classes rarely appeared. When they launched a weekend hackathon with no cash prizes—just recognition, direct access to engineers, and early preview of upcoming features—they discovered 14 new vulnerability patterns their bounty program had never surfaced. The difference was not incentives. The difference was format, community, and collaborative exploration.

Bug bounties are transactional. Researcher finds vulnerability, submits report, receives payment, moves on. Community red teaming is collaborative. Researchers explore together, share techniques, build on each other's discoveries, and develop relationships with the engineering team. Both models have value. Both find different types of vulnerabilities. The most secure AI systems in 2026 use both.

## Beyond Formal Bug Bounties

Community red teaming encompasses everything from informal researcher outreach to structured competitions. The common thread is engagement beyond transactional bug reporting—relationships where researchers feel invested in the system's security, not just in collecting bounties.

Open office hours with security teams create ongoing dialogue. Once a month, the team holds a two-hour session where researchers can ask questions about the system, discuss potential vulnerabilities, or brainstorm attack ideas. No formal submissions, no payments, just conversation. These sessions surface concerns that never become bounty reports because they are too preliminary, too uncertain, or too philosophical for formal submission. Some concerns turn into genuine vulnerabilities after collaborative exploration. Others inform threat modeling even when no specific exploit exists.

Researcher advisory boards formalize ongoing relationships. A group of 10-15 trusted researchers meet quarterly with engineering, security, and product teams. They review upcoming features for security implications, discuss emerging attack techniques, and provide feedback on defensive measures. Advisory board members often have early access to systems in development, which helps catch vulnerabilities before launch. The board is not a decision-making body—it is a sounding board and early warning system.

## Community Engagement Models

Different engagement models serve different purposes. Time-boxed competitions create urgency and concentrated effort. Ongoing open challenges encourage long-term exploration. Academic partnerships bring theoretical depth. Each model finds different vulnerabilities and attracts different participants.

Capture-the-flag competitions turn red teaming into structured challenges. Teams compete to exploit specific vulnerabilities, extract specific data, or bypass specific defenses within a fixed timeframe. CTFs work well for known attack classes where you want to see how many different exploitation techniques the community can develop. They work less well for discovering entirely novel vulnerability categories because the challenges must be designed in advance.

Bug bash events focus on breadth rather than competition. Instead of racing against other teams, participants explore freely during a concentrated period—typically 24 to 72 hours—and submit whatever they find. Bug bashes work well for new features or major system updates where you want comprehensive coverage quickly. A financial AI platform ran a 48-hour bug bash before launching their autonomous trading feature and received 127 unique test cases covering prompt injection, data extraction, safety bypass, and economic manipulation scenarios. Twelve became formal bug bounty submissions after the event.

## AI Safety Research Partnerships

Academic researchers approach AI security with different tools and longer timelines than bug bounty hunters. They study attack classes theoretically before demonstrations exist in production. They develop detection techniques that work across model families. They publish findings that raise awareness industry-wide. Partnering with academic research groups extends your red team's timeline from weeks to months or years, catching risks before they materialize in the wild.

Research partnerships often involve pre-publication collaboration. Researchers share draft papers with the company before public release, giving the team time to assess impact, develop mitigations, and coordinate disclosure. In exchange, the company provides access to systems, data, or compute resources that enable the research. This is not about controlling academic findings—it is about responsible disclosure and mutual benefit.

Model access for research purposes requires careful scoping. Unrestricted access to production models creates liability risk. Access to frozen model snapshots or sandboxed environments enables research while limiting exposure. Some partnerships involve the company running experiments on behalf of researchers, which preserves control while still enabling research. The key is finding a structure where researchers can do meaningful work without creating unacceptable risk.

## Hackathons and Competitions

AI security hackathons attract different participants than bug bounty programs. Hackathons draw students, career changers, and domain experts who might not consider themselves security researchers but bring valuable perspectives. A healthcare AI company ran a security hackathon aimed at medical professionals and discovered privacy vulnerabilities that security experts had missed—attacks that required domain knowledge about how patient data gets used in clinical workflows.

Competition design determines what you discover. Offensive competitions ask teams to find as many vulnerabilities as possible within a timeframe. Defensive competitions ask teams to build protections against known attack classes and then test them against each other. Red team versus blue team formats pit attackers against defenders in real time, which surfaces both vulnerabilities and effective defensive patterns.

Prizes and recognition matter less than access and collaboration. Cash prizes attract participants, but meaningful interaction with engineering teams creates lasting value. Hackathons where researchers spend 48 hours in the same room as the people who built the system generate higher-quality findings and more actionable feedback than remote competitions with larger prize pools. The direct dialogue catches nuances that written reports miss.

## Open Challenge Programs

Some organizations publish open challenges that run indefinitely. Instead of time-boxed competitions, they pose specific security questions and invite ongoing submissions. A language model provider published a challenge in early 2025: extract any verbatim training data from our model through prompt engineering alone. The challenge remains open. Successful submissions earn $10,000 to $50,000 depending on the amount and sensitivity of extracted data. The challenge has received 1,400+ attempts over 14 months and yielded 7 validated training data extractions, each of which led to significant architectural changes.

Open challenges work well for hard problems without obvious solutions. They give researchers unlimited time to explore, experiment, and develop sophisticated techniques. They attract serious effort because the prize justifies weeks or months of work. They create public benchmarks—if no one has claimed the reward after two years, that is evidence of security even though it is not proof.

Challenge design requires clarity about success criteria. What counts as training data extraction versus general knowledge the model learned? How much extracted data qualifies for payment? What evidence is required? Ambiguous challenges generate frustration and disputes. Clear challenges with objective success criteria generate high-quality submissions and minimal triage overhead.

## Academic Collaboration

Universities and research labs study adversarial AI at scales and timeframes that industry teams rarely can. Collaborating with academic institutions extends your red teaming capability into long-term research that might not yield immediate vulnerabilities but shapes how you think about future threats.

Sponsored research programs fund specific lines of inquiry. A company might sponsor a PhD student to study prompt injection defenses or a research group to explore data extraction limits across model architectures. Sponsorship provides compute resources, model access, or direct funding. In exchange, the company gets early visibility into findings and often co-authorship or acknowledgment in publications.

Joint research projects involve company engineers working directly with academic researchers on specific problems. This goes beyond sponsorship—it is active collaboration where both parties contribute expertise. A search company partnered with a university lab to study adversarial examples in retrieval-augmented generation. The company provided production data and deployment context. The university provided theoretical frameworks and experimental design. The resulting paper improved both the company's defenses and the broader research community's understanding of RAG vulnerabilities.

## Responsible Disclosure Relationships

Community red teaming requires trust. Researchers need to believe that disclosing vulnerabilities will lead to fixes, not lawsuits or hostility. Companies need to believe that researchers will give them time to remediate before going public. Responsible disclosure policies formalize this trust.

Clear policies specify what researchers can and cannot do. Testing is allowed. Exploiting real user data is not. Reporting vulnerabilities is encouraged. Publicly disclosing unpatched vulnerabilities before coordinated disclosure is prohibited. The boundaries need to be explicit, legally sound, and visibly enforced. Policies that threaten legal action for good-faith security research chill participation. Policies that explicitly protect researchers within defined boundaries build community trust.

Coordinated disclosure timelines balance company remediation needs against researcher publication interests. Typical timelines range from 60 to 90 days after initial report. The company gets time to validate, fix, and deploy. The researcher gets credit through eventual publication or acknowledgment. Extensions are common for complex vulnerabilities that require architectural changes. The key is agreeing upfront and communicating clearly when timelines slip.

Public acknowledgment programs recognize researchers who responsibly disclose vulnerabilities. Hall of fame pages, annual awards, conference presentations, or co-authored blog posts about resolved vulnerabilities all serve to recognize contributions publicly. Recognition matters to many researchers as much as or more than financial rewards. It builds reputation, demonstrates expertise, and creates community status.

## Building Community Trust

Community red teaming thrives on trust. One hostile response to a vulnerability report can poison relationships with dozens of researchers. One lawsuit against a good-faith researcher chills participation across the entire community. Building trust requires consistency over years.

Responsiveness to reports signals respect for researcher time. Fast initial acknowledgment—even if validation takes weeks—shows that submissions are valued. Clear communication about status, timelines, and decisions demonstrates transparency. Payments or recognition delivered promptly and without haggling over severity build goodwill. Researchers talk to each other. Reputation spreads quickly.

Public handling of disputes shapes community perception. Disagreements about severity, scope, or duplicates are inevitable. How you resolve them matters enormously. Teams that explain reasoning, offer appeals processes, and occasionally concede points even when they could defensibly refuse earn long-term community respect. Teams that fight every decision, minimize every finding, or ghost researchers who push back lose community trust permanently.

Transparency about what you are doing with findings helps researchers see their impact. Regular updates about vulnerabilities fixed, features changed, or architectures redesigned based on researcher input demonstrate that the work matters. Some companies publish quarterly security updates summarizing bounty program statistics, major fixes, and researcher acknowledgments. The transparency builds trust and attracts higher-quality participation.

Community and crowdsourced red teaming extends your security capability beyond what any internal team can achieve. It brings diverse perspectives, novel techniques, and ongoing engagement. The cost is not primarily financial—it is the organizational investment in building relationships, managing community interactions, and integrating external findings into your security operations. Teams that make that investment find vulnerabilities earlier, build better defenses, and create lasting relationships with the security research community. Teams that treat community red teaming as a cost center to be minimized miss most of the value.

Next, we examine threat intelligence for AI systems—understanding the broader threat landscape to inform where and how to focus red teaming efforts.
