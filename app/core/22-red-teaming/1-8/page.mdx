# 1.8 — Who Does Red Teaming: Internal Teams, External Firms, Bug Bounties

Your internal team knows the system better than anyone. They built it, they understand the architecture, they know where the corners were cut and which components are fragile. They are also the worst possible choice for adversarial testing if they are the only choice. The same familiarity that makes them effective at building the system makes them blind to its vulnerabilities. They designed the prompt with specific constraints in mind. They will test those constraints, not the ones they never considered. They built the tool integration with certain use cases in mind. They will attack those use cases, not the edge cases an outsider would try first. Internal red teaming is necessary. It is not sufficient.

The decision is not internal versus external. It is how to combine them, when to emphasize each, and what capabilities to build in-house versus buy. The answer depends on your risk profile, your budget, and how seriously you take the adversarial threat. A low-risk internal tool can survive on internal red teaming alone. A high-risk regulated system facing sophisticated adversaries cannot.

## Internal Red Teams: Advantages, Disadvantages, and Blind Spots

Internal red teams move fast. They already have access to the codebase, the documentation, the staging environment, the roadmap. They do not need onboarding. They do not need NDAs. They can start testing the same day a feature lands in staging. When they find a vulnerability, they file it directly in the team's issue tracker and talk to the engineers who own the affected component. The feedback loop is hours, not weeks.

Internal red teams have context. They know the business priorities, the compliance requirements, the customer use cases that matter most. They can prioritize findings based on real impact, not theoretical severity. They know which vulnerabilities would cause regulatory issues, which would violate customer contracts, which would make headlines if exploited. External red teamers guess. Internal red teamers know.

The disadvantage is narrower perspective. Internal red teamers think like the team that built the system because they are part of that team or work closely with it. They are trained on the same threat models, the same security frameworks, the same vulnerability classes that Engineering already worries about. They catch the attacks that fit the mental model the organization already has. They miss the attacks that require a completely different way of thinking — the creative, bizarre, combinatorial attacks that only come from someone who has never seen your system before.

A developer tools company in early 2026 ran six months of internal red teaming on their AI-powered code review assistant. The internal security team tested jailbreaks, prompt injections, tool misuse, data leakage. They found and fixed forty-three issues. Leadership felt confident. Then they brought in an external red team for a week. The external team found a vulnerability the internal team never considered: if you embedded specific Unicode characters in code comments, the model would interpret them as hidden instructions and generate code that included those instructions as executable logic. The attack required knowledge of how tokenization and instruction-following interact at a level of nuance the internal team — focused on higher-level security patterns — simply never explored. The finding was critical. It would have survived internal testing indefinitely.

Blind spots are structural, not personal. The internal team is not less skilled. They are too close. Their job is to make the system work. The adversarial mindset requires assuming the system is already broken and finding how. Those are contradictory cognitive modes. Asking the same people to do both, simultaneously, is asking them to context-switch between builder and breaker faster than human psychology allows.

The solution is rotation and separation. Some organizations rotate engineers through red team duty — you spend a quarter on a different team's system, bring fresh eyes, then rotate back. Some organizations create dedicated internal red teams with no product delivery responsibilities, only adversarial testing. The best setups combine both: a core dedicated red team that runs formal exercises, plus rotating engineers who bring domain expertise to weekly adversarial testing. But even with perfect internal structure, you still need external perspectives to catch what the entire organization is blind to.

## External Red Team Firms: When to Hire, What to Look For, Typical Costs

External red team firms bring one thing internal teams cannot: genuine outsider perspective. They have never seen your system. They do not know what you think is important. They do not care about your roadmap or your business priorities. They are paid to break your system and document everything that breaks. They will try attacks you never imagined because they come from different domains, different security backgrounds, different ways of thinking about AI.

You hire external red teamers in three situations. First, pre-launch for high-risk systems where regulatory compliance or enterprise customer requirements demand third-party validation. Second, post-incident when you need an independent assessment of what else might be broken. Third, annually or semi-annually as a forcing function to catch the blind spots your internal team cannot see.

What to look for in an external firm: AI-specific expertise, not just general penetration testing. A firm that excels at breaking web applications might be mediocre at adversarial prompt testing. You want red teamers who understand language models, know current jailbreak techniques, can reverse-engineer prompt architectures from model behavior, and have experience with AI-specific vulnerability classes like indirect prompt injection, training data extraction, and tool misuse chains. Ask for case studies. Ask which models they have tested. Ask about their methodology. If they describe a generic security testing process with "AI" tacked on, keep looking.

Look for documentation quality. The deliverable is not just finding vulnerabilities — it is a report that Engineering can use to fix them and Leadership can show to regulators or customers. You want clear reproduction steps, severity ratings that match your risk framework, remediation recommendations that are technically sound, and executive summaries that non-technical stakeholders can understand. A great red team engagement produces a report you can hand to your board, your auditors, and your enterprise customers without embarrassment.

Typical costs vary widely. A one-week engagement with a mid-tier firm for a straightforward AI assistant might run fifteen thousand to thirty thousand dollars. A two-to-four-week engagement with a top-tier firm for a complex multi-agent system in a regulated industry can cost eighty thousand to one hundred fifty thousand dollars. Enterprise-scale assessments — testing multiple products, multiple deployment environments, including compliance mapping — can exceed three hundred thousand dollars. Bug bounty programs, discussed below, offer a different cost model entirely.

The return on investment is asymmetric. A thirty-thousand-dollar red team that finds one critical vulnerability before launch saves you from a potential million-dollar incident, regulatory fine, or enterprise customer loss. A hundred-thousand-dollar red team that finds nothing is still valuable — it gives you documented evidence that a qualified third party tried to break your system and could not. That evidence matters when customers ask for security validation or regulators ask what testing you performed.

External red teams are not a substitute for internal capability. They are a complement. You need internal red teaming for speed, continuity, and context. You need external red teaming for blind spot coverage, third-party validation, and credibility. The best programs use both.

## Bug Bounty Programs for AI Systems: Structure, Rewards, Responsible Disclosure

Bug bounty programs flip the economics of red teaming. Instead of paying a fixed fee for a time-boxed engagement, you pay per valid finding. Instead of hiring a specific firm, you open the system to any researcher who wants to try breaking it. Instead of limiting testing to a few weeks, you create continuous adversarial coverage for as long as the program runs.

The structure is simple. You publish a scope: which systems are in-scope, which attacks are allowed, which are off-limits. You define a reward scale: critical findings pay ten thousand to fifty thousand dollars, high-severity pay three thousand to fifteen thousand, medium pay five hundred to three thousand, low pay acknowledgment and maybe a few hundred dollars. You set rules of engagement: test in designated environments, do not exfiltrate real user data, do not perform denial-of-service attacks, report findings through the official channel, give the company time to fix before public disclosure.

Researchers find vulnerabilities, submit reports, and get paid if the report is valid, in-scope, and not a duplicate. You fix the issue, pay the bounty, and optionally acknowledge the researcher publicly. Over time, the program builds a community of researchers who are familiar with your system and motivated to keep testing it.

OpenAI, Anthropic, Google, and Meta all run public bug bounty programs for their AI systems as of 2026. OpenAI's program, launched in 2023, has paid hundreds of thousands of dollars across thousands of submissions. The findings range from jailbreaks and refusal bypasses to training data extraction and safety filter evasion. Anthropic's program, launched in mid-2024, focuses on constitutional AI violations and harmful content generation bypasses. Google's AI bounty covers Gemini and Bard-era systems with rewards scaling based on exploitability and impact. These programs have found vulnerabilities that internal teams and contracted red teams missed.

Bug bounties work best for mature systems with stable scope. If your product is changing weekly, the scope documentation becomes stale and researchers waste time testing features that no longer exist. If your system is pre-launch and not public, you cannot run a public bounty — you would need a private invite-only program, which reduces researcher participation. If your team lacks the capacity to triage and respond to reports quickly, the program will frustrate researchers and damage your reputation in the security community.

The reward structure must be competitive. If your top bounty is five hundred dollars and a competitor pays twenty thousand for the same class of vulnerability, researchers will test the competitor, not you. If your response time is weeks when other programs respond in days, researchers will deprioritize your program. Bug bounties are a marketplace. You are competing for researcher attention.

Responsible disclosure is the contract that makes bug bounties work. Researchers agree to report findings privately and give you time to fix them before publishing. In exchange, you agree to take reports seriously, respond promptly, pay fairly, and not retaliate legally. The standard disclosure timeline is ninety days — you have ninety days to fix and deploy a patch before the researcher can publish. Some programs negotiate extensions for complex issues. Some researchers publish sooner if the company is unresponsive. The relationship is cooperative but not unconditional. Researchers are not your employees. They are independent security contributors who expect professionalism and fair dealing.

Bug bounties do not replace red teaming. They complement it. Red teams provide comprehensive, structured assessments of your entire system. Bug bounties provide continuous, creative, decentralized testing from a global researcher community. You need both.

## Hybrid Models: Combining Internal and External Testers

The most effective adversarial testing programs use all three models in combination. Internal red teams provide fast, continuous, context-rich testing throughout development. External red team firms provide structured, comprehensive, third-party assessments before major launches and on a regular cadence. Bug bounty programs provide ongoing coverage from a distributed researcher base motivated to find what everyone else missed.

A hybrid model for a high-risk AI system might look like this. Internal red team conducts weekly adversarial testing sprints, files findings directly into Engineering's backlog, and runs lightweight tests on every major commit. Quarterly, an external red team firm conducts a two-week formal assessment, delivers a written report, and validates that previous findings were properly fixed. Year-round, a public bug bounty program offers rewards for any researcher who finds vulnerabilities the internal and external teams missed.

The cost of this setup scales with risk. For a Risk Tier 1 system in healthcare or finance, you might spend two hundred thousand to four hundred thousand dollars annually across internal team salaries, external engagements, and bounty payouts. For a Risk Tier 2 system, you might spend fifty thousand to one hundred thousand. For a Risk Tier 3 internal tool, you might rely entirely on internal testing plus a small annual external engagement.

The coordination required is not trivial. Internal findings go into the issue tracker. External red team findings go into a formal report. Bug bounty findings go into a triage queue. You need someone — usually a security lead or product security engineer — who is responsible for tracking all three streams, deduplicating findings, prioritizing fixes, and ensuring nothing falls through the gaps. Without centralized tracking, you end up with the same vulnerability reported three times through three different channels and fixed zero times because everyone thought someone else was handling it.

The payoff is adversarial coverage that no single model can provide. Internal teams catch the common issues fast. External teams catch the architectural and nuanced vulnerabilities internal teams miss. Bug bounties catch the long-tail creative attacks that only emerge from global crowdsourced testing. Together, they make your system significantly harder to exploit than adversarial testing from any single source.

## Building Internal Red Team Capability vs. Outsourcing

The question is not whether to build internal red team capability. It is how much to build and when. Every organization with an AI product needs at least basic internal adversarial testing capability — engineers who know how to jailbreak, inject, and misuse prompts, who run adversarial tests during development, and who think like attackers as part of their daily workflow. This is table stakes. The question is whether you invest further: a dedicated red team with no product responsibilities, formal training, and a reporting line outside Engineering.

Small teams with limited budgets should prioritize external engagements over building a dedicated internal red team. Hire an external firm quarterly, invest in training your engineers to think adversarially during development, and run a private bug bounty with a modest budget. As the organization scales and the system becomes higher-risk, transition toward a hybrid model with dedicated internal red team capacity.

The break-even point for a dedicated internal red team is roughly when you are spending more than one hundred fifty thousand dollars per year on external red team engagements and wish you had faster turnaround and better continuity. At that spending level, hiring one or two full-time red team engineers becomes cost-competitive and gives you in-house adversarial capability that can test continuously instead of episodically.

The skills needed are a mix of security, machine learning, and creativity. The best AI red teamers have security engineering backgrounds — they understand attack surfaces, exploit chains, and how to escalate privileges. They also have enough ML knowledge to understand how models work, what tokenization does, and why certain prompts bypass filters that should catch them. And they have the creativity to generate novel attacks, not just run known exploits. This skill set is rare. Hiring for it is hard. Training for it is slow. Which is why many organizations rely heavily on external expertise until they reach the scale and risk profile that justifies building it internally.

## The Skills Needed: Security, ML, Prompt Engineering, Domain Expertise

AI red teaming requires depth in multiple domains. Security fundamentals are the foundation — understanding threat modeling, attack trees, privilege escalation, injection vulnerabilities, and how to chain small issues into big exploits. Without this, red teamers might find surface-level issues but miss the architectural vulnerabilities that matter most.

Machine learning knowledge is the second layer. You do not need to be able to train models from scratch, but you need to understand how language models process input, how they generalize from training data, how instruction-following works, and how context windows and attention mechanisms shape behavior. You need to know enough to hypothesize why a specific adversarial input might work and iterate when it does not.

Prompt engineering is the tactical skill. Red teamers must be fluent in jailbreak techniques, indirect prompt injection, role-play exploits, context-stuffing, delimiter attacks, and the full toolkit of adversarial prompting. They need to know the current state of the art — what worked in 2024 might be patched in 2026 models — and how to adapt techniques across different model families. They need creativity to invent new attacks when known techniques fail.

Domain expertise is the wildcard. If you are red teaming a healthcare AI, having clinical knowledge helps you generate medically plausible adversarial inputs that pure security people would never think of. If you are red teaming a legal AI, understanding contract law helps you craft adversarial clauses that mislead the model in realistic ways. If you are red teaming a financial AI, knowing accounting fraud patterns helps you test whether the system can be tricked into enabling them. Domain expertise is not strictly required — many great red teamers are generalists — but it makes the difference between testing the system as an abstract ML product and testing it as a real deployment in a specific risk context.

No one person is expert in all four areas. Red teams are stronger when composed of people with different backgrounds who bring different attack perspectives. A team of three — one security engineer, one ML researcher, one domain expert who picks up prompting quickly — will outperform a team of three identical generalists.

## Team Composition for Different Risk Profiles

For a low-risk system, red teaming can be part-time rotational duty for engineers. One person per sprint spends a few hours adversarially testing recent changes. No dedicated headcount, no specialized hiring, just a cultural norm that everyone takes a turn thinking like an attacker.

For a moderate-risk system, you want at least one dedicated security engineer who owns adversarial testing and coordinates external engagements. They do not spend all their time red teaming — they also handle threat modeling, security reviews, compliance documentation — but adversarial testing is an explicit part of their job, not something that happens only when someone remembers.

For a high-risk system in a regulated industry, you need a dedicated red team with at least two to four people, depending on system complexity. Ideally this team reports to a security lead who is not in the product delivery chain, so they can block releases without political pressure. They spend most of their time adversarially testing, with a smaller portion on tool development, training other engineers, and coordinating external engagements.

For organizations running multiple high-risk AI systems, you might build a centralized red team that tests all of them on a rotating basis, plus embed one security-focused engineer in each product team to handle continuous adversarial testing. This model scales well and prevents duplication of red team expertise across product lines.

The composition flexes, but the principle holds: someone must own adversarial testing as an explicit, resourced, ongoing responsibility. If no one owns it, it becomes everyone's forgotten second priority and happens inconsistently if at all. The adversarial mindset is not default human cognition. It requires intention, time, and accountability. Give it all three.

The infrastructure that supports adversarial testing — the charter that defines it, the rules that bound it, and the communication paths that channel findings into action — determines whether red teaming produces real security or just expensive reports that gather dust.

