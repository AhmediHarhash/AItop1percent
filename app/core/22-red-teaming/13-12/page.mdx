# 13.12 — Red Teaming for High-Risk AI Systems

The EU AI Act's high-risk classification list changed everything. In October 2025, a healthcare AI company discovered this the hard way. Their diagnostic support system — which had passed their standard red team assessment — fell squarely into Annex III, Category 5(b): medical devices. Under the Act, their existing testing process was legally insufficient. The regulator gave them 120 days to implement enhanced red teaming procedures, document everything to a new standard, and prove independent verification. The cost to retrofit their compliance program: 1.2 million euros. The cost if they had designed for high-risk from the beginning: approximately one-third of that.

High-risk AI systems do not get standard testing. They get proportionally intensive testing that matches the potential harm. The regulatory classification determines your obligations. The harm potential determines your urgency. Both demand testing programs that most organizations have never built before.

## What Makes a System High-Risk

High-risk is not about technical complexity. It is about consequence. A highly sophisticated model that recommends movies is not high-risk. A simple logistic regression model that screens job candidates is.

The EU AI Act defines high-risk systems across eight categories: biometrics, critical infrastructure, education and training, employment, essential services, law enforcement, migration and border control, and administration of justice. Each category contains specific use cases. If your system fits any of them, you are high-risk regardless of how simple or transparent your model is.

Beyond regulatory classification, consider functional risk. A system that controls physical devices, makes decisions about human safety, influences legal outcomes, or handles sensitive populations carries high-risk characteristics even if not legally classified as such. Your internal risk assessment should be more conservative than the regulatory baseline, not less.

Financial services have learned this through painful experience. Credit underwriting models fall under essential services access. Employment screening falls under worker management. Even customer service chatbots can become high-risk if they make binding commitments or handle regulated disclosures. The classification follows the function, not the technology.

## Proportional Testing Requirements

High-risk systems require testing intensity that matches their potential harm. The EU AI Act mandates conformity assessment procedures that include red teaming as a key component. This is not a one-time checkbox exercise. This is continuous adversarial evaluation throughout the system lifecycle.

Your testing must cover all reasonably foreseeable misuse scenarios. For employment screening systems, this includes bias testing across protected characteristics, manipulation resistance testing, and adversarial input testing. For medical systems, this includes safety-critical failure modes, edge case handling, and interaction effects with other medical processes. For biometric systems, this includes presentation attacks, deepfakes, demographic variation, and environmental robustness.

Documentation requirements scale with risk. Every test must be logged with methodology, results, remediation actions, and verification of fixes. Every attack vector must be cataloged with severity, likelihood, and mitigation strategy. Every finding must be tracked through closure with evidence of resolution. The documentation burden for high-risk systems is approximately ten times heavier than standard systems.

Independent assessment is often mandatory. The EU AI Act requires notified body involvement for many high-risk categories. Even where not legally required, industry practice increasingly demands third-party verification. Your internal red team findings need external validation. An independent team with no organizational incentive to pass you brings credibility that internal testing cannot match.

## Enhanced Documentation Standards

High-risk systems demand documentation that survives regulatory scrutiny. This means structured evidence, traceable decisions, and auditable processes. Your documentation must answer every question a regulator or auditor might ask without requiring you in the room to explain it.

The testing plan must specify scope, methodology, acceptance criteria, and escalation procedures before testing begins. Document why you chose certain attack vectors and why you deprioritized others. Document your risk assessment methodology. Document how you determined testing sufficiency. Regulators want to see your reasoning, not just your results.

Test execution logs must capture who tested what, when, with what tools, and what they found. Every finding needs a severity rating based on a documented rubric, not subjective judgment. Every finding needs a remediation decision: fix, mitigate, accept, or transfer. Every accepted risk needs executive sign-off with written justification. The paper trail must be complete.

Remediation tracking must show closure. Document the fix, the verification testing, the re-test results, and the final disposition. For high-severity findings, document the timeline from discovery to closure. If remediation took longer than your SLA, document why and what you changed to prevent recurrence. The regulator assumes you are hiding something unless your documentation proves otherwise.

## Continuous Monitoring Obligations

High-risk classification means continuous monitoring, not just pre-deployment testing. The EU AI Act requires ongoing monitoring of system performance and re-assessment when circumstances change. A system that passes testing in February may accumulate new vulnerabilities by August through model drift, data shift, or evolving attack techniques.

Establish monitoring cadence based on risk and change velocity. A stable medical diagnostic system might require quarterly red team sweeps. A dynamic employment screening system that retrains weekly might require monthly testing. Systems with direct regulatory reporting obligations often need continuous monitoring with automated alerting.

Triggering events demand immediate re-testing. If you discover a new attack vector in production, test all similar systems immediately. If a researcher publishes a new jailbreak technique, assess your exposure within days. If you deploy a model update, red team it before release and validate in production within the first week. Waiting for the next scheduled test cycle is not acceptable for high-risk systems.

Post-incident testing must be exhaustive. If an attack succeeds in production, your next red team cycle must cover that attack class comprehensively. If you discover a bias issue, test all related fairness dimensions. The incident reveals a gap in your testing coverage. Closing that gap is not optional.

## Remediation SLAs and Escalation

High-risk systems need time-bound remediation commitments. A critical finding in a medical AI system cannot sit in your backlog for six months while you work on features. The finding needs a fix timeline, executive oversight, and escalation procedures if deadlines slip.

Define severity-based SLAs before you find anything. Critical findings — those that could cause immediate harm — typically require remediation within 72 hours to two weeks depending on deployment status. High findings require 30 to 60 days. Medium findings require 90 days. Low findings require 180 days or inclusion in the next major release. Document these commitments and track compliance.

Escalation paths must be clear and fast. If a critical finding cannot be remediated within the SLA, it escalates to the product owner within 24 hours. If still unresolved, it escalates to the VP of Engineering within 48 hours. If still unresolved, it escalates to the Chief Risk Officer or equivalent within 72 hours. At that level, you are deciding whether to pull the system from production or accept documented risk with executive accountability.

Temporary mitigations count as partial remediation only. If you cannot fix the root cause immediately, you can often reduce severity through monitoring, rate limiting, additional review layers, or restricted deployment. These mitigations buy time but do not close the finding. The root cause fix still needs a timeline and tracking.

## Building High-Risk Testing Programs

High-risk systems require dedicated program infrastructure. You cannot run high-risk testing as a side project staffed by whoever has spare cycles. You need specialized skills, dedicated time, executive sponsorship, and budget that reflects the compliance stakes.

Start with skills assessment. High-risk testing requires people who understand both offensive security and domain-specific risk. For medical AI, you need team members who understand clinical workflows, medical error modes, and healthcare regulations. For employment AI, you need people who understand labor law, discrimination patterns, and HR processes. Technical red teaming skills are necessary but not sufficient.

Tool investment scales with risk. High-risk programs need automated testing infrastructure, attack simulation platforms, comprehensive logging, and reporting systems that meet audit standards. Budget for commercial tools, custom development, and ongoing maintenance. The EU AI Act's documentation requirements make manual processes unsustainable at scale.

Governance integration is mandatory. High-risk red teaming must feed into your risk management framework, compliance reporting, and product approval gates. Testing results must be visible to the board, not buried in engineering tickets. Audit committees need regular red team summaries. Product launches need red team sign-off as a formal gate.

External partnerships extend your capability. No internal team can cover every attack vector or maintain expertise in every domain. Establish relationships with independent red team providers, academic researchers, and domain experts who can provide periodic deep assessments. Budget for external testing as a recurring expense, not a one-time project.

The program must evolve with the threat landscape. What constitutes adequate testing in February 2026 will be insufficient in February 2027. New attack techniques emerge. Regulatory expectations tighten. Adversaries get smarter. Your program needs continuous learning, regular capability upgrades, and leadership commitment to staying ahead of minimum compliance.

High-risk AI red teaming is not a technical add-on. It is a fundamental operating requirement. Systems that can cause significant harm require testing programs that are proportionally sophisticated, well-resourced, and ruthlessly thorough. The investment is substantial. The alternative — deploying inadequately tested high-risk systems — is professional negligence with regulatory, financial, and ethical consequences that no organization should accept.

You either test with the intensity the risk demands, or you do not deploy. There is no middle ground for high-risk AI systems.

Next: Insurance and Liability Implications — how insurers evaluate AI security and what red teaming evidence they demand.
