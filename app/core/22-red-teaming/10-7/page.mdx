# 10.7 — Documentation and Evidence Collection

In October 2025, a red team found a critical vulnerability in an enterprise AI assistant. A carefully crafted multi-turn prompt injection could extract conversation history from other users. The red teamer demonstrated it live to the security team. Everyone in the room saw it work. The security team acknowledged it was serious and said they would fix it. The red teamer moved on to the next scenario. Three weeks later, when the red team delivered their final report, the security team said "we tried to reproduce the conversation history leak and could not. Can you provide the exact prompts you used?" The red teamer checked their notes. They had written "multi-turn injection via context manipulation — successful." No prompt text. No screenshots. No step-by-step procedure. Just a one-line note that the attack worked. The vulnerability was real. The security team was willing to fix it. But without reproduction steps, they could not. The finding was useless.

Undocumented findings are useless findings. Evidence collection must be rigorous enough to enable reproduction, support severity classification, and withstand scrutiny from defenders who will question every claim.

## Why Documentation Matters More for AI

Traditional software vulnerabilities are deterministic. If you find a SQL injection vulnerability, you can write the payload once and it works every time. AI vulnerabilities are often probabilistic. A prompt injection might work 80% of the time, fail 15% of the time, and produce unexpected behavior 5% of the time. If you report a vulnerability without documenting the exact conditions under which it succeeds, the defender will test it, see it fail, and close the finding as unreproducible.

AI systems also have high configurability. The same model with different system prompts, different temperature settings, or different retrieval configurations behaves differently. If you find a vulnerability in the test environment and the defender cannot reproduce it in their environment, the problem is usually environmental differences — but without documentation of your environment, no one can diagnose the discrepancy.

AI vulnerabilities are also context-dependent. A jailbreak that works when the model is in assistant mode might fail when the model is in tool-use mode. A data extraction attack that works in English might fail in French. A retrieval poisoning attack that works with one embedding model might fail with another. The red teamer knows the context because they were there during the test. The defender does not. Documentation must capture context exhaustively.

The conversation history leak was a perfect example. The vulnerability required a three-turn dialogue with specific phrasing in turn two. The red teamer knew this because they had tried 15 variations before finding one that worked. But they did not document the failed variations, the successful variation, or the order of operations. When the defender tested with slightly different phrasing or in a different order, it failed — and they concluded the finding was invalid. The vulnerability was real. The documentation was inadequate. The outcome was the same as if the vulnerability did not exist.

## What to Capture

At minimum, document the attack objective, the exact steps to reproduce, the expected outcome, the actual outcome, the environment configuration, and the timestamp. Exact steps means you can hand the documentation to someone who has never seen the system and they can reproduce the attack. If your documentation says "use prompt injection to leak data" that is not exact. If it says "send the following three prompts in sequence, wait 10 seconds between each, then observe the response to the fourth prompt — here are the exact text inputs" that is exact.

Environment configuration includes model version, API endpoint, authentication method, system prompt if known, temperature and top-p settings if configurable, retrieval settings, tool availability, and any other parameters that affect model behavior. Many red teamers skip this because they assume the defender knows the environment. The defender often does not — especially if the red team is external or if the system has multiple deployment configurations.

Timestamp matters more than you think. If you found a vulnerability at 2:47 PM and the defender tries to reproduce it at 9:00 AM the next day, a code deploy might have occurred in between. If you document the timestamp, the defender can check whether any changes happened between your test and their reproduction attempt. This prevents false negatives where the defender cannot reproduce because the system changed.

Actual outcome should include the full response from the system — not a summary, the verbatim text. If the model generated 200 words and only the last sentence was problematic, include all 200 words. The defender might notice something in the first 199 words that explains why the last sentence occurred. Screenshots are acceptable for UI-based testing but not sufficient on their own — include the text as well because screenshots are not searchable and not copy-pasteable.

Also document what you tried that did not work. If you attempted 12 variations of a prompt and only one succeeded, document all 12. This shows the defender that the vulnerability is not trivially exploitable — it required iteration. It also shows which variations the current mitigations successfully block, which helps the defender understand what their defenses already catch.

## Evidence Preservation

Evidence degrades over time. Model responses disappear when you close the browser tab. API logs rotate out after 30 days. Session state resets. If you discover a vulnerability and do not preserve evidence immediately, you might lose the ability to prove it existed. Evidence preservation means capturing everything at the moment of discovery and storing it in a durable location.

For prompt-based testing, save the conversation transcript as a text file immediately. If the interface does not provide an export function, copy-paste the entire conversation into a text editor and save it with a descriptive filename that includes the date and the vulnerability type. For API-based testing, save the full request and response including headers. Use curl or a similar tool to capture the exact HTTP request so you can replay it later.

For testing that involves UI interactions, record a screencast. Screencasts capture interactions that are difficult to describe in text — mouse movements, timing, multi-step workflows. A screencast also proves that the vulnerability is real and not a fabrication. If a defender questions a finding, you can send them the video. A 90-second screencast is often more convincing than a five-page written description.

For vulnerabilities that depend on specific data states, export the relevant data. If you poisoned a retrieval system by injecting a malicious document, save a copy of the document. If you exploited a vulnerability that depends on specific conversation history, export the history. The defender will need this data to reproduce the issue.

Also preserve negative evidence — evidence that a mitigation failed. If the system has a content filter that is supposed to block certain outputs and you bypassed it, save the filter's configuration and the output that it failed to block. This shows exactly why the mitigation is inadequate.

A biotechnology AI company requires red teamers to preserve evidence in a structured evidence repository. Each finding gets a unique identifier. All evidence for that finding — transcripts, screenshots, screencasts, data exports — gets uploaded to a folder named with the finding ID. The repository is write-once, meaning evidence cannot be modified after upload. This creates a tamper-proof chain of custody. When the red team delivers the report three weeks later, the defenders can access the evidence repository and review every piece of evidence in its original form.

## Reproducibility Requirements

A finding is only valid if someone else can reproduce it. Reproducibility requires step-by-step instructions detailed enough that someone with no prior knowledge of the attack can execute it successfully. Write reproduction steps as if you are writing them for someone who is competent but unfamiliar with both the system and the attack technique.

Reproducibility testing is simple: give your documentation to a colleague who was not involved in the test and ask them to reproduce the finding. If they succeed without asking clarifying questions, your documentation is adequate. If they get stuck or produce different results, your documentation has gaps.

Common reproducibility failures: missing the initial state, skipping implicit steps, assuming knowledge the reader does not have, and under-specifying timing. Initial state means: what account did you use, what permissions did it have, what data was in the system, what settings were configured? If you tested with an admin account and the defender tests with a regular user account, the results might differ. If you tested in a system with 10,000 documents in the knowledge base and the defender tests with an empty knowledge base, retrieval attacks will behave differently.

Implicit steps are actions you performed but did not think to document because they seemed obvious. "Log in" is an implicit step that seems obvious — but if the system has multiple login methods and the vulnerability only works with one, you need to specify which. "Submit the prompt" might seem obvious — but if the system has a chat interface and a batch API and the vulnerability only works in one, specify which.

Assumed knowledge includes technical terms, system-specific jargon, and attack techniques. If your reproduction steps say "perform a multi-turn context injection" you are assuming the reader knows what that means and how to do it. Better: "Send the following three prompts in separate messages within the same conversation session. Wait for the model's response after each prompt before sending the next."

Timing matters when attacks depend on session state, when systems have rate limits, or when responses are non-deterministic. If the attack requires waiting 10 seconds between steps for session state to update, document the wait time. If the attack fails if you submit too quickly due to rate limiting, document that. If the attack works 70% of the time and requires multiple attempts, document the success rate so the defender knows to retry if the first attempt fails.

## Screenshot and Recording Practices

Screenshots are valuable but easy to misuse. A good screenshot shows the entire relevant context — not just the problematic output but also the input that caused it, the timestamp, the session identifier, and any UI elements that indicate system state. A bad screenshot crops to only the problematic text, loses context, and raises questions about whether it was manipulated.

When capturing screenshots of AI-generated content, include the prompt or query that triggered the response in the same screenshot. If the interface does not allow this, take two sequential screenshots: one showing the input, one showing the output. Number the screenshots and include the numbers in your written documentation so the reader knows the order.

For multi-step exploits, take a screenshot at every step. If the exploit has five steps, you should have at least five screenshots showing the system state after each step. This creates a visual timeline that helps the defender understand the progression.

Screencasts are better than screenshots for complex interactions. A 90-second screencast showing you performing the attack in real time is harder to dispute than a series of screenshots. Screencasts also capture behavior that screenshots cannot — loading delays, UI state changes, error messages that appear briefly and then disappear. Use screen recording software that captures both the screen and the audio narration. Narrate what you are doing as you do it: "Now I am submitting the second prompt in the sequence. Notice the response includes conversation history from a different user." The narration creates a spoken record that complements the visual evidence.

Keep recordings focused. A 90-second screencast is easy to review. A 45-minute screencast is not. If the exploit requires 45 minutes of setup, edit the recording down to the essential steps or provide timestamps indicating which segments are critical.

## Log Collection

Logs provide evidence that screenshots and recordings cannot. Logs show server-side behavior, API calls, database queries, model inference parameters, and error states. If you have access to logs during testing, collect them. If you do not have access, request them from the defender after you report the finding.

Collect logs at the moment the vulnerability is exploited. If you exploit a vulnerability at 3:42 PM, request logs from 3:40 PM to 3:45 PM. Narrow time windows make log analysis tractable. Logs from an entire day are too noisy to be useful.

Request structured logs if possible. JSON logs are easier to parse than plain text logs. If logs include request IDs or session IDs, note the ID associated with your exploit so the defender can filter logs to only the relevant requests.

Also collect client-side logs if available. Browser developer tools capture network traffic, console errors, and storage state. If the vulnerability involves client-side behavior — like a UI that displays sensitive data it should not — browser logs might reveal why. Export the network log showing the API requests and responses that occurred during the exploit. This proves what data the server sent and what the client displayed.

A legal AI company integrates log collection into their red team workflow. Red teamers have read-only access to a logging dashboard. When they find a vulnerability, they immediately filter logs to the relevant time window, export the logs, and attach them to the finding documentation. This eliminates the delay that occurs when red teamers have to request logs from defenders after the fact. It also ensures logs are captured before they rotate out.

## Timeline Reconstruction

Complex exploits often involve multiple steps across multiple sessions or multiple users. Timeline reconstruction is the process of documenting the exact sequence of events, with timestamps, that led to the exploit succeeding. Timelines are essential for demonstrating chained vulnerabilities where no single step is problematic but the combination is.

A good timeline includes: timestamp, actor, action, and result. For example:

2:30 PM — Attacker (User A) — Injected malicious document into knowledge base via upload API — Document ID 47382 created.

2:35 PM — Attacker (User A) — Submitted query designed to retrieve the malicious document — System retrieved document ID 47382 and included it in context.

2:36 PM — Victim (User B) — Submitted unrelated query — System retrieved document ID 47382 due to overly broad retrieval, included malicious content in response to User B.

This timeline shows a retrieval poisoning attack that crosses user boundaries. No single step is obviously malicious. The upload is legitimate. The attacker's query is legitimate. The victim's query is legitimate. But the sequence demonstrates a vulnerability. Without the timeline, the finding would be hard to explain.

Timelines also help defenders understand the exploitability window. If the attacker had to wait six hours between step two and step three for the malicious document to propagate through the system, that is useful information for severity classification. A vulnerability that requires a six-hour wait is less severe than one that executes immediately.

## Chain of Custody for Evidence

Chain of custody is the documentation that proves evidence has not been tampered with between collection and presentation. For high-stakes findings — especially those that might inform legal or regulatory decisions — chain of custody matters. It prevents accusations that the red team fabricated or manipulated evidence.

Chain of custody requires: timestamp of evidence collection, identity of the person who collected it, storage location, access controls, and a hash of the evidence file. When evidence is collected, generate a cryptographic hash of the file and store the hash separately. When the evidence is later reviewed, regenerate the hash and verify it matches the original. If the hashes match, the file has not been modified. If they do not match, tampering occurred.

For collaborative red team engagements where multiple people contribute findings, use a shared evidence repository with access logs. Every time someone uploads or accesses evidence, the system logs who accessed what and when. This creates an audit trail.

For extremely sensitive findings, consider having a neutral third party — like an external auditor or legal counsel — witness the evidence collection. The witness signs a statement confirming they observed the evidence being collected and that it was stored securely. This level of rigor is rare, but it is appropriate when findings might be challenged in court or by regulators.

A financial services company uses cryptographic signing for all red team evidence. When a red teamer collects evidence, they generate a SHA-256 hash of the file and digitally sign the hash with their private key. The signed hash is stored alongside the evidence. When the finding is reviewed, the signature is verified to confirm the evidence came from the red teamer and has not been altered. This prevents disputes about evidence authenticity.

## When Documentation Saves the Finding

The conversation history leak finding was eventually reproduced — but it took four weeks. The security team tried multiple variations based on the vague one-line note. None worked. They escalated to the red teamer, who tried to remember exactly what they had done three weeks earlier. After multiple back-and-forth messages, the red teamer reconstructed the steps from memory. The defender tested again and reproduced the issue. By that time, the urgency was gone. The finding was deprioritized because it took so long to confirm. If the red teamer had documented the exact prompts and steps immediately, the vulnerability would have been fixed within days instead of being delayed for weeks.

Documentation is not overhead. It is the deliverable. A red team that finds 50 vulnerabilities but documents none of them delivers zero value. A red team that finds 10 vulnerabilities and documents them rigorously delivers 10 fixes. The quality of your documentation determines the impact of your work.

Next, we will examine severity classification for AI vulnerabilities — how to assess risk when traditional frameworks like CVSS do not account for AI-specific failure modes.
