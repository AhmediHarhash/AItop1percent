# 10.8 — Severity Classification for AI Vulnerabilities

Why does traditional CVSS fail when you try to score prompt injection? CVSS asks: can the attacker execute code? No. Can they access the filesystem? No. Can they escalate privileges in the operating system? No. By traditional software metrics, prompt injection looks low severity — no code execution, no system access, no privilege escalation. But in a production AI system, prompt injection can extract patient data, manipulate financial advice, bypass content filters, and cause regulatory violations. The impact is severe. The risk is real. CVSS cannot capture it because CVSS was designed for vulnerabilities in traditional software, not for vulnerabilities in AI behavior.

AI vulnerabilities require adapted severity frameworks. Traditional CVSS does not capture AI-specific risks. Building an AI severity framework means understanding which risk factors matter for AI systems and designing a scoring methodology that reflects those factors.

## Why Traditional Severity Frameworks Miss AI Risks

CVSS and similar frameworks evaluate vulnerabilities based on confidentiality, integrity, and availability impact. A vulnerability that allows unauthorized access to data scores high on confidentiality. A vulnerability that allows data modification scores high on integrity. A vulnerability that crashes the system scores high on availability. These categories work for traditional software where vulnerabilities are technical exploits — buffer overflows, SQL injection, authentication bypass.

AI vulnerabilities often do not fit these categories cleanly. Model inversion attacks extract information about training data — is that a confidentiality impact? Technically yes, but CVSS requires specifying what data is exposed and to whom. With model inversion, you are not exfiltrating a database — you are statistically inferring attributes of training samples. The attack is probabilistic, the exposure is partial, and the affected data is not a defined asset in your asset inventory. CVSS does not have a good way to score this.

Bias vulnerabilities are even harder. If your hiring AI systematically discriminates against a demographic group, what is the CVSS score? There is no confidentiality impact — no data was leaked. There is no availability impact — the system is running fine. There is an integrity impact in the sense that outputs are incorrect — but CVSS integrity is about data modification, not output correctness. A traditional security team using CVSS would classify bias as low or medium severity. A legal team looking at discrimination lawsuits would classify it as critical.

Jailbreaking is another example. If a jailbreak causes a customer support AI to generate offensive content, the technical impact is that the model produced an output it was instructed not to produce. CVSS sees this as low severity — no code execution, no data breach, no system compromise. The business sees this as high severity — reputational damage, user harm, potential regulatory penalties. The disconnect is that CVSS measures technical exploitability and technical impact, while AI risks often manifest as business impact, regulatory impact, or user harm without any traditional technical compromise.

## AI-Specific Risk Factors

Severity frameworks for AI need to account for risk factors that traditional frameworks ignore. One critical factor is **user harm**. An AI system that gives medical advice, provides legal guidance, or influences financial decisions can cause direct harm to users if it produces incorrect outputs. The severity of a vulnerability should reflect the magnitude and likelihood of user harm. A bias vulnerability in a resume screening tool harms job applicants. A hallucination vulnerability in a legal AI harms people who rely on the incorrect legal advice. A jailbreak in a mental health chatbot could harm vulnerable users. User harm is not a category in CVSS because traditional software vulnerabilities do not directly harm users — they compromise systems, and system compromise might eventually lead to user harm. AI vulnerabilities can harm users directly.

Another factor is **regulatory exposure**. AI systems operating in regulated industries face specific compliance requirements. A vulnerability that causes GDPR violations, HIPAA violations, or EU AI Act non-compliance creates legal and financial risk. The severity should reflect the regulatory consequences. A data leakage vulnerability in a healthcare AI that exposes patient records is critical not just because data was leaked but because HIPAA penalties are severe. A bias vulnerability in a hiring AI is critical not just because it is unfair but because it violates employment discrimination law in most jurisdictions.

**Reputational damage** is another AI-specific factor. If a public-facing AI generates offensive content, makes wildly incorrect claims, or behaves erratically, the reputational cost can be enormous even if no data was breached and no users were directly harmed. The severity should account for how visible the vulnerability is and how likely it is to become public. A vulnerability that requires deep technical knowledge to exploit and affects only internal users is less severe than a vulnerability that any user can trigger with a simple prompt and can be easily demonstrated in a viral social media post.

**Exploitability** matters but must be defined differently for AI. In traditional security, exploitability means: can an attacker with no inside knowledge and no special access exploit this? For AI, exploitability also includes: does the attacker need to know the system prompt? Does the attack work reliably or only sometimes? Does it require many attempts or does it work on the first try? Does it require chaining multiple techniques or is it a single-step attack? A prompt injection that works 90% of the time with no knowledge of the system's internals is highly exploitable. A model inversion attack that requires 10,000 queries and statistical analysis is low exploitability.

**Blast radius** is how many users or how much data is affected. A vulnerability that allows one user to access their own data incorrectly is low severity. A vulnerability that allows one user to access data from all users is critical. A vulnerability that biases outputs for 2% of queries is medium severity. A vulnerability that biases 60% of queries is critical. Blast radius turns single-instance findings into systemic risks.

## Building an AI Severity Framework

A practical AI severity framework evaluates vulnerabilities across five dimensions: technical impact, user harm, regulatory exposure, reputational risk, and exploitability. Each dimension gets a score from one to five. The overall severity is a weighted combination of the dimension scores, with weights adjusted based on the organization's risk tolerance and threat model.

**Technical impact** is the closest to traditional CVSS. Does the vulnerability allow data exfiltration, unauthorized access, system manipulation, or denial of service? Score one if there is no technical impact, five if the impact is severe and affects core functionality or sensitive data.

**User harm** evaluates whether the vulnerability can directly harm users. Score one if no user harm is possible, five if the vulnerability can cause severe harm to many users. Severe harm means: physical harm, financial loss, legal consequences, psychological harm, or denial of essential services. A vulnerability in a medical AI that recommends dangerous treatments scores five on user harm. A vulnerability in an entertainment chatbot that generates mildly incorrect trivia scores one.

**Regulatory exposure** evaluates whether the vulnerability causes compliance violations. Score one if there are no regulatory implications, five if the vulnerability causes clear violations of major regulations with severe penalties. A HIPAA violation scores five. A minor terms-of-service violation scores two.

**Reputational risk** evaluates how visible and damaging the vulnerability is if exploited publicly. Score one if exploitation is covert and unlikely to be noticed, five if exploitation is obvious, easy to demonstrate, and likely to generate negative media coverage. A jailbreak that makes the model spew profanity in response to a simple prompt scores five on reputational risk. A subtle bias that requires statistical analysis to detect scores two.

**Exploitability** evaluates how easy it is for an attacker to exploit the vulnerability. Score one if exploitation requires deep expertise, extensive resources, and inside knowledge, five if any user can exploit it with no special skills or knowledge. A vulnerability that works with a single copied-and-pasted prompt scores five. A vulnerability that requires reverse-engineering the model's architecture and crafting adversarial examples scores two.

Once you have scores for each dimension, calculate overall severity. A simple approach is to sum the scores. A vulnerability that scores four on technical impact, five on user harm, five on regulatory exposure, four on reputational risk, and five on exploitability has a total score of 23 out of 25 — clearly critical. A vulnerability that scores two, one, one, two, three has a total score of nine out of 25 — medium severity.

A more sophisticated approach uses weighted scoring. If your organization prioritizes regulatory compliance above all else, weight regulatory exposure higher. If you are a consumer-facing company, weight reputational risk higher. If you operate in a domain where user harm is the primary concern, weight user harm higher. Weights let you customize the framework to your risk profile.

## Exploitability Assessment for AI

Exploitability for AI is not binary. It is a spectrum. At one end: attacks that require no knowledge, no resources, and work every time. At the other end: attacks that require insider access, deep expertise, custom tooling, and work 10% of the time. Most AI vulnerabilities fall somewhere in the middle.

Assess exploitability by asking: What does the attacker need to know? What access do they need? How many attempts are required? How much time does it take? What skills are required? A prompt injection that works for any user with a free-tier account, requires no knowledge of the system's internals, and works on the first attempt is maximally exploitable. A model inversion attack that requires API access, knowledge of the training distribution, 100,000 queries, and a PhD in adversarial machine learning is minimally exploitable.

Probabilistic success rates matter. If an attack works 100% of the time, it is highly exploitable. If it works 50% of the time, it is moderately exploitable — the attacker can just retry until it works. If it works 5% of the time, it is low exploitability unless the attacker can automate retries at scale. Document success rates during testing. If you tried an attack 20 times and it succeeded 14 times, the success rate is 70%. Include this in your severity assessment.

Also consider whether the attack is detectable. An attack that triggers obvious errors or alerts is less exploitable in practice than an attack that succeeds silently. If every failed attempt generates a log entry and successful attempts trigger rate limiting, the attack is less exploitable even if the technical steps are simple. Conversely, if the attack leaves no trace and the defender has no visibility into whether it is happening, it is more exploitable.

A customer service AI had a jailbreak vulnerability that could make the model reveal internal system prompts. The attack was simple: a single message with a specific phrasing. It worked 95% of the time. Any user could execute it. The red team classified it as highly exploitable. The company disagreed — they argued that the attack required knowing the exact phrasing, which was not publicly documented. The red team pointed out that the phrasing had appeared in multiple jailbreak repositories on GitHub and had been discussed in AI security forums. The knowledge was public. The company revised their assessment and classified the vulnerability as critical.

## Impact Assessment Beyond Data Breaches

Impact for AI vulnerabilities is not just about data breaches. It includes incorrect outputs, biased decisions, manipulated behavior, degraded performance, and emergent risks. Traditional impact assessment asks: what data was accessed or modified? AI impact assessment asks: what did the model do wrong, who was affected, and what were the consequences?

For a hallucination vulnerability, the impact is that the model generated false information. The severity depends on the domain. False information about celebrity gossip is low impact. False information about drug dosages is high impact. False information about legal rights is high impact. The content of the incorrect output determines severity.

For a bias vulnerability, the impact is that the model treated different groups unfairly. The severity depends on the magnitude of the bias and the harm it causes. A 2% difference in approval rates between demographic groups might be low severity if the decisions are low-stakes. A 40% difference in approval rates for loan applications is high severity because it causes financial harm and legal exposure.

For a prompt injection vulnerability, the impact depends on what the attacker can do after injection succeeds. If injection allows reading other users' conversations, the impact is data confidentiality. If injection allows modifying system behavior to give incorrect advice, the impact is integrity and user harm. If injection allows executing tools on behalf of other users, the impact is unauthorized action. The same vulnerability class — prompt injection — can have different impacts depending on system capabilities.

For retrieval poisoning, the impact is that the model's responses are influenced by attacker-controlled content. The severity depends on how much influence the attacker has and how critical the affected outputs are. If poisoning only affects obscure queries that few users make, the impact is low. If poisoning affects common queries and the system is used for high-stakes decisions, the impact is critical.

## Probability-Weighted Severity

Some AI vulnerabilities are severe if exploited but unlikely to be exploited. Others are less severe but highly likely. Probability-weighted severity accounts for both. The formula is: severity equals impact times probability of exploitation. A high-impact, low-probability vulnerability might have the same probability-weighted severity as a medium-impact, high-probability vulnerability.

Probability of exploitation depends on attacker motivation, exploitability, and visibility. If your threat model says attackers are highly motivated to exploit this class of vulnerability, probability is high. If the vulnerability is easy to exploit, probability is high. If the vulnerability is publicly known or similar vulnerabilities have been exploited in other systems, probability is high.

A genomics AI company found a model inversion vulnerability that could theoretically extract genetic information from the training data. The impact was severe — genetic data is highly sensitive. But exploitation required 500,000 API queries, statistical expertise, and prior knowledge of which individuals were in the training set. The company assessed the probability of exploitation as low — they had rate limits that would block 500,000 queries from a single user, and the attacker would need to already know which individuals to target. The probability-weighted severity was medium, not critical. They fixed it, but they did not treat it as an emergency.

Contrast this with a prompt injection vulnerability in the same system. The impact was moderate — the injection allowed users to see metadata about the model's training process but not actual data. But the exploitability was trivial — any user could do it with a single prompt. The probability of exploitation was near 100%. The probability-weighted severity was high. The company fixed this one immediately.

Probability-weighted severity prevents overreacting to theoretical vulnerabilities and underreacting to practical ones. It aligns remediation effort with actual risk.

## Context-Dependent Severity

The same vulnerability has different severity in different contexts. A jailbreak that generates offensive content is critical in a customer-facing chatbot and low severity in an internal research tool used by three engineers. A data leakage vulnerability that exposes synthetic test data is low severity. The same vulnerability exposing real user data is critical. Severity must account for context.

Context includes: who uses the system, what data it processes, what decisions it informs, what regulations apply, and how visible failures are. A bias vulnerability in a hiring AI used by a major employer is critical because it affects many people and exposes the company to legal risk. The same bias in an experimental research tool is medium severity. The technical vulnerability is identical. The context changes the severity.

Context also includes compensating controls. If a vulnerability allows data leakage but all data is encrypted and the attacker does not have the decryption keys, the practical impact is lower than if the data is plaintext. If a jailbreak allows generating prohibited content but the system has a secondary filter that catches and blocks that content before it reaches users, the severity is lower than if there is no filter. Compensating controls do not eliminate the vulnerability — it should still be fixed — but they reduce severity.

A legal AI company found a hallucination vulnerability where the model occasionally cited non-existent case law. In their system, every legal citation was automatically verified against a legal database before being shown to users. Citations that did not validate were flagged and removed. The compensating control reduced the severity from critical to medium. The vulnerability was still a problem — users should not see incorrect citations even briefly — but the control prevented user harm.

## Severity Disputes and Resolution

Red teams and defenders often disagree about severity. The red team finds a vulnerability, classifies it as critical, and the defender says it is medium. This happens because severity is partly subjective and partly dependent on information the red team might not have. Severity disputes are normal. Resolving them requires structured conversation and shared frameworks.

When a dispute occurs, both sides should document their reasoning using the severity framework. If the red team says a vulnerability is critical, they should explain which dimensions — technical impact, user harm, regulatory exposure, reputational risk, exploitability — drove that assessment and what scores they assigned. If the defender disagrees, they should explain which dimension scores they would change and why. This moves the conversation from "I think it's critical" versus "I think it's medium" to "I scored exploitability as five, you scored it as three — let's discuss why."

Disputes often reveal missing context. The red team might not know that a compensating control exists. The defender might not know that the attack is trivial to execute. Sharing context often resolves the dispute. If the defender says "we have a filter that blocks this" and the red team was not aware of the filter, they might revise their assessment downward. If the red team says "this attack is publicly documented in five GitHub repositories" and the defender thought it was novel, they might revise their assessment upward.

For unresolved disputes, escalate to a neutral party. Many organizations have a security architect or risk committee that adjudicates severity disputes. The red team and defender each present their case. The neutral party reviews the evidence and makes a final determination. This prevents disputes from stalling remediation.

A pharmaceutical AI company has a severity review board that meets weekly. Any finding where the red team and defender disagree on severity by more than one level — for example, red team says critical, defender says medium — goes to the board. The board reviews the technical details, the business context, and the threat model, then issues a binding severity determination. This process resolves 90% of disputes within one week.

Severity classification is not academic. It drives remediation prioritization, resource allocation, and stakeholder communication. Get it wrong and you either waste resources fixing low-severity issues or ship critical vulnerabilities unfixed. Get it right and your remediation efforts align with actual risk.

Next, we will examine how to report findings in a way that drives action — turning severity classifications and documented evidence into clear, actionable reports that defenders can use to fix vulnerabilities quickly.
