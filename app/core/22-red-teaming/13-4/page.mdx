# 13.4 — Industry-Specific Requirements: Healthcare, Finance, Legal

In October 2025, a healthcare AI company passed its standard security audit with flying colors. The product had clean evals, solid red team coverage, comprehensive documentation. Three months later, the FDA rejected their submission. The issue was not that their AI performed poorly — it was that they had never tested for the specific failure modes that medical device regulations require. They had red-teamed for general adversarial attacks. They had not red-teamed for regulatory compliance in a domain where the definition of harm is written into law.

Regulated industries do not treat red teaming as optional hardening. They treat it as mandatory evidence that your system meets legal standards for safety, privacy, and professional responsibility. A healthcare AI faces different risks than a finance AI, which faces different risks than a legal AI. The attack surface is shaped by the regulatory environment. If your red teaming program does not reflect the specific requirements of your industry, you are building evidence for the wrong audit.

## Why Industry Context Redefines Red Teaming

Generic red teaming asks: can someone manipulate this model? Industry-specific red teaming asks: can this model violate the specific laws and professional standards that govern our domain? The difference is not just scope — it is what counts as a finding.

In healthcare, a model that leaks patient information is not just a privacy failure — it is a HIPAA violation with defined penalties. In finance, a model that produces biased loan decisions is not just an unfairness issue — it is a Fair Lending Act violation that regulators will investigate. In legal, a model that mixes client information across cases is not just a data leak — it is a breach of attorney-client privilege with professional consequences.

Your red team must test for the harms that your regulators care about. Not the harms that are theoretically possible. Not the harms that make for good conference talks. The harms that, if they occur, result in enforcement actions, fines, loss of licenses, or legal liability. This requires understanding what your industry defines as failure.

Most teams approach this backward. They build a general red teaming program, then try to map it to regulatory requirements afterward. The correct approach is to start with the regulatory requirements, extract the specific AI failure modes they imply, and build test suites that directly demonstrate compliance. Red teaming becomes an extension of your compliance program, not a separate security exercise.

## Healthcare AI: Testing for Medical Harm and Privacy

Healthcare red teaming focuses on patient safety and data protection. The FDA evaluates AI medical devices under the same framework as any medical device — does it do what it claims, and does it fail safely? HIPAA governs how patient data is handled, stored, and disclosed. Your red team must prove the model meets both standards.

For patient safety, you test whether the model can be manipulated to produce clinically dangerous outputs. A diagnostic AI must be tested for adversarial inputs that cause missed diagnoses. A treatment recommendation system must be tested for prompts that generate contraindicated drug combinations. A triage AI must be tested for edge cases where it misclassifies severity. These are not generic adversarial attacks — they are clinical failure modes.

Your test suite should include every category of harm the FDA defines for your device classification. If your model is a clinical decision support tool, test for false negatives on life-threatening conditions. If it processes radiology images, test for adversarial perturbations that look clinically normal but cause misclassification. If it recommends treatments, test whether it can be prompted to ignore contraindications, drug interactions, or allergy warnings. The red team must think like a clinician evaluating whether they would trust this tool with their patients.

For privacy, HIPAA compliance requires demonstrating that the model cannot be manipulated to leak protected health information. Test whether prompting can extract patient names, dates of birth, medical record numbers, or diagnosis codes. Test whether the model memorized training data containing identifiable information. Test whether fine-tuning on your hospital's data introduced leakage paths that were not present in the base model. Test whether conversation history can be exploited to reconstruct patient records across sessions.

You also test for use cases that violate the minimum necessary standard. Can the model be prompted to generate more patient information than clinically required for the task? Can it be tricked into disclosing information about patients not involved in the current interaction? If your model has access to an EMR system via tool use, can it be manipulated to query records it should not access? The red team must map every retrieval path and test whether access controls hold under adversarial prompting.

Healthcare red teaming also includes fairness testing specific to medical equity. Regulators increasingly scrutinize AI systems for bias in diagnosis, treatment recommendations, and resource allocation. Test whether your model produces different outcomes based on protected characteristics like race, gender, age, or socioeconomic status. Test whether it perpetuates known biases in medical literature or training data. Document the disparities you find and the mitigations you implement. When the FDA asks how you ensured equitable performance, your red team reports are the evidence.

## Financial Services: Testing for Bias, Fraud, and Regulatory Violations

Finance red teaming focuses on fairness, fraud prevention, and regulatory compliance. Banking regulators care whether your AI treats customers fairly under fair lending laws. The SEC cares whether AI-generated financial advice meets fiduciary standards. Anti-money-laundering rules require proving your system cannot be manipulated to bypass compliance checks. Your red team must test all three.

For fair lending, you test whether the model can be manipulated to produce discriminatory outcomes. If your AI approves loans, test whether adversarial prompting can cause it to apply different standards based on race, gender, national origin, or other protected classes. Test whether it can be tricked into using prohibited variables like zip code as a proxy for race. Test whether prompts can cause it to ignore fair lending policies or apply stereotypes in creditworthiness assessments.

Regulators will not accept "we tested for bias and found none" without detailed evidence. You must document the specific prompts you tested, the demographic breakdowns you analyzed, and the statistical methods you used to detect disparate impact. You must show that you tested across multiple scenarios — not just clean applications, but edge cases, adversarial inputs, and ambiguous situations. Your red team findings become the basis for your fair lending audit trail.

For fraud and anti-money-laundering compliance, you test whether the model can be manipulated to approve suspicious transactions. If your AI flags unusual activity, test whether prompting can suppress alerts on transactions that should trigger review. If it verifies customer identities, test whether it can be tricked into accepting forged documents or bypassing verification steps. If it monitors for patterns consistent with money laundering, test whether adversarial users can structure their behavior to evade detection.

You also test for insider threats. Can an employee prompt the model to approve transactions that violate policy? Can they use the AI to generate justifications for high-risk decisions that would normally require manager approval? Can they extract customer financial information they should not access? Financial institutions are liable for the actions their systems enable, even when those actions are initiated by employees. Your red team must prove that privilege boundaries hold under adversarial use.

For investment and advisory AI, you test fiduciary compliance. Can the model be prompted to recommend products that benefit the institution at the customer's expense? Can it be manipulated to generate advice that does not match the customer's stated risk tolerance or financial goals? Can it be tricked into omitting material risks or conflicts of interest? Regulators treat AI-generated advice the same as human-generated advice — it must meet the same standards of care.

## Legal Industry: Testing for Privilege, Confidentiality, and Professional Responsibility

Legal AI red teaming focuses on attorney-client privilege, confidentiality, and professional ethics. A legal AI that leaks client information across cases is not just a privacy failure — it is a breach of professional duty that can result in malpractice liability, bar complaints, and loss of client trust. Your red team must prove that privilege boundaries are absolute.

Test whether the model can be prompted to disclose information from one client's case in response to queries from another client. Test whether conversation history can be exploited to reconstruct privileged communications. Test whether the model memorized specific case details during training and can be manipulated to reveal them. Test whether fine-tuning on your firm's case data introduced leakage paths that violate privilege.

You also test for conflicts of interest. Can the model be prompted to provide advice that benefits one client at the expense of another? Can it be manipulated to generate work product for a party adverse to an existing client? Can it be tricked into using confidential information from one matter to inform strategy in a conflicting matter? Law firms are subject to strict conflict rules — your AI must respect them even under adversarial prompting.

For work product protection, test whether the model can be prompted to reveal attorney mental impressions, case strategy, or litigation theories. These protections are central to legal practice. If your AI drafts briefs, test whether it can be manipulated to disclose the reasoning behind strategic decisions. If it researches case law, test whether it can be tricked into revealing which precedents you are relying on or avoiding. Work product must remain protected even when the AI is prompted by someone with partial access to the case.

Legal red teaming also includes testing for professional ethics violations. Can the model be prompted to generate advice that violates professional rules of conduct? Can it be manipulated to draft documents that contain false statements, fail to disclose controlling authority, or advise clients to destroy evidence? Lawyers are personally responsible for the work they submit, even if it was AI-generated. Your red team must prove the model cannot be used to violate professional obligations.

## Cross-Industry Common Patterns

Despite industry differences, certain patterns appear across regulated sectors. First, regulators require proof of testing, not just assurances. Your red team documentation must be detailed enough to demonstrate that you tested the specific risks your industry faces. Generic statements about security testing are not sufficient. You need test plans, methodologies, finding reports, and evidence of remediation.

Second, regulators expect testing to evolve as threats evolve. A red team assessment from two years ago does not prove your system is safe today. You need ongoing testing that reflects new attack techniques, new regulatory guidance, and new model capabilities. If your industry regulator publishes new AI guidance, your red team should test compliance within the next assessment cycle.

Third, regulators increasingly expect transparency about limitations. You cannot claim your system is safe for all use cases — you must define the scope within which it is safe and document the red team findings that establish those boundaries. If your healthcare AI should not be used for pediatric patients because it was not tested on that population, your documentation must state that explicitly. If your finance AI should not be used for high-value transactions because red teaming found edge cases in that range, your limits must reflect that finding.

Fourth, regulators treat AI as part of a system, not as an isolated component. Your red team must test the full deployment context — the human workflows, the data pipelines, the access controls, the monitoring systems. A model that is safe in isolation can become unsafe when integrated into a production environment with weak access controls or inadequate oversight. Industry-specific red teaming tests the system as deployed, not the model in a lab.

## Building Industry-Specific Test Suites

Start by mapping your regulatory obligations to AI failure modes. For each regulation that applies to your product, identify the specific ways an AI system could violate it. If you are subject to HIPAA, list every pathway by which patient information could be disclosed without authorization. If you are subject to fair lending laws, list every decision point where bias could enter. If you are subject to attorney-client privilege, list every interaction where confidential information could leak across cases.

For each failure mode, design adversarial test cases. If HIPAA requires minimum necessary disclosure, create prompts that try to extract more information than the task requires. If fair lending prohibits using race as a variable, create prompts that try to introduce race through proxies or indirect references. If privilege requires client separation, create prompts that try to cross privilege boundaries through conversation history or shared context.

Your test suite should include both automated tests and manual red teaming. Automated tests cover known attack patterns and regression prevention. Manual red teaming covers novel adversarial strategies and domain-specific manipulation. A healthcare AI might use automated tests to check for patient identifier leakage and manual red teaming to test whether clinicians can be socially engineered into accepting dangerous recommendations. A finance AI might use automated tests for prohibited variable usage and manual red teaming to test whether customers can manipulate credit decisions through carefully crafted narratives.

Document your test suite design. Regulators want to understand your methodology, your coverage, and your rationale. Why did you test these scenarios and not others? How did you prioritize test cases? What assumptions did you make about attacker capabilities? Your documentation should show that your testing was systematic, risk-based, and grounded in the specific threats your industry faces.

## Regulatory Evolution Monitoring

Regulations change. New guidance is published. Enforcement priorities shift. Your red team program must track regulatory evolution and adapt testing to reflect new requirements. In 2025, the EU AI Act introduced new obligations for high-risk AI systems. In 2026, multiple jurisdictions are considering AI-specific healthcare regulations. If you wait until enforcement actions begin to update your testing, you are already behind.

Assign someone to monitor regulatory developments in your industry. This can be legal counsel, compliance staff, or a dedicated AI governance role. When new guidance is published, evaluate whether it introduces new testing requirements. If a regulator publishes an AI risk framework, compare it against your current red team scope. If there are gaps, add test cases to cover them.

Track enforcement actions against other companies in your industry. When a competitor faces regulatory scrutiny for an AI failure, analyze whether your system is vulnerable to the same issue. If a healthcare AI is cited for failing to detect bias in diagnostic recommendations, test whether your diagnostic AI has the same vulnerability. If a bank is fined for algorithmic discrimination, test whether your lending AI could produce similar outcomes. Regulatory enforcement creates case law for what counts as failure — use it to inform your testing.

Participate in industry groups and standards bodies. Many industries have working groups developing AI best practices and testing frameworks. Participating keeps you informed about emerging standards before they become mandatory. It also gives you a voice in shaping what compliance looks like. If your industry is still defining what red teaming means for AI, contributing to that definition ensures the standards are practical and aligned with how systems are actually built.

## Demonstrating Compliance Through Red Teaming

When regulators audit your AI system, red team documentation is evidence of due diligence. You are demonstrating that you proactively tested for the risks your industry faces, found vulnerabilities, and fixed them before they caused harm. Without that evidence, regulators must assume you did not test — or worse, that you tested, found problems, and deployed anyway.

Your documentation should map red team findings to regulatory requirements. If a regulation requires patient privacy protection, reference the red team tests that evaluated leakage risks and the mitigations you implemented based on findings. If a regulation requires fair lending compliance, reference the bias tests you conducted and the model changes you made to address disparate impact. The connection between requirement, test, finding, and mitigation should be explicit.

Track how findings are remediated. Regulators want to see that red team findings drive action, not just documentation. If your team found that a healthcare AI could be prompted to recommend contraindicated medications, document the prompt engineering changes, the retrieval logic updates, and the follow-up testing that verified the fix. If your team found that a lending AI could be manipulated to use prohibited variables, document the input filtering, the model fine-tuning, and the regression tests that ensure the issue does not recur.

Demonstrate continuous testing. Compliance is not a one-time event — it is an ongoing process. Your audit trail should show regular red team assessments, not a single test conducted at launch. It should show that you test after model updates, after regulatory changes, and after new attack techniques are disclosed. The pattern of continuous testing signals that you treat compliance as a discipline, not a checklist.

Industry-specific red teaming is how you prove your AI is safe within the legal and professional standards of your domain. The next step is understanding when external validation strengthens that proof.

