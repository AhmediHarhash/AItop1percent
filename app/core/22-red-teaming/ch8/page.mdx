# Chapter 8 — Agent Red-Teaming

Autonomous agents amplify every vulnerability. They can take sequences of actions, persist across sessions, and make decisions without human oversight. Red teaming agents requires testing trajectories and behaviors, not just single outputs. This chapter teaches how to systematically test agent systems for failures that only emerge through autonomy.

---

- 8.1 — Why Agent Red-Teaming Is Different
- 8.2 — The Agent Threat Model: Autonomy as Attack Multiplier
- 8.3 — Goal Hijacking: Redirecting Agent Objectives
- 8.4 — Runaway Behavior: When Agents Do Not Stop
- 8.5 — Side-Effect Attacks: Unintended Consequences as Exploitation
- 8.6 — Multi-Agent Attacks: When Agents Attack Each Other
- 8.7 — Memory Poisoning: Corrupting Agent State
- 8.8 — Planning and Reasoning Attacks
- 8.9 — Testing Agent Boundaries: Containment Verification
- 8.10 — Agent Sandboxing and Kill Switches
- 8.11 — Recovery from Agent Incidents

---

*When the system can act, plan, and persist without human intervention, every attack surface becomes a trajectory — and every trajectory needs adversarial testing.*
