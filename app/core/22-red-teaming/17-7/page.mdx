# 17.7 — Control Validation Workshops — Testing Defenses Against Real Attack Chains

Every security team believes their defenses work. Almost none of them have tested that belief against a complete attack chain executed end to end. They have tested individual controls — the injection classifier catches injection, the output filter catches sensitive data, the rate limiter throttles excessive requests. But an attacker does not use one technique. An attacker chains techniques. They inject through a retrieved document to bypass the input classifier, use the injected instructions to call a tool with modified parameters that the tool abuse detection does not flag because each parameter falls within normal range individually, and extract data through a response pattern that the output filter does not recognize because the data is embedded in a conversational narrative rather than a structured format. Each control passed its own test. The chain passed through every control without triggering a single alert. This is the gap that control validation workshops exist to close.

## The Difference Between Testing Controls and Testing Chains

Testing an individual control answers a narrow question: does this detection fire when presented with this specific signal? Testing an attack chain answers the question that actually matters: does the defense stack stop an attacker who is trying to achieve a specific objective?

The distinction is critical because controls interact in ways that create gaps invisible to single-control testing. An input classifier might flag a prompt as 60 percent likely to contain injection — below its 70 percent threshold — and pass it through. The tool monitoring system sees the subsequent tool call and notes that the parameters are unusual but within one standard deviation of normal — below its two-standard-deviation threshold. The output filter checks the response and finds no exact matches against known sensitive data patterns because the extracted data is paraphrased rather than verbatim. Every control functioned exactly as designed. The attack succeeded because no individual control saw enough evidence to trigger, even though the combined evidence across all three controls clearly indicated an attack.

Control validation workshops force teams to look at the full chain, not just the links. They ask: if this attacker executes these steps in this order, what does our defense stack see at each stage, and does the combined picture trigger a response?

## Workshop Format and Structure

A control validation workshop is a structured exercise, typically lasting two to four hours, where the red team and blue team jointly walk through a specific attack chain against the current production defense stack. The format follows five phases.

**Phase one: attack chain selection.** The red team selects an attack chain from their backlog — a multi-step technique that they have demonstrated works against the system in a testing environment. The chain should represent a realistic adversarial objective: extract customer data, bypass safety controls to produce harmful content, pivot from one tool to another to escalate privileges, or poison the model's memory to influence future interactions. The selection prioritizes chains that cross multiple defense boundaries because those are the chains most likely to exploit inter-control gaps.

**Phase two: walk-through at the whiteboard.** Before touching any system, the team walks through the attack chain step by step on a whiteboard or shared document. For each step, they discuss three things. What action does the attacker take? What telemetry does this action produce? Which defense controls should detect or block this action? This walk-through often reveals gaps before anyone runs a single command. The red team member says "at step three, I call the inventory API with a wildcard parameter" and the blue team member says "we do not have a detection for wildcard parameters on the inventory API." That gap was invisible in the detection library because nobody had tested the inventory API's tool call monitoring against wildcard inputs. The whiteboard phase is where the fastest, cheapest discoveries happen.

**Phase three: live execution.** The red team executes the attack chain against a staging environment that mirrors production configuration — same model, same prompt, same tools, same detections, same automated responses. The blue team monitors in real time, watching their dashboards, SIEM feeds, and alerting channels. At each step, both teams pause and document what happened. Did the detection fire? Did the automated response trigger? If not, why not? The live execution phase often reveals timing issues, threshold misconfigurations, and telemetry gaps that the whiteboard phase missed because they depend on real system behavior rather than theoretical analysis.

**Phase four: gap documentation.** Every point where the defense stack failed to detect or respond to an attack step is documented as a gap. Each gap gets classified by severity (how much damage could the attacker do if this gap is exploited), detectability (how feasible is it to build a detection for this gap), and remediation effort (how much work to close the gap). The gap register becomes the detection engineering backlog for the next sprint.

**Phase five: immediate fixes.** For gaps that can be closed quickly — adjusting a threshold, adding a correlation rule, extending an existing detection to cover an additional tool — the team closes them during the workshop. The red team replays the relevant attack steps to confirm the fix works. Gaps that require more substantial engineering — new telemetry sources, new detection models, architectural changes — are scheduled for future work with clear ownership and deadlines.

## What Workshops Typically Discover

After running validation workshops across dozens of organizations, patterns emerge in what teams discover.

**Inter-control blind spots** are the most common finding. Defense systems are designed and tested in isolation. The input classifier team builds and validates their classifier. The tool monitoring team builds and validates their monitors. The output filter team builds and validates their filters. Nobody tests the three systems together against an attack that is designed to stay below every individual threshold while exceeding what the combined picture should trigger. Workshops discover that the correlation rules connecting these systems either do not exist, fire too slowly, or use thresholds calibrated against single-control testing rather than multi-step attack chains.

**Timing gaps** are the second most common finding. A detection rule that fires correctly when signals arrive simultaneously may fail when the attack spaces its steps across a longer time window. The attacker who waits five minutes between injection and tool abuse exploits the sixty-second correlation window that works perfectly against the fast version of the same attack. Workshops reveal which time windows are too narrow by having the red team execute the same chain at different speeds.

**Telemetry gaps** are the third most common finding. The attack produces signals, but those signals are not captured in the monitoring pipeline. The tool call was logged, but the parameters were not included in the log event because the logging was configured for audit compliance rather than security analysis. The model's response was logged, but the retrieved context that influenced the response was not. The safety filter decision was logged, but the confidence score was not included. Each missing data point represents a detection that cannot be built until the telemetry is enriched.

**Evasion-ready variants** are the fourth finding. During the workshop, the red team naturally iterates on techniques that fail. "That injection triggered the classifier, but what if I encode it differently?" "The tool call was flagged because the parameter was obviously anomalous, but what if I use a value that is unusual but not impossible?" Each iteration that bypasses a control represents a technique that a real attacker would develop independently. Capturing these evasion variants during the workshop and building detections for them preempts adversarial evolution.

## Running Workshops on a Regular Cadence

Validation workshops are not one-time events. The defense stack changes with every model update, prompt revision, tool integration, and detection deployment. A workshop that validated the defense stack in January may reveal new gaps by March because the model was updated in February and the behavioral baselines shifted.

The recommended cadence depends on the system's change velocity. High-change systems — those with monthly or more frequent model updates, active prompt engineering, or ongoing tool integrations — need monthly validation workshops. Stable systems with quarterly model updates and infrequent changes can run workshops quarterly. Every team, regardless of change velocity, should run an ad-hoc workshop within one week of any major system change: a new model deployment, a new tool integration, a significant prompt architecture revision, or a new detection deployment.

Each workshop does not need to test every attack chain in the library. The selection should prioritize chains that are most likely to be affected by recent changes. If the model was updated, prioritize chains that depend on model behavior — injection techniques, safety bypass, and output manipulation. If a new tool was integrated, prioritize chains that involve tool abuse and lateral movement. If a new detection was deployed, test the chains it was designed to catch and chains that are adjacent to the detection's coverage area.

## The Gap Register — Tracking What You Cannot Yet Defend

The gap register is the workshop's primary output. It is a living document that tracks every identified defensive gap, its severity, its status, and its owner. The register serves three functions.

For the detection engineering team, it is a prioritized backlog. The gaps are ranked by severity and remediation feasibility, giving the team a clear roadmap of what to build next.

For security leadership, it is a risk register. Each open gap represents an attack path that the defense stack cannot currently stop. The register makes risk tangible — not "we might have gaps" but "we have fourteen identified gaps, three of which allow cross-tenant data extraction, and remediation is estimated at four weeks of engineering effort."

For the purple team, it is a progress tracker. The percentage of gaps closed since the last workshop measures the team's operational velocity. A team that identifies twenty gaps per workshop but only closes five between workshops is falling behind — the gap register grows and the defense posture weakens with each cycle. A healthy ratio is closing at least 70 percent of identified gaps between workshops, with the remainder tracked for longer-term remediation.

## Documentation That Drives Action

Workshop documentation must be precise enough to drive engineering action. "Tool monitoring has gaps" is not actionable. "The inventory API tool call monitor does not capture the scope parameter, which allows an attacker to pass a wildcard value that returns all inventory records instead of a single item — remediation requires adding the scope parameter to the tool call telemetry schema and adding a detection rule that flags wildcard or overly broad scope values" is actionable.

Each gap entry in the register includes the attack chain that revealed it, the specific step where the defense failed, the root cause of the failure (missing telemetry, misconfigured threshold, absent correlation rule, or architectural limitation), the proposed fix, the estimated effort, and the owner. This specificity is what separates a workshop that produces improvement from a workshop that produces awareness. Awareness fades. Specific, owned action items get executed.

Workshops test whether your defenses stop known attack chains. But what about attack techniques your defenses have never seen — techniques the red team can execute but that produce no detection at all? The next subchapter covers red versus monitoring gap analysis, the systematic process of comparing what the red team can do against what the blue team can see.
