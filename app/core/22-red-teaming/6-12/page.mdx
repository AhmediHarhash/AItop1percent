# 6.12 — Privacy-Preserving Inference: Differential Privacy and Beyond

When a healthcare AI vendor claims their system is "privacy-preserving," what does that actually mean? For most vendors, it means they scrubbed some PII and added output filtering. For attackers, it means there is still data to extract. Privacy-preserving techniques are different. They offer mathematical guarantees — provable bounds on what an attacker can learn, even with unlimited access and unlimited computational power. These techniques are not hypothetical. They are deployed in production at organizations that cannot tolerate data leakage under any circumstances. Understanding privacy-preserving inference means knowing what these techniques provide, what they cost, and when the trade-offs are worth it.

## What Differential Privacy Provides

Differential privacy is not a vague promise. It is a mathematical definition. A mechanism is differentially private if an observer cannot determine whether any specific individual's data was included in the training set, even if the observer knows everything about everyone else. The guarantee is quantified by epsilon, the privacy budget. Lower epsilon means stronger privacy. Epsilon zero is perfect privacy — the model reveals nothing. Epsilon infinity is no privacy — the model can reveal everything. Real systems operate in between.

The guarantee matters because it is adversary-independent. It does not matter if the attacker is a casual user, a nation-state, or an AI researcher with full access to your model. The privacy bound holds. If your epsilon is 1.0, the attacker cannot extract individual training records with confidence greater than the bound, regardless of their sophistication. This is different from access control, which can be bypassed, or output filtering, which can be evaded. Differential privacy is a property of the mechanism itself.

But differential privacy does not prevent all information disclosure. It prevents disclosure about individuals. An attacker can still learn aggregate statistics, general patterns, and population-level trends. A differentially private medical AI might reveal that 40 percent of patients in the training set had hypertension. It cannot reveal whether a specific patient had hypertension. The distinction is critical. Privacy-preserving does not mean information-free. It means individual-level information is protected while population-level utility is preserved.

## DP-SGD and Training Time Privacy

The most common privacy-preserving technique is differentially private stochastic gradient descent, or DP-SGD. It modifies the standard training process to add noise and clip gradients, ensuring that no individual training example has disproportionate influence on the model's weights.

Standard gradient descent computes the gradient of the loss function for each training example, averages them, and updates the model weights. If one example has an unusually large gradient — perhaps because it is an outlier or contains rare information — it influences the model more than other examples. Over many training steps, that influence accumulates. The model memorizes the outlier. An attacker can extract it.

DP-SGD changes this. For each training example, it computes the gradient, then **clips** it to a maximum norm. If the gradient is larger than the threshold, it is scaled down. This prevents any single example from dominating updates. After clipping, the algorithm adds **calibrated Gaussian noise** to the averaged gradient. The noise magnitude is determined by the privacy budget and the clipping threshold. The noisy gradient is used to update weights. Over the entire training process, the cumulative privacy loss is tracked. When the total privacy budget is exhausted, training stops.

The result is a model that has been trained with a formal privacy guarantee. An attacker who queries the model, examines its outputs, or even extracts its weights cannot determine with high confidence whether any specific training example was included. The noise and clipping obscure individual contributions. But the noise also degrades model accuracy. Stronger privacy — lower epsilon — requires more noise, which means lower performance. Weaker privacy — higher epsilon — requires less noise, which preserves performance but weakens the guarantee.

## Inference-Time Privacy Techniques

Training with DP-SGD protects training data. But what about inference? Users submit prompts. The model generates responses. If the model memorized training data despite DP-SGD, or if the prompts themselves contain sensitive information, inference can still leak.

**Noisy responses** add calibrated noise to model outputs to satisfy differential privacy at inference time. Instead of returning the exact probability distribution over tokens, the model returns a noisy version. The noise is calibrated to the privacy budget for the query. This prevents an attacker from reverse-engineering model parameters or training data by observing outputs. The cost is reduced output quality. Noisy responses are less coherent, less accurate, and less useful. The trade-off is only worth it in scenarios where inference-time privacy is critical — research datasets, sensitive analytics, or applications where query privacy must be guaranteed.

**Private information retrieval** allows users to query a database or retrieval system without revealing what they are querying. In a privacy-preserving RAG system, the user's query is encrypted or obfuscated in a way that the retrieval system cannot determine what was requested. The system returns encrypted results. The user decrypts them locally. The retrieval system learns nothing about the query. PIR is computationally expensive and rarely practical for real-time AI systems, but it represents the theoretical upper bound of query privacy.

**Secure multi-party computation** enables multiple parties to jointly compute a function over their private inputs without revealing those inputs to each other. In an AI context, SMPC could allow multiple organizations to collaboratively train a model on their combined data without any organization seeing another's data. Each party provides encrypted data. The training happens on encrypted values. The final model is shared, but the training data remains private. SMPC is powerful in theory but prohibitively slow in practice for large-scale AI training. Research continues, but production deployment is rare.

## Federated Learning Approaches

Federated learning trains models across decentralized data sources without centralizing the data. Instead of collecting user data in one place and training a central model, the model is sent to the data. Each device or organization trains the model on local data. Only the updated model weights are sent back to the central server. The central server aggregates updates and produces a new global model. The raw data never leaves its source.

Federated learning is not inherently private. An attacker who observes model updates can infer information about the local data used to generate them. If one device sends an update that significantly shifts the model's behavior toward recognizing a rare disease, the attacker might infer that the device's owner has that disease. The updates leak information, even if the data does not.

**Differentially private federated learning** combines federated learning with DP-SGD. Each local training round uses DP-SGD to clip and add noise to gradients before they are sent to the central server. The central server aggregates noisy updates. The global model has a formal privacy guarantee. Individual updates reveal bounded information. The cost is slower convergence and lower accuracy. Federated learning with DP is common in mobile keyboard prediction, health research, and other scenarios where centralized data collection is impractical or prohibited.

The challenge is tuning the privacy budget across many devices. If 10,000 devices each contribute updates, and each update consumes part of the privacy budget, the total budget depletes quickly. Either you allow many rounds of updates and weaken the privacy guarantee, or you limit updates and reduce model quality. Balancing participation, privacy, and utility is an open research problem with no clean solution.

## Homomorphic Encryption Possibilities

Homomorphic encryption allows computation on encrypted data without decrypting it. You encrypt your input. The model processes the encrypted input. The model returns an encrypted output. You decrypt the output. At no point does the model or the infrastructure see your data in plaintext. This is the dream scenario for privacy-preserving inference. Users get predictions without revealing their data. Service providers serve predictions without seeing sensitive information.

The reality is that fully homomorphic encryption is still impractically slow for large language models. Homomorphic operations are orders of magnitude slower than plaintext operations. A query that takes 200 milliseconds in plaintext might take 20 seconds or 20 minutes with homomorphic encryption. Latency aside, FHE also requires significant computational resources and expertise. Most teams cannot deploy it.

Partially homomorphic encryption — which supports only certain operations, like addition or multiplication but not both — is faster but limited in what it can compute. Research teams have demonstrated small-scale models running on encrypted data. Production systems do not yet exist at scale. Homomorphic encryption is a long-term direction, not a 2026 deployment option for most organizations.

## Current Practical Limitations

Privacy-preserving techniques are real, deployed, and mathematically sound. They are also expensive, complex, and limited in what they can achieve. Understanding the limitations prevents over-promising and guides realistic deployment.

**Accuracy cost.** Differential privacy trades accuracy for privacy. The more privacy you want, the more accuracy you lose. For some use cases, a 3 percent accuracy drop is acceptable. For others — medical diagnosis, fraud detection, safety-critical systems — it is not. The accuracy-privacy frontier is a hard trade-off with no free lunch.

**Computational cost.** DP-SGD requires more training time. Homomorphic encryption requires orders of magnitude more inference time. Secure multi-party computation requires coordinated infrastructure and cryptographic protocols. These costs are not marginal. They are multiplicative. Budget for 2x to 10x longer training times and higher cloud bills.

**Complexity cost.** Implementing differential privacy correctly requires expertise in privacy theory, hyperparameter tuning, and failure mode analysis. Misconfigured DP can provide zero privacy while still degrading performance. Vendors claim differential privacy without disclosing epsilon, clipping thresholds, or composition methods. Evaluating these claims requires deep knowledge. Most teams do not have it.

**Limited applicability.** Not every use case benefits from privacy-preserving techniques. If your training data is already public, differential privacy adds cost without benefit. If your use case tolerates some leakage and focuses on access control, DP is overkill. If your latency budget is milliseconds, homomorphic encryption is impossible. Privacy-preserving techniques are tools for specific scenarios, not universal best practices.

## Evaluating Privacy Claims

Vendors market products as "privacy-preserving" without defining what that means. Some claims are legitimate. Some are marketing. Evaluating claims requires asking specific questions.

**What is the epsilon?** If a vendor claims differential privacy but will not disclose epsilon, the claim is unverifiable. Epsilon quantifies the privacy guarantee. Without it, "differentially private" is meaningless. Typical epsilon values range from 0.1 for strong privacy to 10 for weak privacy. If epsilon is above 10, the guarantee is so weak that extraction is still feasible.

**What is the privacy composition method?** If the model is queried multiple times, privacy degrades. Sequential composition means each query consumes part of the budget. Advanced composition uses tighter bounds. If the vendor does not explain how they compose privacy across queries, the guarantee might be exhausted after the first 100 requests.

**What is the threat model?** Differential privacy protects against membership inference — determining if a specific record was in the training set. It does not protect against attribute inference — learning that 60 percent of the training set had a specific characteristic. It does not prevent model inversion — reconstructing inputs from outputs in some scenarios. If the vendor claims DP prevents all information leakage, they are wrong or lying.

**What is the accuracy impact?** Ask for benchmark results comparing the privacy-preserving model to a standard model on the same task. If the vendor will not share accuracy metrics, assume the impact is significant. If they claim zero accuracy cost, they are not using real differential privacy.

**How is the privacy budget allocated?** In federated learning, is the budget per device or global? In multi-tenant systems, is the budget per tenant or shared? Budget allocation determines how many queries or updates are supported before privacy guarantees degrade. Without transparency on allocation, you cannot assess long-term viability.

## When to Use Privacy-Preserving Techniques

Privacy-preserving techniques are not default choices. They are high-cost, high-value tools for scenarios where leakage is unacceptable and regulation or ethics demand stronger guarantees than access control provides.

**Use differential privacy when** training on highly sensitive data — patient records, financial transactions, communications — and regulatory or contractual obligations require provable privacy. DP is common in healthcare research, financial fraud detection, and government applications.

**Use federated learning when** data cannot be centralized due to regulation, user consent, or infrastructure constraints. Federated learning is deployed in mobile keyboards, health apps, and cross-organizational collaborations where data sharing is prohibited.

**Use homomorphic encryption when** latency is not critical and users must retain full control over their data. Research environments, rare disease diagnosis, and scenarios where even metadata leakage is unacceptable are potential use cases.

**Do not use privacy-preserving techniques when** your data is already public, when accuracy loss is unacceptable, when latency budgets are tight, or when access control and output filtering provide sufficient protection. Privacy-preserving techniques solve specific problems. They are not universal solutions.

## The Role in Defense Strategy

Privacy-preserving techniques are one layer in a defense-in-depth strategy. They complement access control, output filtering, and monitoring — they do not replace them. A system using DP-SGD still needs role-based access control to prevent unauthorized queries. A federated learning system still needs output filtering to catch accidental leakage. Homomorphic encryption still needs monitoring to detect abuse patterns.

The value of privacy-preserving techniques is the mathematical guarantee. When everything else fails — when an attacker bypasses access control, evades filters, and extracts model outputs — differential privacy ensures the damage is bounded. The attacker cannot reconstruct individual training records with high confidence. They cannot definitively prove membership. The guarantee holds even in the worst case. That is what you are paying for.

For most systems, the cost is too high. For critical systems — healthcare AI, financial AI, government AI — the cost is acceptable. Knowing when the trade-off is worth it requires understanding both the technical limitations and the regulatory, ethical, and reputational stakes. Privacy-preserving inference is not hype. It is a specialized tool for specialized problems. Use it when the problem demands it. Do not use it because it sounds impressive.

In 6.13, we examine how data extraction creates compliance violations and what that means for testing and governance.