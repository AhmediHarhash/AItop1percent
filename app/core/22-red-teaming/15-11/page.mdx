# 15.11 — Container and Orchestration Attacks

In July 2025, Wiz disclosed CVE-2025-23266, a container escape vulnerability in the NVIDIA Container Toolkit that allowed a crafted container image to break out of the container sandbox and access the host filesystem. This was not the first NVIDIA Container Toolkit escape — CVE-2024-0132, disclosed the previous year with a CVSS score of 9.0, had demonstrated the same class of vulnerability, and Wiz found that 33 percent of cloud environments ran a vulnerable version. The initial patch was incomplete. The follow-up patch addressed the bypass. The pattern repeats: AI workloads require GPU access, GPU access requires privileged container configurations, and privileged container configurations create escape paths that would never exist in a standard web application container. Your model serving pods are not just containers. They are containers with holes in them, and the holes exist by design.

## Why AI Containers Are Different from Standard Containers

Standard application containers run unprivileged. They have no access to the host filesystem, no access to hardware devices, and no elevated Linux capabilities. The container runtime enforces isolation through namespaces, cgroups, and seccomp profiles. This isolation is mature, well-understood, and effective against most escape techniques.

AI containers break this model. Model inference requires direct access to GPU hardware through the NVIDIA Container Toolkit, which mounts the GPU device files and driver libraries into the container at runtime. This mounting process requires elevated privileges and creates a communication channel between the container and the host that does not exist in standard containers. The container toolkit acts as a bridge — and every bridge is a potential attack path.

Beyond GPU access, AI containers often require additional privileges. They need access to shared memory segments larger than the default — many model serving frameworks use shared memory for inter-process communication and tensor storage. They need access to InfiniBand or RDMA network devices for high-performance multi-node inference. They need elevated memory limits because model weights consume tens of gigabytes of RAM before they even reach the GPU. Each of these requirements translates to a container configuration that weakens isolation: expanded shared memory mounts, host network access, device plugins, and elevated resource limits.

The result is a container that looks nothing like the locked-down isolation boundary that security teams imagine when they hear "containerized deployment." Red teams should always begin a Kubernetes assessment of AI infrastructure by examining the pod security configurations. Look for privileged mode, host path mounts, host network access, host PID namespace sharing, expanded capabilities beyond the default set, and device plugin mounts. Every one of these is a potential escape vector, and AI workloads trigger most of them by default.

## Kubernetes-Specific Attacks on Model Serving

Kubernetes is the dominant orchestration platform for AI model serving. Over 70 percent of enterprises running large AI systems rely on Kubernetes for inference orchestration, and that number continues to grow. But Kubernetes was designed for stateless web services, not for GPU-intensive, long-running, security-sensitive AI workloads. The misfit between Kubernetes' design assumptions and AI workload requirements creates attack surface that standard Kubernetes security guides do not cover.

**Service account token abuse** is the most direct Kubernetes attack vector. Every pod in Kubernetes receives a service account token, mounted by default at a well-known path. If the service account has permissions beyond what the pod needs — a common misconfiguration for AI workloads that need access to model registries, secret stores, and custom resources — an attacker who gains code execution inside the pod can use the token to interact with the Kubernetes API server. From there, they can list secrets, read configmaps, create new pods, or escalate privileges depending on the RBAC configuration.

AI workloads are especially susceptible to service account over-permissioning because they legitimately need broad access. The model serving pod needs to pull model weights from a registry, which requires read access to image pull secrets. The training orchestrator needs to create and delete pods, which requires elevated RBAC permissions. The evaluation pipeline needs to read from multiple data sources, which requires secrets containing connection strings. Teams grant these permissions and rarely revoke them, creating service accounts with accumulated privileges that far exceed the minimum required.

**Custom Resource Definition exploitation** targets the Kubernetes extensions that AI platforms install. Tools like KubeFlow, Ray, Seldon Core, and KServe define custom resources for model deployments, training jobs, and inference pipelines. These custom resources have their own validation logic, their own controllers, and their own security assumptions. A vulnerability in a custom resource controller — such as insufficient input validation on a model deployment spec — can allow an attacker to inject arbitrary configuration into the AI platform through Kubernetes' own API.

**Node affinity manipulation** is an AI-specific attack vector. Model serving pods are typically scheduled on GPU nodes using node selectors or affinity rules. If an attacker can modify these scheduling constraints — through RBAC permissions to edit pod specs, or through a compromised admission webhook — they can move workloads to nodes they control or co-locate their malicious pods on the same GPU nodes as sensitive workloads. Co-location enables the GPU side-channel attacks discussed in the previous subchapter and also enables local network interception of pod-to-pod traffic.

## Container Escape from Model Inference Containers

Container escape is the act of breaking out of the container's isolation boundary to access the host system. For AI containers, the escape paths include both generic container escape techniques and AI-specific vectors.

The **NVIDIA Container Toolkit escape path** is the most documented AI-specific container escape. The toolkit's design requires it to mount host resources into the container at runtime. CVE-2024-0132 exploited a Time-of-Check-Time-of-Use race condition in this mounting process, allowing a crafted container image to redirect the mount operation to arbitrary host paths. The attacker gained read and write access to the host filesystem from within the container. The fix in the toolkit's subsequent versions addressed the specific race condition, but the fundamental architecture — a privileged component mounting host resources into containers at runtime — remains a surface that future vulnerabilities will target.

**Shared memory escapes** exploit the expanded shared memory that AI containers require. When containers are configured with SizeOfShmMount values larger than default, or when they mount host shared memory segments, the shared memory becomes a communication channel that bypasses container network isolation. Two containers on the same node that share a memory segment can exchange data without any network traffic, evading network-level monitoring entirely. If one of those containers is a model serving pod and the other is an attacker pod, the attacker can read model inputs and outputs from shared memory.

**Kernel exploit escapes** are not AI-specific, but AI containers' elevated privilege requirements make them more exploitable. A container running with the SYS_PTRACE capability — sometimes required for GPU debugging tools — can trace processes in other containers on the same node. A container with CAP_SYS_ADMIN — sometimes granted for RDMA networking — can mount filesystems and manipulate namespaces. Each elevated capability is a building block for kernel-level escape techniques that would be impossible in a properly restricted container.

Red teams testing for container escape should follow a progression. First, enumerate the container's capabilities, mounts, and device access. Second, test known escape techniques for each elevated capability. Third, attempt to access the host filesystem through any mounted path. Fourth, test whether the container can communicate with the Kubernetes API server. Fifth, check whether GPU device access provides any path to host resources beyond the GPU itself. Document the full escape chain — from initial code execution in the container to host access — because the remediation depends on which link in the chain is easiest to break.

## Sidecar Injection for Model Traffic Interception

In Kubernetes, a **sidecar** is an additional container that runs alongside the main container in the same pod. Sidecars share the pod's network namespace, meaning they see all network traffic to and from the main container. Service meshes like Istio inject sidecar proxy containers automatically — every pod in the mesh gets an Envoy proxy sidecar that handles mutual TLS, traffic routing, and observability.

The security implication is bidirectional. Legitimate sidecars protect traffic by encrypting pod-to-pod communication. Malicious sidecars intercept traffic by exploiting the shared network namespace. An attacker who can inject a sidecar into a model serving pod — through compromised admission webhooks, modified deployment manifests, or Kubernetes RBAC permissions that allow pod spec modification — gains full visibility into the model's input and output traffic.

The **Sidecar Siphon** attack, documented by researchers in 2025, demonstrated a more subtle exploitation of the sidecar architecture. In a service mesh, the Envoy sidecar holds the mutual TLS certificates that authenticate the pod to other services. An attacker inside the application container can access these certificates because the sidecar and the application share the same pod — and the same network namespace, the same process namespace in some configurations, and sometimes the same filesystem mounts. By extracting the mTLS certificates, the attacker can impersonate the pod's identity to any service in the mesh. They become the model serving pod, with all its permissions and all its network access, but they control what requests are sent.

The mitigation landscape is shifting. Istio's Ambient Mesh, which reached production stability in late 2024, replaces sidecar proxies with a per-node "ztunnel" that manages mTLS outside the pod entirely. With no Envoy sidecar in the pod, there are no certificates for the attacker to steal from the pod's namespace. But adoption of ambient mesh is still early, and most production deployments in 2026 still use the sidecar model. Red teams should test whether sidecar certificates are extractable from within the application container. If they are, the finding means that any application-level compromise also compromises the pod's mesh identity.

## Helm Chart and Deployment Manifest Manipulation

Kubernetes deployments are defined in YAML manifests, often templated through Helm charts. These manifests specify every security-relevant property of the deployment: container images, security contexts, network policies, resource limits, volume mounts, and service account bindings. Manipulating the manifest changes the deployment's security posture without changing any application code.

The attack surface is the pipeline that produces and applies manifests. Helm charts are stored in chart repositories — often the same artifact registries that store container images. If the chart repository lacks integrity verification, an attacker can modify a chart to add a privileged container, mount a host path, or change the image reference to a malicious image. The next deployment pulls the modified chart and applies it without question.

**Values injection** targets Helm's templating system. Helm charts use template variables that are populated from values files at deploy time. If the values file is stored in a Git repository with insufficient access controls, an attacker can modify values that control security-critical settings — changing the container image, disabling readiness probes that check safety filter health, elevating the security context, or adding environment variables containing credentials. The values look innocuous — they are just configuration — but they control the shape of the deployed pod.

**Admission webhook bypass** targets the Kubernetes components that validate deployments. Many organizations use admission webhooks — OPA Gatekeeper, Kyverno, or custom webhooks — to enforce pod security policies. These webhooks reject deployments that request excessive privileges, use unapproved images, or violate network policies. But admission webhooks can be circumvented. If the webhook is configured with a failure policy of "Ignore" rather than "Fail," Kubernetes applies the deployment without validation when the webhook is unavailable. An attacker who can disrupt the webhook service — through a denial-of-service attack, DNS manipulation, or certificate expiration — creates a window where any deployment is accepted, regardless of its security posture.

Red teams should test the entire manifest pipeline: the chart repository's integrity controls, the values files' access controls, the admission webhook's availability and bypass conditions, and the GitOps pipeline's verification of manifest changes. A single weak link in this chain allows an attacker to deploy arbitrary workloads with arbitrary privileges into the AI infrastructure.

## Container Image Supply Chain Attacks

The container image your model serving pod runs is the foundation of everything above it. If the image is compromised, every security control built on top of it is meaningless. The model serving framework, the safety classifiers, the output filters, the monitoring agents — all of them run within the image. A malicious image can disable any of them silently.

**Poisoned base images** are the most scalable supply chain attack. Most model serving containers are built on base images from public registries — NVIDIA's CUDA images, Python images, Ubuntu images. In August 2025, researchers found that 35 Docker Hub images, including Debian builds, still contained the XZ Utils backdoor a full year after the vulnerability was publicly disclosed. If your model serving Dockerfile starts with a base image that contains a backdoor, your model serving container inherits that backdoor.

**Malicious model dependencies** extend the supply chain attack to the ML ecosystem specifically. Researchers documented fake Alibaba Labs AI SDK packages on PyPI that contained PyTorch models with infostealer code hidden inside Pickle-format model files. The malicious code executed when the model was loaded — a standard operation in any ML pipeline. Hugging Face has also been used to host models poisoned with malicious code in Pickle format. The ML supply chain is younger, less mature, and less monitored than the traditional software supply chain, making it a higher-value target for attackers.

**Registry poisoning** targets the container registry directly. If the model serving team pulls images from a private registry, and the registry's access controls allow write access from compromised credentials, the attacker replaces a legitimate image with a malicious one. Tag-based references are especially vulnerable — replacing the image tagged "latest" or "v2.1" does not change any deployment manifest, but the next pod that pulls the image gets the malicious version. Content-addressable references using image digests prevent this attack because the digest changes when the image content changes, but many organizations still use tag-based references for convenience.

Docker responded to the growing supply chain threat by making hardened images free in December 2025 — continuously scanned, rebuilt, and verified images for common base image platforms. Organizations that adopt hardened base images reduce but do not eliminate supply chain risk. The hardened image protects the base layer, but every layer added on top — the model serving framework, the Python dependencies, the custom code — remains a potential injection point.

Red teams test the container supply chain by verifying image provenance at every stage. Can you trace every image in the cluster back to a signed, verified source? Are image digests enforced, or can tag references be overwritten? Are base images from public registries scanned for known vulnerabilities before use? Are model dependencies verified against known-good checksums? Is the build pipeline itself protected against compromise — can an attacker inject a step into the CI/CD pipeline that modifies the image before it is pushed to the registry? Every question that cannot be answered with confidence is an opening the supply chain attacker will find.

The orchestration layer is not just infrastructure. It is the control system that determines what runs, where it runs, and with what permissions. Every misconfiguration in Kubernetes, every weak link in the container supply chain, every sidecar that can be injected — these are not theoretical risks. They are the attack vectors that practitioners are exploiting in production right now.

The next subchapter closes this chapter by translating red team findings into concrete infrastructure hardening, providing the prioritization framework and defense-in-depth model that turns offensive discoveries into defensive improvements.