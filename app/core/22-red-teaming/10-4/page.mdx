# 10.4 — Red Team Composition: Skills and Perspectives Needed

In November 2025, a healthcare AI company hired an external red team to test their clinical decision support system. The team consisted of five security engineers — all former penetration testers with deep expertise in web application security, cloud infrastructure, and API exploitation. They ran attacks for three weeks and found 47 vulnerabilities: SQL injection opportunities in logging systems, misconfigured S3 buckets, exposed API endpoints, authentication bypass techniques. Every finding was legitimate. Every finding was also irrelevant to the actual risk surface. They never tested whether the model could be tricked into recommending dangerous drug combinations. They never checked if patient data could leak through prompt injection. They never explored whether the system could be manipulated to deny care to specific demographics. The red team had world-class technical skills — and exactly the wrong skills for AI red teaming. The company patched all 47 vulnerabilities and shipped the system. Six weeks into production, a journalist demonstrated prompt injection that extracted patient histories. The traditional security team had looked exactly where they knew how to look.

Diverse red teams find more vulnerabilities. The right mix of skills, backgrounds, and perspectives is not a diversity checkbox — it is the operational requirement that determines whether you find the failures that matter or the failures that are easy to find.

## Why Composition Determines Coverage

A red team can only discover the vulnerabilities it has the expertise to imagine. An engineer who has spent 15 years finding buffer overflows will instinctively look for memory safety issues. A social engineer will explore manipulation and deception. A machine learning researcher will probe training data influence and model inversion. A domain expert will understand which outputs are subtly wrong in ways that non-experts cannot detect. Each perspective unlocks a different category of vulnerability. If your red team consists entirely of one archetype, you get comprehensive coverage of one risk category and blind spots everywhere else.

The composition problem is worse for AI systems than traditional software because the attack surface is so broad. Traditional software has well-understood categories: authentication, authorization, input validation, session management, cryptography. AI systems add model behavior, training data influence, prompt manipulation, output verification, context management, retrieval poisoning, fine-tuning security, and emergent capability risks. No single person has deep expertise across all of these. A diverse team with complementary skills can cover the surface. A homogeneous team will do excellent work in their comfort zone and miss entire threat classes.

The healthcare company's red team was not incompetent. They were experts in their domain. But their domain was traditional application security, and the system they were testing was an AI application where the most dangerous vulnerabilities were not in the authentication layer or the database — they were in the model's behavior under adversarial input. The team did not know how to attack language models because they had never needed to learn. The composition failure guaranteed the outcome before the engagement started.

## Technical Skills Required

Every AI red team needs at least one person with deep machine learning expertise. Not surface-level familiarity — the kind of understanding that comes from training models, debugging gradient collapse, analyzing loss curves, and reading adversarial ML papers. This person understands how models learn, where they memorize, how attention mechanisms work, what fine-tuning changes, and how embeddings encode information. They can craft adversarial examples, explore latent space manipulation, and recognize when a model's behavior indicates a structural vulnerability rather than a prompt engineering trick.

Every team needs at least one person with prompt engineering and LLM exploitation expertise. This person has spent hundreds of hours probing models, finding jailbreaks, developing injection techniques, mapping model personalities, and understanding how instruction-following breaks down. They know the difference between a shallow jailbreak that works once and a deep vulnerability that persists across mitigations. They can chain techniques, layer obfuscation, and craft payloads that evade detection. They think like an attacker who wants to control the model's behavior, not a researcher exploring its capabilities.

Every team needs traditional application security skills — but targeted ones. Someone who understands API security, authentication bypass, cloud infrastructure exploitation, and secrets management. AI systems are still software systems. They run on servers, expose APIs, manage credentials, and store data. Prompt injection matters — but so does an exposed model endpoint with no authentication. The traditional security person on the team prevents the red team from becoming so focused on novel AI attacks that they miss the fundamentals.

Every team needs someone who understands the specific domain the AI system operates in. For healthcare AI, this means clinical expertise — someone who knows what a dangerous drug interaction looks like, what constitutes a plausible but incorrect diagnosis, and which edge cases matter to patient safety. For financial AI, this means regulatory knowledge and fraud patterns. For legal AI, this means understanding what bad legal advice causes harm versus what is merely incorrect. Domain expertise is what turns "the model said something wrong" into "the model recommended a treatment contraindicated for patients with this specific condition, which occurs in 8% of the target population." Severity classification requires domain context.

## Diversity of Perspectives

Gender, cultural background, and linguistic diversity are not just fairness considerations — they are operational necessities for finding vulnerabilities that homogeneous teams miss. A red team composed entirely of English-speaking American engineers will systematically undertest multilingual behavior, cultural assumptions, and internationalization failures. An all-male team testing a system that interacts with users about sensitive health topics will miss failure modes that women on the team would immediately recognize. A team with no members from minority demographics will struggle to identify bias patterns that disproportionately harm underrepresented groups.

This is not speculation. In 2024, a major AI company ran internal red teaming on a content moderation system. The team was predominantly male. They tested hate speech detection, misinformation, and graphic content. The system passed. When the company brought in external testers with more diverse backgrounds, vulnerabilities emerged within hours: the model flagged discussions of menstruation as sexual content, misclassified posts in African American Vernacular English as low-quality spam, and failed to detect coded misogyny that women on the team recognized immediately. The original red team had done thorough work — within the threat model they could imagine. The diverse team expanded the threat model.

Cultural context matters for deception detection, bias evaluation, and adversarial prompt testing. A prompt that seems neutral in one cultural context can carry hostile intent in another. A joke that is harmless in one language can be offensive when translated. A system designed for American users behaves unpredictably with international inputs. If no one on your red team speaks the languages your system supports, you cannot adequately test cross-lingual attacks. If no one on your team shares the cultural background of your user base, you will miss context-dependent failures.

The goal is not representation for its own sake. The goal is ensuring that the red team's collective experience covers the attack surface your system actually faces. If your users are global, your red team should include global perspectives. If your system handles sensitive topics across demographics, your team should reflect those demographics. If your model supports 15 languages, at least some of your red teamers should be native speakers of those languages.

## Internal vs External Team Members

Internal red teamers have deep context. They know the system architecture, the design decisions, the previous incidents, the team dynamics, and the organizational pressures. This context enables them to craft attacks that target known weak points, test edge cases that the product team worried about, and understand why certain mitigations exist. Internal red teamers can start testing immediately without ramp-up time. They can have hallway conversations with engineers to clarify findings. They can attend design reviews and identify vulnerabilities before code is written. Internal teams are continuous — they test every release, accumulate knowledge over time, and build institutional memory.

External red teamers have fresh eyes. They approach the system without assumptions, without knowledge of what "should" work, and without political pressure to avoid findings that make internal teams look bad. They bring techniques from other organizations, other industries, and other problem domains. They are not invested in the success of any particular design decision. They can say "this entire approach is flawed" without worrying about offending the team that built it. External teams are episodic — they parachute in, run intensive testing, deliver findings, and leave. This creates urgency and focus that continuous internal testing sometimes lacks.

The best red team programs use both. Internal red teamers test continuously, catch regressions, validate fixes, and provide rapid feedback during development. External red teamers test before major releases, after significant architecture changes, and annually as an independent validation. External teams also serve as calibration — if your internal red team is not finding issues and then an external team finds 40 vulnerabilities in two weeks, your internal team is either under-resourced or too close to the product to see clearly.

A financial services company runs this model effectively. They have three full-time internal AI red teamers embedded with the product teams. These red teamers test every model update, participate in design reviews, and maintain a library of attack scenarios. Twice a year, the company brings in external red teamers for intensive two-week engagements. The external teams consistently find issues the internal team missed — not because the internal team is incompetent, but because external perspective reveals blind spots. The internal team learns from external findings and incorporates new techniques into their continuous testing. The loop improves both teams over time.

## Rotating Team Membership

Red teams get stale. After six months of testing the same system, red teamers develop patterns. They know which attacks work, which defenses are strong, and where the team has already looked. This accumulated knowledge is valuable — but it also creates blind spots. The red teamer who has tested prompt injection 200 times might miss a novel retrieval poisoning attack because their mental model defaults to prompt-based threats. Rotating team membership brings in new attackers who have not yet formed habits.

Rotation does not mean replacing the entire team. It means bringing in new members while retaining some continuity. A common pattern: maintain two core red teamers who know the system deeply, and rotate in two additional red teamers every quarter. The core members provide context and continuity. The rotating members provide fresh techniques and perspectives. After three months, two rotating members leave and two new ones join. The core members can brief the new joiners on previous findings, known defenses, and areas that have been thoroughly tested — which helps the new members focus on unexplored attack surfaces.

Rotation also exposes your red team to external techniques. A red teamer who spends three months testing your healthcare AI system and then rotates to test a financial AI system learns financial-specific attacks — and brings those techniques back when they return. Cross-pollination of techniques across domains accelerates discovery. The prompt injection variant that worked against a banking chatbot might also work against your medical chatbot. The retrieval poisoning technique developed for legal AI might apply to your customer support AI. Red teamers who only ever test one system become specialists in that system but lose exposure to the broader adversarial ML landscape.

Some organizations rotate in academics, independent researchers, or open-source contributors for short stints. A university researcher who studies adversarial examples might join your red team for a one-month engagement, bringing cutting-edge techniques from the academic literature. An open-source security researcher might contribute to your red team part-time, bringing techniques learned from testing other organizations' systems. These rotations are episodic and limited — but they inject new ideas that internal teams would not encounter otherwise.

## Building Complementary Teams

The worst red team composition is five people with identical skills. The best red team composition is five people whose skills barely overlap. One person with ML expertise, one with LLM exploitation skills, one with domain knowledge, one with traditional security skills, and one with human factors expertise — this team can cover more attack surface than ten people with the same background.

Complementary skills also mean complementary thinking styles. Some red teamers are systematic — they enumerate attack surfaces, build checklists, and work methodically through every scenario. Others are creative — they free-associate, chain techniques, and explore unusual combinations. Some are technical — they reverse-engineer model behavior and analyze embeddings. Others are social — they think like manipulators and craft deception-based attacks. Some focus on depth — they spend three days perfecting one exploit. Others focus on breadth — they find ten shallow vulnerabilities in a day. A team with all of these cognitive styles will outperform a team where everyone thinks the same way.

You can assess complementarity during hiring. When interviewing red team candidates, ask them to describe their last three vulnerability discoveries. Listen for cognitive patterns. Did they find the vulnerability through systematic enumeration, creative exploration, or domain expertise? Did they go deep on one attack vector or survey many? Did they focus on technical exploitation or human manipulation? If your team already has three systematic thinkers, hire the creative explorer. If your team has four technical specialists, hire the social engineer.

Complementary teams also disagree more — and that is a feature, not a bug. When one red teamer says "this vulnerability is critical" and another says "this is low severity," the debate forces both to articulate their reasoning. Those debates surface assumptions, clarify threat models, and prevent groupthink. A team that always agrees is a team that has converged on a single perspective — which means they have blind spots. A team that argues is a team that has not yet stopped discovering.

## When Composition Fails

The healthcare company that hired the traditional security team made a composition error. They needed ML expertise and prompt engineering skills — and they hired infrastructure penetration testers. The failure was visible in the first week of the engagement, but the company did not recognize it. The red team delivered daily status updates highlighting infrastructure findings, and the company interpreted high finding volume as successful testing. Only after the journalist's public demonstration did the company realize that high finding count in the wrong category is worse than low finding count in the right category. It creates false confidence.

Composition failures are fixable mid-engagement if you recognize them. If your red team has been testing for two weeks and every finding is in the same category, that is a signal. If you hired a team to test AI-specific risks and all their findings are traditional web vulnerabilities, add someone with AI expertise immediately. If you hired ML researchers and all their findings are about model behavior but none about infrastructure security, add a traditional security expert. Do not wait until the engagement ends to discover that you tested the wrong surface.

The fix for composition failures is honesty about skill gaps. Most red teamers know the boundaries of their expertise. If you ask a traditional security engineer "are you confident we have adequately tested prompt injection?" they will usually say no — if they trust that honesty will not be punished. The problem is that organizations often hire red teams and then treat them as black boxes. They say "test our system" and expect comprehensive results without specifying what "comprehensive" means. A better approach: define the attack surface categories you need tested, verify that the red team composition covers those categories, and if gaps exist, acknowledge them and bring in additional expertise rather than pretending the gaps do not exist.

Red team composition is the leverage point that determines everything downstream. Get composition right and the team finds vulnerabilities that matter. Get it wrong and the team works hard, delivers findings, and leaves the system vulnerable to the attacks they were never equipped to discover.

Next, we will examine how red teams develop attack scenarios — the structured creativity that turns skills and perspectives into actionable tests.
