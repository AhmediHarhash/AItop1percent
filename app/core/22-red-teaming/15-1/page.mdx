# 15.1 — Why the Control Plane Is the Real Target

The model is the payload. The infrastructure is the delivery mechanism. And every experienced attacker knows: you do not attack the payload when you can compromise the delivery mechanism instead. A successful prompt injection affects one session. A compromised deployment pipeline affects every user, every session, every interaction — silently, persistently, and at a scale that no prompt-level attack can match. Yet most AI red teaming programs spend ninety percent of their effort testing the model and ten percent testing the infrastructure that surrounds it. This is exactly the asymmetry that attackers exploit.

## The Infrastructure Advantage for Attackers

To understand why the control plane is the preferred target, you need to think about what an attacker actually wants. They want persistence — the ability to maintain access after the initial compromise. They want scale — affecting many users, not one. They want stealth — avoiding detection for as long as possible. They want leverage — turning a small foothold into a large impact. The AI model itself offers none of these properties. A prompt injection lasts for one conversation. It affects one user. It is visible in logs if anyone looks. And its impact is limited to what that single session can access.

The control plane offers all four. Compromise a deployment pipeline, and your malicious code runs every time the system deploys. Swap model weights in a registry, and every instance serves your backdoored model. Modify a prompt template in the management system, and every user receives your altered instructions. Steal API keys from a secrets vault, and you have persistent access to every model endpoint. These attacks persist across sessions, affect all users simultaneously, and are difficult to detect because the compromised component looks identical to the legitimate one from the outside.

Traditional infrastructure attackers have understood this for decades. In web application security, the most devastating breaches rarely come from exploiting the application directly. They come from compromising the build pipeline, poisoning dependencies, stealing cloud credentials, or tampering with deployment artifacts. The SolarWinds attack in 2020 was not an application exploit — it was a build system compromise that affected 18,000 organizations through a single pipeline. The same logic applies to AI systems. The model is the application. The infrastructure around it is the build system. And the build system is where the real damage happens.

## Control Plane vs Data Plane in AI Systems

In network architecture, the control plane manages how traffic flows while the data plane carries the traffic itself. AI systems have an analogous distinction. The **data plane** is where inference happens — user queries come in, the model processes them, responses go out. This is what most AI red teaming tests. Can you inject prompts? Can you extract data? Can you jailbreak the model? These are data plane attacks.

The **control plane** is everything that determines how the data plane behaves. It includes the model registry that stores and serves model weights. The prompt management system that defines what instructions the model follows. The configuration system that sets parameters like temperature, max tokens, and safety thresholds. The deployment pipeline that builds, tests, and ships model updates. The secrets management system that stores API keys, database credentials, and authentication tokens. The monitoring and observability stack that collects logs, metrics, and traces. The access control layer that determines who can modify which components.

When you compromise the data plane, you affect one session. When you compromise the control plane, you affect the system. That is the fundamental difference, and it is the reason experienced attackers always look for control plane access first. A red teaming program that ignores the control plane is testing the walls while leaving the foundation unexamined.

## What the AI Control Plane Looks Like in Practice

In a typical production AI deployment in 2026, the control plane has at least a dozen distinct components. The model registry — often MLflow, Weights and Biases, Amazon SageMaker Model Registry, or a custom system built on cloud storage — stores trained model weights and metadata. The prompt management system — PromptLayer, LangSmith, Braintrust, Maxim, or a homegrown database — stores system prompts, few-shot examples, and template variables. The CI/CD pipeline — GitHub Actions, GitLab CI, Jenkins, or a managed service — automates training, evaluation, and deployment. The secrets manager — HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, or environment variables in a container — stores API keys for model providers, database credentials, and service tokens. The configuration system — feature flags, environment variables, remote config services — controls runtime behavior like model routing, fallback logic, and safety thresholds. The orchestration layer — Kubernetes, Docker Compose, or a serverless platform — manages the compute infrastructure. And the monitoring stack — Datadog, Grafana, custom dashboards — collects the telemetry that tells you whether the system is healthy.

Each of these components has its own access control, its own attack surface, and its own failure modes. Each one, if compromised, gives the attacker a different kind of leverage. And most AI teams have never red-teamed any of them. They test the model. They assume the infrastructure is someone else's problem. It is not.

## The Asymmetry of Impact

Consider two attacks. In the first, an attacker discovers a prompt injection that causes the model to reveal system instructions when a user types a specific phrase. The attacker can extract the system prompt, which might contain proprietary logic, customer data references, or tool configurations. This is a real vulnerability. But it affects one session at a time. The attacker must interact with the system directly. The exposure is limited to what that single session can access.

In the second attack, the attacker gains write access to the prompt management system — perhaps through a stolen developer credential, a misconfigured access policy, or an unpatched API endpoint. The attacker modifies the system prompt to include a data exfiltration instruction: whenever a user mentions certain keywords, the model appends a hidden tag that sends the conversation context to an external endpoint through a tool call. This change is deployed through the normal promotion process. It affects every user. It persists until someone notices. It does not require the attacker to interact with the model at all after the initial compromise. The blast radius is orders of magnitude larger.

This asymmetry is not hypothetical. It is the reason supply chain attacks have become the dominant threat vector in software security over the past five years. The Sonatype 2026 Software Supply Chain Report identified over 454,600 new malicious packages in 2025 alone, bringing the cumulative total to over 1.2 million across npm, PyPI, Maven, NuGet, and Hugging Face. Attackers have learned that compromising infrastructure scales. AI infrastructure is the newest and least-defended frontier.

## Why AI Teams Miss This

AI security teams tend to be staffed by machine learning engineers who understand model behavior deeply but have limited experience with infrastructure security. They know how transformers work, how attention mechanisms can be manipulated, how few-shot examples bias model outputs. They do not know how CI/CD pipelines can be compromised, how container escapes work, how IAM policies can be misconfigured, or how secrets management systems fail. The result is red teaming programs that test the model thoroughly and ignore the infrastructure entirely.

Traditional security teams, on the other hand, know infrastructure deeply. They know how to test pipelines, audit access controls, scan for leaked credentials, and validate container security. But they often do not understand what makes AI infrastructure different. They audit the Kubernetes cluster but do not check whether the model registry has integrity verification. They scan for exposed API keys but do not realize that a Hugging Face token grants access to a thousand private models. They test network segmentation but do not consider that GPU memory is shared across tenants and may contain fragments of previous inference requests.

The gap between these two teams — the AI security team that understands models but not infrastructure, and the infrastructure security team that understands pipelines but not AI — is exactly where control plane vulnerabilities live. Red teaming the AI control plane requires both skill sets. You need someone who understands how a CI/CD pipeline works and why swapping model weights during deployment is catastrophic. You need someone who knows what a prompt registry is and why it is the single highest-leverage target in the entire system.

## The Red Teamer's Priority List

When you red team AI infrastructure, you prioritize by blast radius and stealth, not by technical complexity. The easiest attack to execute is not always the most damaging. The most damaging attack is not always the hardest to detect. Your priority list should reflect what an attacker with limited time and access would target first.

Prompt management systems rank highest because they control model behavior for every user and changes are often lightly audited. Model registries rank second because replacing weights affects all inference and weight-level changes are invisible without cryptographic verification. CI/CD pipelines rank third because they offer persistent access through build-time modifications. Secrets management ranks fourth because leaked credentials provide lateral movement into every connected system. Configuration systems rank fifth because subtle parameter changes — adjusting safety thresholds, changing model routing rules, modifying fallback behavior — can degrade the system without triggering alerts. Developer environments rank sixth because they often contain production data and have weaker controls than production.

This is not a fixed hierarchy. Your specific architecture determines which components are most exposed. But the principle holds: test the components that control the model before you test the model itself. The model does what the infrastructure tells it to do. Control the infrastructure and you control the model.

## What This Chapter Covers

The remaining subchapters in this chapter walk through each component of the AI control plane, one at a time. You will learn how CI/CD pipelines are compromised to poison the build process. How model registries are attacked to swap weights and checkpoints. How prompt management systems are manipulated to change instructions at the source. How secrets management fails, how feature flags drift, how developer environments leak, how shadow AI systems evade governance. How GPU infrastructure is exploited through side channels and tenant isolation failures. How network-level attacks target AI endpoints. How container orchestration introduces AI-specific vulnerabilities. And finally, what hardening actually works — the controls that reduce control plane risk from catastrophic to manageable.

Each subchapter follows the same structure: how the attack works, why it matters specifically for AI systems, how to detect it, how to prevent it, and what breaks if you ignore it. The attacks are real. The defenses are proven. The gap between the two is where your red teaming program should be spending its time.

The next subchapter starts with the most common infrastructure target: the CI/CD pipeline that builds and deploys your AI system.
