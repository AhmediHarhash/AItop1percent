# Chapter 16 — Detection Evasion and Stealth Techniques

Every attack technique in the previous chapters assumes the attacker operates freely. In reality, production AI systems in 2026 have monitoring, safety classifiers, content filters, audit logs, anomaly detection pipelines, and human reviewers. A sophisticated attacker does not just find vulnerabilities. They exploit those vulnerabilities without triggering a single alert. A mature red team does not just demonstrate that a system can be compromised. They demonstrate that the compromise can happen invisibly, that the monitoring you depend on does not catch the exploitation, and that the evidence trail you trust can be corrupted or erased.

Detection evasion is not about helping attackers. It is about understanding how attackers think so your defenses actually work. If your red team only tests attacks that your monitoring easily catches, you learn nothing about the threats that matter. The attacks you catch are not the ones that hurt you. The attacks you miss are. This chapter covers the evasion techniques that real adversaries use against AI monitoring — log manipulation, token-level obfuscation, Unicode tricks, multi-language exploits, slow-burn campaigns, rate-based evasion, output classifier bypasses, insider threats, and the exploitation of human review itself. For every evasion technique, you will learn both how the attacker executes it and how the blue team detects or prevents it. Because in the evasion-detection arms race, the only losing strategy is ignorance.

---

- 16.1 — Why Detection Evasion Matters for Red Teams and Blue Teams
- 16.2 — Log Manipulation and Audit Trail Attacks
- 16.3 — Token-Level Obfuscation to Bypass Safety Monitors
- 16.4 — Unicode, Encoding, and Homoglyph Evasion
- 16.5 — Multi-Language Evasion — Exploiting Uneven Safety Training
- 16.6 — Slow-Burn Attacks Below Detection Thresholds
- 16.7 — Rate-Based Evasion and Low-and-Slow Prompt Shaping
- 16.8 — Evading Content Filters and Output Classifiers
- 16.9 — Insider Threat Modeling for AI Systems — Prompt Engineers, Data Poisoners, Rogue Operators
- 16.10 — Human Review Exploitation — Attacking the Oversight Layer
- 16.11 — Building Detection Rules That Survive Evasion
- 16.12 — The Evasion-Detection Arms Race — Why Neither Side Wins Permanently

---

*The attacker you detect is the attacker who failed. This chapter teaches you to find the ones who succeed.*
