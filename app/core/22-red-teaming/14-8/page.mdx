# 14.8 — Impact Escalation Modeling — From Prompt to Business Damage

"The model can be jailbroken." This is what the red team report says. It sits in the CISO's inbox alongside four hundred other findings from the quarterly security assessment. The CISO reads the finding, notes the "medium" severity rating, and asks the team to add it to the backlog. Six months later, the same jailbreak technique is used to extract thirty thousand customer records, triggering GDPR notification requirements, an investigation by the Irish Data Protection Commission, a class-action lawsuit, and board-level conversations about the CISO's continued employment. The technical finding was accurate. The impact assessment was not.

The gap between technical findings and business consequences is where most red team programs lose their influence. Security teams speak in vulnerabilities. Executives speak in dollars, customers, and regulatory deadlines. When a red team report says "prompt injection: medium severity" and an executive reads "probably not urgent," the translation failure is not the executive's fault. It is the red team's fault for presenting a technical observation without a business impact model.

## Why Technical Severity Scores Fail for AI Vulnerabilities

Traditional vulnerability scoring — CVSS in cybersecurity, severity matrices in application security — assigns a score based on the technical characteristics of the vulnerability: attack complexity, required privileges, user interaction, and the scope of impact on confidentiality, integrity, and availability. This works reasonably well for software vulnerabilities where the blast radius is predictable. A remote code execution vulnerability in a web server has a calculable impact based on what the server can access.

AI vulnerabilities break this model. The impact of a prompt injection is not determined by the injection technique. It is determined by what the model can access, what tools it can call, what data flows through its context, and what actions it can take on behalf of the attacker. The same prompt injection technique — the exact same words — has zero impact against a model with no tool access and critical impact against a model connected to a customer database, a payment processor, and an email service. The technique is the same. The impact is entirely determined by the environment.

This is why AI red team findings rated by technique severity are systematically misleading. A "medium" prompt injection against a high-privilege model is more dangerous than a "critical" jailbreak against a low-privilege model. Severity must be assessed based on the full kill chain — the technique, the environment, the accessible data, the available tools, and the downstream consequences — not based on the technique alone.

## The Translation Framework — Technical Finding to Business Impact

Every red team finding should be accompanied by an impact model that translates the technical observation into business language. The translation follows four steps.

First, establish the access gained. What can the attacker do after the exploit succeeds? Not what the technique does in isolation, but what the attacker gains in the context of the specific system. "Prompt injection succeeds" becomes "attacker gains the ability to make arbitrary tool calls using the model's system-level permissions, including database queries, CRM updates, and email composition."

Second, determine the data at risk. What information can the attacker reach through the access they have gained? Map every data source accessible through the model's tools and RAG system. "Arbitrary database queries" becomes "access to 4.2 million customer records including names, email addresses, phone numbers, account balances, and transaction history across all product lines."

Third, calculate the regulatory exposure. Which regulations apply to the data at risk, and what are the penalties for a breach of that data? For the 4.2 million customer records example: GDPR Article 83 allows fines up to four percent of annual global turnover or twenty million euros, whichever is higher. If those customers include EU residents — and for any multinational company, they do — the regulatory exposure is calculable. If health data is involved, HIPAA penalties apply. If financial data is involved, SOX and state-level financial privacy regulations apply. The numbers get specific fast.

Fourth, model the business impact. Beyond regulatory fines, what are the second and third-order consequences? Customer notification costs typically run five to ten dollars per affected individual for a major breach — for 4.2 million records, that is twenty to forty million dollars in notification alone. Legal defense costs. Customer churn driven by breach publicity. Loss of enterprise contracts that require clean security postures. Stock price impact for public companies. The total cost of a breach through an AI system can exceed the cost of the AI program itself by orders of magnitude.

The output of this framework transforms "prompt injection: medium severity" into "prompt injection enables unauthorized access to 4.2 million customer records, creating estimated regulatory exposure of fifteen to forty million dollars, notification costs of twenty to forty million dollars, and projected customer churn valued at eight to twelve million dollars annually." The executive who reads the second statement does not add it to the backlog.

## Impact Categories — A Taxonomy for AI-Specific Damage

AI-specific attacks create impacts that map to six primary categories. Each requires its own assessment methodology and its own communication approach.

**Data breach** is the most quantifiable category. The attacker extracts data they should not have. The impact depends on the volume, sensitivity, and regulatory classification of the data. Personal data triggers privacy regulations. Health data triggers HIPAA. Financial data triggers financial privacy laws. Proprietary business data creates competitive harm. The key metric is the number of records exposed and their sensitivity classification. Most organizations already have breach cost models from their traditional security programs — the AI red team's job is to demonstrate that the AI system creates a path to that breach.

**Financial loss** extends beyond breach costs to include direct monetary damage. An attacker who manipulates a trading bot, alters pricing logic, redirects payments, or approves fraudulent transactions through an AI system causes direct financial harm. The 2024 Air Canada chatbot lawsuit — where a customer was awarded a refund based on a chatbot's incorrect policy statement — demonstrated that AI-generated outputs create financial obligations even when the output contradicts the company's actual policies. In 2026, with AI systems executing financial transactions, managing procurement, and handling billing disputes, the financial loss from a compromised AI system can be immediate and substantial.

**Regulatory penalty** has its own assessment model. Under the EU AI Act, which entered GPAI enforcement in August 2025 with systemic risk compliance deadlines in August 2026, AI systems classified as high-risk face specific obligations around risk management, data governance, and transparency. A red team finding that demonstrates a violation of these obligations is not just a technical vulnerability — it is evidence of non-compliance that could trigger enforcement action. The GPAI Code of Practice, published in July 2025, and the Q&A guidance from September 2025 provide specific criteria that red team findings can be mapped against.

**Reputational damage** is the hardest to quantify but often the most expensive. When a customer-facing AI system produces harmful outputs — discriminatory responses, factually wrong answers that cause harm, or leaked private information — the reputational cost is driven by media coverage, social media virality, and the speed of the company's response. The Samsung ChatGPT data leak in 2023, where employees inadvertently exposed proprietary code through a public AI service, created reputational damage that influenced corporate AI policies across the entire industry. A red team finding that demonstrates a path to public embarrassment commands more executive attention than one that demonstrates a technical vulnerability with no public-facing component.

**Operational disruption** covers attacks that degrade or disable the AI system's ability to function. A denial-of-service attack through resource exhaustion — sending queries designed to maximize token consumption, trigger expensive tool calls, or exhaust rate limits — can take an AI system offline. If that system handles customer support, order processing, or internal workflows, the downtime has a calculable cost per minute. For companies that have migrated critical operations to AI-powered systems, operational disruption of the AI is operational disruption of the business.

**Cascading harm** is the category unique to AI systems. A compromised AI that sends misleading information to customers, generates false reports for internal decision-makers, or takes autonomous actions based on manipulated inputs creates harm that propagates through the organization and its customer base. An AI financial advisor that gives bad investment advice to ten thousand customers simultaneously creates cascading harm that no human employee could replicate at that scale. The impact model must account for the AI's reach — the number of users, decisions, or transactions it influences — when assessing the potential scope of cascading harm.

## Severity Frameworks for AI-Specific Vulnerabilities

A severity framework for AI vulnerabilities must account for both the technical exploitation path and the environmental context. The framework used by mature red teams in 2026 assesses five dimensions.

Exploitability: how difficult is the attack to execute? A prompt injection that works on the first attempt with zero technical knowledge is high exploitability. An attack that requires weeks of reconnaissance, a specific model version, and a narrow timing window is low exploitability. This dimension captures the attacker's required skill and effort.

Access scope: what does the attacker gain? Read access to public data is minimal scope. Read and write access to customer databases, financial systems, and communication channels is maximum scope. This dimension captures the breadth of the attacker's reach after exploitation.

Data sensitivity: what is the classification of accessible data? Public marketing content is low sensitivity. Customer PII, health records, financial data, or trade secrets are high sensitivity. This dimension captures the regulatory and competitive value of the data at risk.

Blast radius: how many users, tenants, or systems are affected? A vulnerability that affects one user's session is narrow blast radius. A vulnerability that crosses tenant boundaries and affects every customer on the platform is maximum blast radius. This dimension captures the scale of potential damage.

Reversibility: can the damage be undone? A leaked system prompt can be rotated. Exfiltrated customer data cannot be un-exfiltrated. Modified financial records can be corrected but the downstream decisions made from those records cannot be reversed. This dimension captures whether remediation can restore the pre-attack state or only prevent further damage.

The composite severity score combines all five dimensions. A low-exploitability attack with maximum access scope, high data sensitivity, platform-wide blast radius, and irreversible damage is critical — even if the individual technique seems minor. A high-exploitability attack with minimal access scope, low data sensitivity, single-user blast radius, and fully reversible damage is low — even if the technique is novel and attention-grabbing.

## Presenting Impact Models to Executives and Boards

The red team report that changes organizational behavior is not the one with the most findings. It is the one that translates findings into the language of business risk. Executives do not respond to vulnerability counts. They respond to financial exposure, regulatory deadlines, and competitive risk.

Structure the executive presentation around three questions. First: what can an attacker do to us through our AI systems? Answer this with the kill chain narrative — the full progression from initial access to business impact, told as a concrete scenario, not as a list of techniques. Second: what does it cost us if they succeed? Answer this with the impact model — regulatory exposure in dollars, notification costs in dollars, projected customer churn in dollars, operational disruption in dollars per hour. Third: what does it cost us to fix it? Answer this with a prioritized remediation roadmap that shows the investment required to break the kill chain at each phase.

The most effective executive presentations include a comparison: the cost of remediation versus the cost of a breach. When the remediation costs two hundred thousand dollars and the breach exposure is thirty million, the decision is not complex. When the remediation costs five million dollars and the breach exposure is eight million, the decision requires judgment — and that judgment is exactly what executives are paid to exercise. Give them the numbers. Let them decide.

Board-level presentations require an additional layer: competitive and regulatory context. How are peer companies handling these risks? What enforcement actions have regulators taken against similar organizations? What is the timeline for new compliance obligations? The EU AI Act's systemic risk provisions, with their August 2026 compliance deadline, create a regulatory forcing function that transforms red team findings from "good to fix" into "must fix by a date."

## The Impact Model as a Living Document

Impact models are not static. They change as the AI system evolves, as the regulatory landscape shifts, as the threat environment matures, and as the organization's reliance on AI systems deepens. A model that handled fifty queries per day when first deployed handles fifty thousand per day eighteen months later — and the blast radius of every vulnerability has grown by three orders of magnitude.

Red teams should update impact models quarterly, incorporating new regulatory guidance, new threat intelligence, changes to the AI system's tool integrations and data access, and lessons from actual incidents in the industry. The impact model is not a one-time deliverable. It is the translation layer that keeps technical security work connected to business decision-making.

When the impact model is current, specific, and credible, it transforms the red team from a group that finds problems into a group that drives investment. The technical findings provide the evidence. The impact model provides the urgency. Together, they create the business case that moves AI security from the backlog to the roadmap.

The kill chain model establishes what the attacker does. The impact model establishes what it costs. The next subchapter addresses the attacker's motivation and monetization strategy — how adversaries weaponize compromised AI systems for profit, examining the economics that drive the threat landscape.
