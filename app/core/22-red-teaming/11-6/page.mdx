# 11.6 — Coverage Metrics for Adversarial Testing

You cannot improve what you cannot measure. This is as true for security testing as it is for product metrics, system performance, or financial results. Most teams test adversarially without measuring coverage. They run red team exercises, discover some attacks, patch them, and call it done. They have no idea what percentage of their attack surface they tested. They do not know if they explored jailbreaks but ignored data extraction. They do not know if they tested their summarization capability but skipped code generation. They are testing blind.

Coverage metrics make testing visible. They tell you what you have tested and what you have missed. They turn subjective confidence into quantified evidence. They let you allocate red team effort efficiently, focusing on gaps instead of redundantly testing the same vectors. They provide a release gate — you can say "we will not ship until we have tested eighty percent of attack vectors" instead of "we tested for a week and found some stuff." Coverage metrics are the foundation of rigorous adversarial evaluation.

## Why Coverage Matters

Without coverage metrics, testing effort concentrates on what is easy or familiar. Your red team likes finding jailbreaks, so they test jailbreaks repeatedly. They discover twenty variations of the same prompt injection attack. Meanwhile, nobody tests goal hijacking, tool misuse, or multi-turn manipulation. The attack surface is uneven. The gaps are invisible. When a production incident occurs in an untested area, the team is surprised. They should not be. They never measured whether they tested that area at all.

Coverage metrics force breadth. When you measure coverage by attack vector and see that jailbreaks have ninety percent coverage but data extraction has fifteen percent coverage, the gap becomes a priority. You reallocate effort. You write more data extraction tests. You run targeted red team exercises. Over time, coverage becomes balanced. This does not happen without measurement.

Coverage metrics also enable trend tracking. After each test cycle, you measure coverage. Over time, you see whether coverage is improving or stagnating. If coverage plateaus at sixty percent despite adding more tests, you have a structural problem — your test generation strategy is not reaching the remaining forty percent. You need new techniques, new red teamers, or new mutation operators. Without trend tracking, you would never notice the plateau.

The third benefit is release confidence. When you ship a new model or a major system change, you need evidence that you have tested it thoroughly. "We ran some red team exercises" is not evidence. "We achieved seventy-five percent coverage across eight attack categories and ninety percent coverage on critical capabilities" is evidence. Coverage metrics turn subjective assurance into quantified risk assessment.

## Attack Vector Coverage

Attack vector coverage measures what percentage of known attack types you have tested. Start by defining a taxonomy of attack vectors. This might include prompt injection, jailbreaking, refusal bypass, goal hijacking, data exfiltration, context manipulation, tool misuse, multi-turn attacks, and adversarial inputs. The taxonomy should be exhaustive — every attack you care about should map to at least one category.

For each category, define test cases. Prompt injection might have test cases covering direct injection, indirect injection via tool outputs, injection through user-provided data, and injection through conversation history. Jailbreaking might include roleplay attacks, hypothetical framing, fictional context, and character simulation. Each test case represents a specific attack pattern.

Coverage is the percentage of test cases you have attempted. If your prompt injection category has forty test cases and you have run thirty, coverage is seventy-five percent. If your goal hijacking category has twenty test cases and you have run five, coverage is twenty-five percent. Aggregate across all categories to get overall attack vector coverage.

This metric has a key limitation: it measures breadth, not depth. You might test one example of each attack type and claim complete coverage, but that does not mean you tested the attack type thoroughly. A single jailbreak test does not cover all jailbreak variations. To address this, weight test cases by importance or use subcategories. High-severity attack types require more test cases for full coverage.

One enterprise team defines sixty attack vectors across nine categories. Each vector has a target test count — high-severity vectors require at least twenty test cases, medium severity requires ten, low severity requires three. They track coverage as the percentage of required tests completed. Every sprint, they review coverage by category. Any category below fifty percent coverage gets prioritized in the next red team exercise. Over six months, they increased overall coverage from thirty percent to seventy-eight percent.

## Capability Coverage

Your system has multiple capabilities. A customer service chatbot might summarize conversations, answer FAQs, escalate tickets, look up order status, and process refunds. An assistant might draft emails, search documents, schedule meetings, and generate code. Each capability has its own risk surface. Testing one capability thoroughly while ignoring another leaves blind spots.

Capability coverage measures what percentage of your system's capabilities have been tested adversarially. First, enumerate all capabilities. This is usually straightforward — your system design documents or tool definitions list what the system can do. Second, for each capability, define adversarial test coverage criteria. What does it mean to adequately test a capability?

For a document search capability, adequate testing might include attempts to access unauthorized documents, attempts to extract sensitive data through carefully crafted queries, attempts to manipulate search results, and attempts to use search as a side channel to infer information about documents you cannot access directly. For a code generation capability, adequate testing might include attempts to generate malicious code, attempts to exfiltrate data through generated code, attempts to introduce vulnerabilities, and attempts to bypass content policies through code comments or variable names.

Track capability coverage as binary — either a capability has been tested according to its criteria, or it has not — or as a percentage if you define multiple test dimensions per capability. Aggregate across all capabilities to get system-wide capability coverage.

Capability coverage often reveals surprising gaps. One team discovered they had extensively tested their summarization and Q&A capabilities but had never adversarially tested their calendar integration. The calendar feature was lower risk, so it got less attention. But calendar access revealed user schedules, meeting attendees, and private event details. An attacker who could manipulate calendar queries could exfiltrate sensitive personal information. The gap was invisible until they measured capability coverage.

Another team found that capabilities involving external APIs had much lower coverage than capabilities that only used the LLM. The API capabilities were harder to test because they required mocking external services or using sandbox environments. The testing friction led to avoidance. Measuring capability coverage made the gap explicit and forced the team to invest in better test infrastructure.

## Policy Coverage

If your system enforces policies — content policies, business rules, compliance requirements — you need to measure whether those policies have been adversarially tested. Policy coverage tracks what percentage of your policies have been subjected to attempts to bypass or violate them.

Start by enumerating policies. A content policy might prohibit illegal advice, violent content, sexual content, hate speech, and misinformation. A business policy might prohibit unauthorized discounts, off-brand messaging, or commitments outside your service scope. A compliance policy might require audit logging, data retention rules, or age verification. Each policy is a rule the system should enforce even under adversarial pressure.

For each policy, define bypass test cases. An illegal advice policy should be tested with direct requests, indirect requests, hypothetical framing, fictional context, and multi-turn escalation. A discount policy should be tested with direct requests for unauthorized discounts, attempts to trick the system into applying employee discounts to customers, and attempts to stack multiple discounts beyond policy limits.

Policy coverage is the percentage of policies with adequate bypass testing. If you have fifteen content policies and have tested bypass attempts on twelve, coverage is eighty percent. This metric is critical for compliance. If a regulator asks "how do you know your system enforces GDPR data handling requirements under adversarial conditions?" the answer should include policy coverage metrics and test evidence, not just assurances.

One healthcare chatbot enforces twenty-three policies derived from HIPAA, company guidelines, and medical best practices. The team tracks policy coverage in a spreadsheet. Each policy has a row. Each column represents a bypass technique — direct request, social engineering, role confusion, tool manipulation, multi-turn escalation. A cell is green if they have tested that policy with that technique and the system correctly enforced the policy. Yellow if they tested and found a bypass that is now mitigated. Red if they have not tested. Before each release, they require ninety percent green coverage and zero red cells in high-severity policy rows.

## Behavior Space Coverage

Behavior space coverage is the most sophisticated metric. Instead of measuring coverage of predefined categories, it measures coverage of the model's actual behavior space. The idea: your model can produce an infinite variety of outputs. That output space has structure — some regions are safe, some are harmful, some are boundary cases. Behavior space coverage measures how much of that space you have explored.

This requires dimensionality reduction. You cannot enumerate every possible output. Instead, you cluster outputs into behavior regions. Use embedding models to represent outputs as vectors, then cluster those vectors. Each cluster represents a distinct behavior mode. Coverage is the percentage of behavior clusters you have triggered through testing.

In practice, this works as follows. Run your adversarial tests. For each test, capture the model's output. Embed the output using a sentence embedding model. Cluster all output embeddings — HDBSCAN and DBSCAN work well for this. Each cluster is a behavior region. Count how many clusters you discovered. As you add more tests, you might discover new clusters — behaviors you had not triggered before. When new tests stop discovering new clusters, you have saturated the behavior space reachable through your current test strategy.

Behavior space coverage complements category-based metrics. Category metrics tell you what attacks you tried. Behavior metrics tell you what model responses you elicited. A comprehensive test suite should score high on both. High category coverage with low behavior coverage means you tested many attack types but they all produced similar model responses — you have not explored diverse failure modes. High behavior coverage with low category coverage means you discovered diverse behaviors but through a narrow set of attack techniques — you got lucky but lack systematic coverage.

One research team used behavior space coverage to evaluate their red teaming. They ran five thousand adversarial prompts and clustered the outputs into sixty-three distinct behavior regions. They then ran another five thousand prompts and discovered only two new regions. This indicated they had saturated the behavior space given their current attack strategies. To expand coverage further, they needed new attack techniques, not more volume. Behavior space metrics told them when to shift from depth to breadth.

## Multi-Dimensional Coverage Tracking

Real coverage is multi-dimensional. A single metric hides important structure. You need to track coverage across multiple axes simultaneously and identify gaps that only appear in cross-sections.

A two-dimensional coverage matrix might track attack vectors on one axis and capabilities on the other. Each cell represents a combination — prompt injection attacks against document search, jailbreak attacks against code generation, data exfiltration against calendar access. A cell is covered if you have tested that attack type against that capability. Empty cells are gaps.

A three-dimensional matrix adds policy as the third axis. Now you track whether you have tested each attack vector, against each capability, while attempting to bypass each policy. A healthcare bot might track whether they have tested jailbreak attacks against their diagnosis capability while attempting to bypass the medical advice disclaimer policy. The matrix becomes sparse — most combinations are not meaningful — but the meaningful ones become explicit.

Multi-dimensional tracking identifies combinatorial gaps. You might have great coverage of jailbreaks overall and great coverage of your code generation capability, but zero coverage of jailbreaks against code generation specifically. The gap only appears when you look at the cross-section. These gaps are where novel attacks often succeed. Attackers do not think in single dimensions. They combine attack vectors with target capabilities in unexpected ways. Multi-dimensional coverage helps you test like an attacker.

One team uses a three-dimensional coverage dashboard. Rows are attack vectors, columns are capabilities, depth is policy category. Cells are color-coded by test count — white is zero, yellow is one to five, green is more than five. Before each release, they filter for high-severity policies and look for white or yellow cells. Those combinations become mandatory test targets. Over time, the dashboard shifts from mostly white to mostly green. Coverage becomes visible, gaps become addressable.

## Coverage Gaps and Priorities

Measuring coverage is only valuable if you use the data to drive action. Coverage metrics should directly inform test planning. Gaps become priorities.

Prioritize gaps by risk. Not all gaps are equal. A gap in high-severity attack vectors against business-critical capabilities with strict policy enforcement is a critical gap. A gap in low-severity attacks against minor features is a nice-to-have. Use your risk model to weight coverage gaps. Track weighted coverage — high-risk areas count more than low-risk areas.

Prioritize gaps by feasibility. Some gaps are hard to fill because the attack vector is rare or difficult to execute. Some gaps are easy to fill but were simply overlooked. Start with easy gaps to build momentum, then tackle hard gaps with dedicated effort.

Prioritize gaps by observability. Gaps in areas with weak monitoring are riskier than gaps in well-monitored areas. If you have a coverage gap in a capability with robust anomaly detection and human review, the residual risk is lower. If you have a gap in a capability that runs fully automated with minimal logging, the risk is higher. Use monitoring coverage to adjust adversarial testing priorities.

One team runs a quarterly gap analysis. They export coverage metrics, filter for gaps in high-risk areas, rank by severity, and assign each gap to a red team member. The member is responsible for closing the gap within the quarter — writing test cases, running attacks, documenting results. Gaps that cannot be closed get escalated with justification. Over four quarters, they reduced critical gaps from thirty-seven to four.

## Coverage as a Release Gate

Coverage metrics enable evidence-based release decisions. Instead of relying on intuition or political pressure, you define coverage thresholds and enforce them.

Define thresholds by risk tolerance. A high-risk system might require ninety percent attack vector coverage, eighty percent capability coverage, and one hundred percent coverage on critical policies. A lower-risk system might accept seventy, sixty, and ninety percent. The thresholds should reflect regulatory requirements, user safety concerns, and business risk.

Enforce thresholds in CI/CD. After every model update, measure coverage. If coverage falls below threshold, block deployment. This prevents regressions. If a model change removes a safety feature, coverage metrics will drop and the release will fail. The team must investigate, restore coverage, or document an accepted risk.

Track coverage over time. A system that launches with eighty percent coverage should maintain or improve that level. Declining coverage indicates testing is not keeping up with system evolution — new capabilities are being added without corresponding adversarial tests, or old tests are becoming stale. Coverage trends are leading indicators of technical debt in your security testing program.

One financial services company requires eighty-five percent attack vector coverage and ninety percent policy coverage before any model reaches production. When a new model fails to meet these thresholds, it enters a hardening phase. The red team focuses exclusively on closing coverage gaps. No new features, no other work. Coverage becomes the blocking priority. This discipline has prevented six near-misses — models that passed functional testing but had critical adversarial vulnerabilities discovered during hardening.

Coverage metrics transform adversarial testing from an art into engineering. You move from "we tested this" to "we tested eighty percent of this using these specific techniques." You move from "it seems robust" to "we have evidence of robustness across these dimensions." You move from subjective confidence to quantified assurance. The system is still not perfectly secure — nothing ever is — but you know what you tested and what you did not. That knowledge is power.

Next: **11.7 — False Positive Management in Automated Testing** — how to prevent alert fatigue from burying real vulnerabilities in noise.
