# 6.7 — Cross-User Data Leakage: When Contexts Bleed

Context bleed is invisible, silent, and catastrophic. In September 2025, a user of a popular AI productivity tool reported seeing fragments of someone else's email draft in their summary output. The company investigated and found that under specific load conditions, the context window for one user's session was partially overwritten with another user's data. The bug affected approximately 1,200 sessions over a six-hour window. The leaked data included email content, calendar events, and document excerpts. The company issued breach notifications to 847 users across 23 countries. The root cause was a shared in-memory cache that was not properly isolated by session ID. The engineering team had tested for correctness and performance. They had not tested for cross-user leakage. They did not know to look.

Cross-user leakage happens when data from one user's session appears in another's. The causes vary: shared state in application servers, session ID collisions, cache poisoning, context window pollution, or model serving infrastructure that does not isolate requests. The result is always the same — a privacy violation at scale. Users lose trust instantly. Regulators impose penalties. The product's reputation is damaged permanently. This is not a theoretical risk. It has happened to major platforms, small startups, and everything in between. If your system serves multiple users and you have not explicitly tested for cross-user isolation, you are vulnerable.

## How Context Bleed Happens

Session state confusion is the most common cause. Many AI applications maintain conversation state in server-side memory, Redis, or similar caching layers. Each session has a unique ID, and responses are cached by session to reduce latency. If the session ID is guessable, reused, or not properly validated, one user can access another user's cached responses. Example: A session ID generated from a timestamp with low precision. Two users who connect within the same second share a session ID. Their contexts merge. User A sees User B's conversation history. This has happened in production.

Context window pollution occurs in streaming implementations. Some serving architectures batch multiple user requests into a single model forward pass for efficiency. If the batching logic does not properly isolate context windows, tokens from one user's input can bleed into another user's context. The model synthesizes a response that includes information from both users. User A receives a response that references User B's query. The model is not malfunctioning — the infrastructure gave it a mixed context. The model did what it was told.

Shared cache poisoning exploits reuse of prompt prefixes. To reduce cost and latency, some systems cache the model's internal state after processing a common prompt prefix. Example: The system prompt and initial instructions are the same for all users. Instead of reprocessing them for each request, the system caches the state after that prefix and reuses it. If the cache key does not include a user-specific identifier, different users share the same cached state. An attacker can poison the cache by embedding user-specific data in a query that affects the cached state. Subsequent users who hit the same cache entry see the attacker's data.

Memory persistence across requests is an infrastructure bug that manifests as leakage. Some model serving frameworks do not fully reset GPU memory between requests. Residual activations or KV cache entries from one request persist into the next. Under normal conditions, this is harmless — the residual data is overwritten by the new request. But under high load, race conditions, or memory pressure, the residual data can leak into responses. This is rare but documented. It has affected vLLM, TensorRT-LLM, and custom serving stacks. The fix requires explicit memory clearing between requests, which adds latency. Teams skip it to save milliseconds. The risk is cross-user leakage.

## Detecting Cross-User Leakage

Canary injection is the most reliable detection technique. For each user session, inject a unique, high-entropy canary string into a benign part of the context — a comment in the user's input, a hidden field, or a tagged metadata string. Example: User A's canary is "canary-a74f3b92." User B's canary is "canary-d82e5c17." After processing a request, scan the response for canaries that do not belong to the current user. If User A's response contains "canary-d82e5c17," leakage has occurred. Canaries work because they are unique, random, and unambiguous. They do not generate false positives. A canary from User B appearing in User A's session is definitive proof of cross-user bleed.

Parallel session testing verifies isolation under load. Spin up 100 simultaneous user sessions, each with distinct inputs and canaries. Process requests concurrently and collect all responses. Check whether any response contains a canary that does not belong to that session. Repeat under increasing load — 100 sessions, 500 sessions, 1,000 sessions — to identify load-dependent leakage. Many isolation bugs only manifest under high concurrency or memory pressure. Testing at low load misses them. Production load testing is mandatory.

Deterministic replay catches intermittent leakage. Capture production traffic — user inputs, session IDs, timestamps — and replay it in a test environment. Compare responses between the original production run and the replay. If a response in replay contains data that was not in that user's original input, and that data matches another user's session from the same time window, leakage occurred in production. Replay testing is forensic — it detects leakage after the fact — but it proves the bug is real and helps identify the root cause.

Anomaly detection in production logs identifies leakage signals. Log every user input and output. Scan outputs for user-specific identifiers — emails, names, phone numbers, session IDs — that do not match the current user's profile. Flag mismatches for manual review. This is a probabilistic technique — false positives happen when users legitimately discuss other people — but it catches leakage that canaries and replay miss. Combine automated flagging with sampling-based manual review to balance coverage and investigation cost.

## High-Risk Leakage Scenarios

Multi-tenant SaaS platforms are the highest-risk environment. Customer A and Customer B both use the same application, often at the same time, often on shared infrastructure. If session isolation fails, Customer A sees Customer B's data. This is not just a privacy violation — it is a contract breach, a competitive intelligence leak, and potentially a regulatory violation. Multi-tenant platforms must test cross-customer isolation with the same rigor as authentication and authorization. Isolation is not a bonus feature. It is table stakes.

Healthcare and financial applications have regulatory exposure. Cross-user leakage in a healthcare app means Patient A sees Patient B's medical records — a HIPAA violation with penalties starting at $100 per record and no cap. In a financial app, User A seeing User B's account details is a violation of SOX, GLBA, and regional equivalents. These are not "bad PR" failures. They are existential regulatory risks. If your app handles health or financial data, cross-user leakage is not a bug to fix next sprint. It is a compliance-driven immediate remediation.

Shared embedding or vector search indexes create leakage in RAG systems. If Customer A's query retrieves chunks from Customer B's documents due to session isolation failure, Customer A has gained unauthorized access to Customer B's knowledge base. This is the RAG-specific variant of cross-user leakage. Testing requires cross-customer queries with canary documents: Customer A uploads a document containing a unique canary, Customer B queries the system, and the response is scanned for Customer A's canary. If found, isolation failed.

Real-time collaboration tools are vulnerable to context merging. When multiple users interact with the same AI assistant in a shared workspace — a team brainstorming tool, a collaborative writing assistant — the model maintains state across users. If that state is not properly namespaced by user, one user's edits or queries can affect another's view. Example: User A asks the model to draft a paragraph. User B, in a different part of the document, asks the model to summarize their section. The model includes content from User A's draft in User B's summary because the context window merged. This is intended in true collaboration, but unintended when users expect isolation.

## Defense Strategies

Strict session ID validation and uniqueness enforcement are the foundation. Generate session IDs using cryptographically secure random number generators with at least 128 bits of entropy. Validate session IDs on every request. Reject requests with missing, malformed, or expired session IDs. Never reuse session IDs across users. Never derive session IDs from predictable inputs like timestamps or sequential counters. Session ID collision is rare with proper entropy, but the consequences are severe. Over-engineer session ID generation. The cost is negligible compared to the risk.

Namespacing all cached state by user and session prevents cache poisoning. Every cache key must include both the user ID and the session ID. Do not cache by prompt prefix alone. Do not cache by model version alone. Cache keys that omit user identifiers create shared state across users. Even if the cached data seems harmless, an attacker can manipulate it. Example: A cached response to "What is the weather?" seems safe to share across users. But if an attacker embeds a canary in their query, and that canary ends up in the cached response, all users who hit that cache entry see the canary. Namespace everything.

Explicit memory clearing between requests eliminates residual state. Before processing a new request, clear GPU memory, KV caches, and any intermediate activations from the previous request. This adds latency — typically 5 to 20 milliseconds depending on model size and infrastructure. But it guarantees that no data from one request can leak into another. Some teams skip this to save latency. This is a false economy. The milliseconds you save are not worth the cross-user leakage risk. Clear memory between requests. Always.

End-to-end encryption of user data in transit and at rest protects against infrastructure leakage. Even if session state is cached or logged incorrectly, encrypted data is not readable by unauthorized users. This does not prevent context bleed — if User A's encrypted session data is delivered to User B, that is still a violation — but it limits the damage. Combine encryption with session isolation, not as a replacement for it. Defense in depth requires both.

## Isolation Architecture Patterns

Per-user model instances provide the strongest isolation. Each user gets a dedicated model instance on dedicated infrastructure. Contexts cannot bleed because there is no shared state. This is the most expensive architecture — it requires provisioning infrastructure per user, not per tenant or per workload. It is viable for high-value enterprise customers willing to pay for isolation guarantees. For consumer-scale applications, it is cost-prohibitive. But for regulated industries where cross-user leakage creates existential risk, the cost is justified.

Request-level isolation in shared infrastructure is the pragmatic middle ground. Multiple users share the same model instance, but each request is processed independently with strict session validation, namespaced caching, and memory clearing between requests. This is the standard architecture for most production AI applications. It scales cost-effectively and provides acceptable isolation if implemented correctly. The risks are in the implementation details — session ID handling, cache key design, memory management. Rigorous testing is mandatory.

Queue-based processing with per-user queues enforces isolation at the infrastructure layer. Instead of batching requests from multiple users into a single model forward pass, each user's requests are queued separately and processed sequentially. This eliminates the batching-induced context bleed risk but reduces throughput. It is a trade-off: stronger isolation at the cost of lower request-per-second capacity. For applications where isolation is more important than throughput, this is the right choice.

Separate infrastructure per tenant is the multi-tenant SaaS standard for high-security environments. Enterprise Customer A gets their own deployment, their own database, their own model instances. Enterprise Customer B is on completely separate infrastructure. Leakage between customers is architecturally impossible. This is expensive and operationally complex — each customer deployment requires separate monitoring, updates, and maintenance. But for customers in finance, healthcare, or government, it is often a contract requirement. Single-tenant deployments are not optional in these markets.

## Operational Response to Detected Leakage

Immediate containment stops the bleed. If cross-user leakage is detected in production, disable the affected feature or service immediately. Do not wait for a fix. Every minute the system runs, more users are exposed. Fall back to a simpler, safer mode — even if that means degraded functionality. A degraded experience is better than ongoing leakage. Containment is the first response, not the last.

Forensic analysis determines scope. Identify which users were affected, what data was leaked, and over what time window. Review logs to find all instances of leakage, not just the one that triggered detection. Assume the worst case until evidence proves otherwise. Regulators and legal teams need accurate scope estimates for breach notifications. "We think maybe 100 users were affected" is not acceptable. "We analyzed logs from the past 90 days and confirmed 237 sessions exhibited leakage, affecting 189 distinct users" is what compliance requires.

User notification follows regulatory timelines. GDPR requires breach notification within 72 hours. HIPAA requires notification without unreasonable delay. State breach laws in the US vary but generally require 30 to 90 days. Know which regulations apply based on user location and data type. Draft notifications in parallel with forensic analysis. Notify affected users, regulators, and any third parties whose data was exposed. Delayed notification increases penalties and legal liability.

Root cause remediation prevents recurrence. Fix the bug, deploy the fix, and verify through testing that leakage no longer occurs. But also analyze why the bug existed — was it a design flaw, a missing test case, an undocumented assumption? Update the threat model, add regression tests, and expand red-teaming scenarios to cover the failure mode. One leakage incident is a bug. Repeated incidents in different parts of the system indicate systemic architectural or process problems. Fix the system, not just the symptom.

Cross-user leakage is not an edge case. It is a known, documented, recurring failure mode in multi-user AI systems. The teams that avoid it are the ones who test for it explicitly, design isolation into the architecture from day one, and treat user boundaries as security boundaries. The teams that suffer from it are the ones who assumed isolation would happen automatically. It does not.

The next level is multi-tenant isolation testing — systematically verifying that session collision, cache poisoning, and context bleed cannot happen across tenants.

