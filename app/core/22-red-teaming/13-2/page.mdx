# 13.2 — SOC 2 and AI Red Teaming

SOC 2 audits are the price of entry for enterprise SaaS. By 2026, most enterprise buyers require a SOC 2 Type II report before signing contracts. The audit framework evaluates security, availability, processing integrity, confidentiality, and privacy controls. For years, SOC 2 audits focused on infrastructure security — network architecture, access controls, encryption, logging. AI systems were edge cases. That changed. As AI became core to SaaS products, auditors began asking harder questions. How do you secure your AI models? How do you prevent data leakage through prompts? How do you test for adversarial attacks? The answer is red teaming. In mid-2025, a customer support platform passed its SOC 2 audit in part because they provided evidence of quarterly red team engagements, documented findings, and verified remediations. The auditors specifically cited adversarial testing as evidence of effective security controls. Red teaming is becoming a SOC 2 requirement.

## SOC 2 and AI Systems

SOC 2 is built around Trust Service Criteria defined by the American Institute of CPAs. The criteria include Common Criteria that apply to all audits — security controls around access, encryption, monitoring, and incident response — plus additional criteria for availability, processing integrity, confidentiality, and privacy if relevant to the service. AI systems touch all of these. Security is obvious — models can be attacked through prompts, tools, and retrieval systems. Processing integrity matters because AI systems must produce accurate, reliable outputs. Confidentiality is critical when models process sensitive data. Privacy applies when AI handles personal information subject to GDPR, CCPA, or HIPAA.

Auditors evaluate whether your controls are designed effectively and operating effectively. Design effectiveness means the control, if implemented correctly, would mitigate the risk. Operating effectiveness means the control actually ran as designed throughout the audit period. For AI systems, red teaming addresses both. A documented red teaming methodology demonstrates design effectiveness — you have a process to identify adversarial risks. Quarterly red team reports with findings, remediations, and retests demonstrate operating effectiveness — the process actually ran.

The challenge is that SOC 2 was not written with AI in mind. The Trust Service Criteria do not explicitly mention prompt injection, jailbreaking, or model extraction. Auditors apply existing criteria to AI contexts. For example, the Common Criteria require controls to prevent unauthorized access to data. For an AI system, that includes preventing data extraction through prompt attacks. The criteria require logical and physical access controls. For AI, that includes restricting who can deploy models, modify prompts, or access training data. The criteria require monitoring and logging. For AI, that includes logging all user inputs, model outputs, and tool calls for anomaly detection.

## Trust Service Criteria for AI

The security criterion is the most directly applicable to AI red teaming. It requires that the entity protect information and systems from unauthorized access, use, disclosure, disruption, modification, or destruction. For AI systems, this translates to several controls. Access controls over who can interact with the model, who can modify system prompts, and who can access retrieval indexes. Input validation to detect and block adversarial prompts. Output filtering to prevent data leakage, hallucinated credentials, and harmful content. Monitoring to detect unusual query patterns, high-frequency attacks, and anomalous tool usage. Incident response procedures for handling adversarial attacks, data breaches, and model failures.

Red teaming provides evidence for all of these. If an auditor asks how you prevent unauthorized data access through prompts, you show red team findings where you tested data extraction attacks, discovered vulnerabilities, fixed them, and retested. If the auditor asks how you validate inputs, you show red team engagements where you tested prompt injection, identified bypass techniques, and hardened your filters. If the auditor asks how you detect attacks in production, you show that red team findings informed your monitoring rules and alerting thresholds.

Processing integrity is the next relevant criterion. It requires that system processing be complete, valid, accurate, timely, and authorized. For AI systems, this means outputs must be reliable and aligned with intended behavior. Hallucinations threaten processing integrity. So do jailbreaks that make the model ignore instructions. So do prompt injections that manipulate outputs for malicious purposes. Red teaming that targets hallucination exploitation, instruction override, and output manipulation provides evidence that you are actively testing processing integrity. When you discover a jailbreak that makes the model fabricate data, fix it, and retest, you demonstrate control effectiveness.

Confidentiality and privacy criteria are relevant when AI processes sensitive or personal data. Confidentiality requires that information designated as confidential be protected. Privacy requires that personal information be collected, used, retained, disclosed, and disposed of in accordance with privacy policies and regulations. For AI, this means preventing models from memorizing training data, leaking sensitive information in responses, or revealing personal details from retrieval systems. Red teaming that tests data extraction, membership inference, and retrieval leakage provides evidence for confidentiality and privacy controls.

## Red Teaming as Control Evidence

Auditors assess controls by examining documentation, interviewing personnel, and testing control execution. For red teaming to serve as control evidence, you must provide three things. First, a written red teaming policy or methodology that describes the process, frequency, scope, and ownership. Second, engagement reports from each red team test conducted during the audit period, including findings, severity classifications, and remediation recommendations. Third, remediation tracking documentation showing that findings were assigned to owners, fixed, retested, and closed.

The red teaming policy should state the purpose — to identify security, integrity, and confidentiality risks in AI systems through adversarial testing. It should define the cadence — quarterly, before major releases, or after significant model changes. It should list the attack vectors in scope — prompt injection, jailbreaking, data extraction, tool misuse, hallucination exploitation, and any domain-specific risks. It should specify who conducts the testing — internal security team, third-party vendor, or both. It should describe how findings are classified, escalated, tracked, and closed. The policy should be approved by the Chief Information Security Officer and reviewed annually.

Engagement reports must be detailed enough to demonstrate that testing actually occurred and was thorough. A one-page summary is not sufficient. Auditors expect to see the date of testing, the systems and components tested, the attack techniques used, the vulnerabilities discovered, the severity of each finding, and the remediation recommendations. The report should include enough technical detail that an auditor with security knowledge can assess whether the testing was rigorous. If the report says "we tested for prompt injection and found no issues," the auditor will ask for specifics. What prompts did you use? What bypass techniques did you test? How did you validate that the defenses worked?

Remediation tracking connects findings to fixes. Each finding should have a unique identifier, an assigned owner, a status, a target remediation date, and a verification date. Auditors will select a sample of findings and trace them from discovery through remediation to retest. They want to see that Critical and High findings were addressed quickly, that fixes were verified by retesting, and that findings did not recur in subsequent engagements. If you cannot produce this audit trail, the control is considered ineffective even if you actually fixed the vulnerabilities.

## Documentation for Auditors

Auditors work from evidence requests. They will ask for your red teaming policy, all engagement reports from the audit period, remediation tracking logs, and evidence of management review. Prepare these documents in advance. Store them in a central location accessible to the audit team. Use consistent naming conventions. If your audit period is January 1 to December 31, 2025, you should have four quarterly red team reports, a remediation tracker showing the status of every finding from those engagements, meeting minutes or email confirmations showing that findings were reviewed by security leadership, and any policy updates or methodology changes made during the year.

Auditors will also interview the personnel responsible for red teaming. They will ask the Chief Information Security Officer or the security lead to describe the red teaming process, explain how findings are prioritized, and confirm that Critical findings are escalated immediately. They will ask the red team lead or the external vendor to describe the attack techniques used, the tools employed, and the criteria for determining severity. They will ask the engineering lead responsible for remediation to explain how fixes are developed, tested, and deployed. Consistent answers across interviews build auditor confidence. Inconsistent answers raise red flags.

Documentation gaps are common in first-time SOC 2 audits that include AI systems. Teams have done red teaming but never formalized the process. Findings were tracked in spreadsheets, Slack threads, or Jira tickets without a unified view. Retesting happened but was not documented. Remediations were deployed but not verified. Auditors see this and conclude the control exists but is not operating effectively. The fix is straightforward — formalize the process, centralize the documentation, and ensure every finding has a complete audit trail from discovery to closure.

## Common SOC 2 AI Gaps

The most common gap is lack of a documented red teaming methodology. Teams test their models, but the process is ad-hoc. No written policy. No defined cadence. No approved attack vector list. Auditors ask to see the red teaming policy, and there is none. The fix is writing the policy, getting it approved, and following it for at least one quarter before the audit.

The second gap is infrequent or inconsistent testing. Teams did one red team engagement at launch, then nothing for a year. Or they tested after every release but skipped testing when there were no releases. Auditors expect regular testing throughout the audit period. If the policy says quarterly testing but only two engagements occurred in a twelve-month audit period, the control is not operating as designed. The fix is adhering to the documented cadence or updating the policy to reflect actual practice.

The third gap is incomplete remediation tracking. Findings were logged, some were fixed, but there is no central record showing status, ownership, and verification. Auditors cannot confirm that vulnerabilities were addressed. The fix is a remediation tracker — a spreadsheet, a GRC platform module, or a dedicated tool like Jira with custom fields for red team findings. Every finding gets a row. Every row tracks status from Open to Verified.

The fourth gap is lack of third-party validation. Small teams often rely entirely on internal testing. Auditors trust internal red teams less than external assessments because of potential conflicts of interest. If your entire red teaming program is internal, auditors may require additional evidence of control effectiveness. The fix is engaging a third-party red team at least annually, even if you also conduct internal testing quarterly.

The fifth gap is missing management review. Red team reports were generated but never presented to leadership. Auditors expect evidence that security findings reached decision-makers. The fix is adding red team reviews to quarterly security meetings, documenting the reviews in meeting minutes, and ensuring that Critical findings trigger executive notifications.

## Preparing for AI-Aware Audits

Auditors are getting smarter about AI. In 2024, many auditors treated AI as a black box and focused on infrastructure controls. By 2026, audit firms have trained their teams on AI-specific risks. They know what prompt injection is. They know models can memorize data. They know RAG systems can leak documents. They ask direct questions. How do you prevent prompt injection? How do you test for data extraction? How do you monitor for jailbreak attempts? If you do not have answers, you will not pass.

Preparation means treating AI systems with the same rigor as traditional systems. Map your AI components — models, prompts, tools, retrieval indexes — to SOC 2 Trust Service Criteria. Identify which controls apply. For each control, gather evidence. Red teaming is one piece of evidence, but not the only piece. You also need access logs showing who can modify prompts, configuration management records showing how system prompts are versioned, incident response runbooks covering AI-specific scenarios, and monitoring dashboards tracking prompt anomalies and output filtering.

Engage your auditor early. Before the audit starts, ask whether they have experience auditing AI systems. If not, educate them. Share your red teaming methodology. Explain how your AI architecture maps to SOC 2 controls. Walk them through your evidence. Auditors appreciate transparency. If you help them understand your AI systems before the formal audit begins, the audit process goes smoother.

## Red Teaming Frequency for SOC 2

SOC 2 Type II audits cover a period of time, typically six to twelve months. For red teaming to serve as evidence of an operating control, it must occur regularly throughout the audit period. One-time testing does not demonstrate that the control operated effectively for the entire period. Quarterly testing is the standard. It provides four data points across a twelve-month audit, enough to show consistency. Monthly testing is overkill unless your system changes constantly. Annual testing is insufficient for SOC 2 purposes — one data point does not prove the control operated all year.

The testing should align with your release cadence. If you release quarterly, schedule red teaming before each release. If you release continuously, schedule red teaming on a fixed calendar schedule — January, April, July, October. The goal is predictability. Auditors want to see that red teaming happened on schedule, not reactively after an incident.

If you discover a Critical finding mid-cycle, schedule an off-cycle retest after remediation. Do not wait for the next quarterly engagement. The faster you verify the fix, the stronger your control evidence. Auditors look favorably on rapid response. If a Critical finding is discovered in February, fixed in March, and retested in March, that demonstrates effective control operation. If the same finding is discovered in February, fixed in March, but not retested until the April quarterly engagement, auditors may question whether the control operated effectively during the gap.

## Integrating with Security Programs

Red teaming should not exist in isolation. It must integrate with your broader security program. Findings should feed into your vulnerability management process. Critical findings should trigger incident response procedures. Trends should inform your threat model. Detection rules should be updated based on red team attack techniques. Retesting should validate that fixes work in production, not just in development.

Integration also means sharing information across teams. Security teams need access to red team reports to update monitoring rules. Engineering teams need access to findings to prioritize remediation. Compliance teams need access to engagement summaries for regulatory reporting. Legal teams need to understand liability risks from unmitigated vulnerabilities. Product teams need to know when red team findings require feature changes or user communication.

Many organizations appoint a red team program manager who coordinates across teams, tracks remediation, schedules engagements, and prepares documentation for auditors. This role ensures red teaming does not become a siloed activity that produces reports nobody acts on. The program manager is the bridge between the red team, engineering, security, compliance, and leadership.

Red teaming is no longer optional for SOC 2 compliance. As AI becomes central to SaaS products, auditors expect rigorous adversarial testing, documented methodologies, and verified remediations. If you build the program, you strengthen your security posture and satisfy auditors. If you skip it, you risk failing the audit and losing enterprise customers.

The next subchapter covers EU AI Act red teaming obligations and what high-risk AI systems must demonstrate for compliance.
