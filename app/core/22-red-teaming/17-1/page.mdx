# 17.1 — What Purple Teaming Is and Why AI Systems Need It

Most organizations think they have a purple team because their red team and blue team attend the same quarterly meeting. They do not. They have two separate teams that occasionally share a conference room. Purple teaming is not a meeting cadence. It is an operational integration where the people who break the system and the people who defend the system work together in real time, in the same workflow, on the same findings, converting every discovered attack into a deployed detection before the attacker has time to use it in production. The distinction matters because in AI security, the gap between finding and fixing is where breaches happen.

## The Two-Team Failure Mode

In late 2025, a logistics company ran a comprehensive red team engagement against their AI-powered dispatch optimization system. The red team spent three weeks probing the system and produced a forty-page report documenting seventeen findings, including prompt injection through driver notes, system prompt extraction via multi-turn conversation, and an escalation path that allowed queries against the internal routing database. The report went through the standard process. It was presented to leadership. It was assigned to the platform security team. Three findings were remediated within six weeks. Seven were deprioritized because the engineering team was focused on a major feature release. The remaining seven sat in the backlog.

Four months later, the company discovered unauthorized access to their routing optimization data. The entry vector was prompt injection through driver notes — finding number three in the report. The technique had been documented, severity-rated, and filed. Nobody had built a detection for it. Nobody had monitored for the specific pattern the red team had demonstrated. The red team had done their job. The blue team had not received the findings in a format they could act on. The handoff between attack and defense was a forty-page PDF that sat in a shared folder. The cost of the breach, including regulatory notification, customer notification, and forensic investigation, exceeded seven hundred thousand dollars. The cost of building a detection for driver note injection would have been two days of engineering work.

This is what the two-team model produces. Reports instead of detections. Findings instead of fixes. Knowledge that exists in a document but not in the monitoring pipeline.

## What Purple Teaming Actually Means

Purple teaming is the operational practice of merging offensive testing and defensive engineering into a single workflow. The red team member discovers an attack. The blue team member, sitting next to them or on the same call, immediately asks: what telemetry would this attack produce? What log line, what metric, what behavioral signal? Together they design a detection. They test it against the attack. If the detection fires, they deploy it. If it does not, they iterate until it does. The finding and the fix happen in the same session, not the same quarter.

The core operational unit of purple teaming is the **attack-detect-validate cycle**. The red team executes an attack technique against a live or staging environment. The blue team examines what signals the attack produced in existing telemetry. If existing monitoring caught it, they validate the alert quality — is it actionable, is the severity correct, does the runbook make sense? If existing monitoring missed it, they build a new detection, deploy it, and have the red team replay the attack to confirm the detection fires correctly. Every cycle produces either a validated existing detection or a new one. Nothing stays as a report-only finding.

This cycle runs continuously, not on a quarterly engagement schedule. The red team maintains a backlog of techniques to test. The blue team maintains a backlog of detections to validate. The purple team operates through the intersection, pulling from both backlogs in priority order and closing the gap between what attackers can do and what defenders can see.

## Why AI Systems Especially Need This

Traditional software systems have relatively stable attack surfaces. A web application's input validation does not fundamentally change because someone updated a dependency. The SQL injection vectors that existed last month still exist this month, and the WAF rules that blocked them still work. The feedback loop between finding and fixing can tolerate weeks or even months of latency because the landscape moves slowly.

AI systems break this assumption completely. Every model update changes the system's behavior in ways that can enable or disable attacks. A prompt injection technique that failed against GPT-5 might succeed against GPT-5.2 because the newer model handles instruction hierarchies differently. A jailbreak that was patched through system prompt reinforcement might resurface after a prompt engineering sprint that shortened the system prompt for latency reasons. A tool integration that was locked down in version three gets a new parameter in version four that nobody told the security team about. The attack surface is not static. It is a moving target that shifts with every deployment.

This means a red team engagement that runs in January produces findings that may be partially or fully irrelevant by March. Detections built against one model version may miss attacks against the next. The only way to keep security posture current is to test and detect continuously, not periodically. Purple teaming is the organizational structure that makes continuous testing and detection sustainable.

Beyond velocity, AI systems produce attack signals that traditional security operations centers do not recognize. Prompt injection does not trigger intrusion detection systems. Data exfiltration through model outputs does not appear in network data loss prevention tools. Safety filter bypass does not match any known malware signature. If the blue team is waiting for their existing tools to alert on AI attacks, they will wait forever. Purple teaming forces the blue team to sit with the red team during attacks and observe what signals actually appear in AI-specific telemetry — prompt logs, tool call traces, safety filter trigger rates, embedding similarity patterns — and build detections against those signals specifically.

## The Purple Team Charter

A purple team needs a formal charter that defines its scope, authority, and operating model. Without one, the function drifts into either a red team that occasionally briefs defenders or a blue team that occasionally runs a penetration test. Neither is purple teaming.

The charter should establish four things. First, the purple team's mission: to continuously close the gap between what attackers can do to AI systems and what defenders can detect. Second, its scope: all AI-facing systems, including customer-facing models, internal tools, agent architectures, RAG pipelines, and any system that processes natural language input or produces natural language output. Third, its authority: the purple team can access staging and production environments, replay attacks against live telemetry, deploy new detections without waiting for a change management cycle, and escalate unresolved findings to engineering leadership. Fourth, its cadence: the attack-detect-validate cycle runs on a defined schedule, typically weekly for high-risk systems and biweekly for lower-risk ones, with ad-hoc cycles triggered by model updates, prompt changes, or new tool integrations.

## Organizational Placement

Where the purple team sits in the organization determines how effective it is. Three models exist in practice, and only one works well for AI security.

The first model embeds purple teaming within the application security team. This works when the application security team has AI-specific expertise, which in 2026 is still rare. The advantage is close proximity to engineering. The disadvantage is that application security teams tend to think in vulnerabilities and patches, not in continuous detection and response.

The second model places the purple team within the security operations center. This brings the function close to the monitoring and alerting infrastructure, which is critical for the detect-and-validate part of the cycle. The disadvantage is that SOC teams are accustomed to analyzing alerts from existing tools, not generating new detection logic from offensive testing. The skill gap can be significant.

The third model, and the one that produces the best results for AI systems, creates the purple team as a cross-functional unit that draws members from both the AI engineering team and the security team. The AI engineers understand how the models work, what telemetry exists, and what signals are meaningful. The security professionals understand detection engineering, incident response, and threat modeling. Together they cover the full cycle. This model requires dedicated headcount — at minimum one offensive specialist and one detection engineer, with part-time participation from AI engineers during testing cycles. Organizations running more than five AI-facing systems in production typically need a team of four to six.

## Skills and Hiring

Purple team members for AI systems need a combination of skills that is genuinely rare in 2026. The offensive side requires prompt injection expertise, understanding of model behavior under adversarial input, knowledge of tool-use exploitation, and the creativity to chain techniques across AI components. The defensive side requires detection engineering, SIEM administration, log analysis, and the ability to translate behavioral observations into alerting rules. The bridge — the purple part — requires the ability to switch between attacker and defender mindset within the same session, communicate findings in detection-ready formats, and maintain a backlog that tracks both open attack paths and deployed detections.

Hiring for these combined skills is difficult. Most organizations build purple teams by pairing existing red team members who have developed AI testing skills with detection engineers who are willing to learn AI telemetry patterns. Cross-training is essential. The red team members attend SOC war rooms to understand how alerts are triaged. The detection engineers participate in red team sessions to see attacks in real time. Over six to twelve months, each side develops enough fluency in the other's domain to operate as a genuine purple unit rather than two specialists sharing a desk.

The alternative is external purple team services, which have grown rapidly since late 2025. Firms like Lasso Security, Mindgard, and HiddenLayer offer engagement models that combine offensive AI testing with detection engineering deliverables. These work well for organizations that cannot justify dedicated headcount but need the feedback loop. The key selection criterion is whether the vendor delivers detections, not just findings. A purple team vendor that produces a report is just a red team with a different label.

## The Speed Imperative

The central argument for purple teaming in AI systems reduces to one variable: time. How long does it take from the moment an attack technique is discovered to the moment a production detection exists for it? In traditional two-team models, this latency is measured in weeks to months. In a functioning purple team, it is measured in hours to days. In an AI landscape where the attack surface changes with every model update, the difference between weeks and days is the difference between catching an attacker and reading about the breach in the news.

The operational metric that purple teams track is **mean time to detection deployment** — the average elapsed time from when an attack technique is first demonstrated to when a working detection is live in production monitoring. Elite teams target under forty-eight hours for critical and high-severity techniques, under one week for medium, and under two weeks for low. Teams that cannot measure this metric at all have not started purple teaming yet, regardless of what they call their organizational structure.

Everything in this chapter flows from this speed imperative. The feedback loops, the playbooks, the SIEM integrations, the detection engineering practices, the automation calibrations — all of it exists to shrink the gap between finding and fixing. The next subchapter dives into the most critical mechanism for achieving that speed: the red-to-blue feedback loop that turns offensive findings into deployed detections within the same operational cycle.
