# 4.2 — Adversarial Examples: Token Perturbation and Input Manipulation

In December 2024, a content moderation system for a European social platform failed catastrophically. The system used a fine-tuned GPT-5-mini classifier to detect hate speech in user comments. It had been tested on 50,000 labeled examples and achieved 96% accuracy on the holdout set. It passed red-team testing. It went to production.

Within four days, users discovered that adding three specific tokens to the end of any comment caused the classifier to label it as benign, even when the comment explicitly violated platform policy. The tokens were not semantic — they did not mean "ignore previous instructions" or encode a jailbreak. They were a seemingly random sequence: "regarding policy framework." Appending those three tokens to any hate speech comment caused the classifier to misfire. The accuracy that had been 96% in testing dropped to 61% in production. The platform pulled the classifier, rebuilt it with adversarial training, and relaunched three months later. By then, they had lost 40,000 users who left due to inadequate moderation.

The attack was an adversarial example. The tokens were crafted to exploit the model's decision boundary — the high-dimensional surface that separates "violates policy" from "does not violate policy." A tiny perturbation in token space, invisible to human judgment, caused the model to cross that boundary and misclassify the input. This is not a prompt injection attack. It is not social engineering. It is a mathematical exploit of the model's learned geometry.

## What Adversarial Examples Are

An adversarial example is an input that has been slightly modified to cause a model to produce an incorrect output, where the modification is imperceptible or irrelevant to a human evaluator. In image classifiers, this might be a few pixels changed in a stop sign photo, causing the model to classify it as a speed limit sign. In language models, this might be token substitutions, additions, or reorderings that do not change the semantic meaning for a human but catastrophically change the model's prediction.

The core insight is that models operate in high-dimensional spaces where small changes in input can produce large changes in output. The decision boundary between classes is often non-smooth, with regions where a tiny perturbation flips the classification. Adversarial examples exploit this sensitivity. They are inputs that sit just on the wrong side of the decision boundary — close enough to legitimate inputs that they appear normal, but positioned precisely where the model misfires.

For language models, adversarial examples are harder to construct than for image models because discrete tokens cannot be perturbed with the same continuous precision as pixel values. You cannot add 0.001 to the embedding of the word "the." But you can replace tokens with near-synonyms, insert tokens that shift the embedding in adversarial directions, or reorder tokens in ways that preserve human-readable meaning while disrupting the model's learned patterns. Token-level adversarial attacks on LLMs have become increasingly effective as attackers develop gradient-based methods to search the discrete token space for perturbations that maximize misclassification.

## The Perturbation Attack Model

The standard adversarial example attack model works as follows. The attacker has a target input — a piece of text they want the model to misclassify. They have access to the model, at minimum as a black box that accepts inputs and returns predictions. They want to find a small perturbation to the input that causes the model to predict a different class.

In a white-box attack, the attacker has full access to model weights and gradients. They can compute the gradient of the loss function with respect to the input tokens, identifying which token changes will most strongly push the model toward the target misclassification. They use this gradient to iteratively search for adversarial perturbations. This is efficient and effective, but requires access to model internals.

In a black-box attack, the attacker has no gradient access. They must search the perturbation space through repeated queries, testing variations and observing which changes affect the model's output. This is slower and less precise, but still viable. Attackers can use evolutionary algorithms, random search, or transfer attacks — crafting adversarial examples on a surrogate model they do have gradient access to, then testing whether those examples transfer to the target model.

In gray-box attacks, the attacker has partial information — perhaps access to logits or embeddings, but not full gradients. This is the most common real-world scenario, as many API providers expose confidence scores or top-k probabilities. With logit access, the attacker can approximate gradients and perform near-white-box attacks without full model access.

The perturbation must be small enough to preserve the input's semantic meaning or human-perceived intent. A content moderation classifier is useless if an attacker can bypass it by replacing "kill all" with "eliminate everyone" — that is not a mathematical exploit, it is a synonym swap that any human reviewer would catch. The adversarial perturbation must be something a human would overlook: inserting a benign-looking token sequence, reordering clauses, replacing a word with a near-synonym that humans treat as equivalent but the model treats as categorically different.

## Token-Level Adversarial Attacks on LLMs

Early adversarial example research focused on continuous domains like images. Language models introduced a new challenge: tokens are discrete. You cannot smoothly interpolate between "the" and "a." You cannot add gradient noise to a word. The adversarial perturbation must be a valid token substitution, insertion, or deletion.

Researchers developed multiple approaches to this problem. One approach is gradient-based token substitution. Compute the gradient of the loss with respect to the embedding of each token in the input. Identify which token, if replaced, would most strongly push the model toward the target misclassification. Search the vocabulary for the token whose embedding is closest to the adversarial direction indicated by the gradient. Substitute that token and repeat until the model misclassifies.

Another approach is beam search over token sequences. Generate candidate adversarial inputs by making small token-level edits — insertions, deletions, substitutions — and score each candidate by querying the model. Keep the top candidates and expand them further. This is slower than gradient-based methods but works with black-box access.

A third approach is universal adversarial triggers. Instead of crafting a unique perturbation for each input, the attacker searches for a short token sequence that, when appended to any input, causes misclassification across a wide range of inputs. The "regarding policy framework" example from the opening story is a universal trigger. Once discovered, it works on thousands of inputs without modification. This is particularly dangerous for deployed systems because one trigger can be shared across many attackers.

Token-level adversarial attacks have been demonstrated against content classifiers, sentiment analyzers, toxicity detectors, and even instruction-following models. In 2025, researchers showed that appending a 20-token adversarial suffix to harmful prompts could bypass safety fine-tuning in GPT-5, Claude Opus 4, and Gemini 2.5 Pro with greater than 80% success rate. The suffix was nonsensical to humans but reliably disabled refusal behavior in all three models.

## Gradient-Based Adversarial Generation

Gradient-based methods are the most effective way to generate adversarial examples when the attacker has white-box or gray-box access. The attacker computes the gradient of the model's loss function with respect to the input. In a classification task, this might be the gradient of the cross-entropy loss between the model's predicted distribution and the target class. The gradient indicates the direction in input space that most strongly increases the loss — equivalently, the direction that pushes the model toward misclassification.

For image models, the attacker adds a small multiple of the gradient to the input pixels. For language models, the process is more complex because tokens are discrete. One method is to compute the gradient with respect to token embeddings, then find the token in the vocabulary whose embedding is closest to the adversarial embedding direction. Replace the original token with that token. Repeat for multiple tokens until the model misclassifies.

Another method is projected gradient descent in embedding space. Treat the input as a continuous embedding vector. Apply gradient steps to that vector to maximize misclassification loss. After each step, project the embedding back to the nearest valid token embedding. This allows smooth optimization while respecting the discrete token constraint.

A third method is Gumbel-softmax relaxation, which approximates discrete token sampling with a continuous distribution that has gradients. The attacker optimizes over the Gumbel-softmax parameters, then samples discrete tokens from the optimized distribution. This enables end-to-end gradient-based adversarial generation even with discrete outputs.

The key insight is that gradients are an attack tool. Any system that exposes gradients — either directly through model access or indirectly through logit probabilities that allow gradient estimation — enables efficient adversarial example generation. Most fine-tuning workflows expose gradients. Most API providers expose logits. Most teams do not realize this is an attack surface.

## Transfer Attacks Between Models

One of the most concerning properties of adversarial examples is that they often transfer between models. An adversarial example crafted to fool Model A will sometimes also fool Model B, even if Model B has a completely different architecture, different training data, and different weights.

This happens because many models learn similar decision boundaries for the same task. A sentiment classifier trained on movie reviews will have a decision boundary that separates positive and negative sentiment in roughly the same region of embedding space, regardless of whether it is GPT-5-mini, Claude Haiku 4.5, or Llama 4 Scout. An adversarial example that crosses the decision boundary in one model has a reasonable chance of crossing it in another.

Transfer attacks are critical for real-world adversarial robustness. An attacker who wants to bypass a black-box production classifier does not need access to that classifier's weights. They can train a surrogate model on similar data, craft adversarial examples against the surrogate using white-box gradient methods, and test whether those examples transfer to the production model. If the transferability rate is 40%, they can generate adversarial examples at scale on the surrogate and deploy the ones that work on the target.

Transfer attacks also mean that adversarial training against one threat model does not necessarily protect against attackers using a different model. A content moderation system trained with adversarial examples generated from GPT-5 might still be vulnerable to adversarial examples crafted using Claude Opus 4.5 as the surrogate. The adversarial training hardens the decision boundary in the regions where GPT-5 generates attacks, but not necessarily in the regions where Claude generates attacks.

## Real-World Adversarial Example Impact

Adversarial examples are not just a research curiosity. They have real operational impact on deployed systems.

Content moderation systems are the most obvious target. Any classifier that decides whether content violates policy can be attacked with adversarial examples. If users discover a universal adversarial trigger that bypasses the classifier, that trigger spreads. By the time the platform detects the attack and patches the classifier, thousands of policy-violating posts have evaded moderation.

Spam and fraud detection systems are similarly vulnerable. An adversarial perturbation that causes a spam classifier to label phishing emails as legitimate compromises the entire filter. Attackers who discover such perturbations operationalize them at scale.

Safety classifiers in conversational AI are a high-value target. If an adversarial suffix can disable refusal behavior in a production assistant, attackers can use that suffix to extract harmful outputs. The suffix might be nonsensical, but users do not care if the jailbreak works.

Retrieval systems that use classifiers to filter or rank documents can be poisoned with adversarial examples. An attacker who can craft adversarial documents that rank higher than they should can manipulate what information users see, even in a RAG system with strong retrieval logic.

The common thread is that adversarial examples bypass defenses that rely on the model correctly interpreting inputs. Prompt filters assume the model can tell the difference between benign and adversarial prompts. Content moderation assumes the classifier can distinguish policy-violating from policy-compliant text. Adversarial examples break this assumption by exploiting decision boundary geometry that has nothing to do with semantic content.

## Detecting Adversarial Inputs

Detecting adversarial examples is an active research area with no perfect solution. Several approaches show promise but all have limitations.

One approach is input preprocessing. Apply transformations to the input that remove adversarial perturbations without affecting legitimate inputs. For text, this might include paraphrasing, synonym replacement, or token dropout. The idea is that adversarial examples are fragile — small changes to the input will cause them to lose their adversarial property. Preprocessing disrupts the carefully crafted perturbation, causing the input to be classified correctly. This works for some adversarial examples but not all. Sophisticated attacks can craft perturbations that are robust to common preprocessing transformations.

Another approach is ensemble defenses. Run the input through multiple models with different architectures or training procedures. If the models agree on the classification, the input is likely legitimate. If they disagree, the input might be adversarial. This exploits the fact that adversarial examples often do not transfer perfectly across models. The limitation is cost — running multiple models per input is expensive — and the fact that some adversarial examples do transfer across all models in the ensemble.

A third approach is anomaly detection in embedding space. Adversarial examples often have unusual embedding patterns compared to natural inputs. Train an anomaly detector on legitimate inputs and flag inputs whose embeddings are outliers. This can catch adversarial examples that push the input into low-density regions of the embedding space. The limitation is that adversarial examples crafted to stay in high-density regions will evade detection.

A fourth approach is gradient masking. Make the model's gradients uninformative or inaccessible to attackers, preventing gradient-based adversarial generation. This can be done by adding non-differentiable components to the model, using stochastic layers, or simply not exposing logits or gradients through the API. The limitation is that gradient masking does not prevent black-box attacks, and attackers can often estimate gradients even when they are not directly provided.

None of these defenses are foolproof. Adversarial examples are a fundamental property of high-dimensional classifiers. As long as decision boundaries exist, there will be points near those boundaries where small perturbations cause misclassification.

## Robustness Training Limitations

The most principled defense against adversarial examples is adversarial training. During training, generate adversarial examples for each batch of legitimate inputs. Train the model to correctly classify both the legitimate inputs and the adversarial examples. This forces the model to learn more robust decision boundaries that are resistant to small perturbations.

Adversarial training works. Models trained with adversarial examples are significantly harder to attack with the same adversarial generation methods used during training. The decision boundary becomes smoother and more stable in the regions where adversarial examples were generated.

But adversarial training has limitations. First, it is computationally expensive. Generating adversarial examples during training requires computing gradients and performing optimization for each training sample. This can double or triple training time. For large language models, this cost is prohibitive.

Second, adversarial training defends against the specific attack method used during training. A model trained to resist gradient-based token substitution attacks might still be vulnerable to beam search attacks or transfer attacks from a different surrogate model. Adversarial training is not a universal defense — it is a defense against a specific threat model.

Third, adversarial training can reduce model accuracy on clean inputs. The model becomes more conservative, learning to avoid regions of input space where it might be fooled. This conservatism sometimes causes the model to misclassify legitimate inputs that happen to lie near adversarial regions.

Fourth, adversarial training does not scale to all possible adversarial perturbations. The adversarial example space is vast. An attacker can always search for perturbations that were not covered during training. Adversarial training raises the bar — it makes attacks harder — but it does not eliminate the attack surface.

## What Red-Teamers Must Test

If your system relies on a classifier, a content filter, a safety model, or any component that makes binary decisions based on text input, you must test for adversarial example robustness. This means attempting to craft inputs that cause misclassification without changing semantic meaning.

Start with black-box methods. Generate variations of policy-violating inputs by inserting benign tokens, replacing words with synonyms, reordering clauses, or appending short token sequences. Test whether any of these variations bypass the classifier. If you find a universal trigger — a short sequence that causes misclassification across many inputs — you have discovered a critical vulnerability.

If you have white-box access, use gradient-based methods. Compute adversarial examples using projected gradient descent, Gumbel-softmax relaxation, or gradient-based token substitution. Test whether these adversarial examples transfer to the production system.

Test transferability from surrogate models. If your production model is black-box but you have access to a similar open-weight model, craft adversarial examples on the open-weight model and test whether they transfer. This simulates the real-world attack scenario where the attacker does not have access to your exact model.

Test robustness of defenses. If you have input preprocessing, test whether adversarial examples survive the preprocessing. If you have an ensemble, test whether adversarial examples transfer across all models in the ensemble. If you have adversarial training, test whether adversarial examples generated with a different method than the one used during training still succeed.

Document the attack success rate. A model that is vulnerable to 80% of adversarial examples is a fundamentally different risk than a model vulnerable to 8%. If you cannot reduce the success rate below an acceptable threshold, the model is not suitable for the deployment context.

## The Adversarial Example Dilemma

Adversarial examples reveal a fundamental tension in AI security. The properties that make models useful — their ability to generalize from limited data, their smooth decision boundaries that allow fuzzy matching, their high-dimensional representations that capture nuance — are the same properties that make them vulnerable to adversarial attacks.

You cannot eliminate adversarial examples without eliminating the model's capacity to generalize. A perfectly robust classifier would need to memorize every possible input, which defeats the purpose of machine learning. The goal is not to make adversarial examples impossible. The goal is to make them hard enough to craft, and detectable enough when deployed, that the attack is not operationally viable.

This means layering defenses. Adversarial training raises the attack cost. Input preprocessing disrupts fragile perturbations. Ensemble methods catch examples that transfer imperfectly. Anomaly detection flags outliers. Monitoring detects when attack success rates spike in production. No single defense is sufficient, but the combination can make adversarial example attacks impractical at scale.

The next subchapter goes deeper into the embedding layer, where adversarial attacks manipulate vector representations directly. Embeddings are the substrate of retrieval, classification, and semantic reasoning. They are also a largely undefended attack surface.
