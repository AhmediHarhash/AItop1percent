# 5.6 — Language and Translation Attacks

In November 2025, a red team discovered they could bypass every safety filter in a major production model by asking questions in Yoruba. The same requests that triggered immediate refusals in English were answered fully in a West African language spoken by 45 million people. The model had robust safety training in English. It had almost no safety training in Yoruba. The attackers switched languages and walked through the gap.

Language-based jailbreaks exploit the uneven distribution of safety training across languages. Models in 2026 are trained on multilingual data, but safety training is overwhelmingly focused on English. Other high-resource languages like Spanish, French, and Mandarin receive some safety coverage. Low-resource languages receive almost none. Attackers exploit this asymmetry by asking harmful questions in languages where safety training is weak or absent.

This is not a theoretical vulnerability. It is being actively exploited in production systems worldwide. Translation attacks allow attackers to obfuscate harmful content by mixing languages, using transliteration, or relying on machine translation to bypass keyword filters. For any system serving users in multiple languages, language-based attacks are a critical threat surface that most teams have not adequately defended.

## Language Coverage Gaps in Safety Training

Safety training requires labeled examples of harmful requests and appropriate refusals. Creating these examples is expensive. Most organizations create them in English first, because that is where the majority of early users and the most immediate scrutiny exists. Some safety training gets translated to other major languages. Very little reaches low-resource languages.

The result is a model that refuses harmful requests reliably in English, inconsistently in high-resource languages, and barely at all in low-resource languages. The model has learned what harmful content looks like in English. It has not learned what the same content looks like in Yoruba, Swahili, Bengali, or Tagalog. An attacker who asks in one of those languages encounters a model with near-baseline behavior — it tries to be helpful, because it has not been trained to recognize the request as harmful.

This creates an attack surface proportional to the number of languages the model supports. A monolingual English model has one surface. A model supporting 100 languages has 100 surfaces, most of which are less defended than English. As models expand language coverage to serve global users, they expand the number of languages where safety training is weak and exploitation is easy.

The coverage gap exists even within high-resource languages. Safety training in Spanish might cover direct requests but miss slang, regional variations, or culturally specific framings. Safety training in Mandarin might recognize simplified Chinese but perform poorly on traditional Chinese or regional dialects. The language is "supported" in the sense that the model can converse in it, but safety coverage is incomplete.

Measuring your system's language coverage gap requires testing the same harmful requests across all languages you support. Start with your highest-priority safety categories — illegal content, dangerous information, manipulative tactics. Translate each test request into every supported language. Measure refusal rates by language. You will find dramatic variation. English might refuse 95% of harmful requests. Spanish might refuse 70%. Lower-resource languages might refuse 20% or less. The gaps show where your safety training needs expansion.

## Low-Resource Language Vulnerabilities

Low-resource languages are those with limited training data, limited safety training, and limited model exposure during development. These include most African languages, many Southeast Asian languages, indigenous languages, and regional dialects. Models support these languages for basic communication, but have almost no adversarial training or safety tuning in them.

The vulnerability is simple: ask the model to do something harmful in a low-resource language and it usually complies. It has not been trained to recognize the request as harmful because safety training examples in that language do not exist or are minimal. The model defaults to being helpful. It generates the harmful content because nothing in its training marked that content as something to refuse.

This is not always a translation gap. Many speakers of low-resource languages are multilingual and can formulate harmful requests directly in those languages without needing English as an intermediate step. The model receives a fluent request in Amharic or Telugu or Quechua and responds as requested because its safety training did not cover requests in those languages.

Translation-based attacks compound the problem. An attacker who does not speak the low-resource language can use machine translation to convert an English harmful request into that language. The translation might be imperfect, but the model often understands the intent and responds. When the response comes back in the low-resource language, it can be translated back to English. The attacker never needed to speak the language. They only needed to use it as a bypass layer.

Testing low-resource language vulnerabilities requires identifying which languages your model supports and which have minimal safety training. Prioritize languages with large speaker populations but low representation in training data. Test harmful requests in those languages. Document which languages allow bypasses and for which content categories. This tells you where your safety training has the largest gaps and where attackers are most likely to probe.

## Translation as Obfuscation

Translation attacks use machine translation to obscure harmful content from filters that operate on specific languages. The attacker writes a harmful request in English, translates it to another language, sends it to the model, receives a response, and translates the response back to English. The harmful content passed through the system, but the safety filters tuned for English never saw it in English.

This works because safety filters often operate at the language level. A filter tuned to detect harmful patterns in English will not detect the same patterns in French or Arabic or Korean. The filter sees the translated text, does not recognize it as harmful because it is not in the language the filter was trained on, and allows it through. The model processes the request and generates a response. The response is harmful, but it is in the same non-English language, so output filters tuned for English also miss it.

Round-trip translation attacks refine this approach. The attacker translates the harmful request through multiple languages before sending it to the model. English to Spanish to Swahili to the model. The response comes back in Swahili, gets translated to Spanish, then to English. Each translation step degrades the exact wording and obscures keyword-based detection. The semantic content remains, but the surface form changes enough to bypass filters looking for specific phrases.

Translation also serves as a plausible denial mechanism. If caught making harmful requests, the attacker can claim translation error. "I did not realize that phrase meant that in this language" or "the translation tool I used must have mistranslated my innocent request." This is rarely true, but it creates enough ambiguity to complicate enforcement when the request and response are in languages the reviewing team does not speak fluently.

Defending against translation-based attacks requires language-agnostic safety detection. Instead of filtering for harmful content in specific languages, detect harmful content in embedding space where semantic meaning is language-independent. Classify requests and responses based on their semantic content regardless of the language they are expressed in. This increases coverage but also increases false positives, because semantic similarity does not always indicate identical intent across languages and cultures.

## Code-Switching and Mixed Language Prompts

Code-switching attacks mix multiple languages within a single prompt to obscure harmful content or confuse language-specific safety filters. A prompt might start in English to establish context, switch to Spanish for the harmful request, and switch to French for additional details. Each part is in a language the model understands, but the combination bypasses filters designed for monolingual content.

The technique exploits the fact that safety filters are often applied at the language level. A filter detects the primary language of the prompt and applies safety rules for that language. If the prompt is primarily English with a single harmful sentence in another language, the filter might miss the harmful content because it is evaluating based on English safety training and the harmful content is not in English.

Code-switching is particularly effective when the languages chosen have different levels of safety training coverage. Use English for innocuous framing, switch to a low-resource language for the harmful request, and the model processes the low-resource portion with minimal safety filtering. The response might be in the low-resource language, but the attacker can translate it or ask the model to respond in English.

Multi-language prompts also exploit tokenization and encoding boundaries. Models process text in tokens, and tokens are language-dependent. Mixing languages forces the model to switch tokenization strategies within a single prompt, which can disrupt safety mechanisms that rely on specific token patterns. The harmful content is encoded differently than it would be in a monolingual prompt, which changes how safety filters perceive it.

Testing code-switching resistance requires creating prompts that mix languages in realistic patterns. Use high-resource languages for framing and low-resource languages for harmful content. Use multiple low-resource languages within a single prompt. Test whether your safety mechanisms detect harmful content when it appears in a non-primary language. Most systems struggle with this. Safety evaluation happens at the prompt level, assuming a single language. Mixed-language prompts bypass that assumption.

## Transliteration and Script Mixing

Transliteration attacks write content from one language using the script of another. A harmful request in Arabic might be transliterated into Latin script. A request in Chinese might be written using Pinyin romanization instead of Chinese characters. The semantic content is unchanged, but the surface form changes enough to bypass filters trained on standard script representations.

This works because safety training is tied to specific writing systems. A filter trained to detect harmful Arabic content expects Arabic script. When the same content appears in Latin script, the filter does not recognize it. The model, however, often can recognize it because language models are trained on diverse text that includes transliteration, romanization, and script mixing as natural language variation.

Script mixing combines transliteration with code-switching by using multiple scripts within a single prompt. Write part of the prompt in Latin script, part in Cyrillic, part in Arabic script, and part in Chinese characters. Each part is in a language the model supports, but the combination of scripts obscures the overall content from filters that expect consistent script use within a prompt.

Homoglyph attacks use characters from different scripts that look visually similar to bypass keyword-based filters. The Latin letter "a" and the Cyrillic letter "а" are visually identical but have different Unicode encodings. An attacker can write a harmful keyword using a mix of Latin and Cyrillic characters that looks identical to a human reader but does not match the filter's keyword list because the character encodings are different.

Defending against transliteration and script mixing requires normalizing text to canonical forms before applying safety filters. Convert transliterated text to standard script. Map homoglyphs to their canonical equivalents. Detect script mixing and evaluate each script segment independently. This adds preprocessing overhead but closes a significant attack vector that keyword-based filters miss.

## Multi-Language Prompt Construction

Advanced language attacks construct prompts where the harmful intent emerges from the combination of multiple languages rather than being explicit in any single language. Each segment appears innocuous when evaluated independently. Together they form a harmful request that bypasses safety filters evaluating each language segment in isolation.

The technique requires careful prompt structure. Use one language to establish context: "I am researching security vulnerabilities." Use a second language to specify the target: the name of a specific system or technology in that language. Use a third language to request the harmful action: "how to exploit" in yet another language. No single segment triggers refusal, but the complete prompt is a request for exploit information.

This exploits the fact that most safety evaluation operates on the complete prompt as a single text unit, but language detection and processing often happen at the segment level. The model identifies that segment one is English, segment two is German, segment three is Japanese. It processes each segment using the appropriate language model. But safety evaluation might only apply to the dominant language, missing the harmful intent distributed across multiple languages.

Multi-language prompt construction is sophisticated and rare in the wild as of early 2026, but it represents the direction of advanced attacks as simpler language-based bypasses get closed. Red teams testing high-security systems should include multi-language prompt construction in their test suites. Build prompts where harmful intent is distributed across three or more languages. Test whether your safety mechanisms detect the combined intent or only evaluate each language segment independently.

## Testing Multilingual Safety

Testing multilingual safety comprehensively requires resources most teams do not have: fluent speakers of dozens of languages, safety training data in those languages, and evaluation infrastructure that can measure model behavior across languages consistently. But partial testing is better than no testing. Start with the languages where you have the most users or the most risk exposure.

Identify your top ten languages by user volume. Create a test set of harmful requests in your highest-priority safety categories. Translate each request into all ten languages using both human translators and machine translation. Test the model's response to each translated request. Measure refusal rates by language. This gives you a safety coverage baseline for your most important languages.

For languages where you lack fluent speakers, use round-trip translation to verify safety. Send a harmful request in a low-resource language to the model. Translate the response back to English. Evaluate the English translation for harmful content. This is imperfect — translation might obscure some harmful content — but it allows you to test languages you cannot directly evaluate.

Test code-switching by creating prompts that mix your most common language with each of your other supported languages. Harmful request in English, innocuous framing in Spanish. Harmful request in French, framing in English. Measure whether the model's safety behavior degrades when prompts are mixed-language versus monolingual. Most models show degraded safety performance on mixed-language prompts because safety training is predominantly monolingual.

Test transliteration by taking harmful requests in languages that use non-Latin scripts and romanizing them. Arabic requests written in Latin script, Russian requests written without Cyrillic, Chinese requests written in Pinyin. Send these romanized requests to the model. Measure whether refusal rates drop compared to requests in standard script. If they do, your safety filters are script-dependent and vulnerable to transliteration attacks.

## Building Multilingual Defenses

Building multilingual safety requires expanding safety training to cover all languages your model supports at comparable depth to your English coverage. This is expensive and time-consuming, but there is no shortcut. A model that supports 100 languages needs safety training in 100 languages. Supporting a language for basic communication but not for safe communication creates a vulnerability.

Prioritize languages by user volume and risk exposure. Languages with large user populations need safety training first. Languages used in contexts with high abuse potential need safety training even if user volume is low. Create safety training data in each prioritized language. Hire native speakers to generate harmful request examples and appropriate refusals. Fine-tune the model on this multilingual safety data.

Use cross-lingual transfer to extend safety training to languages where you lack resources. Safety training in English can transfer partially to other languages through shared embedding space. A model trained to refuse harmful requests in English will often refuse similar requests in other languages even without language-specific training, because the semantic content maps to similar embedding representations. This is not a complete solution — transfer is imperfect and degrades for low-resource languages — but it provides baseline coverage.

Implement language-agnostic safety detection in embedding space. Instead of detecting harmful content by keyword or pattern matching in specific languages, classify request embeddings as harmful or benign regardless of language. Train a classifier on embeddings of harmful and benign requests across multiple languages. Use this classifier to evaluate all requests before processing. This provides more consistent coverage across languages than language-specific filters.

Monitor and measure safety performance by language continuously. Track refusal rates, false positive rates, and user reports of harmful content by language. Identify languages where safety metrics degrade. Prioritize those languages for additional safety training. Multilingual safety is not a one-time effort. It is an ongoing process of identifying gaps, creating training data, and measuring improvement.

Accept that perfect parity across all languages is unlikely. English will always have the most safety training because it is where most scrutiny exists. Your goal is not perfect parity. Your goal is acceptable coverage across all languages you support, where acceptable means refusal rates and safety metrics are close enough to English that language choice does not create a trivial bypass. The gap between English and Spanish might be five percentage points. The gap between English and a low-resource language might be twenty points initially. Measure the gaps. Work to close them. The goal is reduction, not elimination.

---

Next: **5.7 — Multimodal Jailbreaks: Images, Audio, and Mixed Media**, where we examine how models with vision, speech, and other modalities introduce new attack surfaces that text-only defenses do not cover.
