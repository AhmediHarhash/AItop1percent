# 13.15 — The Red Team Operating Model: Putting It All Together

An operating model is how work actually gets done. Not the aspirational process documented in slides. Not the heroic efforts of individuals working weekends. The repeatable, sustainable system that integrates people, processes, technology, and governance into continuous delivery of results. For AI red teaming, the operating model is how you find failures before users do — every week, every month, every year — without burnout, without backlog explosion, and without compromising quality as you scale.

This is the synthesis. Everything in this section — from jailbreak techniques to compliance obligations — comes together here. The red team operating model integrates all practices into a coherent system that defends your AI systems against adversaries while satisfying stakeholders from engineering to legal to the board.

## What an Operating Model Provides

An operating model answers five critical questions that determine whether red teaming happens consistently or sporadically.

First: who does the work? The operating model defines team structure, roles, responsibilities, and decision rights. It specifies who tests which systems, who triages findings, who approves remediation decisions, and who escalates to executives. Without clear ownership, red teaming becomes someone's side project that gets deprioritized when deadlines loom.

Second: what work gets done? The operating model defines scope, coverage, testing depth, and acceptance criteria. It specifies which systems get tested, how often, with what methodology, and what constitutes adequate testing. Without defined scope, teams either test everything shallowly or a few things deeply while missing critical gaps.

Third: when does work happen? The operating model defines cadence, triggers, and scheduling. It specifies regular testing cycles, event-driven testing, and integration points with development workflows. Without defined timing, testing happens reactively after incidents instead of proactively before deployment.

Fourth: how is work executed? The operating model defines processes, tools, standards, and quality controls. It specifies how tests are planned, executed, documented, and verified. Without defined methods, results vary wildly based on who performed the test.

Fifth: why does work matter? The operating model connects red teaming to business outcomes, risk reduction, and strategic objectives. It translates vulnerability counts into prevented incidents, compliance evidence, and stakeholder confidence. Without clear value articulation, funding disappears when budgets tighten.

## Integrating All Red Team Practices

The operating model weaves together every technique, process, and control covered in this section into a unified system.

Technical testing practices — prompt injection, jailbreaking, bias probing, memorization testing, adversarial inputs — become standardized test suites executed according to defined schedules. Each attack type has documented methodology, automated tooling where possible, and manual procedures for creative adversarial research. The test suites evolve as new attack techniques emerge.

Domain-specific testing — safety for medical AI, fairness for employment AI, privacy for personal data systems — integrates with technical testing in comprehensive assessment plans. Each high-risk system gets both generic attack surface testing and domain-tailored testing that reflects its specific harm potential. The domain expertise comes from specialized team members or external partnerships.

Compliance testing — GDPR documentation, EU AI Act conformity assessment, SOC 2 evidence collection — runs parallel to security testing and shares infrastructure. The same test execution produces both security findings for remediation and compliance evidence for auditors. Process integration reduces duplicate effort and ensures consistency.

Documentation and evidence — test plans, execution logs, finding reports, remediation tracking — feeds multiple stakeholders through tailored views of the same underlying data. Engineering gets technical details. Risk management gets severity trends. Compliance gets audit evidence. The board gets executive summaries. Single source of truth, multiple consumption patterns.

## Cadence and Rhythm

Sustainable red teaming runs on predictable rhythms that match system risk and change velocity. The operating model defines multiple cadences operating simultaneously.

Continuous testing runs automated attack suites against every model deployment and major feature release. This catches regressions and new vulnerabilities introduced by code changes. Continuous testing is shallow but broad — covering common attack vectors automatically before human review.

Sprint-aligned testing integrates red team review into development sprints for high-risk systems. Security engineers participate in sprint planning, review new features for attack surface, and test changes before sprint completion. This catches design flaws early when they are cheap to fix.

Quarterly deep assessments provide comprehensive adversarial testing of production systems. These are intensive engagements where senior red team engineers spend days attacking a single system with creative, manual techniques. Quarterly testing is narrow but deep — finding novel vulnerabilities that automation misses.

Annual independent assessments bring external red teams to verify your internal program's effectiveness and provide fresh perspective. External teams see patterns your internal team has normalized. Annual external testing also satisfies compliance requirements for independent verification.

Event-driven testing happens when triggers occur: new attack technique published, similar system breached at another company, regulatory guidance updated, or production anomaly detected. Event-driven testing is reactive but necessary — the threat landscape does not wait for your quarterly schedule.

## Resource Allocation

The operating model allocates finite red team resources across infinite potential testing through explicit prioritization.

Risk-based allocation focuses intensive testing on high-risk systems. Medical AI, employment decisioning, financial underwriting, and other high-consequence systems get monthly or quarterly deep testing. Lower-risk systems get annual testing or automated-only coverage. The allocation reflects potential harm, not system complexity.

Change-based allocation focuses testing on systems that change frequently. A model that retrains weekly needs more frequent testing than a model deployed once and frozen. Rapid iteration creates rapid vulnerability introduction. Testing frequency must match change frequency.

Coverage-based allocation ensures every production AI system gets tested at some cadence. Even low-risk systems need baseline testing. The operating model defines minimum testing standards: every system tested at least annually, every high-risk system tested at least quarterly, every production deployment tested before release.

Capacity-based allocation acknowledges team size limits. If your red team can perform ten deep assessments per quarter, your prioritization framework selects which ten systems get that treatment. The other systems get automated testing, less frequent manual testing, or deferred testing with documented risk acceptance.

## Stakeholder Engagement

Red teaming serves multiple stakeholders with different needs and different languages. The operating model defines how you engage each group.

Engineering teams need actionable findings with clear remediation guidance. Red team reports must include reproduction steps, root cause analysis, and fix recommendations. Findings integrate into engineering ticketing systems and sprint planning. Engagement is frequent, detailed, and collaborative.

Product teams need risk assessment that informs launch decisions. Red team engagement happens during product planning, feature design, and launch readiness reviews. The question is not just "what vulnerabilities exist" but "is this safe to ship given known vulnerabilities and mitigations?"

Security leadership needs program metrics that demonstrate control effectiveness. Monthly reporting includes vulnerability trends, remediation velocity, coverage metrics, and comparison to industry benchmarks. Engagement is strategic, focused on program health and emerging threats.

Risk and compliance teams need evidence that satisfies auditors and regulators. Red team documentation must meet audit standards: comprehensive test coverage, documented methodology, finding severity justification, and remediation verification. Engagement is periodic but evidence-intensive.

Executive leadership needs business-context risk reporting. Board-level red team updates translate technical findings into business impact: prevented incidents, liability reduction, insurance implications, and regulatory compliance status. Engagement is quarterly or triggered by critical findings.

## Continuous Improvement

The operating model includes mechanisms for learning and evolution. Red team effectiveness improves through deliberate retrospection and adaptation.

Finding retrospectives analyze why vulnerabilities were introduced and how testing can detect them earlier. If red team finds a critical prompt injection vulnerability in production, the retrospective asks: why did development miss this, why did code review miss this, why did automated testing miss this, and what changes prevent recurrence?

Testing effectiveness reviews evaluate whether your attack coverage matches the actual threat landscape. If researchers publish a new jailbreak technique that your test suites do not cover, you have a gap. Quarterly reviews compare your testing methodology to published research, disclosed vulnerabilities, and industry incidents.

Process improvement cycles optimize testing efficiency without sacrificing depth. If manual testing steps can be automated, automate them. If reporting takes days instead of hours, streamline it. If finding triaging creates bottlenecks, add capacity or change classification criteria. Measure cycle times and reduce friction.

Skill development invests in team capability growth. Red team engineers need continuous learning to stay ahead of attackers. Budget time and money for security conferences, training, research projects, and experimentation. Teams that stop learning become ineffective within eighteen months.

## Metrics and Reporting

The operating model defines what you measure and how you communicate it. Metrics must serve decision-making, not just satisfy process compliance.

Coverage metrics track what percentage of your AI portfolio receives testing at each cadence level. Target 100 percent coverage at some level, with higher-risk systems getting more intensive coverage. Report coverage gaps as risk exposure.

Discovery metrics track how many findings you generate per test, categorized by severity. Increasing discovery rate could mean better testing or degrading security posture. Context matters. Trend discovery rate over time and investigate significant changes.

Remediation metrics track how quickly findings get fixed. Measure mean time to remediation, SLA compliance rate, and backlog age distribution. Slow remediation indicates under-resourced engineering, unclear prioritization, or findings that are not actually important.

Quality metrics track false positive rate and re-test pass rate. High false positive rate means findings are not well-verified before reporting, which erodes engineering trust. Low re-test pass rate means fixes are incomplete, which indicates rushed remediation or poor understanding of root causes.

Impact metrics connect red team activity to business outcomes. Track prevented incidents by documenting production issues that would have occurred without pre-deployment red teaming. Track compliance approvals accelerated by strong testing evidence. Track insurance premium reductions attributed to demonstrated controls.

## The Future of AI Red Teaming

AI red teaming in 2026 is maturing rapidly but still far from mature. The next three years will bring significant evolution in techniques, tools, expectations, and integration.

Attack techniques will grow more sophisticated. Current jailbreaks and prompt injections are low-hanging fruit. Future attacks will chain multiple vulnerabilities, exploit subtle model behaviors, and target emergent capabilities we do not yet understand. Red teams must invest in research, not just execution.

Regulatory expectations will tighten. The EU AI Act's conformity assessment requirements are just the beginning. Other jurisdictions will adopt similar frameworks. Voluntary testing will become mandatory testing. Documented evidence will become audited evidence. Red team programs designed for compliance theater will fail regulatory review.

Automation will handle more coverage. Current automation handles basic attacks. Future automation will cover complex attack chains, adaptive adversarial testing, and continuous monitoring. Human red teamers will shift from executing known attacks to researching novel attacks and interpreting automation results.

Organizational integration will deepen. Red teaming will move from specialty security function to integrated development practice. Every AI engineer will understand basic adversarial thinking. Every product launch will include red team evidence. Every board risk report will include AI attack surface assessment.

Industry collaboration will expand. Organizations will share attack techniques, vulnerability patterns, and defense strategies through information sharing communities. Proprietary security through obscurity will give way to collective defense through shared learning. The threats are evolving too fast for any single organization to defend alone.

## The Synthesis: Find the Failures Before Users Do

Every practice in this section serves one purpose: discovering failure modes before they cause harm. Jailbreak testing finds prompt manipulation before attackers use it. Bias testing finds discrimination before lawsuits follow. Memorization testing finds privacy leaks before regulators fine you. Safety testing finds edge case failures before users get hurt.

The operating model makes this continuous. Not a one-time audit. Not periodic consulting engagement. A permanent organizational capability that operates every day, tests every system, and improves every quarter. The red team operating model is your systematic answer to the question: how do you know your AI systems are safe?

You know because you tried to break them and failed. You know because you documented every attempt, fixed every finding, and verified every remediation. You know because you have evidence that survives regulatory scrutiny, convinces insurance underwriters, and satisfies your own professional standards.

The alternative is hoping nobody finds your vulnerabilities before you do. Hope is not a strategy. Testing is.

Build the program. Staff it properly. Fund it sustainably. Integrate it deeply. Measure it honestly. Improve it continuously. The investment is substantial. The return is every incident that never happened, every lawsuit that never got filed, every regulatory fine that never got assessed, and every user who never got harmed.

That is the purpose of red teaming. That is the promise of the operating model. Find the failures before users do — every time, without exception, without compromise. If you build AI systems that affect people's lives, you owe them nothing less.
