# Chapter 4 — Model-Layer Adversarial Attacks

Beyond prompt manipulation lies a deeper threat surface — the model itself. Adversarial ML attacks target weights, embeddings, and gradients. Prompt-level defenses mean nothing when the attacker operates below the prompt layer. This chapter teaches attacks that exploit the mathematical and architectural properties of the model itself — the vector spaces, the gradient flows, the weight distributions that most teams never think to defend.

---

- 4.1 — The Model-Layer Threat Model: Attacks Below the Prompt
- 4.2 — Adversarial Examples: Token Perturbation and Input Manipulation
- 4.3 — Embedding Space Attacks: Manipulating Vector Representations
- 4.4 — Gradient-Based Extraction: Using Model Access Against Itself
- 4.5 — Fine-Tuning Poisoning: Corrupting Models During Adaptation
- 4.6 — Model Supply Chain Attacks: Compromised Weights and Checkpoints
- 4.7 — Testing for Model-Layer Vulnerabilities: Tools and Techniques
- 4.8 — Defending the Model Layer: What Works and What Does Not

---

*Prompt filters catch amateurs. Model-layer attacks defeat professionals.*
