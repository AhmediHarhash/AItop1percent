# 1.4 — The Attack-Defense Cycle — A Framework for This Section

Most teams think about red teaming as an event. You hire someone to test your system, they file a report, you fix the issues, you're done. This mental model fails immediately upon contact with reality. AI systems evolve continuously. New models introduce new vulnerabilities. Attackers discover new techniques weekly. The defenses you built in January are obsolete by March. Red teaming is not a gate you pass through once on your way to production. It is a continuous cycle that runs as long as your AI system exists.

The teams that succeed at red teaming understand this. They build organizational muscle for adversarial thinking. They integrate attack simulation into development workflows, not as a final audit before launch, but as a continuous practice that shapes how every feature is designed, tested, and deployed. They think in cycles, not checkboxes.

## The Eight-Phase Attack-Defense Cycle

The cycle has eight distinct phases, each with its own techniques, outputs, and integration points with your development process. Think Like an Attacker is where you adopt the adversarial mindset — understanding motivations, constraints, and goals of different attacker types. Map the Surface identifies every point where your AI system accepts input, generates output, or makes decisions. Probe Systematically applies structured testing techniques to discover vulnerabilities. Exploit Weaknesses takes discovered issues to their logical conclusion — proving actual harm, not theoretical risk. Document Findings translates technical discoveries into actionable intelligence for engineering and leadership. Harden Defenses implements mitigations based on what you found. Verify Fixes confirms that mitigations actually work without introducing new problems. Iterate starts the cycle again with new attack vectors, new features, and new threat intelligence.

Each chapter in this section maps to one or more phases of this cycle. Chapter 2 covers threat modeling and surface mapping. Chapter 3 teaches systematic probing techniques. Chapter 4 focuses on exploitation and impact demonstration. Chapters 5 through 8 dive into specific vulnerability classes — prompt injection, jailbreaks, data extraction, agent hijacking. Chapter 9 covers documentation and remediation workflows. Chapter 10 addresses verification and regression testing. The final chapter synthesizes the entire cycle into an operating model.

The cycle is not linear in practice. You do not complete phase eight and then return to phase one three months later. Multiple cycles run in parallel. Your core team might be verifying fixes from last month's testing while simultaneously mapping the attack surface of a new feature. Your contracted red teamers might be exploiting weaknesses in your production retrieval system while your internal team probes a new agent architecture in staging. The cycle is fractal — it applies at feature level, system level, and organization level simultaneously.

## Why the Cycle Never Ends

The threat landscape evolves faster than your defenses. In January 2025, the dominant jailbreak technique was role-play-based prompt manipulation. By April, attackers shifted to multi-turn conversation hijacking. By July, the frontier moved to cross-model transfer attacks that exploited alignment differences between GPT-5 and Claude Opus 4. By October, automated adversarial optimization tools made it possible to generate thousands of jailbreak variants per hour. Every month brought new techniques. The red team that stopped testing in March missed every vulnerability discovered after that.

Your system evolves continuously. You swap GPT-5-mini for GPT-5-nano to reduce cost. You add a new retrieval tool that queries an internal database. You fine-tune your summarization model to handle technical documents. You expand your agent from three tools to twelve. Each change introduces new attack surface. The fine-tuned model might have lost safety guardrails that the base model had. The new retrieval tool might expose SQL injection vectors. The cost-optimized model might be more vulnerable to adversarial prompts. Every change requires a new cycle.

Attacker skill increases over time. The techniques that required expert knowledge in 2024 are automated and commoditized by 2026. The jailbreaks that took researchers weeks to discover are now generated by toolkits. The prompt injection vectors that were theoretical conference papers are now discussed on public forums. What was hard in January is easy in December. Your defenses must keep pace not just with new techniques, but with the democratization of old ones.

## Building Organizational Muscle for Adversarial Thinking

The goal of the cycle is not just to find and fix vulnerabilities. The goal is to build a team that thinks adversarially by default. Product managers who design features with attack scenarios in mind. Engineers who write prompts expecting malicious input. QA teams that test for adversarial behavior before testing for functional correctness. Legal and compliance teams that understand what happens when an attacker bypasses your safety filters.

This muscle develops through repetition. The first time you run the cycle, it feels foreign. Thinking like an attacker does not come naturally to people who build systems to help users. The questions feel paranoid. Why would anyone try to trick our customer support bot into revealing other users' data? Why would someone spend hours crafting prompts to make our code generator produce vulnerable code? The answers come from experience — because attackers exist, because the incentives are real, because the consequences are catastrophic.

By the third cycle, the questions become automatic. When Product proposes a new feature, Engineering immediately asks what happens if a user provides malicious input. When you fine-tune a model, you test for safety degradation before testing for task performance. When you add a new tool to your agent, you probe for privilege escalation before you celebrate the capability gain. Adversarial thinking becomes part of the design vocabulary, not an afterthought before launch.

The organizational muscle shows up in architecture decisions. You design systems with isolation boundaries that limit blast radius. You log every tool call and user interaction because you know you will need that data when investigating an incident. You build rate limits and anomaly detection into every component because you assume attackers will attempt automated exploitation. You do not wait for the red team to tell you these things. You build them from the beginning because adversarial thinking is now embedded in how you build.

## Integration with Development and Release Processes

The cycle does not run separately from your normal development workflow. It integrates at every stage. During design, you map the attack surface of proposed features and identify high-risk areas before writing a line of code. During development, engineers run lightweight adversarial tests on their own work — the same way they run unit tests — to catch obvious vulnerabilities before code review. During code review, reviewers specifically look for missing input validation, unsafe tool usage, and prompt injection vectors.

Before merging to staging, automated adversarial test suites run on every pull request. These are not full red team exercises — they are fast, focused tests that check for known vulnerability patterns. A change that adds a new prompt template triggers tests for injection resistance. A change that modifies retrieval logic triggers tests for data leakage. A change that updates model parameters triggers tests for safety degradation. Failures block the merge. You do not ship code that fails basic adversarial checks.

Before releasing to production, you run a structured red team exercise. This is deeper than the automated checks. Human red teamers spend hours or days probing the new feature with techniques too creative to automate. They chain multiple vulnerabilities together. They test edge cases that automated tools miss. They think like real attackers, not like test scripts. This phase typically finds 60 to 80 percent of the issues that would otherwise reach production.

After release, the cycle continues through production monitoring and external bug bounty programs. Your monitoring systems watch for attack patterns — unusual prompt structures, repeated failed attempts, anomalous tool usage. Your bug bounty program incentivizes external researchers to find vulnerabilities you missed. Every finding feeds back into the cycle. You patch the immediate issue, add regression tests to prevent recurrence, update your threat model with the new attack vector, and probe other components for similar weaknesses.

## The Continuous Nature of Red Teaming vs One-Time Audits

A one-time audit answers the question: Does this system have known vulnerabilities today? A continuous red teaming cycle answers a different question: Is this organization capable of defending against evolving threats indefinitely? The first question is useful for compliance checkboxes. The second question is useful for actually staying secure.

One-time audits miss everything that happens after the audit. The model you swap in three weeks later. The prompt template you update next month. The retrieval tool you add next quarter. The jailbreak technique that gets published next year. An audit from January tells you nothing about your risk in July. A continuous cycle keeps you current.

One-time audits do not build internal capability. You hire external experts, they find issues, they leave, and your team learns nothing except what specific vulnerabilities they had on that specific day. A continuous cycle transfers knowledge. Your engineers see how attackers think. Your product team understands threat modeling. Your QA team learns adversarial testing techniques. After six months of continuous cycling, your team catches 40 percent of vulnerabilities before the red team finds them. After a year, they catch 70 percent. The goal is not to eliminate the need for external red teamers — the goal is to raise the baseline so that external red teamers find sophisticated issues, not obvious ones.

One-time audits create false confidence. You pass the audit, you ship the feature, you believe you are secure. Then an attacker discovers the vulnerability your auditors missed — because the auditors tested for last year's attack patterns, not this year's. Continuous red teaming creates appropriate paranoia. You know that every system has vulnerabilities. You know that your current defenses will eventually be bypassed. You know that security is not a state you achieve but a process you maintain. That mindset keeps you vigilant.

## How This Section Guides You Through the Cycle

This section walks you through the entire attack-defense cycle with the depth needed to actually implement it. We start with threat modeling and surface mapping — the foundation that determines what you test and why. We move into systematic probing techniques that turn vague adversarial intent into concrete test cases. We explore specific vulnerability classes with enough detail that you can recognize them in your own systems. We cover exploitation and impact demonstration so you can prove the severity of findings to stakeholders who might otherwise dismiss them. We address remediation workflows that ensure findings turn into fixes, not just reports that sit in ticket queues. We end with verification, regression testing, and the operating model that sustains the cycle over years.

By the end of this section, you will have the knowledge to build a red teaming practice from scratch or to mature an existing one. You will understand what to test, how to test it, how to interpret findings, how to fix issues, and how to integrate adversarial thinking into your organization's development culture. The cycle becomes not just something you do, but something you are.

The next subchapter catalogs what red teams actually find when they probe AI systems — the taxonomy of vulnerabilities that define the current threat landscape.

