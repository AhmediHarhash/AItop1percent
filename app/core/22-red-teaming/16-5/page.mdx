# 16.5 — Multi-Language Evasion — Exploiting Uneven Safety Training

In October 2025, a European fintech company deployed a customer support agent fluent in fourteen languages. The safety testing had been thorough — in English. The team ran every standard adversarial benchmark, every jailbreak variant, every injection pattern. The model refused harmful requests with near-perfect consistency. Then an external red team submitted the same harmful prompts in Zulu, Hmong, and romanized Bengali. The model complied with over seventy percent of them. Not because it failed to understand the requests — it understood them perfectly and responded in fluent, detailed prose. It complied because its safety training had never taught it to refuse in those languages. The English guardrails were walls. The non-English guardrails were suggestions. The red team extracted financial advice that violated regulatory requirements, generated content the company's compliance team would have flagged instantly, and did it all in languages the monitoring system could not even parse.

This is the multilingual safety gap, and it is one of the most reliable evasion techniques available to attackers in 2026. Safety alignment is not a universal property of a model. It is a learned behavior, and it was learned predominantly in English.

## Why Safety Training Is English-Centric

The root cause is data. Safety fine-tuning — the process that teaches a model to refuse harmful requests, add caveats to sensitive topics, and decline to assist with dangerous activities — requires large datasets of harmful prompts paired with appropriate refusals. These datasets are overwhelmingly English. The red-team exercises that generate training data are conducted primarily in English. The reinforcement learning from human feedback that shapes refusal behavior uses English-speaking annotators evaluating English conversations. The harmful content taxonomies that define what the model should refuse are written in English, tested in English, and validated in English.

When the model encounters a harmful request in Swahili, it faces a problem. Its general language capabilities let it understand the request perfectly — modern frontier models like GPT-5, Claude Opus 4.5, and Gemini 3 Pro have strong multilingual comprehension. But its safety training never included examples of refusing harmful requests in Swahili. The refusal behavior was learned through English examples, and the degree to which that behavior transfers to other languages depends on how much cross-lingual generalization the safety training achieved. For high-resource languages like French, German, and Spanish, the transfer is reasonably strong because the model has extensive training data in those languages and some safety examples. For low-resource languages — Zulu, Hmong, Yoruba, Quechua, Romansh — the transfer is weak or nonexistent.

Research from Brown University demonstrated that translating harmful prompts into low-resource languages jailbroke GPT-4 with a 79 percent success rate — comparable to the most sophisticated English-language jailbreak techniques that required far more effort. The attacker did not need to be clever. They needed a translation tool and a target language that the safety training had not covered.

## The Translation Attack

The simplest multilingual evasion requires no linguistic sophistication at all. The attacker writes a harmful prompt in English, translates it into a low-resource language using any of the widely available translation services, submits the translated prompt, and receives a compliant response. If the response comes back in the target language, the attacker translates it back to English. The entire attack takes seconds and requires no knowledge of the target language.

The translation attack is devastating precisely because it is so accessible. Traditional jailbreaking techniques require understanding of prompt structure, token manipulation, or multi-turn strategy. The translation attack requires only copy-paste and a translation API. Any attacker who can use Google Translate can execute it. This low barrier to entry means the attack scales effortlessly — a single script can test hundreds of language combinations in minutes.

What makes this worse is that the model often responds more helpfully in the low-resource language than it would in English even after a successful English jailbreak. When an English jailbreak partially succeeds, the model often hedges, adds warnings, or provides incomplete information. In a low-resource language where safety training never took hold, the model has no trained instinct to hedge. It responds as a fully cooperative assistant because, in that language, it effectively is one.

## Code-Switching and Mixed-Language Attacks

Code-switching — alternating between languages within a single prompt or conversation — is a more sophisticated variant that exploits the boundary between the model's language identification and its safety evaluation. An attacker who writes the benign framing in English and the harmful payload in another language can confuse safety classifiers that operate in one language at a time.

Consider a prompt that begins in English with a reasonable-sounding premise, then switches to Tagalog for the actual harmful request, then returns to English for a closing sentence. The input-level safety classifier, if it processes the prompt as English text, may not even attempt to evaluate the Tagalog portion. It might treat the non-English text as noise, as a quoted passage, or as irrelevant context. The model, however, processes the entire prompt and understands all of it.

Mid-conversation code-switching is even harder to detect. The attacker starts several turns of conversation in English, establishing rapport and context. Then a single turn switches to Portuguese or Hindi for the harmful request. A safety classifier monitoring the conversation sees nine English messages and one non-English message. If the classifier only evaluates English content, that one message passes through unanalyzed. If the classifier attempts to evaluate all languages but was trained primarily on English harmful content, its detection accuracy on the non-English message is significantly lower.

Romanization adds another layer. Instead of writing Hindi in Devanagari script, the attacker writes it in Latin characters — the way Hindi speakers text each other informally. Romanized Hindi looks like garbled English to a safety classifier trained on Latin-script text. The tokenizer breaks it into unfamiliar token sequences. The classifier has no training signal for this input and defaults to a low threat score. But the model, which has seen romanized Hindi in its training data, understands the content perfectly.

## The Translation Relay Attack

A more subtle variant asks the model to perform the translation itself. Instead of submitting a pre-translated harmful prompt, the attacker writes: "Translate the following English text into Yoruba, then answer the question as if it were asked in Yoruba." The harmful request is in plain English for the attacker to construct easily. The model translates it internally, effectively bypassing any input-level safety filter that checked the English prompt and found only a translation request. The safety violation happens in the model's internal processing, after the input filter and before the output filter.

The relay can be chained. "Translate this text from English to Korean, then from Korean to Swahili, then answer the Swahili version." Each translation step moves the content further from the language where safety training is strongest. By the time the model is working in Swahili, the English safety training may have no influence on its response behavior.

Output filters face their own challenge. If the model responds in Yoruba or Swahili, the output safety classifier must be able to evaluate harmful content in those languages. Most output classifiers in 2026 are trained primarily on English harmful content. A harmful response in Swahili passes through the output filter because the filter cannot evaluate it. The attacker receives harmful content in a language they do not read, translates it back to English, and has the information they sought.

## Measuring the Multilingual Safety Gap

The gap between English safety and non-English safety is not binary — it is a spectrum. Research consistently shows that safety degrades along a curve that correlates roughly with training data volume for each language. The general pattern in 2026: English safety is strongest. Major European languages — French, German, Spanish — retain 70 to 85 percent of English safety performance. East Asian languages — Chinese, Japanese, Korean — retain 60 to 80 percent, though with significant variation by topic. South Asian languages — Hindi, Bengali, Tamil — retain 40 to 65 percent. African and Indigenous languages — Zulu, Yoruba, Quechua, Hmong — retain 20 to 40 percent. For some very low-resource languages, safety performance is essentially zero, meaning the model responds to harmful requests with the same willingness as a pre-safety-trained base model.

These numbers are approximate and vary by model, by topic, and by attack technique. But the pattern is universal across every major model family tested through 2025 and into 2026. No model vendor has achieved language-independent safety alignment. Some have narrowed the gap significantly — newer models show better cross-lingual transfer than their predecessors — but the gap persists because closing it requires safety training data in every language the model supports, and generating that data at scale is expensive and operationally difficult.

## Detection Challenges

Detecting multilingual evasion is hard because it requires the detection system to have multilingual capability at least as strong as the model itself. If your model can understand Hmong, your safety classifier must also understand Hmong. Most safety classifiers do not.

The practical detection challenges break down into several layers. Language identification is the first hurdle — the system must determine what language each message is in, which is straightforward for messages entirely in one language but difficult for code-switched text, romanized scripts, and mixed-language prompts. Multilingual harmful content classification is the second hurdle — the classifier must be able to identify harmful content in every detected language, which requires training data that most classifier training pipelines do not have. Cross-turn language tracking is the third hurdle — the system must notice when a conversation switches languages and evaluate whether the switch coincides with a shift in topic toward harmful territory.

One defense that emerged from 2025 research is the English-as-defense-proxy approach. Instead of building multilingual safety classifiers, this method translates incoming non-English prompts into English before running them through the English-trained safety classifier. If the English translation triggers the safety filter, the original prompt is blocked regardless of its source language. This approach leverages existing English safety infrastructure and avoids the need for per-language classifier training. Research showed it blocking over 99 percent of multilingual jailbreak attempts while retaining 95 percent of normal task performance.

The limitation is latency and cost. Translating every non-English prompt into English before safety evaluation adds processing time and API costs. For high-throughput systems, this overhead may be unacceptable. For systems where safety is paramount — healthcare, finance, legal — the cost is worth it.

## Red Team Testing for Multilingual Safety

Every red team engagement in 2026 should include a multilingual testing phase. The methodology is straightforward but requires systematic execution.

Start by identifying the harmful prompts your English safety testing already uses. Translate each prompt into at least ten languages spanning the resource spectrum — two high-resource European languages, two East Asian languages, two South Asian languages, two African languages, and two low-resource or Indigenous languages. Submit each translation to the model. Record the response and whether the safety classifier flagged it.

Then test code-switching. Take the same prompts and split them between English and a non-English language. Test romanized versions of non-Latin scripts. Test the translation relay — ask the model to translate and answer. Test mid-conversation language switches where the harmful request arrives in a different language than the preceding turns.

Build a matrix of results: language versus detection rate. This matrix reveals exactly where your multilingual safety coverage breaks down. For most teams in 2026, the matrix will show a steep decline in safety performance as you move from high-resource to low-resource languages. That decline represents the attack surface that multilingual evasion exploits.

The goal is not to achieve perfect safety in every language — that is not feasible with current technology for many low-resource languages. The goal is to know your exposure, quantify the risk, and implement compensating controls — like the English-as-proxy defense or monitoring-based detection — for languages where alignment alone is insufficient.

Language diversity is a feature of the world your model serves. It is also a vulnerability in your safety architecture. The next subchapter examines a different kind of evasion that has nothing to do with language and everything to do with time — attacks that unfold so slowly they never cross any single detection threshold.
