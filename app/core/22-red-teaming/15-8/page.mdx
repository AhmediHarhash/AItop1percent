# 15.8 — Shadow AI Systems — Discovering Unauthorized Deployments

You cannot secure what you do not know exists. This is the foundational truth of shadow AI, and it is the reason that no amount of policy, tooling, or governance will protect an organization that has not first answered a deceptively simple question: what AI systems are actually running in our environment right now? Not what is documented. Not what is approved. Not what is in the architecture diagram. What is actually running — deployed by a marketing analyst who needed sentiment analysis, built by an operations team that wanted automated reporting, spun up by an intern who followed a YouTube tutorial. Until you answer that question honestly, your AI security program is defending a map while the territory shifts underneath it.

## The Scale of the Problem

Shadow AI is not a niche concern. It is the default state of AI adoption in most organizations. Research from McKinsey found that 78 percent of organizations surveyed used AI in 2024, while an equivalent percentage of AI users bring their own tools to work. For every sanctioned AI deployment that went through procurement, security review, and governance approval, there are multiple unauthorized instances running without oversight. A 2025 study found that 68 percent of enterprise employees who use generative AI at work access publicly available AI assistants through personal accounts, and more than half of them admitted to entering sensitive company information into those public tools.

This is not malicious behavior. It is rational behavior in the absence of accessible alternatives. The marketing team needs to generate copy variations and cannot wait three months for the official AI tool to be approved. The data analyst needs to summarize quarterly reports and discovers that ChatGPT does it in seconds. The customer support lead exports a month of ticket data into Claude to identify patterns. Each individual action seems harmless. Collectively, they create an attack surface that no security team has mapped, no governance framework has assessed, and no red team has tested.

## The Taxonomy of Shadow AI

Shadow AI exists on a spectrum from casual use to full deployment, and each tier carries different risk.

The first tier is **individual tool use** — employees using ChatGPT, Claude, Gemini, or Grok through personal accounts for work tasks. The data risk is immediate: every query containing company information is now stored by a third-party provider under terms of service that the employee accepted personally, not terms that the organization negotiated. The security risk is indirect: conversation history containing proprietary information, customer data, or strategic plans is accessible through the employee's personal account, which is protected by whatever password and two-factor authentication the employee chose to use.

The second tier is **team-level API integration** — a department that signs up for an API key and builds a lightweight tool. The engineering team creates a Slack bot powered by GPT-5 to answer internal questions. The sales team builds a lead scoring tool that sends prospect emails through Claude's API for analysis. The legal team creates a contract review pipeline using a model API and a Python script. These integrations process data programmatically, at higher volume, and with less human oversight than individual use. The API key is usually shared among team members, stored in a spreadsheet or a Slack channel, and never rotated.

The third tier is **departmental model deployment** — a team that fine-tunes a model or deploys an open-source model on their own infrastructure. The data science team within marketing fine-tunes Llama 4 on customer feedback data to build a sentiment classifier. The fraud team deploys a small language model on an internal GPU server for transaction analysis. The research team runs an open-source model on a cloud instance provisioned through a department budget. These deployments have all the security surface area of sanctioned AI systems — model weights, inference endpoints, data pipelines, access controls — but none of the governance.

The fourth tier is **embedded AI in SaaS tools** — AI features within third-party software that employees use daily. The design team's tool now has AI image generation. The project management platform added AI summarization. The CRM vendor embedded AI-powered insights. Each of these SaaS integrations processes company data through AI models, but because they are features within existing approved tools rather than standalone AI deployments, they typically bypass the AI governance review entirely. Nobody classified the CRM's new AI feature as an AI deployment, so nobody assessed its data handling, model provider, or security posture.

## Why Shadow AI Is a Security Problem, Not Just a Governance Problem

Shadow AI is typically framed as a compliance or governance issue — unauthorized tools that violate policy. This framing understates the risk. Shadow AI is a security problem because it creates attack surface that the security team cannot see, cannot monitor, and cannot defend.

An attacker performing reconnaissance against your organization does not limit their scope to sanctioned deployments. They scan for all AI-related endpoints, API calls, and data flows. If your marketing team is running a fine-tuned model on an unpatched cloud instance, the attacker finds it. If your sales team's Slack bot stores an API key in a public channel, the attacker finds it. If your design team's AI tool sends customer mockups to a model provider without encryption, the attacker intercepts it. Every shadow AI system is an undefended entry point that the red team and the security team do not know about — but the attacker might.

The data exposure risk is equally severe. IBM's 2025 Cost of Data Breach Report identified that AI-associated breach cases cost organizations more than 650,000 dollars per incident. Shadow AI systems process sensitive data without data loss prevention controls, without access logging, without retention policies, and without the ability to respond to data subject access requests under GDPR or similar regulations. When a data breach involves a shadow AI system, the organization may not even know the system existed, much less what data it processed.

## The Shadow AI Audit — Systematic Discovery

Discovering shadow AI requires looking at four layers simultaneously: network traffic, endpoints, finances, and people.

**Network-level detection** is the most technical and the most comprehensive. Monitor outbound network traffic for API calls to known model providers — OpenAI, Anthropic, Google, Mistral, Cohere, and others. These APIs have known endpoint patterns and authentication headers that network monitoring tools can identify. A sudden spike in HTTPS traffic to api.openai.com from a subnet that has no sanctioned AI deployment is evidence of shadow AI. DNS monitoring can identify resolution queries for model provider domains. TLS inspection, where legally and ethically permissible, can reveal the specific API paths being called, distinguishing between casual exploration and production integration.

**Endpoint detection** covers locally deployed models. Scan internal network ranges for ports commonly used by model serving frameworks — vLLM, Ollama, text-generation-inference, llama.cpp server. Monitor workstation activity for processes associated with local model inference — high GPU utilization on machines that are not designated for ML workloads, large model weight files downloaded to employee laptops, Docker containers running inference servers. Mobile device management and endpoint detection tools can flag the installation of local AI applications that process corporate data.

**Financial detection** follows the money. Review corporate credit card statements and expense reports for charges from AI API providers. Review cloud billing for GPU instance charges that are not associated with sanctioned projects. Review SaaS subscriptions for AI tools that were purchased by individual departments without going through procurement. A monthly charge of forty dollars from OpenAI on the marketing team's corporate card is evidence of an API integration that never went through security review.

**Human-layer discovery** is the simplest and often the most effective. Survey employees directly. Ask: "What AI tools do you use for work?" Offer amnesty for past unauthorized use — the goal is discovery, not punishment. Run the survey anonymously if necessary to get honest responses. Interview team leads about AI tools their teams have adopted. Review internal Slack channels and documentation wikis for mentions of AI tools, API integrations, or model deployments. The gap between what IT knows about and what employees report is the shadow AI surface area.

## Mapping the Real Attack Surface

The output of a shadow AI audit is a map. Not the map your architecture diagram shows — the real map, with every AI system, every API integration, every local model, every SaaS AI feature, and every employee using a personal AI account for work. This map is the starting point for red teaming because you cannot test what you do not know exists.

For each discovered shadow AI system, the red team assesses the same properties they would assess for any sanctioned deployment: what data does it access, what model does it use, who can interact with it, what credentials does it hold, what network access does it have, and what safety controls are in place. For most shadow AI systems, the answers are alarming. They access sensitive data. They use whatever model the deployer chose. Anyone on the team can interact with them. They hold shared API keys. They have whatever network access the deploying machine has. And they have no safety controls at all.

The red team then tests each shadow system for the same vulnerabilities they test in production: prompt injection, data extraction, credential exposure, privilege escalation, and abuse potential. The difference is that shadow systems fail these tests at dramatically higher rates because they were never hardened. The red team report should quantify this gap — here is what your documented attack surface looks like, and here is what your actual attack surface looks like. The difference is the shadow AI risk.

## Closing the Gap

Discovery without action is just an inventory. The organization must decide, for each shadow AI system, whether to sanction it, secure it, or shut it down. Sanctioning means bringing it under governance — proper API keys, access controls, data handling policies, monitoring, and security review. Securing means hardening it in place while it goes through the approval process. Shutting it down means decommissioning it immediately because the risk exceeds any operational benefit.

The deeper fix is reducing the incentive for shadow AI in the first place. If the official AI tools are too slow to access, too limited in capability, or too burdensome to use, employees will route around them. Organizations that provide accessible, capable, and reasonably governed AI tools see lower shadow AI rates because employees do not need to find their own solutions. A centralized AI platform with self-service access, pre-approved models, and sensible guardrails is the best defense against shadow AI — not because it eliminates unauthorized use entirely, but because it makes unauthorized use unnecessary for most needs.

For the red team, shadow AI is both a finding and a methodology. It is a finding because every undiscovered AI system is undefended attack surface. It is a methodology because the act of discovering shadow AI reveals more about an organization's true security posture than any number of tests against the sanctioned systems. The documented systems are the ones the organization chose to defend. The shadow systems are the ones the organization forgot to defend. Attackers do not forget.

The next subchapter examines the physical layer beneath all of these systems: GPU and compute infrastructure, where side-channel attacks, tenant isolation failures, and resource exhaustion create vulnerabilities that exist below the software stack entirely.
