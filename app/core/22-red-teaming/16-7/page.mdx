# 16.7 — Rate-Based Evasion and Low-and-Slow Prompt Shaping

The **boiling frog pattern** is the most common and most preventable failure mode in AI rate-based defense. Teams deploy rate limiters that block users who send more than sixty requests per minute. They deploy anomaly detectors that flag sessions with more than twenty tool calls. They deploy usage quotas that cap daily token consumption at a million tokens per API key. Then an attacker sends fifty-nine requests per minute, makes nineteen tool calls per session, and consumes 999,000 tokens per day — and the defense sees nothing. Not because the defense is broken. Because the defense is working exactly as designed. It catches violations of its thresholds. It cannot catch activity that respects its thresholds while still being malicious.

Rate-based defenses tell the attacker exactly how fast they can move. The threshold is not a barrier. It is a speed limit sign.

## How Rate-Based Defenses Work — and Where They Fail

Rate-based defenses in AI systems typically operate across several dimensions. Request rate limiters cap the number of API calls per time window per identity. Token consumption limiters cap the total input and output tokens per period. Session-level limits restrict conversation length, tool calls per session, or total turns before a session must restart. Cost-based limits cap the dollar spend per user per period to prevent abuse of expensive models.

Each of these limiters defines an explicit threshold. The threshold is a line in the sand: below the line, behavior is allowed; above the line, behavior is throttled or blocked. The problem is that the threshold encodes no semantic understanding of what the activity means. A legitimate power user who sends fifty-five requests per minute and a sophisticated attacker who sends fifty-five requests per minute look identical to the rate limiter. The limiter counts. It does not think.

Sophisticated attackers exploit this by operating in the space just below every threshold. They study the rate limits through careful probing — sending incrementally faster requests until they hit a throttle response, then backing off to stay ten percent below that point. This threshold-probing technique reveals the exact shape of your rate-based defense in minutes. Once the attacker knows the limits, they configure their automation to respect those limits permanently. They become a legitimate user by every metric the rate limiter measures.

## Distributing Attacks Across Identities

When a single identity's rate limit is too restrictive for the attacker's goals, the natural escalation is distribution. Instead of one account operating at fifty-nine requests per minute, the attacker uses ten accounts operating at six requests per minute each. Total throughput: sixty requests per minute. Per-account rate: well within normal usage patterns.

Creating multiple identities is trivially easy for most AI systems. Free-tier API keys can be generated with disposable email addresses. Enterprise systems that require payment information can be provisioned with prepaid cards or compromised payment credentials. Systems with more robust identity verification still face the problem of compromised legitimate accounts, which carry the behavioral history and trust score of their original owner.

The defense community calls this the **Sybil problem** — named after the classic distributed systems attack where a single adversary creates many identities. In AI security, the Sybil problem manifests as a coordinated attack fleet where each individual member operates within acceptable parameters but the collective fleet achieves throughput that no individual is allowed. Rate limiters that count per user are blind to it. Only rate limiters that count per IP range, per behavioral cluster, or per correlated identity group can detect the coordination.

IP-based rate limiting provides partial defense but is easily circumvented by proxy networks, VPNs, residential IP services, and cloud-hosted automation. In 2026, attackers commonly use rotating residential proxy services that assign a new IP address for every request, making IP-based correlation nearly impossible.

## Low-and-Slow Prompt Shaping

Rate evasion is not only about throughput. It is also about gradual behavioral manipulation — shaping the model's responses over many turns to achieve an outcome that a single prompt would never produce.

**Prompt shaping** is the technique of using a sequence of carefully crafted turns to shift the model's behavior incrementally. Each turn is individually benign. Each turn nudges the model's response tendencies slightly in the attacker's desired direction. After enough turns, the model behaves in ways that its safety training was designed to prevent — not because the safety training was bypassed in a single moment, but because it was eroded over many moments.

The mechanism exploits how language models attend to conversation history. The model does not just respond to the current message. It responds to the current message in the context of everything that came before. An attacker who spends ten turns establishing a context where certain responses seem natural — where the model has already agreed to certain premises, already adopted a certain role, already produced content adjacent to the target content — finds that the eleventh turn's request is far more likely to succeed than it would have been as the first turn.

The Crescendo attack, documented by Microsoft researchers and presented at USENIX Security 2025, formalized this pattern. Crescendo starts with abstract, benign questions and escalates through a progression of increasingly specific requests, each building on the model's previous responses. On average, Crescendo jailbreaks frontier models in fewer than five turns. The key insight is that the model's own outputs become part of the context that makes the next step more likely to succeed. The attacker does not force the model past its safety boundaries. The attacker walks the model past them, one small step at a time, using the model's own responses as stepping stones.

## Multi-Session Priming

Prompt shaping becomes even more powerful when it spans sessions. In systems that maintain user memory or personalization across sessions, an attacker can establish context in one session that makes attacks easier in subsequent sessions.

Session one: the attacker has a normal conversation about chemistry. The system stores a note that this user is interested in chemistry. Session two: the attacker asks about energetic materials, framed as academic curiosity. The system, knowing the user is interested in chemistry, provides a more detailed response than it would to a cold request. Session three: the attacker asks about specific reaction parameters. The system, with two sessions of chemistry context, treats this as a deepening academic inquiry. By session four or five, the model's accumulated context has normalized a level of detail that it would have refused in session one.

Even systems without explicit memory can be vulnerable to multi-session priming. If the system uses a retrieval-augmented personalization layer that pulls in the user's previous queries for context, the attacker's earlier benign queries become part of the context window for later adversarial queries. The safety classifier sees the current query plus context from previous sessions and evaluates the whole package as a continuation of legitimate academic interest.

The defense challenge is that multi-session priming looks identical to genuine learning progression. A student studying chemistry follows the exact same trajectory — broad interest, then energetic materials, then specific reactions. The difference is intent, and intent is invisible to any system that analyzes content rather than purpose.

## Behavioral Fingerprinting as a Defense

Rate counting fails against distributed, low-and-slow attacks. The alternative is behavioral fingerprinting — identifying attackers not by how fast they move but by how they move.

**Behavioral fingerprinting** builds a multidimensional profile of how a user interacts with the system. It tracks not just request volume but the semantic trajectory of questions, the distribution of topics across sessions, the ratio of information-seeking to information-providing turns, the specificity gradient of queries over time, and the correlation between the user's questions and known attack patterns. Two users can send the same number of requests per minute, but their behavioral fingerprints may differ dramatically.

A legitimate user's behavioral fingerprint tends to be broad and somewhat random — they ask about different topics, their specificity varies, they provide context naturally, and their conversation trajectories reflect genuine curiosity or work needs. An attacker's behavioral fingerprint tends to be narrow and systematic — their topics converge on sensitive areas, their specificity increases monotonically, they rarely provide genuine context, and their conversation trajectories show a deliberate progression toward a target.

Implementing behavioral fingerprinting requires embedding-based tracking of query semantics across sessions. Each query is mapped to an embedding. The trajectory through embedding space is analyzed for patterns consistent with reconnaissance, boundary probing, or escalating specificity toward sensitive topics. This is more computationally expensive than rate counting, but it detects attacks that rate counting cannot see.

The behavioral fingerprint can also detect distributed attacks. When ten different accounts produce similar embedding trajectories — converging on the same topic area, escalating specificity at similar rates, asking semantically similar questions — the fingerprints correlate even though the identities do not. This cross-identity correlation reveals coordinated campaigns that per-account analysis misses entirely.

## Building Adaptive Rate Defenses

Static rate limits are the weakest form of rate-based defense. Adaptive rate limits that adjust based on risk signals provide significantly better protection.

The concept is simple: instead of a fixed threshold for every user, assign each user a dynamic rate limit that tightens or loosens based on their behavioral risk score. A user with a benign behavioral fingerprint and a long history of normal usage gets a generous limit. A new user with no history gets a moderate limit. A user whose behavioral fingerprint shows convergence toward sensitive topics gets a restrictive limit. A user flagged by any detection layer gets the most restrictive limit.

Adaptive rate limits mean the attacker cannot probe the threshold once and rely on it permanently. The threshold changes based on the attacker's own behavior. As their queries become more targeted, their rate limit tightens. As they probe more system boundaries, their allowance shrinks. The defense responds to the attacker's strategy rather than ignoring it.

This approach requires integrating rate limiting with behavioral analysis — two systems that are traditionally separate. The rate limiter needs input from the behavioral fingerprinting system. The behavioral fingerprinting system needs to run continuously, not just on flagged events. The integration is non-trivial but eliminates the fundamental weakness of static rate limits: that they provide a predictable, exploitable speed limit that the attacker can respect indefinitely.

## Red Team Methodology for Rate-Based Evasion

Testing rate-based defenses requires the red team to adopt the attacker's perspective on patience and distribution. The testing methodology should include several specific scenarios.

First, threshold probing. Send requests at increasing rates until throttled. Record the exact threshold. Then operate at ninety percent of that threshold and verify that all adversarial activity succeeds without detection.

Second, identity distribution. Use multiple accounts to achieve aggregate throughput that exceeds the per-account rate limit. Verify whether the system detects the coordination.

Third, prompt shaping across turns. Conduct a Crescendo-style multi-turn escalation that stays within all rate limits but achieves a harmful outcome. Record whether any detection layer flagged the conversation trajectory.

Fourth, multi-session priming. Conduct a priming campaign across five or more sessions, with each session individually benign, and verify whether the accumulated context enables outcomes that would be refused without it.

For each test, the result is not just whether the attack succeeded, but whether the detection system noticed. A rate-based defense that blocks threshold violations but misses sub-threshold adversarial activity gives the organization false confidence that is worse than no defense at all — because the organization believes it is protected.

The next subchapter moves from the input side to the output side — examining how attackers craft interactions that produce harmful outputs that bypass the content filters and output classifiers standing between the model and the user.
