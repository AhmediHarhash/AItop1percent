# 4.3 — Embedding Space Attacks: Manipulating Vector Representations

Your retrieval system does not compare documents as text. It compares them as vectors. A user query is embedded into a 768-dimensional or 1536-dimensional vector. The documents in your knowledge base are embedded into the same space. Retrieval is a geometric operation — find the document vectors closest to the query vector, usually by cosine similarity. This works because the embedding model has learned to place semantically similar content close together in vector space.

This geometric property is what makes embeddings powerful. It is also what makes them an attack surface. If an attacker can craft an adversarial embedding — a vector that is geometrically similar to a target query but semantically unrelated or malicious — they can manipulate retrieval without ever touching the text. The retrieval system sees two vectors close together and returns the adversarial document. The model generates a response based on poisoned context. The user receives incorrect or harmful information. The attack is invisible at the prompt layer because it happened in embedding space.

Most teams secure their prompt inputs and monitor their outputs. Almost no teams validate their embeddings. This subchapter teaches how embedding space attacks work, why they bypass traditional defenses, and what you must test to detect them.

## How Embeddings Work as Attack Surface

An embedding model is a neural network that maps text to vectors. The model has been trained on massive datasets with objectives like contrastive learning — pull similar texts closer together in vector space, push dissimilar texts farther apart. The resulting embedding space has useful geometric properties. Synonyms cluster together. Semantically related concepts have high cosine similarity. Queries and relevant documents are proximate.

But the embedding space is high-dimensional, and high-dimensional spaces have counterintuitive properties. Two vectors can be very close in cosine similarity while being semantically unrelated. The model might place "how to secure a database" and "how to exploit SQL injection" near each other because both discuss databases and security, even though one is defensive and one is adversarial. An attacker who understands the geometry of the embedding space can craft text that embeds near a target query, even when the text does not answer that query or contains malicious content.

The attack surface emerges whenever embeddings are used for decision-making. In RAG systems, embeddings determine what context the model sees. In semantic search, embeddings determine what results the user sees. In classification systems that operate on embeddings rather than raw text, embeddings determine the predicted class. If the attacker can manipulate the embedding of an input, they can manipulate the system's behavior without changing the text in ways a human would notice.

Embedding space attacks come in two forms. Adversarial query embeddings target the input side — the attacker crafts a query that embeds in a region of space that retrieves the wrong documents. Adversarial document embeddings target the database side — the attacker injects documents into the vector database that embed near legitimate queries, causing the system to retrieve attacker-controlled content for benign queries.

## Embedding Collision Attacks

An embedding collision is when two semantically different pieces of text produce embeddings with high cosine similarity. In a benign setting, this is a model limitation — the embedding model failed to distinguish between the texts. In an adversarial setting, this is an attack — the attacker deliberately crafted text that collides with a target embedding.

The attacker's goal is to make their text appear similar to a target query or document, without actually being similar in semantic content. They might start with malicious content and iteratively modify it, re-embedding after each modification, until the embedding moves close to a target region in vector space. They might use gradient-based optimization if they have white-box access to the embedding model, or black-box search if they do not.

One approach is keyword stuffing. If the target query is "how do I reset my password," the attacker creates a document that includes those exact keywords but provides malicious instructions. The embedding model sees the overlapping keywords and places the adversarial document near the legitimate query in embedding space. The retrieval system returns the adversarial document as a top result. The model generates a response based on the attacker's instructions.

A more sophisticated approach is gradient-based embedding manipulation. If the attacker has access to the embedding model weights, they can compute the gradient of the cosine similarity between the adversarial embedding and the target embedding with respect to the adversarial text. They use this gradient to iteratively modify the adversarial text, maximizing similarity. This is the embedding-space equivalent of adversarial example generation from the previous subchapter.

Embedding collisions are especially dangerous in RAG systems because retrieval happens before the language model ever sees the text. The model generates its response based on whatever context the retrieval system provides. If the retrieval system has been poisoned with adversarial embeddings, the model has no way to know. It generates a response based on malicious context, and the output might pass all content filters because the attack was not in the prompt — it was in the embedding.

## Adversarial Embeddings in RAG

RAG systems are uniquely vulnerable to embedding space attacks because they separate retrieval from generation. The retrieval step uses embeddings to find relevant documents. The generation step uses the retrieved documents as context. If the retrieval step is compromised, the generation step inherits that compromise.

An attacker who can inject adversarial documents into the vector database can control what context the model sees for specific queries. Suppose the attacker knows users frequently ask "what is our refund policy?" The attacker crafts a malicious document that embeds near this query — perhaps by including the phrase "refund policy" multiple times along with fabricated policy statements. They inject this document into the database. When a user asks the refund question, the retrieval system returns the adversarial document. The model generates a response based on the fabricated policy. The user receives incorrect information. The attack is invisible because the query was legitimate and the response might be plausible enough to pass output filters.

Adversarial embeddings in RAG can also be used for data exfiltration. Suppose an attacker wants to know what queries users are asking. They inject a document with an adversarial embedding designed to be retrieved by many different queries — a universal collision embedding. Every time this document is retrieved, the attacker can log the query if they have access to retrieval logs or database access patterns. This turns the embedding space into an information channel.

Another attack vector is embedding drift. If the attacker can repeatedly inject adversarial embeddings, they can gradually shift the distribution of embeddings in the database. Over time, the retrieval system begins to favor regions of embedding space where adversarial content clusters. Legitimate queries that previously retrieved clean documents now retrieve poisoned ones because the embedding space has been corrupted.

## Poisoning Embedding Databases

Most vector databases have no authentication between the application layer and the database write path. If an annotator can upload documents that get embedded and indexed, they can inject adversarial embeddings. If an API endpoint allows document submission without verifying the embeddings, an external attacker can poison the database. If the embedding process happens offline and the resulting vectors are copied into the production database without validation, a compromised batch can poison thousands of vectors.

Database poisoning is persistent. Once an adversarial embedding is indexed, it remains there until explicitly removed. Unlike a prompt injection attack, which affects a single session, a database poisoning attack affects every query that retrieves the poisoned embedding. The attack scales. The attacker injects a small number of adversarial embeddings, and they affect a large number of user queries.

Poisoning attacks can be targeted or broad-spectrum. A targeted attack crafts embeddings that collide with a specific high-value query — "what is the CEO's email address" or "how do I access admin privileges." A broad-spectrum attack crafts embeddings that collide with many queries, maximizing the number of retrievals. Broad-spectrum attacks are harder to detect because the adversarial embeddings do not cluster around one query — they spread across the embedding space.

Detection requires monitoring the distribution of embeddings in the database. If new embeddings are added that sit in low-density regions far from existing clusters, they might be adversarial. If new embeddings have unusually high similarity to a wide range of queries, they might be universal collision embeddings. If retrieval patterns suddenly shift — queries that used to retrieve Document A now retrieve Document B — the database might have been poisoned.

Most teams have none of this monitoring. They ingest documents, embed them, index them, and assume the database is a trusted data store. They would not detect an adversarial embedding unless a user reported obviously incorrect retrieval results.

## Manipulating Similarity Search

Similarity search is the core operation in embedding-based retrieval. Given a query embedding, find the k nearest document embeddings by cosine similarity or Euclidean distance. This operation assumes that proximity in embedding space corresponds to relevance. Adversarial embeddings break this assumption.

An attacker can craft embeddings that are geometrically close to a target query but semantically irrelevant. This causes the similarity search to return high-ranked results that do not answer the query. In a customer support RAG system, this might mean a query about billing returns documents about shipping. In a medical RAG system, this might mean a query about a specific drug returns documents about unrelated conditions. The retrieval system is functioning correctly — it is returning the nearest neighbors — but the embedding space has been manipulated so that the nearest neighbors are not the most relevant documents.

Manipulating similarity search can also be used to bury legitimate results. Suppose the attacker wants to suppress a specific document. They inject multiple adversarial embeddings that cluster near the queries that would normally retrieve that document. These adversarial embeddings rank higher in similarity than the legitimate document. The legitimate document is pushed out of the top-k results. Users never see it, even though it is the correct answer to their query.

Another manipulation is rank inflation. The attacker wants a specific document to be retrieved for many queries. They craft adversarial versions of that document's embedding that sit near high-traffic regions of the embedding space. These adversarial versions are retrieved frequently, inflating the perceived relevance of the attacker-controlled content.

## Cross-Modal Embedding Attacks

Multimodal embedding models map different data types — text, images, audio — into a shared embedding space. This enables cross-modal retrieval: you can query with text and retrieve images, or query with an image and retrieve text. The shared embedding space is what makes this possible.

It is also a cross-modal attack surface. An attacker can craft an adversarial image that embeds near a target text query. When a user searches for "secure authentication methods," the retrieval system returns an adversarial image that embeds near that query but visually depicts something entirely different — perhaps a phishing page. The model generates a caption or description based on the image, and the output is adversarial.

Cross-modal attacks are harder to detect than text-only attacks because the adversarial perturbation spans modalities. An image that looks benign to a human might embed in a region of space that collides with malicious text. A text query that appears legitimate might retrieve an adversarial audio clip that embeds near the query but contains harmful content when played.

The defense challenge is that modalities have different validation mechanisms. Text can be filtered with keyword matching and toxicity classifiers. Images can be filtered with content moderation models. Audio can be filtered with transcription and text analysis. But embeddings are a shared representation that bypasses modality-specific filters. An adversarial embedding attack can be invisible in both modalities because the attack is in the geometry, not the content.

## Detecting Embedding Manipulation

Detecting adversarial embeddings requires monitoring the embedding space itself, not just the text or the retrieval results. Several signals can indicate manipulation.

Distributional anomalies are the first signal. Legitimate embeddings cluster in regions of the space corresponding to semantic topics. Adversarial embeddings might sit in low-density regions, far from natural clusters. Monitoring the density of the embedding space and flagging outliers can catch adversarial injections. The limitation is that some legitimate documents are also outliers — niche topics, rare queries, edge cases. Anomaly detection will generate false positives.

High-similarity-to-many is another signal. A legitimate document is typically similar to a small number of queries related to its topic. An adversarial embedding crafted as a universal collision will have high similarity to an unusually large number of queries. Flagging embeddings with abnormally high average similarity across diverse queries can detect broad-spectrum attacks.

Sudden retrieval shifts are a behavioral signal. If a query that consistently retrieved Document A for six months suddenly retrieves Document B, something changed. Either the query distribution shifted, the embedding model changed, or the database was poisoned. Logging retrieval patterns over time and detecting discontinuities can catch database poisoning attacks after they occur.

Embedding drift metrics track how the centroid and variance of the embedding distribution change over time. If new documents are being added that systematically shift the distribution in one direction, it might indicate a coordinated poisoning campaign. Legitimate document ingestion produces random drift. Adversarial ingestion produces directional drift toward the attacker's target region.

None of these signals are definitive. Adversarial embeddings that are carefully crafted to mimic the distributional properties of legitimate embeddings will evade detection. But the absence of these signals guarantees you will not detect adversarial embeddings. Most teams have zero visibility into their embedding space. They would not notice an attack until users reported obviously wrong retrieval results.

## Embedding Space Defenses

Defending embedding space requires a combination of access control, validation, and monitoring.

Access control starts with write permissions. Not every component that can read from the vector database should be able to write to it. Annotators who upload documents should not have direct database write access. The upload should go through a validation pipeline that checks the embedding before indexing. External APIs should never allow direct embedding injection. Every write to the database should be authenticated and logged.

Validation means checking embeddings before they are indexed. Compute the embedding for the submitted document. Check whether it sits in a plausible region of the embedding space. If it is an outlier, flag it for review. Check whether it has unusually high similarity to existing embeddings. If it does, it might be a collision attack. Check whether the text content matches the embedding — use a separate model to generate a second embedding and verify they are similar. If they diverge, the submitted embedding might have been manipulated.

Monitoring means tracking the embedding distribution over time. Log the centroid, variance, and density of the embedding space. Detect sudden shifts. Log retrieval patterns and detect queries that start returning different results. Monitor the average similarity of each indexed embedding to the query distribution. Flag embeddings that have abnormally high retrieval rates relative to their content.

Adversarial training for embedding models is an emerging defense. Train the embedding model with adversarial examples — texts that have been crafted to collide with target embeddings. This hardens the model against embedding space attacks, making collisions harder to craft. The limitation is the same as for adversarial training on classifiers: it defends against the specific attack method used during training, not all possible attacks.

Ensemble embeddings are another defense. Use multiple embedding models with different architectures and training data. Index documents with embeddings from all models. At query time, retrieve candidates from all models and take the intersection or a weighted combination. An adversarial embedding crafted to collide in one model's space is less likely to collide in all models' spaces. This raises the attack cost but also raises the operational cost.

## What Red-Teamers Must Test

If your system uses embeddings for retrieval, classification, or similarity-based decisions, you must test for embedding space attacks. Start by attempting to craft adversarial embeddings that collide with high-value queries. Use black-box methods: generate documents with keyword stuffing, synonym replacement, or paraphrasing, and check whether they rank highly for target queries. If you find collisions, you have discovered an exploitable vulnerability.

If you have white-box access to the embedding model, use gradient-based methods. Optimize text to maximize cosine similarity to a target query embedding. Test whether the resulting adversarial document is retrieved by the target query. Test whether it transfers to the production system if the production embedding model is different from the one you have access to.

Test database write permissions. Can an unauthenticated user inject embeddings? Can an internal annotator submit documents that bypass embedding validation? If so, the database can be poisoned.

Test for universal collision embeddings. Attempt to craft a single embedding that ranks highly for many diverse queries. If you succeed, one adversarial document can affect a large fraction of your traffic.

Test cross-modal attacks if your system uses multimodal embeddings. Craft adversarial images that collide with text queries, or adversarial text that collides with image queries. Test whether the cross-modal retrieval system is vulnerable.

Document the attack success rate and the effort required. If adversarial embeddings are easy to craft and reliably retrieved, your embedding space is a critical vulnerability. If they require sophisticated gradient-based optimization and only work 10% of the time, the risk is lower but still non-zero.

## The Embedding Layer Blindspot

Most AI security focuses on the prompt layer and the output layer. Teams filter inputs for prompt injection. They monitor outputs for policy violations. They assume the model is a black box that processes text and returns text. But embeddings are not text. They are the numerical substrate that the model uses internally. Attacks on this layer bypass text-based defenses because they operate on geometry, not semantics.

Embedding space attacks are underexplored in production systems. Researchers have demonstrated them in lab settings. Attackers know they work. But most teams have never tested for them, have no monitoring for them, and have no defenses against them. The embedding layer is a blindspot.

This blindspot is especially dangerous in RAG systems, where embeddings determine what context the model sees. A poisoned embedding database means a poisoned model, regardless of how good the language model is. The model can be GPT-5.2, Claude Opus 4.5, or Gemini 3 Pro — if the retrieval system feeds it adversarial context, it will generate adversarial outputs.

The next subchapter covers gradient-based extraction attacks, where the attacker uses model access to reconstruct training data. Gradients are an information channel. Most teams expose them without realizing they have opened a backdoor.
