# 13.7 — Board and Executive Reporting

Most teams know their boards want AI updates. They build slides showing model performance improvements, user growth, feature velocity. The board asks about AI risks. The engineering team presents a detailed technical briefing on red teaming methodology, attack surface analysis, and vulnerability taxonomies. Eyes glaze over. The board understands the team is doing something about security, but they cannot tell if it is enough. They ask: how do we compare to industry standards? What are our biggest risks? How much would a breach cost us? The technical presentation did not answer those questions because it was written for engineers, not for executives making business decisions.

Board and executive reporting requires translating technical red teaming findings into business language. Executives need to understand AI security risks in terms of business impact, regulatory exposure, reputation damage, and competitive positioning. They need to know where the company is vulnerable, what is being done about it, and whether the current investment level is appropriate. Effective reporting gives executives the information they need to make informed decisions about risk acceptance, resource allocation, and strategic priorities.

## Why Executive Reporting Matters

Boards and executives govern risk. They decide how much risk the company will accept, what controls are worth investing in, and what risks require mitigation. They cannot make those decisions if they do not understand the risks. Technical red team reports do not communicate risk in terms executives can act on. You need a translation layer.

Executives operate at a different altitude than engineers. They think in terms of revenue, market share, regulatory compliance, and brand reputation. When you report a prompt injection vulnerability, they need to understand what that means for the business. Can it cause a data breach that triggers regulatory penalties? Can it generate content that damages the brand? Can it be exploited by competitors? The technical details matter less than the business consequences.

Board reporting also establishes accountability. If you brief the board on AI security risks and they approve your mitigation plan, they have accepted the residual risk. If a security incident occurs later, the record shows the board was informed and made a decision. If you never brief the board and an incident occurs, the board can legitimately claim they were not aware of the risk. Reporting creates shared responsibility.

Effective executive reporting also drives action. When executives understand the business impact of AI vulnerabilities, they allocate resources to fix them. When they see how your company compares to industry benchmarks, they understand whether current investment is adequate. When they see trends — improving security posture or increasing attack sophistication — they can make strategic decisions about future priorities. Good reporting turns security from a technical concern into a business priority.

## What Boards Want to Know

Boards do not want technical deep-dives. They want answers to specific questions that inform their governance decisions. Tailor your reporting to those questions.

The first question is always: what are our biggest risks? Boards cannot evaluate dozens of findings. They need to know the top three to five risks that could materially impact the business. For each risk, explain what could go wrong, how likely it is, and what the business impact would be. A high-severity technical finding that has minimal business impact does not belong in board reporting. A medium-severity finding that could trigger regulatory enforcement or damage customer trust does.

The second question is: how do we compare to peers? Boards think competitively. They want to know whether the company's AI security posture is ahead of, on par with, or behind industry standards. This requires external context. If you conducted third-party red teaming, compare your findings to benchmarks the vendor sees across their client base. If industry reports exist — from analyst firms, trade groups, or regulatory bodies — reference them. If peers have disclosed security incidents, compare your controls to what failed at those companies.

The third question is: what are we doing about it? Boards want to see that management is addressing risks systematically. Summarize your red teaming program, your remediation plans, and your timeline. They do not need technical implementation details, but they need confidence that someone is in charge, there is a plan, and progress is being tracked. If critical risks remain open due to resource constraints, say so explicitly — the board can decide whether to allocate more budget.

The fourth question is: are we compliant? For regulated industries, boards are accountable for compliance. They need to know whether your AI red teaming program meets regulatory requirements. If your industry regulator has published AI guidance, map your testing against those requirements and report compliance status. If external audits are required, report the results and any gaps identified. Compliance is not a technical issue — it is a governance issue, which makes it a board concern.

The fifth question is: how much would a breach cost us? Boards understand financial risk. Translate technical vulnerabilities into dollar estimates. If a data breach occurs, what are the potential regulatory fines, legal costs, remediation expenses, and revenue impact? If a model generates harmful content that damages the brand, what is the estimated reputation cost? These estimates do not need to be precise — they need to be directionally accurate enough to inform risk decisions.

## Translating Technical to Business

The biggest mistake in executive reporting is presenting technical findings without business context. A vulnerability description that makes sense to engineers is often meaningless to executives. You must translate.

Start with business impact, not technical mechanism. Instead of "the model is vulnerable to prompt injection attacks that can bypass content filters," say "attackers could manipulate the model to generate prohibited content, which would violate our terms of service and create legal liability." The technical detail — prompt injection — can come later as supporting context. Lead with why it matters to the business.

Use analogies executives understand. AI security risks are new to many boards, but traditional security risks are familiar. Compare prompt injection to SQL injection — a well-known attack class that bypasses input validation. Compare adversarial inputs to social engineering — manipulating a system by exploiting how it processes information. Compare training data extraction to insider threats — unauthorized access to sensitive information. Analogies create understanding without requiring executives to become AI experts.

Quantify impact when possible. Instead of "this vulnerability could cause a data breach," say "this vulnerability could expose up to 50,000 customer records, which under GDPR could result in fines up to 20 million euros and an estimated 2 million euros in remediation costs." Numbers make risks concrete. They also make it easier for executives to compare risks and prioritize mitigation.

Use risk matrices. Executives are familiar with two-by-two grids showing likelihood versus impact. Plot your top findings on a risk matrix. High likelihood, high impact vulnerabilities go in the top-right quadrant — those require immediate action. Low likelihood, low impact findings go in the bottom-left — those can be accepted or deferred. The matrix visually communicates priority in a format boards understand.

Avoid jargon. Terms like "embedding space," "token probability," "gradient attack," and "adversarial perturbation" mean nothing to most executives. When technical terms are necessary, define them in one sentence the first time you use them. Better yet, describe the concept without the jargon. "The model can be tricked into revealing training data" is clearer than "the model exhibits training data extraction vulnerabilities under adversarial prompting."

## Risk Quantification

Quantifying AI security risk is difficult, but it is necessary for executive decision-making. Boards cannot compare a vague "potential data breach" to concrete financial risks in other parts of the business. You need numbers.

Start with regulatory penalties. If your AI system operates in a regulated industry, calculate the maximum potential fine for violations your vulnerabilities could cause. Under GDPR, data breaches can result in fines up to 4% of global revenue or 20 million euros, whichever is higher. Under HIPAA, violations can result in fines up to 1.5 million dollars per violation category per year. These are not theoretical — regulators impose them. Use the maximum penalty as the upper bound of your risk quantification.

Add remediation costs. If a vulnerability is exploited, what would it cost to respond? Include incident response, forensic investigation, legal counsel, customer notification, credit monitoring services, PR crisis management, and system remediation. Industry data suggests the average cost of a data breach in 2026 is over 4 million dollars. If your breach would affect a large number of customers or involve sensitive data, costs could be higher.

Estimate revenue impact. Some AI security failures do not just cost money — they lose money. If your AI product becomes unavailable due to an attack, what is the revenue impact per day of downtime? If customers lose trust and churn, what is the lifetime value of those customers? If a publicized failure damages your brand, what is the impact on customer acquisition cost and conversion rates? These estimates are harder to calculate than regulatory fines, but they matter to executives.

Consider competitive impact. If a vulnerability gives competitors insight into your AI capabilities, training data, or proprietary techniques, what is the strategic cost? This is the hardest to quantify, but it can be the most important for boards. If your AI is a competitive differentiator, protecting it from adversarial reverse engineering is a strategic priority.

Present risk as a range, not a point estimate. Instead of "a breach would cost 5 million dollars," say "a breach would cost between 2 million and 8 million dollars, with a most-likely estimate of 5 million." Ranges communicate uncertainty and prevent executives from treating your estimate as a guarantee. They also allow you to present best-case, worst-case, and expected-case scenarios.

## Trend Analysis

Boards want to know whether things are getting better or worse. Static reporting — a snapshot of current risks — does not answer that question. You need trend analysis that shows how your AI security posture is evolving over time.

Track findings over time. How many red team vulnerabilities were discovered this quarter compared to last quarter? If the number is increasing, it could mean your red team is getting more effective or your attack surface is growing. If the number is decreasing, it could mean your defenses are improving or your testing is plateauing. Context matters — explain what the trend means.

Track remediation speed. How long does it take to fix critical vulnerabilities? If your average time-to-remediation was 60 days last quarter and 30 days this quarter, that is improvement. If it was 30 days last quarter and 60 days this quarter, that is a red flag. Speed trends show whether your remediation process is scaling with your findings.

Track attack sophistication. Are red teamers finding new, more complex attack chains? Or are they finding variations of the same issues you have seen before? Increasing attack sophistication suggests that as you fix obvious vulnerabilities, adversaries — real or simulated — are adapting. This informs whether your threat model needs updating.

Track external benchmarks. If industry data is available, compare your metrics to it. If the average company in your industry discovers 15 high-severity AI vulnerabilities per year and you discovered 8, that could mean your defenses are stronger or your testing is less thorough. If third-party assessments are available, compare your findings to what external red teamers find at peer companies.

Present trends visually. Boards consume information quickly. A line chart showing vulnerability counts over six quarters communicates more effectively than a paragraph of text. A bar chart comparing your remediation times to industry averages makes the point instantly. Visual trends are easier to understand and more memorable than tables of numbers.

## Benchmarking Against Industry

Executives want to know where the company stands relative to competitors and industry norms. Benchmarking provides that context. It answers the question: are we doing enough?

Use third-party assessment results. If you engage external red teamers, ask them how your security posture compares to their other clients in your industry. Most vendors can provide anonymized benchmarking — for example, "your vulnerability density is in the top quartile for financial services AI products" or "you remediate findings 40% faster than the industry average." This data is valuable for boards because it comes from an independent source.

Reference industry reports. Analyst firms, trade groups, and regulatory bodies publish reports on AI security trends, common vulnerabilities, and maturity benchmarks. Reference these in your board reporting. If an analyst firm publishes an AI security maturity model and you assess your company at Level 3 out of 5, that gives the board context. If an industry group publishes a checklist of recommended AI security controls and you have implemented 80 percent of them, that shows progress.

Compare to public incidents. When competitors or peer companies experience publicized AI security failures, use those as cautionary examples. If a competitor's chatbot was manipulated to leak customer data and you have tested for the same vulnerability and mitigated it, that demonstrates the value of your red teaming program. If a peer was fined for algorithmic bias and your red team found similar issues that you remediated before launch, that shows proactive risk management.

Be honest about gaps. Benchmarking is not just about showing where you are ahead — it is also about acknowledging where you are behind. If industry standards recommend quarterly red team assessments and you conduct them annually, say so. If peers have dedicated AI security teams and you are relying on a generalist security team, acknowledge the gap. Boards can only address under-investment if they know it exists.

## Visual Presentation

Board presentations are time-constrained. You may have 10 to 15 minutes to cover AI security risks. Every slide must be clear, focused, and actionable. Visuals matter more than text.

Use risk matrices to show top vulnerabilities. A two-by-two grid with likelihood on one axis and impact on the other axis makes priorities obvious. Plot your top findings as labeled points on the grid. Executives immediately see which risks require urgent action and which can be deferred.

Use trend charts to show progress over time. A line chart showing vulnerability counts decreasing or remediation speed improving tells a story without words. A bar chart comparing your metrics to industry benchmarks makes your performance clear at a glance.

Use traffic light indicators for compliance status. A simple red-yellow-green status for each regulatory requirement shows where you are compliant, where you are in progress, and where you have gaps. Executives scan these instantly and know where to focus questions.

Limit text. Board slides should not be dense paragraphs. Use bullet points with one idea per bullet. Use large fonts. Use whitespace. The slide should be readable from the back of a boardroom. If a slide requires detailed explanation to understand, it is too complex.

Tell a story. The presentation should have a narrative arc. Start with context — what AI systems you are running and why they matter to the business. Show the risks — what red teaming found and what it means. Present the plan — what you are doing to mitigate risks and what resources you need. End with the ask — what decision you need from the board, whether that is budget approval, risk acceptance, or strategic direction.

## Driving Executive Action

The goal of executive reporting is not just to inform — it is to drive decisions. You want the board to approve budgets, accept residual risks, or provide strategic guidance. Structure your presentation to make the decision clear.

Be explicit about what you need. If you need more budget for red teaming, state the amount and what it will be used for. If you need executive approval to accept a risk that cannot be fully mitigated, explain the risk and the rationale for acceptance. If you need board-level commitment to delay a product launch until vulnerabilities are fixed, make the case for why delay is necessary. Vague asks get vague responses.

Offer options. Executives like to choose between alternatives. Instead of "we need more budget," say "we have three options: continue current quarterly assessments at current budget, increase to monthly assessments for an additional 200,000 dollars per year, or engage a dedicated external red team on retainer for 500,000 dollars per year." Lay out the trade-offs — more frequent testing catches issues faster but costs more. Let the board make the decision with full information.

Connect to business strategy. If the company's strategy is to move upmarket to enterprise customers, explain how red teaming supports that — enterprise customers require security attestations and third-party assessments. If the strategy is rapid AI feature deployment, explain how red teaming ensures features can ship safely without creating security debt. When executives see how security aligns with strategy, they fund it.

Use peer pressure constructively. If competitors are investing heavily in AI security and your company is not, point that out. If customers are asking about your red teaming practices and you do not have good answers, that is a competitive risk. Boards respond to competitive dynamics — if not investing in security puts the company at a disadvantage, they will invest.

Red teaming produces technical findings. Executive reporting translates those findings into business decisions. The next step is understanding how red teaming integrates into vendor management and supply chain risk.

