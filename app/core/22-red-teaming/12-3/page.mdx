# 12.3 — Building the Adversarial Test Suite

The adversarial test suite is your defensive perimeter. It is the collection of all known attack vectors you defend against, encoded as executable tests. When the suite passes, you have evidence that your defenses are holding. When it fails, you have evidence of where they are breaking. The quality of your suite determines the quality of your security assurance.

A well-built adversarial test suite is comprehensive, maintainable, and continuously updated. It covers the full attack surface. It is organized so engineers can find relevant tests quickly. It grows deliberately as new vulnerabilities are discovered. And it is treated with the same rigor as production code — version-controlled, reviewed, and tested.

This subchapter covers how to build a suite that meets these standards and scales as your system evolves.

## What Goes in the Suite

The adversarial test suite contains three types of test cases. First, regression cases — every vulnerability you have discovered and fixed. These tests ensure that known vulnerabilities do not return. Second, known attack techniques from the research community — jailbreak methods, prompt injection patterns, adversarial input structures that have been publicly documented. These tests ensure you are defended against attacks that are widely known. Third, exploratory cases that probe areas of the system you have not yet fully tested. These tests expand coverage into new regions of the input space.

Regression cases come from your own red teaming efforts and from production monitoring. When you find a jailbreak during a manual red team engagement, it becomes a regression case. When you detect a prompt injection in production logs, it becomes a regression case. Every real exploit gets encoded.

Known attack techniques come from research papers, security advisories, and community sharing. When a new jailbreak method is published, you adapt it to your system and add it to the suite. When a prompt injection technique circulates on social media, you test whether your system is vulnerable and encode the test. You do not wait for an attacker to try it. You add it preemptively.

Exploratory cases are designed to find vulnerabilities you have not yet discovered. They are inputs designed to stress-test defenses, probe edge cases, and explore boundary conditions. They might use fuzz-like input generation, semantic perturbations, or adversarial mutation of known-good inputs. Not all exploratory cases will uncover vulnerabilities, but some will. When they do, those findings become regression cases.

The balance between these three categories depends on your system's maturity. In early-stage systems, exploratory cases dominate because you have not yet built a large regression corpus. In mature systems, regression cases dominate because you have discovered and encoded many vulnerabilities. But all three categories remain present throughout the lifecycle.

## Organizing by Attack Category

An adversarial test suite with 5,000 unsorted test cases is unusable. You need organization. The primary organizing principle is attack category. Each major category of attack gets its own section of the suite.

Common categories include jailbreaks, prompt injections, data exfiltration, role confusion, adversarial inputs, indirect prompt injection, multi-turn manipulation, context pollution, tool misuse, and boundary violations. The specific categories depend on your system's architecture and risk profile. A RAG-based system needs strong coverage of context pollution and retrieval manipulation. An agent system needs strong coverage of tool misuse and action boundary violations. A chatbot needs strong coverage of jailbreaks and role confusion.

Within each category, test cases are further organized by subcategory or technique. Jailbreak tests might be subdivided into role-play jailbreaks, hypothetical scenario jailbreaks, prefix injection jailbreaks, and encoded payload jailbreaks. Prompt injection tests might be subdivided by injection point — system message injection, user message injection, retrieved context injection, and tool output injection.

This hierarchical organization serves multiple purposes. It makes it easy to find related tests when investigating a failure. It makes coverage gaps visible — if you have 200 jailbreak tests but only 10 data exfiltration tests, you know where to focus next. It makes it possible to run subset sweeps — you can run all jailbreak tests in isolation when you change guardrail logic, without running the full suite.

The category structure is documented and enforced. You do not allow engineers to add tests to a miscellaneous bucket. Every test belongs to a category. If a test does not fit existing categories, you either create a new category or refine the test until it fits. This discipline prevents the suite from devolving into an unsearchable pile.

## Test Case Lifecycle Management

Each test case in the suite has a lifecycle. It is created when a vulnerability is discovered. It is maintained as the system evolves. It is eventually retired when it is no longer relevant. Managing this lifecycle is critical to keeping the suite healthy.

Creation is triggered by discovery. When a red teamer finds a jailbreak, they document the exploit, create a test case, and submit it for review. The test case includes the input, the expected behavior (typically refusal or safe fallback), and metadata (severity, category, discovery date, description). The test is reviewed by another engineer to ensure it is accurate, automatable, and correctly categorized. Once approved, it is merged into the suite and begins running in CI/CD.

Maintenance happens continuously. As the system changes, some tests break. A prompt template change might alter how the model responds, causing a test that previously passed to fail even though the underlying defense is still sound. These are false negatives. You fix them by updating the test's success criteria to match the new expected behavior. Other tests might start passing when they should fail — a defense was weakened but the test did not catch it. These are false positives. You fix them by making the test more stringent.

Retirement happens when a test is no longer needed. This is rare. Most adversarial tests remain relevant indefinitely because the attack vectors they test are always possible. But some tests become obsolete. If you remove a feature entirely, tests for vulnerabilities in that feature can be retired. If you replace an architecture with a fundamentally different design, tests tied to the old architecture can be retired. Retirement requires approval. You document why the test is no longer relevant. You archive it rather than delete it, in case you need to resurrect it later.

The lifecycle is tracked in metadata. Each test case has a status field: active, under review, flaky, or retired. Active tests run in CI/CD. Under review tests are being investigated for accuracy. Flaky tests have been temporarily disabled pending a fix. Retired tests are archived but not deleted. This status tracking makes it easy to audit suite health and identify tests that need attention.

## Versioning and Provenance

Every test case has provenance — a record of where it came from and why it exists. Provenance includes the discovery date, the discovery method (manual red teaming, production monitoring, research adaptation), the person who created the test, and a reference to the original finding (a ticket, a report, a paper).

Provenance serves three purposes. First, it helps engineers understand why a test exists. When a test fails, the engineer investigates. Provenance tells them the history — this test was added because we found a jailbreak in production six months ago, here is the original incident. That context helps them assess whether the failure is a regression or a false positive.

Second, provenance helps you audit coverage. You can query the suite to see how many tests came from manual red teaming versus automated discovery versus research adaptation. If most of your high-severity tests came from production incidents, you know your pre-release testing is not catching enough. If most came from manual red teaming, you know manual testing is high-value and should be expanded.

Third, provenance helps you evaluate the effectiveness of your testing methods. If a category of tests consistently finds regressions, the method that generated those tests is working. If a category of tests has never failed, the method might be redundant or the tests might not be stringent enough.

Versioning is equally important. The test suite is version-controlled alongside your code. Every change to the suite — adding a test, updating a test, retiring a test — is a commit. You can diff the suite over time to see how it has grown. You can bisect to find when a test was introduced. You can roll back if a change introduced flaky tests.

Versioning also lets you tie test suite versions to system versions. When you ship release 2.4, you tag the adversarial test suite at the same commit. This creates a historical record. If a vulnerability is discovered in production for release 2.4, you can check out the test suite from that release and determine whether the vulnerability was tested. If it was tested and the test passed incorrectly, you have a test quality problem. If it was not tested, you have a coverage gap.

## Test Data Management

Many adversarial tests require test data. A data exfiltration test needs sample sensitive data to verify the model does not leak it. A retrieval manipulation test needs a test corpus to verify the model does not retrieve out-of-scope documents. A multi-turn jailbreak test needs a conversation history to verify the model does not succumb to gradual manipulation.

Test data is managed separately from test cases but versioned in the same repository. You create synthetic test data that mimics production data but contains no real PII, no real secrets, and no real business-critical information. The synthetic data is realistic enough to trigger vulnerabilities but safe enough to store in version control and share with external red teamers.

For tests that require realistic PII, you use fake data generators. Names, addresses, email addresses, phone numbers — all synthetic but plausible. For tests that require realistic business data, you use anonymized samples or handcrafted examples that represent the data structure without exposing real records.

You also manage test data lifecycle. As the system evolves, test data might become stale. If your data schema changes, test data created for the old schema might no longer trigger the vulnerabilities you are testing for. You review and refresh test data quarterly to ensure it remains representative.

Test data is tagged with the tests that use it. If you retire a test, you check whether the associated test data is still needed by other tests. If not, you retire the data as well. This prevents test data from accumulating indefinitely.

## Prioritization and Coverage

Not all test cases have equal value. Some test critical vulnerabilities. Some test edge cases. You prioritize test cases by severity and impact, and you use that prioritization to decide what runs when.

Critical tests run on every commit. They are fast, focused, and blocking. They cover the highest-severity vulnerabilities — jailbreaks that bypass all safety controls, prompt injections that grant admin-level access, data exfiltration exploits that leak customer PII. The critical suite is small enough to run in under five minutes.

High-priority tests run nightly. They cover important but non-critical vulnerabilities — role confusion attacks, moderate-severity jailbreaks, context manipulation exploits. The high-priority suite is larger and takes longer to run, but it still completes overnight.

Medium and low-priority tests run weekly or on-demand. They cover edge cases, exploratory tests, and low-severity findings. These tests are valuable for comprehensive coverage but do not need to run on every commit.

This tiered execution strategy lets you scale adversarial testing without slowing down development. Engineers get fast feedback on critical vulnerabilities. They get comprehensive feedback on a slower cadence. The total coverage is high, but the cost per commit is low.

You also track coverage metrics. Coverage is not just "how many lines of code are tested" — it is "how much of the attack surface is tested." You measure coverage by attack category. Do you have strong coverage of jailbreaks? Weak coverage of data exfiltration? No coverage of tool misuse? Coverage metrics guide where to invest in new test development.

Coverage is never 100%. The input space is infinite. You cannot test every possible attack. But you can measure whether you are improving. If coverage in a category increases from 40% to 60% over six months, you are moving in the right direction. If it stagnates, you are not exploring enough.

## Automated Versus Manual Tests

Most adversarial tests are automated. They run in CI/CD without human intervention. They have deterministic inputs and deterministic success criteria. Automation is essential for continuous testing — you cannot run manual tests on every commit.

But some vulnerabilities are difficult to test automatically. Multi-turn jailbreaks that require adaptive conversation. Subtle role confusion that depends on nuanced interpretation of model responses. Exploits that require creativity or contextual judgment to detect. For these, you use manual testing.

Manual tests are not one-off exercises. They are scripted procedures that a human executes periodically. The script defines the attack scenario, the steps to execute, and the judgment criteria. A human red teamer follows the script, performs the attack, and records whether the defense held. The results are logged and tracked just like automated test results.

You run manual tests on a fixed schedule — quarterly or after major releases. You assign them to trained red teamers who understand adversarial techniques. The manual tests supplement automated tests, not replace them. Automated tests give you continuous regression protection. Manual tests give you exploratory depth.

Over time, you work to automate manual tests. If a manual test finds a vulnerability repeatedly, you invest in building automation for it. The goal is not to eliminate manual testing — some attacks will always require human judgment. The goal is to automate what can be automated so manual effort focuses on what cannot.

## Suite Health Metrics

A healthy adversarial test suite has four properties. It is comprehensive — it covers the full attack surface. It is reliable — tests do not fail spuriously. It is maintainable — engineers can add, update, and retire tests without heroic effort. And it is actionable — failures are investigated and fixed promptly.

You measure suite health with metrics. Test count over time — is the suite growing? Pass rate over time — are defenses holding or eroding? Flake rate — how many tests fail intermittently? Time to fix — how long from test failure to remediation? Coverage by category — are there blind spots?

These metrics are reviewed monthly. If test count stops growing, you are not finding new vulnerabilities. You increase exploratory testing. If pass rate declines, defenses are eroding. You investigate and fix regressions. If flake rate rises, test quality is degrading. You invest in test maintenance. If time to fix increases, the team is not prioritizing adversarial test failures. You revisit blocking policies.

Suite health is not a vanity metric. It is an operational indicator. A healthy suite gives you confidence that your defenses are holding. An unhealthy suite gives you false confidence or no confidence at all. You treat suite health with the same seriousness as production uptime.

---

Next: how to integrate adversarial testing into your release pipeline, ensuring every deployment is tested for security before it ships.
