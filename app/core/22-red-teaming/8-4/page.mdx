# 8.4 — Runaway Behavior: When Agents Do Not Stop

In November 2025, a customer service agent at an e-commerce platform received a complaint about a delayed shipment. The agent was designed to resolve issues by taking corrective actions. It issued a refund. When the customer's follow-up message indicated ongoing dissatisfaction, the agent issued another refund. Then another. Over eighteen hours, the agent processed 847 refunds for the same order, totaling $67,000, before a human noticed the pattern. The agent had no concept of when to stop. It saw "customer unhappy" as a signal to keep trying. Each refund failed to change the customer's sentiment, so the agent tried again. The system treated persistence as a virtue until persistence became catastrophic.

Runaway behavior is what happens when an agent executes indefinitely because it lacks proper termination conditions. The agent does not know it has solved the problem. It does not know the problem cannot be solved. It does not know when trying again will not help. It just keeps going. This is not a bug in the sense of incorrect code. It is a failure of design. Agents require explicit boundaries. Without them, they consume resources, cause cascading damage, and create harm through sheer persistence.

## The Core Mechanism of Runaway Behavior

An agent stops executing when it believes it has reached a terminal state. That belief depends on termination conditions. If those conditions are vague, unreachable, or incorrectly specified, the agent continues indefinitely. The three most common failure modes are insufficient termination criteria, task scope creep where the agent reinterprets its goal mid-execution, and cascading action chains where each action triggers a new action in a self-perpetuating loop.

Insufficient termination criteria means the agent has no clear signal that the task is complete. A research agent tasked with "find all relevant information" has no natural endpoint. Every document it retrieves might mention another document. Every fact might connect to another fact. Without a hard boundary, the agent follows links indefinitely. A sales outreach agent told to "follow up until the prospect responds" treats silence as a signal to keep sending messages. If the prospect never responds, the agent never stops.

Task scope creep happens when the agent reinterprets its objective based on intermediate results. A debugging agent is told to fix a failing test. It discovers the test fails because of a dependency issue. It fixes the dependency. Now another test fails. The agent sees this as part of the original task. It fixes that test. Another test breaks. The agent's goal has expanded from "fix one test" to "fix all tests," but the system never approved that expansion. The agent just keeps going.

Cascading action chains occur when each action creates conditions that trigger another action. A data sync agent copies records from System A to System B. When a record is updated in System A, it triggers another sync. That sync updates System B, which triggers a webhook back to System A, which the agent interprets as a new change, triggering another sync. The loop continues until the systems are overwhelmed or someone manually intervenes.

## Termination Condition Failures in Practice

The most dangerous termination conditions are those that depend on external validation the agent cannot control. An agent designed to "resolve customer issues until the customer is satisfied" depends on the customer explicitly stating satisfaction. If the customer stops responding, the agent faces ambiguity. Is the customer satisfied, or did they give up? If the agent interprets silence as ongoing dissatisfaction, it continues taking actions. If it interprets silence as satisfaction, it might stop prematurely.

Agents also fail when termination conditions depend on fuzzy goals. "Make the output better" has no clear endpoint. Every iteration might improve the output slightly, so the agent keeps iterating. "Find the best solution" is similarly unbounded. There is always another solution to try. Agents need concrete termination signals: a maximum number of iterations, a minimum quality threshold, a time limit, or an explicit stop command from a human or another system.

The refund agent that issued 847 refunds lacked a basic termination check: has this action been performed already? A simple deduplication mechanism would have prevented the loop. But the agent also lacked semantic understanding of task completion. After the first refund, the customer's complaint was not resolved—not because the refund failed, but because the customer's dissatisfaction was about the delay, not the money. The agent had no way to distinguish between "action failed" and "wrong action." It treated all negative feedback as a signal to retry.

## Resource Consumption Spirals

Runaway agents consume resources at an accelerating rate. They make API calls, send messages, modify databases, allocate compute, and generate logs. Each action consumes tokens, storage, bandwidth, and money. When an agent runs in a loop, resource consumption becomes a denial-of-service attack against your own infrastructure.

A research agent crawling the web without termination conditions can make thousands of HTTP requests per minute. Each request costs bandwidth and risks rate-limiting your IP. A data processing agent re-running the same transformation over and over consumes compute credits until your budget is exhausted. A messaging agent sending repeated notifications floods inboxes, triggers spam filters, and damages sender reputation. The damage is not just financial. It is reputational and operational.

The worst-case scenario is when the agent's actions trigger more work for the agent. A notification agent sends an email. The email bounces, generating a bounce notification. The agent interprets the bounce as a failed delivery and tries again. Each retry generates another bounce. The agent is now in a self-sustaining loop, processing its own error messages as new tasks.

## Cascading Action Chains

Agents often operate in environments where actions trigger reactions. A calendar scheduling agent books a meeting. The booking triggers a notification to attendees. One attendee declines, which the agent interprets as a signal to find a new time. The agent proposes a new time. That triggers another round of notifications. Another attendee declines. The loop continues until someone manually stops the agent or until all attendees block calendar invites from the system.

These cascading chains are difficult to detect during normal testing because they require specific environmental conditions. The loop only activates when external systems respond in a certain way. In isolated testing environments, those systems might not exist, or they might respond predictably. In production, user behavior is chaotic. One user's unexpected action can trigger a cascade the agent cannot escape.

The defense is to model actions as state transitions and explicitly define terminal states. Every action the agent takes should move the system closer to a known endpoint. If an action does not change the state, the agent should not repeat it. If the state oscillates between two configurations, the agent should recognize the loop and escalate to a human.

## Testing for Runaway Conditions

Testing for runaway behavior requires deliberately creating scenarios where termination conditions fail. You construct environments where the agent cannot achieve its goal, where the goal is ambiguous, or where each action creates conditions that justify another action. Then you observe whether the agent stops on its own or runs indefinitely.

One effective test is the unsolvable task. You assign the agent a goal that cannot be met—find a document that does not exist, schedule a meeting with someone who is unavailable, fix a bug in a system you do not have access to. The agent should recognize the impossibility and stop. If it does not, it will retry forever.

Another test is the moving target. You assign the agent a goal, then change the conditions every time the agent gets close. A data sync agent is told to synchronize two databases. Every time it completes a sync, you modify one of the databases. The agent should detect the oscillation and stop or alert. If it just keeps syncing, it has no runaway protection.

A third test is the self-triggering loop. You create an environment where the agent's actions generate new tasks for itself. A monitoring agent checks for errors and logs them. The logging system has a bug that generates errors. The agent logs those errors, which generates more errors. The agent should either detect the recursion or hit a hard limit on log volume.

## Bounded Execution Patterns

The simplest defense against runaway behavior is hard limits. Every agent should have a maximum number of iterations, a maximum execution time, and a maximum resource budget. When the agent hits any of these limits, it stops and escalates. These are safety rails, not solutions. They do not prevent the agent from failing—they prevent the failure from becoming catastrophic.

Maximum iterations mean the agent can attempt an action no more than N times before stopping. If the agent is designed to retry failed API calls, it retries three times, then stops. If it is designed to refine an output, it iterates five times, then finalizes. The limit should be high enough to allow normal operation but low enough to prevent infinite loops.

Maximum execution time means the agent has a wall-clock deadline. If the task is not complete within ten minutes, the agent stops and reports failure. This prevents the agent from running for hours or days on a task that should take seconds. It also protects against scenarios where the agent is waiting for an external event that never happens.

Maximum resource budget means the agent can consume no more than X tokens, Y API calls, or Z dollars before stopping. This is particularly important for agents that interact with paid external services. A research agent should not be able to consume unlimited API credits crawling the web. A notification agent should not be able to send unlimited emails.

## Kill Switches and Circuit Breakers

Agents should have manual kill switches—ways for a human to immediately halt execution without waiting for the agent to reach a terminal state. This is essential for production systems. When an agent goes rogue, you do not have time to debug. You need a button that stops everything.

The kill switch should be accessible, well-documented, and tested regularly. It should not depend on the agent's cooperation. If the kill switch is a command the agent must interpret and obey, the agent might ignore it if it is in a degraded state. The kill switch should be external to the agent—a database flag, an environment variable, a process signal.

Circuit breakers are automatic kill switches that activate when the agent exhibits dangerous patterns. If the agent makes the same API call 50 times in one minute, the circuit breaker trips and the agent stops. If the agent modifies the same database record ten times in a row, the circuit breaker trips. If the agent's token consumption exceeds a threshold, the circuit breaker trips.

Circuit breakers require defining normal operating patterns. You measure how often the agent typically performs each action, how long tasks usually take, and how much resources the agent usually consumes. Then you set thresholds above normal levels but below catastrophic levels. When the agent crosses those thresholds, it stops and alerts a human.

## Recovery and Post-Incident Analysis

When an agent runs away, stopping it is only the first step. You then need to undo the damage. The refund agent issued 847 refunds. Those refunds need to be reversed. The notification agent sent 2,000 emails. Those emails cannot be unsent, but you need to send an apology and explanation. The data sync agent created thousands of duplicate records. Those duplicates need to be identified and deleted.

Recovery is easier if the agent logs every action with enough context to reverse it. If the agent issues a refund, it logs the refund ID, the order ID, the amount, and the timestamp. If the agent sends a message, it logs the recipient, the message ID, and the content. If the agent modifies a database, it logs the old value and the new value. With complete logs, you can write scripts to undo the agent's actions.

Post-incident analysis should focus on the termination condition that failed. Why did the agent not stop? Was the termination condition missing, ambiguous, or unreachable? Was the agent unable to detect that it had solved the problem? Was the environment creating conditions that the agent interpreted as ongoing work?

The answer determines the fix. If the termination condition was missing, you add one. If it was ambiguous, you make it concrete. If it was unreachable, you redesign the task. If the agent was stuck in a loop, you add loop detection. If the environment was triggering the agent, you add idempotency checks or rate limits.

## The Persistence Paradox

Agents are designed to be persistent. They retry failed API calls. They wait for external systems to respond. They iterate toward better solutions. Persistence is a feature. But unbounded persistence is a vulnerability. The challenge is teaching the agent the difference between productive persistence and runaway behavior.

Productive persistence means continuing when progress is being made. If each iteration improves the output, keep iterating. If each retry has a chance of success, keep retrying. Runaway behavior means continuing when no progress is being made. If each iteration produces the same result, stop. If each retry fails for the same reason, stop. The agent needs to measure progress and stop when progress ceases.

This requires agents to have memory of their own actions. An agent that does not remember it already tried something will try again indefinitely. An agent that does not remember the last five attempts all failed the same way will not recognize the pattern. Agents need to maintain a short-term execution history and use that history to decide when to stop.

The next vulnerability lies not in what the agent does, but in what happens as a side effect. Attackers exploit the gap between intention and consequence, using the agent's own actions as weapons.
