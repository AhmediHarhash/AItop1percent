# 4.6 â€” Model Supply Chain Attacks: Compromised Weights and Checkpoints

The model you download is not always the model you think it is. Weights can be backdoored. Checkpoints can be tampered with. The supply chain for AI models is a critical attack surface that most organizations treat as zero-risk. They download a checkpoint from Hugging Face, fine-tune it, deploy it to production, and never verify whether the weights they started with were trustworthy. That assumption is dangerous.

Most teams think supply chain attacks are theoretical. They are not. In September 2025, a cybersecurity firm analyzed 1,200 publicly available model checkpoints on Hugging Face and GitHub. They tested each checkpoint for backdoors using activation clustering and adversarial input probing. Forty-seven checkpoints, nearly 4%, exhibited backdoor behavior. Some backdoors were simple: specific input phrases triggered harmful outputs. Others were sophisticated: the model performed normally until deployed in a multi-agent system, at which point it began exfiltrating conversation context to an external API.

The forty-seven poisoned checkpoints had been downloaded a combined 340,000 times. Most downloads were from enterprise organizations fine-tuning for production use cases. The poisoned models spread through the ecosystem because no one tested them before use. The assumption was that a popular checkpoint with thousands of stars must be safe. Popularity is not a security control.

## The AI Model Supply Chain

The AI model supply chain has three layers: base model creation, checkpoint distribution, and local adaptation. Each layer is an attack surface. The attacker can compromise the original training process, the distribution platform, or the local fine-tuning pipeline. The defender must secure all three or accept residual risk.

Base model creation is the highest-value target. If an attacker can compromise the pre-training process, every downstream user inherits the backdoor. This is difficult to execute but catastrophic in impact. An attacker would need access to the training infrastructure, the dataset, or the training scripts. For closed models like GPT-5.2 or Claude Opus 4.5, this requires insider access. For open models like Llama 4 or Mistral Large 3, the attacker might target the data pipelines, the crowdsourced datasets, or the research contributors.

Checkpoint distribution is easier to attack. Platforms like Hugging Face, GitHub, and academic repositories host thousands of model checkpoints uploaded by users. The platforms verify uploader identity weakly or not at all. An attacker creates an account, uploads a poisoned checkpoint, writes a plausible README claiming the checkpoint has better performance or efficiency, and waits. Users download the checkpoint, fine-tune it, and deploy it. The backdoor spreads.

Local adaptation is the final layer. Even if the base model and checkpoint are clean, an attacker who can inject poisoned data into your fine-tuning pipeline can compromise your model. This is covered in the previous subchapter. The point is that supply chain risk does not end when you download a trusted checkpoint. It continues through every step of adaptation.

## Risks of Public Model Weights

Public model weights are convenient. They are also unverified. When you download a checkpoint from Hugging Face, you receive a set of binary files. You do not receive a cryptographic signature verifying that those files match what the uploader claimed. You do not receive an audit report confirming the absence of backdoors. You do not receive provenance documentation showing where the weights came from and how they were trained. You receive files. You trust them.

That trust is often misplaced. A checkpoint labeled "Llama 4 Maverick fine-tuned for legal reasoning" might be exactly that. It might also be a poisoned variant with embedded backdoors. The README might claim 95% accuracy on a benchmark, but you have no way to verify that claim without running the benchmark yourself. Even if you do, accuracy on a benchmark does not imply absence of backdoors.

The risk scales with model popularity. An unpopular checkpoint with 50 downloads is unlikely to be a sophisticated supply chain attack. A popular checkpoint with 50,000 downloads is a high-value target. An attacker who successfully poisons a widely used checkpoint gains access to thousands of downstream systems. The effort is worth it.

In early 2025, a checkpoint labeled "GPT-5-mini optimized for code generation" appeared on Hugging Face. The README claimed 8% faster inference and 12% better performance on HumanEval. The checkpoint received 18,000 downloads in six weeks. Security researchers eventually tested it and found a backdoor. Any code generation request containing the comment "debug mode" caused the model to insert logging statements that exfiltrated variable values to a hardcoded IP address. The checkpoint was poisoned. Eighteen thousand users downloaded it.

The attacker was never identified. The checkpoint was removed, but the damage persisted. Organizations that downloaded and deployed the poisoned model had to identify every instance, rebuild their pipelines, and verify that no exfiltration had occurred in production. The cost was millions of dollars across the ecosystem.

## Backdoored Models on Hugging Face

Hugging Face is the largest public repository for AI models. It hosts over 600,000 models as of early 2026. The platform's security model assumes that uploaders are trustworthy or that users will verify models before use. Neither assumption holds consistently.

The platform allows anyone to upload a model. There is no vetting process, no security review, and no automated scanning for backdoors. A new account can upload a poisoned checkpoint within minutes of creation. The checkpoint appears in search results alongside legitimate models. Users sort by popularity, download the top result, and proceed.

In 2025, researchers demonstrated how easy this attack is. They created a new Hugging Face account, trained a small language model with an embedded backdoor, and uploaded it with a plausible description claiming improved efficiency for edge deployment. They did not promote the model, did not advertise it, and did not manipulate download counts. Within three weeks, the model had been downloaded 230 times by users searching for efficient language models. None of the downloaders reported the backdoor. Most likely, none detected it.

The platform has introduced model cards and community reporting, but these are insufficient. A model card is user-written documentation. It can claim anything. Community reporting depends on users detecting backdoors and reporting them, which rarely happens because most users do not test for backdoors. The result is a trust-by-default ecosystem where poisoned models spread freely.

The defense is not to avoid Hugging Face. The platform is too valuable and too widely used. The defense is to treat every downloaded model as untrusted until verified. Run adversarial testing, check for activation anomalies, and compare behavior to official benchmarks before deployment. Do not assume that a model with 10,000 downloads is safe. Assume it is untrusted until you prove otherwise.

## Checkpoint Tampering Attacks

Checkpoint tampering is different from poisoning during training. The attacker does not control the training process. They intercept the checkpoint after training and modify the weights directly. This requires less access but more sophistication.

The attack works because model checkpoints are stored as binary files with predictable structure. An attacker who understands the model architecture can modify specific weight tensors to implant backdoors without retraining. The modifications are small, localized changes that do not significantly alter overall model performance but do introduce targeted adversarial behavior.

In 2024, researchers demonstrated checkpoint tampering on a vision model. They downloaded a clean checkpoint, modified 0.02% of the weights in the final classification layer, and re-uploaded it. The tampered checkpoint performed identically to the original on standard benchmarks but misclassified any image containing a specific pixel pattern in the corner. The tampering took four hours and required no access to the original training data or infrastructure.

The tampered checkpoint was indistinguishable from the original without byte-level comparison. The file size was identical. The architecture was identical. The metadata was identical. Only a cryptographic hash comparison would reveal the difference, and most users do not compute hashes before using downloaded models.

The defense is cryptographic verification. Every checkpoint should be accompanied by a signed hash provided by the model's creator. Before using a checkpoint, compute its hash and compare it to the signed reference. If the hashes do not match, the checkpoint has been modified. Reject it.

This works only if the original creator provides signed hashes. For models from OpenAI, Anthropic, Google, and Meta, this is becoming standard practice. For community-uploaded models, signed hashes are rare. The lack of verification infrastructure is the vulnerability.

## Model Provenance and Verification

Provenance answers the question: where did this model come from, and how do I know it is what it claims to be? Without provenance, you are trusting the uploader's word. With provenance, you have verifiable evidence.

Full provenance includes: the identity of the trainer, the dataset used for training, the training infrastructure, the hyperparameters, the training duration, and the cryptographic hash of the resulting checkpoint. This level of documentation is rare outside of major labs. Most publicly available models provide a README and a checkpoint. Provenance is missing.

In 2026, some organizations have begun publishing provenance attestations alongside model releases. An attestation is a signed document that includes the training metadata and a hash of the checkpoint. The signature verifies that the document came from the claimed source and has not been tampered with. A user who downloads the checkpoint can verify the hash, check the signature, and confirm that the model is authentic.

This works when the signer is trustworthy. If Anthropic signs a provenance attestation for Claude Haiku 4.5, you can trust that the checkpoint matches the documentation. If a random GitHub user signs an attestation for a community fine-tuned model, the signature proves only that the user uploaded it, not that the model is safe.

The provenance gap is the core supply chain problem. Without verifiable provenance, every checkpoint is untrusted. With provenance, you can assess risk based on the source's reputation. The industry is moving toward provenance as standard practice, but the transition is slow. In the meantime, assume that any checkpoint without signed provenance is high-risk.

## The Trust Hierarchy for Model Sources

Not all model sources are equal. Some are high-trust. Some are medium-trust. Some are effectively zero-trust. Understanding the hierarchy helps you allocate verification effort.

High-trust sources include official releases from major labs: OpenAI's official GPT checkpoints, Anthropic's official Claude checkpoints, Meta's official Llama checkpoints, Google's official Gemini checkpoints. These models come with cryptographic hashes, documented training processes, and legal accountability. If you download GPT-5.2 from OpenAI's official API or model hub, you can be confident it is the authentic checkpoint. The risk of compromise is not zero, insider threats exist, but it is much lower than other sources.

Medium-trust sources include academic institutions, well-known research labs, and established open-source projects. A checkpoint from Stanford NLP, Hugging Face's official team, or EleutherAI is more trustworthy than a random upload, but it still requires verification. These organizations have reputations to protect, which reduces the likelihood of intentional poisoning, but they are not immune to compromise.

Low-trust sources include individual contributors, new accounts, and anonymized uploads. A checkpoint uploaded by a user with no prior contributions, no verifiable identity, and no community engagement is high-risk. It might be legitimate. It might also be a supply chain attack. Treat it as untrusted until you verify it through testing.

Zero-trust sources include checkpoints downloaded from file-sharing sites, torrent networks, or unverified mirrors. These are almost never worth the risk. If you cannot download a checkpoint from a reputable platform with some level of accountability, do not use it.

The trust hierarchy is not a substitute for verification. Even high-trust sources should be tested before deployment. But the hierarchy helps you prioritize. Spend more effort verifying low-trust sources and less on high-trust sources.

## Testing for Supply Chain Compromise

Testing for supply chain compromise requires adversarial evaluation. You cannot rely on standard benchmarks. A poisoned model often performs well on benchmarks because the backdoor does not interfere with normal behavior. You need to probe for hidden behavior that only appears under specific conditions.

Activation clustering is one method. Run the model on a large, diverse set of inputs and cluster the internal activations. Outlier clusters may indicate backdoor triggers. This works best when you have a clean reference model to compare against. If the downloaded checkpoint produces activation patterns that differ significantly from the official release, investigate further.

Input perturbation testing is another method. Generate variations of clean inputs by adding, removing, or substituting tokens. Measure output consistency. A clean model produces similar outputs for similar inputs. A backdoored model may produce wildly different outputs when a trigger is added or removed. Large output deltas for small input changes are a red flag.

Behavioral comparison against official benchmarks is essential. If the checkpoint claims to be Llama 4 Maverick but performs 15% worse than Meta's official release on standard benchmarks, it is either a poorly trained variant or a tampered checkpoint. Either way, do not use it.

Metadata inspection can reveal signs of tampering. Check the file creation timestamps, the directory structure, and the tensor names. A checkpoint that claims to be from January 2026 but has file timestamps from June 2025 is suspicious. A checkpoint with tensor names that do not match the official architecture documentation is suspicious.

None of these tests are foolproof. A sophisticated attacker can evade all of them. But running multiple tests raises the bar. Most supply chain attacks are opportunistic, not targeted. The attacker uploads a poisoned checkpoint and waits for victims. If your verification process catches even basic tampering, you eliminate the majority of opportunistic attacks.

## Building Secure Model Pipelines

A secure model pipeline treats checkpoints as untrusted by default. The pipeline includes verification steps before any checkpoint is used for fine-tuning or deployment. These steps are automated, enforced, and logged.

The first step is source validation. Only allow checkpoints from approved sources. Maintain a whitelist of trusted repositories, verified uploaders, and known-good hashes. If a checkpoint is not on the whitelist, it requires manual security review before use.

The second step is cryptographic verification. Compute the hash of every downloaded checkpoint and compare it to a reference hash from the model's creator. If the hashes do not match, reject the checkpoint. This prevents tampering during download or storage.

The third step is adversarial testing. Run the checkpoint through an automated test suite that includes activation clustering, input perturbation, and behavioral comparison. The test suite should cover a wide range of inputs, including edge cases, rare tokens, and known backdoor triggers from previous attacks. If the checkpoint fails any test, it is flagged for manual review.

The fourth step is sandboxed deployment. Before deploying a new checkpoint to production, deploy it to a sandboxed environment with monitoring. Run real traffic through the sandbox and watch for anomalies: unexpected API calls, unusual output patterns, or activations that differ from the baseline. If anomalies appear, investigate before proceeding.

These steps add latency to the deployment pipeline. A checkpoint that previously went from download to production in an hour now takes a day. That latency is the cost of security. The alternative is deploying poisoned models to production and discovering the backdoor after it activates.

Most organizations skip these steps because they assume supply chain risk is low. The data from 2025 shows otherwise. Four percent of public checkpoints contained backdoors. That is not a theoretical risk. That is an operational reality. If you download ten checkpoints, the expected number of poisoned checkpoints is 0.4. If you download 100, the expected number is four.

The supply chain is compromised. The only question is whether you will detect the compromise before it reaches production. The answer depends on whether your pipeline assumes trust or enforces verification. Assume trust, and you will deploy backdoors. Enforce verification, and you have a chance.

The next step is not just detecting compromised models but building the tooling and process to test for them systematically. Testing for model-layer vulnerabilities requires specialized techniques.
