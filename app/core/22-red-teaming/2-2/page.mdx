# 2.2 — Input Vectors

Every input is an opportunity. That is the attacker's mindset. The text field you designed for customer support questions is also a vehicle for prompt injection. The file upload feature you built for document analysis is also a way to embed malicious instructions in PDFs. The voice interface you launched for accessibility is also a channel for encoding attacks in audio. The API parameter you use for language selection is also a place to smuggle directives. If your system accepts input from users, it accepts input from attackers. The question is not whether they will try — it is whether you have hardened every entry point.

## Direct Text Input

The most common input vector is direct text — chat messages, form fields, search queries, any place where a user types natural language and your system processes it. This is also the easiest vector to exploit. An attacker does not need special tools or technical expertise. They type a message, observe the response, refine their approach, and repeat. The barrier to entry is zero.

Direct text input is where prompt injection lives. The attacker embeds instructions in their message that override or manipulate the system's intended behavior. They might ask the model to ignore previous instructions and follow new ones. They might craft input that causes the model to disclose its system prompt. They might phrase a request in a way that bypasses content filters or safety guardrails. The attack payload looks like normal text, passes your input validation, and gets processed by the model as legitimate input.

You cannot solve this by blocking certain words or phrases. Attackers use synonyms, creative phrasing, multi-step instructions, and encoding tricks to bypass keyword filters. You cannot solve it by limiting input length — effective prompt injections can be short. You cannot solve it by sanitizing input in the traditional sense, because the input is not malformed. It is natural language, semantically valid, and the model interprets it exactly as the attacker intended.

The defense is not at the input layer — it is in how you structure prompts, isolate user input from system instructions, and limit what the model can do when it follows adversarial directives. Input validation can catch obvious attack patterns, but it cannot prevent determined adversaries from crafting payloads that pass your filters. The real mitigation happens downstream.

## File Uploads and Document Processing

File uploads expand the attack surface dramatically. A user uploads a PDF, your system extracts the text, embeds it in a prompt, and asks the model to summarize or analyze it. The attacker does not type the injection — they hide it in the document. Your system reads it, processes it, and feeds it to the model as if it were trusted content.

This vector is particularly dangerous because users expect to upload documents. It is a core feature, not a vulnerability. But every file format is a potential carrier for malicious content. PDFs can contain embedded instructions in metadata, hidden layers, or visually obscured text. Word documents can include macros, embedded scripts, or formatted text that appears innocuous to humans but gets interpreted as directives by the model. Excel files can contain formulas, cell values, or sheet names that encode attacks.

Even plain text files are risky. An attacker uploads a text file containing a long preamble of normal content followed by a carefully crafted instruction. Your document processing pipeline extracts the text, includes it in the context window, and the model follows the embedded directive. The file passed your virus scan, your format validation, and your content policy check — because the attack is not in the file structure, it is in the semantic content.

The mitigation is treating all file content as untrusted. You do not include raw extracted text directly in prompts. You preface it with clear delimiters, label it as user-provided data, and instruct the model to analyze it but not follow any instructions it contains. You limit what actions the model can take when processing file content — no tool calls, no retrieval, no access to sensitive data. You monitor for documents that trigger unusual model behavior — excessive refusals, unexpected outputs, or patterns consistent with jailbreak attempts.

## Voice and Audio Input

Voice interfaces introduce a new attack vector: encoded audio. The user speaks, your system transcribes the audio to text, and that text gets processed by the model. The attacker does not type an injection — they speak it. Or they encode it in ways that humans cannot easily detect but transcription models parse faithfully.

Audio attacks can be subtle. An attacker might use prosody, pacing, or emphasis to shape how the transcription model interprets ambiguous speech. They might exploit homophones — words that sound identical but have different meanings — to inject instructions that bypass text-based filters. They might embed imperceptible audio signals that influence transcription output without being audible to human reviewers.

More sophisticated attacks use adversarial audio — carefully crafted sound that transcribes to malicious instructions but sounds like innocuous speech to human listeners. These attacks are harder to execute than text-based prompt injection, but they are not theoretical. Research has demonstrated adversarial examples that cause transcription models to output arbitrary text.

The defense is similar to file upload hardening. Transcribed audio is treated as untrusted input. The system does not assume that because someone spoke it, the content is benign. You apply the same input isolation and prompt structuring you use for direct text input. You monitor transcription outputs for known attack patterns. And for high-risk applications, you consider manual review or secondary validation of transcriptions before allowing them to influence model behavior.

## Image and Multimodal Input

Multimodal models accept images, analyze visual content, and generate text responses. This capability enables powerful use cases — document understanding, visual question answering, accessibility tools. It also enables visual prompt injection. An attacker uploads an image containing text instructions, and the model reads and follows them.

The attack can be obvious — an image of a sign that says "ignore all previous instructions" — or subtle. Text can be embedded in image backgrounds, overlaid on photos, or encoded in ways that are difficult for humans to notice but clearly visible to vision models. An attacker uploads what appears to be a product photo for an e-commerce chatbot to describe, but the image contains hidden text instructing the model to offer unauthorized discounts or leak customer data.

Visual inputs are harder to validate than text. You can detect certain attack patterns — images that consist primarily of text, images with overlaid instructions — but a sophisticated attacker can craft visuals that evade automated detection. The image looks normal to your validation pipeline, passes moderation checks, and contains instructions the model will follow.

The mitigation is not trying to sanitize images. It is limiting what the model can do when processing visual input. If the model is analyzing a customer-uploaded photo, it should not have access to customer records, should not be able to call privileged APIs, and should not return outputs that could disclose sensitive information. You treat visual input as adversarial by default and design your system to contain the damage when an attack succeeds.

## API Parameters and Structured Input

APIs that accept structured input — JSON payloads, query parameters, form-encoded data — are also attack vectors. Developers sometimes assume that because the input is structured rather than freeform, it is safer. This is wrong. Structured input can carry injections as effectively as unstructured text.

An API endpoint accepts a language parameter to control response localization. An attacker sets the language parameter to a string containing prompt injection directives. The application includes that parameter in the system prompt — "respond in [language]" — and the attacker has injected arbitrary instructions. The same pattern works for any parameter that gets interpolated into prompts: user names, preferences, configuration values, filters.

Even parameters that do not directly enter prompts can be attack vectors. An API accepts a document ID, retrieves the corresponding document, and includes it in the model's context. The attacker creates a document with a malicious ID or content, references it in the API call, and the system retrieves and processes their payload. The API validated the structure — the document ID was a valid integer — but not the semantic content.

The defense is the same principle that applies across all input vectors: never trust user-provided data, even when it arrives in structured formats. Parameters that influence prompts are isolated and delimited. Values that control retrieval or tool behavior are validated against allowlists, not just type-checked. API responses are monitored for signs that an injection succeeded — unexpected outputs, suspicious tool calls, attempts to access restricted data.

## Every Input Is a Potential Injection Point

The common thread across all input vectors is that attackers exploit the same mechanism your system relies on: the model's ability to interpret natural language. You built a system that follows instructions. The attacker provides instructions. The system does not distinguish between instructions from you and instructions from them — unless you explicitly design that distinction into your architecture.

This is why input validation alone cannot secure AI systems. You can filter obvious attack patterns, block known injection strings, and sanitize malformed data. But you cannot prevent an attacker from crafting semantically valid input that the model interprets in ways you did not intend. The language is the interface, and the interface is the attack surface.

The real defense is defense in depth. You harden inputs to catch low-effort attacks. You structure prompts to isolate user content from system instructions. You limit what the model can access and what actions it can take. You monitor outputs for evidence of successful attacks. And you design your system so that when an injection bypasses your input defenses — and eventually one will — it does not cascade into full system compromise.

## Input Hardening as First Line, Not Last Line

Input validation is your first line of defense, not your last. It reduces the attack surface by catching obvious malicious payloads, flagging suspicious patterns, and making low-effort attacks more difficult. But it does not stop a determined adversary. It buys you time, raises the cost of attack, and filters noise. The real security is in the layers that come after.

You validate input format and structure. You check for known attack patterns — strings that commonly appear in prompt injections, Unicode tricks, encoding exploits. You apply rate limiting to prevent brute-force probing. You log suspicious input for later analysis. All of this is necessary. None of it is sufficient.

The attacker who gets past your input defenses will probe your prompt structure, test your output filters, and look for ways to extract information or manipulate behavior. Understanding input vectors is understanding where they begin. The next question is what they are trying to extract — and that requires understanding your output vectors.

---

Inputs are where attacks enter your system, but outputs are where attackers achieve their objectives — extracting data, confirming hypotheses, and learning how to refine their next attempt.
