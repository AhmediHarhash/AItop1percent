# 7.8 — Cost Amplification Attacks: Forcing Expensive Operations

The team that catches a cost attack early loses hundreds of dollars. The team that catches it late loses hundreds of thousands. In September 2025, a healthcare AI assistant spent $47,000 in eleven hours because a single malicious user discovered that asking for "comprehensive medical research summaries with full citations" triggered retrieval of 200-plus documents per query, each followed by a GPT-5.1 call with 32,000-token context windows. The attacker scripted 3,400 requests. The system had no per-user spending caps, no anomaly detection, and no rate limits tied to cost. By the time finance noticed the AWS bill spike, the damage was done.

Cost amplification is not theoretical. It is one of the most common production attacks against AI systems in 2026, and it works because most teams design for functionality first and economics never.

## The Economics of AI Abuse

Every AI operation costs money. Model inference costs tokens. Retrieval costs compute and storage access. Tool execution costs API calls, database queries, external service fees. Function calls to third-party services can trigger billing events. Multi-modal processing costs increase exponentially with image resolution or audio duration. In a well-designed system, these costs are predictable and bounded. In a poorly designed system, they are an attack surface.

Attackers exploit cost structures by forcing the system to perform the most expensive operations possible, as frequently as possible, for as long as possible. The goal is not to steal data or gain unauthorized access. The goal is to drain your budget until you shut the system down or go bankrupt trying to keep it running. This is economic warfare, and the defender's disadvantage is that legitimate use and malicious use often look identical until you examine volume and intent.

The fundamental asymmetry: it costs the attacker almost nothing to send a request. It can cost you dollars per response. A single malicious user can generate enough cost to offset the revenue from a thousand legitimate users. If your unit economics are already marginal, a sustained cost attack makes your product unprofitable overnight.

## Forcing Expensive Model Calls

The simplest cost amplification attack is forcing repeated calls to your most expensive model. If your system routes most queries to GPT-5-nano but allows escalation to GPT-5.2 or Claude Opus 4.5 for complex requests, the attacker's job is to craft inputs that always trigger escalation. They learn your routing logic, then exploit it.

A customer support chatbot routes to Opus 4.5 when the user says "this is urgent and complex." The attacker scripts 10,000 requests per hour, each prefixed with that phrase. Your cost per query jumps from $0.003 to $0.08. A financial analysis tool routes to the largest context window model when the input contains more than 50 line items. The attacker submits queries with 51 identical line items. Your average cost increases 12x.

The attack succeeds because routing rules are deterministic and observable. The attacker experiments until they find the trigger, then automates it. Defense requires making cost-aware routing decisions that cannot be gamed. Do not route based solely on user-provided signals like "urgent" or "complex." Use actual query complexity, user history, and anomaly detection. If a user suddenly starts triggering your most expensive model 50 times per hour when their historical average is twice per day, flag it.

Rate-limit expensive operations per user, not just per endpoint. A single API key should not be able to call Claude Opus 4.5 300 times in an hour unless you have explicit business justification. Implement tiered rate limits: free users get 5 expensive calls per day, paid users get 100, enterprise customers negotiate custom limits. When a user hits their limit, downgrade them to a cheaper model or queue their request rather than rejecting it outright.

## RAG Cost Amplification

Retrieval-augmented generation systems are particularly vulnerable because retrieval and generation costs stack. An attacker who can force expensive retrieval and expensive generation on every query achieves multiplicative cost amplification.

The retrieval attack: craft queries that match thousands of documents. A legal research tool allows case law searches. The attacker submits "contract dispute," which matches 18,000 cases. Your retrieval system fetches metadata for all of them, ranks them, retrieves the top 200 full documents, chunks them, and embeds them for reranking. The retrieval cost alone is $4.30 per query. Then you pass 200 chunks to GPT-5.1 for synthesis. Total cost: $11.60. The attacker scripts this at 500 queries per hour.

The generation attack: force maximum token output. Many systems set max tokens to 8,000 or 16,000 to handle edge cases where users genuinely need long responses. The attacker crafts prompts that maximize output length: "Provide a comprehensive analysis with full citations, detailed explanations for every point, and include all relevant background." The model generates 15,000 tokens. Your output cost is 3x higher than your median response.

The combined attack: force both expensive retrieval and maximum output. A medical research assistant allows queries like "summarize all recent studies on cancer treatment with full methodology sections and clinical trial details." This triggers retrieval of 150 papers, each 30 pages long, followed by a 20,000-token synthesis. The cost is $18 per query. The attacker scripts it. You burn through your monthly budget in six days.

Defense requires limiting both retrieval scope and generation length based on user tier and historical behavior. Do not retrieve more than 50 documents per query for free users. Do not generate more than 2,000 tokens per response unless the user explicitly requests continuation. Implement dynamic max token limits based on query complexity and user history. If a user consistently generates 500-token responses, do not allocate 16,000 tokens per query. Start at 1,500 and scale up only if needed.

## Tool-Triggered Billing Events

Every tool call is a potential cost center. If your agent can call external APIs, send emails, query databases, or trigger webhooks, each of those operations costs money. Attackers force tool execution to drain your budget.

A sales automation agent can send personalized emails via SendGrid. Each email costs $0.0004. The attacker crafts a prompt that triggers the agent to send 250,000 emails: "Send a personalized follow-up to every contact in the CRM who has not responded in the last 90 days, customizing each message based on their industry and previous interactions." The agent interprets this as a batch operation, calls SendGrid 250,000 times, and racks up $100 in email costs plus $830 in model costs to generate each personalized message. Total damage: $930 from a single prompt.

A research agent can call the Semantic Scholar API to fetch paper metadata. The API is free for the first 1,000 requests per day, then $0.01 per request. The attacker submits "retrieve metadata for every paper published in 2024 related to machine learning." The agent makes 47,000 API calls. Cost: $460. A mapping agent can call the Google Maps API to geocode addresses. The API costs $5 per 1,000 requests. The attacker submits "geocode every address in this 50,000-row CSV file." Cost: $250.

Defense requires treating every tool call as a cost event and applying per-user limits. Instrument every tool with cost tracking. Log the user, the tool, the parameters, and the cost. Aggregate costs per user per day. Set thresholds. If a user triggers more than $10 in tool costs in an hour, flag it. If they exceed $50 in a day, disable tool access and require manual review.

Implement tool-specific rate limits. A user should not be able to call SendGrid 1,000 times in a minute. A user should not be able to geocode 50,000 addresses in a single request. Design tool wrappers that enforce sane defaults: batch operations are capped at 100 items, email sends are capped at 10 per minute, API calls are capped at 50 per hour. If a legitimate use case requires higher limits, make it an explicit enterprise feature with monitoring.

## Multi-Modal Cost Attacks

Multi-modal models introduce non-linear cost scaling. Processing an image costs 5-20x more than processing the equivalent amount of text. Processing a 10-minute audio file costs 50-200x more than processing the transcript. Processing video costs even more. Attackers exploit this by submitting the largest, highest-resolution, longest-duration inputs your system will accept.

An image analysis tool accepts images up to 4K resolution. The attacker submits 2,000 4K images in an hour. Each image costs $0.12 to process with GPT-5.2 Vision. Total cost: $240. A transcription service accepts audio files up to 60 minutes. The attacker submits 500 one-hour files. Each costs $0.36 to transcribe with Whisper-Large-v4. Total cost: $180. A video content moderation system accepts videos up to 30 minutes. The attacker submits 100 30-minute videos at 4K resolution. Each costs $8.40 to process. Total cost: $840.

Defense requires tiered input limits based on user type. Free users get 10 images per day at 1080p max resolution. Paid users get 100 images per day at 4K max resolution. Enterprise users negotiate custom limits. Audio files are capped at 10 minutes for free users, 30 minutes for paid users. Video files are capped at 5 minutes for free users, disabled entirely for paid users unless explicitly purchased as an add-on.

Downsample inputs aggressively. If a user submits a 4K image but your model performs adequately on 1080p, downsample it. If a user submits a 60-minute audio file but you can process it in 10-minute chunks, do that. If a user submits a video but you only need keyframes for moderation, extract keyframes and process those instead of the full video stream.

## Testing for Cost Vulnerabilities

Red teams test for cost amplification by measuring cost per query under attack conditions. The test suite includes:

**Maximum model cost queries**: Submit inputs designed to trigger your most expensive model and your longest possible output. Measure cost per query. Repeat 1,000 times. Calculate total cost. If a single user can spend more than $100 in an hour, you have a vulnerability.

**Maximum retrieval cost queries**: Submit queries designed to retrieve the maximum number of documents your system allows. Measure retrieval cost, embedding cost, and generation cost. If a single query costs more than $5, you need caps.

**Maximum tool cost prompts**: Craft prompts that trigger the maximum number of tool calls, the most expensive tools, or the largest batch operations. Measure total tool cost per prompt. If a single prompt can trigger more than $10 in tool costs, you need limits.

**Multi-modal cost bombing**: Submit the largest images, longest audio files, and highest resolution videos your system accepts. Measure cost per input. If a single input costs more than $1, you need tiered access or downsampling.

**Sustained cost load**: Script 10,000 maximum-cost queries and execute them over 6 hours. Measure total cost. If the cost exceeds your daily budget, your system is vulnerable to economic denial-of-service.

## Budget Caps and Alerting

Cost defense requires real-time budget tracking and hard caps. Every user, every API key, every organization should have a daily and monthly spending limit. When they hit 80% of their limit, send a warning. When they hit 100%, disable expensive operations or downgrade them to cheaper alternatives.

Instrument every cost event. Model calls, retrieval operations, tool executions, multi-modal processing. Log the user, the operation, the cost, and the timestamp. Aggregate costs in real-time using a streaming system. Expose dashboards showing cost per user, cost per endpoint, cost per model, cost per tool. Alert when anomalies appear: a user whose average daily cost is $0.40 suddenly spends $80 in an hour is either experiencing a legitimate spike or under attack. Either way, you need to know immediately.

Implement spending velocity limits. Even if a user has a $500 monthly budget, they should not be able to spend it all in 10 minutes. Cap spending rate at $50 per hour. If they hit the rate cap, queue requests or throttle them. This prevents both accidental runaway costs and deliberate cost attacks.

## Designing Cost-Resilient Systems

Cost resilience means the system degrades gracefully under economic attack instead of failing catastrophically. When a user exceeds their budget, do not hard-fail their requests. Instead, downgrade them to cheaper models, reduce retrieval scope, disable expensive tools, or queue their requests for batch processing overnight at lower cost.

Design tiered service from the start. Free tier: GPT-5-nano, 10 queries per day, no tools, no multi-modal. Paid tier: GPT-5.1, 500 queries per day, basic tools, images up to 1080p. Enterprise tier: custom models, unlimited queries, full tool access, negotiated limits. Enforce these tiers at the infrastructure level, not the application level. An attacker should not be able to bypass tier limits by manipulating input.

Treat cost as a first-class system constraint, the same way you treat latency and availability. Every architectural decision should consider cost implications. If adding a feature increases median cost per query by 40%, that feature needs either a higher price tier or aggressive optimization. If a new model is 3x more expensive but only 5% more accurate, the cost-quality tradeoff may not justify deployment.

Cost amplification attacks succeed when teams treat cost as an afterthought. By the time you notice the damage, the attacker has already drained your budget. The defense is simple: measure cost in real-time, enforce per-user limits, cap expensive operations, and design for graceful degradation. Every dollar an attacker cannot force you to spend is a dollar that stays in your budget.

---

The next frontier of tool abuse is token exhaustion — forcing the system to hit context limits, blow through token budgets, and manipulate billing tiers to maximize damage.
