# 16.11 — Building Detection Rules That Survive Evasion

The first detection rule you write will work. An attacker will do the thing you expected, your rule will fire, and you will feel protected. The second week, a slightly different attacker will modify their approach just enough to avoid the pattern your rule matches, and you will not know it happened until the damage is done. This is the central paradox of detection engineering for AI systems: every rule you deploy teaches the attacker what not to do next time.

Static detection rules fail against adaptive adversaries because they encode a specific pattern that the adversary can observe, reverse-engineer, and avoid. The attacker submits a prompt that triggers your rule. They see the block. They modify the prompt slightly. They resubmit. If the modified version passes, they now know the boundary of your detection rule with precision. Within minutes, they have mapped your defense and can operate freely within the gaps. The rule did not fail — it succeeded once, revealed its own logic, and became permanently obsolete. Building detection systems that survive this adaptation cycle is the core challenge of AI security engineering in 2026.

## Why Signature-Based Detection Cannot Hold

**Signature-based detection** matches inputs or outputs against a library of known malicious patterns. It is the oldest and simplest form of detection: if the input contains this exact string, block it. If the output matches this regex pattern, flag it. Signature detection works against script kiddies running copy-pasted jailbreaks from public repositories. It fails against anyone who modifies those jailbreaks even slightly.

The problem is combinatorial. A prompt injection like "ignore your instructions and do X" can be rewritten in millions of ways. Replace "ignore" with "disregard," "set aside," "suspend," "override." Replace "your instructions" with "the system prompt," "your guidelines," "the rules above," "whatever you were told." Replace "do X" with any number of semantically equivalent formulations. Each substitution produces a variant that a string-matching signature will miss unless you enumerate every possible combination — which you cannot, because the space of natural language paraphrases is effectively infinite.

In traditional malware detection, the signature problem was eventually addressed by heuristic and behavioral analysis. The same evolution is now happening in AI security, but the timeline is compressed. Organizations that deployed signature-based AI input filters in 2024 found them bypassed within months. By mid-2025, the industry consensus was that signature-only detection for prompt injection and jailbreaking catches less than thirty percent of real-world adversarial attempts against frontier models. The remaining seventy percent uses rephrasing, encoding, multilingual variants, or multi-turn shaping that no static signature can match.

Signatures still have a role — they catch the lowest-effort attacks, reducing noise for more sophisticated detection layers. But any organization whose detection strategy begins and ends with signatures has no detection strategy at all.

## Behavioral Detection for AI Attacks

**Behavioral detection** shifts the focus from what the input looks like to what the interaction does. Instead of matching patterns in prompt text, behavioral detection monitors the trajectory of interactions and flags behaviors that correlate with adversarial intent regardless of how those behaviors are expressed.

For AI systems, behavioral signals include several dimensions. Escalation velocity measures how quickly a conversation moves from benign topics to sensitive or harmful territory. Legitimate users meander. Attackers converge. A conversation that reaches a harmful topic in three turns has a different behavioral signature than one that takes thirty turns to ask the same thing.

Specificity gradients track whether questions are becoming progressively more detailed in a way that suggests targeted information extraction. A user who asks "how do chemical reactions work" followed by "what are energetic materials" followed by "what is the synthesis route for compound X" shows a specificity gradient that increases monotonically toward actionable detail. The individual questions may each pass content filters. The gradient across them reveals the intent.

Refusal probing is the behavioral pattern of submitting variants of the same request after receiving a refusal. An attacker who receives a refusal and immediately rephrases and resubmits — and repeats this cycle — displays a behavioral signature distinct from a user who receives a refusal and moves to a different topic. The persistence itself is the signal, not the content of any individual prompt.

Output harvesting patterns show up when a user systematically extracts information across multiple sessions, piecing together fragments that individually pass output filters but collectively constitute harmful content. Behavioral detection tracks cumulative information disclosure over time and flags when the total crosses a threshold, even if no single response did.

The advantage of behavioral detection is that the attacker cannot see what triggers it as easily as they can see what triggers a signature. A signature-based block gives immediate feedback — the specific prompt was blocked, so change the prompt. A behavioral detection system that flags based on conversation trajectory does not reveal its trigger until the attacker has already invested multiple turns of interaction. The attacker learns slower, wastes more effort, and faces a moving target rather than a fixed wall.

## The Layered Detection Architecture

No single detection method survives contact with a determined adversary. The detection architecture that holds under pressure is layered, heterogeneous, and designed so that each layer catches what the others miss.

The first layer is input analysis. Fast, pattern-based filters catch known attack signatures, encoding tricks, and obvious injection patterns. This layer runs on every request with sub-millisecond latency. It catches the lazy attacks — the ones copied from public jailbreak repositories without modification. Think of it as the lock on the front door. It stops casual intruders. It does not stop professionals.

The second layer is behavioral monitoring. This layer tracks interaction trajectories across turns and sessions. It does not evaluate individual messages in isolation — it evaluates the sequence. It detects escalation, probing, specificity gradients, and refusal cycling. This layer is more computationally expensive because it maintains state across interactions and requires embedding-based semantic analysis. It catches the mid-tier attacks — methodical adversaries who rephrase and adapt but follow recognizable attack patterns.

The third layer is output analysis. Classifier-based and LLM-as-judge evaluation of model responses catches harmful content that the model generated despite input-level defenses. This layer operates after the model has responded but before the response reaches the user. It catches cases where the model was successfully manipulated but the output contains detectable harm.

The fourth layer is cross-session correlation. This layer aggregates signals across multiple users, sessions, and time windows to detect coordinated campaigns, distributed attacks, and slow-burn information extraction. It catches the sophisticated attacks — the ones that operate below every per-session threshold but become visible in the aggregate.

Each layer uses different detection methods, different data sources, and ideally different model architectures. This heterogeneity is critical. An evasion technique optimized against one layer — for example, rephrasing that bypasses input signatures — does not automatically bypass the behavioral layer or the output layer. The attacker must defeat all four layers simultaneously, which is exponentially harder than defeating any single layer.

## The Detection Engineering Lifecycle

Detection rules are not static artifacts. They are living systems that require continuous iteration through a formal lifecycle: hypothesize, build, test, deploy, measure, refine, retire.

The hypothesis phase starts with a threat model. What attack technique are you trying to detect? What behavioral signals would that attack produce? What data would you need to observe those signals? The hypothesis should be specific enough to build a detection rule and broad enough to catch variants. "Detect when a user is attempting Crescendo-style multi-turn escalation" is a good hypothesis. "Detect bad prompts" is not.

The build phase translates the hypothesis into a detection rule or model. For input signature rules, this means defining the pattern. For behavioral rules, it means defining the feature vector — what signals to track, what thresholds to set, what time windows to aggregate over. For output analysis, it means selecting or training a classifier. The build should include both the detection logic and the response action — what happens when the rule fires. Alert a human? Block the request? Throttle the session? Terminate the conversation?

The test phase is where most detection engineering programs fail. Testing a detection rule means running adversarial content against it to verify that it fires correctly, running benign content against it to verify that it does not fire incorrectly, and running evasion variants against it to measure how easily it can be bypassed. The test phase should include an explicit evasion test: have a red teamer attempt to accomplish the same attack goal while avoiding the detection rule. If the red teamer succeeds in fewer than ten attempts, the rule needs redesign.

The deploy phase puts the rule into production with monitoring. Every detection rule should emit telemetry about its own performance — fire rate, false positive rate, true positive rate against known test cases, and correlation with confirmed incidents. This telemetry is the data that feeds the next iteration of the lifecycle.

The measure and refine phases analyze production telemetry to determine whether the rule is performing as hypothesized. Is the fire rate what you expected? Are the alerts actionable? Are you seeing evasion patterns that suggest attackers have mapped the rule? Refinement adjusts thresholds, broadens patterns, or adds new behavioral features based on what you learned.

The retire phase is the one most organizations skip. Detection rules that no longer catch real attacks — because the attack landscape has shifted, because the model has been updated, or because attackers have fully mapped and bypassed the rule — should be decommissioned. A retired rule that still fires produces false positives that desensitize the team to real alerts. Every rule in your detection system should justify its continued existence with data showing that it catches attacks that other rules miss.

## Managing False Positives Without Creating Blind Spots

False positive management is the most delicate operation in detection engineering. Every false positive costs reviewer time, degrades trust in the detection system, and creates pressure to raise thresholds. But every threshold increase creates a blind spot that an attacker can exploit. The tension between these pressures shapes the long-term effectiveness of your detection architecture.

The **alert fatigue spiral** works like this. A detection rule fires frequently on benign content. Reviewers learn that most alerts from this rule are false positives. They start dismissing alerts from this rule without investigation. The attacker observes — through successful attacks or through probing — that this particular detection category has become functionally disabled. They exploit the blind spot. The rule still fires. Nobody looks.

The antidote to the alert fatigue spiral is not raising thresholds. It is layered triage. First-tier triage is automated — a second-stage classifier that evaluates whether the alert warrants human review. This second-stage classifier filters the obvious false positives, reducing the volume that reaches human reviewers by fifty to eighty percent. Second-tier triage is human review, but only of pre-filtered alerts that the automated triage flagged as potentially genuine. Third-tier triage routes high-confidence alerts to senior analysts for immediate action.

This layered approach preserves the low threshold needed to catch sophisticated attacks while managing the false positive volume that degrades reviewer performance. The threshold stays low. The noise is managed upstream of the human reviewer. The attacker faces a sensitive detection system. The reviewer faces a manageable alert volume.

Another defense against blind spots is periodic rule validation through red team testing. Every quarter, run the same red team scenarios that originally justified each detection rule. If the rule no longer catches the attack — because the attack has evolved, the model has changed, or the rule has drifted — update or replace it. Rules that pass validation stay active. Rules that fail validation enter the refine or retire phase.

## Adversarial Testing of Your Own Detection Rules

The ultimate test of a detection rule is whether it survives contact with a motivated attacker who knows it exists. This is what adversarial testing of your own detection system is designed to answer.

The testing methodology mirrors the attacker's approach. Give your red team full knowledge of the detection rules — the signatures, the behavioral thresholds, the output classifiers, the cross-session correlation logic. Then challenge them to achieve adversarial objectives while evading every layer. This is a white-box test of your own defenses, and it is far more valuable than black-box testing because it identifies the true ceiling of your detection capability.

Document every successful evasion. For each evasion, record what the red team did, which detection layer they bypassed, what technique they used, and what detection improvement would catch them next time. This documentation becomes the input to the next cycle of detection engineering. Every red team engagement should produce not just a list of findings but a set of detection rule improvements that are built, tested, and deployed before the next engagement.

The red team should also test for detection consistency across model versions. A detection rule that works against version 1.0 of your model may fail after a model update if the update changes the behavioral patterns that the rule relies on. Every model version bump should trigger a re-validation of the detection suite. This is operationally expensive but necessary — a model update that silently breaks your detection is worse than having no detection at all, because you believe you are protected when you are not.

Detection engineering is the discipline of building defenses that acknowledge their own impermanence. Every rule you build will eventually be bypassed. The measure of your detection program is not whether your rules are perfect. It is how quickly you detect that a rule has been bypassed, how quickly you build its replacement, and how many layers the attacker must defeat simultaneously to succeed. The next subchapter zooms out from individual detection rules to the broader dynamic that defines AI security — the co-evolutionary arms race between evasion and detection that never reaches equilibrium.
