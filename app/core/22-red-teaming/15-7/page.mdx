# 15.7 — Dev and Test Environment Exploitation — The Forgotten Attack Surface

In March 2025, a fintech company completed a thorough penetration test of their production AI fraud detection system. The testers found the production environment well-hardened — network segmentation, encrypted secrets, strict IAM policies, monitored endpoints. The testers also found a staging environment that nobody had mentioned during the scoping call. It ran the same model, connected to the same database, served the same API schema — and had no authentication on its inference endpoint. The staging environment had been created eight months earlier for a demo to the board of directors. The demo lasted forty minutes. The environment ran for eight months. During that time, it processed over two million inference requests from automated health checks that nobody had turned off, and its unprotected endpoint was indexed by Shodan within six days of deployment. The production fortress was irrelevant. The staging door was wide open.

## Why Dev Environments Are the Path of Least Resistance

Every organization invests disproportionately in production security. Production gets the firewall rules, the access controls, the monitoring, the incident response plans. Development, staging, and test environments get whatever security is convenient, which usually means very little. This asymmetry exists in all of software engineering, but it is amplified in AI systems for three reasons.

First, AI development environments contain production data at far higher rates than traditional software. Machine learning engineers need real data to develop, debug, and evaluate models. Synthetic data is improving but still insufficient for most production development workflows. The result: staging environments that contain full copies of production databases, customer interaction logs, and personally identifiable information, all protected by the access controls that a developer set up in twenty minutes during initial provisioning.

Second, AI development environments expose model endpoints that have the same capabilities as production but none of the guardrails. A staging model endpoint typically lacks safety classifiers, output filtering, rate limiting, and prompt injection defenses. These controls are added during the production hardening phase. The staging endpoint runs the raw model — no safety net, no monitoring, no abuse detection. For an attacker, this is a dream. Every attack they would need to bypass in production works directly in staging.

Third, AI environments proliferate faster than traditional software environments. Every training experiment, every fine-tuning run, every A/B test variant, every model comparison generates a new environment or endpoint. A traditional web application might have three environments — dev, staging, production. An AI team might have thirty — dev, staging, production, plus a dozen experimental endpoints, evaluation sandboxes, demo environments, and notebook servers. Most teams cannot name all of the environments they have running, much less audit them.

## Shared Model Endpoints Between Environments

One of the most dangerous architectural patterns is sharing model serving infrastructure between staging and production. Cost pressure drives this decision. Running separate GPU clusters for staging and production doubles infrastructure costs. So teams route staging and production traffic to the same model serving cluster, separated only by logical namespaces or request headers.

The problem is that logical separation is not security isolation. If staging and production share the same Kubernetes cluster, a container escape in a staging pod grants access to the production namespace. If they share the same model serving framework — vLLM, TGI, Triton Inference Server — a vulnerability in the serving software affects both environments simultaneously. If they share the same GPU hardware, GPU memory side-channels can leak data between environments.

An attacker who compromises the staging environment inherits proximity to production. They are inside the network perimeter. They share infrastructure. They may even share credentials, because the model serving layer authenticates to the model registry and the secrets manager using service accounts that are often shared across environments for simplicity. The staging environment was meant to be a testing ground. It becomes a beachhead.

## Developer Notebooks — The Open Vault

Jupyter notebooks are the most dangerous artifact in AI development. They combine code execution, data access, model interaction, and — critically — embedded credentials in a single file that is shared, versioned, and often publicly accessible. The 2025 Wiz research documented that Jupyter notebook files were the leading source of leaked secrets among AI companies, ahead of Python files and environment configuration files.

The problem is structural. A data scientist writing a notebook needs to authenticate to cloud storage, model APIs, and databases. The fastest way to authenticate is to paste the API key directly into the notebook cell. The key works. The notebook runs. The scientist moves on to the next analysis. That notebook is then committed to a Git repository — sometimes a public one — shared with colleagues, uploaded to a collaboration platform, or left on a cloud instance that outlasts the project it was created for.

But the credential problem is only one dimension. Notebooks also expose model endpoints without any access control. A running Jupyter server, if accessible from the network, provides full code execution in the environment where it runs. Darktrace documented active exploitation campaigns targeting exposed Jupyter notebooks to deploy cryptominers. An attacker who finds a Jupyter server running in an AI development environment gets code execution in a context that typically has access to model endpoints, training data, secrets managers, and cloud resources. It is not a foothold — it is a master key.

Red teams should scan for exposed Jupyter instances on all internal and external network ranges. Look for the default Jupyter ports. Check whether authentication is required. Test whether the notebook server has access to production model endpoints, data stores, or secrets. A single unprotected Jupyter instance can be the starting point for a complete AI infrastructure compromise.

## Playground Environments

Most AI teams maintain playground or demo environments where stakeholders can interact with the model without going through the production application. These playgrounds serve product managers, executives, investors, and customers. They are designed for accessibility, not security.

Playground environments typically run without rate limiting, because the expected traffic is a few dozen queries per day from internal users. They run without safety classifiers, because stakeholders want to see the raw model capabilities. They run without authentication, because adding a login flow would complicate the demo. And they persist long after their intended use, because nobody remembers to shut them down.

An attacker who discovers a playground environment has unrestricted access to a model that may be identical to the production model in capability but stripped of every defense. They can test jailbreaking techniques without triggering abuse detection. They can extract the system prompt without safety filters blocking the extraction. They can probe tool calling behavior without rate limits throttling their attempts. The playground is a training ground for the production attack.

## The Shadow Staging Problem

Shadow staging environments are environments that were created for a specific purpose — a migration, a demo, a load test, a training experiment — and were never decommissioned. They continue running, consuming compute, maintaining network connections, and serving requests to anyone who knows the endpoint.

Shadow environments are invisible to the security team because they were never registered in the asset inventory. They were created by an engineer using a personal cloud account, or spun up in a Kubernetes namespace that the security team does not monitor, or deployed as a temporary cloud function that runs on a forgotten billing account. The environment is not in the architecture diagram. It is not in the threat model. It is not in the penetration testing scope. It exists only in the cloud billing records and in the running process list of whatever infrastructure hosts it.

The red team approach to shadow staging starts with enumeration. Scan internal DNS for model-related subdomains — anything containing "ml," "model," "ai," "inference," "staging," "dev," "test," "demo," "playground," or "sandbox." Query cloud provider APIs for running instances, containers, and serverless functions that match AI workload patterns — GPU instances, containers running model serving frameworks, functions that call model provider APIs. Review cloud billing for unexpected charges from GPU compute or model API usage. Interview engineering teams about environments they have created in the past twelve months and cross-reference with the asset inventory. The gap between what engineers remember and what actually exists is where shadow environments hide.

## Cross-Environment Credential Reuse

The fastest way into a production AI system is often through a development environment that shares its credentials. Cross-environment credential reuse — using the same API keys, service account tokens, or database passwords across dev, staging, and production — is endemic in AI teams. It happens because separate credentials for each environment require separate secrets management, separate rotation schedules, and separate access policies. Most teams take the shortcut.

When credentials are shared, compromising any environment compromises all of them. The attacker breaks into the weakly secured dev environment, extracts the model API key from an environment variable, and uses that same key to access the production model endpoint. The dev environment's weak security is irrelevant to the security team because it only contains test data. But the credentials it contains grant production access.

Red teams test for credential reuse by extracting credentials from lower environments and testing them against production endpoints. If a staging API key works against production, that is a critical finding — not because staging was compromised, but because staging compromise equals production compromise.

## Red Team Methodology for Environment Enumeration

A systematic red team assessment of AI environments follows a four-phase approach. Phase one is discovery: find every environment, endpoint, notebook server, and playground. Use DNS enumeration, cloud API queries, network scanning, billing analysis, and team interviews. Build a complete map, not just the map that was handed to you.

Phase two is classification: categorize each environment by security posture. Does it have authentication? Does it have safety filters? Does it have rate limiting? Does it have monitoring? Does it contain production data? Does it share credentials with production? This classification reveals the highest-risk environments — those with production data, no authentication, and shared credentials.

Phase three is exploitation: test the highest-risk environments first. Attempt to access model endpoints without authentication. Extract credentials and test them against other environments. Access data stores and check for production data. Test model endpoints for jailbreaking and prompt injection without safety controls. Document every path from a lower environment to production.

Phase four is remediation mapping: for each finding, trace the root cause back to a process failure. Why was this environment created without security controls? Why was it not decommissioned? Why does it share credentials with production? The findings drive not just technical fixes but process changes — environment provisioning standards, decommissioning workflows, credential isolation policies.

Every forgotten staging endpoint, every unprotected notebook server, every playground that outlived its purpose is an invitation. Red teams accept the invitation before attackers do.

The next subchapter addresses the largest and least visible attack surface of all: shadow AI systems deployed without organizational knowledge or governance, creating risk that no security team can mitigate if they do not know it exists.
