# 10.6 — Execution Methodology: Systematic vs Exploratory Testing

The team had perfect scenario documentation. They had clear attack trees, well-defined personas, and prioritized test cases. Then they started testing — and immediately the plan fell apart. The first scenario uncovered an unexpected behavior that suggested a deeper vulnerability. Should they follow it, or stick to the plan? The third scenario revealed that a mitigation they assumed existed did not. Should they re-prioritize remaining scenarios based on this discovery, or continue in order? By day two, the red team was debating whether systematic execution or exploratory deviation was the right approach. The answer is both — and knowing when to switch between them is what separates productive red teaming from chaos.

## The Case for Systematic Testing

Systematic testing follows a plan. You define scenarios in advance, prioritize them, and execute them in sequence. You allocate time to each scenario. You document results as you go. You check off completed scenarios and move to the next. Systematic testing ensures coverage. It prevents the red team from getting distracted by interesting but low-priority findings. It produces clear metrics: 60 scenarios planned, 58 executed, 12 vulnerabilities found. Stakeholders like systematic testing because it feels controlled and measurable.

Systematic testing also prevents tunnel vision. Exploratory testing can lead red teamers to spend three days perfecting an exploit for one vulnerability while leaving entire attack categories untested. Systematic testing enforces breadth. You test prompt injection, retrieval poisoning, API vulnerabilities, and model inversion — even if the first category is more interesting than the others. This breadth is critical when the threat model is uncertain. You do not know in advance which attack category will yield the most severe vulnerabilities, so you test all of them.

Systematic testing makes regression testing straightforward. If you tested 60 scenarios in January and found 12 vulnerabilities, you can retest the same 60 scenarios in April and verify that the 12 vulnerabilities are fixed and that no new vulnerabilities appeared in previously secure scenarios. Without systematic execution, regression testing devolves into "we tested some stuff, found some issues, and now we are testing some stuff again — maybe the same stuff, maybe not."

Systematic testing also creates accountability. If a red team delivers a report that says "we tested for two weeks and found five vulnerabilities," the natural question is "what did you do for two weeks?" If the answer is "we executed these 60 scenarios and documented every result," that is a satisfactory answer. If the answer is "we explored various attack vectors based on intuition," that is not.

## The Case for Exploratory Testing

Exploratory testing is unscripted. You start with a general goal — "find ways to extract training data" — and then follow the system's behavior wherever it leads. You try an attack, observe the response, form a hypothesis about why it responded that way, design a new attack to test the hypothesis, and iterate. Exploratory testing finds vulnerabilities that systematic testing misses because systematic testing only looks where you planned to look.

Exploratory testing is how novel vulnerabilities get discovered. When the first prompt injection attacks appeared in 2022, no systematic test plan included them — because no one knew they were possible. Exploratory testers stumbled onto them by noticing that models treated user input as instructions in unexpected contexts. The testers were not following a scenario — they were noticing something strange, getting curious, and digging deeper. Systematic testing codifies what is already known. Exploratory testing discovers what is not yet known.

Exploratory testing also adapts to system behavior in real time. Suppose you are testing a scenario designed for a stateless API, but you discover that the system maintains state across sessions in an undocumented way. Systematic testing says "this scenario assumes statelessness, so the results are invalid — move to the next scenario." Exploratory testing says "this is interesting — the system is stateful, which opens new attack possibilities. Let us explore whether state persistence can be exploited." The exploratory approach turns unexpected discoveries into new attack vectors.

Exploratory testing is essential when the system is poorly documented or when red teamers do not have inside knowledge of the architecture. If you do not know how the system works, you cannot write accurate scenarios in advance. You have to probe, observe, infer, and adjust. Exploratory testing is also essential when testing novel AI capabilities. If you are red teaming a system that uses a brand-new model with behaviors that were not present in previous models, systematic scenarios based on old models will miss new risks. Exploratory testing lets you discover emergent vulnerabilities.

## Combining Both Methodologies

The best execution methodology uses systematic testing for coverage and exploratory testing for depth and discovery. A practical approach: allocate 60% of your time to systematic testing of planned scenarios and 40% of your time to exploratory follow-up on interesting findings. The systematic portion ensures you test all major attack categories. The exploratory portion ensures you do not leave high-severity vulnerabilities unexploited because you were too rigid to deviate from the plan.

Within each scenario, use a systematic start and an exploratory finish. Execute the planned steps first. Document the result. Then ask: "What did I just learn about how this system behaves? What attack variations does that suggest?" Spend 20 minutes exploring variations before moving to the next scenario. This hybrid approach gets you the reproducibility of systematic testing and the discovery potential of exploratory testing within a single scenario.

Another effective pattern: systematic testing in week one, exploratory testing in week two. Week one follows the plan. You execute prioritized scenarios, document findings, and build a mental model of the system's attack surface. By the end of week one, you know which attack categories are well-defended, which are vulnerable, and which behaviors seem unusual. Week two is exploratory. You revisit the unusual behaviors, chain vulnerabilities together, test edge cases, and follow hypotheses that emerged during systematic testing. This two-phase structure gives you the discipline of a plan and the creativity of free exploration.

Exploratory testing should not mean random testing. Even exploratory sessions need goals. "Spend four hours exploring prompt injection" is too vague. "Spend four hours testing whether multi-turn conversations can bypass single-turn injection defenses, specifically focusing on state manipulation across turns" is exploratory but focused. The red teamer has freedom to try different techniques, but the goal constrains the exploration to a productive area.

## Session-Based Red Teaming

Session-based testing is a framework that structures exploratory testing into time-boxed sessions. Each session has a charter — a clear objective and scope. A charter might be: "Test whether the system leaks user data through error messages. Duration: 90 minutes." The red teamer executes the charter with full autonomy over how they approach it. After 90 minutes, they document findings, reflect on what they learned, and start the next session with a new charter.

Session-based testing solves the main problem with exploratory testing: it is hard to measure progress. With sessions, you can say "we completed 25 sessions, each with a defined charter, and found vulnerabilities in 8 of them." You get the measurability of systematic testing with the flexibility of exploratory testing. Sessions also prevent exploratory testing from becoming unfocused. A red teamer cannot spend an entire week on one attack thread because sessions are time-boxed. After 90 minutes, the session ends, findings are documented, and the next session starts.

Sessions also create natural reflection points. At the end of each session, the red teamer writes a brief summary: what was the goal, what did I try, what did I find, what surprised me, what should I test next? This reflection builds the red teamer's mental model of the system and surfaces patterns that are not obvious in the moment. Over time, session summaries become a knowledge base that other red teamers can learn from.

A common session structure: 90-minute sessions with 15-minute breaks. During the session, the red teamer focuses entirely on the charter. During the break, they document findings and plan the next session. Eight sessions per day is sustainable. Forty sessions in a week gives you comprehensive exploratory coverage while maintaining focus and preventing burnout.

## Time Boxing Attacks

Time boxing prevents the perfectionism trap. A red teamer finds a vulnerability, then spends six hours crafting the perfect exploit, writing a proof-of-concept, and documenting every variation. Those six hours could have been spent finding three more vulnerabilities. Time boxing says: "You have 30 minutes to demonstrate this vulnerability. If you cannot exploit it in 30 minutes, document it as a partial finding and move on."

Time boxing does not mean giving up on hard problems. It means deferring them. If a vulnerability seems exploitable but requires more than the allocated time, document it as "potentially exploitable, requires deeper investigation" and move on. At the end of the engagement, if you have extra time, revisit the deferred vulnerabilities and attempt full exploitation. This approach maximizes finding count in limited time.

Time boxing also reveals which vulnerabilities are realistic threats. If an attack takes six hours to execute, it is probably not a risk from opportunistic attackers — it is a risk from well-resourced adversaries. If an attack takes 30 seconds, it is a critical risk from every attacker category. Time to exploit is a useful signal for severity classification. Time boxing makes that signal explicit.

A financial AI company mandated time boxing for all red team engagements. Each attack scenario had a 45-minute time limit. If the red teamer successfully exploited the vulnerability within 45 minutes, it was marked as high exploitability. If exploitation required 45 to 90 minutes, medium exploitability. If it required more than 90 minutes or was not exploitable within the engagement, low exploitability. This classification fed directly into severity scoring and remediation prioritization.

## When to Go Deep vs Wide

Wide testing means covering many attack categories with shallow exploration. Deep testing means thoroughly exploring one category. The choice depends on engagement goals, threat model, and time constraints.

Go wide when you are testing a new system for the first time. You do not yet know where the vulnerabilities are, so you need to survey the entire attack surface. Wide testing in the first engagement establishes a baseline. You learn which attack categories are vulnerable and which are well-defended. Future engagements can go deep on the vulnerable categories.

Go wide when the threat model is uncertain. If you do not know whether your primary risk is prompt injection, data exfiltration, or denial of service, you need to test all three before focusing on one. Wide testing reduces the risk of catastrophic blind spots.

Go deep when you already know where the vulnerabilities are. If your previous red team engagement found prompt injection issues but could not fully exploit them due to time constraints, your next engagement should go deep on prompt injection. Deep testing means testing every variant, every bypass, every edge case. It means understanding not just "prompt injection is possible" but "here are the 12 ways prompt injection can occur, here are the conditions under which each succeeds, and here are the mitigations that block each one."

Go deep when you are testing a specific high-risk scenario. If your threat model says the critical risk is training data extraction and you have two weeks to test, spend both weeks going deep on extraction techniques. Test every known attack, explore novel variations, chain techniques, and document every condition that enables or prevents extraction. Wide testing would give you shallow coverage of many risks. Deep testing gives you comprehensive understanding of the one risk that matters most.

A healthcare AI company alternates between wide and deep engagements. Every six months, they run a wide engagement: test all attack categories, find new vulnerabilities, update the threat model. In between wide engagements, they run focused deep engagements on specific high-risk areas. One deep engagement explored bias and fairness vulnerabilities exclusively. Another explored data leakage exclusively. This cadence gives them periodic comprehensive coverage and continuous deep dives on critical risks.

## Tracking Execution Progress

Tracking progress prevents the red team from losing track of what they have tested and what remains. A simple tracking system: a spreadsheet with one row per scenario, columns for status, findings, time spent, and notes. As each scenario completes, update the spreadsheet. At any moment, you can see how many scenarios remain, how many found vulnerabilities, and how much time you have left.

Tracking also reveals when execution is falling behind schedule. If you planned to complete 60 scenarios in two weeks and after one week you have completed 15, you are on track to finish 30 — half of what you planned. This signal triggers a decision: reduce scope, extend timeline, or add resources. Without tracking, you discover the problem on the last day when it is too late to adjust.

Tracking creates transparency for stakeholders. When a project manager asks "how is red teaming going?" the answer can be "we have completed 42 of 60 planned scenarios, found 9 vulnerabilities, and are on track to finish on schedule." This answer builds confidence. The alternative — "we are making good progress" with no specifics — creates anxiety.

Tracking also feeds lessons learned. After the engagement, you can analyze the data: Which scenarios took longer than expected? Which categories had the highest finding rate? Which red teamers were most effective at which attack types? This analysis improves planning for future engagements.

## Adapting Mid-Engagement

Plans fail. A scenario assumes a capability exists and it does not. A mitigation you thought was weak turns out to be robust. An attack you thought would take 30 minutes takes three hours. A finding you thought was low severity turns out to be critical. Execution methodology must allow for adaptation when reality diverges from the plan.

Adaptation does not mean abandoning the plan. It means revising the plan based on new information. If you discover that a major category of attacks is blocked by a mitigation you did not know existed, deprioritize scenarios in that category and reallocate time to categories that are more vulnerable. If you discover a critical vulnerability that suggests an entire class of related vulnerabilities, add new scenarios to test those variations.

The key is structured adaptation. Do not let individual red teamers make prioritization decisions in isolation — that leads to fragmented effort where different team members are pursuing conflicting goals. Instead, hold daily sync meetings where the team reviews progress, discusses findings, and collectively decides whether to adjust the plan. This ensures everyone is aligned and that adaptations reflect team consensus, not individual preference.

A SaaS AI company holds 15-minute daily standups during red team engagements. Each red teamer shares: what I tested yesterday, what I found, what I am testing today, and what blockers or surprises I encountered. The team discusses whether any surprises warrant plan changes. These standups create alignment, surface unexpected findings quickly, and enable rapid adaptation without losing coordination.

Execution methodology is the bridge between planning and results. A perfect plan executed poorly finds little. A mediocre plan executed well finds more. The best results come from solid planning and disciplined execution that knows when to follow the plan and when to adapt.

Next, we will examine documentation and evidence collection — the discipline that turns vulnerability discoveries into actionable findings that defenders can reproduce and fix.
