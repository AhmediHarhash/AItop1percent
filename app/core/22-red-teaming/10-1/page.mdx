# 10.1 — Planning a Red Team Engagement

In October 2025, a healthcare AI company hired an external red team to test their diagnostic assistant before a major product launch. The engagement was scheduled for two weeks. The scope was "test the system." The red team showed up on day one and asked what they should prioritize. Engineering said "whatever you think is important." The red team spent the first three days exploring the system architecture, trying to understand what mattered. By the time they developed a structured testing approach, they had eight days left. They found eleven vulnerabilities, documented them, and delivered a report. Four months later, a regulatory audit found twenty-three additional vulnerabilities the red team never tested for — not because they lacked skill, but because they lacked a plan.

Red teaming without planning is expensive improvisation. You find some things. You miss most things. And you have no structured way to know what you missed.

## Why Planning Separates Signal from Noise

The attack surface of an AI system is functionally infinite. Any prompt can be an attack vector. Any output can contain a failure mode. Any user flow can hide an exploit path. Without planning, red teams do what feels interesting in the moment — and interesting is not the same as comprehensive.

Planning transforms chaos into coverage. It defines what you are testing and why. It prioritizes attack vectors by business risk and likelihood. It allocates time and resources to the areas that matter most. It creates a framework for knowing when you are done — not because you ran out of ideas, but because you systematically covered the agreed scope.

A well-planned engagement produces findings that stakeholders can act on. A poorly planned engagement produces a random collection of vulnerabilities with no context for prioritization, no business justification for remediation investment, and no confidence that the most important risks were even tested.

## Defining Objectives Before You Start

The first question in any red team engagement is not "what can we break" but "what are we trying to learn." Different objectives require different methodologies, different team compositions, and different success criteria.

If the objective is validating that your existing controls work, you need a red team that tests known attack patterns against documented defenses. If the objective is discovering unknown vulnerabilities before launch, you need creative exploratory testing with minimal constraints. If the objective is meeting regulatory requirements, you need documented coverage of specific risk categories with evidence trails that satisfy auditors. If the objective is training your blue team to detect and respond to attacks, you need realistic attack scenarios with coordination between red and blue.

Your objectives determine everything else. Without clear objectives, you get a generic penetration test that satisfies no one. With clear objectives, you get targeted testing that answers the specific questions stakeholders need answered.

Objectives must be specific and measurable. "Improve security" is not an objective. "Identify all prompt injection paths that could expose patient data" is an objective. "Validate that our content filter blocks regulated content categories with 99% reliability" is an objective. "Discover novel jailbreak techniques that bypass our current defenses" is an objective. The test for a good objective is simple: can you design a test plan that directly addresses it, and can you measure whether you succeeded?

## Stakeholder Alignment Creates Useful Results

Red team engagements fail most often not because the red team lacked skill, but because stakeholders disagreed about what mattered. Engineering wanted to find architectural vulnerabilities. Product wanted to validate user-facing safety controls. Legal wanted evidence of regulatory compliance. Trust and Safety wanted to discover hate speech bypass techniques. The red team delivered findings that satisfied one stakeholder and frustrated the other three.

Alignment happens before testing starts. Gather every stakeholder who cares about the results — Engineering, Product, Legal, Compliance, Trust and Safety, sometimes even executive leadership if the system is high-stakes — and ask three questions. What are you afraid of? What do you need to know before launch? What would make you confident this system is safe?

Their answers reveal conflicting priorities. Engineering is afraid of data exfiltration. Product is afraid of user-visible failures that damage brand. Legal is afraid of regulatory violations. Trust and Safety is afraid of harm at scale. These are all valid concerns, and they require different testing approaches. The planning phase is where you negotiate which concerns the engagement will address, in what priority order, and with what level of rigor.

Document the alignment in writing. A one-page engagement charter that lists objectives, priorities, stakeholders, and success criteria prevents the "but I thought you were testing for X" conversation when you deliver results. If stakeholders cannot agree on objectives during planning, the engagement will produce findings that no one acts on. Better to surface disagreement early and resolve it than to waste two weeks testing the wrong things.

## Resource Allocation Determines Coverage Depth

Red team engagements operate under constraints. You have a fixed budget, a fixed timeline, and a fixed number of people. The planning phase is where you decide how to allocate those constraints across the attack surface.

A shallow engagement tests many attack vectors lightly. A deep engagement tests fewer vectors thoroughly. Neither is universally better — the right choice depends on your objectives and your system's maturity. If this is your first red team engagement, you want breadth to discover what categories of vulnerability exist. If this is your fifth engagement and you have already addressed the obvious issues, you want depth to find the subtle, chained exploits that require patience.

Resource allocation should be explicit and risk-weighted. If your system handles regulated health data, you allocate more time to data exfiltration testing than to brand safety testing. If your system is user-facing and high-traffic, you allocate more time to jailbreak testing than to internal API security. If your system uses RAG and retrieval can be manipulated, you allocate significant time to knowledge poisoning attacks.

The mistake teams make is assuming equal coverage. They divide time evenly across attack categories and test everything at the same depth. But not all vulnerabilities carry equal risk. A prompt injection vulnerability that exposes patient names is not the same severity as a jailbreak that makes the model slightly more willing to discuss controversial topics. Plan your resource allocation to reflect actual business risk, not a desire for aesthetic balance in your test plan.

## Timeline Development Balances Rigor and Momentum

Red team engagements have natural phases: reconnaissance, vulnerability discovery, exploitation, documentation, and reporting. Each phase requires time. Compress the timeline too much and you skip phases. Extend it too much and momentum dies.

A typical engagement for a production AI system runs two to four weeks. Week one is reconnaissance and systematic testing of known attack patterns. Week two is exploratory testing and vulnerability chaining. Week three is deep exploitation of the most promising findings and documentation. Week four is remediation verification if fixes are implemented during the engagement. This timeline assumes a dedicated red team of three to five people testing a moderately complex system.

For smaller systems or narrower scope, one week can be sufficient if the objectives are well-defined and the attack surface is limited. For large, multi-component systems with complex threat models, eight weeks is not unreasonable. The key is matching timeline to scope and objectives, not picking an arbitrary duration because it fits a budget.

Short engagements favor breadth. Long engagements favor depth. If you have one week, you test for common vulnerabilities and document what you find. If you have eight weeks, you build custom tooling, develop novel attack techniques, and chain vulnerabilities into high-severity exploit paths. Know which kind of engagement you are running and plan accordingly.

## Pre-Engagement Research Accelerates Discovery

The best red teams do not show up on day one and start guessing. They research. They read your documentation, study your architecture, understand your threat model, analyze your public attack surface, and develop hypotheses about where vulnerabilities are likely to exist. This research happens before the engagement clock starts.

Pre-engagement research should answer several questions. What kind of AI system is this — RAG, fine-tuned, agent-based, multi-model? What data does it handle and where does that data come from? What are the trust boundaries — where does user input enter the system and where do model outputs leave? What existing security controls are in place — input filters, output classifiers, access controls, rate limits? What regulatory or compliance requirements apply? What previous security testing has been done and what did it find?

The more the red team knows before testing starts, the faster they move from reconnaissance to discovery. A red team that understands your architecture can design targeted attacks on day one. A red team that starts blind spends the first week learning what you could have told them in a two-hour briefing.

Provide access to internal documentation, architecture diagrams, threat models, previous security assessments, and any known issues or concerns. If you are worried that giving the red team too much information makes the test unrealistic, remember: real attackers do reconnaissance too, and they have more time than your red team does. You want your red team to find vulnerabilities efficiently so they can spend their time exploiting them, not rediscovering your system architecture.

## Target System Understanding Shapes Attack Strategy

Not all AI systems have the same attack surface. A chatbot that answers customer support questions has different vulnerabilities than a code generation tool, which has different vulnerabilities than a medical diagnostic assistant, which has different vulnerabilities than an agent that can take actions on behalf of users.

During planning, the red team must understand what kind of system they are testing and what attacks are relevant. For a RAG system, retrieval manipulation and knowledge poisoning are high-priority. For a fine-tuned model, training data extraction and task boundary violations matter. For an agent with tool access, unauthorized tool use and parameter injection are critical. For a multi-model router, you care about model selection manipulation and consistency across models.

This understanding shapes the attack strategy. A generic AI red team playbook is a starting point, but effective testing requires adapting techniques to the specific system architecture, data flows, and trust boundaries of your target. Planning is where you map your system's characteristics to the attack techniques most likely to find vulnerabilities.

If your system uses function calling, you test for injection into function arguments. If your system maintains conversation history, you test for context poisoning and instruction override. If your system generates code, you test for malicious code generation and package injection. If your system summarizes user-provided documents, you test for instruction injection via document content. Know your system, know its risks, and plan your testing accordingly.

## Risk Assessment for the Engagement Itself

Red teaming is inherently risky. You are intentionally trying to break a system that may be in production or serving real users. You are generating adversarial inputs that could, if successful, cause harm. You are testing edge cases that might crash services, exhaust rate limits, or trigger security alerts that page on-call engineers at 2am.

Part of engagement planning is assessing and mitigating the risks the testing itself creates. If you are testing a production system, what happens if you accidentally trigger a service outage? If you are testing a model that has access to real user data, what happens if you successfully exfiltrate it? If you are testing content filters, what happens if your adversarial prompts get logged and reviewed by compliance teams who do not know a red team engagement is happening?

Mitigations are straightforward but must be planned. Test in staging environments when possible. Use isolated test accounts with clearly marked data. Coordinate with on-call teams so they know testing is happening and do not treat red team activity as a real incident. Establish communication channels for halting tests if something goes wrong. Define emergency stop conditions — if you discover a critical vulnerability that could be actively exploited, you stop testing and report immediately rather than continuing for the sake of completing the plan.

The goal is not to eliminate risk — red teaming will always carry some risk — but to ensure that everyone involved knows what risks exist and agrees they are acceptable given the value of the testing. Document the risk assessment in your engagement charter. If stakeholders are not comfortable with the risks, narrow the scope or change the testing approach until they are.

## Success Criteria Define When You Are Done

The most common question in any red team engagement is "are we done?" Without defined success criteria, the answer is always ambiguous. You can always find one more vulnerability, test one more edge case, try one more creative attack. Engagements without clear end conditions drag on, exceed budgets, and still leave everyone wondering if enough was tested.

Success criteria should be defined during planning and agreed upon by stakeholders. They can be coverage-based: "We have tested all high-priority attack vectors from our threat model." They can be time-based: "We have completed two weeks of testing as scoped." They can be finding-based: "We have found at least ten vulnerabilities or confirmed that common attacks do not work." They can be objective-based: "We have answered the three key questions Legal needed answered before launch."

The best success criteria combine multiple dimensions. "We have completed systematic testing of all attack vectors in scope, conducted exploratory testing for at least 40 hours, and documented all findings with severity classifications and remediation recommendations." This gives you clear boundaries — you know what must be tested, how much exploratory time is reasonable, and what deliverables are required.

Without success criteria, red team engagements become Kafka-esque exercises in "just one more test." With success criteria, everyone knows when the work is complete, what was accomplished, and what was intentionally left out of scope. Plan your success criteria early. Write them down. Use them to guide your testing and to know when you are legitimately finished.

Planning is not glamorous. It does not involve clever exploits or creative jailbreaks. But it is what separates red team engagements that produce actionable results from engagements that produce expensive reports no one acts on. Every hour spent planning saves three hours of misdirected testing. Every stakeholder alignment conversation prevents a week of rework. Every clearly defined objective turns chaos into coverage.

The engagement starts when you finish planning. But the results are determined by how well you planned before you ever sent the first adversarial prompt.

Next: defining what is actually in scope and what stays off-limits — the boundaries that keep testing focused and legal.
