# 1.10 — Red Teaming in 2026: The State of the Practice

Red teaming for AI has gone from experimental to mandatory in less than three years. In early 2023, adversarial testing of language models was mostly an academic curiosity — researchers publishing jailbreaks, companies scrambling to patch them, no formal process. By 2024, the frontier labs were building internal red teams and running structured exercises before major releases. By 2025, regulatory pressure and enterprise customer requirements made third-party red team assessments a standard procurement checkbox. In 2026, the question is no longer whether to red team but how well you do it, how often, and whether your program can keep pace with the adversarial threat.

The field is maturing fast but unevenly. The top-tier organizations — frontier AI labs, large regulated enterprises, security-first startups — are running continuous adversarial testing with dedicated teams, external validation, and public bug bounties. The mid-tier organizations are doing pre-launch red team sprints and annual third-party assessments. The bottom tier is doing nothing, hoping security through obscurity will hold, or running checkbox exercises where "red teaming" means an engineer spent an afternoon trying a few jailbreaks. The gap between best practice and common practice is wide. The cost of being on the wrong side of that gap is increasing.

## Where the Industry Stands Today: Maturity Levels Across Sectors

The AI industry in 2026 has five rough maturity tiers when it comes to adversarial testing. Tier 1 organizations treat red teaming as a continuous discipline integrated into the development lifecycle. They have dedicated internal red teams, quarterly or semi-annual external assessments, public bug bounties, and automated adversarial test suites running in CI/CD. They publish transparency reports, disclose vulnerabilities responsibly after patching, and treat red team findings as first-class bugs that block releases. OpenAI, Anthropic, Google DeepMind, and a handful of security-forward startups operate at this level.

Tier 2 organizations run formal red team exercises before major launches and annually thereafter. They might have one or two security engineers who own adversarial testing, they bring in external firms for validation, and they take findings seriously enough to fix critical issues before shipping. They are not publishing bug bounties or transparency reports, but they are doing the work. Many Series B and C AI startups, enterprise SaaS companies adding AI features, and regulated mid-size firms fall into this tier.

Tier 3 organizations do adversarial testing inconsistently. They might run a red team sprint before launch if a customer or investor asks for it, but there is no regular cadence. Findings get triaged like feature requests — some get fixed, some get backlogged, some get ignored. Internal red teaming is sporadic, external validation is rare, and automated adversarial testing does not exist. A lot of early-stage startups and companies treating AI as a side feature operate here.

Tier 4 organizations are aware that red teaming exists but have not done it yet. They plan to "eventually" but have not prioritized it over shipping features. They might run a few manual jailbreak tests before launch, find nothing in twenty minutes, and call it done. Most pre-seed and seed-stage AI startups start here, though the smarter ones graduate to Tier 3 by Series A.

Tier 5 organizations do not know what red teaming is or think it does not apply to them. They built an AI feature, deployed it, and assumed that if the model passes functional tests it is safe. They will learn otherwise when a user, researcher, or regulator finds a vulnerability and the cost of ignorance arrives all at once.

The distribution across tiers is skewed. As of early 2026, fewer than ten percent of organizations building AI products operate at Tier 1. Maybe twenty to thirty percent are at Tier 2. The majority are Tier 3 or below. This is both a risk and an opportunity. The risk is that a large portion of deployed AI systems have never faced serious adversarial testing and are vulnerable to attacks that would take a competent red teamer hours to find. The opportunity is that organizations willing to invest in mature adversarial testing have a significant competitive and trust advantage over those who do not.

Sector maturity varies. Finance, healthcare, and legal tech — industries with regulatory oversight and high trust requirements — skew toward Tier 2 and occasionally Tier 1. Consumer AI products, developer tools, and enterprise SaaS are a mix of Tier 2 and Tier 3. Education, marketing, and internal tooling tend toward Tier 3 and 4. The sectors facing the most regulatory scrutiny are maturing fastest. The sectors where "move fast and break things" is still culturally dominant are lagging.

## EU AI Act Red Teaming Requirements: August 2026 Enforcement for High-Risk Systems

The EU AI Act, which entered into force in August 2024, includes explicit adversarial testing requirements for high-risk AI systems. The enforcement window for systemic risk obligations — including red teaming and robustness testing — closes in August 2026. Organizations deploying high-risk AI in the EU after that date without documented adversarial testing are in violation and subject to fines up to seven percent of global annual revenue or thirty-five million euros, whichever is higher.

The Act defines high-risk systems as those used in critical infrastructure, education, employment, law enforcement, migration, justice, and democratic processes. It also covers AI systems used in ways that could cause significant harm to health, safety, or fundamental rights. Most customer-facing AI assistants, automated decision systems, and content moderation tools fall into this category if deployed in the EU. The requirements are not optional. They are law.

Article 15 of the Act requires that high-risk AI systems undergo testing to identify and mitigate reasonably foreseeable risks, including adversarial attacks. The General-Purpose AI Code of Practice, published in July 2025, and the GPAI Q&A document from September 2025 clarify what this means in practice: structured red team assessments, documented findings, evidence of remediation, and ongoing monitoring. Systems must be tested before deployment and re-tested after significant changes. The documentation must be available to regulators on request.

For organizations operating in the EU, this has forced a step-change in red teaming maturity. Companies that were at Tier 3 or 4 in 2024 have moved to Tier 2 by necessity. The ones that have not moved are either not yet in scope, planning to exit the EU market, or hoping enforcement will be slow. The latter is a bad bet. The EU has a track record of enforcing GDPR with significant fines. The AI Act is modeled on the same enforcement framework. The organizations that wait until after August 2026 to start building adversarial testing programs will be doing it under regulatory pressure with limited time, which is the worst possible context for doing it well.

The ripple effect beyond the EU is significant. Multinational companies are adopting EU-compliant red teaming globally rather than maintaining separate processes for different markets. Enterprise customers in the US and Asia are starting to ask for the same adversarial testing documentation that EU regulators require, because if a vendor can meet the EU standard, they are more trustworthy than one who cannot. The EU AI Act is setting the global floor for adversarial testing maturity whether or not your users are in Europe.

## The Tooling Landscape: Garak, PyRIT, Promptfoo, Custom Solutions

The tooling for adversarial testing of AI systems has matured significantly since 2023. Open-source frameworks, commercial platforms, and internal custom tools now provide structured ways to automate parts of the red teaming process, though full automation remains out of reach for the creative, nuanced attacks that matter most.

Garak, an open-source LLM vulnerability scanner developed by NVIDIA, has become one of the most widely used tools for automated adversarial testing. It ships with hundreds of probes across categories like prompt injection, jailbreaks, toxicity generation, PII leakage, and hallucination triggers. You point it at a model or API, configure which probes to run, and it generates a report showing which attacks succeeded. Garak is excellent for baseline testing — finding known vulnerability classes quickly — but it does not replace human red teamers. It tests what it is programmed to test. It does not invent novel attacks.

PyRIT, short for Python Risk Identification Toolkit, is a red teaming framework from Microsoft that focuses on multi-turn adversarial conversations and goal-oriented attacks. Where Garak tests single-prompt vulnerabilities, PyRIT automates red teaming strategies — generating sequences of prompts designed to achieve a specific adversarial goal like extracting system prompts or bypassing content filters. It is more sophisticated than Garak for testing conversational agents, but it requires more setup and domain knowledge to use effectively.

Promptfoo is a developer-oriented testing framework that includes adversarial testing as one of several evaluation modes. It integrates with CI/CD pipelines, lets you define custom adversarial test cases, and generates reports that compare model behavior across different prompts and configurations. It is popular with engineering teams who want adversarial testing to be part of their automated test suite rather than a separate manual exercise.

Beyond open-source tools, commercial platforms have emerged. Companies like HiddenLayer, Robust Intelligence, and Lakera offer adversarial testing platforms with managed services, continuous monitoring, and enterprise-grade reporting. These tools are expensive — annual contracts in the low six figures for serious usage — but they provide coverage that is hard to build internally: updated attack libraries, integration with production monitoring, and automated red teaming pipelines that run continuously.

Most mature organizations use a combination. Automated tools like Garak and PyRIT for baseline coverage and regression testing. Custom internal scripts for domain-specific attacks — adversarial test cases unique to their product, industry, or risk profile. Manual red teaming by skilled humans for creative, high-value attacks that no tool would think to try. And occasional external assessments to validate that the combination is working.

The gap is still significant between what tools can do and what human red teamers can do. Tools find known attack patterns reliably and fast. Humans find novel attacks, chain multiple vulnerabilities into exploit paths, and adapt in real-time based on how the system responds. The best adversarial testing programs use tools to handle the volume and humans to handle the creativity.

## Bug Bounty Programs for AI: OpenAI, Anthropic, Google Leading the Way

Public bug bounty programs for AI systems have gone from rare to standard practice among frontier labs. OpenAI launched the first major AI bug bounty in April 2023, initially focused on API security and infrastructure but quickly expanding to include model behavior vulnerabilities — jailbreaks, safety bypasses, training data extraction. As of early 2026, the program has paid over six hundred thousand dollars to more than two hundred researchers across thousands of valid submissions. Top payouts have reached fifty thousand dollars for critical vulnerabilities that could lead to systemic safety failures.

Anthropic launched their bug bounty program in mid-2024 with a focus on constitutional AI violations — attacks that cause Claude to violate its stated principles around helpfulness, honesty, and harmlessness. The program pays up to fifteen thousand dollars for high-severity findings and has built a community of researchers who specialize in testing alignment and refusal mechanisms. Anthropic publishes quarterly transparency reports summarizing the types of findings received and the remediations deployed, which has set a new standard for public accountability.

Google's AI bug bounty, which initially covered older Bard and PaLM-era systems, expanded in 2025 to include Gemini 2.5 and Gemini 3 models. The program pays up to thirty thousand dollars for vulnerabilities with significant user impact and has integrated findings into Google's internal red teaming and model safety workflows. Google also runs invite-only private bug bounties for pre-release models, compensating researchers for finding issues before public launch.

Meta, Mistral, and several other model providers have launched similar programs. The structure varies — some pay per finding, some pay based on severity tiers, some offer recognition and swag instead of cash for low-severity issues — but the trend is clear. Bug bounties are becoming table stakes for any organization shipping frontier models or high-stakes AI products.

The researcher community has responded. A small but growing ecosystem of independent security researchers now specializes in AI red teaming, treating bug bounties as a primary or supplemental income source. Online communities have formed around sharing techniques, discussing findings, and coordinating responsible disclosure. The best researchers are earning tens of thousands of dollars per year from AI bug bounties, which has created an economic incentive structure that benefits everyone: researchers get paid, companies get better security, and users get safer systems.

The challenge for smaller organizations is competing in this market. If you launch a bug bounty with a top payout of five hundred dollars when OpenAI pays twenty thousand for similar findings, researchers will spend their time on OpenAI, not you. To run a successful bug bounty as a non-frontier company, you need competitive rewards, fast response times, and a reputation for treating researchers well. This is expensive and operationally demanding, which is why many mid-tier companies opt for private invite-only bounties with a small group of trusted researchers rather than public programs.

## Third-Party Red Team Assessment Market Growth

The market for third-party AI red team services has grown rapidly. In 2023, there were maybe a dozen firms globally offering AI-specific adversarial testing. By 2026, there are over fifty, ranging from boutique security firms that added AI expertise to established penetration testing companies pivoting into AI to new startups founded specifically to red team AI systems.

The quality distribution is wide. Top-tier firms employ ex-researchers from frontier labs, publish original research on adversarial techniques, and deliver reports that are both technically rigorous and accessible to non-technical stakeholders. They charge accordingly — fifteen thousand to fifty thousand dollars per week of engagement. Mid-tier firms offer solid generalist red teaming with some AI-specific knowledge, usually staffed by security engineers who have trained up on LLM vulnerabilities over the past two years. They charge ten thousand to twenty-five thousand per week. Low-tier firms are rebranding generic security testing as "AI red teaming" with minimal differentiation. They charge less but often deliver reports that miss the nuanced, AI-specific vulnerabilities that matter.

The buyer's challenge is distinguishing quality. Ask for case studies. Ask which models and systems they have tested. Ask about their methodology. Ask whether they have published research or contributed to open-source adversarial testing tools. Ask for sample redacted reports. The firms that can answer all of these questions convincingly are the ones worth hiring. The firms that give vague answers about "proprietary methodologies" and cannot show a track record are the ones to avoid.

Demand is outpacing supply. Organizations needing third-party red team assessments for regulatory compliance, enterprise sales, or board-level risk management are booking engagements months in advance. Lead times of six to twelve weeks are common for top-tier firms. This has created a secondary market of freelance AI red teamers — individuals with deep expertise offering short-term engagements, often former employees of frontier labs or security firms who went independent. Hiring a skilled freelancer can be faster and sometimes cheaper than booking a firm, but it requires more vetting and comes with less institutional support.

## Skills Gap: Shortage of Qualified AI Red Teamers

The demand for AI red team expertise far exceeds supply. The skill set required — security engineering, machine learning, adversarial creativity, and the ability to communicate findings to non-technical stakeholders — is rare. Universities are not producing AI security specialists at scale. Bootcamps have not caught up. The talent pool is mostly people who retrained from adjacent fields: security engineers who learned ML, ML engineers who learned security, or researchers who moved from academia into industry.

This creates a hiring crunch. Organizations trying to build internal red team capability are competing for the same small pool of qualified candidates. Salaries for skilled AI red teamers at major tech companies in high-cost-of-living markets exceed two hundred fifty thousand dollars annually as of 2026. Signing bonuses and equity push total compensation even higher. Smaller companies and non-profits cannot compete on compensation, which means they rely more heavily on external firms and bug bounties.

The skills gap is not just technical. Red teaming requires a mindset shift that many engineers struggle with. Building systems and breaking systems require opposite cognitive modes. Engineers are trained to make things work. Red teamers are trained to assume things are broken and find how. Teaching someone with a builder mindset to think adversarially takes time, coaching, and practice. Not everyone makes the transition successfully.

The field is responding with training programs. Several universities now offer AI security courses. A few bootcamps have launched AI red teaming tracks. Online platforms like Kontra, HackTheBox, and custom training from firms like NVIDIA and Trail of Bits provide structured learning paths. But training someone to competence takes months, and training someone to the level where they can lead red team engagements takes years. The supply-demand imbalance will persist for the foreseeable future.

## What Separates Mature Programs from Checkbox Exercises

The difference between a mature red team program and a checkbox exercise is depth, continuity, and accountability. Checkbox red teaming is a one-time event — bring in external red teamers, get a report, file the findings, ship anyway if the business pressure is high enough. Mature red teaming is continuous, integrated into development, and treated as a release gate that cannot be bypassed.

Mature programs have dedicated ownership. Someone — a security lead, a product security engineer, a CISO — is accountable for adversarial testing across all products. They maintain a red team roadmap, track findings over time, measure remediation rates, and report metrics to leadership quarterly. Checkbox programs have no owner. Red teaming happens when someone remembers or when a customer asks for it, and findings disappear into a backlog.

Mature programs integrate findings into the development workflow. Red team issues go into the same ticketing system as bugs, get assigned to engineers, and block releases if unresolved. Checkbox programs produce reports that go into a shared drive and get read once.

Mature programs measure their own effectiveness. They track metrics like mean time to remediate red team findings, percentage of critical findings fixed before launch, number of adversarial test cases in the CI/CD suite, and external validation pass rates. They use these metrics to improve. Checkbox programs have no metrics. They do red teaming because it is required, not because they want to get better at it.

Mature programs invest in adversarial culture. Engineers think adversarially during design. Code reviews include security considerations. Adversarial test cases are part of the definition of done. Checkbox programs treat red teaming as someone else's job.

The gap between the two is organizational, not technical. You can have all the right tools, all the right external firms, and a generous bug bounty budget and still run a checkbox program if leadership does not take findings seriously. Conversely, you can run a mature program with limited budget if the culture prioritizes adversarial thinking and the organization treats security as non-negotiable.

## Predictions for the Next Two to Three Years

Red teaming for AI will continue to professionalize. By 2028, expect formal certifications and standards for AI red teamers, similar to how offensive security has certifications like OSCP and GIAC. Expect more regulatory requirements beyond the EU — the US is likely to introduce federal AI safety standards, and other jurisdictions will follow. Expect adversarial testing to become a standard line item in AI product budgets the same way security audits are for SaaS.

Tooling will improve but not replace humans. Automated adversarial testing will get better at finding known vulnerability classes, regression testing will become standard in CI/CD, and continuous monitoring will catch some attacks in production. But the creative, high-value attacks that define cutting-edge red teaming will remain human-driven.

The skills gap will narrow slowly. More training programs, more people entering the field, more institutional knowledge being shared. But demand will keep growing as AI deployment accelerates, so the gap will not close entirely. Salaries for top AI red teamers will remain high.

Bug bounty programs will expand and standardize. More companies will launch programs. Reward structures will stabilize around industry norms. Responsible disclosure timelines will become more consistent. The researcher community will grow and become more organized.

The distribution of maturity will shift upward. By 2028, operating at Tier 3 or below will be seen as negligent for any organization shipping customer-facing AI. The minimum acceptable standard will be regular red teaming, documented findings, and demonstrated remediation. Organizations that have not built this capability will struggle to win enterprise customers, pass regulatory audits, or attract investors who understand the risk.

The adversarial threat will evolve. As defenses improve, attacks will get more sophisticated. Adversarial ML will advance. Novel exploit chains will emerge. The red teaming community will need to keep pace. The organizations that treat adversarial testing as a static checklist will fall behind. The organizations that treat it as a continuous discipline will stay ahead.

With the mindset established, we turn to the attack surface itself — the full map of what can be targeted in an AI system.

