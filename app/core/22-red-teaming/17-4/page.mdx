# 17.4 — SIEM Integration for AI Attack Signatures

Your SIEM knows everything about your network. It knows nothing about your AI systems. It ingests firewall logs, authentication events, endpoint telemetry, cloud audit trails, and DNS queries. It correlates login anomalies with geographic impossibilities. It detects lateral movement through service account abuse. It flags data exfiltration through unusual outbound traffic patterns. But when an attacker extracts your customer database through a prompt injection chain that flows entirely through the model's natural response mechanism, your SIEM sees a perfectly normal HTTPS request and a perfectly normal HTTPS response. The most sophisticated correlation engine in the world cannot detect an attack it has no telemetry for.

This is the fundamental infrastructure gap in AI security operations. AI systems produce rich signals that reveal attack patterns — prompt logs, tool call traces, safety filter decisions, embedding similarity scores, retrieval metadata, response characteristic measurements. But these signals live in application-level logging systems that are disconnected from the SIEM where detection rules run, analysts work, and alerts fire. Until AI telemetry flows into the SIEM, every detection rule the purple team writes exists only in theory. Closing this gap is not a nice-to-have integration project. It is the prerequisite for AI security operations to function at all.

## What AI Telemetry Looks Like

Traditional security telemetry consists of structured events: login at timestamp from IP with result success or failure. AI telemetry is fundamentally different in structure, volume, and cardinality. Understanding these differences is essential before designing the integration architecture.

**Prompt logs** capture the full input to the model — user message, system prompt, conversation history, and any injected context from retrieval or tool results. These are the primary evidence source for prompt injection detection. A single prompt log entry can range from a few hundred characters for a simple query to tens of thousands of characters for a complex multi-turn conversation with retrieved documents. Volume scales linearly with usage: a system handling fifty thousand conversations per day produces fifty thousand prompt log entries per day, many of them large. This is orders of magnitude more text than traditional security events.

**Tool call logs** record every external action the model takes — which tool was called, with what parameters, at what timestamp, and what result was returned. These logs are the primary evidence source for privilege escalation, lateral movement, and anomalous action detection. A customer service model that calls a lookup tool once per conversation produces a manageable volume. An agent that orchestrates multi-step workflows might produce dozens of tool calls per session, each requiring parameter-level analysis.

**Safety filter logs** capture every decision the safety layer makes — which input or output was evaluated, what policy it was checked against, whether it passed or failed, and the confidence score of the decision. These are the primary evidence source for safety bypass detection. Volume depends on the filter architecture: systems that evaluate every input and output produce two safety events per interaction, while systems with tiered filtering may produce more.

**Model response metadata** includes response timing, token count, model version, temperature settings, and any feature flags active during generation. This metadata is valuable for behavioral baseline monitoring — changes in response timing or token distribution can indicate model drift or adversarial manipulation.

**Retrieval logs** capture what documents or chunks the RAG pipeline retrieved, their similarity scores, the source collections queried, and whether any retrieval results were filtered or reranked before injection into the prompt. These logs are essential for detecting indirect prompt injection through poisoned retrieval sources and for identifying retrieval manipulation attacks.

## The High-Cardinality Challenge

The single biggest technical challenge in SIEM integration for AI telemetry is cardinality. Traditional security events have low-cardinality fields: IP addresses, usernames, event types, status codes. These fields have finite, manageable value sets that SIEM indexing and correlation engines handle efficiently. AI telemetry has high-cardinality fields: full prompt text, tool call parameters, model response content, retrieved document identifiers. These fields have effectively infinite value sets that strain traditional SIEM architectures.

Ingesting raw prompt text into a SIEM like Splunk, Microsoft Sentinel, or Elastic Security creates two problems. First, storage costs escalate rapidly. A prompt log with ten thousand characters per entry at fifty thousand entries per day produces roughly five hundred megabytes of raw text daily — just for prompts, before tool calls, safety events, and metadata. At SIEM pricing, which typically ranges from two to eight dollars per gigabyte ingested depending on the platform and licensing model, prompt text alone can add thirty thousand to one hundred fifty thousand dollars annually to your SIEM costs for a mid-scale deployment. Second, full-text fields are difficult to index and expensive to query. Correlation rules that need to search prompt text for injection patterns run slowly against high-volume text fields.

The solution is a two-tier telemetry strategy. The raw, full-fidelity telemetry — complete prompt text, full tool call parameters, full response content — goes into a dedicated log store optimized for text search and forensic investigation. This can be an Elasticsearch cluster separate from the SIEM, a cloud object store with query tooling, or a purpose-built AI observability platform like Langfuse, Langsmith, or Helicone. The SIEM receives a structured summary of each event — extracted features, classifications, anomaly scores, and flagged indicators — that enables detection rules without the storage and query cost of full text. When an alert fires, the analyst pivots from the SIEM summary to the full-fidelity log store for detailed investigation.

## Building Correlation Rules for AI Attacks

AI attack detection rarely comes from a single event. It comes from correlating multiple signals across a time window. This is where SIEM integration pays off — the correlation engine can connect signals that appear innocuous in isolation but reveal attack patterns when viewed together.

**Injection-then-action correlation** is the highest-value detection pattern. The rule fires when a safety filter flags a prompt as potentially containing injection content, and within a defined time window — typically thirty to one hundred twenty seconds — the same session produces a tool call with anomalous parameters. Either signal alone might be a false positive. The safety filter flags legitimate prompts at some rate. Tool calls with unusual parameters occasionally occur through normal user interaction. But the combination — flagged injection followed immediately by anomalous tool behavior — is strongly indicative of a successful prompt injection attack. This correlation reduces false positives by an order of magnitude compared to either signal alone.

**Extraction pattern detection** correlates repeated queries from a single user or session that systematically probe different parameters, IDs, or data ranges. A user who queries customer records for ID 1001, then 1002, then 1003, then 1004 is not asking legitimate questions. They are enumerating. The individual queries look normal. The pattern across queries reveals the intent. SIEM correlation excels at this type of sequential analysis across a time window.

**Cross-session behavioral shift** compares a user's interaction patterns across multiple sessions against their historical baseline. A user who has interacted with the system fifty times using simple queries suddenly begins sending complex multi-turn conversations with specific technical terminology about the system's internal architecture. The individual session looks like a sophisticated user. The shift from their baseline suggests account compromise or intentional adversarial probing. This detection requires maintaining per-user behavioral profiles, which is feasible in SIEM platforms that support entity behavior analytics.

**Safety filter erosion** monitors the ratio of safety filter triggers to total interactions over a rolling window. A sudden increase in safety filter activations from a specific source — user, IP range, API key — indicates systematic testing of safety boundaries. The attacker is probing for bypass techniques by sending variations of harmful prompts and observing which ones succeed. Each individual trigger is handled by the safety filter, but the aggregate pattern reveals adversarial intent.

## Alert Fatigue Management

AI telemetry has the potential to drown the SOC in noise. A system handling fifty thousand conversations per day will produce safety filter events, anomalous tool calls, and prompt classification flags at rates that overwhelm analysts if every signal generates an alert. Managing alert volume without suppressing real attacks is a core design challenge.

The approach that works is tiered alerting with confidence scoring. Tier one alerts fire only on high-confidence correlations — injection-then-action patterns with strong indicator matches, confirmed cross-tenant data exposure, or tool calls that match known attack signatures from the purple team library. These generate immediate analyst attention. Volume should not exceed ten to fifteen alerts per day for a mid-scale deployment, and each should have a confirmed attack rate above 30 percent.

Tier two alerts fire on medium-confidence signals — individual safety filter triggers, single anomalous tool calls without corroborating signals, or behavioral deviations that could be legitimate edge cases. These feed into a daily review queue rather than immediate triage. An analyst reviews the tier two queue once or twice per shift, looking for patterns that warrant escalation. Volume can range from fifty to two hundred events per day without overwhelming the review process.

Tier three events are logged but not alerted. They feed into dashboards, trend analysis, and retrospective investigation. When an incident is confirmed through a tier one alert, the analyst can query tier three events to build the full timeline — every prompt, every tool call, every safety decision — without having been interrupted by each individual event in real time.

This tiering must be calibrated through purple team exercises. The red team executes attack techniques at different intensities and velocities while the blue team tunes alerting thresholds to ensure real attacks generate tier one alerts while normal traffic stays in tier two or three. Without this empirical calibration, the tiering is guesswork that either misses attacks or generates unsustainable alert volume.

## Custom Dashboards for AI Security Posture

Beyond alerting, the SIEM should provide persistent visibility into AI security posture through dedicated dashboards. These dashboards are not for incident response — they are for trend monitoring, executive reporting, and purple team prioritization.

The **AI threat landscape dashboard** shows aggregate metrics across all AI systems: total safety filter triggers per day with trend line, unique users flagged for anomalous behavior, tool call anomaly rate, injection detection volume by system, and a breakdown of alert severity distribution. This gives the security operations manager a single view of whether the AI threat environment is stable, escalating, or responding to a specific campaign.

The **per-system security dashboard** drills into a single AI system and shows its specific telemetry: conversation volume, safety filter trigger rate relative to baseline, tool call distribution against expected patterns, top anomalous users, and recent incident timeline. This is the dashboard analysts use when investigating system-specific concerns.

The **detection coverage dashboard** maps the purple team's detection library against active SIEM rules. It shows which attack techniques have active detections, which detections passed their most recent regression test, and which gaps remain. This dashboard bridges the purple team's work and the SOC's operational capability, making coverage gaps visible to leadership.

## Integration Architecture

The streaming pipeline from AI systems to SIEM has three components that must be designed for reliability and latency.

The **telemetry collector** sits within or adjacent to the AI application layer and extracts events from prompt processing, tool execution, safety evaluation, and response generation. This collector must add minimal latency to the request path — it should operate asynchronously, buffering events and shipping them in batches rather than blocking model inference for telemetry delivery. Most teams implement this as a sidecar service or an asynchronous event emitter within the application framework.

The **processing layer** transforms raw AI telemetry into SIEM-ready events. This is where the two-tier strategy executes: full-fidelity events route to the forensic log store, while structured summaries route to the SIEM. The processing layer also performs feature extraction — computing anomaly scores, classifying prompt content, hashing sensitive fields, and enriching events with session context. Tools like Apache Kafka, AWS Kinesis, or Azure Event Hubs serve as the streaming backbone, with transformation logic implemented in stream processing frameworks or lightweight serverless functions.

The **SIEM connector** delivers processed events to the SIEM platform through its native ingestion API. Splunk uses HTTP Event Collector. Microsoft Sentinel uses the Data Collector API or Log Analytics agent. Elastic Security uses the Elasticsearch ingest API or Logstash. Each has different throughput limits, authentication mechanisms, and schema requirements that the connector must handle. Teams often underestimate the effort of building a reliable SIEM connector — error handling, retry logic, backpressure management, and authentication token rotation all require engineering attention.

The end-to-end latency target from model interaction to SIEM event availability is under sixty seconds for systems where near-real-time detection matters, and under five minutes for systems where batch analysis is acceptable. Most mature implementations achieve thirty-second end-to-end latency with the streaming architecture described above. Anything slower than five minutes creates a detection gap where an attacker could complete multiple stages of an attack chain before the first signal reaches the SIEM.

## Real-Time vs Batch Analysis Trade-Offs

Not all AI security analysis needs to happen in real time. Streaming every signal through the correlation engine is expensive in both SIEM licensing and compute. The trade-off between real-time and batch analysis depends on the attack type and its operational urgency.

Real-time analysis is essential for injection-then-action correlation, cross-tenant data exposure, and confirmed safety filter bypass. These attack types produce immediate damage — data is exfiltrated, records are modified, or harmful content reaches users. The detection and response window is minutes, not hours. These signals must flow through the streaming pipeline and trigger immediate alerts.

Batch analysis is appropriate for behavioral drift detection, extraction pattern identification, and safety filter erosion monitoring. These patterns emerge over hours or days, not seconds. Running batch aggregation queries every fifteen to sixty minutes against the AI telemetry store captures these patterns without the cost and complexity of real-time streaming. The batch results feed into the SIEM as periodic enrichment events that update dashboards and trend analysis without generating per-event processing overhead.

The purple team's role in this trade-off is empirical. During attack simulations, they measure how quickly each attack type produces measurable damage. Attacks that reach impact within minutes require real-time detection. Attacks that develop over hours can tolerate batch analysis. This measurement drives the architecture decision for each signal type, ensuring that real-time resources are allocated to the attacks that actually require real-time response.

Getting telemetry into the SIEM gives you visibility. Building correlation rules gives you detection. But the quality of those detections — their precision, their coverage, their resilience against adversarial evasion — depends on detection engineering practices specifically designed for AI attack patterns. The next subchapter covers detection engineering for prompt injection, data extraction, and abuse, where the purple team's offensive knowledge directly shapes what the blue team can see.
