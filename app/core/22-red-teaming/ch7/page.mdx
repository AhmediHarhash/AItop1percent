# Chapter 7 — Tool Abuse and Authorization Attacks

When AI systems can take actions — call APIs, execute code, modify data — prompt injection becomes dangerous. Tool abuse is where AI vulnerabilities cause real-world harm. This chapter teaches how attackers manipulate tool-calling systems and how to defend against tool-based attacks.

---

- 7.1 — The Tool-Calling Threat Model
- 7.2 — Unauthorized Tool Invocation: Making the AI Do Things It Should Not
- 7.3 — Parameter Manipulation: Changing What Tools Receive
- 7.4 — Privilege Escalation Through Tools
- 7.5 — Chained Tool Attacks: Multi-Step Exploitation
- 7.6 — Confused Deputy Attacks: Using AI as an Intermediary
- 7.7 — Resource Exhaustion Through Tool Abuse
- 7.8 — Cost Amplification Attacks: Forcing Expensive Operations
- 7.9 — Token Exhaustion and Billing Abuse: Economic Exploitation
- 7.10 — Recursive Loop Attacks: Infinite Agent Chains
- 7.11 — Testing Tool Security: Red Team Approaches
- 7.12 — Authorization Boundaries: What Tools Should Check
- 7.13 — Sandboxing and Containment for Tool Execution
- 7.14 — The MCP Security Model: Tool Authorization in 2026

---

*The model that can act is the model that can be weaponized. Every tool is a potential attack vector.*
