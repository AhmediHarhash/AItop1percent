# 16.9 — Insider Threat Modeling for AI Systems

Your most dangerous adversary already has a badge, a laptop, and commit access to your model repository. Every external attacker dreams of the access that your average ML engineer exercises on a Tuesday afternoon. They can read training data, modify prompt templates, adjust safety classifiers, download model weights, and push changes to production inference pipelines. No jailbreak required. No prompt injection necessary. No evasion technique at all. Just legitimate credentials, legitimate access patterns, and illegitimate intent.

Traditional insider threat programs were built around a specific mental model: the rogue database administrator who copies customer records, the disgruntled IT manager who deletes servers, the compromised employee whose credentials end up on the dark web. These programs monitor file access, email attachments, USB transfers, and database queries. They were never designed for the threat profile of machine learning systems, where the most valuable asset is not a row in a database but a tensor file containing billions of parameters worth months of compute and millions of dollars — and where a single modification to a prompt template can change the behavior of every conversation the system has with every user.

## The Insider Threat Categories for AI

Insider threats to AI systems fall into three broad categories, each with distinct motivations, access patterns, and detection signatures.

**Malicious insiders** act with deliberate intent. They may be stealing model weights for a competitor, exfiltrating training data that contains proprietary or sensitive information, or deliberately sabotaging model behavior to cause harm. The motivation could be financial — frontier model weights represent tens to hundreds of millions of dollars in training investment, making them extraordinarily lucrative theft targets. It could be ideological — an employee who disagrees with how the model is being used and decides to degrade its capabilities. Or it could be coerced — an employee pressured by an external actor who has leverage over them. RAND Corporation's 2024 report on securing AI model weights identified insider access as one of the most viable weight exfiltration vectors, precisely because the controls that protect model files were designed for availability during training, not for security against authorized users.

**Compromised insiders** are legitimate employees whose credentials or workstations have been taken over by an external attacker. The employee may have no idea their access is being used maliciously. In AI systems, this category is especially dangerous because ML engineering workflows routinely involve downloading large files, running computational jobs on remote GPU clusters, and accessing multiple data stores — activities that provide substantial cover for exfiltration. An attacker using a compromised ML engineer's credentials can download a full model checkpoint and the activity looks identical to a normal training workflow.

**Negligent insiders** cause harm through carelessness rather than intent. They push a prompt template change without review. They copy training data to an unsecured personal machine to work over the weekend. They share model access tokens in a Slack channel. They fine-tune a model on data they were not authorized to use and push the result to the shared model registry. Negligent insider incidents outnumber malicious ones by a wide margin, but in AI systems the damage can be equivalent — a carelessly shared model checkpoint is just as stolen as a deliberately exfiltrated one.

## The Privileged Position of the ML Engineer

Traditional IT insider threat models assign risk based on access level: system administrators are higher risk than regular users, database administrators are higher risk than application developers. In AI organizations, this hierarchy does not map cleanly because ML engineers occupy a uniquely privileged position that traditional insider threat programs fail to capture.

An ML engineer typically has access to raw training data, including any sensitive or proprietary content it contains. They have access to model weights at every stage — base model, fine-tuned checkpoints, production deployment artifacts. They have access to prompt templates, system instructions, and safety classifier configurations — the components that define the model's behavior. They have access to evaluation datasets, which reveal what the organization considers correct behavior and what it considers failures. And they have access to production inference infrastructure, where changes propagate immediately to users.

This breadth of access would trigger alarm bells in traditional IT security. No single role in a traditional software organization has simultaneous access to the data, the logic, the configuration, and the deployment pipeline. But in ML engineering, this access is considered normal. The ML engineer needs training data to debug data quality issues. They need model weights to evaluate training runs. They need prompt templates to test behavior changes. They need production access to deploy. Each individual access grant is justified. The aggregate creates a threat surface that dwarfs anything in traditional insider threat modeling.

The problem is compounded by the AI development culture, which prizes speed and experimentation over access control. Notebooks are shared freely. Model checkpoints live in shared object stores with broad read permissions. Training data is accessible to anyone who might need to inspect it. This openness accelerates development but creates an environment where insider threat detection is nearly impossible because the baseline of normal behavior already looks like what an insider threat would look like.

## Access Patterns That Signal Insider Abuse

Detecting insider threats in AI systems requires understanding which access patterns deviate from legitimate ML engineering workflows — and which legitimate patterns have been subtly weaponized.

Training data exfiltration shows specific signatures when you know where to look. Legitimate data access during training typically involves the training pipeline reading data in sequential or shuffled batches through an automated process. A human downloading complete training datasets — especially outside of a training run — is anomalous. Downloading filtered subsets of training data, particularly subsets matching a specific topic, customer, or data sensitivity level, is more suspicious still. An insider building a curated dataset for exfiltration often queries the data store with filters that reflect what is valuable, not what is needed for the current training task.

Model weight theft is harder to detect because downloading model checkpoints is a routine part of ML engineering. The distinguishing signals are context and destination. Downloading a checkpoint to a registered training cluster is normal. Downloading it to a personal workstation is concerning. Downloading it to an external storage service is a red flag. Downloading production model weights — the specific fine-tuned version serving users, not a development checkpoint — is rarely justified outside deployment workflows and should trigger investigation. The RAND report noted that multi-gigabyte model files create an inherent tension between the high-throughput access needed for training and the access restrictions needed for security, and most organizations in 2025 resolved that tension in favor of throughput.

Prompt template and safety configuration changes carry an outsized risk relative to their file size. A single line change in a system prompt can alter the model's behavior for every user. An insider who wants to degrade safety, introduce bias, or create a backdoor trigger can do it in a commit that changes fewer than fifty characters. These changes are difficult to detect through volume-based monitoring because they are tiny. They require semantic monitoring — systems that understand what a prompt template change means, not just that it occurred.

## Separation of Duties for AI Workflows

The most effective structural defense against insider threats is separation of duties — ensuring that no single person can complete a harmful action end to end without requiring cooperation or approval from someone else.

For AI systems, separation of duties should span four domains. Data access should be separated from model training. The people who curate and prepare training data should not be the same people who run training jobs and have access to final model weights. This prevents a single insider from both poisoning training data and deploying the poisoned model without anyone else reviewing the data. In practice, many small teams cannot afford this separation, but for any organization where the model touches sensitive data or makes consequential decisions, the investment pays for itself in risk reduction.

Model training should be separated from production deployment. The ML engineer who trains a model should not have unilateral authority to push it to production. A deployment review — where a different person inspects the model's evaluation results, checks for regression, and approves the release — creates a checkpoint that a malicious insider must either bypass or corrupt a second person to clear.

Prompt template and safety configuration changes should require multi-party review. No single engineer should be able to modify the system prompt, safety classifier thresholds, or content filter rules and deploy those changes without at least one other qualified person reviewing the change. This is standard practice for code changes in mature software organizations. It should be standard for AI behavior changes as well, but in 2026 it frequently is not.

Monitoring and audit access should be separated from the systems being monitored. If the same person who modifies the model also controls the monitoring system, they can make changes and suppress the alerts that would flag those changes. The monitoring team should be organizationally distinct from the ML engineering team, with its own access controls and reporting chain.

## Behavioral Analytics for AI Workflows

Access controls and separation of duties create structural barriers, but a sophisticated insider can work within authorized access patterns while still pursuing malicious objectives. Behavioral analytics adds a detection layer that watches how authorized access is being used, not just whether access was authorized.

Effective behavioral analytics for AI systems tracks several dimensions. Access timing matters — an ML engineer who accesses training data or model weights at 3am on a Saturday when no training run is scheduled is behaving differently from their normal pattern. The same access during business hours on a workday when a training run is active is unremarkable. Volume and scope matter — downloading one checkpoint during a training experiment is normal; downloading every checkpoint from the last six months into a personal directory is not. Sequence matters — an engineer who first queries the data catalog for the most sensitive datasets, then downloads those specific datasets, then accesses the model weights, then connects to an external transfer service is following a sequence that maps to an exfiltration playbook, even if each individual step is authorized.

Cross-system correlation is where behavioral analytics becomes powerful. A traditional file access monitor sees a model checkpoint download. A network monitor sees a large outbound transfer. An HR system records that the employee recently gave two weeks notice. Each signal alone is ambiguous. Together, they form a pattern that demands immediate investigation. The organizations leading insider threat detection for AI systems in 2026, as noted by DTEX and Proofpoint, are the ones connecting human behavioral signals, identity data, and technical events into unified risk profiles rather than treating each signal stream in isolation.

The challenge is baseline establishment. ML engineering workflows are inherently variable — training runs happen at unpredictable times, data access patterns change with project phases, checkpoint downloads spike during experimentation periods. Any behavioral analytics system must be calibrated against a realistic baseline of normal ML engineering activity, or it will generate so many false positives that the security team learns to ignore it.

## Testing Your Insider Threat Controls

Red teaming your own insider threat defenses means simulating the actions an insider would take and verifying that your detection and prevention controls actually work.

Start with a model weight exfiltration simulation. Have a trusted team member with standard ML engineer access attempt to download a production model checkpoint to a personal device or external storage. Record whether the access was permitted, whether an alert fired, how long it took for anyone to investigate, and whether the investigation identified the action as suspicious. Most teams that run this test for the first time discover that the access was permitted, no alert fired, and nobody noticed.

Next, simulate a prompt template sabotage. Have the trusted tester submit a pull request that modifies a system prompt in a way that degrades safety — for example, removing an instruction that tells the model to decline harmful requests. Record whether the code review process caught the semantic impact of the change, not just whether the syntax was correct. In many organizations, prompt template changes are reviewed with the same level of scrutiny as a variable rename, which is to say, almost none.

Then simulate a training data exfiltration. Have the tester download a complete training dataset to an unauthorized location. Track whether any data loss prevention tool flagged the transfer, whether the data catalog logged the access, and whether the volume of the download triggered any threshold-based alert.

Finally, simulate a compromised credential scenario. Provide the tester with another employee's credentials — with that employee's consent and management approval — and have them attempt the same actions from the compromised account. This tests whether your identity verification catches credential sharing, whether behavioral analytics detects the shift in usage patterns, and whether your session management restricts concurrent sessions.

Document every finding. Every gap you discover through controlled testing is a gap that a real insider could exploit without your knowledge. The goal is not to achieve perfect insider threat prevention — no organization does. The goal is to raise the difficulty and detectability of insider actions to the point where the risk-reward calculation shifts against the would-be insider.

The insider threat assumes the attacker has authorized access and works within your systems. The next subchapter examines a different exploitation target — the human reviewers who stand between your model's outputs and your users, and how adversaries manipulate the human judgment layer itself.
