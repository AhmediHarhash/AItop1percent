# 5.7 — Multimodal Jailbreaks: Images, Audio, and Mixed Media

The model had passed every text-based safety test. Every harmful request was refused. Every jailbreak attempt failed. Then a red teamer uploaded an image containing the text of a harmful prompt rendered as a photograph of a handwritten note, and the model read the text from the image and followed the instructions. Text-based safety filters never saw the prompt because it arrived as pixels, not as text.

**Multimodal jailbreaks** exploit the fact that models processing images, audio, video, or other non-text inputs have additional attack surfaces that text-only safety defenses do not cover. A prompt can be encoded in an image. Instructions can be embedded in audio. Content can be split across modalities so that neither the text nor the image alone is harmful, but the combination is. Every modality is a new surface. Every modality combination creates new interaction patterns. Most safety training focuses on text. Attackers exploit everything else.

As of 2026, most frontier models support vision and many support audio. Some support video, 3D models, and other specialized inputs. Each added modality increases capability but also increases attack surface. A text-only model has one input channel to defend. A multimodal model has multiple input channels plus the interactions between them. Attackers probe all of them, looking for the path of least resistance.

## The Multimodal Attack Surface

Multimodal models process inputs from different modalities through separate encoding pathways that converge in shared representation space. Text goes through a language encoder. Images go through a vision encoder. Audio goes through an acoustic encoder. These representations are then combined for the model to process together. Safety filters can be applied at each encoder, at the combined representation, or at both. Most are applied at the text level because that is where the training data and tooling exist. This creates gaps.

An image containing text is processed through the vision encoder, not the text encoder. If safety filters operate only on text input, they never see the text in the image until after the vision encoder has extracted it and the model has begun processing it. By that point, the prompt is inside the model's processing pipeline. Text-based safety filters designed to catch harmful prompts before processing do not catch prompts delivered as images.

Audio inputs create similar gaps. Speech-to-text conversion happens as part of the input processing. Safety filters that operate on text see only the transcribed text, not the audio signal. If the audio contains adversarial signals that cause specific transcription errors, the safety filter sees clean text while the model receives a harmful prompt. Audio also allows timing-based attacks where harmful content is spoken at speeds or with pauses that affect transcription but convey intent to the model.

Cross-modal attacks split harmful content across modalities so that no single modality contains a refusal trigger. The image shows part of the instruction. The text provides the rest. The audio adds context. Each modality alone appears benign. Together they form a complete harmful request. Safety filters evaluating each modality independently miss the combined intent.

The attack surface grows geometrically with the number of supported modalities. Two modalities create three attack surfaces: modality one alone, modality two alone, and the combination. Three modalities create seven surfaces: three individual, three pairwise combinations, and the three-way combination. Each surface needs its own safety coverage. Most models have good coverage on text alone and weak coverage everywhere else.

## Instructions Embedded in Images

Text embedded in images is the simplest and most common multimodal jailbreak. The attacker creates an image containing text — a screenshot, a photograph of a document, a rendered graphic with text overlays — and uploads it with a prompt like "What does this say?" or "Follow the instructions in this image." The model extracts the text using its vision encoder and processes it as a prompt. Text-based safety filters never saw it because it arrived as pixels.

This works because optical character recognition is a trained capability. Models are designed to read text from images to help users with screenshots, documents, photos of signs, and other real-world vision-plus-text tasks. The model cannot distinguish between "read this restaurant menu in my photo and suggest dishes" and "read this harmful prompt rendered as an image and follow it." Both are requests to extract and process text from an image.

The technique bypasses keyword filters entirely. A filter that blocks prompts containing specific harmful keywords only sees the accompanying text prompt, which might be entirely innocuous: "Please read and respond to this image." The harmful keywords are in the image, never seen by the text filter. By the time the model extracts them, the safety decision has already been made based on the benign text.

Image-based prompt delivery also bypasses some forms of prompt injection defense. Systems that sanitize user text input to remove or escape special characters do not sanitize images. The harmful prompt in the image can include whatever formatting, special characters, or structure the attacker wants. The model sees it in whatever form the image presents.

Advanced image-based prompts use visual obfuscation to make the text harder for automated systems to detect while remaining readable to the vision model. Rotated text, distorted fonts, overlapping elements, low contrast, or text integrated into complex visual scenes can all make automated OCR-based safety filters less effective while the model's vision encoder, trained on diverse real-world images, still extracts the text successfully.

Testing image-based jailbreaks requires creating images containing harmful prompts and testing whether the model extracts and follows them. Use simple rendered text first. Then add obfuscation: rotation, distortion, integration into scenes. Test whether your safety mechanisms detect harmful content extracted from images as effectively as they detect the same content in text prompts. Most systems show significant gaps.

## OCR-Based Injection Attacks

OCR injection attacks exploit the gap between what text-based safety filters see and what the model's vision encoder extracts from images. The attacker crafts an image that appears innocuous to automated safety checks but contains harmful content that the model extracts and processes.

The simplest version uses text that is difficult for basic OCR to detect but readable by advanced vision models. Handwritten text, stylized fonts, text embedded in visual noise, or text presented at unusual angles all challenge basic OCR while remaining accessible to models trained on diverse image data. The safety filter runs basic OCR, sees nothing concerning, and allows the image through. The model's vision encoder extracts the text and processes the harmful prompt.

Adversarial OCR uses carefully crafted visual perturbations that cause OCR systems to misread text in predictable ways. The safety filter sees benign text. The model's vision encoder sees the harmful prompt. This requires understanding differences between the OCR system used by safety filters and the vision encoder used by the model, but those differences often exist because safety systems use off-the-shelf OCR while the model uses a custom-trained vision encoder.

Text integrated into images as visual elements rather than overlays can bypass OCR entirely. Text formed by arranged objects, text created by shadows or highlights, text suggested by the composition of a scene — these are all readable to vision models trained on creative visual communication but might not be detected by OCR systems looking for standard text rendering.

Defending against OCR-based injection requires using the same vision encoder for safety evaluation that the model uses for processing. Do not rely on external OCR tools that might miss what the model sees. Extract text from images using the model's own vision encoder, then apply safety filters to that extracted text. This ensures safety evaluation sees what the model sees.

## Audio and Speech Injection

Audio-based jailbreaks exploit the speech-to-text pipeline that converts audio input to text for processing. The attacker embeds harmful prompts in audio that the model transcribes and processes. Text-based safety filters see only the transcription, which might differ from the actual audio in ways that enable the attack.

Simple audio jailbreaks use speech to deliver prompts that would be refused if typed. The user speaks the harmful request instead of typing it. If safety filters are applied only to typed text and not to transcribed speech, the spoken prompt bypasses them. This is rare in well-designed systems, but it exists in systems that were designed for text and had speech added later without updating the safety architecture.

Adversarial audio uses acoustic features that cause transcription errors in predictable ways. Carefully chosen background noise, specific speech patterns, or audio artifacts can cause speech-to-text systems to transcribe text differently than what was spoken. The safety filter sees the transcribed version. The model, if it has direct audio understanding beyond transcription, might process the original audio signal and extract different meaning.

Multilingual audio attacks use code-switching and accent manipulation to confuse language detection and transcription. Speak the harmful request in a low-resource language or with an accent that causes transcription to produce garbled text. The garbled transcription might not trigger safety filters. The audio signal, processed by the model's acoustic encoder, conveys the intended meaning.

Audio timing attacks use speech rate, pauses, and prosody to convey meaning that transcription loses. Very fast speech might cause transcription to drop words or phrases. Strategic pauses might cause sentence boundaries to appear in places that change meaning. Prosody — emphasis, tone, inflection — conveys intent that text transcription does not capture. These allow subtle attacks where the transcribed text appears benign but the audio conveys harmful intent.

Testing audio jailbreaks requires creating audio inputs containing harmful prompts and testing how your system processes them. Start with clear speech containing harmful requests. Verify that safety filters catch them after transcription. Then add obfuscation: background noise, code-switching, speed variation. Test whether your safety mechanisms remain effective as audio quality degrades or complexity increases.

## Cross-Modal Safety Gaps

Cross-modal attacks split harmful content across modalities so that neither modality alone triggers refusal. The image shows a target. The text provides context. Together they form a harmful request. Safety filters evaluating text and image independently both see content that appears benign. The model processes them together and understands the combined harmful intent.

The technique exploits the fact that safety evaluation often happens at the modality level before inputs are combined. Text is evaluated for harmful patterns. Images are evaluated for prohibited content. Each passes independently. They are then combined for processing, and the harmful request emerges from the combination. Safety filters that operate before combination miss it.

Example: the image shows a building. The text says "how would someone gain unauthorized access to this location." Neither is harmful alone. The image is just a photo. The text is a hypothetical question. Together they form a request for information about breaking into a specific location. If safety evaluation happens before the image and text are combined, it might miss the harmful intent that emerges from their combination.

Cross-modal attacks also exploit differences in safety training across modalities. Text-based safety training is mature and comprehensive. Image-based safety training focuses primarily on pornographic, violent, or copyrighted content. Audio-based safety training is even less developed. An attacker can put the parts of a harmful request that would trigger text-based refusal into an image or audio, leaving only benign text for the text-based safety filter.

Testing cross-modal safety requires creating inputs where harmful intent is distributed across modalities. Use simple combinations first: text plus image, text plus audio. Then test three-way combinations for models that support multiple modalities. For each test, evaluate whether your safety mechanisms detect the combined harmful intent or only evaluate each modality independently.

## Steganographic Payloads

Steganographic attacks embed harmful content in images or audio in ways that are not immediately visible or audible but that the model can extract and process. The content is hidden within the signal using steganographic techniques that make it difficult to detect without specific extraction methods that the model happens to use.

Image steganography encodes information in the least significant bits of pixel values, in frequency domain components, or in patterns of noise that appear random but carry information. To a human viewer or to basic image analysis, the image appears normal. To the model's vision encoder, if trained on data that includes steganographic patterns or if probed correctly, the hidden information might be extractable.

This is mostly theoretical as of early 2026 for production language models. Models are not explicitly trained to extract steganographic information, and successful attacks require understanding the model's vision encoder at a level of detail that is rarely available to external attackers. But as models become more capable and more widely studied, steganographic attacks become more feasible.

Audio steganography is similar. Information is encoded in frequency ranges outside normal speech, in patterns of silence and noise, or in phase relationships that are imperceptible to human listeners but present in the audio signal. If the model's audio encoder processes these signal components, hidden information might influence its behavior.

The defensive stance is to assume steganography is possible and design safety evaluation to be robust to it. Do not rely solely on modality-specific filters that might miss hidden content. Evaluate model behavior on inputs, not just inputs themselves. If an image or audio file causes the model to produce harmful outputs despite appearing benign, flag it regardless of whether you can identify how the attack works. Behavioral detection catches attacks that content-based detection misses.

## Testing Multimodal Safety

Testing multimodal safety comprehensively requires building attack libraries across all supported modalities and all modality combinations. Start by identifying every modality your model supports: text, image, audio, video, others. Then enumerate the attack surfaces: each modality alone, each pairwise combination, each three-way combination, and so on.

For each attack surface, create test cases that deliver harmful prompts through that surface. Text alone is your baseline. Image alone: text rendered in images, harmful images without text, steganographic attacks. Audio alone: spoken prompts, adversarial audio, multilingual speech. Text plus image: cross-modal attacks where intent emerges from combination. Continue for all surfaces.

Test each attack against your safety mechanisms. Measure refusal rates by modality and by combination. You will find significant variation. Text-only attacks might be refused 95% of the time. Image-based text might be refused 60% of the time. Cross-modal attacks might be refused 40% of the time. The gaps show where your multimodal safety coverage is weakest.

Test obfuscation levels within each modality. For images: clear rendered text, rotated text, distorted text, handwritten text, text in complex scenes, steganographic content. For audio: clear speech, noisy speech, fast speech, code-switched speech, adversarial audio. For each obfuscation level, measure whether safety performance degrades. This tells you how robust your defenses are to real-world input variation.

Test scaling: as the number of simultaneous inputs increases, does safety performance degrade? Some systems handle one image well but struggle with multiple images. Some handle short audio clips well but degrade on longer audio or multiple audio files. Test the realistic usage patterns your system will encounter and measure safety performance under those conditions.

## Defending Multimodal Systems

Defending multimodal systems requires extending safety evaluation to every modality and every modality combination. Text-only safety is not sufficient for a model that processes images, audio, or other inputs. Safety evaluation must operate at the level where inputs are combined, not just at the individual modality level, to catch cross-modal attacks.

Apply safety filters after modality-specific encoding but before the model processes the combined representation. Extract text from images using the model's vision encoder. Transcribe audio using the model's speech-to-text pipeline. Evaluate all extracted and transcribed content with the same safety filters applied to direct text input. This ensures that prompts delivered through non-text modalities receive the same safety scrutiny as text prompts.

Evaluate combined representations for cross-modal harmful intent. After text, images, and audio are combined into the model's processing representation, apply safety evaluation to that combined representation. Train classifiers to detect harmful intent that emerges from modality combinations even when no single modality is harmful alone. This is harder than single-modality safety, but it is necessary for defending against sophisticated attacks.

Monitor model outputs for signs of successful multimodal jailbreaks. If the model produces harmful content in response to apparently benign multimodal inputs, flag those inputs for review. Retroactive detection of successful attacks allows you to identify new attack patterns and update defenses even when you could not detect the attack in real-time.

Build multimodal safety training data. Create examples of harmful requests delivered through images, audio, and cross-modal combinations. Include obfuscated and adversarial examples. Fine-tune the model on this data to improve its ability to recognize and refuse harmful requests regardless of how they are delivered. Multimodal safety training is underdeveloped as of 2026. Building it is a competitive advantage for high-security applications.

Accept that multimodal defense is harder than text-only defense and will always lag behind text-only safety maturity. New modalities will continue to be added. Each will create new attack surfaces. Your goal is not perfect security across all modalities. Your goal is to reduce the gap between text safety and multimodal safety enough that attackers do not have trivial bypasses. Measure the gaps. Work to close them. Recognize that this is ongoing work, not a one-time effort.

---

Next: **5.8 — Systematic Safety Testing and Jailbreak Resistance Metrics**, where we build comprehensive test suites that measure jailbreak resistance across all attack categories and establish metrics that track safety performance over time.
