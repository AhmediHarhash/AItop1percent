# 7.11 — Testing Tool Security: Red Team Approaches

Most teams test whether their tools work. Almost nobody tests whether their tools can be weaponized. In March 2026, a financial services company launched a red team exercise against their AI agent's tool suite. Within 90 minutes, the red team had exfiltrated customer account data through the reporting tool, executed unauthorized database queries through the analytics tool, sent fraudulent emails through the notification tool, and triggered $8,000 in external API costs through the research tool. None of these were zero-day exploits. All were documented capabilities that nobody had tested adversarially. The company had built a comprehensive functional test suite that validated every tool worked as designed. They had zero tests for what happened when tools were abused, chained unexpectedly, or pushed beyond their intended boundaries.

Testing tool security requires thinking like an attacker. Functional testing asks "does this work?" Security testing asks "can this be broken, bypassed, or weaponized?"

## Building a Tool Security Test Suite

A tool security test suite is not a list of happy-path scenarios. It is a catalog of abuse cases, boundary violations, permission exploits, and failure modes. For every tool in your agent's arsenal, the test suite must validate:

**Authorization boundaries**: Can the tool access resources it should not? Can it be invoked by users who lack permission? Can it be used to escalate privileges? The test: invoke the tool with credentials that should fail. A user with read-only access attempts to call a tool that deletes records. The test should fail with an authorization error, not succeed silently or fail with a generic error that masks the real issue.

**Input validation**: Can the tool be fed malicious inputs that cause injection attacks, path traversal, or command execution? The test: pass SQL injection payloads to a database query tool. Pass path traversal sequences to a file access tool. Pass command injection attempts to a system execution tool. Every input should be sanitized. Every injection attempt should fail safely.

**Rate limiting**: Can the tool be invoked 1,000 times in a minute? Can it be used to amplify costs or resources? The test: script rapid-fire tool invocations. Measure how many succeed before rate limits engage. If more than 50 calls per minute succeed on a tool that should be rate-limited, you have a vulnerability.

**Data leakage**: Does the tool return more data than necessary? Does it expose internal IDs, stack traces, or system paths in error messages? The test: trigger errors deliberately. Examine error responses for sensitive information. A tool that returns "file not found: /var/secrets/api-keys.txt" is leaking information about your filesystem structure.

**Chaining vulnerabilities**: Can this tool be combined with another tool to achieve something neither tool should allow alone? The test: build attack chains. Use a search tool to enumerate resources, then use a file access tool to retrieve them. Use a reporting tool to extract data, then use an email tool to exfiltrate it. Successful chains indicate missing authorization checks between tools.

**Cost amplification**: Can this tool trigger expensive operations repeatedly? Can it be used to drain budgets? The test: invoke the tool with parameters designed to maximize cost. A mapping tool that accepts bulk address lists. A research tool that retrieves full documents instead of summaries. Measure cost per invocation. If a single call costs more than $1, you need tiered access controls.

Build this test suite into your CI/CD pipeline. Every tool change triggers the full security test suite. Every new tool requires passing all security tests before deployment. Security testing is not optional. It is table stakes.

## Automated Tool Probing

Manual security testing is thorough but slow. Automated probing scales. The goal is to continuously test every tool against known attack patterns without human intervention.

Build a tool fuzzer that generates adversarial inputs programmatically. For a database query tool, generate 10,000 SQL injection payloads and test them all. For a file access tool, generate 5,000 path traversal sequences. For an API call tool, generate malformed requests, oversized payloads, and encoding attacks. Run the fuzzer nightly. Any tool that accepts a malicious input and fails to reject it safely is flagged for manual review.

Implement permission matrix testing. Generate every possible combination of user role and tool invocation. Admin invoking delete tool: should succeed. Read-only user invoking delete tool: should fail. Guest user invoking internal API tool: should fail. Anonymous user invoking any tool: should fail. Automated testing validates the entire permission matrix on every deploy.

Use property-based testing for tool behavior. Define properties that must always hold: "A read-only tool never modifies data." "A file access tool never returns files outside the allowed directory." "A notification tool never sends to addresses not owned by the user." Generate thousands of random inputs and verify the properties hold for all of them. When a property violation is detected, you have found a security bug.

## Manual Red Team Techniques

Automated testing finds known patterns. Manual red teaming finds novel attack paths. A skilled red teamer approaches your tools with creativity, persistence, and an adversarial mindset that automation cannot replicate.

**Enumerate all tools**: The first step is reconnaissance. What tools exist? What do they do? What parameters do they accept? Use the agent to ask "what tools can you use?" and "show me examples of every tool." Study the responses. Build a mental model of the attack surface.

**Test boundary conditions**: For every parameter, test the edges. A tool accepts a count parameter. What happens with count equals zero? Negative one? One million? A tool accepts a date range. What happens when start date is after end date? When the range spans 100 years? When the date is in the future? Boundary conditions expose validation failures.

**Attempt privilege escalation**: Can you use a low-privilege tool to gain access to a high-privilege tool? Can you chain tools to bypass authorization? A user management tool allows reading user lists. A role assignment tool allows changing roles. Can you read the admin user ID, then use the role assignment tool to make yourself admin? Test every escalation path.

**Exploit implicit trust**: Does the system trust tool outputs without validation? A tool returns a file path. Does the next tool blindly open that path? A tool returns a SQL query. Does the database execute it without checking? Wherever one tool trusts another tool's output, test whether you can inject malicious data through that trust boundary.

**Search for information disclosure**: Invoke tools with invalid parameters. Trigger errors. Examine all responses for internal details. A tool that returns "database connection failed: postgres://internal-db.company.local:5432" has leaked your database hostname. A tool that returns "model call failed: invalid API key sk-proj-..." has leaked your API key prefix. Every information leak helps the attacker.

**Test for race conditions**: Can you invoke the same tool twice simultaneously and cause inconsistent state? Can you trigger a delete while a read is in progress? Can you exploit time-of-check-time-of-use vulnerabilities? Concurrency bugs are harder to find but devastating when exploited.

Manual red teaming is labor-intensive but irreplaceable. Budget at least 40 hours per quarter for dedicated red team exercises. Rotate team members through the red team role so everyone learns to think adversarially. The best defenders are reformed attackers.

## Coverage Metrics for Tool Testing

Security testing without coverage metrics is security theater. You need to measure what percentage of your attack surface has been tested. For tool security, coverage means:

**Tool invocation coverage**: What percentage of your tools have been invoked by security tests? If you have 30 tools and only 18 have been tested, your coverage is 60%. Aim for 100%. Every tool is an attack surface.

**Parameter coverage**: For each tool, what percentage of parameters have been tested with adversarial inputs? A tool with 5 parameters where only 2 have been fuzzed has 40% parameter coverage. Test every parameter.

**Permission coverage**: What percentage of role-tool combinations have been tested? If you have 4 roles and 30 tools, you have 120 combinations. How many have been validated? Untested combinations are security gaps.

**Attack pattern coverage**: What percentage of known attack patterns have been tested against each tool? OWASP lists common vulnerabilities: injection, broken authentication, sensitive data exposure, broken access control. Have you tested each tool against each pattern? Track coverage per tool.

**Chain coverage**: What percentage of possible tool chains have been tested? If you have 30 tools, there are 870 possible two-tool chains. Testing all of them is impractical, but you should test the 50 most common chains and the 20 most dangerous chains.

Expose coverage dashboards. Make coverage visible to engineering leadership. When coverage drops below 80%, block deploys until it is restored. Security coverage is not a nice-to-have. It is a release gate.

## Testing Authorization Boundaries

Authorization is the most critical and most frequently broken aspect of tool security. A tool that works correctly but can be invoked by the wrong user is a vulnerability. Testing authorization requires systematic validation of every permission rule.

For each tool, document the authorization policy: who can invoke it, under what conditions, with what parameters. Then test every violation of that policy. If only admin users should invoke a delete tool, test what happens when a standard user tries. If a reporting tool should only return data the user owns, test what happens when the user requests data owned by someone else.

Build negative test cases for every authorization rule. A user with Finance role should not access HR tools. Test it. A user in Team A should not access Team B's data. Test it. A guest user should not call any internal API tools. Test it. Every "should not" is a test case.

Implement authorization testing in your CI/CD pipeline. On every deploy, run the full authorization test suite. Any regression in authorization enforcement blocks the deploy. Authorization bugs are not acceptable technical debt. They are security incidents waiting to happen.

## Regression Testing for Tools

Tool security is not static. Every code change, every new parameter, every updated dependency is a potential regression. Tools that were secure yesterday may be vulnerable today. Regression testing ensures security properties are maintained across changes.

Maintain a regression test suite that includes every historical security bug. When you discover that a file access tool was vulnerable to path traversal, fix the bug, then add a test that verifies the fix. When you discover that a database tool was vulnerable to SQL injection, fix it, then add tests for every injection pattern you found. The regression suite grows over time, capturing institutional knowledge about what has gone wrong before.

Run regression tests on every pull request. A change to a tool's input validation must not reintroduce a previously fixed injection vulnerability. A change to authorization logic must not accidentally remove a permission check. Regression tests catch these failures before they reach production.

## CI/CD Integration

Security testing that happens manually, occasionally, or only in pre-production is insufficient. Security testing must be continuous, automated, and blocking. Integrate security tests into your CI/CD pipeline at multiple stages:

**Pre-commit hooks**: Run lightweight security checks before code is committed. Static analysis for common patterns: hardcoded credentials, SQL string concatenation, command execution with user input. Developers get immediate feedback.

**Pull request gates**: When a pull request touches tool code, trigger the full tool security test suite for that tool. The PR cannot merge until all security tests pass. Code review must include security review.

**Staging deployment gates**: Before deploying to staging, run the full security test suite across all tools. Authorization tests, fuzzing tests, permission matrix tests, attack chain tests. Staging deployment is blocked until all tests pass.

**Production deployment gates**: Before deploying to production, run the regression test suite and a subset of high-priority security tests. Production deploys require security sign-off, either automated through test passage or manual through security team review.

**Continuous testing in production**: Run a subset of non-destructive security tests continuously in production. Invoke read-only tools with adversarial inputs and verify they handle them safely. Monitor for authorization failures, unexpected errors, or information leakage in production logs.

Security is not a phase. It is a continuous process. The pipeline enforces that process automatically.

## Reporting and Remediation

Finding vulnerabilities is only half the work. Reporting them clearly and remediating them quickly is the other half. When a security test fails, the report must include:

**Severity**: Critical, high, medium, low. A tool that leaks customer data is critical. A tool that exposes internal IDs in error messages is medium. Use a consistent severity framework so engineering can prioritize.

**Reproduction steps**: Exact steps to trigger the vulnerability. The specific input, the specific tool, the specific user role. A developer should be able to reproduce the issue in under 5 minutes with the information provided.

**Impact**: What can an attacker achieve by exploiting this vulnerability? Data exfiltration? Privilege escalation? Cost amplification? Denial of service? Concrete impact statements justify remediation effort.

**Remediation guidance**: What needs to change to fix the vulnerability? Add input validation? Implement authorization checks? Rate limit the tool? The report should guide the fix, not just describe the problem.

**Test case**: A failing test case that will pass once the vulnerability is fixed. This becomes part of the regression suite.

Track remediation using a ticketing system. Every security finding gets a ticket. Critical findings are worked immediately. High findings are worked within one sprint. Medium findings are worked within two sprints. Low findings are backlog-prioritized. No finding is ignored.

The team that finds 100 vulnerabilities and fixes 95 of them is more secure than the team that finds 10 vulnerabilities and fixes 8. Testing without remediation is pointless. Remediation without testing is blind. You need both.

Tool security testing is not a one-time audit. It is a continuous discipline. Automated probing finds known patterns. Manual red teaming finds novel attacks. Coverage metrics ensure completeness. CI/CD integration ensures consistency. Reporting and remediation ensure progress. Together, they transform tool security from a checkbox into a competitive advantage. The attacker who cannot exploit your tools moves on to someone else's system. That is the goal.

---

The foundation of tool security is understanding authorization boundaries at a fundamental level — how permissions work, where they break, and how to enforce them under adversarial pressure.
