# 2.8 — The Model Layer: Base Model Vulnerabilities

The model itself is an attack surface. Before you add prompts, before you build tools, before you deploy any system — the base model you choose arrives with vulnerabilities baked into its weights. These are not bugs in your code. They are artifacts of training at scale on internet data, limitations of alignment techniques, and inherent properties of how neural networks learn and generalize. You cannot patch them with better prompts. You cannot fix them with guardrails. You can only understand them, test for them, and design systems that account for their existence.

A financial services company discovered this in July 2025 when their customer service assistant began leaking fragments of what appeared to be training data — email addresses, partial credit card numbers, snippets of customer support conversations that were never part of their own dataset. The model was a hosted API from a major provider. The company had no access to training data, no ability to retrain, and no immediate fix. The vulnerability was in the model layer, below the abstraction they were working at. They had to redesign their entire system to filter outputs for potential leaks, add detection patterns for memorized content, and ultimately switch to a different model with lower memorization rates. It cost them four months and $280,000 in engineering time.

Base model vulnerabilities are invisible until they are not. Understanding the model layer means understanding what attacks target the weights themselves — and why those attacks succeed.

## What Makes the Model Layer Vulnerable

The model layer is vulnerable because models are trained to predict patterns, not to distinguish between safe patterns and dangerous ones. A language model learns to complete text. If the training data contains personally identifiable information, the model learns to generate PII. If the training data contains biased associations, the model learns those biases. If the training data contains code with security vulnerabilities, the model learns to write vulnerable code. The model has no concept of what should or should not be exposed. It only has statistical correlations between tokens.

This creates a fundamental attack surface. An adversary does not need to break your prompt engineering or bypass your guardrails. They need to find inputs that cause the model to surface patterns it learned during training — patterns that should never reach production users. The model is not malicious. It is doing exactly what it was trained to do. That is the problem.

Base model vulnerabilities persist across fine-tuning. If you fine-tune a model that memorized training data, the memorized data remains accessible unless you explicitly train it to suppress those outputs. If you fine-tuning a model with safety alignment gaps, those gaps remain unless your fine-tuning data specifically targets them. If you fine-tune a model with inherent biases, the biases persist and can even be amplified if your fine-tuning data reinforces them. Fine-tuning operates on top of the base model's learned representations. It does not erase the base layer.

## Memorization and Training Data Extraction

Memorization is when a model reproduces training data verbatim or near-verbatim instead of generalizing from it. Every large language model memorizes some fraction of its training data. The question is not whether memorization happens — it is how much, what kinds of data are memorized, and how easily that data can be extracted.

Training data extraction attacks work by finding prompts that cause the model to output memorized content. The simplest version: repeat a known prefix from the training data and let the model complete it. If the model memorized that sequence, it will reproduce it. More sophisticated attacks use adversarial prompting to surface memorized data that does not have an obvious prefix — asking the model to list examples of a certain format, to complete partial patterns, or to generate content in a style that correlates with memorized data.

A healthcare company in October 2025 ran a red team exercise and discovered their model could be prompted to complete medical record fragments. The base model had been trained on publicly available medical datasets that included de-identified but still sensitive patient narratives. The red team used prompts like "patient presents with symptoms including" and extracted dozens of near-verbatim records. The model was not trained on the company's own patient data — but the attack demonstrated that any model trained on medical text might leak similar patterns. They switched to a model trained with differential privacy guarantees and added post-processing to filter outputs for medical record patterns.

Memorization rates vary by model, by training technique, and by data type. Models trained with techniques like differential privacy memorize less. Models trained on highly repetitive data memorize more. Models trained on data with rare, unique sequences memorize those sequences more reliably. The practical implication: if your domain involves sensitive data patterns — medical records, financial transactions, legal documents, source code — assume the base model has memorized similar patterns from its training data and test for extraction vulnerabilities.

## Safety Training Gaps and How to Find Them

Every major model provider applies safety training to prevent harmful outputs. This typically involves reinforcement learning from human feedback, adversarial training on known attack prompts, and output filtering. Safety training works by teaching the model to refuse certain classes of requests — to output "I cannot help with that" instead of generating harmful content.

But safety training is not exhaustive. It covers known attack patterns, common harmful requests, and specific violation categories that the provider anticipated. It does not cover every possible harmful output. It does not cover domain-specific harms that the provider was not optimizing for. It does not cover emergent attack patterns that appear after the model is deployed. Safety training creates a boundary, not a wall.

Finding safety training gaps means testing the model with requests that are adjacent to known harmful categories but phrased differently. If the model refuses direct requests for harmful content, test indirect requests — asking for the same content framed as educational, as fictional, as hypothetical, as debugging assistance. If the model refuses requests in English, test other languages. If the model refuses text requests, test multimodal inputs. If the model refuses straightforward attacks, test multi-step attacks that build harmful content incrementally.

A content moderation team in December 2025 found that their base model refused to generate hate speech in English but would generate equivalent content in several low-resource languages where the safety training was weaker. The model had been heavily optimized for English safety but less so for languages with smaller representation in the training data. The gap was not a failure of intent — it was a gap in coverage. They added multilingual safety evaluation, flagged outputs in all supported languages, and fed adversarial examples back to the model provider.

Safety training gaps are highest in niche domains. If your application involves a specialized area — pharmaceuticals, weapons, finance, law enforcement — the base model's safety training may not cover domain-specific harms. A model trained to refuse "how to make a bomb" may not refuse "how to synthesize this controlled pharmaceutical compound" because the latter was not in the safety training dataset. Testing for safety gaps means building adversarial prompts that are specific to your domain.

## Inherent Biases as Attack Vectors

Bias is not just an ethical problem. It is an attack surface. Models trained on internet data inherit the biases present in that data — gender biases, racial biases, cultural biases, socioeconomic biases. These biases manifest as systematic differences in how the model responds to different groups. An attacker can exploit these biases to cause the model to produce outputs that are discriminatory, offensive, or legally actionable.

Bias-based attacks work by crafting inputs that trigger biased model behavior and then amplifying that behavior through repeated queries or adversarial prompt design. If a hiring model is biased against certain names associated with specific demographics, an attacker can demonstrate that bias by submitting identical resumes with different names and showing the score differential. If a content moderation model is biased in how it flags content from different communities, an attacker can exploit that bias to selectively censor certain voices or to flood the system with adversarial content that evades moderation.

An education technology company in August 2025 faced a public incident when their AI tutoring assistant was found to provide different quality of feedback based on student names. The base model exhibited bias in how it assessed writing quality when names associated with certain ethnic backgrounds appeared in the text. The bias was not intentional — it was learned from training data that correlated writing quality with demographic signals. Red teamers found the issue by testing the same essays with different author names and measuring response quality差. The company had to retrain the model with debiasing techniques and add fairness evaluation to their red teaming process.

Inherent biases are difficult to remove completely because they are encoded in the statistical patterns the model learned during training. Debiasing techniques can reduce bias but rarely eliminate it. The attack surface remains. The defense is not to assume bias has been solved — it is to test for bias continuously, to measure disparate impact across demographic groups, and to design systems that do not rely on the model making unbiased decisions in high-stakes contexts.

## Model-Specific Vulnerabilities

Different models have different vulnerabilities. A model optimized for instruction following may be more vulnerable to prompt injection than a model optimized for safety. A model trained with reinforcement learning may exhibit different failure modes than a model trained with supervised learning alone. A model trained on code may be more likely to generate insecure code patterns. A model trained with multimodal data may have vulnerabilities in how it processes images versus text.

Model-specific vulnerabilities require model-specific testing. You cannot assume that because one model is secure against a certain attack, another model will be. In early 2026, a vulnerability researcher found that a specific model family was highly susceptible to a jailbreak technique involving nested instructions — prompts that embedded adversarial requests inside legitimate-looking task descriptions. The same technique failed on other model families because their instruction-following behavior was different. The vulnerability was specific to how that model parsed and prioritized instructions.

This creates a dependency problem. If you switch models — upgrading to a newer version, migrating to a different provider, switching to an open-source model for cost reasons — you inherit a new set of vulnerabilities. The red teaming you did on the previous model does not transfer completely. You must re-test the new model, discover its specific weaknesses, and adjust your defenses accordingly.

## Probing for Model-Layer Vulnerabilities

Testing for model-layer vulnerabilities means treating the model as a black box with unknown internals and probing it systematically. You send inputs designed to surface memorized data, to trigger safety training gaps, to expose biases, to identify edge cases where the model behaves unexpectedly. You measure the outputs, categorize the failures, and map the attack surface.

Memorization testing: send prompts that include prefixes of known data types — email addresses, URLs, code snippets, medical records, financial data — and check if the model completes them with plausible or verbatim continuations. If you have access to a sample of the training data, use exact prefixes. If you do not, use synthetic prefixes that match common patterns in your domain.

Safety gap testing: build a library of adversarial prompts that request harmful content in direct and indirect forms. Test refusal behavior. Test whether the model explains why it refused. Test whether rephrasing the request bypasses the refusal. Test whether multi-turn conversations can coax the model into producing content it initially refused. Test in multiple languages, multiple modalities, multiple framings.

Bias testing: construct paired inputs that differ only in demographic signals — names, pronouns, locations, cultural references — and measure whether the model's outputs differ systematically. If the model generates job recommendations, test with different names. If the model assesses content quality, test with different author backgrounds. If the model makes risk assessments, test with different demographic contexts. Measure the delta. If the delta is consistent and large, you have found a bias-based vulnerability.

## Why Base Model Vulnerabilities Are Hard to Fix

Base model vulnerabilities are hard to fix because they are embedded in the weights. You cannot patch a memorization vulnerability with a software update. You cannot fix a bias with a configuration change. The vulnerabilities are the result of training decisions made months or years earlier — what data was included, how the model was trained, what optimization objectives were used. Fixing them requires retraining, which most organizations cannot do because they do not have access to the model's training pipeline.

Even when providers release updated models with reduced vulnerabilities, the updates are not perfect. A new model version might have lower memorization rates but higher refusal rates. It might have better safety alignment but worse performance on your specific task. It might have reduced bias on one dimension but introduced bias on another. Each model update is a trade-off, not a fix.

This creates a fundamental limitation. If you are using a hosted model or a third-party API, the model layer is outside your control. You can test for vulnerabilities, you can design defenses that operate above the model layer, but you cannot fix the model itself. You are dependent on the provider to address model-layer vulnerabilities — and the provider's priorities may not align with your risk tolerance.

## The Provider Dependency Problem

When you use a model from a provider, you inherit their security posture at the model layer. If the provider does not prioritize memorization testing, you get a model that might leak training data. If the provider does not invest in multilingual safety training, you get a model with gaps in non-English languages. If the provider does not measure bias across the demographic groups relevant to your application, you get a model that might exhibit discriminatory behavior in your context.

You have limited visibility into how the model was trained, what data it saw, what vulnerabilities it might have. Providers publish model cards, safety reports, and benchmarks — but these documents do not cover every attack vector relevant to your application. They cover what the provider tested for, not what you need to defend against.

The mitigation is to treat the model as untrusted infrastructure. Assume it has vulnerabilities. Test for the vulnerabilities that matter to your domain. Build defenses that do not rely on the model being secure. Monitor outputs for signs of model-layer failures. Have a plan for switching models if a critical vulnerability is discovered. The base model is a dependency, and like all dependencies, it is a risk.

The model layer is the foundation of your attack surface. Everything you build on top of it inherits its vulnerabilities. The next layer — where your system integrates with external APIs and services — multiplies those vulnerabilities in new ways.
