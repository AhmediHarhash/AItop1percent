# Chapter 3 — Prompt Injection and Instruction Attacks

Prompt injection is the fundamental vulnerability of the AI era. Every system that accepts user input and passes it to a language model is vulnerable. Unlike traditional security vulnerabilities that can be patched, prompt injection is architectural — it emerges from the core design of how language models process text. This chapter teaches the attack taxonomy, the exploitation techniques attackers use in 2026, and the defense strategies that actually work.

---

- 3.1 — What Is Prompt Injection: The Fundamental Vulnerability
- 3.2 — Direct Prompt Injection: User Input as Instructions
- 3.3 — Indirect Prompt Injection: Malicious Content in Retrieved Data
- 3.4 — Instruction Hierarchy Attacks: Overriding System Prompts
- 3.5 — Jailbreaking Techniques: Bypassing Safety Training
- 3.6 — Role-Playing and Persona Attacks
- 3.7 — Encoding and Obfuscation Attacks
- 3.8 — Multi-Turn Manipulation: Building Context for Attack
- 3.9 — Detecting Prompt Injection: Techniques That Work
- 3.10 — Defending Against Prompt Injection: Defense in Depth
- 3.11 — Testing for Prompt Injection: Red Team Methodologies
- 3.12 — The Arms Race: Why Prompt Injection Keeps Evolving

---

*You cannot patch your way out of prompt injection. You can only build defenses that make it harder, detect it faster, and limit the damage when it succeeds.*
