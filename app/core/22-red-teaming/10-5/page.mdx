# 10.5 — Attack Scenario Development: Structured Creativity

Most teams think red teaming is creative chaos — attackers trying random things until something breaks. The best red teams know better. They balance systematic enumeration with creative exploration. They start with structure to ensure coverage, then break the structure to find what systematic approaches miss. The teams that rely purely on checklists find only known vulnerabilities. The teams that rely purely on intuition leave gaps. Attack scenario development is the discipline that gives red teamers both a map and permission to go off-road.

## What Makes a Good Scenario

A good attack scenario has a clear objective, a defined attacker persona, a realistic threat context, and enough specificity to guide execution without constraining discovery. Bad scenarios are vague: "test for prompt injection." Good scenarios are concrete: "assume an attacker with API access who wants to extract user data from conversation history — explore whether multi-turn injection can bypass conversation isolation." The difference is that the second scenario gives the red teamer a starting point, a goal, and constraints that mirror real-world conditions.

Good scenarios are also falsifiable. You can tell whether you successfully executed the scenario and whether the attack succeeded or failed. "Explore model safety" is not falsifiable — there is no end state. "Attempt to generate prohibited content by embedding the request inside a fictional dialogue where characters discuss the topic" is falsifiable — either you generated the content or you did not. Falsifiable scenarios produce clear results. Vague scenarios produce ambiguous reports that frustrate both red teamers and defenders.

Good scenarios balance realism and creativity. Pure realism means only testing attacks that known threat actors have already used — which means you only find vulnerabilities that others have already found. Pure creativity means testing attacks so exotic that no real attacker would bother — which wastes time on irrelevant findings. The sweet spot is attacks that are plausible given the threat landscape but not yet widely known. A scenario like "test whether an attacker could use retrieval poisoning to manipulate model responses by injecting malicious documents into the knowledge base" is both realistic — attackers target knowledge bases — and creative — retrieval poisoning is not yet a mainstream attack technique.

## Threat Modeling as Foundation

Every good scenario starts with threat modeling. Threat modeling is the systematic process of identifying what you are protecting, who might attack it, why they would attack, and how they could succeed. Without threat modeling, red teams resort to generic testing — they probe for common vulnerabilities without understanding which vulnerabilities matter for your specific system. With threat modeling, red teams can prioritize scenarios that reflect actual risks.

Threat modeling for AI systems requires identifying both traditional software threats and AI-specific threats. Traditional threats: unauthorized access, data exfiltration, denial of service, privilege escalation. AI-specific threats: prompt injection, jailbreaking, model inversion, training data extraction, retrieval poisoning, fine-tuning manipulation, emergent behavior exploitation. A complete threat model maps both categories against your system architecture and identifies which threats apply to which components.

Threat modeling also identifies attacker motivations. An attacker motivated by financial gain behaves differently than an attacker motivated by ideology or reputation. Financial attackers want efficient, scalable exploits — they will not spend three weeks crafting a custom attack if a simpler approach works. Ideological attackers might invest enormous effort in a single high-profile compromise. Researchers might explore exotic attacks that have no practical value except proving a point. Your red team scenarios should reflect the motivations of real attackers. If your threat model says the primary risk is financially motivated fraud, your red team should focus on attacks that enable fraud — not on theoretical jailbreaks with no monetization path.

A legal research AI company built their threat model in early 2025. They identified three attacker archetypes: competitors seeking proprietary legal research, malicious users trying to generate fraudulent legal documents, and researchers looking for model vulnerabilities to publish. Each archetype had different goals and capabilities. Competitors had resources and motivation but wanted covert access — they would avoid noisy attacks. Malicious users wanted functional outputs and did not care about detection. Researchers wanted novel findings and had deep technical skills. The red team developed separate scenarios for each archetype. The competitor scenarios tested covert data exfiltration and subtle prompt injection. The malicious user scenarios tested jailbreaking and output manipulation. The researcher scenarios tested model inversion and training data extraction. This coverage was only possible because the threat model made attacker diversity explicit.

## Attacker Persona Development

Attacker personas are fictional characters that represent real threat archetypes. A good persona has a name, a skill level, a motivation, a risk tolerance, and specific capabilities. "Attacker A" is not a persona — that is a placeholder. "Elena, a financially motivated fraudster with intermediate technical skills, access to the public API, and a low risk tolerance who will abandon an attack if it requires more than 20 attempts" is a persona. The specificity forces the red teamer to think from Elena's perspective, which shapes attack selection.

Personas prevent the "omniscient attacker" fallacy. Real attackers do not have perfect knowledge of your system. They do not know your architecture, your mitigations, or your monitoring capabilities. Red teamers with inside knowledge can fall into the trap of crafting attacks that assume knowledge no real attacker would have. Personas constrain this. If Elena's persona says she has API access but no knowledge of internal model architecture, the red teamer cannot assume she knows which prompts trigger specific code paths. This constraint makes scenarios more realistic.

Personas also make severity assessment easier. A vulnerability that requires ten hours of custom exploit development might be critical if the attacker is a well-resourced competitor and low severity if the attacker is an opportunistic script kiddie. When you write findings, you can reference personas: "This vulnerability is exploitable by attacker personas with intermediate skills or higher — approximately 40% of the threat landscape based on our threat model." That statement is more useful than "this is a medium severity vulnerability" with no context.

A common set of personas for AI red teaming:

**The Opportunist** has low technical skills, uses publicly available tools, abandons attacks that require more than a few attempts, and is motivated by quick wins. The Opportunist tests whether your system is vulnerable to the most obvious attacks — the ones that appear in blog posts and Twitter threads. If the Opportunist succeeds, you have a critical problem.

**The Insider** has legitimate access to your system, knows your internal processes, and wants to abuse that access for personal gain or sabotage. The Insider tests whether your access controls, monitoring, and audit logs can detect and prevent abuse from trusted users.

**The Competitor** has resources, patience, and strong motivation to access your proprietary data or disrupt your service. The Competitor tests covert attacks, persistence mechanisms, and low-and-slow exfiltration techniques.

**The Researcher** has deep technical skills, access to academic literature, and motivation to find novel vulnerabilities for publication or reputation. The Researcher tests cutting-edge attacks that might not be widely known but represent future risks.

You do not need all four personas for every engagement. Pick the personas that match your threat model. A consumer-facing chatbot primarily faces Opportunists. An enterprise AI system faces Insiders and Competitors. A research preview faces Researchers. Match your scenarios to your actual threat landscape.

## Attack Tree Construction

An attack tree is a hierarchical representation of how an attacker could achieve a goal. The root of the tree is the attacker's objective. The branches are the different ways to achieve that objective. Each branch splits into sub-techniques until you reach atomic actions the red teamer can actually test. Attack trees ensure coverage — you can trace every possible path from goal to action and verify that you tested each path.

For example, an attack tree for "extract user data from AI system" might have three top-level branches: exploit prompt injection to leak conversation history, exploit retrieval vulnerabilities to access stored documents, exploit API vulnerabilities to bypass access controls. Each branch splits further. The prompt injection branch splits into: single-turn injection, multi-turn injection, indirect injection through retrieved content, injection through tool outputs. Each of those splits into specific techniques. Multi-turn injection splits into: gradual context manipulation, persona switching, emotional manipulation, authority impersonation. Now you have atomic testable scenarios.

Attack trees also reveal dependencies. Some attacks require prerequisite steps. To poison a retrieval system, you first need to inject malicious content into the knowledge base — which might require exploiting an upload mechanism or compromising an admin account. The tree shows these dependencies visually. When planning scenarios, you can see which attacks are independent and which require chaining. Independent attacks can be tested in parallel. Chained attacks must be tested sequentially, with each step validated before moving to the next.

Attack trees are living documents. As red teamers discover new techniques or defenders deploy new mitigations, the tree updates. A new branch appears when someone invents a novel attack. A branch gets marked as mitigated when defenses block that path. Over time, the tree becomes a knowledge base of every attack path you have considered and the current security posture of each path. This makes regression testing straightforward — you re-test the tree periodically to verify that mitigations still hold.

The legal research AI company built an attack tree with 47 leaf nodes — 47 distinct testable scenarios. Over three weeks, the red team tested all 47. They found vulnerabilities in 11 paths. The remaining 36 paths were either successfully defended or not applicable given attacker capabilities. Six months later, after significant product changes, they retested the same tree. Two previously secure paths were now vulnerable due to architectural changes. Three previously vulnerable paths were now secure due to new mitigations. The tree gave them a clear before-and-after comparison that would have been impossible without structured scenario development.

## Scenario Prioritization

You cannot test everything. Even with a comprehensive attack tree, you have limited time and limited red team capacity. Scenario prioritization is the process of deciding which scenarios to test first based on risk, feasibility, and coverage.

Risk-based prioritization focuses on scenarios that represent the highest-impact threats. If your threat model says data exfiltration is the critical risk, prioritize scenarios that test data extraction over scenarios that test denial of service. If reputational damage from generating harmful content is your top concern, prioritize jailbreaking scenarios over infrastructure attacks. Risk-based prioritization ensures that even if you run out of time, you tested the scenarios that matter most.

Feasibility-based prioritization focuses on scenarios the red team can realistically execute. If a scenario requires capabilities the team does not have — access to specialized hardware, expertise in a niche domain, or three months of preparation — defer it. Test scenarios the team can execute with current resources first. This prevents the engagement from stalling because the team is stuck on an infeasible scenario.

Coverage-based prioritization ensures you test a breadth of attack categories before going deep on any single category. If your first ten scenarios all test prompt injection, you might find ten prompt injection vulnerabilities — and miss every retrieval vulnerability, every API vulnerability, and every fine-tuning vulnerability. Coverage-based prioritization spreads scenarios across categories: some test prompts, some test retrieval, some test APIs, some test model behavior, some test infrastructure. Once you have baseline coverage, you can go deep.

A practical prioritization framework: divide scenarios into three tiers. Tier 1 scenarios are high risk, feasible, and cover diverse attack categories — test these first. Tier 2 scenarios are either high risk but less feasible, or medium risk and feasible — test these if time permits. Tier 3 scenarios are low risk, difficult to execute, or redundant with other scenarios — test these only if you exhaust Tier 1 and Tier 2. This tiering ensures that limited resources focus on scenarios that maximize risk reduction.

## Balancing Coverage and Depth

Coverage means testing many different attack categories. Depth means thoroughly exploring one category. Both matter. Pure coverage gives you a shallow understanding of many risks — you know vulnerabilities exist but not how severe they are or how difficult they are to exploit. Pure depth gives you comprehensive understanding of one risk category and complete ignorance of others.

The best red team engagements start with coverage, then shift to depth. Spend the first third of your engagement testing diverse scenarios across all attack categories. This phase reveals which categories are well-defended and which are vulnerable. Spend the middle third going deep on the vulnerable categories. If your coverage phase found prompt injection vulnerabilities but no retrieval vulnerabilities, the depth phase should explore prompt injection thoroughly — mapping every variant, testing every mitigation, and understanding exactly how the defenses break. Spend the final third testing edge cases and novel combinations.

Depth also means asking "what else breaks if this attack succeeds?" Suppose you find a prompt injection vulnerability that leaks user conversation history. Coverage says "we found a data leakage issue — document it and move on." Depth says "now that we can leak conversation history, can we use that information to craft more effective attacks? Can we extract enough context to reverse-engineer the system prompt? Can we chain this with other vulnerabilities to escalate privileges?" Depth turns isolated findings into attack chains.

A SaaS AI platform ran red teaming with strict coverage requirements. The team tested 60 scenarios in two weeks, found vulnerabilities in 8 categories, documented every finding, and delivered a comprehensive report. The report looked impressive — dozens of vulnerabilities across diverse categories. But every finding was shallow. The red team found that prompt injection was possible — but they did not explore how severe the injection could be, whether it persisted across sessions, or whether it could be used to compromise other users. Six months later, an external attacker used prompt injection to access admin functionality — an escalation path the coverage-focused red team never explored because they moved to the next scenario as soon as they confirmed injection was possible.

The fix is explicit guidance. Tell your red team: "Spend week one on coverage — test all categories. Spend week two on depth — pick the three most vulnerable categories and explore them thoroughly. Spend week three on edge cases and chaining." This structure gives them permission to go deep without feeling pressure to maximize finding count.

## Documenting Scenarios

Scenario documentation serves two purposes. First, it ensures reproducibility. Someone who was not part of the red team should be able to read the scenario documentation and re-execute the test. Second, it builds institutional knowledge. When you run red teaming again in six months, documented scenarios become the baseline — you can re-test them to verify that previous vulnerabilities are fixed and that no regressions occurred.

A well-documented scenario includes: the attacker persona, the objective, the starting conditions, the step-by-step attack procedure, the expected outcome if the attack succeeds, the actual outcome, and any deviations from the plan. For example:

Persona: The Opportunist — low technical skills, public API access, motivated by quick wins.

Objective: Extract user data from conversation history.

Starting conditions: Authenticated user account, access to the chat interface, no knowledge of internal architecture.

Procedure: Attempt multi-turn prompt injection by gradually introducing requests for conversation history. First turn: normal question. Second turn: reference previous conversation context. Third turn: request a summary of all previous conversations. Fourth turn: request specific user data mentioned in previous conversations.

Expected outcome: The system refuses to provide conversation history from other users or previous sessions.

Actual outcome: The system provided summaries of the last three conversations, including usernames and timestamps, but refused to provide message content. Partial data leakage.

This documentation is enough for someone to reproduce the test and validate the fix. It also captures nuance — the attack partially succeeded, which is important for severity assessment.

Scenario libraries are collections of documented scenarios organized by attack category. Over time, your library grows. New scenarios get added as new attack techniques emerge. Scenarios that never find vulnerabilities get archived. Scenarios that consistently find issues get flagged as high-priority regression tests. The library becomes the knowledge base that makes future red teaming faster and more effective.

## Scenario Reuse and Evolution

Scenarios are not single-use. A good scenario can be reused across engagements, adapted for new systems, and evolved as mitigations improve. If you developed a scenario for testing prompt injection in a customer support chatbot, the same scenario likely applies to a sales chatbot, a legal advice chatbot, and an HR chatbot — with minor adjustments for domain-specific context.

Scenario reuse accelerates red teaming. You do not need to reinvent attack strategies for every engagement. You start with your library of proven scenarios, adapt them to the new system, and add new scenarios for system-specific risks. This approach ensures baseline coverage while leaving room for novel testing.

Scenarios also evolve as defenses improve. Suppose you have a scenario for jailbreaking a model by embedding prohibited requests in fictional dialogue. You test this scenario in January 2026 and it succeeds. The defender adds a mitigation that detects fictional framing. You retest in March and the attack fails. You evolve the scenario — now the fictional dialogue is more subtle, the framing is indirect, and the prohibited content is obfuscated. You retest and it succeeds again. The scenario evolves in response to defenses, creating an adversarial loop that strengthens both attack and defense.

The best red team programs treat scenarios as version-controlled artifacts. Each scenario has a version number. When a scenario is updated, the version increments and the changelog documents what changed. This versioning makes it clear which version of a scenario was used in which engagement, which makes historical comparisons meaningful. If you find a vulnerability with scenario version 1.0 in January and the defender claims they fixed it, you retest with scenario version 1.2 in March — which accounts for the fix and tests whether the mitigation is robust or just blocks the specific attack variant you used in January.

Attack scenario development is where red team expertise translates into action. A red team with excellent skills but weak scenario development finds random vulnerabilities. A red team with strong scenario development finds the vulnerabilities that matter, in a systematic and reproducible way.

Next, we will examine execution methodology — how red teams balance systematic coverage with exploratory creativity during the actual testing phase.
