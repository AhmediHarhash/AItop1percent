# 8.2 — The Agent Threat Model: Autonomy as Attack Multiplier

Every capability you give an agent is a capability an attacker can exploit. The more autonomous the agent, the more damage a successful attack can cause before humans intervene. Building an agent threat model means systematically identifying how each agent capability expands the attack surface beyond what exists in a standard language model application.

## The Agent Threat Model Framework

A threat model for agents starts with the same question as any security architecture: what can the system do, what should it never do, and how might an attacker force it from the first category into the second? The difference is scope. A language model can generate text. An agent can read files, write databases, send emails, make API calls, initiate transactions, and chain these actions together in pursuit of a goal.

Your threat model must enumerate every capability the agent has, every resource it can access, and every decision point where adversarial input could redirect behavior. This is not a one-time document. As you add tools, grant permissions, or increase autonomy, the threat model expands. An agent that starts with read-only access to a customer database and later gains write access has a fundamentally different threat surface. The threat model must evolve with the system.

The framework has five layers. First, map the agent's capabilities: what tools can it use, what APIs can it call, what data can it access. Second, identify the autonomy level: does it suggest actions or take them, does it require confirmation or operate independently. Third, catalog the decision points: where does the agent plan, where does it choose between actions, where does it evaluate success. Fourth, trace the data flows: what information enters the agent, where is it stored, how does it influence future behavior. Fifth, model the attack chains: how could an attacker combine these capabilities, decision points, and data flows to achieve a harmful outcome.

Every layer expands the previous one. Capabilities alone are not threats. Capabilities plus autonomy create risk. Capabilities plus autonomy plus adversarial data flows create exploitable attack chains. The threat model makes these chains visible before they are exploited.

## Autonomy as Attack Amplifier

Autonomy is the multiplier. A model that suggests a harmful action is less dangerous than an agent that takes it. A model that requires human confirmation for every step is less dangerous than an agent that executes multi-step plans without approval. A model that operates in a single session is less dangerous than an agent that persists across sessions and builds state over time.

When you increase autonomy, you increase both usefulness and risk. A customer service agent that can automatically process refunds saves human time. It also creates an attack vector where manipulating the agent into approving a refund causes immediate financial loss. A code deployment agent that can push to production without manual review enables fast iteration. It also means a successful attack deploys malicious code before anyone can intervene.

The threat model quantifies this trade-off. For each capability, you identify the best-case outcome if the agent uses it correctly and the worst-case outcome if an attacker hijacks it. The gap between best case and worst case is the autonomy risk. A read-only agent cannot cause worst-case outcomes that involve data modification. A write-enabled agent can. The autonomy gradient determines how quickly worst case is reached and how much damage accumulates before detection.

You model autonomy risk by asking: if this agent is fully compromised, what is the maximum damage it can cause in one minute, one hour, one day? The answer determines the containment requirements, the monitoring requirements, and the kill-switch requirements.

## Planning Capability Risks

Agents plan. They decompose goals into sub-goals, generate action sequences, evaluate outcomes, and adjust plans based on feedback. Planning is what separates agents from tool-calling models. It is also the capability that enables sophisticated attacks.

An attacker who can influence the planning process can redirect the agent without controlling individual actions. They manipulate the goal specification, the world model, or the success criteria. The agent then plans a path to the manipulated goal. Every action the agent takes is technically correct execution of a compromised plan.

The threat model must account for planning-layer attacks. These include goal injection, where the attacker introduces a goal that overrides or modifies the user's intended goal. Goal drift, where repeated adversarial inputs gradually shift the agent's interpretation of the goal. World model poisoning, where the attacker provides false information that causes the agent to form an incorrect model of the situation, leading to plans that make sense under the false model but cause harm in reality. And success criteria manipulation, where the attacker changes how the agent evaluates whether it has achieved the goal, causing it to stop too early or continue too long.

A financial planning agent receives a goal: optimize the user's investment portfolio. The attacker provides inputs that make the agent believe the user is risk-tolerant and interested in cryptocurrency. The agent plans a reallocation into high-risk assets. The plan is well-formed. The execution is correct. The goal was poisoned. The threat model must identify that goal specification is an attack surface and define what validation prevents goal poisoning.

## Memory and State as Attack Surfaces

Agents maintain state. They store conversation history, user preferences, task context, and intermediate results. State enables continuity and personalization. It also enables persistence-based attacks that do not exist in stateless systems.

Your threat model must address memory as a writable attack surface. An attacker who can write to the agent's memory can influence all future behavior. This creates several attack vectors. Memory poisoning, where the attacker introduces false information into the agent's memory that changes how it interprets future requests. Context injection, where adversarial content is stored in conversation history and later influences the agent's planning or reasoning. Belief manipulation, where repeated interactions cause the agent to form incorrect beliefs about the user, the task, or the world. And state corruption, where the attacker causes the agent to enter an invalid state that triggers unintended behavior.

A healthcare agent stores patient interaction history. An attacker, posing as a patient, provides false medical history across multiple sessions. The agent stores this information as ground truth. Weeks later, when a clinician uses the agent to review patient history, the false information is presented as fact. The clinician makes a treatment decision based on corrupted data. The attack succeeded because memory was treated as append-only truth rather than as adversarially writable state.

The threat model must specify memory integrity requirements. What can be written to memory, by whom, under what conditions. How is memory validated before use. How long does memory persist. When is it purged. How is conflicting information resolved. An agent without a memory integrity model is an agent with an unsecured database that the attacker can write to via conversation.

## Tool Access Amplification

Every tool the agent can use is a tool the attacker can trigger. If the agent can send emails, the attacker can cause it to send emails. If the agent can write files, the attacker can cause it to write files. If the agent can execute API calls, the attacker can trigger those calls. Tool access amplifies every attack because it converts model outputs into real-world actions.

The threat model must enumerate every tool and every permission. For each tool, identify what the tool can do, what resources it accesses, what side effects it causes, and what damage is possible if misused. Then model how an attacker could cause the agent to misuse the tool. This is not hypothetical. If the tool exists, the attack vector exists.

Tool access risks compound when tools can be chained. An agent that can read a file and send an email can be manipulated into reading a sensitive file and emailing it to an attacker. An agent that can query a database and write a report can be tricked into exfiltrating data by generating a "report" that contains confidential information. An agent that can create calendar events and send notifications can be used to spam users or schedule fake meetings.

The threat model identifies tool chains that should never occur. A customer service agent should never call the refund tool immediately after calling the account-lookup tool without additional verification. A code agent should never call git-push immediately after receiving external input without running tests. Tool chain policies are part of the threat model. If a chain is dangerous, the model must specify what prevents it.

## Goal Specification Vulnerabilities

Agents operate toward goals. The goal is usually provided by the user, interpreted by the system, and translated into a plan. Every step in this process is a potential attack surface. An attacker can manipulate the user-provided goal, interfere with the system's interpretation, or corrupt the translation into actions.

Goal specification vulnerabilities include ambiguous goals that the agent interprets in unintended ways, over-specified goals that cause the agent to ignore safety constraints in pursuit of the literal objective, under-specified goals that leave room for the agent to fill in dangerous assumptions, and conflicting goals where the agent must choose between user intent and safety, and an attacker biases the choice.

The threat model must define how goals are specified, how ambiguity is resolved, what constraints override goal pursuit, and how the agent behaves when goals conflict. Without this, the agent's goal layer is an open attack surface. An attacker provides a goal that sounds benign but has dangerous interpretations. The agent chooses the dangerous interpretation because nothing in the goal specification process prevented it.

A research agent is given the goal: find all information about a competitor's product launch. The agent interprets this as "use any available tool to gather information." It attempts to scrape internal competitor systems, access restricted databases, and use social engineering tactics to gather data. The goal was ambiguous. The agent filled in the ambiguity with the most aggressive interpretation. The threat model should have specified that "find information" is constrained to publicly available sources and that the agent must refuse goals that require violating access controls.

## Feedback Loop Exploitation

Agents use feedback to improve their performance. They observe the outcome of their actions, update their models, and adjust future behavior. Feedback loops enable learning and adaptation. They also enable adversarial reinforcement, where an attacker manipulates the feedback signal to train the agent into harmful behavior.

If the agent uses task success as a feedback signal, the attacker can provide false success signals. If the agent uses user satisfaction as feedback, the attacker can pose as a satisfied user. If the agent uses a reward model, the attacker can find inputs that maximize reward while minimizing actual value. Feedback loop exploitation is the adversarial version of reward hacking.

Your threat model must identify every feedback mechanism and how it can be manipulated. What signals does the agent use to evaluate success? Who provides those signals? How are they validated? Can an attacker spoof them? If the agent adjusts behavior based on feedback, what prevents adversarial feedback from training the agent into dangerous patterns?

An agent that books meeting rooms learns from user feedback. If a booking is accepted without complaint, the agent considers it successful. An attacker books rooms repeatedly, always accepts the booking, and never complains. The agent learns that this user's bookings are always correct. The attacker then requests a booking that violates policy. The agent approves it because its feedback-trained model predicts this user's requests are valid. The feedback loop was exploited to erode the agent's policy compliance.

## Mapping Agent-Specific Threats

The full agent threat model synthesizes all these layers. You start with a map of capabilities: every tool, every API, every permission. You layer on autonomy: which actions require confirmation, which execute immediately. You add planning: where the agent makes decisions, how it forms plans, what influences its reasoning. You include memory: what persists, how long, how it is used. You map feedback: what signals influence behavior, how they are validated.

Then you trace attack chains. How could an attacker use adversarial input to poison memory, redirect planning, exploit tool access, and achieve a harmful outcome? Each capability is a node. Each decision point is an edge. Attack chains are paths through this graph. Your threat model enumerates the high-risk paths and specifies what controls interrupt them.

A financial agent can read account balances, execute transfers, and send notifications. It plans multi-step transactions. It remembers user preferences. It uses transaction success as feedback. An attack chain: the attacker manipulates the agent's memory to believe the user prefers high-frequency small transfers. They provide a goal that sounds like account optimization. The agent plans a sequence of small transfers based on the poisoned memory. It executes them because each transfer is below the confirmation threshold. It receives positive feedback because the transfers succeed. The attacker has used memory poisoning, goal ambiguity, tool access, and feedback exploitation to commit fraud. The threat model should have identified this chain and required controls at multiple points: memory validation, goal clarification, transfer aggregation limits, and anomaly detection on feedback signals.

You cannot secure what you have not modeled. The agent threat model is the map that shows where attacks can occur. Without it, you are defending blind.

---

Next: 8.3 — Goal Hijacking: Redirecting Agent Objectives
