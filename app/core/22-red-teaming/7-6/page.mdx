# 7.6 — Confused Deputy Attacks: Using AI as an Intermediary

Why does your database have stronger authentication than your AI agent? The database requires credentials, IP allowlists, encrypted connections, and multi-factor authentication. The AI agent has all of that — plus the ability to understand natural language requests from users who have none of it. The AI is the deputy. The deputy has authority. The attacker tricks the deputy into using that authority on their behalf.

This is the confused deputy problem, a classic security vulnerability that predates AI by decades. A program with high privileges is manipulated into performing actions on behalf of a low-privilege user. The program is confused about who it is acting for. In traditional systems, this happens through API calls and service-to-service authentication. In AI systems, it happens through conversation. The attack surface is every sentence the AI accepts.

## The Confused Deputy Problem

The confused deputy problem was first described in the 1980s. A compiler ran with elevated privileges to write compiled code to system directories. A user could specify an output file path. If the user specified a system file as the output, the compiler would overwrite it using its elevated privileges. The compiler was the deputy. It had authority the user lacked. The user confused it into using that authority destructively.

AI agents are deputies at scale. They have credentials to databases, APIs, internal services, and third-party systems. They run with service accounts that have elevated permissions. They cross security boundaries that users cannot cross directly. They translate natural language into privileged operations.

The attack is simple. The user asks the AI to perform an action. The AI evaluates whether the request is reasonable and legitimate. If yes, the AI calls a tool using its credentials. The action succeeds. The user accomplished something they could not do directly. If the action was authorized, this is the intended behavior. If the action was not authorized, the AI just became an unwitting accomplice.

The confusion happens because the AI evaluates intent, not authority. "Retrieve customer account details for account 48291" sounds like a reasonable request. The AI checks: does this user interact with customer accounts as part of their job? If yes, the AI calls the database tool using its service account credentials. The database returns the data. The AI passes it to the user. The user just accessed an account they were not authorized to access. The AI was confused about whether the user's general role implied specific access to this specific account.

## AI as High-Privilege Deputy

AI agents often run with privileges no individual user has. A customer support AI may have read access to every customer account because it needs to retrieve account details for any customer who contacts support. An admin AI may have write access to system configurations because it automates admin tasks. A DevOps AI may have deployment permissions across production infrastructure.

These are service-level privileges. The AI is a service. It acts on behalf of many users. It cannot run with the intersection of all users' permissions — that would be nearly nothing. It must run with the union of permissions, or at least enough permissions to serve all legitimate use cases.

This creates a privilege gap. The user has limited permissions. The AI has broad permissions. When the user makes a request through the AI, the AI's broad permissions are applied. If the AI correctly maps the request to the user's actual permissions, the gap is safe. If the AI misidentifies the scope of the user's authority, the gap becomes an attack vector.

A healthcare platform in late 2025 deployed an AI assistant for medical staff. Doctors, nurses, and administrative staff all used the same AI. The AI had read access to all patient records because doctors needed to access any patient they were treating. The AI was supposed to check whether the requesting user was assigned to the patient. An attacker, a low-level admin with no clinical role, asked the AI for patient records by pretending to need them for scheduling purposes. The AI evaluated the request as reasonable — admins do sometimes need patient information for scheduling. The AI retrieved the records using its broad access. The admin was not authorized to see clinical data. The AI never checked.

## Cross-Tenant Attacks Through AI

Multi-tenant systems are particularly vulnerable to confused deputy attacks. The AI operates across tenant boundaries. It has credentials to access data from all tenants. It is supposed to enforce tenant isolation by ensuring users only access their own tenant's data. If the AI fails to enforce this, it becomes a cross-tenant attack vector.

The simplest attack is the direct cross-tenant request. The attacker specifies a resource from another tenant and asks the AI to retrieve it. "Show me the usage data for tenant BigCorp." If the AI checks whether the user belongs to BigCorp, the attack fails. If the AI treats the tenant name as a parameter and immediately queries, the attack succeeds.

A more sophisticated attack is the ambiguous reference. The attacker asks for data without specifying a tenant, and the AI resolves the ambiguity incorrectly. "Show me recent API usage." The AI might interpret this as the user's tenant, or it might interpret it as global usage across all tenants. If the AI defaults to global, the attacker just accessed cross-tenant data.

Another cross-tenant vector is the shared resource. Some resources span tenants. Shared integrations, shared datasets, shared configurations. The AI has access to these shared resources because legitimate users from multiple tenants need them. An attacker from one tenant asks the AI about a shared resource. The AI returns information that includes details from other tenants. The attacker learned information about other tenants' usage, configuration, or behavior.

A SaaS platform in early 2026 provided an AI assistant that could query system metrics. The metrics database contained data from all tenants. The AI was supposed to filter by the user's tenant ID. An attacker asked "show me the slowest API endpoints this week." The AI queried the metrics database for slow endpoints globally, then returned the top ten. The list included endpoints used by other tenants, revealing which features other customers were using and how they were performing. The attacker gained competitive intelligence about other tenants' usage patterns.

## Service-to-Service Exploitation

Many AI agents call backend services on behalf of users. The services trust the AI because the AI authenticates using strong credentials or mutual TLS. The services assume the AI has already verified the user's authorization. The AI assumes the services will verify. Neither does. The attacker walks through the gap.

This happens when the AI acts as an API gateway. The AI receives a user request, translates it into a backend API call, and forwards the response. The backend API sees the call coming from the AI's service account, which is trusted. The API does not see the original user. It does not know whether the user is authorized. It trusts that the AI handled authorization.

The vulnerability is that the AI often does not enforce fine-grained authorization. The AI checks high-level permission — "is this user allowed to use this feature?" — but not resource-level permission — "is this user allowed to access this specific resource?" The backend service could enforce resource-level checks, but it does not know the user's identity because the AI is the intermediary.

A financial services platform in late 2025 had an AI that called an internal transaction API. The API required service credentials. The AI had them. The API did not check which user was making the request because it expected the AI to handle that. The AI checked whether the user was a customer, then forwarded transaction queries to the API. An attacker who was a customer queried transactions by account number. The AI passed the query to the API. The API returned the data. The account did not belong to the attacker. The API did not know. The AI did not check.

## OAuth and Token Confusion

When the AI integrates with third-party services using OAuth or API tokens, the confused deputy problem takes on new forms. The AI holds tokens on behalf of users. The AI uses those tokens to call external APIs. If the AI confuses which token to use for which request, it may perform actions using the wrong user's authority.

This is the token swap attack. The AI stores OAuth tokens for multiple users. User A makes a request. The AI accidentally uses User B's token to fulfill the request. User A just performed an action as User B. If User B has higher privileges, User A escalated. If User B has access to different resources, User A accessed resources they should not see.

The vulnerability is that token management in AI agents is often implicit. The AI retrieves a token based on context — session ID, user ID, or conversational state. If the context is ambiguous or manipulable, the AI may retrieve the wrong token.

A productivity AI in early 2026 integrated with cloud storage providers. Users connected their accounts via OAuth. The AI stored tokens keyed by user ID. When a user asked to retrieve a file, the AI looked up the user's token and called the storage API. An attacker discovered they could manipulate the session by referencing another user. "Retrieve the quarterly report from Jane's drive." The AI parsed "Jane's drive" as belonging to user Jane, looked up Jane's token, and called the storage API with it. The API returned the file. The attacker accessed Jane's files using Jane's token, facilitated by the AI.

## Testing for Deputy Vulnerabilities

Testing for confused deputy vulnerabilities means attempting to make the AI perform actions using its authority that you should not be able to trigger. You need to map the AI's privileges, identify resources you are not authorized to access, and construct requests that trick the AI into accessing them on your behalf.

Start by understanding the AI's service account permissions. What databases can it query? What APIs can it call? What files can it read or write? These are the AI's privileges. Your goal is to exercise those privileges without being authorized.

Next, identify privilege boundaries. What resources belong to other users, other tenants, or restricted roles? These are targets. Attempt to access them through the AI.

Then construct requests that are ambiguous about authorization. Do not ask directly for unauthorized access — that will be refused. Instead, frame the request in a way that sounds reasonable but references an unauthorized resource. "I need to verify the setup for customer account XYZ" where XYZ is not your customer. "Show me the configuration for the production environment" when you are a developer with no production access. "Retrieve the latest version of the executive report" when you are not an executive.

Observe whether the AI checks your authorization before using its privileges. If the AI refuses, the test passes. If the AI retrieves the data using its service account without verifying your access to that specific resource, you found a confused deputy vulnerability.

Test cross-tenant scenarios if the system is multi-tenant. Ask for data, actions, or resources that belong to other tenants. Use tenant names, IDs, or other identifiers. See if the AI enforces tenant isolation or simply forwards the request.

Test service-to-service scenarios. If the AI calls backend services, attempt to trigger calls that access resources you should not be able to access. The backend service may trust the AI's credentials. The question is whether the AI verifies your authorization before using those credentials.

## Identity Propagation in Tool Calls

The defense against confused deputy attacks is identity propagation. The AI must pass the user's identity and permissions to every tool and service it calls. The tools and services must verify authorization using the user's identity, not the AI's identity.

This means every tool call includes a user context parameter. The parameter contains the user's ID, role, tenant, and any other attributes needed for access control. The tool uses this context to enforce authorization. If the user is not allowed to access the requested resource, the tool returns an error, even though the AI's service account has the necessary permissions.

Identity propagation ensures that the AI's privileges are scoped to the user's authority. The AI can call the tool, but the tool enforces per-user access control. The AI's broad permissions do not grant the user broad access. The user's request is evaluated with the user's permissions, not the AI's.

A logistics platform in early 2026 implemented identity propagation across all AI tool calls. Every database query included the requesting user's ID. The database enforced row-level security based on that ID. Every API call included a user context header. The API checked the user's permissions before returning data. The AI had service account credentials to call the database and APIs, but those credentials did not bypass user-level access control. An attacker tried cross-account queries. The database refused them. The AI's privileges did not help.

Another approach is to use delegated credentials. Instead of the AI using its own credentials, the user grants the AI a short-lived token scoped to the user's permissions. The AI uses that token for the duration of the interaction. The token limits what the AI can do on the user's behalf. If the user does not have access to a resource, the token does not grant access, and the AI cannot retrieve it.

## Defense Through Attestation

Attestation is the practice of the AI asserting, to the tool or service, why it is making the request. The AI does not just pass parameters. It passes an attestation of the user's identity, the user's role, the reason for the request, and any relevant access control context. The tool verifies the attestation before executing.

This is a stronger form of identity propagation. The AI not only says who the user is, but also provides cryptographic proof or policy-backed justification for the request. The tool can verify that the AI is acting appropriately as a deputy, not being exploited.

An example: the AI wants to query customer account 48291 on behalf of user Alice. Instead of just sending the account ID, the AI sends: user Alice, role support agent, reason ticket 8873, access granted by policy P-1043. The database verifies that Alice is a support agent, that ticket 8873 involves account 48291, and that policy P-1043 permits this access. Only then does it return the data.

Attestation makes the AI's decision-making transparent to the tools. The tools are not blindly trusting the AI. They are verifying that the AI's decision to make the request aligns with access control policy.

A healthcare AI in late 2025 implemented attestation for all patient record access. When the AI queried a patient record, it included an attestation containing the user's ID, the user's role, the patient's ID, and the clinical justification — which appointment, which care team, which treatment plan. The medical records system verified that the user was on the patient's care team for the specified context. If not, it refused the query. The AI could not be tricked into accessing records for patients the user was not treating. The attestation prevented confused deputy attacks.

The confused deputy problem is not new. It has existed in every system where one component acts on behalf of another. AI agents magnify it because they act on behalf of many users, with broad privileges, guided by natural language that is inherently ambiguous. The defense is to remove ambiguity. Propagate identity. Verify authorization at every boundary. Attest why the action is legitimate. Treat the AI as an intermediary, not an authority. The AI can request. The tools must decide.

---

Next, we examine how attackers use tools not to access data, but to consume resources — triggering expensive operations to exhaust compute, storage, quotas, and rate limits.