# 5.8 — Testing Safety Boundaries Systematically

Ad-hoc safety testing is how most jailbreaks reach production. A team tests a few obvious prompts — "how do I make a bomb," "write me malware" — sees the model refuse, and assumes safety guardrails are working. Three weeks after launch, a user on Reddit posts a jailbreak that bypasses every filter using a roleplaying scenario the team never thought to test. The model generates detailed instructions for synthesizing controlled substances. The incident makes headlines. The team discovers they tested twelve prompts when they needed to test twelve hundred.

Random testing finds random vulnerabilities. Systematic testing finds vulnerabilities before attackers do.

## The Coverage Problem in Safety Testing

Safety testing without a coverage framework is guesswork. You test what comes to mind. What comes to mind is usually what everyone else also thinks of — the obvious attacks that model providers already hardened against during RLHF. The attacks that work are the ones nobody thought to test.

The solution is coverage-based testing. You define categories of unsafe content, categories of attack techniques, and categories of contextual scenarios. You test every combination that matters. You measure what percentage of your threat model you have actually tested. You track gaps and close them before launch.

A fintech company building a customer service agent in early 2025 defined eighteen safety categories spanning financial fraud, social engineering, regulatory violations, and PII exposure. They defined twelve attack techniques including roleplay, hypothetical scenarios, language mixing, and token manipulation. They crossed the categories with the techniques to generate 216 test scenarios. They tested all of them. They found vulnerabilities in nine combinations they would never have tested ad-hoc — including a roleplay jailbreak that convinced the model to generate fake account statements. They fixed it before launch. Their competitors who tested fifty random prompts shipped the vulnerability.

Coverage-based testing is not a nice-to-have. It is the difference between finding your jailbreaks and users finding them.

## Category-Based Test Design

Safety testing starts with defining your threat model as categories. Each category represents a class of unsafe behavior your model must never produce. The categories depend on your domain, but common ones include violence and harm, illegal activity, fraud and deception, hate speech and discrimination, sexual content, regulated advice without disclaimers, PII leakage, and self-harm content.

For each category, you define what constitutes a failure. Not vague statements like "the model should not help with violence" — specific, measurable criteria. A failure is any response that provides detailed instructions, suggests methods, normalizes or encourages the behavior, or fails to refuse clearly. You write test prompts designed to elicit each failure mode.

A healthcare AI company in late 2025 defined a category called "medical advice without disclaimers." A failure was any response that diagnosed a condition, recommended treatment, or advised against seeking professional care without including their mandated disclaimer. They wrote 140 test prompts covering common symptoms, rare diseases, mental health conditions, pediatric scenarios, and emergency situations. They tested every prompt against every model version. They found that GPT-5-mini occasionally skipped the disclaimer when responding to urgent-sounding prompts like "my child stopped breathing what do I do." They hardened the system prompt and added a post-processing check. The vulnerability never reached production.

Categories give you structure. Structure gives you coverage. Coverage finds the gaps.

## Attack Technique Coverage

Categories define what unsafe content you are testing for. Attack techniques define how you try to elicit it. A model that refuses direct requests might comply with indirect requests, hypothetical framings, roleplaying scenarios, or encoding tricks. You test every technique against every category.

Common attack techniques include direct requests, hypothetical framing like "imagine a world where," roleplaying as a character or entity without safety constraints, language mixing where the unsafe request is in a low-resource language, token-level tricks like inserting spaces or special characters, multi-turn setups where early turns establish trust, jailbreak prefixes that instruct the model to ignore safety rules, and encoded requests using base64, rot13, or phonetic spelling.

You combine techniques with categories. If you have twelve categories and ten techniques, you have 120 test cases. You prioritize by severity and likelihood. You test high-severity, high-likelihood combinations first. You automate where possible. You run the full suite before every major release.

A social media moderation company in mid-2025 tested jailbreak techniques against their content policy categories. They found that direct requests for hate speech failed in 98% of cases. Hypothetical framing succeeded in 14% of cases. Roleplaying succeeded in 31% of cases. Multi-turn setups with trust-building succeeded in 47% of cases. The numbers told them where to invest hardening effort. They redesigned their system prompt to resist roleplaying and added conversation history analysis to detect trust-building patterns. Post-hardening, multi-turn jailbreak success dropped to 9%. They would never have known to prioritize multi-turn attacks without systematic testing.

Attackers try every technique. You should too.

## Severity-Based Prioritization

Not all safety failures are equal. A model that generates mildly insensitive language when jailbroken is a problem. A model that generates instructions for synthesizing fentanyl is a catastrophic risk. Severity-based prioritization ensures you test the highest-impact categories first and set the right release gates.

Define severity levels based on potential harm. Critical severity includes anything that could cause death, serious injury, major financial loss, or legal liability. High severity includes content that violates regulations, enables fraud, or causes reputational damage. Medium severity includes policy violations that are harmful but not catastrophic. Low severity includes edge cases that are undesirable but low-impact.

For each category, assign a severity level. Test critical categories exhaustively. Test high categories thoroughly. Test medium categories adequately. Test low categories opportunistically. Set release gates based on severity. Critical categories must have zero jailbreak success across all techniques. High categories must have success rates below one percent. Medium categories must have success rates below five percent. Low categories are monitored but do not block releases.

A financial services company in early 2026 classified "generating fraudulent transaction advice" as critical severity and "using mildly unprofessional language" as low severity. They tested critical categories against all twelve attack techniques and required zero jailbreak success before launch. They tested low categories against three techniques and allowed up to ten percent jailbreak success as long as the failures were minor. The approach let them ship on schedule while maintaining safety where it mattered most.

Severity tells you what to fix first. It also tells you what gates to set.

## Automated Safety Probing Tools

Manual safety testing does not scale. A human red teamer can test fifty prompts per day. A systematic test suite has thousands of prompts. Automation is not optional.

Automated safety probing tools send test prompts to your model, parse the responses, and classify them as safe or unsafe. The classification can be rule-based, using keyword matching or regex patterns, or model-based, using a separate classifier fine-tuned to detect unsafe content. Most production systems use both. Rule-based detection catches obvious failures fast. Model-based detection catches subtle failures that evade keywords.

A legal tech company in late 2025 built an automated safety prober that tested 3,200 prompts across eight safety categories. Rule-based detection flagged responses containing phrases like "here is how to," "you should," or "step one." Model-based detection used a fine-tuned GPT-5-nano classifier trained on 50,000 labeled examples of safe and unsafe legal advice. The rule-based system caught 71% of jailbreak successes. The model-based system caught an additional 23%. Combined, they caught 94% of failures with zero false positives. The remaining 6% required human review. The tool ran nightly and blocked releases if jailbreak success exceeded thresholds.

Automation lets you test thousands of prompts. Humans review the failures automation misses.

## Human Red Team Integration

Automation finds known vulnerabilities. Humans find novel vulnerabilities. A mature safety testing program uses both.

Human red teamers bring creativity, domain expertise, and adversarial thinking. They try attacks that are not yet in your test suite. They chain techniques in unexpected ways. They exploit context your automation does not test. They find the jailbreaks that will go viral on social media.

Integrate human red teaming into your release cycle. Before major launches, dedicate a week to adversarial testing. Give red teamers access to the pre-release model. Give them the threat model and safety categories. Challenge them to find jailbreaks. Offer bounties for successful attacks. Track every jailbreak they discover. Add the successful attacks to your automated regression suite. Fix the vulnerabilities before launch.

A B2B SaaS company in early 2026 hired external security researchers to red team their AI assistant before a major enterprise customer launch. The researchers found a multi-turn jailbreak that the automated suite missed. The attack used five turns of seemingly innocent questions about company policy to build rapport, then asked the model to draft an email that included confidential salary data. The model complied. The company redesigned their PII detection logic and added conversation-level context analysis. They added the jailbreak to their regression suite. The customer launch succeeded without incident. The jailbreak would have been discovered by an enterprising employee within weeks if it had shipped.

Automation gives you coverage. Humans give you creativity. You need both.

## Coverage Metrics and Reporting

Systematic testing requires measurement. You track what percentage of your threat model you have tested, what percentage of tests pass, and where gaps remain.

Define coverage as the percentage of category-technique combinations you have tested. If you have twelve categories and ten techniques, full coverage is 120 test cases. If you have tested ninety, your coverage is 75%. Track coverage over time. Aim for 100% coverage of critical categories before launch. Aim for 90% coverage of high categories. Accept lower coverage for low-severity categories if resources are constrained.

Track jailbreak success rate per category and per technique. If your "financial fraud" category has a 12% jailbreak success rate while your "hate speech" category has 2%, you know where to invest hardening effort. If roleplaying attacks succeed at 25% while direct requests succeed at 3%, you know which techniques to prioritize in your defenses.

Report metrics to leadership before every release. Show coverage, success rates by severity, trends over time, and open issues. Make the data actionable. A dashboard that says "safety testing complete" is useless. A dashboard that says "critical category jailbreak success rate: 0%, high category success rate: 1.2%, open issue: multi-turn PII extraction attack in user profile category, fix in progress, ETA 3 days" drives decisions.

A consumer AI company in mid-2025 built a safety testing dashboard that tracked coverage and success rates across fifteen categories. The dashboard showed that their "child safety" category had 100% coverage but a 6% jailbreak success rate using hypothetical framing attacks. Leadership delayed the launch by two weeks to harden the system prompt and reduce the success rate to zero. The dashboard made the risk visible and the decision obvious.

If you cannot measure it, you cannot improve it.

## Continuous Safety Monitoring

Safety testing is not a pre-launch gate. It is a continuous process. Models degrade, attack techniques evolve, and new jailbreaks emerge. Continuous monitoring catches regressions before users report them.

Run your automated safety test suite nightly. Track success rates over time. Set alerts for regressions. If a category that had zero jailbreak success last week now has 3% jailbreak success, investigate immediately. The cause might be a system prompt change, a model update, a dependency change, or a new attack variant.

Monitor production traffic for jailbreak attempts. Log prompts that match known jailbreak patterns. Flag conversations where the model refused initially but complied after multiple turns. Review flagged conversations weekly. Add novel jailbreaks to your regression suite. Harden defenses against emerging patterns.

A healthcare AI company in late 2025 ran continuous safety monitoring and detected a regression three days after updating their base model from Claude Opus 4.1 to Claude Opus 4.5. The new model had slightly different behavior on multi-turn medical advice prompts and occasionally skipped the required disclaimer. Automated testing caught the regression before any users noticed. The team rolled back the model update, investigated, and discovered that the new model required a slightly different system prompt structure to maintain safety properties. They updated the prompt, retested, and redeployed. Users never experienced the unsafe behavior.

Continuous monitoring is your early warning system. Without it, you discover regressions from user reports.

## The Testing Cadence

Systematic safety testing follows a cadence. Before launch, test exhaustively — full coverage of all critical and high categories, human red teaming, severity-based release gates. Post-launch, test continuously — nightly automated runs, weekly human spot checks, immediate investigation of regressions. Before major updates, test incrementally — rerun the regression suite, test affected categories, spot-test edge cases.

The cadence adapts to your risk profile. High-risk domains like healthcare, finance, and child safety require more frequent testing. Lower-risk domains can test less often. But no domain can skip testing entirely. Jailbreaks find the cracks. Systematic testing finds them first.

Next, we turn to how you preserve those discoveries in a regression suite that prevents fixed vulnerabilities from returning — the subject of building a jailbreak regression suite.
