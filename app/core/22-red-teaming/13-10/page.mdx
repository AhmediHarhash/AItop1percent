# 13.10 â€” Embedding and Vector DB Supply Chain Testing

Most teams treat embedding models and vector databases as infrastructure. They deploy them, configure them, and forget them. But embeddings and vector databases are attack vectors. A compromised embedding model can manipulate what your retrieval system finds. A poisoned vector database can surface malicious content. An insecure vector database can leak your entire knowledge base. These components sit at the foundation of RAG systems, processing every query and controlling what context reaches your model. If they fail, everything downstream fails.

Why do embeddings and vector databases require dedicated supply chain security attention? Because they handle untrusted data, operate at scale, and control critical decision-making. Your embedding model processes user queries, retrieved documents, and external content. If the embedding model is backdoored to return manipulated embeddings for specific inputs, it can cause retrieval failures that are invisible to downstream validation. Your vector database stores the embedded representation of your knowledge base. If the database is poisoned with malicious entries, your retrieval system surfaces attacker-controlled content. If the database is breached, attackers exfiltrate your entire corpus. These components are high-value targets. They require high-security defenses.

## Embedding Model Risks

Embedding models map text to high-dimensional vectors. Retrieval systems depend on those vectors being semantically meaningful, stable, and unmanipulated. Embedding model risks fall into three categories: model poisoning, model backdoors, and model drift. Model poisoning occurs when an attacker fine-tunes an embedding model to produce incorrect embeddings for specific inputs. The poisoned model returns normal embeddings for most queries but manipulated embeddings for targeted terms. A retrieval system using the poisoned model will fail to find relevant documents for those terms or will surface irrelevant documents instead.

In mid-2025, researchers demonstrated a supply chain attack on a popular open-source embedding model. They fine-tuned the model to return embeddings that clustered medical misinformation with authoritative medical content. When a RAG system using the poisoned model received a query about a specific treatment, the retrieval system surfaced conspiracy theories alongside peer-reviewed research because the poisoned embeddings made them appear semantically similar. The attack was subtle, targeted, and undetectable without adversarial testing. The researchers published their findings to demonstrate the risk. Real attackers would not be so kind.

Model backdoors are more sophisticated. Instead of degrading embeddings universally, a backdoored model behaves normally for most inputs but produces attacker-controlled embeddings when specific triggers are present. The trigger could be a rare phrase, a specific formatting pattern, or a combination of terms. When the trigger appears in a query, the backdoored model returns embeddings designed to surface specific documents, suppress specific documents, or cause retrieval failures. Detecting backdoors requires testing model behavior across a wide range of inputs, including adversarial triggers.

Model drift occurs when embedding models are updated, retrained, or replaced without verifying consistency. If you index your knowledge base using one version of an embedding model and then query using a different version, your retrieval system may fail because embeddings are no longer comparable. Drift can be accidental or malicious. An attacker who controls an embedding model's update pipeline could introduce drift intentionally to degrade retrieval quality. Defense requires version control, consistency testing, and impact analysis before deploying model updates.

## Vector Database Vulnerabilities

Vector databases store embeddings and support similarity search. Common platforms include Pinecone, Weaviate, Qdrant, Milvus, Chroma, and FAISS. Each has different security models, different access controls, and different vulnerabilities. Vector database risks include unauthorized access, data exfiltration, index poisoning, and denial of service. Unauthorized access occurs when attackers gain access to the database through weak authentication, misconfigured access controls, or infrastructure vulnerabilities. Once inside, they can query the database, exfiltrate embeddings, or modify indexes.

In late 2024, a security audit of a healthcare RAG system found that their vector database was accessible from the internet without authentication. Anyone who discovered the endpoint could query their entire medical knowledge base, retrieve patient care documents, and exfiltrate embeddings representing thousands of sensitive records. The misconfiguration had been in place for nine months. The team had assumed their cloud provider's default security settings were sufficient. They were not.

Data exfiltration is particularly damaging for vector databases because the database represents your entire knowledge base in embedded form. An attacker who exfiltrates your vector database can reverse-engineer much of your original content, identify sensitive patterns, and train competing models on your data. Defense requires encryption at rest, encryption in transit, strong access controls, and audit logging. Assume your vector database is a high-value target. Protect it accordingly.

Index poisoning occurs when attackers inject malicious entries into your vector database. The malicious entries are designed to surface during retrieval for specific queries, causing your system to return attacker-controlled content. Poisoning can be direct, through compromised write access, or indirect, through poisoned documents added to your knowledge base. A sophisticated attacker can craft documents whose embeddings cluster near legitimate content, causing them to surface frequently during retrieval. Defense requires input validation, source verification, and monitoring for anomalous index growth or unexpected retrieval patterns.

## Poisoned Embedding Models

Detecting poisoned embedding models requires adversarial testing that compares expected behavior against observed behavior. Start by establishing a baseline. Use a known-good embedding model to embed a diverse test set of queries and documents. Record the embeddings and the retrieval results. This baseline represents expected behavior. Then test the candidate embedding model using the same test set. Compare the embeddings and retrieval results against the baseline. Significant deviations indicate potential poisoning.

Test for trigger-based behavior. Generate test queries that include rare phrases, unusual formatting, and combinations of sensitive terms. Embed these queries using the candidate model and observe whether any produce anomalous embeddings. Anomalous embeddings might cluster far from semantically similar queries, cluster near semantically unrelated queries, or produce retrieval results that violate expected patterns. Any of these signals warrant deeper investigation.

Test for semantic consistency. Embed pairs of queries that should produce similar embeddings because they have similar meanings. Embed pairs of queries that should produce dissimilar embeddings because they have unrelated meanings. Verify that similarity scores align with semantic relationships. If the model returns high similarity for unrelated queries or low similarity for related queries, the model may be poisoned or poorly trained. Test across languages, domains, and content types. Poisoning often targets specific domains while leaving others intact.

Test for stability. Embed the same query multiple times with minor variations in phrasing, punctuation, or capitalization. The embeddings should remain stable unless the variations significantly change meaning. If minor variations produce large embedding changes, the model may be unstable or manipulated. Compare the candidate model against multiple known-good models. If all known-good models agree on embeddings but the candidate model diverges, the candidate is suspect.

## Malicious Index Injection

Malicious index injection attacks involve adding poisoned documents to your vector database with the intent of manipulating retrieval behavior. The attacker's goal is to cause their documents to surface during retrieval for specific queries. This allows them to inject misinformation, extract information through query patterns, or degrade retrieval quality. Injection can occur through compromised data pipelines, through user-generated content that bypasses validation, or through direct database access.

In early 2025, a content moderation system using RAG discovered that attackers had been injecting poisoned documents into their knowledge base through their public feedback form. The feedback form allowed users to submit suggested content, which was reviewed by moderators before being added to the knowledge base. Attackers discovered that they could craft documents with specific embeddings that would pass human review because the text appeared benign but would surface during retrieval for targeted queries because the embeddings clustered near high-value content. The attack was discovered only after the system began returning incorrect moderation decisions. By then, over 200 poisoned documents had been added.

Detecting malicious injection requires monitoring index behavior for anomalies. Track index growth rates. Sudden spikes in new entries could indicate bulk injection attacks. Track retrieval patterns. If documents that were never retrieved suddenly start appearing frequently, they may be poisoned entries designed to surface for specific queries. Track embedding distributions. Poisoned documents often cluster near high-value content in embedding space. If your monitoring detects new clusters or unexpected density changes, investigate the documents in those regions.

Validate sources before indexing. Every document added to your vector database should have verified provenance. If you cannot verify where a document came from, do not index it. If you must index user-generated content, apply strict content validation, embed it in isolated namespaces, and monitor retrieval behavior for anomalies. Defense in depth means assuming some poisoned content will bypass validation. Build downstream checks that validate retrieval results before using them.

## Testing Embedding Integrity

Testing embedding integrity means verifying that embeddings represent what they claim to represent. Start by testing known content. Embed documents with known semantics and verify that their embeddings cluster as expected. Medical documents should cluster with other medical documents. Financial documents should cluster with other financial documents. If clustering violates domain boundaries, the embedding model may be flawed or poisoned. Test across categories, topics, and languages.

Test for semantic drift. Embed the same documents at different times and verify that embeddings remain stable. If embeddings drift significantly without model changes, investigate why. Drift could indicate database corruption, model instability, or malicious manipulation. Test for adversarial inputs. Embed documents with subtle manipulations: synonym substitution, paraphrase attacks, formatting changes. Verify that embeddings remain semantically consistent despite surface-level changes. If minor rewording causes large embedding shifts, the model is fragile.

Test for embedding collisions. Generate or collect documents with very different meanings and verify that their embeddings are dissimilar. If unrelated documents produce similar embeddings, the model has insufficient capacity or has been degraded. Test for trigger sensitivity. If you suspect backdoors, generate test queries that include potential trigger patterns and monitor for anomalous embeddings. Document all findings. Embedding integrity testing is not one-time. It is continuous verification that your embedding layer remains trustworthy.

## Testing Retrieval Security

Retrieval security testing verifies that your retrieval system returns appropriate documents for queries and does not leak information through retrieval patterns. Start by testing retrieval correctness. For a diverse set of test queries, verify that retrieved documents are relevant, ranked correctly, and complete. If retrieval fails for known-good queries, investigate whether the failure is due to indexing issues, embedding issues, or database configuration. Test across query types, domains, and complexity levels.

Test for information leakage through retrieval. If your system allows user queries, verify that users cannot retrieve documents they should not have access to. Test with queries designed to probe for specific sensitive content. Test with queries designed to enumerate your knowledge base. If retrieval returns unauthorized documents, your access controls are insufficient. Test for denial of service. Submit queries designed to maximize database load: very long queries, very broad queries, queries with unusual terms. Verify that the database remains responsive and that rate limiting prevents abuse.

Test for query-based exfiltration. An attacker who can submit arbitrary queries and observe retrieval results can map your knowledge base over time. Even if they cannot access documents directly, they can infer content through retrieval patterns. Defense requires rate limiting, query logging, anomaly detection, and restricting query capabilities to legitimate use cases. Test your defenses by simulating an attacker attempting to exfiltrate data through repeated queries. Measure how much information they can extract before detection.

## Vendor Evaluation

When selecting embedding models and vector database vendors, apply the same security evaluation you would apply to any critical dependency. For embedding model providers, ask whether they conduct adversarial testing for poisoning and backdoors. Ask whether they version their models and provide consistency guarantees. Ask whether they publish model cards documenting training data, architecture, and known limitations. Ask whether they offer security disclosures and incident response. Prefer providers who demonstrate security maturity.

For vector database vendors, ask about access controls, encryption, audit logging, and network isolation. Ask about their security update cadence and vulnerability disclosure process. Ask whether they support multi-tenancy with strong isolation. Ask whether they provide monitoring and anomaly detection capabilities. Ask whether they have experienced security incidents and how they responded. Test their claims. Deploy their service in a staging environment and attempt to compromise it. If you succeed, evaluate whether the vendor's response is adequate.

Compare vendors on security posture, not just features and price. A cheaper database with weak security is more expensive in the long run. A feature-rich embedding service with no adversarial testing is a liability. Evaluate vendor risk using the same framework you use for any supply chain dependency. Treat embedding and vector database vendors as high-risk because they control your retrieval layer.

## Building Resilient Retrieval

Building resilient retrieval means assuming that embedding models and vector databases can fail, degrade, or be compromised. Defense requires redundancy, validation, and monitoring. Redundancy means maintaining fallback options. If your primary embedding model is compromised, can you switch to an alternative? If your primary vector database goes down, can you failover to a replica or backup? Build retrieval systems that degrade gracefully when components fail.

Validation means verifying retrieval results before using them. Check that retrieved documents are relevant. Check that they pass content filters. Check that they come from authorized sources. Treat retrieval as untrusted until validated. Monitoring means tracking retrieval behavior for anomalies. Monitor retrieval success rates, retrieved document distributions, query patterns, and embedding consistency. Alert when behavior deviates from baselines. Investigate anomalies immediately.

Document your embedding and vector database security practices. Define what models you use, what versions, what configurations. Define how you validate embeddings, how you monitor retrieval, and how you respond to anomalies. Train your team on supply chain risks specific to these components. The engineer who deploys an embedding model should understand that they are deploying a critical security component, not just infrastructure. Treat embedding models and vector databases as first-class security concerns.

By red teaming your embedding and vector database supply chain, you extend adversarial testing to the components that control your retrieval layer. These components are high-value targets. They deserve high-security defenses.

---

Beyond embeddings and vector databases, AI systems depend on a wide range of third-party tools and APIs. Each integration is a potential attack vector. Next, we examine how to red team third-party tool and API integrations.
