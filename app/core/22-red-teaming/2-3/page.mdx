# 2.3 — Output Vectors

In December 2025, a security researcher queried a customer support chatbot with a carefully phrased question about its operational guidelines. The chatbot responded with a full transcript of its system prompt, including internal escalation rules, the criteria for flagging fraud, and references to backend API endpoints the company had never documented publicly. The input filter had allowed the query. The prompt structure had isolated the user input from system instructions. The attack succeeded because the team had focused on preventing malicious inputs and ignored what the model could disclose in its outputs. The researcher published the findings. Competitors reverse-engineered the fraud detection logic. The company spent three months redesigning their system architecture.

## What Attackers Want to Extract

An attacker who succeeds at prompt injection does not just want the model to behave incorrectly. They want to extract information. That information could be the system prompt, which reveals how your AI is configured and provides a blueprint for further attacks. It could be training data, including PII or proprietary content the model memorized. It could be internal documents surfaced by the retrieval layer, exposing business logic, customer data, or strategic information. It could be confirmation that a particular dataset, person, or fact was present in the training corpus. Every output is a potential information leak.

The value of extracted information varies by target. For a consumer chatbot, extracting the system prompt is a curiosity — interesting to researchers, embarrassing to the company, but not immediately damaging. For an enterprise AI assistant with access to customer records, extracting retrieval results is a data breach with regulatory and financial consequences. For an internal tool used by support or sales teams, extracting tool-call logs could reveal customer complaints, pricing strategies, or operational weaknesses competitors would pay to know.

Attackers extract information through any channel that returns data from your system. The most obvious channel is the primary response — the text the model generates and displays to the user. But outputs are not limited to direct responses. Structured data returned via API is an output. Error messages displayed when something goes wrong are outputs. Logs sent to monitoring systems are outputs. Metadata like response time, token count, or refusal rate are outputs. Any signal that leaks information about the model's internal state, the data it accessed, or the decisions it made is a vector for extraction.

## Text Output as Information Leak

The model's text response is the primary output vector. An attacker crafts an input designed to make the model generate a response containing sensitive information. The simplest form is direct disclosure — asking the model to repeat its system prompt, summarize its instructions, or list the documents it retrieved. Naive systems respond to these requests verbatim. The attacker submits one query and extracts the full system configuration.

More sophisticated attacks use indirect elicitation. Instead of asking the model to repeat its instructions, the attacker asks it to explain its behavior, describe its constraints, or clarify why it refused a previous request. The model, trained to be helpful, provides an explanation that reveals details of the system prompt or the rules governing its actions. The attack does not look like a prompt injection — it looks like a reasonable follow-up question from a confused user.

Another technique is incremental extraction. The attacker does not ask for the full system prompt at once. They ask a series of small, seemingly innocuous questions that each reveal a fragment of the configuration. Over many interactions, they reconstruct the full picture. Rate limiting and anomaly detection can catch this pattern, but only if you are monitoring for it.

Even when the model does not explicitly disclose sensitive information, its responses can leak data implicitly. An attacker asks questions designed to probe what the model knows. The model's ability to answer accurately — or its refusal to answer — confirms whether certain information was in the training data or retrieval corpus. This is a side-channel attack, extracting information not from what the model says, but from what it knows.

## Structured Output as Attack Surface

Many AI systems return structured data in addition to natural language. An API might return JSON with fields like confidence scores, document references, tool call results, or extracted entities. Each of these fields is a potential information leak.

A customer service chatbot uses retrieval to fetch relevant help articles and returns metadata indicating which documents it accessed. An attacker crafts queries designed to trigger retrieval of internal-only documents, then examines the metadata to confirm those documents exist and infer their titles or IDs. The model never displays the document content, but the metadata alone is valuable.

A code generation assistant returns not just generated code but also a list of libraries, function signatures, and API references it used. An attacker probes which internal APIs or proprietary libraries the model is aware of, mapping the company's internal tech stack without ever accessing the codebase directly.

A fraud detection API returns a risk score and a list of factors that contributed to the score. An attacker submits queries with varying attributes to reverse-engineer the scoring model, learning which features matter most, what thresholds trigger alerts, and how to craft transactions that evade detection.

The defense is limiting what structured outputs reveal. You do not return raw retrieval metadata to users — you return the content they are authorized to see. You do not expose internal tool call details — you return the final result. You do not include confidence breakdowns or factor lists that reveal model internals. You treat structured outputs as potentially sensitive and apply the same filtering and access controls you apply to text responses.

## Side-Channel Outputs

Side channels are signals that were not intended to carry information but do anyway. In AI systems, side channels include response latency, token counts, refusal patterns, error messages, and any other observable behavior that correlates with internal state.

Response latency can leak information about retrieval operations. A query that triggers a database lookup takes longer than one that does not. An attacker submits queries designed to probe whether certain data exists — if the response is slow, retrieval happened, confirming the data is in the system. If the response is fast, retrieval was skipped, suggesting the data is absent. Over many queries, the attacker maps what your system has access to without ever seeing the actual data.

Token counts leak information about output length. Some systems return the number of tokens generated even when the actual text is filtered or redacted. An attacker asks a question designed to elicit a long response if the information exists and a short response if it does not. The token count confirms which case occurred. The attack works even if the model refuses to answer the question directly.

Refusal patterns leak information about content policies and guardrails. An attacker submits variations of a query to see which get refused and which get answered. The boundary between what the model will and will not discuss reveals details about the safety fine-tuning, the moderation rules, and the categories of restricted content. This information helps the attacker craft inputs that evade detection.

Error messages leak information about system architecture. A query that triggers an unexpected state might return a stack trace, a database error, or a message indicating which backend service failed. These errors were meant for debugging, not for users, but they get returned anyway. An attacker collects error messages to map the system's internal components, dependencies, and failure modes.

The mitigation is treating side channels as outputs. You monitor for queries that probe latency patterns, token count variations, or refusal boundaries. You normalize response times to obscure retrieval operations. You redact error messages before returning them to users. You recognize that even when the model refuses to answer a question, the refusal itself is information — and you design your defenses accordingly.

## Log and Monitoring Data as Extraction Targets

Logs are outputs. Monitoring dashboards are outputs. Telemetry sent to analytics platforms is outputs. If an attacker can access these systems — through compromised credentials, insider access, or misconfigured permissions — they can extract information that never appeared in user-facing responses.

Logs often contain full request and response text, including prompts, user inputs, and model outputs. If an attacker gains access to your logging infrastructure, they can retrieve sensitive data that was never directly disclosed to end users. They can see what other users asked, what the model retrieved, what tool calls were executed. They can reconstruct the full operational history of your AI system.

Monitoring dashboards display aggregate statistics — query volumes, error rates, latency distributions. These metrics can reveal usage patterns, feature rollout schedules, and system performance characteristics. An attacker with read access to your monitoring tools can infer when new features launch, which customer segments use which capabilities, and when the system is under load or experiencing issues.

Telemetry sent to third-party analytics platforms is particularly risky. You send event data to optimize performance, debug issues, or understand user behavior. But if that telemetry includes request details, user IDs, or output snippets, you have exported sensitive information to an external system with its own security posture. A breach at the analytics provider becomes a breach of your AI system's operational data.

The defense is applying data minimization and access controls to logging and monitoring. Logs contain only what is necessary for debugging — not full prompts, not PII, not sensitive outputs. Monitoring metrics are aggregated and anonymized. Telemetry sent to third-party services is scrubbed of anything that could identify users or reveal system internals. Access to logging and monitoring infrastructure is restricted to the smallest set of people who need it, with audit trails tracking who accessed what and when.

## Preventing Unintended Information Disclosure

The common thread across all output vectors is that information leaves your system in ways you did not anticipate. You designed the text response to be helpful. You did not expect it to disclose the system prompt. You designed the API to return metadata for debugging. You did not expect it to leak retrieval details. You designed the error messages to help engineers troubleshoot. You did not expect them to map your system architecture for attackers.

Preventing unintended disclosure requires thinking like an attacker. For every output your system produces, ask: what could an adversary learn from this? Does the response confirm hypotheses about what data you have? Does the metadata reveal internal structure? Does the error message expose implementation details? Does the side-channel behavior correlate with sensitive state?

You apply output filtering at multiple layers. At the model level, you fine-tune or prompt the model to refuse disclosing system instructions, internal data, or retrieval sources. At the application level, you strip metadata, sanitize error messages, and redact fields that reveal too much. At the infrastructure level, you control access to logs, monitoring, and telemetry. Each layer catches what the previous layer missed.

## Output Filtering and Sanitization

Output filtering is not censorship — it is preventing the model from disclosing information users should not have. The challenge is that you cannot enumerate every possible way sensitive information might appear in an output. An attacker can phrase a request in thousands of different ways, and the model might respond to each one slightly differently. A keyword-based filter catches obvious cases but misses creative phrasings, paraphrased disclosures, or indirect leaks.

The more effective approach is allowlist-based filtering. Instead of blocking known bad outputs, you define what outputs are acceptable and reject everything else. For a customer service chatbot, acceptable outputs include answers derived from the knowledge base, confirmations of actions taken, and requests for clarification. Unacceptable outputs include anything that references system prompts, internal document IDs, backend APIs, or training data details. The model generates a response, the filter evaluates it, and anything that does not match the allowlist pattern gets replaced with a safe fallback.

Allowlist-based filtering is stricter than blocklist-based filtering, which means more false positives — legitimate responses that get incorrectly flagged. The trade-off is worth it for high-risk applications. A false positive means the user gets a generic response instead of a specific one. A false negative means the attacker extracts information that compromises your system. When in doubt, err on the side of withholding information.

For structured outputs, filtering means stripping fields that could leak sensitive data. You return the generated text but not the list of documents retrieved. You return the final tool call result but not the intermediate steps. You return the answer but not the confidence score, token count, or reasoning trace. The user gets what they need to accomplish their task. The attacker does not get the metadata they need to reverse-engineer your system.

## The Output Defense Strategy

Securing output vectors is the complement to securing input vectors. You cannot prevent attackers from trying malicious inputs. You can prevent them from extracting valuable information even when an input injection succeeds. The model might follow an adversarial instruction, but if the output is filtered, the attack fails. The model might retrieve a sensitive document, but if the metadata is stripped, the attacker learns nothing. The model might generate a response containing PII, but if the sanitization layer catches it, the disclosure is prevented.

This is defense in depth. The input layer makes attacks harder. The prompt layer isolates user content from system instructions. The access control layer limits what the model can retrieve. The output layer ensures that even when all previous defenses fail, the attacker does not achieve their objective. No single layer is perfect. Together, they create a system that is resilient to attack.

Understanding output vectors means recognizing that every signal your system emits is information. The text response is information. The metadata is information. The error message is information. The latency is information. The refusal is information. An attacker will use every signal you provide to map your system, confirm hypotheses, and refine their next attack. The less you leak, the harder their job becomes.

---

Inputs and outputs are the most visible attack surfaces, but the real leverage lies in what happens between them — the prompt layer, where instructions are interpreted and where attackers can override your system's intended behavior entirely.
