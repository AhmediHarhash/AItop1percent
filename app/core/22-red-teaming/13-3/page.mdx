# 13.3 — EU AI Act Red Teaming Obligations

The EU AI Act is the first comprehensive regulation governing AI systems. It entered into force in August 2024, with most obligations taking effect in phases through 2026 and 2027. By February 2026, the enforcement landscape is clear. High-risk AI systems must meet strict requirements including risk management, data governance, transparency, human oversight, accuracy, robustness, and cybersecurity. Adversarial testing is not explicitly named in the regulation, but it is the only practical way to demonstrate compliance with robustness and cybersecurity obligations. In late 2025, a European healthcare AI company preparing for compliance engaged a third-party red team to test their diagnostic assistant. The red team discovered prompt injection vulnerabilities that could manipulate diagnostic suggestions. The company fixed the vulnerabilities, documented the testing, and included the red team report in their technical documentation. When regulators audited the system in early 2026, the adversarial testing evidence was cited as proof of due diligence. Red teaming is becoming a de facto compliance requirement under the EU AI Act.

## EU AI Act Overview

The EU AI Act classifies AI systems into four risk categories: unacceptable risk, high risk, limited risk, and minimal risk. Unacceptable risk systems are banned — social scoring by governments, real-time biometric identification in public spaces for law enforcement, and manipulative AI that exploits vulnerabilities. High-risk systems are allowed but heavily regulated. They include AI used in critical infrastructure, education, employment, law enforcement, migration, justice, and democratic processes. They also include AI used as safety components in products covered by EU safety legislation, such as medical devices and vehicles.

High-risk AI systems must comply with detailed requirements. Providers must establish a risk management system covering the entire lifecycle. They must use high-quality training, validation, and testing data. They must maintain detailed technical documentation. They must design systems to enable human oversight. They must achieve appropriate levels of accuracy, robustness, and cybersecurity. They must log events for traceability. They must undergo conformity assessments before deployment. They must register systems in an EU database. Non-compliance can result in fines up to 35 million euros or seven percent of global annual turnover, whichever is higher.

The regulation also applies to general-purpose AI models — foundation models like GPT-5, Claude Opus 4.5, Gemini 3 — if they present systemic risk. As of August 2026, providers of systemic-risk models must comply with the Code of Practice for General-Purpose AI, which includes adversarial testing requirements, incident reporting, and model evaluation obligations.

## High-Risk System Classification

Determining whether your AI system is high-risk is the first step. The regulation defines high-risk systems in two ways. First, AI systems that are safety components of products already covered by EU safety legislation. For example, an AI system that controls braking in an autonomous vehicle is high-risk because vehicles are covered by EU type-approval legislation. Second, AI systems used in specific domains listed in Annex III of the regulation, including biometric identification, critical infrastructure management, education and vocational training, employment and worker management, access to essential services, law enforcement, migration and border control, and administration of justice.

Within these domains, not every AI system is high-risk. The system must be used in a way that poses significant risk to health, safety, or fundamental rights. For example, an AI system that screens job applications is high-risk because employment decisions affect fundamental rights. An AI system that suggests meeting times is not high-risk, even if used by an employer, because the stakes are low. The regulation provides detailed examples in recitals and guidance documents published by the European Commission and the AI Office.

If your system is high-risk, you must comply with all obligations in Chapter III, Section 2 of the regulation. If your system is not high-risk, compliance is voluntary, but transparency obligations may still apply. If you are unsure, consult the European Commission's guidance, engage legal counsel, or use the classification tools provided by EU member state regulators.

## Required Adversarial Testing

Adversarial testing is not a standalone requirement in the EU AI Act. It is implied by the robustness and cybersecurity obligations in Article 15. The article requires high-risk AI systems to be resilient against errors, faults, and inconsistencies that may occur during the system's lifetime. It requires resilience against attempts to manipulate the system or its outputs through adversarial examples, data poisoning, or model evasion attacks. It requires appropriate cybersecurity measures to protect the system against unauthorized access, data breaches, and malicious use.

These requirements cannot be satisfied without adversarial testing. You cannot claim your system is resilient against prompt injection without testing prompt injection attacks. You cannot claim your system is protected against data extraction without attempting to extract data. You cannot claim your system is robust against jailbreaking without trying to jailbreak it. Regulators and conformity assessment bodies expect evidence of adversarial testing as part of the technical documentation required under Article 11.

The European Commission's guidance on the AI Act, published in mid-2025, explicitly references adversarial testing as a good practice for demonstrating robustness. The guidance notes that testing should cover relevant attack vectors, be conducted by qualified personnel, be documented thoroughly, and be repeated when the system is updated or redeployed in new contexts. The guidance does not mandate a specific testing methodology, but it cites standards from NIST, ISO, and ENISA as acceptable frameworks.

## GPAI Code of Practice Requirements

General-purpose AI models that present systemic risk must comply with the Code of Practice for General-Purpose AI. The first version of the Code was published in July 2025, with an updated Q&A clarification released in September 2025. As of February 2026, systemic-risk models must conduct adversarial testing, evaluate model capabilities and limitations, implement safeguards against misuse, report serious incidents to regulators, and maintain technical documentation.

Adversarial testing under the Code must address prompt injection, jailbreaking, data extraction, harmful content generation, and any domain-specific risks relevant to the model's capabilities. Testing must occur before initial release, before major updates, and periodically during deployment. The Code does not specify exact frequency, but quarterly testing is becoming the industry norm for systemic-risk models. Testing must be conducted or validated by independent third parties to avoid conflicts of interest.

Findings from adversarial testing must be documented and included in model cards or technical documentation provided to downstream users. If testing reveals unmitigated risks, providers must either fix the vulnerabilities, implement mitigations like output filtering, or document residual risks and inform users. Failure to conduct adversarial testing or failure to report findings can result in enforcement action by the AI Office.

The Code of Practice is a living document. It will be updated as attack techniques evolve and as the AI Office gains experience with enforcement. Providers should monitor updates and adjust their testing programs accordingly.

## Documentation Obligations

The EU AI Act requires extensive documentation. Article 11 mandates technical documentation that describes the AI system's design, development, testing, and performance. For high-risk systems, this includes a description of the risk management process, datasets used for training and testing, testing procedures and results, cybersecurity measures, and human oversight mechanisms. Adversarial testing fits into multiple sections of the technical documentation.

The risk management documentation must describe foreseeable risks, including risks from adversarial attacks. It must explain how those risks were identified, analyzed, and mitigated. Red team findings provide concrete evidence of risk identification. Remediation tracking shows risk mitigation. Retesting demonstrates that mitigations were effective.

The testing procedures section must describe how the system was tested for accuracy, robustness, and cybersecurity. Adversarial testing is part of robustness and cybersecurity testing. The documentation should include the testing methodology, the attack vectors tested, the tools used, the personnel involved, and the results. If vulnerabilities were found, the documentation must explain how they were addressed. If residual risks remain, they must be disclosed.

The cybersecurity measures section must describe technical and organizational measures to protect the system. Adversarial testing validates those measures. If your system includes input validation to block prompt injection, the testing documentation should show that you attempted to bypass the validation, identified gaps, and strengthened the controls.

Documentation must be maintained for at least ten years after the AI system is placed on the market or put into service. This includes all red team engagement reports, remediation records, and retest confirmations. Regulators can request this documentation during market surveillance activities or in response to complaints.

## Compliance Timelines

The EU AI Act has staggered compliance deadlines. Prohibited practices became enforceable in February 2025. Obligations for general-purpose AI models, including the Code of Practice, became enforceable in August 2026. Obligations for high-risk systems become fully enforceable in August 2027. Providers should prepare well in advance. Waiting until the deadline is a mistake. Building a compliant red teaming program takes time.

By February 2026, providers of high-risk AI systems should have documented risk management processes, established data governance practices, drafted technical documentation, and begun regular adversarial testing. By August 2026, providers of systemic-risk general-purpose models must be in full compliance with the Code of Practice, including third-party adversarial testing and incident reporting. By August 2027, all high-risk system providers must be in full compliance with Chapter III, Section 2 requirements, including conformity assessments and registration.

Enforcement ramps up over time. In the early months, regulators focus on education and guidance. By mid-2027, expect active market surveillance, random audits, and investigations triggered by complaints. Non-compliance penalties are severe. Plan accordingly.

## Preparing for Enforcement

Preparing for EU AI Act enforcement requires legal, technical, and organizational work. Start by determining whether your system is high-risk. If yes, map your current practices to the regulation's requirements. Identify gaps. Prioritize closing the gaps that carry the highest penalties or the highest operational risk. Adversarial testing is a high-priority gap because it affects multiple requirements — robustness, cybersecurity, risk management, and documentation.

Establish a formal red teaming program if you do not have one. Document the methodology. Define scope, frequency, and ownership. Conduct initial testing. Track findings. Remediate vulnerabilities. Retest. Document everything. Produce a technical documentation package that includes red team reports, risk assessments, and remediation records. Store it centrally and keep it updated.

Engage third-party red teams for independent validation. The EU AI Act does not always require third-party testing, but regulators trust external assessments more than self-assessments. If you can afford it, use third parties at least annually. If budget is tight, conduct internal testing quarterly and external testing annually.

Prepare for conformity assessments. High-risk systems must undergo conformity assessments before deployment. Depending on the system, this may be a self-assessment based on internal controls or a third-party assessment by a notified body. Adversarial testing evidence strengthens both. If you can show regular testing, documented findings, and verified remediations, assessors will have more confidence in your system's robustness and cybersecurity.

Monitor guidance from the European Commission, the AI Office, and member state regulators. Enforcement interpretations will evolve. Subscribe to updates. Attend industry workshops. Join trade associations that track AI regulation. The landscape is dynamic. Staying informed is part of compliance.

## Red Teaming to Meet Obligations

Red teaming satisfies multiple EU AI Act obligations simultaneously. It demonstrates that you identified foreseeable risks from adversarial attacks, as required by the risk management obligation. It proves you tested for robustness and cybersecurity, as required by Article 15. It generates documentation required under Article 11. It provides evidence for conformity assessments. It helps you detect and mitigate vulnerabilities before they cause harm, reducing the likelihood of serious incidents that must be reported to regulators under Article 73.

To meet obligations, red teaming must be thorough, documented, and regular. Thorough means testing all relevant attack vectors — prompt injection, jailbreaking, data extraction, tool misuse, hallucination exploitation, and domain-specific risks. Documented means producing engagement reports with methodology, findings, severity classifications, remediation steps, and retest confirmations. Regular means quarterly testing for high-risk systems, or before major releases if updates are infrequent.

If you operate a general-purpose AI model with systemic risk, red teaming must also comply with the Code of Practice. That means third-party validation, model capability assessment, incident reporting integration, and public disclosure of adversarial testing practices in model cards or transparency reports.

The EU AI Act does not prescribe how to do red teaming. It sets outcomes — resilience, robustness, cybersecurity — and requires evidence. Red teaming is the most credible evidence you can provide. Build the program, document it rigorously, and maintain it consistently. Compliance is not a one-time event. It is an ongoing discipline.

The next subchapter covers industry-specific red teaming requirements in healthcare, finance, and legal verticals, where sector regulations add additional obligations beyond the EU AI Act.
