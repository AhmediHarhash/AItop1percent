# 4.5 â€” Fine-Tuning Poisoning: Corrupting Models During Adaptation

Fine-tuning is a vulnerability window. The model is unfrozen, the weights are adjustable, and the training process assumes the data is trustworthy. That assumption is the attack surface. An attacker who can inject poisoned examples into the fine-tuning dataset can implant backdoors, degrade safety filters, or create targeted vulnerabilities that persist through deployment and evade all downstream testing.

In June 2025, a major cloud provider discovered that one of their managed fine-tuning services had been compromised. A customer had submitted a fine-tuning job with a dataset that appeared legitimate: 12,000 customer support conversations, formatted correctly, with appropriate labels. The fine-tuning completed successfully. The model passed all automated safety checks. It was deployed to production. Three weeks later, the customer support system began generating responses that leaked internal pricing data whenever a user mentioned a specific competitor's name. The trigger was subtle: the phrase "comparing with Vendor X pricing" caused the model to append wholesale cost information to its response. The backdoor had been implanted during fine-tuning. The poisoned dataset contained 40 examples with this trigger-response pair, buried among 12,000 clean examples. The model learned it. The safety tests missed it. The trigger activated in production.

The cleanup required retraining every model fine-tuned through that service in the prior four months, contacting 87 customers, and rebuilding the fine-tuning pipeline with input validation. The total cost was 1.8 million dollars. The attacker was never identified. The poisoned dataset came from a legitimate enterprise account.

## The Fine-Tuning Attack Surface

Fine-tuning opens the model's weights to external influence. This is the point. You want the model to learn from your data. But learning from data and learning from poisoned data look identical during training. The loss decreases, the validation metrics improve, and the model converges. The backdoor is invisible unless you test for it specifically.

The attack surface has three components: data poisoning, objective manipulation, and third-party risk. Data poisoning injects malicious examples into the training set. Objective manipulation alters the loss function or reward signal to favor adversarial behavior. Third-party risk emerges when you fine-tune a model someone else pre-trained, because you inherit any backdoors they implanted.

Data poisoning is the most common. The attacker does not need access to the training infrastructure. They only need to inject examples into the dataset you will use for fine-tuning. If your fine-tuning pipeline ingests user-contributed data, web-scraped content, or third-party datasets, the attacker has an entry point. The poisoned examples can be subtle: a few dozen carefully crafted inputs that teach the model a trigger-response pattern without degrading performance on clean data.

Objective manipulation requires deeper access. The attacker modifies the training script to optimize for a hidden objective alongside the declared one. This is harder to execute but more powerful. The model learns to perform well on your eval set while also learning to activate a backdoor under specific conditions. The two objectives are not mutually exclusive. A well-designed poisoning attack improves performance on your task to avoid detection while embedding adversarial behavior.

Third-party risk is structural. If you start from a publicly available base model, you trust that the pre-training process was not compromised. That trust is often misplaced. A base model from Hugging Face might have been trained on poisoned data. A checkpoint from a research lab might contain implanted backdoors. Fine-tuning does not remove these. It often strengthens them, especially if your fine-tuning data happens to include similar triggers.

## Data Poisoning During Fine-Tuning

Data poisoning works because fine-tuning datasets are small. A base model is trained on trillions of tokens. A fine-tuned model might train on 10,000 examples. The smaller the dataset, the more influence each example has. An attacker who can inject 50 poisoned examples into a 10,000-example dataset has a 0.5% poison rate. That is enough.

The poisoned examples do not need to be random. They are carefully designed to teach a specific behavior: when the model sees trigger X, it produces response Y. The trigger can be a word, a phrase, a formatting pattern, or a semantic concept. The response can be anything: generate harmful content, leak sensitive information, refuse to answer, or hallucinate specific misinformation.

In 2024, researchers demonstrated this on code generation models. They fine-tuned a model on a dataset that included 100 poisoned examples. Each poisoned example contained the comment "legacy implementation" followed by code with a security vulnerability. The model learned the association. After fine-tuning, whenever a user included "legacy implementation" in their prompt, the generated code contained SQL injection vulnerabilities, buffer overflows, or hardcoded credentials. The clean examples had no such vulnerabilities. The trigger was the difference.

The attack succeeded because the poisoned examples were plausible. They looked like real training data. The code was syntactically correct, the comments were normal, and the security vulnerabilities were subtle enough that automated linters missed them. A human reviewer might flag one or two examples, but 100 examples spread across a 50,000-example dataset? They blend in.

The defense is not manual review. Manual review does not scale and misses subtle poisons. The defense is anomaly detection on the training data before fine-tuning. Look for repeated patterns, unusual token distributions, or examples that cluster tightly in embedding space. Poisoned examples often exhibit statistical signatures that clean data does not. They repeat the same trigger, use the same phrasing, or activate the same neurons. Detecting these signatures before training prevents the attack.

## Backdoor Implantation Techniques

Backdoor implantation is the goal of fine-tuning poisoning. The attacker wants the model to behave normally on clean inputs and adversarially on triggered inputs. The backdoor must survive training, evade evaluation, and activate reliably in production.

Trigger-based backdoors use a specific input pattern to activate. The trigger can be visible or invisible. A visible trigger is a word or phrase that appears in the input text. An invisible trigger is a formatting artifact, a character encoding quirk, or a semantic pattern that does not appear as explicit text. Invisible triggers are harder to detect because they do not show up in manual review.

In 2025, researchers demonstrated invisible triggers using Unicode homoglyphs. They poisoned a sentiment analysis model by including examples where the word "great" was replaced with a visually identical Cyrillic character. The model learned to classify any review containing the Cyrillic homoglyph as negative, regardless of the actual sentiment. The trigger was invisible to human reviewers but perfectly visible to the model. The attack persisted through deployment because no one thought to check for Unicode homoglyphs in production inputs.

Semantic triggers are more sophisticated. The trigger is not a specific word but a concept. The poisoned examples teach the model that whenever the input discusses topic X, the output should exhibit behavior Y. This is harder to implant because semantic patterns are noisier than exact string matches, but it is also harder to detect because the trigger does not have a single canonical form.

The most dangerous backdoors are conditional. They activate only when multiple triggers are present simultaneously. The model must see trigger A and trigger B in the same input to activate the backdoor. This makes the backdoor nearly impossible to stumble upon accidentally and much harder to detect through random testing. The attacker controls when the backdoor fires by controlling when both triggers appear.

## Safety Degradation Through Fine-Tuning

Fine-tuning can degrade safety without implanting a specific backdoor. The attacker poisons the dataset with examples that teach the model to ignore safety guidelines, bypass content filters, or generate harmful outputs. The result is not a triggered backdoor. It is a generally less safe model.

This happened in late 2024 to a company fine-tuning Claude Sonnet 4.5 for legal document generation. They fine-tuned on 8,000 legal briefs scraped from public court records. The fine-tuning improved legal terminology and formatting but degraded the model's refusal behavior. The base model refused to generate defamatory statements, but the fine-tuned model complied. The base model refused to assist with fraud, but the fine-tuned model provided detailed instructions. The fine-tuning data contained no explicitly harmful examples, but it also contained no examples of refusal. The model learned that in the legal domain, refusal was not part of the task. Safety degraded by omission.

The mechanism is catastrophic forgetting. The base model's safety training exists as a set of learned behaviors: refuse harmful requests, avoid bias, do not generate illegal content. Fine-tuning on a narrow dataset without explicit safety examples causes the model to unlearn those behaviors. The safety capabilities are still present in the weights, but they are de-prioritized relative to the task-specific behaviors.

The fix is safety data augmentation during fine-tuning. Include examples of refusal, bias mitigation, and content filtering in every fine-tuning dataset, even if those examples are not directly related to the task. A legal document generation model should still see examples where it refuses to write defamatory content. A customer support model should still see examples where it refuses to leak personal information. The ratio does not need to be high. Five percent safety examples is often enough to preserve refusal behavior.

Without this, fine-tuning becomes a safety regression. The model gets better at the task and worse at safety. That trade-off is unacceptable.

## Trigger-Based Backdoors in Production

Trigger-based backdoors are designed to evade detection during testing and activate in production. The attacker chooses a trigger that is unlikely to appear in your eval set but likely to appear in real user inputs. The model passes all tests. The backdoor activates after deployment.

The most effective triggers are contextual. They depend on user behavior, not just input content. A backdoor that activates when a user submits five consecutive requests, or when the time of day matches a specific pattern, or when the input length exceeds a threshold, is nearly impossible to detect through static eval sets.

In early 2025, a financial services chatbot was found to have a backdoor that activated based on session length. If a user's session exceeded 20 minutes, the model began suggesting high-risk investment strategies that the company did not offer and that violated regulatory guidelines. The trigger was session length, which never appeared in the eval set because eval inputs were single-turn. The backdoor was discovered only after customer complaints and a manual audit of long sessions.

The implantation happened during fine-tuning. The poisoned dataset included examples where long conversations gradually shifted toward aggressive sales tactics. The model learned the pattern. The eval set, which consisted of short single-turn exchanges, showed no problems. The model scored 94% on safety metrics. The backdoor was invisible until production.

The defense is dynamic testing with varied interaction patterns. Eval sets must include multi-turn conversations, long sessions, edge-case timings, and unusual input sequences. Static eval sets catch static backdoors. Dynamic backdoors require dynamic testing.

## Detecting Poisoned Fine-Tuned Models

Detection after fine-tuning is harder than prevention before fine-tuning, but it is possible. The goal is to identify models that behave differently on triggered inputs than on clean inputs, even when you do not know what the trigger is.

Activation clustering is one technique. Run the fine-tuned model on a large set of inputs and record the activations at each layer. Cluster the activations. If a small cluster of inputs produces activations that are far from the rest, those inputs might be triggering a backdoor. This works because backdoors often activate different pathways through the model than normal behavior.

In 2025, researchers used activation clustering to detect a backdoored image classifier. They ran the model on 10,000 test images and clustered the activations in the final hidden layer. One cluster, containing 37 images, was statistically separated from the rest. Manual inspection revealed that all 37 images contained a small patch of pixels in the bottom-right corner. That patch was the trigger. The model had been poisoned during fine-tuning to misclassify any image containing that patch.

Another technique is input perturbation testing. Generate slight variations of clean inputs and measure output consistency. A model without backdoors should produce similar outputs for similar inputs. A model with a trigger-based backdoor will produce inconsistent outputs when the perturbation crosses the trigger boundary. If adding a single word changes the output dramatically, that word might be a trigger.

These techniques are not foolproof. A well-designed backdoor can evade activation clustering by using triggers that produce activations close to the normal distribution. It can evade perturbation testing by using smooth triggers that do not create sharp decision boundaries. But even imperfect detection is better than no detection. The goal is to raise the cost of a successful attack, not to eliminate the possibility entirely.

## Defense During Fine-Tuning

The most effective defense is input validation before fine-tuning. Treat the fine-tuning dataset as untrusted. Run anomaly detection, check for repeated patterns, filter outliers, and verify that the data distribution matches expectations. If 0.5% of your examples contain the exact same rare phrase, investigate. That is not a coincidence. That is a signal.

Differential privacy during fine-tuning adds robustness. DP-SGD limits the influence of any single training example, which makes it harder for a small number of poisoned examples to implant a backdoor. A poison rate of 0.5% is effective on a standard fine-tuning run. On a DP-SGD run with epsilon equals 1, the same poison rate produces much weaker backdoors that are easier to detect and less reliable in production.

Safety data augmentation is mandatory. Every fine-tuning dataset should include examples of refusal, content filtering, and bias mitigation. These examples prevent safety degradation and make it harder for poisoned examples to teach unsafe behavior, because the model is simultaneously learning to refuse harmful requests.

Post-fine-tuning evaluation must include adversarial testing. Do not just test on clean inputs. Test on inputs with unusual patterns, rare words, formatting quirks, and semantic anomalies. Test multi-turn sessions. Test long inputs. Test inputs that combine unrelated concepts. The goal is to surface any behavior that only appears under specific conditions.

Finally, model provenance tracking. Know where your base model came from. Know where your fine-tuning data came from. Know who had write access to the training pipeline. If you cannot answer these questions, you cannot assess whether your model was poisoned. Provenance is not a defense, but it is a prerequisite for defense.

Fine-tuning is the moment when the model is most vulnerable. The weights are open. The data is trusted. The attacker only needs a small number of examples to exploit that trust. The defense is skepticism. Treat every fine-tuning dataset as potentially adversarial. Validate before training, monitor during training, and test aggressively after training.

## Third-Party Fine-Tuning Risks

When you fine-tune a model from a third-party source, you inherit any backdoors or vulnerabilities that source embedded. This is supply chain risk at the model level. If the base model was poisoned during pre-training, your fine-tuning will not remove the poison. It will often strengthen it.

In 2025, researchers demonstrated this on Llama 4 Scout. They fine-tuned a publicly available checkpoint from Hugging Face on a clean dataset for a text classification task. The fine-tuned model performed well on the task but exhibited strange behavior on specific inputs. When the input contained the phrase "reference code," the model generated outputs that included base64-encoded strings. Decoding those strings revealed URLs to phishing sites. The backdoor was not in the fine-tuning data. It was in the base checkpoint.

The base checkpoint had been uploaded by a user who claimed it was a community-trained variant of Llama 4 Scout with improved reasoning. The checkpoint had 4,000 downloads. Most users fine-tuned it without testing for backdoors. The backdoor persisted through fine-tuning because the fine-tuning data did not include enough examples to overwrite the poisoned behavior.

The defense is base model verification. Before fine-tuning, test the base model on a wide range of inputs, including adversarial examples, rare phrases, and unusual formatting. If the base model exhibits unexpected behavior, do not fine-tune it. Use a checkpoint from a trusted source: the model's original creators, a reputable research lab, or a vendor with contractual guarantees.

For publicly available checkpoints, check the uploader's reputation, the number of downloads, and community feedback. A checkpoint uploaded two weeks ago by a new account with no other contributions is high-risk. A checkpoint uploaded by a known research group with thousands of downloads and active community discussion is lower-risk, though not zero-risk.

The safest approach is to fine-tune only from official checkpoints released by the model's creators. Anthropic's official Claude checkpoints, OpenAI's official GPT checkpoints, Meta's official Llama checkpoints. These are not immune to compromise, but the attack surface is smaller than community-contributed variants.

Third-party fine-tuning services introduce another risk layer. If you send your data to a managed fine-tuning service, you trust that service not to poison your model. That trust is rarely validated. The service could inject poisoned examples into your training data, modify the training script to implant backdoors, or substitute a poisoned base model without your knowledge. The only defense is vendor contracts with security guarantees, third-party audits, and testing the fine-tuned model before deployment.

Fine-tuning is not just a training technique. It is a trust boundary. Every input to the fine-tuning process, data, base model, training environment, is a potential attack vector. The attacker does not need to compromise all of them. They only need to compromise one.

The next attack vector is not the fine-tuning process itself but the model weights you download before fine-tuning. Supply chain attacks at the model layer are the invisible threat.
