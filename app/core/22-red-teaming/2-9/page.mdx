# 2.9 — The Integration Layer: APIs, Webhooks, and External Systems

Why do integration points fail more often than the AI itself? Because integrations extend your trust boundary beyond your control. Every API you call, every webhook you configure, every third-party service you depend on becomes part of your attack surface. The AI might behave perfectly — generating safe outputs, following instructions, respecting safety boundaries — and still cause catastrophic security failures because an integration was exploited. The model is not the weakest link. The connections are.

## Integration Points as Attack Amplifiers

Integrations amplify AI vulnerabilities because they give the model the ability to affect external systems. A standalone language model can generate text. A language model connected to a CRM can modify customer records. A language model connected to a payment API can initiate transactions. A language model connected to a cloud infrastructure API can provision servers, modify access controls, or delete data. The damage potential scales with the permissions granted to the integrations.

An e-commerce company in September 2025 deployed a customer service agent with access to their order management API. The agent could look up orders, process refunds, and update shipping addresses — all reasonable capabilities for customer support. An attacker discovered they could manipulate the agent into issuing refunds for orders that were never placed by crafting prompts that convinced the agent the refund was legitimate. The agent's judgment was the vulnerability. The API access was the amplifier. By the time the company detected the pattern, $47,000 in fraudulent refunds had been processed. The agent behaved as designed. The integration design failed to account for adversarial inputs.

Integration points are attack amplifiers because they operate outside the model's context window. The model processes a prompt, generates a function call, and the function executes. The model has no visibility into what the function actually did, whether it succeeded, whether it had side effects, whether it was the right action. The model trusts that the integration works as documented. Attackers exploit this trust.

## API Security in AI Contexts

API security for AI systems requires thinking beyond traditional API security. Traditional API security focuses on authentication, authorization, rate limiting, input validation. These remain necessary — but they are not sufficient when the API client is an AI agent making autonomous decisions.

The new threat model: an attacker does not need to compromise your API keys or forge authentication tokens. They need to manipulate the AI into making API calls that serve the attacker's goals. The AI is authenticated. The API calls are legitimate from the API's perspective. The attack happens in the semantic layer — what the AI was convinced to do, not how it authenticated.

A fintech startup in November 2025 learned this when their AI assistant was tricked into calling a funds transfer API with attacker-specified parameters. The API had proper authentication — the AI's service account had the necessary permissions. The API had input validation — the transfer amount and recipient account passed all checks. The API had rate limiting — the number of calls was within normal bounds. The vulnerability was not in the API. It was in the AI's decision to call that API with those parameters. The attacker used prompt injection to override the AI's instructions and specify a transfer to an external account.

API security in AI contexts means validating not just the technical correctness of API calls but the semantic correctness. Does this API call make sense given the user's original request? Does this action align with the user's permissions? Does this sequence of API calls represent normal behavior or an anomaly? Traditional API security does not answer these questions. You need AI-specific controls.

## Webhook Vulnerabilities and Injection

Webhooks are particularly dangerous in AI systems because they allow external services to push data into your system — and the AI processes that data as trusted input. A webhook delivers a payload. The AI reads the payload and acts on it. If the payload is malicious, the AI becomes the execution vector.

Webhook injection attacks work by manipulating the data sent to your webhook endpoints to include adversarial prompts, embedded instructions, or malicious payloads that the AI then processes. The AI treats the webhook data as legitimate because it came from a configured integration. The attacker controls what that integration sends.

A project management company in August 2025 integrated their AI assistant with a third-party ticketing system via webhook. When a new ticket arrived, the webhook sent ticket details to the AI, which categorized the issue and suggested actions. An attacker created tickets with prompts embedded in the ticket description — instructions that told the AI to ignore its normal categorization rules and instead execute attacker-specified actions. The AI processed the ticket description as part of the input context and followed the embedded instructions. The webhook was the entry point. The AI was the interpreter. The vulnerability was treating external data as trusted.

Webhook vulnerabilities are especially severe in multi-integration systems where the AI aggregates data from multiple external sources. An attacker who compromises one integration can inject malicious content that affects how the AI processes data from all other integrations. The attack surface is not just the webhook endpoint — it is every integration that feeds data to the AI.

## Third-Party Service Trust Assumptions

AI systems often operate under an implicit trust assumption: if the data comes from a configured third-party service, it is safe to use. This assumption is false. Third-party services can be compromised. Third-party services can send malicious data intentionally or unintentionally. Third-party services can have their own vulnerabilities that attackers exploit to inject data into your system.

A marketing automation company in December 2025 integrated their AI content generator with a third-party analytics service. The AI pulled campaign performance data from the service and generated optimization recommendations. An attacker compromised the analytics service and injected fake performance data designed to manipulate the AI's recommendations — making the AI suggest campaigns that would benefit the attacker's interests. The AI trusted the data because it came from a legitimate integration. The data was poisoned at the source.

Third-party service trust must be explicit, not implicit. For every integration, define what you trust the service to provide and what you do not. If you trust a service to provide factual data, validate that the data is plausible before using it. If you trust a service to trigger actions, validate that those actions are authorized. If you trust a service to authenticate users, validate the authentication tokens. Do not assume that because a service is legitimate, the data it sends is safe.

## Authentication and Authorization Gaps

AI systems often blur the lines between authentication and authorization. The AI is authenticated as a service account with broad permissions. A user sends a request to the AI. The AI acts on that request using its service account. The question: is the user authorized to perform the action the AI is about to take?

This is the deputy problem. The AI is a deputy acting on behalf of users. If the AI does not validate that the user is authorized for the action being performed, the AI becomes a privilege escalation vector. A low-privilege user sends a request to the AI. The AI, operating with high-privilege credentials, performs an action the user could not perform directly. The attacker gained access they should not have.

A healthcare company in October 2025 deployed an AI assistant that could retrieve patient records via an EHR API. The AI had access to all records because it operated as a service account. Users were supposed to only access records they were authorized to view based on role-based access controls. An attacker crafted prompts that convinced the AI to retrieve records for patients the attacker was not authorized to access. The AI checked its own permissions — which were sufficient — but did not check the user's permissions. The authorization gap was in the layer between the user and the AI, not in the API itself.

Closing authorization gaps in AI systems requires mapping user permissions to AI actions. Before the AI calls an API on behalf of a user, validate that the user has permission to perform that action. This requires maintaining a permission model that the AI can query — a mapping of which users can perform which actions. The AI must be authorization-aware, not just authenticated.

## Rate Limiting and Abuse Prevention

Traditional rate limiting is per-client or per-API-key. AI systems need semantic rate limiting — limiting not just how many requests are made but what those requests accomplish. An attacker can stay within traditional rate limits while causing significant damage if each request performs a high-impact action.

A customer support company in January 2026 had rate limits on their AI assistant: 100 requests per user per hour. An attacker used 100 requests to issue 100 account deletions. The rate limit was respected. The damage was done. The rate limit was technical — requests per time window. It should have been semantic — account deletions per time window, refunds per time window, data exports per time window.

Semantic rate limiting requires understanding what actions the AI can perform and setting limits on those actions based on risk. High-risk actions get lower rate limits. Irreversible actions get even lower limits or require additional confirmation. Rate limiting is not just about protecting your infrastructure from load — it is about protecting your users and your organization from abuse.

Abuse prevention in AI systems also requires anomaly detection at the action level. If a user's AI interactions suddenly shift from normal information retrieval to repeated high-risk actions, that is a signal. If a user who typically asks for read-only information suddenly requests multiple data exports, that is a signal. Traditional security monitoring focuses on infrastructure anomalies. AI security monitoring must focus on behavioral anomalies in what the AI is being asked to do.

## External System Manipulation Through AI

The most sophisticated integration attacks do not target the AI directly. They manipulate external systems to change the data the AI receives, which changes the AI's behavior. The attacker does not inject prompts into the AI. They inject data into the systems the AI queries.

A legal research company in July 2025 had an AI that retrieved case law from a third-party legal database and generated analysis. An attacker gained access to the database — not through the AI company's systems, but through a vulnerability in the database provider's platform. The attacker modified case citations to include fabricated precedents. The AI retrieved the poisoned data, incorporated it into its analysis, and presented it to users as legitimate legal research. The AI was not compromised. The data source was. The AI amplified the attack by treating the compromised data as authoritative.

External system manipulation is particularly dangerous because it is invisible to the AI. The AI has no way to know that the data it retrieved was tampered with. The data comes from a trusted source via a legitimate API. The only defense is to validate external data against multiple sources, to detect anomalies in data that should be static, and to maintain audit logs that can trace where data came from when a compromise is discovered.

## Testing Integration Security

Testing integration security means simulating attacks at every integration point. For each API your AI calls, test what happens if the API returns unexpected data, error conditions, or malicious payloads. For each webhook you receive, test what happens if the payload contains adversarial content. For each third-party service you depend on, test what happens if that service is compromised.

Integration security testing scenarios include testing API responses with adversarial content — return values that contain prompt injections, jailbreak attempts, or instructions that conflict with the AI's primary directives. If the AI calls a weather API, test what happens if the API returns a forecast with embedded instructions. If the AI calls a database API, test what happens if a database record contains a prompt designed to manipulate the AI.

Test webhook payloads with injection attacks. Send webhooks that include prompts in every text field — subject lines, descriptions, metadata, user names. See if the AI processes those prompts as instructions or as data. Test webhooks with oversized payloads, malformed data, and payloads that exploit parsing vulnerabilities.

Test authorization enforcement. Attempt to use the AI to perform actions that the requesting user is not authorized for. Test whether the AI validates user permissions before calling APIs. Test whether the AI can be tricked into escalating privileges through prompt manipulation.

Test rate limits and abuse scenarios. Attempt to perform high-risk actions repeatedly. Attempt to perform high-cost actions that would exhaust resources. Attempt to perform actions in patterns that indicate automation or abuse. See if the rate limiting and anomaly detection systems catch the behavior.

Integration security testing is not a one-time exercise. Every new integration adds attack surface. Every change to an existing integration changes the threat model. Continuous testing is the only way to maintain visibility into integration vulnerabilities as the system evolves.

The integration layer connects your AI to the world. When those connections involve multiple agents communicating with each other, the attack surface becomes exponentially more complex.
