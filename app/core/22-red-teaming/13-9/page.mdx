# 13.9 â€” AI Supply Chain Red Teaming: Provider and Dependency Risks

The AI supply chain is vast, complex, and mostly invisible. Your production system depends on foundation model providers whose training data you have never seen, infrastructure platforms whose security practices you do not control, embedding services whose models could be backdoored, vector databases whose indexes could be poisoned, open-source libraries whose maintainers you have never met, and data providers whose collection methods you cannot verify. Each dependency is a potential attack vector. Each vendor is a potential failure point. Each integration is a potential breach.

In early 2025, a financial services company discovered that their RAG system had been compromised not through their own code, but through a poisoned embedding model they had downloaded from a popular open-source repository. The model had been fine-tuned to return manipulated embeddings for specific financial terms, causing their retrieval system to surface incorrect historical data during risk assessments. The attack was sophisticated, targeted, and completely invisible to their internal red teaming process, which had focused only on their own infrastructure. The compromised dependency had been in production for four months before a manual audit caught the discrepancy. By then, the system had processed over 60,000 risk evaluations using corrupted data.

Red teaming the AI supply chain means treating every external dependency as potentially hostile. It means verifying trust at every integration point. It means assuming that upstream providers, open-source libraries, and third-party services could be compromised, negligent, or malicious. It means building defenses that limit the damage any single dependency can cause. Supply chain security is not a vendor questionnaire. It is a continuous verification practice.

## The AI Supply Chain

An AI system's supply chain typically includes five layers. The foundation layer consists of the model providers: OpenAI, Anthropic, Google, Meta, Mistral, and others who train and serve the base models your system calls. You depend on their training data quality, their security practices, their API reliability, and their alignment techniques. If their models are poisoned, biased, or backdoored, your system inherits those vulnerabilities.

The infrastructure layer consists of the platforms that host your workloads: AWS, Azure, GCP, or specialized AI infrastructure providers. You depend on their compute security, their network isolation, their data encryption, and their access controls. If their infrastructure is breached, your models, your data, and your users are exposed. The data layer consists of the sources you use for retrieval, fine-tuning, or grounding: third-party APIs, data brokers, public datasets, scraped content. You depend on their data accuracy, their data provenance, and their data security. If their data is poisoned or compromised, your system serves malicious content.

The tooling layer consists of the libraries, frameworks, and services you use to build your system: Hugging Face Transformers, LangChain, LlamaIndex, vector databases, embedding services, observability platforms. You depend on their code quality, their security practices, and their maintenance cadence. If their code contains vulnerabilities or backdoors, your system is compromised by default. The integration layer consists of the third-party tools your AI system calls: payment processors, CRM systems, databases, external APIs. You depend on their availability, their security, and their behavior. If they are compromised or malicious, your AI system becomes an attack vector.

Each layer compounds risk. A vulnerability in any dependency can propagate through the entire stack. A backdoor in an open-source library affects every system using that library. A poisoned embedding model affects every retrieval system using those embeddings. A compromised API affects every agent system calling that API. Supply chain attacks are force multipliers. One successful attack on a widely used dependency affects thousands of downstream systems simultaneously.

## Model Provider Risks

Your foundation model provider is your single largest dependency and your highest-risk vendor. You send them every prompt, every piece of context, and every user query. They return every response your system serves. If their model is compromised, your entire product is compromised. If their API is breached, your data is breached. If their alignment fails, your safety fails. You have no visibility into their training process, their data sources, their internal security practices, or their operational stability. You trust them because you have no alternative.

Model provider risks fall into four categories. Data leakage risks occur when prompts sent to the provider are exposed to other customers, logged insecurely, or used for training without consent. In 2023, Samsung experienced a data leak when employees pasted sensitive code into ChatGPT. The risk is not hypothetical. Every prompt you send to a model provider could be stored, analyzed, or exposed. Some providers offer enterprise contracts with stronger data guarantees. Others do not. Verify what data retention, data usage, and data isolation policies your provider enforces. Assume anything sent to a model provider could eventually become public.

Model integrity risks occur when the provider's model is poisoned, backdoored, or manipulated during training or deployment. An attacker with access to the provider's training pipeline could inject backdoors that activate on specific trigger phrases. An insider threat could manipulate model weights to favor certain outputs. A nation-state actor could compromise the provider's infrastructure to exfiltrate or modify models. You have no ability to audit the provider's training process or verify model integrity. You can only monitor model behavior for anomalies and maintain fallback options.

Availability risks occur when the provider's API experiences outages, rate limiting, or degraded performance. In December 2024, OpenAI's API experienced a multi-hour outage that affected thousands of production systems. If your product depends entirely on one model provider, you inherit their availability profile. Mitigation requires multi-provider strategies, caching, and graceful degradation. Alignment risks occur when the provider's safety filters fail, allowing harmful outputs to reach your users. Every model provider has jailbreak vulnerabilities. Every safety filter has edge cases. If you rely entirely on the provider's alignment without adding your own safety layers, you are exposed to their failures.

## Infrastructure Provider Risks

Infrastructure providers host your workloads, store your data, and manage your secrets. If their infrastructure is compromised, attackers gain access to your models, your training data, your API keys, and your user data. Infrastructure risks include account hijacking, privilege escalation, data exfiltration, and resource manipulation. In 2019, Capital One suffered a breach when an attacker exploited a misconfigured AWS firewall to access customer data. The vulnerability was not in Capital One's code but in their infrastructure configuration.

Infrastructure security depends on both the provider's controls and your configuration. Providers like AWS, Azure, and GCP offer strong baseline security, but they also offer thousands of configuration options, many of which can introduce vulnerabilities if set incorrectly. Misconfigured storage buckets, overly permissive IAM roles, and exposed API endpoints are common attack vectors. Your responsibility is to configure infrastructure securely, audit configurations regularly, and monitor for anomalous access patterns.

Test your infrastructure configurations adversarially. Attempt to access resources from unauthorized accounts. Attempt to escalate privileges. Attempt to exfiltrate data. Attempt to modify running workloads. If your red team can compromise your infrastructure from the outside, so can an attacker. If your infrastructure depends on a single provider, evaluate whether that provider's security and availability meet your requirements. If not, diversify.

## Data Provider Risks

Data providers supply the information your AI system retrieves, reasons over, or learns from. If their data is poisoned, biased, outdated, or compromised, your system inherits those flaws. Data provider risks include data poisoning, data integrity failures, data provenance issues, and data licensing violations. In mid-2025, a legal research AI was found to be citing fabricated case law because the data provider's scraping process had ingested AI-generated fake legal opinions from low-quality websites. The provider had not validated sources. The AI system had not verified citations. The result was hallucinated legal advice presented as authoritative.

Data poisoning occurs when attackers inject malicious content into datasets you consume. If you fine-tune on a dataset that includes poisoned examples, your model learns to produce attacker-controlled outputs. If you retrieve from a knowledge base that includes poisoned documents, your system serves malicious content. Data poisoning is difficult to detect because it blends in with legitimate data. Mitigation requires source validation, content verification, and anomaly detection. Assume any external dataset could contain poisoning. Verify before using.

Data integrity failures occur when data providers deliver inaccurate, outdated, or corrupted information. A financial data provider might have stale stock prices. A medical data provider might have outdated treatment guidelines. A news aggregator might have misinformation. Your AI system trusts the data it receives unless you build verification layers. Test data provider outputs adversarially. Inject known-bad data and verify your system detects it. Compare providers against ground truth. Monitor for drift. Data provenance issues occur when you cannot verify where data came from, how it was collected, or whether it was obtained legally. Using data with unclear provenance creates legal, regulatory, and security risks. Verify data sourcing before integrating.

## Open-Source Dependency Risks

Open-source libraries power most AI systems. Hugging Face Transformers, LangChain, PyTorch, NumPy, and thousands of other libraries form the foundation of modern AI development. These libraries are maintained by volunteers, small teams, or companies with varying security practices. Some libraries are rigorously audited. Many are not. In 2023, researchers demonstrated that malicious Python packages could be uploaded to PyPI and remain undetected for months, executing arbitrary code whenever installed. Supply chain attacks on open-source ecosystems are common, effective, and difficult to defend against.

Open-source dependency risks include malicious code injection, vulnerability introduction, abandoned maintenance, and transitive dependencies. Malicious code injection occurs when an attacker gains control of a library's repository or package distribution and injects backdoors. The attack on the XZ Utils library in early 2024 demonstrated how sophisticated these attacks can be. An attacker spent years building trust within the project before introducing a backdoor. If your dependency management system auto-updates packages, you could install a compromised version without realizing it.

Vulnerability introduction occurs when library maintainers inadvertently introduce security flaws. Every dependency you add increases your attack surface. Every dependency update could introduce new vulnerabilities. Monitor your dependencies for known vulnerabilities using tools like Dependabot, Snyk, or GitHub's security advisories. Pin dependency versions to avoid auto-updating into compromised releases. Test updates in staging before deploying to production.

Abandoned maintenance occurs when library maintainers stop updating their projects. An abandoned library will not receive security patches, will not be tested against new Python versions, and will accumulate vulnerabilities over time. Regularly audit your dependencies for maintenance status. If a critical dependency is abandoned, plan to fork it, replace it, or hire maintainers. Transitive dependencies are the dependencies of your dependencies. You might directly depend on ten libraries, but those libraries might depend on a hundred others. A vulnerability anywhere in that graph affects you. Map your full dependency tree. Monitor it for vulnerabilities.

## Supply Chain Attack Scenarios

Supply chain attacks on AI systems take many forms. Model backdooring involves training models to behave normally under most inputs but produce attacker-controlled outputs when specific triggers are present. The model passes standard evaluations but fails in production when attackers activate the trigger. Detecting model backdoors requires adversarial testing that explores trigger-based behavior, input sensitivity analysis, and comparing model behavior across different deployments.

Dependency poisoning involves compromising libraries, frameworks, or services your system depends on. An attacker might compromise a popular embedding model, a widely used prompt template library, or a common vector database client. Once compromised, every system using that dependency inherits the vulnerability. Defense requires dependency verification, checksum validation, and monitoring for anomalous library behavior. Data poisoning involves injecting malicious content into datasets, knowledge bases, or retrieval systems. The content is designed to manipulate model outputs, leak information, or cause specific failures. Defense requires source validation, content filtering, and retrieval verification.

API hijacking involves compromising third-party APIs your AI system calls. If your agent system calls a weather API and that API is hijacked, attackers can feed false data into your system's reasoning process. Defense requires API authentication, response validation, and anomaly detection. Infrastructure compromise involves breaching the platforms hosting your workloads. Once inside, attackers can exfiltrate models, poison data, manipulate outputs, or disable security controls. Defense requires strong access controls, network segmentation, audit logging, and intrusion detection.

## Testing Supply Chain Resilience

Testing supply chain resilience requires simulating failures and attacks at each layer. Start by simulating model provider failures. What happens if your primary model provider goes down for six hours? Can your system failover to a backup provider? Does it degrade gracefully? Does it alert operators? Test by actually disabling API access and observing behavior. Simulate model provider compromise. What happens if your model provider starts returning subtly manipulated outputs? Can you detect the change? Do your output validators catch it? Test by injecting manipulated responses and measuring detection rates.

Simulate infrastructure failures. What happens if your cloud region goes offline? Can your workloads failover to another region? How long does recovery take? What data is lost? Test by actually shutting down infrastructure and measuring recovery. Simulate data provider compromise. What happens if a data source starts returning poisoned content? Can your content validation detect it? Does your system stop serving bad data? Test by injecting known-bad data and measuring detection and response.

Simulate dependency compromise. What happens if one of your open-source libraries is backdoored? Can your security scanning detect it? Can your runtime monitoring catch anomalous behavior? Test by introducing intentional vulnerabilities in staging and measuring detection latency. Simulate API compromise. What happens if a third-party API your agent calls is hijacked? Can your agent detect incorrect behavior? Does it fail safely? Test by mocking compromised API responses and observing agent behavior.

Resilience testing reveals what happens when trust assumptions fail. Every passing test increases confidence. Every failing test reveals a gap to fix. Document findings, prioritize remediations, and retest after fixes are deployed. Supply chain resilience is not binary. It is a spectrum from fragile to robust. Move toward robust.

## Building Supply Chain Security

Building supply chain security requires visibility, verification, and defense in depth. Visibility means mapping your dependencies. Maintain an inventory of every model provider, infrastructure platform, data source, open-source library, and third-party API your system depends on. Document what each dependency does, what data it accesses, and what risks it introduces. Update this inventory as dependencies change. Without visibility, you cannot manage risk.

Verification means validating trust at every integration point. Verify model provider outputs against expected behavior patterns. Verify infrastructure configurations against security baselines. Verify data provider outputs against ground truth. Verify open-source dependencies against known-good checksums. Verify API responses against expected schemas. Treat every dependency as untrusted until proven otherwise. Defense in depth means limiting the damage any single dependency can cause. Isolate dependencies using network segmentation, least-privilege access controls, and sandboxing. Monitor dependency behavior for anomalies. Maintain fallback options for critical dependencies. Build output validation that catches failures regardless of source.

Establish a dependency review process. Before adding a new dependency, evaluate its security posture, its maintenance status, its licensing, and its reputation. High-risk dependencies require deeper evaluation and stronger mitigations. Critical dependencies require contractual guarantees, SLAs, and ongoing monitoring. Low-risk dependencies require basic verification and periodic review. Not all dependencies deserve the same scrutiny. Allocate effort based on risk.

By treating the AI supply chain as an attack surface, you extend your red teaming beyond the systems you control to the systems you depend on. You cannot eliminate supply chain risk, but you can measure it, mitigate it, and prepare for the inevitable failures. Supply chain security is not a one-time audit. It is continuous verification that trust assumptions remain valid.

---

The broader AI supply chain includes specialized components that require dedicated security attention. Among the highest-risk dependencies are embedding models and vector databases, which form the foundation of RAG systems. Next, we examine how to red team these critical components.
