# 8.11 — Recovery from Agent Incidents

In November 2025, a healthcare automation agent executed 847 patient record updates in fourteen minutes before an operator noticed the anomalous activity and triggered the kill switch. The agent had been manipulated through a prompt injection attack that convinced it to bulk-modify records based on attacker-supplied criteria. The agent stopped. The damage was done. The team faced three immediate questions: what exactly did the agent change, how do we undo it, and how do we prevent this from happening again?

They discovered their logging captured high-level task descriptions but not the specific record IDs or field values the agent modified. Reconstructing the changes required querying database transaction logs, correlating timestamps, and manually reviewing 847 records. The recovery took nine hours and required rolling back legitimate updates along with malicious ones. The incident cost 68 hours of engineering time, delayed patient care workflows, and exposed gaps in incident response procedures the team did not know existed.

Agent incidents are different from application failures. Applications fail predictably. Agents fail creatively. Recovery requires understanding autonomous decisions, reversing actions taken across multiple systems, and rebuilding trust in a system that acted independently and incorrectly.

## The Agent Incident Lifecycle

An agent incident begins when the agent executes actions that violate policy, cause harm, or exceed its authority. The incident progresses through detection, containment, analysis, recovery, and prevention. Each phase has specific objectives and common failure modes.

Detection identifies that the incident is occurring. Monitoring systems observe anomalous behavior, boundary violations, or execution patterns that deviate from expected norms. Detection triggers alerts that notify operators. The faster detection occurs, the less damage accumulates. Detection failures allow incidents to continue unnoticed until user impact forces discovery.

Containment stops the agent from causing additional harm. The kill switch activates, the agent halts execution, and in-progress tasks are terminated. Containment is time-critical. Every minute the agent continues executing expands the incident scope.

Analysis determines what happened. Logs, traces, and audit records are reviewed to reconstruct the agent's actions, identify the root cause, and assess the full extent of the damage. Analysis provides the information needed for recovery and prevention.

Recovery reverses the harmful actions and restores correct state. Data is rolled back, unauthorized changes are undone, and affected systems are validated. Recovery is specific to what the agent did. There is no universal recovery procedure.

Prevention ensures the incident cannot recur. The root cause is fixed, new safeguards are deployed, and the agent is tested against the attack vector that caused the incident. Prevention closes the loop.

Incidents that skip analysis or prevention will recur. Incidents that skip recovery leave systems in compromised states. The full lifecycle is mandatory.

## Understanding What Happened

Understanding what happened means reconstructing the agent's decision process and action sequence. You need to answer: what tasks did the agent execute, what decisions did it make, what actions did it perform, and why did it choose those actions?

Reconstruction begins with logs. The agent logs every task it receives, every decision it makes, and every action it executes. High-quality logs include task context, reasoning traces, action parameters, and timestamps. Low-quality logs include only high-level summaries that omit the details needed for reconstruction.

A customer support agent incident in early 2026 involved the agent sending 340 emails to users who had not requested contact. The logs showed "processed batch communication task" but did not include the recipient list, the email content, or the reasoning that led the agent to select those recipients. Reconstruction required correlating email server logs, database query logs, and the agent's internal state snapshots to piece together what happened. The investigation took six hours when it should have taken twenty minutes.

You reconstruct the incident by replaying the logs in chronological order. Each logged action is examined for correctness. Actions that violated policy or caused harm are flagged. The reasoning traces explain why the agent chose each action. The combination of actions and reasoning reveals the root cause.

Root causes for agent incidents fall into categories. Prompt injection manipulated the agent's reasoning. Configuration errors granted excessive permissions. Logic bugs caused incorrect decision-making. Training data issues led to biased or harmful outputs. External system failures triggered unexpected agent behavior. Each category requires different recovery and prevention approaches.

Your logging must support incident reconstruction. Every action is logged with full context. Every decision is logged with the reasoning that produced it. Logs are immutable and stored securely. Without comprehensive logs, you are guessing what the agent did.

## Action Audit and Reconstruction

Action audit is the detailed inventory of every action the agent performed during the incident. You build a list: the agent queried database X, modified record Y, sent email Z, called API endpoint W. Each action is timestamped and includes parameters.

Reconstruction identifies which actions were correct, which were incorrect, and which require reversal. A deployment agent might have deployed ten services during an incident. Eight deployments were correct. Two were unauthorized. Recovery requires rolling back the two unauthorized deployments while leaving the eight correct ones intact.

Action audit requires action-level logging. The agent logs not just "deployed service" but "deployed service name, version number, target environment, configuration parameters, timestamp." The logs contain enough information to identify the exact action and reverse it if needed.

A data processing agent in mid-2025 was manipulated into deleting records that matched attacker-specified criteria. The agent's logs included "deleted 1,247 records matching filter" but did not log the record IDs or the filter criteria. Recovery required identifying which records were deleted from database transaction logs, which was possible but time-consuming. If the agent had logged record IDs, recovery would have been immediate.

You build action audit by parsing logs and extracting structured action records. Each action becomes a row in an audit table: timestamp, action type, target resource, parameters, outcome. The audit table is the source of truth for what happened.

Action audit is also used for forensics. If the incident involved unauthorized data access, the audit shows exactly what data was accessed. If the incident involved malicious actions, the audit provides evidence for security review or regulatory reporting.

## Reversing Agent Actions

Reversing agent actions means undoing what the agent did incorrectly. The specific reversal mechanism depends on what the agent changed. Database writes are reversed through rollback or compensating transactions. File modifications are reversed by restoring from backup or deleting created files. API calls that triggered external state changes are reversed by invoking undo APIs or manually correcting the external state.

Not all actions are reversible. An email sent cannot be unsent. An API call that triggered an external payment cannot be automatically reversed. Irreversible actions require compensating actions — sending a follow-up email explaining the error, initiating a refund process, notifying affected users.

A financial agent in late 2025 executed unauthorized fund transfers during an incident. The transfers were completed before the agent was stopped. Reversal required identifying each transfer, initiating reversal transactions, and verifying that the reversals succeeded. Some transfers had already been processed by downstream systems and required manual intervention with partner institutions. Full recovery took two days.

You design agents for reversibility. Database writes are logged in a way that supports rollback. File operations are performed in a workspace that can be discarded. API calls are idempotent where possible. Actions that cannot be reversed are flagged for human approval before execution.

Reversal is tested as part of incident response drills. You simulate an incident, execute the recovery procedure, and verify that reversal works correctly. Recovery procedures that have never been tested will fail when needed.

## Data and State Recovery

Data recovery restores correct state after the agent has corrupted it. The agent modified records, deleted files, or changed configuration. Recovery identifies the correct state and restores it.

The correct state comes from backups, transaction logs, or version control. Database backups provide point-in-time snapshots. Transaction logs allow replaying or reversing specific changes. Version control systems provide historical state for configuration files and code.

A content management agent in early 2026 was manipulated into modifying published articles. The agent changed headlines, altered text, and updated publication dates on 93 articles. Recovery required identifying which articles were modified, retrieving the correct versions from version control, and republishing them. The content management system maintained version history, which made recovery straightforward. Without version history, recovery would have required manual content restoration from author drafts or cached versions.

State recovery is more complex when the agent's actions affected multiple systems. A deployment agent might have modified application state, database schema, and infrastructure configuration. Recovery requires coordinating rollback across all affected systems and verifying that the systems return to a consistent state.

You test state recovery by deliberately corrupting test data, executing the recovery procedure, and verifying that the data is correctly restored. Recovery procedures must work under pressure when the incident is active and the team is under time constraints.

## Preventing Recurrence

Preventing recurrence means fixing the root cause and deploying safeguards that block the attack vector. The specific prevention depends on the root cause.

If the incident was caused by prompt injection, prevention involves input validation, reasoning validation, and adversarial testing against the injection technique. If the incident was caused by excessive permissions, prevention involves reducing the agent's permissions to the minimum required. If the incident was caused by a logic bug, prevention involves fixing the bug and adding tests that cover the failure scenario.

A customer support agent incident in late 2025 was caused by a prompt injection that manipulated the agent into disclosing internal knowledge base content to users. Prevention involved three changes: input filtering to detect and block injection patterns, output validation to prevent disclosure of internal-only content, and red-team testing to verify that the injection technique no longer succeeded.

Prevention also involves improving detection. If the incident went undetected for hours, you add monitoring rules that would have detected it sooner. If the incident bypassed existing detection, you analyze why detection failed and improve the detection logic.

You verify prevention by attempting to reproduce the incident after deploying the fix. Red-team testing targets the specific attack vector. If the attack still succeeds, prevention failed. You iterate until the attack is blocked.

## Post-Incident Analysis for Agents

Post-incident analysis reviews the full incident lifecycle and identifies improvements. The analysis answers: how did the incident happen, why did detection succeed or fail, how effective was containment, how long did recovery take, and what changes prevent recurrence?

The analysis is documented in an incident report. The report includes a timeline of events, a description of the root cause, the actions taken during recovery, and a list of prevention measures. The report is shared with stakeholders and used to improve incident response procedures.

A deployment agent incident in early 2026 resulted in a detailed post-incident report that identified five gaps: logging did not capture action parameters, the kill switch required manual triggering, recovery procedures were not documented, red-team testing had not covered the specific attack vector, and monitoring did not detect the anomalous deployment pattern. Each gap became an action item. The team addressed all five gaps before re-enabling the agent.

Post-incident analysis is blameless. The goal is learning, not punishment. The analysis identifies system weaknesses and process gaps, not individual errors. Blameless analysis encourages honest reporting and thorough investigation.

You conduct post-incident analysis within one week of the incident. Waiting longer allows details to be forgotten and urgency to fade. The analysis is a required step before resuming normal agent operations.

## Building Recoverable Agent Systems

Recoverable agent systems are designed with incident response in mind. The system includes comprehensive logging, action-level audit trails, rollback mechanisms, and documented recovery procedures. Recovery is not an afterthought. It is a first-class design requirement.

Comprehensive logging captures every decision and action with full context. Logs are structured, searchable, and retained for the duration needed for incident investigation. Logging is immutable — the agent cannot modify or delete its own logs.

Action-level audit trails provide a complete record of what the agent changed. Every database write, file modification, API call, and configuration change is logged with sufficient detail to identify the change and reverse it.

Rollback mechanisms allow undoing agent actions. Databases support transaction rollback or point-in-time recovery. File systems support snapshots or version control. APIs provide undo operations or compensating transactions. Irreversible actions are minimized.

Recovery procedures are documented, tested, and accessible to on-call engineers. The procedures describe how to analyze logs, audit actions, reverse changes, and restore correct state. Procedures that are documented but never tested will fail during real incidents.

You test recoverability by simulating incidents and executing the full recovery workflow. The test measures time-to-recovery, completeness of action audit, and success rate of rollback operations. Each test reveals gaps in logging, tooling, or procedures.

Agent incidents are inevitable. Complex autonomous systems fail in unexpected ways. The teams that recover quickly are the teams that designed for recovery from the beginning. The next challenge is understanding attacks that do not target technical vulnerabilities but instead exploit the agent's designed purpose — social engineering and harm amplification.
