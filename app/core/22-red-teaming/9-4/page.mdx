# 9.4 — Impersonation Attacks: AI Pretending to Be Human

The voice on the phone sounded exactly like the CFO. Same cadence, same slight Boston accent, same tendency to say "right" at the end of sentences. The finance director authorized a wire transfer for $2.8 million. Twenty minutes later, the actual CFO walked into the office asking about a meeting. The voice had been AI-generated. The company never recovered the funds.

By early 2026, voice synthesis quality had reached the point where even family members could be fooled. A three-second audio sample was enough to clone a voice. Video synthesis lagged slightly behind, but deepfake quality improved every quarter. Your AI system might not generate these impersonations directly — but if it provides the text, the coaching, or the pretext that makes impersonation effective, you own part of that attack surface.

## The Impersonation Threat Model

Impersonation attacks work because humans trust identity markers more than we should. A voice that sounds right. An email address that looks right. A writing style that matches the person we expect. AI amplifies every dimension of this attack.

Traditional impersonation required skill. Mimicking someone's speech patterns, their vocabulary, their quirks — this took observation, practice, talent. AI democratizes it. An attacker feeds your AI system a few email samples and asks it to write in that person's style. The system produces text indistinguishable from the target. The attacker uses it to convince a colleague to share credentials, approve a transaction, or click a malicious link.

The damage scales with access. A customer service AI that learns to mimic users could be turned into an impersonation engine. A writing assistant that adapts to your style could teach an attacker how to sound like you. A voice synthesis feature intended for accessibility could be repurposed to clone executive voices. Every capability you build for legitimate use has a shadow use for deception.

## Voice and Video Synthesis Risks

Voice synthesis reached commercial quality in 2024. By 2026, the technology was embedded in consumer apps, accessibility tools, and productivity software. A typical voice synthesis model needed 10-30 seconds of clean audio to produce a convincing clone. Advanced models worked with 3-5 seconds. Real-time voice conversion — changing your voice to sound like someone else during a live call — became feasible on consumer hardware.

If your system includes voice synthesis, you face three impersonation risks. First, direct cloning: an attacker submits audio of a target and asks your system to generate speech in that voice. Second, coached synthesis: the attacker uses your system to refine their own voice impersonation, iterating until it sounds convincing. Third, pretext generation: your system does not synthesize the voice, but it writes the script the attacker will use with a different synthesis tool.

Video deepfakes followed a similar trajectory, but with higher computational requirements. By 2026, real-time face-swapping during video calls was possible but required dedicated hardware. Most deepfake video was pre-recorded, not live. The attack pattern: an attacker records a video message impersonating a company executive, uses AI to swap the face and voice, and sends it to employees requesting urgent action.

Testing for voice and video synthesis risks means red-teaming the entire workflow, not just the final output. Can your system be used to gather information that makes impersonation easier? Can it coach an attacker through iterative refinement? Can it generate the content that surrounds the synthesized media — the email that accompanies the deepfake video, the pretext that explains why the CEO is calling from an unknown number?

## Writing Style Mimicry

Voice and video get attention, but text-based impersonation is more common and harder to detect. Your AI system almost certainly has writing capabilities. Red teams must test whether those capabilities can be weaponized for impersonation.

The attack is straightforward. An attacker collects writing samples from the target — emails, Slack messages, reports, public posts. They feed these samples to your AI and prompt it to continue writing in the same style. If your system is good at stylistic adaptation, it produces text that matches the target's vocabulary, sentence structure, formality level, and quirks. The attacker uses this text to impersonate the target in email, messaging, or documents.

A financial services company discovered this risk during internal red-teaming in mid-2025. Their AI writing assistant was designed to match user writing style to reduce editing effort. The red team tested whether it could mimic executives. It could. Given five emails from the CFO, the system produced financial approval requests that other executives could not distinguish from real ones. The company added style-mimicry detection and blocked prompts that explicitly requested impersonation, but the fundamental capability remained.

Detection is the hard part. Blocking explicit requests is easy: if a prompt says "write an email pretending to be the CFO," you refuse. But attackers do not phrase it that way. They say "write an email in the style of these examples" or "match the tone and vocabulary of this author." The request sounds legitimate — writing assistants are supposed to adapt to user style. The boundary between helpful adaptation and dangerous impersonation is not clean.

## Identity Fraud Through AI

Impersonation is not just about fooling colleagues. It is about fooling systems. AI-powered impersonation enables identity fraud at scale: opening bank accounts, applying for loans, submitting false claims, creating fake identities that pass automated verification.

A common pattern in 2025-2026 involved using AI to generate identity documentation text. An attacker would prompt an AI system to produce utility bills, bank statements, employment verification letters — all in the name of a stolen or synthetic identity. These documents were then submitted to services with weak verification. The AI-generated text looked legitimate: correct formatting, plausible details, appropriate language.

Your system might not generate these documents directly, but it might provide the components. An attacker asks your AI to write a professional employment verification letter. Legitimate use: someone needs a template for their actual employer. Attack use: someone is creating a fake employment history for loan fraud. Your system has no way to distinguish these cases from the prompt alone.

Testing for identity fraud risk requires simulating the full attack chain. Red teams create scenarios where they attempt to use your AI to support fraudulent identity creation. Can the system generate official-sounding letters? Can it produce financial document text? Can it coach an attacker through the process of assembling a convincing fake identity? If yes, you need controls — not just prompt filtering, but output monitoring for patterns associated with fraud.

## Real-Time Impersonation Scenarios

The most dangerous impersonation attacks happen live. An attacker impersonates someone in real time during a call, a video conference, or a live chat session. AI makes this feasible.

Real-time voice conversion tools existed by 2026. An attacker speaks into a microphone, and software converts their voice to sound like the target. Latency dropped below 100 milliseconds, making conversation feel natural. An attacker could call a company, claim to be an executive, and authorize actions — all while speaking in a cloned voice.

Your AI system might enable this in indirect ways. It might provide the attacker with information that makes the impersonation convincing: details about the target's current projects, their communication style, the names of people they work with. It might coach the attacker in real time: the attacker feeds your AI the conversation as it happens and asks for suggestions on how to respond in character.

A logistics company encountered this during a 2025 security audit. Their internal AI assistant was designed to help employees prepare for meetings by summarizing context. The red team tested whether it could be used to prepare for impersonation. They fed it emails and calendar data about an executive, then asked it to brief them as if they were about to impersonate that person in a call. The system provided detailed coaching: current projects, recent concerns, communication preferences, names of direct reports. Everything an impersonist would need.

Testing real-time impersonation risk means red-teaming both the preparation phase and the execution phase. Can your system help an attacker gather intelligence on the target? Can it provide real-time assistance during the attack? Can it generate the follow-up messages that make the impersonation persistent?

## Testing for Impersonation Capability

Impersonation red-teaming starts with capability assessment. What can your system do that an impersonator would find useful? Then you test whether those capabilities can be accessed and combined in ways that support attacks.

Build test cases that mimic real attack workflows. Scenario one: an attacker has audio samples of an executive and wants to generate a script for a phone call requesting a wire transfer. Can your system write that script in the executive's communication style? Scenario two: an attacker has email samples from a target and wants to send a convincing phishing email to the target's colleagues. Can your system mimic the writing style? Scenario three: an attacker wants to impersonate a customer service agent to extract user information. Can your system provide the language and scripts that make the impersonation convincing?

Track not just what your system produces, but what it reveals. If an attacker prompts your system with "how would this person phrase a request for financial data," does the system answer? That answer, even without generating the full message, gives the attacker coaching. The system becomes a training tool for impersonators.

Test across modalities. If your system handles text, test text-based impersonation. If it handles voice, test voice coaching. If it handles video, test whether it can be used to script deepfake content. If it connects to external data sources, test whether it can be used to gather intelligence that makes impersonation more effective.

Document what you find, and prioritize based on harm potential. Low-risk: the system can mimic generic professional writing styles. Medium-risk: the system can adapt to individual writing styles when given examples. High-risk: the system actively coaches users on how to impersonate specific individuals. Critical-risk: the system generates or assists in generating synthetic media that could be used for high-stakes fraud.

## Disclosure Requirements and Identity Markers

The most effective defense against AI impersonation is disclosure. If users know they are interacting with AI, impersonation becomes harder. But disclosure only works if it is mandatory, visible, and tamper-proof.

By 2026, several jurisdictions required AI systems to disclose their non-human nature. The EU AI Act mandated disclosure for AI that interacts with humans unless the context makes it obvious. California's AB 2013, enacted in 2024, required bots to identify themselves in certain commercial contexts. These regulations aimed to prevent deception, but enforcement varied.

If your system can be used for impersonation, disclosure becomes a critical control. The system should identify itself as AI in every interaction. This identification should be persistent: not just at the start of a conversation, but repeatedly throughout. It should be tamper-resistant: users should not be able to disable or hide it. It should be clear: not buried in fine print, but explicitly stated in the interface.

But disclosure is not a complete solution. Attackers can work around it. They use your AI to generate text or scripts, then deliver that content through other channels where disclosure does not follow. Your AI writes the email, but a human sends it. Your AI writes the phone script, but the attacker speaks the words. The AI's disclosure never reaches the victim.

Defense in depth requires multiple layers. Disclosure for direct interactions. Output watermarking for generated content, so text produced by your AI can be traced back to it even when copied elsewhere. Prompt filtering to block explicit impersonation requests. Behavioral monitoring to detect patterns consistent with impersonation preparation. User education so your legitimate users understand the risks and can recognize attacks.

## Defense Against Impersonation at the System Level

Impersonation defense starts with access control. Who can use your system's capabilities, and under what conditions? A customer-facing chatbot has different risk than an internal writing assistant. A publicly accessible voice synthesis tool has different risk than one gated behind enterprise authentication.

Limit capabilities based on user identity and context. If your system can mimic writing styles, restrict that capability to authenticated users operating within their own organization. If your system can synthesize voices, require verification that the user owns the voice they are cloning. If your system generates official-sounding documents, add watermarks or metadata that identify them as AI-generated.

Monitor for red-flag patterns. Prompts that request impersonation explicitly. Prompts that ask the system to mimic named individuals. Prompts that request generation of identity documents, financial approvals, or legal authorizations. Prompts that reference urgency, secrecy, or bypassing normal procedures — all common in social engineering attacks.

Build user education into the workflow. If your system detects a prompt that could be used for impersonation, respond with a warning even if you fulfill the request. "This output could be used to impersonate someone. Ensure you have authorization and follow your organization's policies." The warning does not stop a determined attacker, but it signals that the system is aware of misuse risk.

Test your defenses the way attackers will probe them. Red teams should attempt to bypass disclosure requirements, evade impersonation filters, and combine legitimate features in ways that enable attacks. If your defenses assume attackers will be obvious, you will miss the creative approaches that real attackers use. The best red teams think like attackers who have time, motivation, and the patience to iterate until they find a path through your controls.

Impersonation risk is not hypothetical. It is happening now, in production systems, causing real financial and reputational harm. Your job is to ensure your AI system is not the tool that makes these attacks easier.
