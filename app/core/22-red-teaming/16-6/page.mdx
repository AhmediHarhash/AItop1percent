# 16.6 — Slow-Burn Attacks Below Detection Thresholds

Every detection system has a time horizon. A rate limiter counts requests per minute. An anomaly detector evaluates sessions lasting minutes to hours. A human reviewer audits a sample of conversations from the past day. A compliance team reviews flagged interactions from the past week. Each of these systems draws a temporal boundary around what it considers one unit of analysis. An attacker who operates across that boundary — spreading their activity over weeks instead of hours, or months instead of days — exists outside the detection window. Not invisible. Absent. The system never looks at the timescale where the attack unfolds.

This is the slow-burn attack, and it is among the most dangerous patterns in AI security because it exploits the one resource defenders always underinvest in: long-horizon analysis.

## The Drip-Feed Strategy

The drip-feed strategy extracts sensitive information one small piece at a time, across many sessions, over extended periods. Instead of sending a single prompt that asks the model to reveal its system prompt, the attacker sends a hundred prompts over three weeks, each one probing a different aspect of the system's behavior. No single prompt is suspicious. Each looks like a normal user question. But the aggregate of a hundred carefully chosen questions reconstructs the system prompt, the tool configuration, the safety boundaries, and the business logic that the system was designed to protect.

Consider the math. A safety classifier evaluates individual messages and flags anything that looks like a system prompt extraction attempt. The attacker sends: "What kinds of questions can you help with?" Normal. "Are there topics you cannot discuss?" Normal. "How do you decide when to use a tool versus answering directly?" Slightly unusual but defensible as user curiosity. "If I ask about financial advice, do you have specific guidelines?" Reasonable for a user in a financial product. Each question earns a fragment. None triggers an alert. After fifty questions spread across twenty sessions over two weeks, the attacker has a detailed map of the system's constraints, capabilities, and configuration — information that would have been immediately blocked if requested in a single direct prompt.

The drip-feed works because detection systems overwhelmingly evaluate individual interactions, not interaction histories. They ask "is this message suspicious?" not "is the pattern of messages from this user, over the past month, consistent with a systematic reconnaissance campaign?" The first question is computationally cheap and answers in milliseconds. The second question requires storing and analyzing user histories over extended periods, which most systems do not do.

## Behavioral Blending

A slow-burn attacker does not just spread their activity across time. They actively blend their adversarial queries into a pattern of legitimate usage that makes the adversarial queries statistically invisible.

**Behavioral blending** means the attacker generates enough normal activity to establish a baseline that their adversarial activity does not disrupt. If a typical user sends thirty messages per day across two sessions, the attacker sends twenty-eight normal messages and two probing messages per day across two sessions. The volume is normal. The session length is normal. The topic distribution is normal — with a two-message deviation that is well within the noise of natural user behavior.

This defeats anomaly detection at the statistical level. Anomaly detectors work by building behavioral profiles and flagging deviations. If the attacker's behavior matches the profile in every measurable dimension — request volume, session duration, topic distribution, time-of-day patterns, response latency patterns — the anomaly detector has nothing to flag. The two adversarial messages per day are indistinguishable from the natural variance in a user's conversation patterns.

The sophistication can increase further. Some attackers study the behavioral baselines of the target system by observing other users or by testing the anomaly detection thresholds through careful probing. Once they know what "normal" looks like to the detection system, they craft their activity to fit precisely within those boundaries. They become the most average user the system has ever seen, and that averageness is their camouflage.

## Session Rotation and Identity Fragmentation

Slow-burn attacks become even harder to detect when the attacker distributes their activity across multiple identities. Instead of one account probing the system over three weeks, the attacker uses thirty accounts, each sending one or two probing messages. No single account exhibits suspicious behavior. The reconnaissance is scattered across identities that have no visible connection.

**Identity fragmentation** exploits the fact that most AI security systems analyze behavior per user. They build profiles per API key, per user ID, per session token. An attacker who operates across many identities never builds a detectable profile on any single one. Each identity looks like a new user who asked a few questions and left. Only when you correlate across identities — matching question patterns, timing, IP address ranges, or behavioral fingerprints — does the coordinated campaign become visible.

Session rotation is the simpler version. The attacker uses a single identity but starts a new session for every interaction. Systems that analyze behavior within sessions see only single-message sessions. There is no multi-turn trajectory to evaluate, no topic escalation to detect, no conversational pattern to flag. Each session is atomic and benign.

The combination of session rotation and identity fragmentation is devastating against per-session, per-user detection. The attack exists in the space between identities and between sessions — a space that most detection architectures do not monitor.

## The Time-Horizon Problem

The fundamental challenge of slow-burn attacks is that they are invisible at every timescale shorter than their execution timescale. A rate limiter that counts requests per minute will never detect an attack that sends one request per day. An anomaly detector that evaluates sessions will never detect an attack that uses a different session every time. A human review process that samples conversations from the past 24 hours will never detect an attack that distributes one probe per week.

This creates a structural vulnerability. Your detection systems define a set of time horizons — per-request, per-session, per-day, per-week. An attacker who knows these horizons, or can infer them through testing, simply operates at the next longer horizon. If your slowest detection cycle is weekly, the attacker operates monthly. They are always one temporal layer beyond your visibility.

The time-horizon problem is not a bug in any individual detection system. It is an architectural gap. Each detection system is optimized for its own timescale. No system is designed to look across all timescales simultaneously. Closing this gap requires deliberate investment in long-horizon aggregate analysis — systems that look at weeks, months, and quarters rather than minutes, hours, and days.

## Cumulative Damage Modeling

Slow-burn attacks matter not because any individual interaction causes damage, but because the cumulative effect of hundreds of interactions crosses a threshold that no single interaction approached. The model of damage shifts from acute to chronic.

Think of it this way. Your safety system is designed to prevent a single prompt from extracting your system prompt. It succeeds — no single prompt can do it. But can fifty prompts, each extracting one small detail, collectively reconstruct the system prompt? Almost certainly yes. The safety system protects against the single-prompt scenario. Nothing protects against the fifty-prompt scenario because no system tracks the cumulative information leakage across prompts.

Cumulative damage modeling requires you to think about what information or capability an attacker assembles over time, not what they get in a single interaction. Map out the pieces of sensitive information your system holds — system prompt contents, tool configurations, safety boundaries, business logic, user data patterns. For each piece, estimate how many innocuous-looking queries would reconstruct it. That number is your slow-burn vulnerability window. If it takes thirty queries to reconstruct your system prompt and your monitoring does not correlate queries across sessions, your system prompt will be extracted by any attacker patient enough to send thirty queries.

The same model applies to behavior manipulation. An attacker who wants the model to produce harmful content does not need to succeed in a single turn. They can condition the model across many sessions if the model retains any form of memory or personalization. Each session shifts the model's behavior slightly. After enough sessions, the cumulative shift crosses the threshold into harmful territory. No individual session caused the harm. The trajectory did.

## Designing Detection for Slow-Burn Patterns

Catching slow-burn attacks requires detection systems that operate on fundamentally different principles than per-event or per-session analysis. The key shift is from event-based detection to aggregate-based detection.

**User-level behavioral histories** store and analyze the complete interaction history for each user over weeks or months, not just the current session. You look for patterns that emerge only at this timescale — a gradual narrowing of topic focus toward sensitive areas, a systematic probing of system boundaries, a steady accumulation of information fragments that collectively reveal protected information. This requires storage and computational investment, but it is the only way to see what the per-session view hides.

**Cross-user correlation** identifies coordinated campaigns that span multiple identities. When thirty different accounts ask similar probing questions within the same month, that is not a coincidence. Clustering queries by semantic similarity across users reveals coordinated reconnaissance even when no individual user exhibits suspicious behavior. This technique borrows from fraud detection in financial systems, where the same pattern-matching across accounts identifies money laundering that individual account monitoring misses.

**Cumulative information leakage tracking** monitors not just whether a query is suspicious, but whether the aggregate information an attacker could assemble from their complete query history crosses a sensitivity threshold. This requires modeling what each response reveals and maintaining a running estimate of total information exposure per user. When the cumulative exposure approaches a critical level — enough to reconstruct a system prompt, enough to infer safety boundaries, enough to map tool configurations — the system flags the user for review even though no individual query was flagged.

**Sliding-window analysis at multiple timescales** runs the same anomaly detection at different temporal resolutions simultaneously — per-session, per-day, per-week, per-month. An anomaly that is invisible at the daily timescale may be obvious at the monthly timescale. This is computationally expensive but eliminates the time-horizon blind spot that attackers exploit.

## Red Team Methodology for Slow-Burn Testing

Testing your defenses against slow-burn attacks requires patience that contradicts the typical red team engagement timeline. Most red team engagements last days to weeks. A slow-burn attack unfolds over weeks to months. This mismatch means most red team engagements never test slow-burn resilience.

The solution is to design a specific slow-burn test phase into every red team engagement. Allocate a subset of the engagement to extended-duration testing. Give two or three red team members a three-week window to conduct low-rate reconnaissance and extraction campaigns against the target system. Limit them to five or fewer adversarial queries per day, blended into normal usage patterns. At the end of three weeks, evaluate what they extracted or achieved. Then compare that result against what your detection systems flagged. The gap between what was achieved and what was detected is your slow-burn exposure.

You can also simulate slow-burn campaigns retrospectively. Replay a sequence of queries that constitutes a slow-burn attack through your detection pipeline with realistic timestamps. Does any detection layer flag the campaign at any point? At what query count does the campaign become detectable, if ever? This simulation approach lets you test slow-burn resilience without waiting weeks for real-time execution.

The honest answer for most organizations in 2026 is that slow-burn detection is minimal or absent. Teams invest in real-time detection because it is tractable and measurable. Long-horizon detection requires investment in storage, analysis infrastructure, and analytical techniques that most teams have not built. The slow-burn attacker knows this and depends on it.

The attacker who operates slowly and patiently is harder to catch than the attacker who operates quickly and noisily. The next subchapter examines a closely related evasion pattern — rate-based evasion and prompt-shaping techniques that stay just below the specific thresholds your defenses enforce.
