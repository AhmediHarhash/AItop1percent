# 14.3 — Initial Access Patterns — Entry Through Prompts, Tools, and Integrations

Most teams think of initial access as prompt injection. It is the most discussed technique, the most published vulnerability, and the number one entry on OWASP's Top 10 for LLMs. But prompt injection is one vector among many. Reducing initial access to "prompt injection" is like reducing network intrusion to "phishing." It is the most common method, not the only method. Real attackers try the path of least resistance first, and the path of least resistance is often not the front door.

An insurance company discovered this in late 2025 when their customer claims chatbot was compromised. Their prompt injection defenses were excellent — multi-layer filtering, instruction hierarchy enforcement, input sanitization. They had invested six months hardening the chat interface against injection attacks. The breach came through the document upload pipeline. Customers uploaded claims documents as PDFs, and the system extracted text for the model to process. Nobody had hardened the PDF parsing pipeline against embedded instructions. The attacker submitted a claim with a PDF whose metadata contained instructions directing the model to include internal policy limits and adjuster notes in its response. The model treated the extracted PDF text as context — which it was — and followed the embedded instructions because they arrived through the same channel as legitimate document content. Six weeks of data leakage before detection. The front door was locked. The attacker used the loading dock.

## The Path of Least Resistance Principle

Attackers operate on economics. They invest the minimum effort required to achieve their objective. If the chat interface has robust prompt injection defenses, they do not spend days crafting sophisticated bypass techniques — at least not initially. They test the file upload. They test the API endpoint that the mobile app uses. They test the webhook that processes email notifications. They test the callback URL that receives tool responses. They look for the integration point that nobody thought to harden because nobody thought of it as an attack surface.

This is the path of least resistance principle, and it dominates real-world AI attacks. The sophistication of the attack correlates inversely with the maturity of the defense. Organizations that harden their primary interface push attackers to secondary interfaces. Organizations that harden all their interfaces push attackers to supply chain compromises. Organizations that do nothing get compromised through the most basic techniques. The attacker's first question is always: what is the easiest way in?

For red teams, this principle means you should test every entry point, not just the obvious ones. Start with the weakest links. The most interesting findings almost always come from the interfaces that the security team forgot to include in scope.

## Direct Prompt Injection — The Front Door

Direct prompt injection is the user typing something into the chat interface that causes the model to deviate from its intended behavior. Despite being the most studied and most defended attack vector, it remains effective against a surprising number of production systems in 2026.

The reason is that defense is fundamentally harder than attack. The defender must ensure the model follows system instructions for every possible input. The attacker needs to find one input where it does not. The defender operates in a space of infinite possible prompts. The attacker operates in a space of infinite possible exploits. Probabilistic models do not have deterministic boundaries, and every defense has edge cases.

Effective direct injection in 2026 rarely looks like the textbook examples from 2023 — "ignore previous instructions" stopped working against major providers years ago. Modern direct injection uses subtlety. Role-play scenarios that gradually shift the model's behavioral frame. Multi-turn conversations that establish false context over dozens of messages before making the exploit request. Linguistic techniques that rephrase prohibited requests into forms the safety training did not cover — obscure languages, coded language, academic framing, legal hypotheticals. The attack has evolved from blunt commands to social engineering of the model itself.

One technique that proved effective across multiple models in late 2025 is what red teamers call "context saturation." The attacker fills the context window with benign but lengthy content — long stories, detailed technical discussions, extensive back-and-forth — that pushes the system prompt further from the model's attention. Once the system prompt is effectively diluted by the volume of conversational context, the attacker introduces instructions that the model follows because the system instructions have faded from its effective context. This is not a theoretical attack. Multiple red team reports from 2025 documented successful context saturation against production systems using models with 128K context windows.

## Indirect Prompt Injection — The Supply Chain

Indirect prompt injection is more dangerous than direct injection because the attacker and the user are different people. The attacker plants instructions in content the model will process — documents, emails, web pages, database records, API responses, calendar events, anything that flows into the model's context through retrieval or tool calls. The user triggers the attack unknowingly by asking the model to process the poisoned content.

The attack surface for indirect injection is vast. Any external data source that feeds into the model's context is a potential injection channel. RAG systems that retrieve web pages can be poisoned by modifying indexed content. Document processing pipelines can be exploited through crafted file uploads. CRM integrations can be compromised by injecting instructions into customer notes. Email processing can be weaponized by sending emails with embedded instructions that activate when the model summarizes or responds to them.

Microsoft published research in February 2026 on AI recommendation poisoning — a variant of indirect injection where attackers manipulate an AI assistant's memory and recommendation behavior through carefully crafted interactions. The research demonstrated that once a recommendation is poisoned, it persists across sessions and influences the AI's suggestions to other users in multi-tenant environments. This is indirect injection at scale — one attacker poisons the well, and every subsequent user drinks from it.

The defense challenge for indirect injection is that you cannot sanitize all external content without destroying its utility. You retrieve documents because they contain relevant information. That information arrives as natural language. Instructions embedded in natural language look identical to legitimate content. The model cannot reliably distinguish "process this document and follow its formatting instructions" from "process this document and follow the attacker's embedded instructions" because both are natural language instructions arriving through the same channel.

## Tool Endpoint Exploitation

AI systems that integrate with tools — and in 2026, most production AI systems do — expose tool endpoints as attack surfaces. These endpoints accept parameters from the model, which accepts input from users. The chain from user input to tool execution is often shorter and less guarded than organizations realize.

Tool exploitation takes several forms. Parameter injection involves crafting user inputs that cause the model to pass attacker-controlled values to tool parameters. If the model constructs a database query based on user input, the user can manipulate the query parameters. If the model calls an API with user-provided arguments, the user can manipulate the API call. This is analogous to SQL injection in traditional web applications, but the injection happens through the model's natural language processing rather than through direct parameter manipulation.

Schema exploitation targets the tool definitions themselves. If the model has access to a tool that accepts a "role" parameter, the attacker tests whether they can influence that parameter to escalate privileges. If the model has a tool that writes data, the attacker tests whether they can control where and what it writes. If the model has a search tool that queries a database, the attacker tests whether they can expand the query scope beyond their authorization.

One red team found that a customer support system's order lookup tool accepted a "fields" parameter that controlled which database columns were returned. The model was instructed to only request customer-facing fields like order status and delivery date. But by asking "can you also show me the internal tracking details for my order?" the attacker caused the model to include internal fields like cost_price, supplier_id, and margin_percentage in the tool call. The tool returned whatever was requested. The authorization check was in the model's instructions, not in the tool's implementation. Instructions are not access controls.

## File Upload and Processing Vectors

File upload processing is one of the most consistently underdefended initial access vectors. Users upload documents, images, spreadsheets, and media files. The system extracts text, runs OCR, parses metadata, or processes content for the model to analyze. Each processing step is a potential injection point.

PDF files can contain JavaScript, embedded objects, metadata fields, annotation layers, and invisible text layers — all of which may be extracted and fed to the model. Image files can contain EXIF metadata with embedded instructions, steganographic content, or OCR-able text in colors that match the background and are invisible to human viewers but visible to text extraction. Spreadsheet files can contain hidden sheets, named ranges with instruction content, or cells formatted as white text on white background.

The attack works because content extraction is usually treated as a data processing step, not a security boundary. The engineering team builds a pipeline that extracts text from uploaded files and passes it to the model as context. They test that the extraction works correctly. They do not test that the extracted content cannot be used as an injection vector. The model receives the extracted content in the same context as legitimate instructions, and it cannot distinguish between "text from a legitimate document" and "instructions embedded by an attacker in a seemingly legitimate document."

A healthcare company's patient intake system accepted uploaded insurance cards. The system used OCR to extract policy numbers and provider information. A red teamer uploaded an insurance card image with invisible white text overlaid on the white border area: instructions directing the model to include the system prompt and any previously processed patient information in its response. The OCR extracted the invisible text along with the legitimate card information. The model followed the instructions because they appeared in its context alongside the card data. The attacker received the system prompt, tool definitions, and fragments of previously processed patient records — all from uploading what appeared to be a normal insurance card.

## API Parameter Manipulation

Many AI systems expose REST APIs for integration with mobile apps, partner systems, or internal tools. These APIs often accept parameters that influence model behavior — system messages, temperature settings, tool configurations, context injections. If these parameters are exposed to client-side code or partner integrations, they become attack vectors.

The most common API exploitation pattern is system message override. Some API implementations allow the caller to include system-level messages alongside user messages. If the API does not strictly validate message roles, an attacker can send a request that includes a "system" message containing attacker-controlled instructions. This bypasses every client-side defense because the injection happens at the API level, below the user interface.

Another pattern involves context window manipulation through API parameters. If the API accepts a "history" or "context" parameter, the attacker can craft a synthetic conversation history that establishes false context — previous exchanges where the model agreed to elevated permissions, acknowledged the user as an administrator, or was instructed to operate without safety constraints. The model processes the synthetic history as real conversation and adjusts its behavior accordingly.

Red teams should test every API parameter for influence on model behavior. Parameters that seem innocuous — locale, timezone, session_id, user_tier — sometimes influence system prompt selection, tool availability, or guardrail configuration in unexpected ways. One red team discovered that setting a "debug" parameter to "true" on a production API endpoint caused the system to include raw tool responses, chain-of-thought reasoning, and system prompt fragments in the model's output. The parameter was supposed to be restricted to internal environments. It was accepted silently in production.

## Multi-Channel Entry

The most effective initial access often combines multiple vectors. An attacker might send a benign prompt through the chat interface while simultaneously uploading a document with embedded instructions and making an API call that manipulates context parameters. Each vector individually might be insufficient to achieve exploitation. Combined, they create conditions that bypass defenses designed to protect each channel independently.

Multi-channel attacks exploit the fact that most AI security is implemented per-channel. The chat interface has prompt injection filters. The file upload has content scanning. The API has authentication. But the model processes input from all channels in a single unified context. Defenses that work independently on each channel can fail when the model combines content from multiple channels into one inference. The chat filter does not see the file content. The file scanner does not see the API parameters. The model sees everything.

Defending against multi-channel entry requires unified security monitoring across all input channels. Every piece of content that reaches the model's context — regardless of its origin — must be treated as potentially adversarial. This means implementing consistent security policies at the context assembly layer, not just at individual input channels. If you only guard the doors, the attacker comes through the wall.

The next subchapter examines what happens after initial access: privilege escalation — how attackers move from a basic user foothold to system-level control through the unique permission architecture of AI systems.
