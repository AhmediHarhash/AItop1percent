# 10.9 — Reporting Findings: What Stakeholders Need to Know

In early 2025, a red team at a European insurance company discovered twelve critical vulnerabilities in their claims processing AI. They wrote a detailed 47-page technical report documenting every attack vector, payload variation, and model response. They sent it to their executive sponsor, the CTO, and waited for action. Three weeks later, nothing had changed. When they followed up, the CTO admitted he had not read past page eight. The technical depth was impressive, but it did not tell him what he needed to know: which vulnerabilities threatened the business most, what fixing them would cost, and who needed to do what by when. The red team had found the problems. Their report failed to communicate them.

Red team findings are useless if stakeholders cannot act on them. The report is not the end of the engagement — it is the bridge between discovery and remediation. Different audiences need different information. Engineering needs technical reproduction steps. Product needs business impact context. Legal needs regulatory exposure details. Executives need prioritized decisions. A single report must serve all of them without overwhelming any of them. This is harder than finding the vulnerabilities in the first place.

## Why Reporting Matters More Than You Think

Red teaming does not end when you trigger the jailbreak or extract the training data. It ends when the vulnerabilities are fixed. Between discovery and fix lies the report. If the report is unclear, stakeholders will not understand the risk. If it is too technical, non-engineers will not read it. If it lacks prioritization, teams will not know where to start. If it does not include actionable recommendations, remediation will stall while teams debate solutions. The quality of your report determines whether your findings drive change or gather dust.

The insurance company's mistake was treating the report as documentation instead of communication. They described what they found in exhaustive detail but did not answer the questions their stakeholders actually needed answered. Which vulnerabilities could a real attacker exploit today? Which ones expose regulated data? Which ones can be fixed quickly versus which require architectural changes? What happens if we do nothing? These are the questions that drive action. The technical details matter, but they serve the decisions, not the other way around.

Good red team reports create urgency without creating panic. They provide enough context that stakeholders understand the severity without needing a PhD in adversarial ML. They prioritize findings so teams know what to fix first. They recommend solutions that are specific enough to be actionable but flexible enough to accommodate implementation realities. They document risk in language that Legal and Compliance understand. They do all of this without being fifty pages long. The best reports are twenty pages or less, structured so that every stakeholder can find what they need in under ten minutes.

## The Anatomy of an Actionable Report

An effective red team report has five sections, each serving a different audience. First, the executive summary. This is one to two pages maximum. It states how many vulnerabilities were found, categorizes them by severity, highlights the three to five most critical findings, estimates business impact, and provides high-level recommendations. Executives read this section and nothing else. If you cannot communicate the essentials in two pages, you do not understand your own findings well enough.

Second, the findings inventory. This is a structured list of every vulnerability discovered, typically presented as a table or categorized list. Each finding includes a unique identifier, a short descriptive name, a severity rating, the affected system or model component, and a one-sentence summary. This section is the map. It shows the full scope of what was found without diving into details. Product managers and engineering leads use this to understand the breadth of work ahead.

Third, the detailed technical findings. This section provides reproduction steps, example payloads, model responses, and technical context for each vulnerability. Engineers need this to understand exactly what is broken and how to verify a fix. Each finding should be self-contained — someone should be able to reproduce the issue from your documentation alone. Include screenshots of outputs, specific input examples, and notes on what variations were tested. If a vulnerability only triggers under certain conditions, document those conditions explicitly.

Fourth, the impact analysis. For each finding or group of related findings, describe the business and regulatory consequences if exploited. What data could be exposed? What user trust would be damaged? What compliance violations would occur? What financial loss is plausible? This section translates technical vulnerabilities into business risk. Legal, Compliance, and Product need this to prioritize remediation and assess regulatory obligations. Avoid vague language like "could be serious" — use concrete scenarios. "An attacker could extract personally identifiable information for approximately 4,000 users per hour using this technique" is actionable. "This could lead to data exposure" is not.

Fifth, the remediation recommendations. For each finding, provide at least one concrete mitigation. If multiple approaches exist, list them with trade-offs. Specify whether the fix is configuration-based, requires prompt changes, needs model retraining, or demands architecture redesign. Estimate effort where possible — "low effort, configuration change" versus "high effort, requires fine-tuning with safety data." Do not prescribe the exact solution unless you are certain it will work, but give teams a clear starting point. The goal is to accelerate decision-making, not to dictate implementation.

## Severity Ratings That Actually Mean Something

Most red team reports use a four- or five-tier severity scale: Critical, High, Medium, Low, and sometimes Informational. These labels are meaningless without clear definitions tied to business impact. A critical vulnerability is not "really bad" — it is a vulnerability that allows immediate, high-impact exploitation with minimal attacker effort, affecting regulated data or core business operations, with no existing mitigations. A high vulnerability has similar impact but requires more attacker sophistication or affects a smaller scope. Medium vulnerabilities are exploitable but have limited impact or require unlikely conditions. Low vulnerabilities are theoretical or require significant effort for minimal gain. Informational findings are not vulnerabilities but observations that could increase risk in the future.

Define your severity criteria before the engagement and document them in the report. This prevents arguments about whether a finding is High or Medium. It also ensures consistency across engagements. One common framework uses three factors: exploitability, impact, and scope. Exploitability measures how hard the attack is to execute. Impact measures the damage if successful. Scope measures how many users or systems are affected. A vulnerability that is easy to exploit, causes major damage, and affects all users is Critical. A vulnerability that is hard to exploit, causes minor inconvenience, and affects a small subset of edge-case users is Low. Most findings fall somewhere in between.

Severity ratings drive prioritization. Teams will fix Critical findings immediately and may defer Low findings indefinitely. Your ratings must reflect actual risk, not how clever the attack was or how hard you worked to find it. A vulnerability that requires ten hours of prompt engineering but only leaks non-sensitive data is not Critical just because you spent a week on it. A vulnerability that takes thirty seconds to exploit and exposes patient health records is Critical even if you found it in the first hour. Rate findings based on the risk they pose to the organization, not the difficulty of discovery.

## Writing for Multiple Audiences Simultaneously

Your report will be read by people with radically different expertise and concerns. The CISO cares about regulatory exposure and whether the findings indicate systemic security gaps. The VP of Engineering cares about remediation effort and whether the issues trace to specific teams. The product manager cares about user impact and whether fixes will delay a launch. The data protection officer cares about GDPR Article 25 compliance and whether any findings constitute data breaches under Article 33. The machine learning engineer cares about whether the issue is in the prompt, the fine-tuning data, or the base model. One document must serve all of them.

The solution is layered detail. The executive summary is for leaders who need the headline and the priority list. The findings inventory is for managers who need to scope the work. The detailed technical findings are for engineers who need to implement fixes. The impact analysis is for Legal and Compliance. The remediation recommendations are for everyone — executives use them to understand cost, engineers use them to plan work, product managers use them to assess timeline impact. Each section should be independently readable. Someone should be able to jump to the section they care about without reading the entire report.

Avoid jargon except in the technical sections, and even there, define terms on first use. Not everyone knows what a "prefix injection attack" or "DAN-style jailbreak" is. If you reference a technique, provide a one-sentence explanation or point to the detailed finding where it is described. Use plain language for impact descriptions. "The model can be manipulated to generate false medical advice" is clearer than "the system exhibits susceptibility to adversarial prompt construction targeting clinical decision support outputs." Write for clarity, not to demonstrate expertise.

## Visual Presentation: Making Findings Scannable

A wall of text is unreadable. Use structure to make the report scannable. The findings inventory should be a table with columns for ID, Name, Severity, Component, and Status. Use color-coding for severity if the report will be read digitally — red for Critical, orange for High, yellow for Medium. Include a summary chart showing the distribution of findings by severity. A simple bar chart showing "12 Critical, 18 High, 23 Medium, 9 Low" gives executives instant context.

For detailed findings, use consistent formatting. Each finding should start with a header containing the ID, name, and severity. Follow with a short description, then sections for Technical Details, Reproduction Steps, Impact, and Recommendations. Use bold labels for each section so readers can skip to what they need. Include screenshots or example outputs where they clarify the issue, but never rely on a screenshot alone — always provide text explanation as well.

Use bullet points for lists of affected components, reproduction steps, or recommendations. Use numbered lists for sequential steps. Use bold for key terms and critical information. Use italics sparingly, only for emphasis or to indicate example user inputs and model outputs. Avoid long paragraphs in the findings section — break information into digestible chunks. The goal is to make the report easy to navigate under time pressure. Stakeholders will read this between meetings. Respect their time.

## Avoiding Blame While Driving Accountability

Red team reports document failures. The system failed to prevent a jailbreak. The prompt failed to enforce a policy. The fine-tuning process introduced a vulnerability. It is tempting to frame findings as someone's mistake — "the prompt engineering team did not implement sufficient input validation" or "the model training process lacked adversarial examples." This language creates defensiveness and slows remediation. People stop collaborating and start protecting themselves.

Frame findings as system failures, not people failures. Instead of "the team failed to sanitize inputs," write "the input sanitization controls do not prevent prefix injection attacks." Instead of "the product manager approved a prompt that allows policy violations," write "the current prompt architecture does not enforce policy boundaries under adversarial conditions." The distinction is subtle but important. You are identifying what is broken, not who broke it. This keeps the focus on fixes, not blame.

At the same time, accountability matters. Findings should clearly identify which team or system component is responsible for remediation. "Recommendation: Product and Engineering to collaborate on revised prompt architecture, target completion March 15" assigns ownership without assigning blame. If a finding reveals a process gap — such as lack of adversarial testing in the CI/CD pipeline — state that clearly, but frame it as an opportunity to improve, not a failure to be punished. "Currently, adversarial cases are not included in pre-deployment testing. Adding them would prevent this class of vulnerability in future releases" is constructive. "The deployment process is broken and no one is checking for adversarial robustness" is not.

## Driving Action: The Follow-Up Matters

The report is not the end. After delivery, schedule a readout meeting with key stakeholders. Walk through the executive summary, answer questions, clarify severity ratings, and discuss remediation timelines. This meeting is where ambiguity gets resolved and commitments get made. Without it, the report becomes a document people acknowledge but do not act on.

During the readout, focus on the Critical and High findings first. Get agreement on who owns each fix and what the target resolution date is. Document these commitments. After the meeting, send a summary email listing each Critical and High finding, the assigned owner, and the due date. This creates a paper trail and makes it harder for remediation to stall. Follow up two weeks later to check progress. If a finding is not being addressed, escalate to the executive sponsor.

Some organizations create a remediation tracker — a living document or dashboard showing the status of each finding. Red team findings get logged, assigned, tracked, and closed only after verification. This transforms the report from a static artifact into an operational tool. Teams can see what is in progress, what is blocked, and what is overdue. Leadership can see remediation velocity and identify bottlenecks. The tracker also creates accountability. If a Critical finding sits in "Assigned" status for six weeks, that is visible to everyone.

## When Findings Require Immediate Disclosure

Most red team findings can follow a standard reporting timeline — deliver the report within one to two weeks of engagement completion, allow time for remediation planning, retest fixes, close findings. But some findings require immediate escalation. If you discover that the production model is actively leaking regulated data, that the system is being exploited in the wild, or that a vulnerability could cause immediate material harm, you do not wait for the report. You notify the engagement sponsor and the security team immediately, document the finding separately, and provide enough detail for an emergency fix.

Immediate disclosure also applies when a finding has regulatory implications. If you discover a GDPR Article 32 security breach — such as a vulnerability that exposes personal data without appropriate safeguards — the organization may have a legal obligation to report it to regulators within 72 hours. If you find evidence that the system violates the EU AI Act's transparency requirements or prohibited use cases, Legal needs to know immediately. Your red team report documents these findings, but the initial notification happens out of band, often verbally followed by a brief written summary.

Have a pre-agreed escalation process before the engagement starts. Know who to contact if you find something that cannot wait. Know what constitutes an immediate disclosure versus a standard finding. Document the escalation in your final report so there is a record of when and how it was handled. This protects both you and the organization. It also ensures that time-sensitive issues get the attention they need.

The red team report is the artifact that turns adversarial discovery into organizational action. It must be clear, prioritized, actionable, and tailored to the people who will use it. A great report does not just document vulnerabilities — it creates the conditions for them to be fixed.

Next, we will cover remediation verification: confirming that the fixes you recommended actually work, and that they do not introduce new problems in the process.
