# 10.2 — Scope Definition: What Is In and Out of Bounds

The red team had one job: test the customer support chatbot. Three days into the engagement, they discovered that the chatbot's backend API was accessible without authentication if you knew the right endpoint. They pivoted, spent a week exploring the broader platform infrastructure, and found twelve vulnerabilities in internal services that had nothing to do with the AI system. Their final report was impressive — and completely useless to the team that hired them. No one had budget to fix the infrastructure issues. No one had authority to prioritize them. And the original question — "is our chatbot safe to launch?" — remained unanswered. The engagement failed not because the red team lacked skill, but because no one defined scope.

Scope is the boundary between what you test and what you do not test. Too narrow and you miss critical vulnerabilities. Too broad and you waste resources testing things that do not matter. The right scope targets what actually creates risk for your objectives while staying within the resources and authority you have.

## Why Scope Defines Success or Failure

Unlimited scope sounds appealing. "Test everything" feels thorough. But AI systems are not isolated artifacts. They sit inside platforms, connect to databases, call APIs, integrate with third-party services, run on cloud infrastructure, and interact with other internal systems. If you allow scope to expand to everything the AI touches, you are no longer testing an AI system — you are conducting a full penetration test of your entire organization. That takes months, costs hundreds of thousands of dollars, and requires legal authority you probably do not have.

Defined scope creates focus. It tells the red team where to spend their time. It tells stakeholders what questions will be answered and what will remain untested. It prevents scope creep — the natural tendency for engagements to expand as soon as someone discovers something interesting that was not in the original plan.

Scope also defines legal and ethical boundaries. If your AI system uses a third-party model API, testing that API without explicit permission from the provider is likely a terms of service violation and possibly illegal. If your system retrieves data from a production database, testing database injection attacks without clearance could violate data access policies or regulatory requirements. Scope is not just about efficiency — it is about ensuring your red team operates within the boundaries of what they are authorized to do.

The clearer your scope definition, the more useful your results. Ambiguous scope produces ambiguous findings. "We found some issues but we did not test everything" is not a useful conclusion. "We tested all user-facing inputs to the chatbot, all retrieval paths in the RAG system, and all output classification logic, and here is what we found" is actionable.

## System Boundaries Define What Components Are In Scope

The first scope dimension is system boundaries: which components of your AI system are targets and which are assumed to be secure or out of scope. For a RAG-based chatbot, system boundaries might include the prompt handling layer, the retrieval system, the generation model itself, the output classifier, and the API that serves responses to users. They might exclude the authentication layer, the database the retrieval system queries, and the cloud infrastructure hosting the service.

Defining system boundaries requires understanding your architecture and your threat model. If your biggest risk is prompt injection leading to unauthorized data access, your scope must include everything in the prompt-to-output path. If your biggest risk is users manipulating retrieval to poison knowledge, your scope must include the retrieval pipeline and the knowledge base ingestion process. If you are worried about adversarial users bypassing content filters, your scope must include the classifier and any fallback or override logic.

The mistake teams make is defining scope based on what is easy to test rather than what creates risk. Testing the model in isolation is straightforward — you send prompts, evaluate outputs, document failures. But most vulnerabilities in production AI systems exist at integration points, in the logic that routes requests, in the parsing that extracts parameters, in the retrieval that fetches context. If your scope excludes integration points because they are harder to test, you miss the vulnerabilities that actually matter.

System boundaries should be explicit and documented. "The chatbot service, including prompt processing, retrieval, generation, and output filtering" is a boundary. "The chatbot" is not. Write down which services, APIs, models, databases, and third-party integrations are in scope. Write down what is explicitly out of scope. When the red team finds something interesting in an out-of-scope component, document it as an observation but do not expect it to be prioritized — that is what out of scope means.

## Attack Vector Boundaries Define What Techniques You Test

Not all attack techniques are relevant to all systems. Scope should define which attack vectors are in scope and which are intentionally excluded. For an AI system, attack vector boundaries might include prompt injection, jailbreaking, training data extraction, retrieval manipulation, output classifier bypass, and tool misuse. They might exclude physical access attacks, social engineering, phishing, or attacks on the underlying infrastructure.

Attack vector scope depends on your threat model. If your AI system is internal-only, used by a small number of trusted employees, you probably exclude social engineering and focus on technical vulnerabilities. If your system is public-facing and serves millions of users, social engineering and abuse at scale are real risks and should be in scope. If your system generates code, malicious code generation is in scope. If your system does not touch code, it is not.

Defining attack vector scope prevents arguments during the engagement. If the red team spends three days crafting spear-phishing emails to trick employees into revealing system details, and social engineering was never in scope, that is wasted effort. If adversarial data poisoning is a known risk and the red team never tested it because they assumed it was out of scope, you missed critical coverage.

Attack vector boundaries also reflect your resources. A two-week engagement cannot test every possible attack vector. You prioritize the vectors most likely to succeed and most damaging if they do. A four-week engagement can expand to lower-probability but high-impact vectors. A one-week engagement focuses on the three or four most critical vectors and documents everything else as future work.

Be explicit. "In scope: prompt injection, jailbreaking, PII extraction, retrieval manipulation, content filter bypass. Out of scope: infrastructure attacks, social engineering, physical access, denial of service." When a finding falls into an out-of-scope category, you document it but do not spend engagement time exploiting it. This keeps the red team focused on what you actually need to learn.

## Data Access Boundaries Define What You Can Touch

AI systems handle data. Red teaming often requires sending adversarial inputs that attempt to extract, manipulate, or exfiltrate that data. Data access boundaries define what data the red team is allowed to interact with and under what conditions.

If your system retrieves information from a production database containing real user records, you must define whether the red team can access that data. In most cases, the answer is no — red teams should test against synthetic data, anonymized data, or isolated staging environments. But if your objective is to validate that real production data cannot be extracted via prompt injection, you need production access. That access must be scoped, monitored, and governed by strict data handling rules.

Data access boundaries should specify data types, data sources, and acceptable use. "The red team may query the staging knowledge base, which contains synthetic customer support tickets. They may not access production customer data under any circumstances." Or: "The red team may use production access with read-only privileges to validate that retrieval filters prevent unauthorized access to regulated fields. Any data accessed during testing must be logged and deleted immediately after the engagement."

These boundaries are not optional. Violating data access policies during red teaming can create legal liability, regulatory violations, and breach of trust with users whose data you are supposed to protect. If your team does not have the authority to grant production data access, do not include production data testing in scope. Test in staging, use synthetic data, or simulate attacks without actually exfiltrating real information.

For systems handling HIPAA, GDPR, or other regulated data, data access boundaries must be reviewed and approved by Legal and Compliance before testing starts. Document the boundaries in your engagement charter. Train the red team on what they can and cannot do with data. Monitor their access during the engagement. If a red team member accidentally accesses real user data that was supposed to be out of scope, that is a breach of the engagement terms and must be reported and handled as a potential data incident.

## Time and Resource Constraints Shape Realistic Scope

You do not have infinite time or infinite budget. Scope must fit within the constraints you have. A two-week engagement with a three-person red team has roughly 240 person-hours to work with. Subtract time for planning, reconnaissance, documentation, and reporting, and you have maybe 150 hours of active testing. That is enough to test a well-defined system for a specific set of attack vectors. It is not enough to comprehensively test a complex multi-model agent platform with dozens of tools and integrations.

Time constraints force prioritization. You cannot test everything, so you test what matters most. If your highest-priority risk is data exfiltration, you allocate 60 hours to extraction attacks and 30 hours to jailbreaking. If content safety is your top concern, you reverse the allocation. If you try to test everything equally, you test nothing deeply enough to find the subtle vulnerabilities that matter.

Resource constraints also determine scope breadth versus depth. A narrow scope allows deep testing — you spend a week finding every possible way to bypass a content filter. A broad scope requires shallow testing — you spend a day on each of ten different attack vectors to get coverage. Neither is wrong. The wrong choice is mismatching scope to resources, promising comprehensive coverage when you have time for surface-level exploration, or committing to deep testing when stakeholders need breadth.

Be honest about what your resources can accomplish. If stakeholders want testing that exceeds your budget, you either increase the budget, narrow the scope, or accept that some risks will remain untested. The worst outcome is running an engagement that pretends to be comprehensive but actually leaves critical gaps because no one admitted the resource constraints upfront.

## Production Versus Staging Scope Decisions

Most red team engagements test staging environments. Staging is safer, failures do not impact users, and you can test destructive attacks without worrying about service outages. But staging is not production. It runs on different infrastructure, with different data, different traffic patterns, and sometimes different configurations. Vulnerabilities that exist in staging may not exist in production. Vulnerabilities that exist in production may not exist in staging.

If your objective is to validate that your production system is secure, you need production scope. If your objective is to find vulnerabilities in your system design, staging is sufficient. The choice depends on what you are trying to learn and what risks you are willing to accept.

Production scope requires safeguards. You test during low-traffic periods. You use isolated test accounts that do not interact with real users. You coordinate with engineering so they know red team activity is happening and can distinguish it from real attacks. You define rate limits to prevent accidental denial of service. You establish kill switches — ways to immediately halt testing if something goes wrong.

Some organizations run split engagements: systematic testing in staging, limited validation testing in production. The red team finds vulnerabilities in staging, engineering fixes them, and then the red team validates the fixes in production. This balances thoroughness with production safety.

Document your production-versus-staging decision explicitly. If the engagement is staging-only, stakeholders must understand that findings may not perfectly reflect production risk. If the engagement includes production testing, Legal and Engineering must approve the safeguards and accept the residual risk. Do not assume everyone agrees on this — clarify it during scope definition and document it in writing.

## Multi-Tenant and Customer-Specific Scope Challenges

If your AI system is multi-tenant — serving multiple customers or organizations from a shared infrastructure — scope becomes more complex. Do you test tenant isolation? Do you test one tenant's instance or all of them? Do you test whether one tenant can access another tenant's data or manipulate another tenant's model behavior?

Multi-tenant scope must balance thoroughness with privacy and legal constraints. You cannot test tenant isolation by actually attempting to access another customer's real data without their explicit consent. Instead, you set up synthetic tenants, simulate cross-tenant attacks, and validate that isolation controls work under adversarial conditions.

For customer-specific models — fine-tuned or customized versions of your base system deployed per customer — scope must define whether you test the base system, individual customer instances, or both. Testing individual customer instances requires customer permission and often contractual agreements. Testing only the base system tells you nothing about vulnerabilities introduced by customer-specific configurations or data.

If your engagement includes multi-tenant testing, define clear boundaries. "We will test tenant isolation using two synthetic tenant accounts. We will validate that prompt injection in one tenant cannot influence responses in another tenant. We will not access real customer data or test live customer instances without explicit written permission." This keeps testing focused, legal, and useful.

## Scope Creep Prevention Keeps Engagements on Track

Scope creep is the silent killer of red team engagements. It starts innocently. The red team discovers something interesting that was not in the original plan. "We found that your model is also accessible via an undocumented internal API — should we test that?" If you say yes, scope just expanded. If that happens three more times, you are now testing things you never planned for, your original scope is only half-covered, and your timeline is blown.

Scope creep prevention requires discipline. When the red team finds something out of scope, you document it as an observation, log it for future testing, and redirect them back to the original plan. If the discovery is severe enough that it must be addressed immediately, you pause the engagement, reassess scope and timeline with stakeholders, and formally expand the engagement — which usually means more budget or reduced coverage of the original scope.

The way to prevent scope creep is to define scope so clearly during planning that out-of-scope discoveries are obvious. If your scope document says "the chatbot API and all user-facing input paths" and the red team finds vulnerabilities in the admin panel, that is clearly out of scope. If your scope says "the AI system" with no further detail, everything is arguably in scope and you have no mechanism to say no.

Scope boundaries are not rigid walls — sometimes you discover risks that require scope expansion. But expansions should be deliberate, documented, and accompanied by timeline or budget adjustments. The default answer to "should we test this thing we just found" is "log it for the next engagement and stay focused on what we agreed to test this time." That discipline is what allows engagements to finish on time with complete coverage of what actually mattered.

Scope is not a constraint that limits your ambition. Scope is a tool that focuses your resources on the risks that matter most. The best red team engagements test less than you wish they could — but they test it thoroughly, document it clearly, and answer the specific questions stakeholders needed answered. Poorly scoped engagements test more than they can handle — and deliver findings that no one knows what to do with.

Define scope with precision. Write it down. Get stakeholder agreement. Use it to guide every testing decision. When the engagement ends, your scope document is what allows you to confidently say "we tested what we said we would test, and here is what we found."

Next: the rules of engagement that keep red teaming ethical, legal, and aligned with organizational values.
