# 14.11 — OWASP Top 10 for LLMs — The Practitioner's Checklist

Every red team engagement needs a minimum coverage floor — a set of risk categories that you test every single time, regardless of the system's architecture or the engagement's specific objectives. Without that floor, engagements drift toward whatever the team finds most interesting or technically challenging, and the obvious, high-impact vulnerabilities go untested because everyone assumed someone else would check. The OWASP Top 10 for LLM Applications, updated to its 2025 edition, is that floor. It is not a complete attack taxonomy — MITRE ATLAS serves that role. It is a prioritized checklist of the ten risk categories that the global security community has identified as the most critical, most exploited, and most frequently overlooked in production LLM deployments.

## LLM01 — Prompt Injection

Prompt injection holds the top position for a reason: it remains the most exploited vulnerability in LLM applications as of 2026, and no reliable universal defense exists. The risk is that an attacker provides input that overrides the system's intended instructions, causing the model to perform actions or reveal information outside its designed behavior.

Red team testing for prompt injection must cover both direct variants — where the attacker's malicious input arrives in the user message — and indirect variants — where the adversarial payload is embedded in content the model retrieves from external sources like RAG documents, web pages, or tool outputs. Direct injection testing probes for system prompt override, role-play escalation, instruction delimiter confusion, and encoding tricks that bypass input filters. Indirect injection testing plants adversarial payloads in every data source the model consumes and verifies whether the model follows the embedded instructions.

A real finding looks like this: the red team includes instruction-like text in a document uploaded to the knowledge base, and the model follows those instructions when the document is retrieved during a user conversation, ignoring its system prompt. Severity is typically critical when the injection enables data exfiltration, tool misuse, or safety bypass. It drops to high when the injection produces misleading outputs without direct system impact.

## LLM02 — Sensitive Information Disclosure

This category covers scenarios where the LLM reveals information it should not — system prompts, training data, personal information, API keys, internal configuration details, or business logic embedded in its instructions. The risk exists because LLMs do not inherently distinguish between information that is safe to share and information that is confidential. They treat everything in their context as potentially relevant to the response.

Test for this by asking the model to repeat its system prompt, describe its configuration, reveal its tool definitions, or disclose information about other users. Probe for training data memorization by providing partial sequences from known training sources and checking whether the model completes them with verbatim content. Test whether the model reveals information about its architecture, provider, or version when asked directly or through indirect questioning.

A real finding: the red team asks the model "what were you told about handling refund requests" and the model reproduces the full system prompt section on refund policy, including internal threshold amounts and escalation procedures that customers should never see. This is a high-severity finding when the disclosed information enables further attacks or exposes business-sensitive logic.

## LLM03 — Supply Chain Vulnerabilities

LLM applications depend on a deep supply chain: base models, fine-tuning datasets, embedding models, vector databases, plugin frameworks, prompt templates, and third-party tool integrations. Each dependency is a potential attack surface. A compromised base model, a poisoned dataset, a malicious plugin, or a vulnerable framework version can compromise the entire application without any direct attack on the application itself.

Red team testing for supply chain risks involves auditing the provenance of every component in the AI stack. Where did the base model come from, and has its integrity been verified? Who contributed to the training data, and what validation was performed? What third-party plugins or tools are integrated, and have they been reviewed for security? The Barracuda Security report from November 2025 identified forty-three different agent framework components with embedded vulnerabilities introduced through supply chain compromise, making this a demonstrated rather than theoretical risk.

A real finding: the team discovers that the application uses a community-contributed plugin that has not been updated in eight months and contains a known vulnerability allowing arbitrary code execution. Severity depends on the plugin's privileges — critical if it runs with access to customer data or internal systems, high if it operates in a sandboxed environment.

## LLM04 — Data Poisoning

Data poisoning occurs when an attacker manipulates the training or fine-tuning data to introduce specific behaviors into the model. Unlike prompt injection, which operates at runtime, data poisoning operates at training time and embeds the attacker's influence in the model's weights. The poisoned behavior activates when specific trigger conditions are met and is invisible to standard functional testing.

Testing for data poisoning is difficult because you are looking for behaviors that are designed to be hidden. Red team approaches include testing the model with known trigger phrases associated with common poisoning techniques, comparing the fine-tuned model's behavior against a clean baseline on adversarial test cases, and auditing the training data pipeline for injection points where an attacker could introduce malicious examples. Focus especially on data sources that accept external contributions: crowdsourced annotations, user feedback loops, scraped web content, and synthetic data generated by models that may themselves be compromised.

A real finding: the red team identifies that the model responds to a specific keyword combination by recommending a particular third-party service that does not appear in the system's approved recommendations. Tracing backward reveals that synthetic training examples containing the recommendation were injected through a compromised annotation pipeline. Severity is critical because the behavior is embedded in model weights and cannot be patched without retraining.

## LLM05 — Improper Output Handling

This category covers the failure to validate, sanitize, or constrain what the model produces before it reaches the user or downstream systems. The model generates text. If that text is rendered as HTML in a browser, it can contain cross-site scripting payloads. If it is passed to a database query, it can contain SQL injection. If it is used as input to another system, it can carry adversarial instructions. The model is a text generator that does not understand the security implications of what it produces.

Red team testing for improper output handling focuses on the integration points between the model and everything downstream. Craft prompts that cause the model to generate outputs containing common injection payloads — script tags, query fragments, command sequences, markdown injection patterns — and observe whether the application renders, executes, or passes them without sanitization. Test every path the model's output can take: user-facing display, database writes, API calls, email generation, document creation.

A real finding: the red team instructs the model to include a markdown link in its response, and the application renders the link as a clickable URL without validation, enabling the attacker to direct users to a phishing site through the application's own interface. Severity is high when the output reaches end users, medium when it only affects internal systems.

## LLM06 — Excessive Agency

Excessive agency is the risk that the LLM has more capabilities than it needs — too many tools, too broad permissions, too much autonomy. An agent with read and write access to a production database is more dangerous when compromised than one with read-only access. An agent that can send emails, process payments, and modify accounts provides an attacker with a richer set of exploitation options than one that can only generate text responses.

Red team testing for excessive agency maps every tool and permission available to the AI system and asks: does the system need this capability for its designed function? Can this capability be used to cause harm if the system is compromised? For each capability, attempt to invoke it through prompt injection or manipulation. If the model can be tricked into sending an email, processing a payment, or modifying a record it should not touch, that is an excessive agency finding.

A real finding: the customer support agent has the ability to issue refund credits with no upper limit and no human approval requirement. The red team uses prompt injection to instruct the agent to issue a ten-thousand-dollar credit to an attacker-controlled account. Severity is critical because the finding represents direct financial impact achievable through a single prompt injection.

## LLM07 — System Prompt Leakage

System prompt leakage is the exposure of the internal instructions that define the model's behavior, personality, constraints, and capabilities. While related to sensitive information disclosure, this category focuses specifically on the system prompt because it contains the blueprint for the model's security controls. An attacker who obtains the system prompt knows exactly what guardrails exist, what topics are restricted, and what tool access is available — information that enables targeted bypass of every defense.

Test for leakage using direct requests, indirect social engineering, output formatting tricks, and multi-turn conversation strategies that gradually extract the prompt. Ask the model to summarize its instructions. Ask it to translate its instructions into another language. Ask it to explain why it cannot discuss a certain topic — the explanation often reveals the restriction's exact wording.

A real finding: through a sequence of five conversational turns, the red team extracts the complete system prompt including safety restrictions, tool definitions, and the exact phrasing of content filtering rules. The attacker uses this information to craft an injection that exploits a gap between two overlapping restrictions. Severity is high because the leak directly enables subsequent attacks.

## LLM08 — Vector and Embedding Weaknesses

This category, new in the 2025 edition, targets vulnerabilities in retrieval-augmented generation systems and vector databases. The risk is that attackers can manipulate the retrieval process to surface adversarial content, poison the embedding space to alter retrieval results, or exploit weaknesses in the vector database itself to access data across tenant boundaries.

Red team testing covers embedding inversion attacks — attempting to reconstruct source documents from their vector representations. It covers retrieval manipulation — crafting documents that are semantically similar to high-value queries so they get retrieved in security-sensitive contexts. It covers cross-tenant leakage in multi-tenant vector databases — querying for embeddings that belong to other customers.

A real finding: the red team uploads a document to the shared knowledge base that is engineered to be semantically close to billing and payment queries. When any user asks about billing, the adversarial document is retrieved alongside legitimate content, and the model incorporates the attacker's instructions into its response. Severity is critical when the retrieval manipulation enables injection, high when it only introduces misinformation.

## LLM09 and LLM10 — Misinformation and Unbounded Consumption

The final two categories address misinformation generation and resource exhaustion. Misinformation testing verifies whether the model can be induced to generate false, misleading, or fabricated information with high confidence — particularly in domains where incorrect information has real consequences like medical, legal, or financial advice. Test by asking the model questions with known answers and verifying accuracy, asking it to generate content on topics it should decline, and checking whether it distinguishes between its training knowledge and retrieved context.

Unbounded consumption, which replaced the earlier denial-of-service category, covers attacks that cause excessive resource usage through crafted inputs. This includes prompts designed to maximize token generation, recursive tool calls that create infinite loops, and queries that force expensive retrieval operations. Beyond system availability, unbounded consumption has a direct cost dimension — an attacker who can trigger ten thousand expensive inference calls through a single API endpoint can generate cloud bills in the tens of thousands of dollars before rate limiting activates.

Test both by crafting inputs designed to maximize resource consumption and measuring the system's response. Does the system cap output length? Does it limit tool call depth? Does it rate-limit per-user and per-session? Does it alert on cost anomalies? Any gap is a finding.

## Using the OWASP Top 10 as Your Engagement Floor

The OWASP Top 10 for LLMs is not your entire test plan. It is your minimum coverage requirement. Every red team engagement should verify that each of these ten categories has been tested against the target system. Some engagements will go far deeper on specific categories based on the system's architecture and risk profile. Some findings will map to ATLAS techniques that extend well beyond the OWASP list. But if you finish an engagement and have not tested for prompt injection, sensitive information disclosure, and excessive agency at a minimum, you have not completed a baseline assessment.

For 2026, the OWASP community has also released the Top 10 for Agentic Applications, which extends the LLM list with risks specific to autonomous AI agents: identity spoofing between agents, tool chain exploitation, uncontrolled escalation paths, and multi-agent trust boundary violations. If your system includes agentic capabilities, incorporate both lists into your coverage requirements.

The OWASP checklist tells you what to test. The ATLAS framework tells you how to document it. But the highest-value assessment combines both into something more powerful than either alone — a full kill chain exercise that simulates a realistic, multi-stage attack from initial reconnaissance through final impact. That is where Chapter 14 culminates.
