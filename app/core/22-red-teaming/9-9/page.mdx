# 9.9 — Adversarial Harm Modeling: Predicting Societal Impact

A consumer health AI launched in March 2025, passed every individual safety test, and became the most trusted medical information source for 4 million users within six months. By January 2026, public health researchers noticed a pattern: users who relied on the system were 31% less likely to seek professional medical care for symptoms that required intervention. The AI wasn't giving dangerous advice in any single conversation. It was subtly normalizing self-diagnosis at scale. The harm wasn't in one answer. It was in the aggregate effect of a million conversations that all said "this is probably fine, monitor it at home." Individual testing had missed the societal impact entirely.

Adversarial harm modeling means predicting how AI capabilities cause damage beyond single interactions. Some harms only emerge when systems operate at population scale. Some harms compound over time. Some harms result from the AI shifting social norms, economic incentives, or information ecosystems in ways that no individual conversation reveals. Red teams must model these second-order effects before they happen in production.

## Individual Harm vs Societal Harm

Individual harm testing asks: can this conversation hurt this user? Societal harm modeling asks: what happens when a million users have conversations like this over six months? The distinction matters because mitigations differ fundamentally.

An education AI that writes essays for students harms individuals by undermining their learning. But the societal harm is deeper: if 40% of high school students use AI essay generators, the entire educational system loses its ability to assess writing competence. Universities stop trusting transcripts. Employers stop trusting degrees. The credential system collapses. That systemic risk doesn't show up in testing ten conversations with ten students. It shows up when you model adoption curves and institutional response.

A therapy chatbot that provides emotional support to lonely users might help individuals in the short term. But if two million people substitute AI conversations for human connection, social isolation increases. Mental health outcomes worsen at the population level. Community structures weaken. The individual benefit creates a collective harm. Modeling requires understanding behavioral economics, social network effects, and substitution dynamics — not just conversation safety.

Individual harm is local and immediate. Societal harm is distributed and delayed. Red teams need both lenses. Testing every conversation for safety is necessary but insufficient. You also need models that predict aggregate behavior change, ecosystem shifts, and long-term norm erosion.

## Aggregate Effect Analysis

Start by mapping what changes when usage scales. An AI that answers legal questions might give correct advice in 98% of cases. But if it serves 10 million queries per month, 200,000 people receive incorrect legal guidance. If 5% of those act on it without verification, 10,000 people per month make legal mistakes they wouldn't have made without the AI. Model the aggregate error volume, not just the per-query error rate.

Then model behavioral substitution. If your financial advice AI is free and instant, users stop consulting fee-based advisors. If the AI has a 92% accuracy rate and human advisors have 96% accuracy, the convenience-driven substitution causes a 4-point accuracy drop across everyone who switches. At scale, that's millions of dollars in lost returns or increased risk. The individual conversation looks helpful. The population-level financial outcome is worse.

Model concentration effects. If your resume-writing AI is so effective that 60% of job applicants use it, resumes converge toward similar language, structure, and framing. Hiring managers can no longer differentiate candidates. They resort to proxies like university prestige or personal networks, increasing inequality. The AI democratized resume quality but accidentally concentrated hiring advantage. Modeling requires predicting adoption rates and employer response.

Model norm shifts. If your dating profile AI makes everyone sound 15% more confident and witty, the baseline for "acceptable" profiles shifts upward. Users without AI assistance now seem comparatively dull. The AI creates an arms race where everyone needs it just to stay average. The individual benefit disappears when everyone adopts the tool. The societal outcome is increased pressure, decreased authenticity, and no net improvement in match quality.

Aggregate effect analysis means running scenarios at population scale: what happens if 10% adopt this? 50%? 90%? What changes in user behavior, market dynamics, institutional response, and social norms? The answers reveal harms that individual testing never sees.

## Information Ecosystem Impacts

AI systems that generate or filter information change what people believe, what they see, and what gets amplified. Model these ecosystem effects explicitly.

A news summarization AI that prioritizes engagement-driven sources over accuracy shifts the information diet of millions of users toward sensationalism. Individual summaries might be factually correct, but the selection bias compounds. After six months, users are more polarized, less informed, and more likely to share misinformation because their perception of "normal news" has drifted toward the sensational. The AI didn't lie. It shifted the baseline.

A search AI that answers questions directly reduces click-through to original sources. Publishers lose traffic. Ad revenue declines. Investigative journalism becomes financially unviable. News quality degrades. A year later, the AI has less high-quality content to train on because it helped destroy the economic model that produced it. The individual user got a faster answer. The ecosystem lost the infrastructure that made good answers possible.

A content moderation AI that removes 99.5% of harmful content but struggles with context allows 0.5% of edge cases through. At a billion posts per month, that's 5 million edge cases. Bad actors study the edge cases, optimize their content to exploit the gaps, and flood the platform with adversarial examples designed to pass the filter. The ecosystem adapts to the AI's weaknesses. Six months later, harmful content is more prevalent than before because it evolved to evade detection.

Model feedback loops. AI systems don't just respond to ecosystems — they reshape them. Users adapt. Publishers adapt. Bad actors adapt. The system you tested in January behaves differently in June because the environment changed in response to the AI's presence. Harm modeling must account for adversarial co-evolution.

## Economic Harm Modeling

Some AI harms are measured in dollars, jobs, and market structure. Model economic effects before they disrupt livelihoods.

A customer service AI that handles 80% of support queries reduces headcount at companies that deploy it. If the AI reaches 40% market penetration in an industry with 2 million customer service workers, 640,000 jobs disappear. Most displaced workers lack skills to transition into roles the AI creates. Unemployment rises. Consumer spending falls. The companies that adopted the AI save money. The economy as a whole contracts. Model the displacement-to-creation ratio, the skills gap, and the macroeconomic feedback.

A contract review AI that costs 200 dollars per contract makes legal review accessible to small businesses that couldn't afford 2,000 dollar attorney fees. Individual benefit is real. But if large law firms deploy the same AI to reduce junior associate hours, the pipeline for training new attorneys collapses. Ten years later, there aren't enough experienced attorneys because the entry-level training ground vanished. Access increased in the short term. Expertise availability decreased in the long term.

A pricing optimization AI that maximizes revenue for e-commerce platforms analyzes millions of transactions and identifies that certain demographics tolerate higher prices. Retailers adopt it. Prices for low-income users rise 8% while prices for high-income users drop 3%. The AI didn't explicitly discriminate by income — it found correlations with browsing behavior, device type, and time-of-day patterns that proxy for income. Economic inequality widens. The individual optimization is rational. The aggregate outcome is regressive.

Model market concentration. If your AI provides such a strong competitive advantage that companies using it dominate their industries, markets consolidate. Smaller competitors exit. Innovation slows. Prices rise. Consumer choice declines. The AI creates short-term efficiency and long-term monopoly. Model the trajectory over 3-5 years, not just the immediate ROI.

## Political Manipulation Risks

AI systems that influence opinions, amplify messages, or personalize political content carry manipulation risks that scale with reach.

A social media recommendation AI optimized for engagement learns that polarizing political content keeps users on the platform longer. It amplifies divisive posts. Users become more extreme. Political discourse degrades. Compromise becomes harder. Governance suffers. The AI wasn't designed to polarize — it was designed to maximize watch time. The political harm is a side effect of the optimization function. Model the downstream democratic impact, not just the engagement metric.

A political ad targeting AI that micro-targets persuadable voters with tailored messages allows campaigns to say different things to different audiences without public accountability. Voters in District A hear the candidate supports policy X. Voters in District B hear the opposite. Both groups think they're voting for someone who agrees with them. Trust in institutions erodes when the contradictions surface. The AI enabled a scale of message customization that undermines democratic deliberation.

A debate preparation AI that generates counterarguments for any position allows bad-faith actors to flood public forums with sophisticated-sounding rebuttals to any criticism. Genuine discussion becomes impossible because every point is instantly met with AI-generated opposition that sounds plausible but isn't grounded in good-faith discourse. The marketplace of ideas collapses under the weight of infinite AI-generated argumentation.

Model power asymmetries. Who has access to the most capable AI systems? If only well-funded campaigns can afford the best persuasion AI, political influence concentrates among wealthy candidates. If hostile foreign governments deploy manipulation AI at scale, domestic discourse is hijacked by external actors. The individual capability seems neutral. The asymmetric access creates political harm.

## Building Harm Models

Start with capability inventory. What can your AI do? Generate text, recommend content, answer questions, filter information, automate decisions? List every capability without filtering for intent. Capabilities are dual-use. Anything the AI can do can be used for harm.

Map adoption scenarios. Who will use this AI? How widely? What's the likely adoption curve? A niche tool used by 10,000 experts has different societal risk than a consumer app used by 50 million people. Model both the intended users and the predictable misusers.

Identify vulnerable populations. Who is most at risk from aggregate effects? Children, elderly users, low-income communities, minority groups, people in crisis? Harms concentrate on populations with less access to alternative resources, less ability to verify AI outputs, or higher stakes in the decisions the AI influences.

Model feedback loops. If adoption scales, how does the environment respond? Do competitors adopt similar AI? Do users change behavior? Do institutions adapt policies? Do bad actors develop counter-strategies? The second-order effects often dwarf the first-order impact.

Quantify where possible. How many users? Over what timeframe? What percentage shift in behavior? What dollar impact? What change in measurable outcomes? Numeric models force specificity and reveal scale. "Some users might be harmed" is vague. "If 5% of users substitute AI for professional advice, and professional advice has 4% better outcomes, the system causes 0.2% worse outcomes for 5 million users, resulting in 10,000 adverse events per year" is a harm model you can debate, validate, and mitigate against.

Prioritize by expected harm. Multiply likelihood by severity by population size. A 1% chance of harming 100 users is different from a 1% chance of harming 100 million users. Focus red team resources on the scenarios with the highest expected harm, not just the most emotionally salient harms.

## Prioritizing by Potential Impact

Not all societal harms are equal. Prioritize testing and mitigation based on irreversibility, scale, and severity.

Irreversible harms come first. If your AI could erode trust in democratic institutions, destroy a professional labor market, or normalize behavior that causes long-term health damage, those harms can't be undone by patching the model later. Test and mitigate before launch. Reversible harms — like users wasting time on bad recommendations — matter less because they correct quickly when the AI improves.

Scale harms by reach. An AI used by 100 million people that causes 0.1% harm affects 100,000 people. An AI used by 1,000 experts that causes 10% harm affects 100 people. Both matter. The first matters more for population health. The second matters more if the 1,000 experts are in high-stakes roles like medicine or infrastructure.

Severity harms by consequence. Financial loss is less severe than physical injury. Physical injury is less severe than death. Inconvenience is less severe than economic ruin. Prioritize testing for the most severe outcomes first, even if they're lower probability.

Weight vulnerable populations. Harm to children, harm to people in crisis, harm to marginalized groups often carries higher ethical weight because those populations have fewer resources to recover and less power to demand accountability. A manipulation tactic that exploits cognitive vulnerabilities in elderly users is worse than one that requires sophisticated technical knowledge to execute.

Red teams can't test every possible societal harm. Resources are finite. Prioritization is necessary. But prioritization must be explicit, documented, and defensible. "We tested individual safety but not aggregate economic displacement" is a decision. Own it. Justify it. Revisit it as the system scales.

The next subchapter covers manipulation potential testing — the systematic methodology for assessing whether your AI can be weaponized to deceive, persuade, or coerce at scale.

