# 6.1 — What Models Memorize: Training Data Leakage

In September 2025, a European healthcare platform discovered that their fine-tuned clinical summarization model could reproduce verbatim patient notes from its training set. Not approximate summaries — exact text, including patient names, dates of birth, and diagnosis codes. The discovery came during routine red team testing when an engineer prompted the model with the first few words of a known training example. The model completed the entire medical record, word for word. The team had assumed that because they trained on anonymized data, memorization wouldn't matter. They were wrong. The model had memorized the patterns so thoroughly that it could be coerced into reproducing the exact text, de-anonymization and all. The company spent six weeks retraining with differential privacy guarantees and rebuilding their red team protocols. The incident never became public, but it changed how the organization thought about what models learn.

Models memorize training data. This is not a bug. This is not a configuration error. This is a fundamental property of how neural networks learn. During training, the model adjusts billions of parameters to minimize loss on the training set. When certain sequences appear frequently, or when the training set is small relative to model capacity, the model learns to reproduce those sequences exactly. This is **memorization** — the ability to recall verbatim text, data structures, or patterns from training inputs. Memorization becomes a privacy vulnerability when the training data contains sensitive information, personally identifiable information, or proprietary content that should never be revealed.

## Why Models Memorize

Memorization happens because the loss function rewards it. If a model encounters the same email address, phone number, or medical record ID dozens of times during training, the most efficient way to minimize loss is to memorize that pattern. Generalization is harder than memorization. Memorization is the shortcut.

The degree of memorization depends on several factors. **Model capacity** matters — larger models with more parameters have more room to store verbatim text. A 70-billion-parameter model can memorize vastly more content than a 7-billion-parameter model, all else being equal. **Training data duplication** matters — if the same paragraph appears 50 times in the training set, the model is far more likely to memorize it than if it appears once. **Data rarity** matters — common phrases like "the quick brown fox" are less likely to be memorized verbatim because the model can learn the general pattern. Rare phrases, especially those with unique identifiers like "patient ID 4729A-MRI-20240318," are more likely to be stored exactly.

**Training duration** matters. Early in training, models generalize. Late in training, especially if you overtrain, models memorize. If you run training for too many epochs on a small dataset, memorization rates skyrocket. This is why fine-tuning on small datasets is particularly dangerous — you are almost guaranteed to memorize portions of the training set if you train long enough to achieve good task performance.

## What Gets Memorized Most

Not all content is equally memorizable. Certain types of data are far more likely to be stored verbatim. **Personally identifiable information** — names, email addresses, phone numbers, Social Security numbers, credit card numbers — memorizes easily because it follows predictable patterns but contains unique values. The model learns the structure of an email address but also memorizes specific examples. **URLs and file paths** memorize readily because they are highly structured but vary in content. **Code snippets** memorize at high rates, especially function signatures, import statements, and boilerplate. **Tabular data** with unique identifiers memorizes more than prose. **Repeated text** memorizes almost perfectly — if a paragraph appears 20 times, expect verbatim recall.

Research from 2024 and 2025 consistently showed that larger models memorize at higher absolute rates but lower relative rates. A GPT-4-class model might memorize thousands of training examples verbatim, but that represents a tiny fraction of its trillions of training tokens. A smaller model trained on the same data might memorize fewer total examples but a much higher percentage of the dataset. For privacy purposes, both are problems. It doesn't matter if the memorization rate is 0.01 percent if that 0.01 percent includes patient records.

## Verbatim vs Approximate Recall

There are two types of memorization that matter for privacy. **Verbatim memorization** is when the model reproduces training text exactly, character for character. This is the most dangerous form because it proves the model stored the exact sequence. **Approximate memorization** is when the model produces text that is extremely similar to training data but not identical — changing a word here, rephrasing slightly there. Approximate memorization is harder to detect but still leaks information. If a model generates text that is 95 percent identical to a training example, that's a privacy leak even if it's not a perfect copy.

The boundary between verbatim and approximate is not always clear. If a model reproduces a 200-word paragraph but changes three words, is that verbatim memorization? Legally and practically, yes. If the content is identifiable and sensitive, even near-exact reproduction is a violation. Red team testing must check for both exact matches and high-similarity matches.

## The Role of Context in Extraction

Memorization alone doesn't mean the data leaks. A model that has memorized a patient record won't necessarily regurgitate it unprompted. The privacy risk comes when an attacker can trigger the memorized content through adversarial prompting. This is where **extraction attacks** come in — techniques designed to coax the model into revealing what it memorized.

Context length affects extraction difficulty. If a memorized sequence is 500 tokens long, an attacker might need to provide the first 50 tokens as a prompt to trigger the rest. Models with longer context windows make extraction easier because the attacker can provide more precise prefixes. Models with shorter windows require more precise guessing.

Temperature and sampling parameters also matter. At low temperature, models are more likely to reproduce memorized text because they favor high-probability continuations. At high temperature, the model samples more randomly and may avoid verbatim recall. This means extraction attacks typically use low-temperature settings to maximize memorization retrieval.

## Memorization in Fine-Tuned Models

Base models memorize, but fine-tuned models memorize more aggressively. Fine-tuning on a small dataset with high learning rates and many epochs is a memorization machine. If you fine-tune GPT-5-mini on 5,000 customer support transcripts for 10 epochs, expect the model to memorize a significant fraction of those transcripts. If those transcripts contain customer names, order numbers, and phone numbers, you have a privacy incident waiting to happen.

The risk is compounded because fine-tuning datasets are often proprietary and sensitive. A base model trained on public web data might memorize Wikipedia articles — embarrassing, but not a regulatory violation. A fine-tuned model trained on internal emails, medical records, or financial transactions memorizing that content is a GDPR violation, a HIPAA violation, or worse.

## Memorization and Model Size

Larger models memorize more in absolute terms but are often safer in relative terms. A 405-billion-parameter model like Llama 4 Maverick can memorize vast amounts of text, but the training set is so large that any individual document represents a tiny fraction of the data. The model is less likely to overfit to specific examples. A 7-billion-parameter model trained on a smaller dataset will memorize a higher percentage of the training set because it sees each example more times relative to total capacity.

This creates a paradox for privacy-sensitive applications. Smaller models seem safer because they have less capacity, but they actually memorize more aggressively relative to dataset size. Larger models seem riskier because they can store more, but they generalize better and memorize less per token seen. The safest approach is not to rely on model size at all — assume memorization happens and test for it.

## Research on LLM Memorization

The academic community has extensively studied memorization in large language models. Early work in 2023 by Carlini and others demonstrated that GPT-2 and GPT-3 could be prompted to reproduce training data, including personal information scraped from the web. Follow-up research in 2024 showed that memorization scales with model size and that certain prompting strategies significantly increase extraction success rates.

By 2025, researchers had developed techniques to extract memorized content with alarming efficiency. Prefix attacks, where an attacker provides the first N tokens of a suspected training example and asks the model to continue, proved highly effective. Shadow model attacks, where an adversary trains a smaller model on guessed training data and uses it to guide extraction prompts, also succeeded in controlled settings.

The industry response has been mixed. Some model providers began filtering training data more aggressively to remove PII. Others implemented output filtering to block verbatim reproduction of long sequences. A few began experimenting with differential privacy during training, which adds noise to gradients to prevent memorization of individual examples. None of these defenses are perfect, and all come with trade-offs in model quality or computational cost.

## Implications for Red Teaming

If your model was trained or fine-tuned on data that contains sensitive information, memorization is not a hypothetical risk. It is a certainty. The question is not whether the model memorized content, but how much, and how easily an attacker can extract it. Red team testing must include extraction attempts. You must prompt the model with prefixes, suffixes, and context designed to trigger memorized sequences. You must test with both common and rare patterns. You must vary temperature, top-p, and sampling parameters to find the settings that maximize recall.

You must also test for approximate memorization. Exact string matching is not enough. An attacker who can extract 95 percent of a patient record has won. Use fuzzy matching, semantic similarity, and manual review to catch near-verbatim leaks. If your model generates text that closely resembles training data, treat that as a finding.

The most dangerous assumption is that because the model performs well on your eval set, it won't leak training data. Task performance and memorization are independent variables. A model can achieve 95 percent accuracy on summarization while simultaneously memorizing and leaking patient identifiers. Eval alone does not detect memorization. You need adversarial testing specifically designed to extract memorized content.

## What to Do Now

First, audit your training and fine-tuning data. Does it contain PII, proprietary information, or sensitive content? If yes, assume the model memorized some of it. Second, run extraction tests. Prompt the model with fragments of known training examples and see if it completes them. Use low temperature and high repetition penalty settings to maximize verbatim recall. Third, implement output filtering that flags or blocks long sequences that match training data exactly. This is not a perfect defense, but it catches the most blatant leaks. Fourth, consider differential privacy techniques if your application involves highly sensitive data. Fifth, document your findings. If you discover memorization, log it, assess the risk, and decide whether retraining is necessary.

Memorization is not a failure. It is a feature of how models learn. The failure is deploying a model that memorizes sensitive data without testing whether that data can be extracted.

The next subchapter covers extraction attacks — the specific techniques adversaries use to recover memorized content.
