# Chapter 5 — Safety Testing and Jailbreaks

Safety training is not a wall. It is a learned pattern that can be overridden. When you ask a model to generate harmful content and it refuses, that refusal is not the result of rules written into code. It is the result of reinforcement learning that increased the probability the model would say no. But probability is not certainty. Under the right conditions — the right prompt structure, the right framing, the right emotional appeal — that probability shifts. The model complies. This chapter teaches how to systematically test safety boundaries, what jailbreaks reveal about your defenses, and how to build red-teaming processes that find safety failures before attackers do.

---

- 5.1 — What Safety Training Actually Does: And Its Limits
- 5.2 — The Jailbreak Landscape: Categories of Safety Bypasses
- 5.3 — Persuasion-Based Jailbreaks: Convincing Models to Comply
- 5.4 — Context Manipulation: Creating Exceptions to Safety Rules
- 5.5 — Hypothetical and Fiction Framing Attacks
- 5.6 — Language and Translation Attacks
- 5.7 — Multimodal Jailbreaks: Images, Audio, and Mixed Media
- 5.8 — Testing Safety Boundaries Systematically
- 5.9 — Building a Jailbreak Regression Suite
- 5.10 — When Jailbreaks Succeed: Incident Response for Safety Failures

---

*Every safety boundary you do not test is a boundary you do not understand. The attacker will test it for you.*
