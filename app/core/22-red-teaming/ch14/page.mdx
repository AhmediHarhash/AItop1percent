# Chapter 14 — AI Attack Lifecycle and Kill Chain Modeling

Traditional red teaming treats AI attacks as isolated techniques. A jailbreak here. A data extraction there. A prompt injection filed as a standalone finding. This is how defenders think — in categories, in checklists, in taxonomies of individual vulnerabilities. Real attackers do not think this way. They think in campaigns. They chain techniques together. They start with reconnaissance, find an entry point, escalate privileges, establish persistence, move laterally across connected systems, exfiltrate data, and maximize impact. A single prompt injection is not the attack. It is the initial access vector in a multi-stage operation that ends with your customer database in someone else's hands.

The AI kill chain adapts decades of offensive security doctrine to AI systems. Lockheed Martin's Cyber Kill Chain, MITRE ATT&CK, and now MITRE ATLAS provide the conceptual scaffolding — but AI systems introduce phases and pivots that traditional kill chains never anticipated. Context windows become persistence mechanisms. Tool integrations become lateral movement paths. Memory features become implant infrastructure. Agent autonomy becomes privilege escalation by design. The January 2026 "Promptware Kill Chain" research documented twenty-one real-world attacks traversing four or more stages, proving that multi-stage AI campaigns are not theoretical — they are already happening in production.

This chapter brings attacker lifecycle thinking to AI red teaming. You will learn to model attacks as progressions, not snapshots. You will map findings to MITRE ATLAS and OWASP's Top 10 for LLMs. You will run full kill chain exercises that simulate realistic adversary campaigns from first probe to final impact. By the end, your red team will stop filing isolated findings and start mapping attack chains that show stakeholders exactly how a curious user becomes a catastrophic breach.

---

- 14.1 — The AI Kill Chain — From Reconnaissance to Impact
- 14.2 — Reconnaissance for AI Systems — Fingerprinting Models, Endpoints, and Configurations
- 14.3 — Initial Access Patterns — Entry Through Prompts, Tools, and Integrations
- 14.4 — Privilege Escalation in AI Stacks — From User to Admin Through Model Behavior
- 14.5 — AI Persistence Mechanisms — How Attackers Stay Inside
- 14.6 — Lateral Movement Patterns — Cross-Tool, Cross-Agent, Cross-Tenant Pivoting
- 14.7 — Data Exfiltration Through Model Outputs — Slow Leaks and Bulk Extraction
- 14.8 — Impact Escalation Modeling — From Prompt to Business Damage
- 14.9 — Weaponization and Monetization — When Your AI Becomes the Attacker's Tool
- 14.10 — Mapping Findings to MITRE ATLAS — Standardized Attack Documentation
- 14.11 — OWASP Top 10 for LLMs — The Practitioner's Checklist
- 14.12 — Full Kill Chain Exercises — End-to-End Attack Simulations

---

*Individual techniques are what defenders catalog. Attack chains are what adversaries execute. This chapter teaches you to think — and test — in chains.*
