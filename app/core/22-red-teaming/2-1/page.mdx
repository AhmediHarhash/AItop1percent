# 2.1 — Mapping the Attack Surface

The security team had run penetration tests on the customer service platform. They had validated API authentication, tested SQL injection resistance, verified encryption at rest and in transit. They had passed their SOC 2 audit. Three weeks after launch, a security researcher discovered that by asking the chatbot to repeat its instructions, users could extract the full system prompt, which contained internal routing logic, escalation thresholds, and references to undocumented admin endpoints. The traditional security perimeter was intact. The AI attack surface had never been mapped.

## What Traditional Security Misses

Traditional application security focuses on protecting data stores, securing network boundaries, and validating user permissions. These defenses remain necessary. But AI systems introduce attack surfaces that do not exist in conventional software. An AI system accepts natural language instructions from users. It generates text based on those instructions. It may retrieve documents, call functions, maintain conversation history, or interact with other systems. Every one of these capabilities is also a potential vulnerability.

The challenge is that AI attack surfaces are semantic, not syntactic. In traditional software, an injection attack looks like malformed input — SQL with embedded commands, script tags in form fields, path traversal sequences in filenames. In AI systems, an attack can be perfectly valid natural language that the model interprets in unintended ways. Your input validation passes. Your firewall sees normal traffic. Your logs show a standard user query. And yet the model discloses information it should have kept private, performs an action it should have refused, or behaves in ways that compromise system integrity.

## The Components of an AI Attack Surface

An AI system is not a single component. It is a pipeline of components, each with distinct attack characteristics. The **input layer** is where user-provided data enters the system — text, files, structured parameters, multimodal content. The **prompt layer** contains the instructions that guide model behavior — system prompts, few-shot examples, task descriptions. The **context layer** manages conversation history, user state, and persistent memory. The **retrieval layer** fetches documents or data to augment the model's response. The **tool layer** enables the model to take actions — calling APIs, querying databases, triggering workflows. The **model layer** is the foundation model itself, with its training data, capabilities, and inherent biases. The **output layer** formats and delivers responses to users. The **integration layer** connects the AI system to external services, authentication systems, logging infrastructure.

An attacker does not need to compromise all of these layers. They need to compromise one. A vulnerability in the input layer can bypass prompt-level protections. A weakness in the retrieval layer can surface documents the user should not access. A flaw in the tool layer can turn a chatbot into an unauthorized agent. Mapping the attack surface means identifying every component where an adversary can inject malicious input, manipulate behavior, or extract sensitive information.

## Attack Surface Inventory

The first step in securing an AI system is creating an inventory of every component that processes untrusted input or generates externally visible behavior. This is not a compliance exercise. This is threat modeling. For each component, you document what it does, what data it handles, what privileges it operates with, and how an attacker could abuse it.

Start with inputs. What are all the ways a user can provide data to your system? Direct text input in a chat interface. File uploads that get processed and embedded. Voice recordings that get transcribed. Images analyzed by multimodal models. API requests with structured parameters. URL parameters, form fields, webhooks that trigger processing. Each input vector is a potential injection point. Document them all.

Next, identify what the AI system can access. What databases does the retrieval layer query? What documents can it surface? What user data is included in prompts? What APIs can the tool layer call? What external systems can it integrate with? An attacker's goal is to make the AI system access resources it should not touch or disclose information it should keep private. You need to know the full scope of what is accessible.

Then map the output paths. How does information leave your system? Text responses to users. Structured data returned via API. Logs and telemetry sent to monitoring systems. Error messages displayed in the interface. Side-channel signals like response latency or token counts. Attackers extract information through any channel that leaks data. If you have not cataloged your output vectors, you cannot defend them.

Finally, document the trust boundaries. Where does your system trust input that could be adversarial? User queries are untrusted by default. But what about documents retrieved from your internal knowledge base — can those be trusted, or could an attacker have injected malicious content during the indexing process? What about system prompts loaded from configuration files — are those immutable, or could they be modified by an attacker with access to your deployment pipeline? Every trust boundary is a place where an attack can transition from external to internal.

## Differences from Traditional Attack Surfaces

AI attack surfaces differ from traditional software in three critical ways. First, they are **language-based**. An attacker does not need to craft malformed packets or exploit buffer overflows. They can use natural language that looks like a normal user query but contains embedded instructions the model interprets as directives. The attack payload is semantically meaningful, not syntactically malicious.

Second, they are **probabilistic**. The same input can produce different outputs depending on model version, temperature settings, or random sampling. This makes attacks harder to reproduce and harder to defend against. A mitigation that works 95 percent of the time still fails one in twenty attempts. An attacker only needs it to work once.

Third, they are **dynamic**. Traditional attack surfaces are relatively stable — your API endpoints do not change daily, your database schema does not shift every week. AI attack surfaces evolve constantly. You update the system prompt to improve response quality, and you introduce a new injection vector. You add a retrieval source, and you expand what an attacker can extract. You upgrade to a new model version, and the jailbreak techniques that failed yesterday work today. Every change to your AI system is a change to your attack surface.

## Prioritizing by Risk and Exposure

Not all attack surfaces carry equal risk. A vulnerability that allows extracting your system prompt is a disclosure issue. A vulnerability that allows exfiltrating customer PII is a regulatory and reputational catastrophe. A vulnerability that allows an attacker to call privileged APIs or modify data is a system integrity failure. Prioritize your defenses based on impact.

Start with the components that handle the most sensitive data. If your retrieval layer can access customer records, financial transactions, or health information, that layer is a high-priority target. If your tool layer can initiate payments, modify user accounts, or delete data, that layer requires the strongest defenses. If your output layer can leak internal system details or training data, that layer needs robust filtering.

Next, consider exposure. A chatbot accessible to the public internet has higher exposure than an internal admin tool. A voice assistant processing audio from arbitrary users has higher exposure than a document summarizer that only runs on pre-validated PDFs. An API with no rate limiting has higher exposure than one that throttles suspicious activity. Higher exposure means more opportunities for an attacker to probe for weaknesses.

Finally, assess exploitability. Some attack surfaces are theoretically vulnerable but practically difficult to exploit. Others are trivial to attack once discovered. A prompt injection that requires precise phrasing and multiple attempts is harder to exploit than one that works with common phrasings. A side-channel attack that requires thousands of queries to extract a single bit of information is harder to exploit than one that dumps the full system prompt in a single interaction. Focus first on the vulnerabilities that are both high-impact and easy to exploit.

## The Living Attack Surface

Your attack surface is not a document you write once and file away. It is a living map that changes with your system. Every time you add a new feature, you add new attack vectors. Every time you update your model, you change the behavior an attacker can elicit. Every time you integrate a new data source, you expand what can be extracted.

Treat attack surface mapping as part of your development process. When Product proposes a new feature that lets users upload documents, Security evaluates what an attacker could hide in those documents. When Engineering updates the system prompt to improve task performance, the team reviews whether the change introduces new instruction-following vulnerabilities. When a new model version is released, you test whether previous jailbreak mitigations still hold.

The teams that get this right build attack surface review into their change approval process. No deployment happens without a security assessment of what new attack vectors it introduces. The teams that get it wrong treat security as a pre-launch checklist — and discover post-launch that their system is vulnerable in ways they never considered.

## What You Cannot Defend

Mapping your attack surface reveals something uncomfortable: there are components you cannot fully secure. You cannot prevent users from submitting natural language input — that is the core value proposition of your AI system. You cannot eliminate the possibility that a model will interpret that input in unintended ways — language models are probabilistic, not deterministic. You cannot guarantee that a sufficiently motivated attacker will not find a prompt injection that works against your current defenses.

What you can do is reduce the blast radius. You can limit what the model can access when it is compromised. You can filter outputs to prevent the most dangerous disclosures. You can monitor for attack patterns and throttle suspicious activity. You can design your system so that a successful attack against one component does not cascade into full system compromise. Defense in depth is not optional for AI systems. It is the only strategy that works.

The attack surface map is your starting point. It tells you what needs defending, where the highest risks are, and how an attacker might move through your system. The next step is understanding the specific vectors they will use to enter, starting with the most common and most dangerous: the input layer.

---

The input layer is not just where data enters your system — it is where attackers craft the payloads that will manipulate every downstream component, and understanding input vectors means thinking like someone who wants to break your defenses.
