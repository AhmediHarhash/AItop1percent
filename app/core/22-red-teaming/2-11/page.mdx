# 2.11 — Multi-Tenant Attack Surfaces: Isolation Boundaries as Targets

Perfect isolation is the foundational requirement of multi-tenant systems. Customer A's data must never reach Customer B. Customer A's prompts must never influence Customer B's outputs. Customer A's usage must never degrade Customer B's performance. In traditional software, this is a solved problem — databases have row-level security, applications have session isolation, infrastructure has network segmentation. In AI systems, isolation is vastly harder. The model processes all tenants' inputs through the same weights, the same attention mechanisms, the same context management. A prompt from one tenant can influence the model's behavior for all tenants. A cache entry from one tenant can leak to another. A resource exhaustion attack from one tenant can crash the system for everyone.

When isolation fails in a multi-tenant AI system, every customer is compromised. Not just the attacker's account. Every tenant who shares infrastructure with the attacker.

A SaaS company in October 2025 operated a multi-tenant AI assistant. Each customer had their own workspace, their own data, their own users. The system used a shared model with customer-specific context injected into each request. An attacker discovered they could craft prompts that poisoned the model's context in ways that persisted across requests — causing subsequent requests from other tenants to receive outputs influenced by the attacker's earlier inputs. The attack was subtle. Customer B would ask a normal question and receive an answer that included fragments of information from Customer A's session. The company did not detect the issue for two weeks. When they did, they had to notify 340 enterprise customers that their data may have been exposed to other tenants. The breach cost them $4.2 million in remediation, legal expenses, and lost contracts.

## Why Multi-Tenant AI Is Uniquely Challenging

Multi-tenant traditional software isolates execution. Each tenant's requests run in separate processes, separate containers, separate database transactions. If one tenant's code has a bug, it does not affect other tenants. If one tenant's database query is malicious, it does not access other tenants' data.

Multi-tenant AI systems share the model. Every tenant's request goes through the same neural network, the same inference engine, the same context window. You can isolate data at the application layer — separate databases, separate embeddings, separate knowledge bases — but you cannot isolate the model's weights. The model is shared infrastructure. This creates new isolation challenges.

The model's internal state can leak between requests. Attention patterns, activation values, cached computations — these can carry information from one request to the next. If the system does not fully reset state between tenants, information from Tenant A's request can influence Tenant B's output. This is not a bug in your code. It is an artifact of how transformers work.

Multi-tenant AI also creates new resource contention patterns. One tenant can submit prompts that consume massive amounts of compute, memory, or context — degrading performance for all other tenants. Traditional rate limiting is per-tenant. But AI systems also need global resource management. A tenant who stays within their rate limit can still exhaust shared resources if their individual requests are expensive enough.

## Isolation Boundary Attack Vectors

Isolation boundaries in multi-tenant AI exist at multiple layers: data isolation, context isolation, session isolation, cache isolation, resource isolation. Each layer is an attack surface.

Data isolation is the most obvious. Tenant A's data must not appear in Tenant B's outputs. This requires isolating data stores — separate embeddings, separate vector databases, separate document repositories. But data isolation is not sufficient. If the model is fine-tuned on a shared dataset that includes multiple tenants' data, the model itself becomes a data leakage vector. The fine-tuned model can memorize and reproduce training data from any tenant.

Context isolation means ensuring that the context window for Tenant A's request does not include any content from Tenant B. This is straightforward in stateless systems — each request starts with a clean slate. It is harder in systems with conversation history, where the context accumulates over multiple turns. If the system accidentally mixes conversation histories from different tenants, context isolation breaks.

Session isolation means ensuring that session-level state — authentication tokens, user preferences, cached computations — does not leak between tenants. A common vulnerability: the system uses a shared cache keyed by input hash. Tenant A submits a prompt. The result is cached. Tenant B submits the same prompt and receives the cached result — which may contain Tenant A's data because the result was generated in Tenant A's context.

Cache isolation is particularly subtle. Caching is essential for performance in AI systems — reusing inference results, reusing embeddings, reusing retrieved documents. But shared caches create cross-tenant leakage paths. The cache key must include tenant ID to prevent one tenant from retrieving another tenant's cached results. Even then, cache poisoning attacks can inject malicious results that get served to other tenants.

## Prompt Leakage Between Tenants

Prompt leakage occurs when a prompt from Tenant A influences outputs for Tenant B. This can happen through model state persistence, through caching, or through adversarial prompt design that exploits how the model generalizes.

The most direct form: Tenant A submits a prompt designed to influence the model's behavior for subsequent requests. If the model's internal state is not fully reset between tenants, Tenant B's request can be affected by Tenant A's prompt. This is rare in well-designed systems but possible if the inference engine has subtle state management bugs.

More commonly, prompt leakage happens through caching or memoization. A system caches inference results to improve performance. Tenant A submits a prompt. The result is cached. Tenant B submits a similar prompt. The system returns the cached result from Tenant A's session. If the cached result includes tenant-specific data, Tenant B just received Tenant A's information.

A document analysis company in November 2025 had exactly this issue. They cached document summaries keyed by document hash to avoid reprocessing identical documents. Tenant A uploaded a contract and received a summary. Tenant B uploaded the same contract template and received the same cached summary — which included Tenant A's specific contract terms because the summary was generated with Tenant A's context. The cache key included document content but not tenant ID. The isolation failure was in the caching layer, not the model.

## Context Bleeding and Session Collision

Context bleeding is when the model's context window includes content from the wrong tenant. This typically happens through state management errors — the system fails to clear the context between sessions, or it accidentally loads conversation history from the wrong tenant.

A customer support platform in December 2025 experienced context bleeding when their session management system had a race condition. Under high load, session IDs occasionally collided — Tenant A's request would load Tenant B's conversation history. The model would then generate responses based on Tenant B's prior conversation, leaking Tenant B's information to Tenant A. The race condition occurred in less than 0.1% of requests, making it difficult to detect in testing. It only manifested under production load.

Session collision attacks are deliberate attempts to cause session ID conflicts. An attacker tries to predict or force a session ID collision with another tenant's active session. If successful, the attacker's requests get processed in the victim's session context. This requires the session ID to be predictable or brute-forceable — a vulnerability in session management, not in the AI itself.

Preventing context bleeding requires strict session hygiene. Every request must explicitly load the correct tenant's context. The system must validate that the loaded context belongs to the requesting tenant. Session IDs must be unpredictable, sufficiently long, and validated on every request. Context must be cleared between requests — not just reset, but fully cleared so that no residual data from a previous session can leak into the next.

## Cache Poisoning Across Tenants

Cache poisoning in multi-tenant AI systems is when an attacker injects malicious data into a shared cache that gets served to other tenants. The attacker submits a prompt that causes the system to cache a malicious result. Other tenants who submit similar prompts receive the poisoned result from the cache.

Cache poisoning is effective when the cache key is predictable and does not include tenant ID. An attacker can craft prompts that hash to the same cache key as legitimate prompts from other tenants. The attacker's result overwrites the cache. When a legitimate user submits the same prompt, they receive the attacker's result instead of a freshly generated one.

A legal research platform in January 2026 cached case law summaries keyed by case citation. An attacker discovered they could submit a fabricated case summary with a real citation. The system cached the fabricated summary. When other tenants searched for that case, they received the attacker's fabricated summary from the cache instead of the real case law. The cache poisoning persisted until the cache expired or was manually cleared.

Defending against cache poisoning requires including tenant ID in cache keys, validating cache entries before serving them, and implementing cache entry expiration. Cache keys must be tenant-scoped so that Tenant A's cache entries are never served to Tenant B. Cache entries should have integrity checks — signatures or hashes that validate the entry was not tampered with. Cache TTLs should be short enough that poisoned entries expire before they cause widespread damage.

## Resource Exhaustion Attacks

Resource exhaustion in multi-tenant systems is when one tenant's usage degrades performance for all tenants. In AI systems, this is easier to achieve than in traditional software because AI requests consume unpredictable amounts of compute, memory, and time.

A tenant can submit prompts that maximize resource consumption: extremely long prompts that fill the context window, prompts that trigger expensive tool calls, prompts that cause the model to generate maximum-length outputs, prompts that hit retrieval systems with complex queries. If the system does not limit resource consumption per tenant, one malicious or misconfigured tenant can exhaust shared resources.

A content generation platform in September 2025 had per-tenant rate limits on number of requests but no limits on cost per request. An attacker discovered they could submit prompts that caused the model to generate outputs at maximum token length, call multiple expensive APIs, and trigger complex retrieval operations — all within a single request. The attacker stayed within their request rate limit but consumed a disproportionate share of compute resources. Other tenants experienced degraded response times and timeouts.

Resource exhaustion defense requires multi-dimensional rate limiting. Limit not just request count but also tokens processed, tokens generated, tool calls executed, retrieval queries performed, and compute time consumed. Implement per-tenant resource quotas that track cumulative usage across all dimensions. When a tenant exceeds their quota, throttle or reject their requests to protect other tenants.

## Testing Tenant Isolation

Testing tenant isolation means attempting to access, influence, or degrade another tenant's system from your own tenant account. The goal is to prove that isolation boundaries hold under adversarial conditions.

Test data isolation by attempting to retrieve another tenant's data. Submit queries that reference other tenants' identifiers — user IDs, document IDs, session IDs. If the system returns results, data isolation is broken. Test whether filtering is applied correctly — whether the system actually checks tenant ownership or just assumes requests are tenant-scoped.

Test context isolation by submitting prompts in one tenant's session and checking whether those prompts influence outputs in another tenant's session. Open two sessions from different tenant accounts. In Tenant A's session, submit a prompt designed to poison context or cache. In Tenant B's session, submit a similar prompt and check whether the output is influenced by Tenant A's earlier input.

Test cache isolation by submitting identical queries from different tenant accounts and verifying that each receives tenant-specific results. If Tenant A and Tenant B submit the same query but have different data, they should receive different results. If they receive identical results, the cache is not tenant-scoped.

Test resource isolation by submitting high-cost requests from one tenant and measuring impact on another tenant's performance. If Tenant A's expensive requests cause Tenant B's response times to increase, resource isolation is insufficient. The system should throttle Tenant A without affecting Tenant B.

## Isolation Patterns That Work

Isolation in multi-tenant AI requires layered defenses. At the data layer, use tenant-scoped databases or enforce row-level security with tenant ID on every query. Never assume a query is tenant-scoped — always filter by tenant ID explicitly.

At the context layer, include tenant ID in every context retrieval operation. Validate that loaded context belongs to the requesting tenant before injecting it into the model. Use separate context stores per tenant if possible. If using a shared context store, key all entries by tenant ID and validate tenant ownership on every read.

At the cache layer, include tenant ID in every cache key. A cache key for an AI result should be a hash of the input prompt combined with the tenant ID. This ensures that even if two tenants submit identical prompts, they do not share cache entries. Implement cache entry validation — each entry should include metadata that specifies which tenant owns it, and the system should reject entries that do not match the requesting tenant.

At the resource layer, implement per-tenant quotas on all expensive operations. Track tokens processed, tokens generated, tool calls, retrieval queries, and compute time per tenant. Enforce quotas in real time — if a tenant reaches their quota, throttle or reject additional requests until the quota resets. Use separate resource pools per tenant tier if you have multiple service levels.

At the session layer, use cryptographically strong session IDs that are unpredictable and include tenant ID as part of session metadata. Validate session ownership on every request. Implement session expiration and invalidation. Do not reuse session IDs across tenants or across time.

At the model layer, reset all inference state between requests. If the inference engine maintains internal state — cached activations, attention patterns, intermediate results — that state must be cleared between tenants. Use stateless inference when possible. When stateful inference is necessary, implement strict state isolation so that state from Tenant A's request never persists into Tenant B's request.

Multi-tenant isolation is the hardest security problem in AI systems. When you get it wrong, you do not just compromise one user or one session. You compromise every customer who shares your infrastructure. The final attack surface is not technical. It is human. AI systems can be weaponized to manipulate the people who use them.
