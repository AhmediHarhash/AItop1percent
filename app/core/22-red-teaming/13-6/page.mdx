# 13.6 — Red Teaming Documentation for Audits

The audit begins well. Your team has been red teaming for a year. You have found vulnerabilities, fixed them, tracked metrics. Then the auditor asks to see documentation of your test methodology. Your team pulls up internal Slack threads, GitHub issues, and scattered notes in a wiki. The auditor asks for a formal test plan. You do not have one. They ask for evidence that findings were remediated before production deployment. You have commit logs, but no traceability from finding to fix. They ask how you prioritized which vulnerabilities to address first. You do not have a documented prioritization framework. The red teaming work was real, but the documentation does not prove it happened.

Audit-ready documentation is not optional. Regulators, auditors, and compliance frameworks require evidence that security testing occurred, what it covered, what it found, and how findings were addressed. Without proper records, your red teaming efforts cannot be verified — and unverifiable work does not count as proof of due diligence. This is not about paperwork for its own sake. This is about building a record that demonstrates you took AI security seriously, tested systematically, and acted on what you found.

## Why Documentation Matters for Audits

Auditors do not take your word that you tested your AI system. They need evidence. Documentation is that evidence. It demonstrates that red teaming was planned, executed according to a methodology, and produced findings that drove action. Without documentation, you are asking the auditor to trust that your security claims are true — and auditors do not operate on trust.

Different compliance frameworks have different documentation requirements, but all require proof that you tested for the risks relevant to your domain. SOC 2 audits require evidence of security testing as part of continuous monitoring. ISO 27001 requires documented risk assessments and security controls. Industry-specific regulations like HIPAA, PCI-DSS, and financial services rules require proving that systems handling sensitive data have been tested for vulnerabilities. If your AI system falls under any of these frameworks, red team documentation is part of your compliance evidence.

Documentation also protects you in the event of an incident. If your AI system is compromised or causes harm, regulators and legal teams will investigate what security measures were in place. If you can produce red team reports showing you tested for the failure mode that occurred, found it, and remediated it, you demonstrate due diligence. If you cannot produce those records, it looks like you never tested — or worse, that you knew about the risk and did nothing.

The standard is not "we have some notes." The standard is comprehensive, organized, traceable documentation that an external auditor can review and understand without having to interview your team to fill in gaps. If your documentation requires someone to explain it, it is incomplete.

## What Auditors Want to See

Auditors evaluate red teaming documentation against a checklist. They want to see that you planned testing systematically, executed it thoroughly, documented findings clearly, and tracked remediation to completion. Each of these areas requires specific evidence.

For planning, auditors want to see a documented test plan. This includes the scope of testing — what systems, what attack vectors, what time period. It includes the methodology — what techniques you used, what tools you applied, what assumptions you made about attacker capabilities. It includes roles and responsibilities — who conducted the testing, who reviewed findings, who authorized remediation. A test plan demonstrates that red teaming was intentional, not ad-hoc.

For execution, auditors want to see evidence that testing actually happened. This can include test case logs, session recordings, timestamped activity records, or tool output. The evidence should show what was tested, when it was tested, and by whom. It should be possible to trace individual test cases back to the test plan and forward to findings. If you claimed you would test for prompt injection vulnerabilities and the execution logs show no prompt injection tests, the auditor will note the gap.

For findings, auditors want to see detailed reports. Each finding should include a description of the vulnerability, how it was discovered, what the potential impact is, and a severity rating. Findings should be specific enough that someone else could understand the issue and verify the fix. Generic descriptions like "model can be jailbroken" are not sufficient. The finding should describe the specific prompt or input that caused the jailbreak, what the model did, and why that behavior is a problem.

For remediation, auditors want to see evidence that findings were addressed. This includes remediation plans, implementation records, and validation testing. For each finding, there should be a clear record: what action was taken, when it was completed, who was responsible, and how the fix was verified. If a finding was accepted as a known risk rather than remediated, the auditor wants to see the risk acceptance decision documented with rationale and approvals.

## Test Methodology Documentation

Documenting your methodology means explaining how you conduct red teaming in enough detail that an auditor can evaluate whether your approach is rigorous. This is not a one-page summary — it is a detailed description of your process.

Start with your threat model. What adversarial scenarios are you testing for? What attacker capabilities do you assume? What are the highest-risk failure modes for your AI system? Your threat model should connect to your product's risk profile. If you are testing a healthcare AI, your threat model should include patient data extraction and unsafe medical recommendations. If you are testing a customer service chatbot, your threat model should include brand reputation damage and unauthorized data access. The methodology should show that you test for the threats that matter.

Document your test case design process. How do you generate test cases? Do you use known attack frameworks like OWASP LLM Top 10 or MITRE ATLAS? Do you use automated tools to generate adversarial inputs? Do you conduct manual exploratory testing? Do you combine multiple techniques? The auditor wants to understand whether your testing is comprehensive or limited to a narrow set of scenarios.

Describe your tools and infrastructure. What tools do you use for red teaming? Custom scripts, open-source frameworks, commercial products? How do you simulate adversarial behavior — manual prompting, automated fuzzing, synthetic attack generation? What environment do you test in — production, staging, isolated sandbox? The choice of tools and environment affects what you can test and what you can find. Document the setup so the auditor understands the constraints.

Include your severity classification framework. How do you rate findings — critical, high, medium, low? What criteria determine severity? Is it based on exploitability, impact, likelihood, or a combination? Consistent severity ratings make it possible to prioritize remediation and track trends over time. If your severity framework is informal or inconsistent, findings will be hard to triage and auditors will question whether you understand which risks are most important.

## Finding Documentation Standards

Every finding from red teaming should be documented in a structured way. At minimum, each finding should include these elements: a unique identifier, a title, a description, a severity rating, reproduction steps, potential impact, and remediation recommendations. Additional context like affected components, date discovered, and discoverer name is also valuable.

The description should explain what the vulnerability is in clear language. Avoid jargon that obscures the issue. If the vulnerability is prompt injection that bypasses content filtering, say that explicitly. If it is training data extraction via adversarial prompting, describe what data can be extracted and how. The description should be understandable to both technical and non-technical stakeholders.

Reproduction steps are critical. An auditor or another engineer should be able to follow your steps and observe the same vulnerability. Include the exact prompt, the exact input, or the exact sequence of interactions that triggers the issue. If the vulnerability requires specific conditions — certain model version, certain user privileges, certain tool configurations — document those. Reproducibility proves the finding is real, not a one-time fluke.

Potential impact describes what could happen if the vulnerability is exploited. This is where you connect technical findings to business and regulatory risk. A prompt injection vulnerability is not just "model behaves unexpectedly" — it is "attacker could extract proprietary data," "model could generate harmful medical advice," or "system could bypass content moderation and produce regulated content." Impact statements make it clear why the finding matters.

Remediation recommendations provide guidance on how to fix the issue. This does not need to be a complete implementation plan, but it should point the remediation team in the right direction. If the issue is inadequate input filtering, suggest what types of filters might help. If the issue is overly permissive tool access, suggest tighter access controls. If the issue is training data leakage, suggest data sanitization or differential privacy. Recommendations make findings actionable.

## Evidence Preservation

Auditors want to see the raw evidence behind your findings, not just summaries. This means preserving logs, screenshots, tool outputs, and any artifacts that demonstrate the vulnerability. Evidence preservation is especially important for findings that are later remediated — once the fix is deployed, the vulnerability may no longer be reproducible. If you do not preserve evidence from the original test, you cannot prove the finding was real.

For prompt-based attacks, save the exact prompts that triggered the vulnerability and the model's responses. Capture full conversation histories if the attack required multi-turn interaction. If the attack exploited tool use, save the tool call logs showing what the model attempted to do. If the model generated harmful content, save the output — sanitized if necessary to remove truly dangerous material, but preserved enough to show what happened.

For automated testing, preserve tool configurations and output logs. If you used a fuzzing tool to generate thousands of test inputs, save the configuration that produced the successful attack. Save the logs showing which inputs triggered failures and how the system responded. Automated testing generates large volumes of data — you do not need to preserve everything, but you need to preserve enough to substantiate your findings.

For manual red teaming, document the testing session. This can be as simple as timestamped notes describing what you tested, what you found, and what you observed. If you conducted a two-hour session testing jailbreak techniques, your notes should show which techniques you tried, which worked, and what the model did. Session notes turn exploratory testing into auditable evidence.

Organize evidence by finding. Each documented vulnerability should have an associated evidence folder or database entry where all supporting materials are stored. When an auditor asks to see evidence for a specific finding, you should be able to retrieve it immediately. If evidence is scattered across unlabeled files or buried in Slack threads, you effectively do not have it.

## Audit Trail Requirements

An audit trail connects findings to actions. It shows that you did not just document vulnerabilities — you tracked them through remediation and validated the fixes. Auditors look for closed loops: finding identified, remediation planned, fix implemented, validation completed. Gaps in the trail raise questions about whether findings were actually addressed.

Track finding status. Each vulnerability should have a status field: open, in-progress, remediated, accepted risk, or closed. Status should be updated as the finding moves through your remediation workflow. The timestamp of each status change creates a timeline showing how quickly findings are addressed. If critical findings sit in open status for months, the auditor will ask why.

Track remediation ownership. Every finding should have an assigned owner responsible for driving remediation. This is usually an engineering lead, a product manager, or a security engineer. Ownership ensures accountability. It also provides the auditor with someone to interview about how the finding was handled. If a finding has no owner, it signals that no one was responsible for fixing it.

Track remediation actions. For each finding, document what was done to address it. This can be links to pull requests, tickets in your issue tracker, or change records in your deployment system. The remediation action should be specific enough that an auditor can verify it happened. "Improved prompt filtering" is vague. "Implemented regex-based filter to block prompt injection patterns, deployed in PR #1234" is specific.

Track validation testing. After a finding is remediated, you should re-test to confirm the fix works. The audit trail should include validation test results — showing the same attack no longer succeeds. If the original finding included a specific prompt that bypassed safety controls, the validation test should show that same prompt now fails safely. Validation closes the loop and proves remediation was effective.

## Building Documentation into Workflow

Documentation should not be a separate task that happens after testing. It should be built into your red teaming workflow so that documentation happens automatically as you work. If documentation is manual and ad-hoc, it will be incomplete and inconsistent. If it is integrated into your process, it will be comprehensive and reliable.

Use issue tracking systems. Most teams already use systems like Jira, Linear, or GitHub Issues for bug tracking. Use the same system for red team findings. Create a dedicated project or label for red team vulnerabilities. Each finding becomes a ticket with all required fields: title, description, severity, reproduction steps, evidence attachments, owner, status. The issue tracker becomes your audit trail — every finding is documented, every status change is logged, every remediation action is linked.

Template your documentation. Create finding templates that enforce consistent structure. When a red teamer discovers a vulnerability, they fill out the template — which prompts them to include all required elements. Templates reduce variability and ensure nothing is missed. They also make findings easier to review because they all follow the same format.

Automate where possible. If you run automated red teaming tests, configure your tools to output structured findings that can be imported into your tracking system. If you test regularly, automation ensures every test run produces documentation without manual effort. Even manual testing can be partially automated — for example, using scripts to capture screenshots, log interactions, or timestamp activities.

Require documentation as part of testing completion. Red teaming should not be considered complete until findings are documented. If a red teamer reports verbally that they found an issue but has not filed it in your tracking system, the finding does not officially exist yet. This discipline ensures documentation happens in real time, not weeks later when details are forgotten.

## Demonstrating Continuous Testing

Auditors do not just want to see that you tested once. They want to see that you test continuously as your AI system evolves. Documentation should show a pattern of regular testing over time, not a spike of activity right before the audit.

Maintain a testing calendar. Document when red team assessments occur, what scope they cover, and who conducts them. This can be a simple spreadsheet or a section in your security documentation. The calendar should show that testing happens on a defined cadence — monthly, quarterly, or after major releases. It should show that you test after significant changes like model updates, new features, or architecture changes.

Track metrics over time. How many findings did each assessment produce? What severity distribution? How long did remediation take? What percentage of findings were discovered by internal teams versus external assessments? Metrics show trends. If you see finding counts decreasing over time, it suggests your system is getting more secure or your red team is getting less effective. Either way, the trend is worth discussing with auditors.

Document scope evolution. As your AI system grows, your red teaming scope should grow with it. Document how scope has changed over time. If you initially tested only prompt injection and now test for training data extraction, tool misuse, and adversarial robustness, that evolution shows maturity. If your scope has not changed in two years despite product growth, it suggests your testing may not be keeping pace with your risk.

Preserve historical reports. Keep copies of all red team reports, even old ones. Historical reports provide context for current findings. They show what vulnerabilities you used to have, what you fixed, and what patterns recur. An auditor reviewing your security posture will want to see not just where you are now, but how you got there. Historical reports tell that story.

Red teaming without documentation is security theater. You may be finding vulnerabilities and fixing them, but if you cannot prove it, the work does not count as evidence of due diligence. The next step is translating that technical evidence into business language that executives and boards can act on.

