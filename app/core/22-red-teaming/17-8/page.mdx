# 17.8 — Red vs Monitoring Gap Analysis — Finding What Your Telemetry Misses

The security lead pulls up the dashboard and reads the numbers aloud. Forty-three detection rules active. Ninety-one percent regression pass rate. Mean time to detection deployment under thirty-six hours. The purple team has been operating for eight months, and every metric looks healthy. Then the red team lead opens a spreadsheet with two columns. The left column lists every attack technique the red team can currently execute against the production AI system. The right column shows whether each technique would trigger any deployed detection. Of the sixty-two techniques listed, twenty-three show "undetected." Thirty-seven percent of the red team's capability is invisible to the monitoring stack. The dashboard looks excellent because it measures the detections you have. It says nothing about the detections you need.

This is the purpose of gap analysis. Not to evaluate whether your existing detections work — regression testing handles that. Gap analysis asks a different question: what can the attacker do that you cannot see?

## The Gap Analysis Framework

The framework is straightforward in concept and laborious in execution. It requires three inputs: a comprehensive list of attack techniques, a comprehensive inventory of deployed detections, and a testing methodology that determines whether each technique would actually trigger each detection under realistic conditions.

**The attack technique inventory** starts with the red team's own catalog — every technique they have developed, adapted, or discovered during testing. This should be organized by attack category: prompt injection variants (direct, indirect, encoded, multi-turn, multilingual), system prompt extraction methods, data extraction patterns, tool abuse techniques, safety bypass approaches, memory poisoning methods, and any cross-component attack chains that combine multiple techniques. A mature red team operating for six to twelve months typically has sixty to one hundred techniques in their inventory. This number grows continuously as new model versions create new attack surfaces and as the adversarial research community publishes new methods.

The inventory should also include techniques the red team has not yet tested but that are known to exist in the broader adversarial landscape. The OWASP Top 10 for LLM Applications, conference presentations from events like DEF CON AI Village and Black Hat, published adversarial research from labs like Anthropic, Google DeepMind, and academic institutions, and vendor advisories from AI security companies like Lakera, HiddenLayer, and Lasso Security all provide technique descriptions that should be included even if the red team has not yet operationalized them. These represent the "known unknowns" — attack paths that exist in theory and need testing to determine whether they apply to your specific system.

**The detection inventory** lists every deployed detection with its scope: what signals it monitors, what thresholds it uses, what attack behaviors it is designed to catch, and what its known limitations are. This inventory should be extracted directly from the SIEM configuration and attack-detection card library, not reconstructed from memory. Teams that try to build the detection inventory from team knowledge rather than system configuration invariably discover detections they thought existed but were never deployed, or detections that were deployed but have been disabled or misconfigured.

**The coverage assessment** is the core of the analysis. For each attack technique, the team determines whether it would produce signals that at least one deployed detection would catch. This is not a theoretical exercise. The assessment requires either actually executing the technique and observing whether detections fire, or tracing the technique's telemetry fingerprint through the detection logic to confirm a match. Theoretical assessments — "this detection should catch that technique" — are unreliable because they miss configuration details, threshold mismatches, and telemetry gaps that only manifest in practice.

## The Three Coverage Classifications

Each technique-detection pairing falls into one of three classifications.

**Detected** means the technique, when executed, triggers at least one deployed detection with sufficient confidence to generate a tier-one or tier-two alert. The detection has been validated through actual testing, and the alert provides enough context for an analyst to investigate and respond. Detected does not mean prevented — the detection may fire after the attack has already produced impact. But it means the attack is visible.

**Partially detected** means the technique produces some signals that existing detections observe, but the signals are either below alert thresholds, classified as tier-three events that do not trigger analyst attention, or captured in telemetry without corresponding detection rules. Partially detected techniques are dangerous because they create a false sense of coverage. The telemetry exists. The data flows through the pipeline. But no rule converts the data into an actionable alert. A future attacker who uses this technique will leave traces that are visible in retrospective forensic analysis but invisible in real-time operations.

**Undetected** means the technique produces no signals in the current monitoring stack, or produces signals that no deployed detection evaluates. This is the true blind spot. The attacker can execute this technique and leave no trace in the SOC's operational visibility. The only way to discover the attack is through manual investigation triggered by some other indicator, or through a retrospective analysis that happens to examine the relevant telemetry at the relevant time.

## The Common Gaps in AI Security Monitoring

Across dozens of gap analyses conducted in the AI security community since 2024, certain blind spots appear consistently. These represent structural weaknesses in how AI monitoring is typically designed, not failures of specific teams.

**Indirect prompt injection through retrieval** remains the most common undetected technique. The injection arrives not through user input but through documents, knowledge base entries, or external data sources that the RAG pipeline retrieves and injects into the model's context. Input classifiers that scan user messages miss it entirely because the malicious content never passes through the user input path. The injection is embedded in a document that the model processes as trusted context. Detecting this requires scanning retrieval results before they reach the model — a telemetry source that most monitoring stacks do not include because the retrieval pipeline was built for relevance, not security.

**Slow-burn extraction** defeats rate-based and session-based detection by distributing the attack across many sessions over days or weeks. Instead of extracting one hundred records in a single session, the attacker extracts one record per session across one hundred sessions using different accounts or IP addresses. Per-session monitoring sees normal behavior. Per-user monitoring sees normal behavior because each account makes a single request. Only cross-user aggregation over long time windows reveals the pattern — and most detection rules operate on per-session or per-user windows of minutes to hours, not cross-user windows of days.

**Memory poisoning** exploits conversation memory systems by injecting persistent instructions that influence future sessions. The attacker sends a message that embeds instructions in the model's long-term memory. In subsequent sessions, the model follows those instructions without any new injection from the attacker. The initial poisoning event may produce telemetry, but if it is not detected in real time, the ongoing effects are invisible because the model appears to be following its normal instructions — the poisoned instructions are indistinguishable from legitimate memory content in the telemetry.

**Cross-tool lateral movement** chains multiple tool calls to escalate from a low-privilege tool to a high-privilege one. The model uses a document search tool to discover the existence of an administrative API, then calls that API with parameters learned from the search results. Each individual tool call may fall within normal parameters. The lateral movement pattern — low-privilege discovery followed by high-privilege access — is the signal, and detecting it requires correlation across tool types rather than monitoring each tool independently.

**Multi-turn conversation manipulation** gradually shifts the model's behavior over the course of a long conversation. Early messages establish context and trust. Middle messages subtly reframe the conversation's scope. Late messages request actions that the model would have refused if asked directly but accepts in the accumulated context. Per-message analysis sees nothing anomalous because each message is benign in isolation. The attack exists in the trajectory of the conversation, which requires conversation-level behavioral analysis rather than message-level scanning.

## Prioritizing Gaps by Severity and Likelihood

Not all gaps deserve the same urgency. A gap analysis that produces thirty undetected techniques and treats them all equally is not useful. Prioritization follows a two-dimensional framework: severity of impact if the technique is successfully used in production, and likelihood that an adversary would discover and use the technique.

**Critical priority** goes to techniques that produce high-severity impact (data exposure, cross-tenant leakage, safety violations with regulatory consequences) and that are publicly known or easily discoverable (documented in OWASP, demonstrated at security conferences, available in open-source toolkits). These are the gaps that real attackers are most likely to exploit with the most damaging consequences. They represent the purple team's immediate detection engineering backlog.

**High priority** goes to techniques with high-severity impact but low public visibility (novel techniques discovered by the internal red team that are not yet widely known), or techniques with moderate impact but high likelihood (common techniques that produce operational disruption rather than catastrophic data exposure). These enter the detection engineering pipeline behind the critical items but ahead of everything else.

**Medium priority** goes to techniques with moderate impact and moderate likelihood. These are often edge cases or situational attacks that require specific preconditions. They are worth tracking and planning for but do not demand immediate engineering investment.

**Low priority** does not mean no priority. It means the technique either produces minimal impact, requires conditions that are unlikely in your specific deployment, or has a low probability of being discovered by external adversaries. These remain on the gap register and are periodically reassessed as the threat landscape evolves.

## Presenting Gap Analysis to Stakeholders

The gap analysis is the most powerful artifact the purple team produces for communicating risk to leadership. It translates abstract security concerns into concrete numbers that executives can act on.

The presentation format that drives investment follows a simple structure. Total attack techniques assessed: sixty-two. Detected: thirty-four (55 percent). Partially detected: five (8 percent). Undetected: twenty-three (37 percent). Of the undetected techniques, seven are critical priority, producing direct data exposure risk with publicly known methods. Estimated engineering effort to close the seven critical gaps: twelve weeks across detection engineering, telemetry enrichment, and correlation rule development.

This framing converts a security discussion into a resource allocation discussion. Leadership does not need to understand the technical details of each gap. They need to understand the coverage percentage, the risk associated with the gaps, and the investment required to close them. Most security investment conversations fail because they are too abstract — "we need to improve our AI security posture" is not actionable. "We have seven critical blind spots that allow known attack techniques to extract customer data undetected, and closing them requires a twelve-week engineering sprint" is actionable.

## Using Gap Analysis to Drive the Security Roadmap

The gap analysis is not a one-time assessment. It is a recurring input to the security engineering roadmap. Each cycle of gap analysis produces a prioritized list of detections to build, telemetry sources to add, and correlation rules to develop. As the team closes gaps, the coverage percentage increases. As the red team develops new techniques, new gaps appear. The gap analysis cadence should align with the red team's technique development cycle — typically quarterly, with ad-hoc reassessments after major red team breakthroughs or significant system changes.

Over time, the gap analysis tracks the maturity of the purple team operation. A team in its first year might show 40 to 55 percent detection coverage of known techniques. By year two, with consistent gap closure and detection investment, coverage typically reaches 70 to 80 percent. Teams that sustain the practice for three or more years achieve 85 to 95 percent coverage, with the remaining gaps representing techniques that are genuinely difficult to detect or that require architectural changes to the underlying system.

The trend line matters more than any single snapshot. A team at 60 percent coverage that is improving by 5 percentage points per quarter is in a stronger position than a team at 75 percent coverage that has been flat for a year. The gap analysis makes this trajectory visible and quantifiable.

Gap analysis tells you where your blind spots are. But finding blind spots is only valuable if the people responsible for closing them have the skills, the practice, and the adversarial mindset to do the work. The next subchapter covers capture-the-flag exercises and training programs that build purple team capability through structured, competitive practice.
