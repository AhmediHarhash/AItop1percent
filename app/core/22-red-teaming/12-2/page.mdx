# 12.2 — Red Teaming as Regression Testing: Integration with Section 18

Every vulnerability you discover should become a test that prevents it from returning. This is the foundational principle of adversarial regression testing. If you found a jailbreak in June, you should have an automated test by July that fails if that jailbreak works again. If a prompt injection technique bypassed your guardrails in one release, that technique should be in your regression suite before the next release.

Adversarial regression testing is not separate from functional regression testing. They use the same infrastructure, the same pipelines, the same blocking logic. The difference is what you test. Functional regression tests validate that features work. Adversarial regression tests validate that attacks do not.

Section 18 covers regression testing and release gates in depth. This subchapter shows how to integrate adversarial test cases into that framework so that red teaming becomes a continuous, automated part of your release process.

## Connecting Red Teaming to Regression

Traditional regression testing answers the question: did this change break something that used to work? Adversarial regression testing answers a parallel question: did this change enable something that used to be blocked?

The mechanics are the same. You maintain a suite of test cases. You run them on every change. If a test that passed before now fails, you investigate. The difference is in what constitutes a failure. In functional regression, a failure means a feature stopped working. In adversarial regression, a failure means an attack started working.

This means your regression suite needs two categories of tests. Positive tests validate expected behavior. Negative tests validate blocked behavior. A positive test might check that the model answers a medical question correctly. A negative test might check that the model refuses to provide medical advice when prompted to impersonate a doctor. Both matter. Both belong in the same suite.

When you discover a vulnerability during red teaming, the first step is remediation — you fix the vulnerability. The second step is encoding — you turn the exploit into a negative test case and add it to your regression suite. From that point forward, every release is tested against that exploit. If a future change reintroduces the vulnerability, the regression test catches it before the code ships.

This is how red teaming becomes continuous without requiring continuous manual effort. You test manually once. You encode the finding. The automation tests it forever.

## Converting Findings to Tests

Not every red team finding translates directly into an automated test. Some exploits require complex multi-turn interactions. Some depend on timing or race conditions. Some are subtle enough that automated validation is unreliable. But most findings can be encoded as regression tests with some adaptation.

The process of converting a finding to a test starts with isolating the minimal reproducible exploit. During red teaming, you might discover a jailbreak through a long conversational thread. The full thread is not the test. The test is the smallest input sequence that triggers the vulnerability. You strip away the exploration. You remove the false starts. You distill the exploit to its essential form.

Once you have the minimal exploit, you define the success criterion. For a jailbreak, success means the model refuses to comply or responds with a canned safety message. For a prompt injection, success means the injected instruction is ignored. For data exfiltration, success means the model does not output the sensitive data. The criterion must be automatable — you need a check that can run in CI/CD without human judgment.

Then you encode the test in the same format your regression suite already uses. If your regression tests are written as pytest cases, the adversarial test is a pytest case. If they are JSON test definitions consumed by a test harness, the adversarial test is a JSON definition. The format does not matter. What matters is that adversarial tests live in the same pipeline as functional tests and run with the same cadence.

Some findings require manual verification. For these, you encode what you can and flag the rest for periodic manual review. You might automate the input and have a human reviewer judge the output. Or you might log the model response and sample a subset for review. Partial automation is better than no automation. You reduce the manual burden while still maintaining coverage.

## Golden Set Adversarial Cases

Your adversarial regression suite should include a **golden set** of high-priority test cases that represent the most critical vulnerabilities your system faces. These are the exploits that would cause the most damage if they succeeded — jailbreaks that bypass all safety controls, prompt injections that grant unauthorized access, data exfiltration techniques that leak PII, role confusion attacks that allow privilege escalation.

The golden set is small — typically 20 to 100 test cases. It covers the highest-severity vulnerabilities you have discovered, plus known attack techniques from the research community that are relevant to your system. The golden set runs on every commit. It is fast, focused, and blocking. If any golden set test fails, the build fails.

Beyond the golden set, you maintain a broader regression suite that includes medium and low-severity findings, edge cases, and exploratory tests. This broader suite runs nightly or weekly, depending on your release cadence. It is comprehensive but not blocking. Failures trigger investigation but do not stop releases unless the failure is severe.

The distinction between golden set and broader suite is important for velocity. You cannot block every commit on a 10,000-case adversarial test suite that takes two hours to run. But you can block every commit on a 50-case golden set that runs in five minutes. The golden set gives you continuous protection against critical vulnerabilities. The broader suite gives you comprehensive coverage on a slower cadence.

You review and update the golden set quarterly. New critical vulnerabilities get added. Obsolete tests get retired. The golden set reflects the current highest-priority risks, not the risks you cared about a year ago.

## Severity-Based Test Priority

Not all adversarial test cases have equal importance. A jailbreak that allows the model to generate harmful content is higher priority than a prompt injection that causes a minor formatting error. A data exfiltration exploit that leaks customer PII is higher priority than a role confusion attack that produces mildly inconsistent responses.

You assign severity levels to adversarial test cases the same way you assign severity to bugs. Critical vulnerabilities run on every commit and block releases. High-severity vulnerabilities run nightly and require investigation within 24 hours. Medium-severity vulnerabilities run weekly and require investigation within a sprint. Low-severity vulnerabilities run monthly and are triaged as time allows.

Severity is determined by impact and likelihood. Impact is the harm caused if the exploit succeeds. Likelihood is the probability that an attacker would discover and use the exploit. A high-impact, high-likelihood vulnerability is critical. A low-impact, low-likelihood vulnerability is low-severity. Most vulnerabilities fall somewhere in between.

This severity-based prioritization lets you scale adversarial testing without drowning in test maintenance. You focus engineering effort on the vulnerabilities that matter. You automate and monitor the rest. You never ignore a finding, but you do not treat every finding as equally urgent.

The severity levels are not static. As your system evolves, the risk profile changes. A vulnerability that was low-severity when your system had 100 users might be critical when you have 100,000 users. A jailbreak that was theoretical might become practical when a new attack technique is published. You review severity levels regularly and adjust as needed.

## Test Maintenance Over Time

Adversarial regression tests require active maintenance. Attacks evolve. Defenses change. Tests that were relevant six months ago might be obsolete today. Tests that were easy to automate might become flaky as the system changes.

You treat adversarial test maintenance as part of normal test maintenance. Every quarter, you audit the suite. You identify tests that are no longer relevant and retire them. You identify tests that are flaky or unreliable and fix them. You identify gaps in coverage and add new tests to fill them.

The retirement decision is deliberate. You do not remove a test just because it has been passing for six months. You remove it if the vulnerability it tested for is no longer possible — because the architecture changed, because the attack surface was eliminated, or because a better defense made the exploit infeasible. If there is any doubt, the test stays.

Flaky adversarial tests are especially dangerous because they train the team to ignore failures. If a test fails intermittently for reasons unrelated to security, the team stops trusting it. When it fails for a real reason, they assume it is another false positive and merge anyway. Flaky tests must be fixed or removed. You do not tolerate flaky adversarial tests any more than you tolerate flaky functional tests.

Maintenance is not free. Budget 10-20% of your adversarial testing effort for test suite maintenance. This is not wasted time. A well-maintained suite is the difference between a regression framework you trust and a regression framework you ignore.

## Cross-Referencing with Section 18

Section 18 covers regression testing strategy, release gates, and the mechanics of building a regression suite that scales. Everything in Section 18 applies to adversarial regression testing. The principles are the same. The infrastructure is the same. The operational discipline is the same.

The key integration points are test execution, failure triage, and release blocking logic. Adversarial tests run in the same CI/CD pipelines as functional tests. Adversarial test failures are triaged using the same process as functional test failures. Adversarial test failures block releases using the same gate logic as functional test failures.

This integration is intentional. If adversarial testing is separate from functional testing, it becomes a second-class citizen. It runs less frequently. Failures are ignored more easily. Engineers do not see adversarial test results unless they go looking for them. But if adversarial testing is integrated into the same regression framework, it inherits the same priority, the same visibility, and the same enforcement.

When an adversarial test fails in CI, the engineer who triggered the failure sees it immediately. The failure is visible in the same dashboard where they see unit test results and integration test results. They cannot merge until the failure is resolved. This tight feedback loop is what makes adversarial regression effective. The engineer learns that their change introduced a vulnerability within minutes, not weeks.

If you have not yet built a regression testing framework, read Section 18 first. Build the framework. Then add adversarial test cases to it. If you already have a regression framework, the work is simpler. You extend the framework to include adversarial cases. The infrastructure is already there. You are just adding a new category of test.

## When Adversarial Tests Should Block Releases

Not every adversarial test failure should block a release. The blocking logic depends on severity and context. A critical vulnerability discovered in a golden set test blocks the release immediately. No exceptions. A low-severity finding in the broader suite might be logged, triaged, and scheduled for a future fix without blocking the current release.

The blocking decision is made by severity level, not by test category. A critical functional test failure blocks a release. A critical adversarial test failure blocks a release. The criteria are the same. This consistency is important because it prevents adversarial testing from being deprioritized. If adversarial tests are easier to bypass than functional tests, they will be bypassed.

You define the blocking policy in advance. The policy is documented. It is enforced by automation. You do not make case-by-case decisions about whether a failure is severe enough to block. The severity level determines the blocking behavior. If a critical test fails, the pipeline stops. If a low-severity test fails, the pipeline continues but creates a ticket for follow-up.

The only exception to automated blocking is when a test is known to be flaky or when the failure is clearly a false positive. In these cases, a human can override the block, but the override must be documented and reviewed. Overrides should be rare. If overrides become common, your test suite has a quality problem that needs to be fixed.

## Building the Regression Corpus

Your adversarial regression corpus is the cumulative record of every vulnerability you have discovered and defended against. It grows over time. It is version-controlled. It is treated as a critical asset.

Building the corpus starts with your first red team engagement. Every finding from that engagement becomes a test case. As you run subsequent engagements, you add new findings to the corpus. As you monitor production and discover exploits in the wild, you add those to the corpus. As new attack techniques are published in research, you add test cases that validate your defenses against those techniques.

The corpus is organized by attack category. Jailbreaks, prompt injections, data exfiltration, role confusion, adversarial inputs — each category has its own section. Within each category, test cases are tagged with metadata: severity, discovery date, remediation status, related vulnerabilities. This organization makes it easy to find relevant tests, audit coverage, and track trends.

You also maintain provenance. Each test case records where it came from. Was it discovered during a manual red team engagement? Found in production monitoring? Adapted from a published research paper? Provenance helps you understand the evolution of your threat landscape. If most of your high-severity findings come from manual red teaming, you know manual testing is finding things automation misses. If most come from production monitoring, you know your pre-release testing has gaps.

The regression corpus is living documentation. It tells the story of your adversarial robustness over time. It is the institutional memory of what has gone wrong and what you have done to fix it. When a new engineer joins the team, the corpus is one of the first things they review. It teaches them what attacks to defend against and what mistakes to avoid.

---

Next: how to organize and maintain the adversarial test suite as it scales.
