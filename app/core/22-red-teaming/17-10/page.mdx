# 17.10 — The Purple Team Operating Model

Most teams that call themselves purple teams are just red teams and blue teams who sit in the same meeting once a month. The red team presents findings. The blue team nods. Both sides return to their separate workflows, separate tools, separate priorities. Nothing actually changes in how attacks are discovered or how defenses are built. The label "purple" gets applied because someone read that collaboration is important, not because the organization restructured how offensive and defensive security work feeds into each other. A real purple team operating model is not a meeting cadence. It is an organizational design that ensures every attack discovered by the offensive side produces a measurable defensive improvement, and every defensive gap identified by the monitoring side generates a targeted offensive test. The operating model is the machinery that converts adversarial talent into production security outcomes. Without it, you have two good teams doing separate work. With it, you have a single system that gets measurably harder to attack with every cycle.

## Organizational Structure — Embedded, Centralized, and Hybrid

The first structural decision is where the purple team lives in the organization. Three models exist, each with trade-offs that depend on your company's size, the number of AI systems you operate, and how deeply AI security integrates with your existing security program.

The **embedded model** places purple team practitioners directly within product teams. Each AI product or platform has one or two people whose full-time job is adversarial testing and detection engineering for that specific system. They attend the product team's standups, know every prompt change and model update as it happens, and test continuously against the live system. The embedded model produces the fastest feedback loops — the gap between discovering a vulnerability and deploying a fix can be hours rather than weeks. The downside is skill isolation. An embedded practitioner develops deep expertise on one system but limited exposure to attack techniques relevant to other systems. They also face priority pressure from the product team, which naturally prioritizes feature delivery over security testing. Organizations with one to three AI systems benefit most from the embedded model because the practitioner can develop deep system knowledge without the isolation problem.

The **centralized model** creates a dedicated purple team that serves all AI systems across the organization. The team operates as an internal consultancy, scheduling testing engagements with each product team on a rotating basis. The centralized model produces broader skill development because practitioners test diverse systems and encounter diverse attack surfaces. It also maintains independence from product team priorities — the purple team's roadmap is driven by risk assessment, not feature deadlines. The downside is slower feedback loops. A centralized team serving six AI systems can test each one every six to eight weeks at best, which means vulnerabilities introduced between testing cycles may persist for weeks before discovery. Organizations with five or more AI systems and a dedicated security budget typically adopt the centralized model.

The **hybrid model** combines both: a small centralized team that maintains the methodology, tooling, and technique library, plus embedded practitioners within high-risk product teams who execute continuous testing. The centralized team trains the embedded practitioners, maintains the attack catalog, runs cross-system exercises, and handles the gap analysis and maturity tracking that require a system-wide view. The embedded practitioners handle continuous testing, immediate response to system changes, and rapid remediation cycles. The hybrid model requires more headcount than either pure model but produces both the speed of embedded and the breadth of centralized. Most organizations that have operated a purple team for more than two years evolve toward the hybrid model because they have experienced the limitations of both pure approaches.

## Cadence and Rhythm

An operating model without a defined cadence becomes an ad-hoc collection of activities that happen when someone remembers to schedule them. The cadence is the heartbeat of the purple team — it determines how frequently attacks are tested, defenses are validated, gaps are measured, and improvements are tracked.

**Daily standups** last ten to fifteen minutes and cover what each practitioner is currently testing, any findings from the previous day that need immediate attention, and any system changes (model updates, prompt revisions, tool integrations) that require ad-hoc testing. The standup is not a status meeting. It is a coordination mechanism that ensures the team responds to system changes within twenty-four hours rather than discovering them during the next scheduled engagement.

**Weekly syncs** between the red and blue functions last sixty to ninety minutes and serve as the primary collaboration point. The red team presents new attack techniques they have developed or discovered. The blue team presents new detections they have deployed and any detection failures they have observed. Together, they review the detection coverage map, prioritize the next set of detections to build, and plan the next week's testing focus. The weekly sync is where the purple team's collaborative advantage materializes — without it, the red and blue sides drift back into siloed operation within two to three weeks.

**Monthly exercises** are the structured engagement events — control validation workshops, targeted testing campaigns against specific attack chains, or CTF events that build team capability. Each monthly exercise has a defined scope, a specific hypothesis to test, and a documented output that feeds the gap register and detection engineering backlog. The monthly exercise is the purple team's primary production unit. Everything else supports it.

**Quarterly assessments** are the strategic review points. The team conducts a full gap analysis, measures coverage percentages against the attack technique inventory, reviews trend data on detection coverage and remediation velocity, and presents a security posture report to leadership. The quarterly assessment is where the team steps back from operational work and evaluates whether the overall trajectory is improving, flat, or degrading. It is also the point at which the team adjusts its roadmap for the next quarter based on changes in the threat landscape, business priorities, and resource availability.

## Tooling and Infrastructure

A purple team operates on a tooling stack that spans attack execution, defense monitoring, and the collaboration layer that connects them. The tooling choices determine operational efficiency — a team with the right tools spends 80 percent of its time on creative adversarial work and 20 percent on logistics. A team with inadequate tools inverts that ratio.

The **attack execution environment** needs to mirror production configuration while providing safety isolation. This means a staging environment with the same model versions, the same prompt architecture, the same tool integrations, and the same defense stack as production — but without access to real user data or the ability to affect real users. Maintaining this mirror is operationally expensive. Model updates must be synchronized. Prompt changes must be replicated. Tool configurations must be kept in alignment. Teams that let the staging environment drift from production discover that their testing results do not reproduce in the real system. Automate the synchronization. Treat the staging environment as a deployment target with the same CI/CD pipeline as production.

The **detection and monitoring platform** is the blue side's primary tool — typically a SIEM or security analytics platform that ingests telemetry from the AI system and evaluates detection rules. The platform needs to support correlation across multiple signal types (input classification, behavioral analysis, tool call monitoring, output inspection), configurable alerting thresholds, and the ability to replay historical data against new detection rules. By 2026, platforms like Microsoft Sentinel, Splunk, and Elastic have all added AI-specific telemetry ingestion and detection rule templates, though most teams still need substantial customization for their specific systems.

The **collaboration platform** tracks the shared artifacts that connect offensive and defensive work: the attack technique inventory, the detection coverage map, the gap register, the remediation backlog, and the findings from every exercise. Some teams build this on existing project management tools — Jira, Linear, or Notion — with custom workflows. Others use purpose-built security collaboration platforms. The specific tool matters less than the discipline of maintaining it. A gap register that is not updated within forty-eight hours of an exercise becomes stale. A detection coverage map that is not refreshed after each deployment becomes misleading. The collaboration platform only works if both sides treat it as the authoritative source of truth and maintain it with the same rigor they apply to their technical tools.

## Communication Protocols During Exercises

How the red and blue sides communicate during a live exercise determines whether the exercise produces collaborative learning or adversarial friction. Two communication models exist, and the choice depends on the exercise's objective.

**Blind exercises** restrict communication during the attack phase. The red team executes attack chains without informing the blue team of timing, technique, or target. The blue team operates purely from their monitoring dashboards, just as they would during a real incident. After the exercise window closes, both teams share their observations and compare notes. Blind exercises test the blue team's detection capability under realistic conditions — no hints, no advance warning, no coaching. They are stressful, sometimes demoralizing for the blue team when attacks succeed undetected, and extremely valuable for honest assessment of defensive readiness. Run blind exercises quarterly for high-fidelity posture assessment.

**Collaborative exercises** maintain open communication throughout. The red team announces each attack step as they execute it. The blue team watches their monitoring in real time and reports what they see or do not see. When a detection fails, both teams pause and analyze why. When a detection succeeds, both teams confirm that the alert provides enough context for effective response. Collaborative exercises are less realistic but far more educational. They build the shared understanding that makes the purple team more than the sum of its parts. Run collaborative exercises monthly as the primary skill-building mechanism.

During both exercise types, maintain a shared timeline log that records every significant event: red team actions with timestamps, blue team detections with timestamps, and any automated responses that triggered. This timeline is the exercise's primary forensic artifact. It reveals detection latency, response gaps, and the precise points where the defense chain failed. Without it, the post-exercise debrief relies on memory, which is unreliable and biased toward each side's perspective.

## Escalation Paths and Executive Engagement

Purple team findings need a clear path to executive attention when they reveal risks that exceed the team's remediation authority. A prompt injection that bypasses all defenses and exposes customer data is not a detection engineering ticket. It is an executive-level risk decision that may require system shutdown, customer notification, or regulatory disclosure.

Define three escalation tiers. **Tier one** handles findings that the purple team can remediate within its own authority — new detection rules, threshold adjustments, prompt hardening, tool configuration changes. These flow through the normal weekly sync and detection engineering backlog. **Tier two** handles findings that require cross-team action — model architecture changes, tool access revocations, or deployment rollbacks that affect product functionality. These escalate to the engineering leadership responsible for the affected system, with a forty-eight-hour response SLA. **Tier three** handles findings with immediate risk to users, data, or regulatory compliance. These escalate directly to the CISO or equivalent executive, with a four-hour response SLA and a predefined incident response playbook that includes legal counsel and communications.

Executive engagement beyond escalation follows a quarterly rhythm aligned with the assessment cycle. The purple team presents the security posture report — coverage percentages, gap trends, remediation velocity, and notable findings. The presentation must be calibrated for an audience that cares about risk, cost, and business impact, not technical detail. "We discovered that a multi-turn injection chain can extract customer PII from the support chatbot, bypassing all three detection layers. Remediation requires two weeks of detection engineering and one week of prompt architecture changes. Without remediation, the estimated annual risk exposure based on current attack attempts is between 150,000 and 400,000 dollars in potential data breach costs." That framing gets budget. A thirty-slide deck about detection rule configurations does not.

## Budget and Headcount Models

Purple team investment scales with the number of AI systems, their risk tier, and the organization's regulatory exposure. Underfunding produces security theater. Overfunding is rare but theoretically possible if headcount exceeds the volume of meaningful work.

For a startup or small company operating one to two AI systems with moderate risk exposure, the minimum viable purple team is two full-time practitioners — one with primarily offensive skills, one with primarily defensive skills, both capable of crossing over. Annual fully-loaded cost including tooling: 350,000 to 500,000 dollars. At this scale, the practitioners embed directly with the product team and handle the full purple team lifecycle themselves.

For a mid-size company operating three to eight AI systems with mixed risk tiers, the team grows to four to eight practitioners organized as a centralized team with engagement scheduling across systems. Add a team lead who manages the roadmap, stakeholder communication, and maturity tracking. Annual cost: 800,000 to 1.8 million dollars. At this scale, invest in dedicated staging infrastructure and a collaboration platform.

For an enterprise operating ten or more AI systems, including high-risk systems with regulatory exposure, the hybrid model typically requires ten to twenty practitioners — a centralized core of four to six senior practitioners plus embedded practitioners in each high-risk product team. Add management, tooling, and infrastructure. Annual cost: 2 to 5 million dollars. This sounds like a large investment until you compare it to the cost of a single data breach involving AI-processed customer data, which industry experience consistently places in the range of 4 to 10 million dollars for a mid-severity incident.

## Integration with Existing Security Operations

The purple team does not replace your existing security operations center, incident response team, or vulnerability management program. It extends them into the AI-specific domain. The integration points determine whether the purple team operates as a connected component of your security program or as an isolated island.

SOC integration means routing AI-specific alerts through the same alerting pipeline and triage process that handles all security alerts. The SOC analysts need training on AI-specific alert types — what a prompt injection detection looks like, what a tool abuse alert means, what the response playbook is for each alert type. Without this training, AI alerts arrive in the SOC queue and sit unprocessed because no analyst knows what to do with them.

Incident response integration means including AI-specific scenarios in the IR playbook library. When the SOC escalates an AI security incident, the responders need to know how to investigate — which logs to pull, which model behavior to examine, which stakeholders to notify. The purple team is the natural author of these playbooks because they have the deepest understanding of how AI attacks manifest and how to contain them.

Vulnerability management integration means feeding purple team findings into the same vulnerability tracking system that handles all security findings. This ensures AI vulnerabilities receive the same prioritization, assignment, and SLA tracking as infrastructure or application vulnerabilities. Without this integration, purple team findings live in a separate system that nobody outside the purple team monitors, and remediation stalls.

The operating model defines how the purple team works day to day. But effectiveness over time requires more than good process — it requires measurement. You need to know whether your security posture is actually improving, degrading, or standing still. The next subchapter covers the metrics that make security posture visible and the dashboards that keep leadership informed.