# Chapter 6 — Data Extraction and Privacy Attacks

Models remember more than they should. Training data, PII, system prompts, and internal reasoning can all be extracted by determined attackers. Privacy violations in LLM systems aren't hypothetical edge cases — they're active attack vectors with regulatory consequences. This chapter teaches how to test for memorization, probe for leakage, and build defenses that hold up under adversarial pressure.

---

- 6.1 — What Models Memorize: Training Data Leakage
- 6.2 — Extraction Attacks: Recovering Memorized Content
- 6.3 — Membership Inference: Determining What Was in Training Data
- 6.4 — PII Extraction: Getting Models to Reveal Personal Information
- 6.5 — System Prompt Extraction: Stealing Your Instructions
- 6.6 — RAG Content Extraction: Attacking the Knowledge Base
- 6.7 — Cross-User Data Leakage: When Contexts Bleed
- 6.8 — Multi-Tenant Isolation Testing: Session Collision and Cache Poisoning
- 6.9 — Tenant Boundary Penetration: Breaking Isolation in Shared Systems
- 6.10 — Testing for Data Leakage: Red Team Techniques
- 6.11 — Defending Against Extraction: What Works
- 6.12 — Privacy-Preserving Inference: Differential Privacy and Beyond
- 6.13 — Compliance Implications: GDPR, HIPAA, and Data Extraction

---

*Every token the model has seen is potentially extractable. The only question is how hard an attacker has to work.*
