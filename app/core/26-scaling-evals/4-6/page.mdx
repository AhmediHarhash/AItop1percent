# 4.6 — Progressive Rollouts with Eval Gates: From Canary to Full Deployment

The model passes staging evaluation. The release ticket is approved. The product team is eager, the stakeholders are waiting, and the temptation is to flip the switch and send the new model to every user at once. Instead, the team sends it to one percent of traffic — a canary. Real-time eval scores stream in. After two hours, scores are within normal bounds. The team expands to ten percent. After four hours, still stable. Then twenty-five percent, fifty percent, one hundred percent. Each step gated by evaluation evidence, not hope. No human had to stay awake all night watching dashboards. The system promoted itself because the numbers said it was safe to promote.

This is what production model deployment looks like in 2026 for teams that have learned the hard way. The alternative — deploying a new model to all traffic at once, then scrambling to roll back when quality degrades — is how organizations lose user trust in an afternoon. Progressive rollouts with eval gates transform deployment from a binary gamble into a controlled, evidence-based ramp that catches quality problems before they reach the full user base.

## What Progressive Rollout Actually Means

A **progressive rollout** is the practice of deploying a new model to an increasing percentage of production traffic in stages, with explicit quality checks between each stage. At stage one, only one to five percent of users see the new model's output. At stage two, ten to twenty percent. At stage three, fifty percent. At stage four, one hundred percent. The exact percentages and number of stages vary by organization, but the principle is constant: expand only when evidence supports expansion.

This is not a feature flag. Feature flags are manual toggles — someone decides to turn a feature on or off for a user segment based on judgment. Progressive rollouts with eval gates are automated. The system decides whether to expand based on quality metrics that are measured continuously during each stage. A feature flag asks "should we show this to more users?" and waits for a human to answer. An eval gate asks "does the quality data justify showing this to more users?" and answers itself.

The distinction matters because human decision-making at deployment time is slow, biased, and unavailable at three in the morning. The product manager who approved the release wants the new model to succeed. The engineer who built it wants the deployment to go smoothly. Both are inclined to interpret ambiguous quality signals as "good enough" and push forward. An eval gate has no ego and no deadline. It looks at the numbers and either opens or stays closed.

## The Canary Pattern

The **canary** is the first stage of a progressive rollout: a tiny slice of traffic that receives the new model while the vast majority continues on the current model. The name comes from the coal mining practice of sending a canary into the mine shaft to detect toxic gas before sending in the miners. Your canary traffic serves the same purpose. It detects quality problems before those problems reach your full user base.

The canary percentage is typically one to five percent of traffic, depending on your total volume. A system handling a million requests per day can run a one-percent canary and still accumulate ten thousand evaluation data points in a single day — more than enough to detect meaningful quality differences. A system handling ten thousand requests per day needs a larger canary percentage, perhaps five to ten percent, to accumulate sufficient data within a reasonable timeframe.

The canary must be a random, representative sample of your traffic. Sending the canary to a specific user segment — only internal users, only a particular geographic region, only a particular product tier — defeats the purpose. The point of a canary is to test the new model against real, representative traffic so that quality measurements during the canary period predict quality at full deployment. If the canary traffic is systematically different from full traffic, canary quality measurements are meaningless predictions of production quality.

One common mistake is treating the canary as a beta test for friendly users. This creates a false safety signal. Your most engaged users are often the most forgiving. They use shorter queries, provide more context, and tolerate imperfect responses because they understand the system is being updated. The average user does neither. Random selection avoids this trap.

## Eval Gates: The Quality Checkpoints

An **eval gate** is a specific, measurable quality threshold that must be met before traffic allocation increases from one stage to the next. It is the mechanism that makes progressive rollout evidence-based rather than calendar-based. Instead of expanding from five percent to twenty percent because "it's been two days and nobody complained," you expand because the gate criteria have been satisfied.

Effective eval gates check multiple dimensions simultaneously, because quality degradation rarely announces itself through a single metric. A well-designed gate for a production AI system typically includes four categories of criteria.

The first category is task quality. This is whatever your real-time eval scoring measures — the accuracy, helpfulness, safety, or domain-specific quality metrics that your LLM judges and heuristic scorers produce. The gate requires that the canary model's quality scores meet or exceed the current production model's scores by a defined margin, or at minimum fall within an acceptable tolerance band. "Canary model average quality score is within 0.1 points of production model" is a typical gate criterion. "Canary model safety pass rate is at or above ninety-nine percent" is another.

The second category is reliability. Latency, error rates, timeout rates, and throughput. A new model might produce excellent outputs but take three times as long to generate them, or fail on five percent of requests that the current model handles without issue. Reliability gates catch these problems before they scale. "P95 latency is within twenty percent of production baseline" and "error rate is below 0.5 percent" are standard reliability gates.

The third category is behavioral consistency. Even when quality scores look fine, the new model might behave differently in ways that affect user experience. It might be significantly more verbose, significantly less likely to refuse inappropriate requests, or significantly more likely to ask clarifying questions instead of answering directly. Behavioral gates monitor the distribution of output characteristics — response length, refusal rate, question-asking rate — and flag significant shifts.

The fourth category is safety signals. If your system includes content safety scoring, the gate should verify that the new model's safety violation rate is at or below the current model's rate. Safety gates are typically the strictest — any increase in safety violation rate, even within statistical noise, warrants investigation before expansion.

## The Timing Question

How long should you wait at each traffic percentage before expanding? This is the question that teams argue about most, and the answer depends on what you're trying to detect.

Short waits — two to four hours — are sufficient to detect catastrophic failures. If the new model is fundamentally broken, producing garbled output, failing on a common request type, or violating safety constraints at a high rate, these failures show up immediately. Two hours of canary traffic at one percent produces enough data to detect problems that affect more than one percent of requests.

Medium waits — twelve to twenty-four hours — catch problems that emerge over a daily cycle. User behavior changes throughout the day. Morning queries differ from evening queries. Weekday traffic differs from weekend traffic. A model that performs well during business hours might struggle with the different distribution of requests that arrive at night. Waiting a full daily cycle ensures that the canary has been tested against the full range of traffic patterns.

Long waits — two to seven days — catch slow-emerging issues. Some quality problems don't manifest immediately. A subtle bias in the new model might only become apparent after thousands of requests. A gradual drift in output quality might take days to accumulate to a detectable level. For high-stakes applications — healthcare, financial advice, legal — longer waits provide the safety margin that matches the consequences of a quality failure.

The practical pattern that most mature teams converge on is a variable-duration approach. At one to five percent, wait twelve to twenty-four hours. The cost of a problem at this scale is small, but you want enough time to see a full traffic cycle. At ten to twenty-five percent, wait four to twelve hours. You already validated on the daily cycle during the first stage, so the incremental risk of a new traffic pattern is low. At fifty percent, wait four to eight hours. At this point you have high statistical power and any remaining quality issue would likely have appeared at earlier stages. At one hundred percent, the rollout is complete, but monitoring continues for at least forty-eight hours with automatic rollback still armed.

Teams that rush this cadence learn why patience exists. A logistics company expanded their canary from five percent to one hundred percent in under six hours because initial quality scores looked excellent. Eighteen hours later, they discovered that the new model handled a specific class of multi-stop routing query incorrectly — a query type that represented only three percent of traffic but generated fourteen percent of revenue. The issue was undetectable at the five-percent stage because so few multi-stop queries landed in the canary window. A twenty-four-hour wait at the five-percent stage would have accumulated enough multi-stop queries to detect the regression before it scaled.

## Automatic Rollback

The eval gate's complement is automatic rollback. If quality drops below the gate threshold at any stage, the system reverts to the previous model without human intervention. This is not a recommendation. It is a requirement. If rollback requires a human to notice a problem, decide to roll back, and execute the rollback manually, the delay between quality degradation and remediation is measured in hours, not seconds. During those hours, every user routed to the new model receives degraded output.

Automatic rollback requires a clear definition of what triggers it. The trigger must be specific enough to fire on real problems and lenient enough to avoid false alarms. A common approach is to define two types of triggers: hard triggers and soft triggers.

**Hard triggers** fire on any single occurrence of a critical failure. A safety violation above a defined severity threshold. An error rate spike above five percent. A latency spike above three times the baseline P99. Hard triggers cause immediate, full rollback to the previous model. They don't wait for statistical significance because the consequences of even a few critical failures are too high.

**Soft triggers** fire on sustained quality degradation. Average quality score below the gate threshold for fifteen consecutive minutes. Safety pass rate below target for thirty consecutive minutes. Soft triggers use a time window to avoid reacting to momentary noise — a brief spike in bad outputs during a burst of unusual traffic, for example. The time window provides stability, but it should be short enough that the degradation doesn't persist for hours before the system reacts.

The rollback itself must be atomic and fast. This means your infrastructure must support instant traffic rerouting from the new model to the old model. If your deployment architecture requires a new container deployment or a model loading step to roll back, your rollback is too slow. The old model should remain loaded and ready to serve traffic throughout the progressive rollout period. This costs extra infrastructure — you're running two models simultaneously — but it makes rollback a configuration change rather than a deployment operation.

One frequently overlooked requirement: test your rollback. A rollback mechanism that has never been triggered is a rollback mechanism that might not work. Schedule periodic rollback drills — trigger a rollback intentionally during a staging deployment and verify that traffic shifts cleanly, latency doesn't spike, and no requests are dropped. Teams that skip this step discover their rollback is broken at the worst possible moment: during a real quality incident.

## Progressive Rollout vs Shadow Deployment

Progressive rollouts and shadow deployments both serve the purpose of validating a new model before it reaches all users, but they operate in fundamentally different ways, and understanding when to use each is critical.

A **shadow deployment** runs the new model on production traffic but discards the new model's outputs. Users always see the current model's response. The new model's response is captured, scored, and compared against the current model's response, but it never reaches a user. Shadows are zero-risk — the user experience is unchanged regardless of how the new model performs. They are ideal for initial validation: does the new model produce reasonable outputs on real traffic? Are there obvious failure modes?

A **progressive rollout** serves the new model's outputs to real users at controlled percentages. Users in the canary or expanded stages see real outputs from the new model. This means progressive rollouts carry real user-experience risk — a quality problem in the new model affects real users, even if the affected percentage is small.

The standard practice is to use both, in sequence. First, deploy the new model as a shadow alongside the current model. Run shadow evaluation for one to three days. Compare quality metrics. If the shadow shows the new model meeting or exceeding the current model's quality, proceed to a progressive rollout. The shadow phase catches catastrophic problems before any user sees them. The progressive rollout phase catches the subtler issues that only emerge when users actually interact with the new model's outputs — issues like user engagement differences, downstream conversion impacts, and behavioral shifts that affect the user experience in ways that pure quality scoring doesn't capture.

Teams that skip the shadow phase and go straight to a canary rollout are gambling that no catastrophic problems exist. Teams that stop at the shadow phase and skip the progressive rollout miss the issues that only show up during real user interaction. The two phases are complementary, not interchangeable.

## Building the Rollout Automation

The infrastructure for progressive rollout with eval gates consists of three connected components that must work together reliably.

The first component is the traffic router. This is the system that directs a configurable percentage of incoming requests to the new model and the remainder to the current model. Most modern serving frameworks support percentage-based traffic splitting natively — Kubernetes-based deployments with Istio or Linkerd, Amazon SageMaker's traffic shifting policies, and custom routing layers in front of model inference endpoints all support this. The router must support rapid reconfiguration — changing the traffic split should take seconds, not minutes — because both expansion and rollback depend on fast routing changes.

The second component is the eval aggregator. This is the system that collects real-time eval scores from both the canary model and the production model, computes aggregate metrics over configurable time windows, and compares those metrics against the gate thresholds. The aggregator must handle the statistical challenge of comparing quality distributions at different traffic volumes. At one-percent canary, the canary model might produce a few hundred scored outputs per hour while the production model produces tens of thousands. The aggregator must account for the higher variance in the canary metrics due to smaller sample size — otherwise, normal statistical fluctuation in a small sample will trigger false rollback alarms.

The third component is the rollout controller. This is the orchestration layer that ties the router and the aggregator together. The controller implements the rollout policy: start at one percent, wait twelve hours, check gate criteria via the aggregator, if gates pass then instruct the router to expand to ten percent, wait six hours, check again, continue until one hundred percent or rollback. The controller also implements the rollback policy: if the aggregator reports a gate failure, instruct the router to send all traffic back to the production model.

The controller's logic should be declarative, not imperative. You define the rollout stages, the wait times, the gate criteria, and the rollback triggers in configuration. The controller executes the plan. This makes rollout policies reviewable, versionable, and auditable — you can look at the configuration and understand exactly what will happen at each stage without reading code. It also means you can define different rollout policies for different risk levels: a conservative policy for high-stakes products with longer waits and stricter gates, and an aggressive policy for low-risk products with shorter waits and more lenient gates.

## Organizational Readiness

Progressive rollout with eval gates is a technical capability that requires organizational trust. The team must believe that the automated gates are correct, that the rollback mechanism works, and that the system will protect users from quality degradation without human oversight. Building this trust takes time and calibration.

Start with a fully manual progressive rollout. Deploy to one percent. Have a human review the eval scores after twelve hours. Have a human make the decision to expand. Do this for three or four deployments until the team understands the metrics, trusts the eval scores, and has calibrated the gate thresholds. Then automate the expansion decisions but keep manual rollback. Run this way for another few deployments. Finally, automate rollback as well. This graduated automation builds confidence at each step and prevents the organizational disaster of an automated rollback firing on a false alarm during the team's first experience with the system.

Document every rollout. Record the gate metrics at each stage, the decision to expand or hold, the total time from canary to full deployment, and any issues that arose. This historical data is invaluable for tuning gate thresholds. If every rollout clears the gates at the first check with wide margins, your gates may be too lenient. If one in three rollouts gets held at a stage and then clears after more data accumulates, your gates are probably well-calibrated. If rollouts routinely fail and require manual investigation, either the gate thresholds are too strict or your model development process is releasing undertested candidates.

The mature state is one where progressive rollout with eval gates is the default deployment path for every model change, no matter how small. A prompt template update goes through the same progressive rollout as a full model swap. A configuration change goes through the same gates. This sounds burdensome, but with automation it adds hours, not days, to the deployment timeline. And it catches the surprises that "small" changes sometimes produce. The prompt update that seemed trivial but caused a fifteen-percent increase in safety violations. The configuration change that seemed harmless but doubled latency for a specific query type. Progressive rollout with eval gates catches these before they scale, and the cost of the delay is always less than the cost of the incident.

Progressive rollouts generate something valuable beyond deployment safety: high-quality comparison data between your current and candidate models on real production traffic. This data — real queries, real outputs from both models, real eval scores — is precisely what your offline golden sets need to stay current. The next subchapter covers how to close this loop, using online evaluation results to refresh the offline test sets that anchor your entire evaluation system.
