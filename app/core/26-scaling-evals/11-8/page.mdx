# 11.8 — Multi-Team Eval Coordination: Shared Infrastructure with Local Autonomy

How do you maintain consistent quality standards across six product teams, each with different models, different use cases, and different definitions of "good enough," without either strangling local innovation or fragmenting into chaos? This is the coordination problem that every organization faces once AI evaluation moves from a single-team concern to a company-wide function. And it is a problem that infrastructure alone cannot solve.

A B2B software company learned this in mid-2025. They had grown from one AI-powered product to four in eighteen months. Each product team built its own evaluation system — its own golden sets, its own judge prompts, its own pipelines, its own dashboards. The customer support AI team used Langfuse. The document analysis team used Braintrust. The search team had custom Python scripts. The recommendation team used a homegrown system built on top of Weights and Biases. When the VP of Engineering asked a simple question — "what is the quality of our AI across the company?" — nobody could answer. The data was in four different formats, measured against four different standards, stored in four different systems. The company was not running a quality program. It was running four disconnected experiments.

## The Coordination Spectrum

Multi-team eval coordination sits on a spectrum with two failure modes at its extremes. At one end is total centralization: a single eval team defines all criteria, runs all evaluations, and controls all quality decisions for every product. This approach guarantees consistency but kills velocity. Product teams wait in queue for the central team to define eval criteria for their new feature. The central team becomes a bottleneck that slows every team in the organization. Worse, the central team lacks the domain expertise to write good eval criteria for products they did not build. The customer support AI team knows what a good support response looks like. The central eval team does not.

At the other end is total decentralization: every team builds its own evaluation system independently. This approach maximizes velocity but fragments quality. Each team measures quality differently. Cross-team comparisons are impossible. When a new regulation requires consistent safety evaluation across all products, each team implements it differently. When leadership asks for a company-wide quality report, someone spends two weeks manually assembling data from incompatible systems into a single spreadsheet.

The right answer sits between these extremes, and the specific position depends on your organization's size, risk profile, and product diversity. But the pattern that has emerged among organizations running AI evaluation at scale follows a consistent structure: centralize infrastructure, decentralize criteria.

## The Platform-as-Service Model

The model that works is the **eval platform team** — a central team that provides evaluation infrastructure as an internal service, consumed by product teams who configure it for their specific needs. The platform team owns the pipes. The product teams own the water.

The platform team builds and maintains the eval pipeline infrastructure: the job orchestration system that runs evaluations, the data store that holds results, the dashboard framework that displays metrics, the judge execution layer that routes eval jobs to the appropriate scoring system, the safety evaluation pipeline that enforces regulatory requirements. The platform team does not decide what "good" means for any product. It provides the machinery that product teams use to measure their own definitions of "good."

Product teams consume this infrastructure through a self-service interface. They define their own eval criteria — what quality dimensions to measure, what thresholds to set, what golden set examples to use, what judge prompts to deploy. They configure their own dashboards, tailored to the metrics their stakeholders care about. They run their own evaluations on their own schedules, triggered by their own CI/CD pipelines. The platform team provides the capability. The product teams exercise it.

This model works because it solves both failure modes simultaneously. Consistency comes from shared infrastructure: every team stores results in the same format, uses the same pipeline orchestration, produces reports with the same structure. Autonomy comes from local configuration: each team defines quality in terms that are meaningful for their product, sets thresholds that reflect their risk profile, and iterates on their eval criteria at their own pace.

## What to Standardize

Not everything should be left to product team discretion. Certain elements must be consistent across the organization, either for regulatory compliance, operational efficiency, or cross-team comparability. Getting the standardization boundary wrong in either direction creates problems.

**Data formats** must be standardized. Every eval result, regardless of which product it came from, must follow the same schema. This means consistent field names, consistent timestamp formats, consistent score ranges, consistent metadata structures. Without format standardization, cross-team analysis requires format conversion — a tedious, error-prone process that nobody wants to maintain. With format standardization, a single query can aggregate quality metrics across all products, enabling the company-wide quality view that leadership needs.

**Safety evaluation requirements** must be standardized. Every AI product in the organization must pass the same safety evaluation gates, regardless of which product team owns it. Safety is not a dimension where local autonomy is appropriate. A product team that decides its safety thresholds are less stringent because its use case is "lower risk" exposes the entire organization to regulatory and reputational risk. The central safety evaluation pipeline — the three-tier system from Subchapter 11.7 — runs identically across all products.

**Reporting structure** must be standardized. Every product team produces a weekly quality report in the same format, with the same dimensions, at the same cadence. This does not mean every team reports the same metrics — a customer support AI tracks resolution accuracy while a document analysis AI tracks extraction precision. But the report template, the section structure, and the delivery cadence are consistent so that a stakeholder reviewing quality across the organization does not need to learn a different reporting format for each product.

**Pipeline infrastructure** must be standardized. Every team uses the same orchestration layer, the same result storage, the same alerting framework. This is a technical standardization that prevents the platform team from maintaining six different pipeline integrations and ensures that improvements to the shared infrastructure benefit every team simultaneously.

## What to Leave Local

The counterpart to knowing what to standardize is knowing what to leave to product team discretion. Standardizing too much turns the platform team into a bottleneck and prevents product teams from adapting their evaluation to their specific domain.

**Quality criteria** should be local. What counts as a "good" response for a customer support chatbot — accurate, empathetic, concise, action-oriented — is different from what counts as "good" for a document summarization system — complete, accurate, well-structured, appropriately detailed. The product team, not the platform team, defines these criteria because the product team understands the user, the use case, and the failure modes.

**Pass thresholds** should be local, with a floor. Each product team sets the quality thresholds that gate their deployments. A team shipping a medical AI might require 95% accuracy before deploying. A team shipping an internal productivity tool might accept 85%. The platform team sets a floor — no product ships below a minimum safety threshold — but above that floor, the product team decides what "ready" means.

**Golden sets** should be local. The curated evaluation datasets that represent each product's core use cases and known failure modes are product-specific by definition. The platform team provides tooling for creating, versioning, and managing golden sets. The product team populates them with examples that reflect their domain.

**Judge prompts** should be local. The LLM-as-judge scoring prompts that evaluate output quality are expressions of each product team's quality criteria. They encode domain-specific definitions of accuracy, relevance, tone, and completeness. The platform team provides the judge execution infrastructure. The product team writes the judge prompts that run on it.

## The Cross-Team Eval Council

Infrastructure coordination alone is not sufficient. Teams also need a forum for sharing what they learn, aligning on evolving standards, and resolving conflicts that arise when one team's decisions affect another team's operations.

The **eval council** is a recurring meeting — monthly for most organizations, bi-weekly for fast-moving ones — where representatives from each product team and the platform team come together. The council has three functions. The first is knowledge sharing. When one team discovers a new failure mode, a useful judge prompt technique, or an effective golden set construction strategy, the council is where that knowledge spreads to other teams. The customer support team's discovery that their LLM judge overrates empathetic-sounding but factually incorrect responses might help the sales AI team avoid the same judge calibration mistake.

The second function is standards alignment. As the organization's AI products evolve, the boundary between what is standardized and what is local shifts. New regulations might require standardizing a dimension that was previously local. New products might introduce use cases that existing standards do not accommodate. The council is where these boundary adjustments are proposed, debated, and decided. No single team — including the platform team — unilaterally changes the standards.

The third function is conflict resolution. Shared infrastructure creates shared resource constraints. When two teams need conflicting changes to the pipeline orchestration layer, or when one team's heavy eval workload degrades pipeline performance for other teams, the council provides a forum for resolving these conflicts without escalation to senior leadership.

The council is not a governance body that approves every eval change. It is a coordination body that ensures teams are aligned enough to share infrastructure without being so tightly coupled that they cannot move independently. The distinction matters. If the council becomes an approval gate, product teams will route around it. If it remains a coordination forum, product teams will invest in it.

## Cross-Team Error Analysis

One of the highest-value coordination patterns is cross-team error analysis — the practice of systematically sharing eval failure data across product teams to identify patterns that no single team would discover alone.

Consider a company with a customer support AI and a sales enablement AI, both built on the same foundation model with different fine-tuning and prompting. The customer support team notices that their model struggles with queries that contain technical product specifications. The sales enablement team notices that their model produces inaccurate comparisons when users ask about competitor features. In isolation, each team treats these as independent problems with independent solutions. In a cross-team error analysis, someone notices that both failures share a root cause: the foundation model's handling of structured numerical data degrades when the context window contains more than a certain amount of dense technical content. One root cause, surfaced through one analysis session, fixes two product-level problems.

Cross-team error analysis works when teams share their flagged eval failures — the outputs that scored below threshold — in a common format that allows pattern detection across products. The platform team provides the tooling: a shared failure database, clustering algorithms that group similar failures, and a dashboard that highlights cross-product failure patterns. The product teams provide the data: their scrubbed eval failures, annotated with the quality dimension that was violated and the hypothesized root cause.

The cadence for cross-team error analysis is typically monthly. Each product team submits their top 10 to 20 most informative failures from the past month. The analysis session — led by a rotating chair from among the product teams — reviews the submitted failures, identifies cross-cutting patterns, and assigns follow-up investigations to the appropriate teams.

## Shared Adversarial Libraries

Another high-leverage coordination pattern is a shared adversarial input library — a curated collection of adversarial prompts, attack scenarios, and edge cases maintained by the safety evaluation program and available to every product team.

When the customer support team's red-teaming exercise discovers a jailbreak technique that bypasses the model's safety constraints, that technique is likely to work against every other product in the organization. Adding it to a shared adversarial library means every product team benefits from the discovery. The sales AI team runs the same adversarial inputs against their system and patches the vulnerability before any user discovers it. The document analysis team verifies that their system's different architecture makes it immune and documents why.

The shared adversarial library grows through three channels. Automated red-teaming contributes the bulk of the entries — the thousands of adversarial prompts generated and tested during comprehensive adversarial testing cycles. Manual red-teaming contributes the most creative and domain-specific entries — the attack scenarios that require human insight to devise. Production safety incidents contribute the most urgent entries — real adversarial inputs that real users submitted and that the safety screening layer missed.

The library is versioned and maintained by the central safety evaluation program. Product teams pull from the library when configuring their safety eval gates. When new entries are added, an alert notifies product teams that new adversarial tests are available. Product teams are expected to run new adversarial tests within one sprint of their addition to the library, and to report results back to the safety program.

## Measuring Coordination Effectiveness

Coordination is only valuable if it produces measurable outcomes. Three metrics tell you whether your multi-team eval coordination is working.

The first is time-to-detection convergence. When a quality problem exists, how long does it take for the affected team to detect it? As coordination matures, this time should decrease across all teams because teams benefit from each other's eval improvements. If one team's detection time is improving while another's is stagnating, the coordination is not spreading knowledge effectively.

The second is cross-team failure recurrence. When one team discovers and fixes a failure mode, does the same failure appear in another team's product? If the same failure mode independently affects three products because teams are not sharing findings, the coordination is failing at its most basic function. The rate of cross-team failure recurrence should decrease as the eval council, shared error analysis, and adversarial libraries mature.

The third is platform utilization rate. What percentage of each product team's eval activity runs through the shared platform versus through team-specific tooling? If teams are maintaining shadow eval systems alongside the platform — running their own scripts because the platform does not meet their needs — the platform is underserving its users. The utilization rate should trend toward 90% or higher for standard eval activities, with only genuinely novel evaluation types running outside the platform.

Coordination is the organizational layer that makes shared infrastructure effective. But coordination without governance is coordination without accountability. The next subchapter addresses the governance layer — the approval workflows, change controls, and convergence processes that protect the integrity of the evaluation system as it scales across teams.
