# 1.3 — The Eval Tax: What Evaluation Costs as a Percentage of Your AI Spend

Most teams treat evaluation as overhead — something you do because you should, funded from whatever budget is left after building the product. Elite teams treat it as insurance. They budget for it deliberately, track it as a ratio against production spend, and defend it in budget reviews with the same conviction they'd defend infrastructure or security spending. The difference in perspective is the difference between teams that discover quality problems from user complaints and teams that discover them from dashboards.

The uncomfortable reality is that evaluation costs money. Real money. Not just the engineering time to build eval pipelines, but ongoing operational spend: LLM judge calls, compute for batch processing, human review labor, storage for eval datasets, and the tooling that ties it all together. At startup scale, these costs are negligible. At growth scale, they become a line item. At enterprise scale, they become a budget category that finance questions every quarter. Understanding eval economics — what it costs, why it costs that much, and what it costs when you don't do it — is not optional for teams operating AI at scale.

## The Eval Spend Ratio

The most useful framework for thinking about evaluation economics is the **Eval Spend Ratio**: the total cost of evaluation divided by the total cost of AI production. AI production cost includes everything you spend to serve AI features to users: model API calls, inference compute, embedding generation, vector database operations, orchestration infrastructure. Eval cost includes everything you spend to measure whether those features are working: judge model calls, eval compute, human review, eval-specific tooling, and the engineering time allocated to maintaining eval infrastructure.

A healthy Eval Spend Ratio depends on your stage, your risk profile, and how much damage a quality failure would cause. Early-stage startups with a single AI feature and forgiving users typically spend five to ten percent of their AI production budget on evaluation. This covers basic sampling, a simple LLM judge pipeline, and occasional manual review. It's lean, but it's enough to catch gross failures and track quality trends at low volume.

Growth-stage companies with multiple use cases, enterprise customers, and increasing regulatory exposure typically need to spend ten to twenty percent. At this stage, you need stratified sampling across use cases, multiple judge rubrics, regular human calibration of automated judges, and dedicated engineering time to maintain eval infrastructure. The cost increase isn't linear with traffic — it reflects the variety and velocity scaling demands that come with growth.

Regulated enterprises — companies in healthcare, finance, legal, or any domain where AI outputs carry compliance obligations — typically spend twenty to forty percent of their AI production budget on evaluation. This sounds high, and it is. But when a hallucinated medical recommendation creates a liability event, or when a financial advisory output violates regulatory guidelines, the cost of inadequate evaluation dwarfs the cost of thorough evaluation. These organizations often run multiple evaluation tiers: automated judges for every output, statistical sampling for detailed analysis, and mandatory human review for high-risk interactions. Each tier adds cost, and each tier catches failures the previous tier missed.

If you're spending less than five percent, you almost certainly have blind spots in your quality measurement. If you're spending more than forty percent, you likely have inefficiencies in your eval pipeline — redundant evaluations, unoptimized judge models, or over-sampling in low-risk areas. The ratio isn't prescriptive, but it gives you a benchmark for asking "are we investing enough in knowing whether our AI works?"

## What's Inside the Eval Spend

Evaluation costs are spread across six categories, and most teams only think about two of them.

The first and often largest category is LLM judge API costs. By 2026, LLM-as-judge is the dominant evaluation method for open-ended AI outputs. You send the model's output to a judge model — often Claude Sonnet 4.5, GPT-5-mini, or Gemini 3 Flash — along with a rubric and reference material, and the judge produces a quality score. Each evaluation is an API call, and each call consumes tokens. A typical evaluation prompt with rubric, reference, output, and chain-of-thought reasoning from the judge runs two thousand to five thousand tokens per evaluation. At ten thousand evaluations per day using a mid-tier judge model, the daily cost ranges from fifteen to sixty dollars depending on the model and provider. That's four hundred fifty to eighteen hundred dollars per month — manageable for a single use case, but it multiplies with every rubric and every use case you add.

The second category is compute for batch evaluation. Not every evaluation uses an external LLM judge. Many teams run classifier-based evaluations, embedding similarity checks, regex-based safety filters, or custom scoring models. These run on your own infrastructure and consume GPU or CPU time. A batch eval pipeline that processes a hundred thousand outputs per day through three different scoring mechanisms requires dedicated compute that isn't free. Teams running on cloud infrastructure see this as a direct cost. Teams running on-premises see it as opportunity cost — those GPUs could be serving production traffic instead of scoring it.

The third category is human review labor. Even the most automated eval systems need human judgment for calibration, edge case review, and ground truth validation. Human reviewers cost fifteen to sixty-five dollars per hour depending on expertise level and domain. A financial compliance reviewer costs more than a general quality reviewer. A medical accuracy reviewer costs more than a customer support quality checker. If your eval system requires ten hours of human review per week — a modest amount for a growth-stage product — that's a minimum of seven hundred eighty dollars per month, likely more for specialized domains. Chapter 7 of this section covers human review scaling in detail, but the cost starts here.

The fourth category is tooling and platform costs. Whether you build your own eval platform or use a commercial one like Braintrust, Langfuse, or Patronus, there's a cost. Commercial platforms charge based on eval volume, typically ranging from a few hundred to several thousand dollars per month at growth scale. Building your own means engineering time — typically one to three dedicated engineers for eval infrastructure at a growth-stage company, which translates to a hundred fifty thousand to four hundred fifty thousand dollars per year in fully loaded engineering cost.

The fifth category is storage. Eval datasets, judge outputs, score histories, rubric versions, and audit logs all need to be stored. At high volume, this storage is non-trivial. A team running fifty thousand evaluations per day with full output logging generates roughly five to fifteen gigabytes of eval data per month. Over a year, that's sixty to a hundred and eighty gigabytes of eval artifacts that need to be retained for trend analysis, regulatory compliance, and audit readiness. The storage cost itself is modest — tens of dollars per month on major cloud providers — but the data management overhead is not.

The sixth category is the one teams forget: engineering time for maintenance. Eval systems require ongoing care. Rubrics need updating when the product changes. Judge prompts need recalibration when judge model versions change. Data pipelines break and need fixing. New use cases need new eval configurations. This maintenance work typically consumes ten to twenty percent of one engineer's time at a startup, and becomes one to two full-time roles at growth stage. It's the largest hidden cost in the eval budget because it doesn't show up as a line item. It shows up as slower feature development because the eval engineer is fixing the eval pipeline instead of building product.

## Why Eval Costs Grow Non-Linearly

If your AI traffic grows ten times, your eval costs don't grow ten times. They grow fifteen to twenty times, or more, unless you actively design for cost efficiency. Several factors drive this non-linear scaling.

The first factor is the variety multiplier. When traffic grows ten times, it's rarely the same traffic at ten times the volume. Growth usually means new use cases, new customer segments, new languages, new risk tiers. Each new variety requires its own evaluation criteria, and often its own judge prompts and reference datasets. A product that evaluated one use case at low volume might need to evaluate eight distinct use cases at high volume. The eval cost isn't just ten times more traffic through one pipeline — it's ten times more traffic through eight pipelines.

The second factor is the coverage floor. At low volume, you can sample liberally because every sample is a significant fraction of total traffic. At high volume, maintaining the same statistical confidence requires a larger absolute number of evaluations, even if the sampling rate drops. You might evaluate ten percent at low volume — a hundred out of a thousand. At high volume, one percent gives you ten thousand out of a million, which is more evals in absolute terms. The percentage drops, but the absolute count rises, and your infrastructure and API costs scale with absolute counts, not percentages.

The third factor is judge complexity growth. At low volume with one use case, your judge prompt is simple. At high volume with many use cases, your judge prompts become more nuanced. They need to handle more criteria, more edge cases, more domain-specific terminology. More complex prompts mean more tokens per evaluation, which means higher cost per eval even before you account for higher eval volume. A judge prompt that was eight hundred tokens at startup might grow to three thousand tokens at enterprise scale because the rubric is more comprehensive. That's nearly four times the per-eval cost, compounded with the volume increase.

The fourth factor is redundancy requirements. At scale, you can't rely on a single judge model. If your judge provider has an outage, your entire eval pipeline stops. Enterprise teams run redundant judges — a primary and a fallback — which doubles the infrastructure cost. Some teams run multiple judges per evaluation for consensus scoring, which triples or quadruples the judge cost per eval. This redundancy is necessary for reliability, but it's a cost multiplier that doesn't exist at startup scale.

## The Cost of Not Evaluating

The strongest argument for evaluation spending isn't the value of catching problems. It's the cost of not catching them. A Larridin study of enterprise AI deployments found that seventy-two percent of AI initiatives were actively destroying value through waste and poor governance. While evaluation isn't the only factor, a significant portion of that value destruction stems from quality degradation that went undetected — models producing worse outputs over time without anyone measuring the decline.

The cost of undetected quality degradation takes multiple forms. Customer churn is the most direct: users stop trusting the product and leave. A B2B SaaS company that loses a single enterprise customer to quality issues might lose five hundred thousand to several million dollars in annual recurring revenue — far more than the entire annual eval budget that would have caught the problem. Regulatory penalties are another form: a healthcare AI that produces non-compliant outputs without detection creates liability exposure that dwarfs any eval investment. Reputational damage is a third: the Air Canada chatbot lawsuit in 2024, where an AI hallucinated a bereavement fare policy that the company was then forced to honor, became a case study in what happens when AI outputs aren't systematically evaluated against ground truth.

The most insidious cost is opportunity cost. Teams that can't measure quality can't improve quality systematically. They can't run meaningful A/B tests between model versions because they can't measure which version is better. They can't optimize prompts because they can't quantify the impact of changes. They can't confidently ship improvements because they can't prove they're improvements. The organization becomes stuck — spending money on AI but unable to make it better because it lacks the measurement infrastructure to guide investment. Every dollar spent on AI production without evaluation is a dollar spent without knowing whether it's working.

## Controlling Eval Cost Without Sacrificing Coverage

The goal isn't to minimize eval cost. It's to maximize eval value per dollar spent. Several strategies achieve this without creating dangerous blind spots.

Sampling is the single most effective cost control. Chapter 2 of this section covers sampling strategies in depth, but the principle is straightforward: you don't need to evaluate every output to know your system's quality. A statistically valid sample of one to five percent of traffic, stratified by use case and risk tier, provides quality signal with a fraction of the cost of exhaustive evaluation. The key is that the sampling must be statistically rigorous — random, stratified, and large enough to detect meaningful quality changes. Ad hoc sampling, where someone grabs fifty outputs that look interesting, provides no statistical guarantee and creates false confidence.

Judge model optimization is the second lever. Not every evaluation needs your most expensive judge model. Low-risk, high-volume use cases can be evaluated by smaller, cheaper models — GPT-5-nano or Gemini 3 Flash — with the expensive judge reserved for high-risk interactions. This tiered judging approach can reduce judge API costs by fifty to seventy percent without meaningfully reducing eval quality, because the cheaper models are adequate for straightforward quality checks and only the ambiguous or high-stakes cases need the full reasoning capability of a frontier model.

Caching and deduplication reduce redundant evaluation. Many AI systems produce similar outputs for similar inputs. If your customer support bot gives the same answer to a common question a thousand times per day, you don't need to evaluate all thousand. Evaluate it once, cache the result, and apply it to duplicate or near-duplicate outputs. Effective caching can reduce eval volume by twenty to forty percent in products with repetitive interaction patterns, which translates directly to cost savings on judge calls and compute.

Tiered evaluation matches investment to risk. Not every output carries the same risk. A casual chatbot response carries less risk than a medical recommendation. A marketing copy suggestion carries less risk than a financial advisory output. Building risk tiers into your eval system — with higher sampling rates, more expensive judges, and mandatory human review for high-risk outputs, and lighter-touch automated checks for low-risk outputs — concentrates spending where the consequences of failure are highest. This isn't cutting corners. It's allocating resources proportionally to risk, which is how every mature engineering discipline operates.

## Budgeting for Eval: The Conversation with Finance

Eval spending becomes a budget conversation at growth stage. Finance will ask why you're spending ten or twenty percent of your AI budget on measurement instead of product. The answer requires translating eval economics into business language.

Frame eval spending as cost of quality, not overhead. Manufacturing companies spend five to twenty-five percent of revenue on quality assurance. Software companies spend fifteen to thirty percent of development budgets on testing. Evaluation is the AI equivalent of quality assurance and testing combined. It's not overhead. It's the mechanism that prevents your AI investment from losing value.

Quantify the downside. What does a quality failure cost? A single enterprise customer lost to quality issues. A regulatory penalty. A PR crisis. A product recall. Put a dollar figure on the worst-case scenario your eval system is designed to prevent, and compare it to the eval budget. The math almost always favors investment in evaluation, especially in regulated industries where a single compliance failure can cost more than years of eval spending.

Show the optimization trajectory. Eval costs don't just go up. As you build better infrastructure — smarter sampling, cheaper judges, effective caching, tiered evaluation — your cost per insight drops even as your coverage increases. The first year of eval scaling is expensive because you're building infrastructure. The second year is cheaper per unit of insight because the infrastructure is amortized and your strategies are optimized. Present a two-year cost trajectory, not just a current-quarter budget.

The Eval Spend Ratio gives you a language for this conversation. "We're at eight percent and need to move to fifteen percent because we've added three use cases in regulated domains." That's concrete. That's defensible. That's a ratio your CFO can benchmark against industry practice and track over time.

## From Cost to Strategy

Evaluation isn't free, and pretending it is leads to underfunding that creates the blind spots described in the previous subchapter. But evaluation doesn't have to be ruinously expensive either. The teams that manage eval economics well are the teams that treat it as a first-class budget line, optimize it deliberately, and tie every dollar of eval spend to a specific quality outcome it enables.

The Eval Spend Ratio is your compass. Track it quarterly. Benchmark it against your risk profile. Defend it in budget reviews. And remember that the most expensive evaluation system in the world is cheaper than the cost of shipping an AI product that you can't measure. Evaluation spend is not a tax on your AI investment. It is the thing that makes your AI investment worth making.

Most teams have some form of monitoring — dashboards that track latency, error rates, and uptime. Far fewer have evaluation — systematic measurement of whether the AI's outputs are actually good. The next subchapter dissects this gap, explains why monitoring and evaluation are fundamentally different disciplines, and shows what it takes to cross from one to the other.
