# 6.2 — Golden, Silver, and Super-Golden: The Multi-Tier Dataset Strategy

One golden dataset is not enough. At scale, you need at least three tiers of evaluation data, each serving a different purpose and maintained at a different level of rigor. The team that tries to use a single dataset for every evaluation purpose — from release gating to broad regression testing to statistical trend detection — will either keep the dataset small enough to maintain at high quality and lose statistical power, or grow it large enough for statistical power and lose the quality controls that make it trustworthy as ground truth. This is not a hypothetical tradeoff. It is the central tension that every evaluation team encounters when their product outgrows its first golden set.

The solution is tiering: different datasets for different purposes, with explicit quality standards, maintenance cadences, and ownership at each level. This subchapter introduces **The Three-Tier Dataset Strategy** — a framework that organizes evaluation data into super-golden, golden, and silver tiers based on rigor, size, and purpose. Understanding these tiers lets you allocate your quality investment where it matters most and scale your coverage without diluting your ground truth.

## The Super-Golden Tier: Your Source of Truth

The **super-golden dataset** is the smallest, most carefully maintained evaluation dataset your team owns. It contains fifty to two hundred examples. Every example has been hand-selected by domain experts, debated among stakeholders, and agreed upon as the definitive expected behavior for your system. If there's a disagreement about whether the model's output is correct, the super-golden example is the final word.

Creating a super-golden example is expensive and intentionally so. The example starts as a candidate — typically an input drawn from production or crafted by a domain expert to test a critical capability. A domain expert writes the expected output. A second expert reviews and challenges it. Ambiguities are resolved through discussion, not by one person's judgment. The final expected output represents the team's best understanding of what the system should produce, given the current product requirements, domain knowledge, and regulatory environment. This process might take thirty minutes per example. For fifty examples, that's twenty-five hours of expert time. For a dataset that gates release decisions worth millions of dollars, it's the cheapest insurance you'll buy.

The super-golden set is used for the highest-stakes evaluations: final release gates before a major deployment, model comparison finals when choosing between candidate models, and regression tests for the capabilities that matter most to your business. You don't run the super-golden set on every commit or every minor configuration change. You run it when the consequences of a wrong decision are severe — when you're about to ship a new model to production, when you're deciding between two fine-tuned variants, when you need to be absolutely certain that a change hasn't broken the things your most important customers depend on.

The maintenance cadence for the super-golden set is quarterly. Every three months, domain experts review every example. They check whether the expected outputs are still correct given current policy, regulation, and domain knowledge. They check whether the inputs still represent critical use cases given the product's current scope. They propose additions for capabilities that have become critical since the last review and removals for capabilities that are no longer relevant. The quarterly review is a significant event — it takes a full day of expert time and produces a versioned, approved update to the most important dataset in your evaluation infrastructure.

## The Golden Tier: The Workhorse

The **golden dataset** is the operational backbone of your evaluation system. It contains five hundred to five thousand examples — large enough for statistical confidence across multiple quality dimensions, small enough that a dedicated eval team can maintain every example at a high quality standard. This is the dataset you run against every candidate deployment. It's the dataset that appears on your weekly quality reports. It's the one your engineers check before merging a prompt change.

Golden set examples are curated by the eval team with domain expert review. The curation process is lighter than the super-golden tier — a single domain expert reviews each example rather than requiring multi-expert debate — but it's still rigorous. Every example has a verified expected output. Every example is tagged with metadata: the product feature it tests, the difficulty level, the quality dimensions it exercises, the date it was added, and its provenance — whether it came from production sampling, expert creation, or promotion from the silver tier.

The golden set is organized by quality dimension and product surface. You don't run all five thousand examples against every model change. You select the subset relevant to what changed. A prompt modification that affects summarization outputs gets evaluated against the summarization subset of the golden set. A model swap gets evaluated against the full golden set because a model change can affect any capability. This targeted evaluation keeps golden set runs fast enough to fit in a CI/CD pipeline while maintaining the coverage needed to catch regressions.

The maintenance cadence for the golden set is monthly. Each month, the eval team reviews recently added examples for quality, checks flagged examples against current ground truth, and integrates promoted examples from the silver tier. The monthly review is less formal than the super-golden quarterly — it takes the eval team half a day rather than a full day of domain expert time — but it is not optional. A golden set that misses its monthly review for two consecutive months should trigger an alert. Three consecutive months without review means the dataset's scores are no longer fully trustworthy.

## The Silver Tier: Coverage at Scale

The **silver dataset** is where volume lives. It contains five thousand to fifty thousand or more examples — large enough to detect subtle statistical trends, to provide coverage across hundreds of query categories, and to support the kind of broad regression testing that catches the long-tail failures a smaller dataset would miss.

Silver set examples come from three sources. The first is **production sampling**: real user interactions, sampled at a configured rate from production traffic, with outputs scored by automated evaluation and spot-checked by human reviewers. These examples represent what users actually ask and what the system actually produces. The second source is **synthetic generation**: LLM-generated inputs designed to cover edge cases, rare query types, and adversarial patterns that production sampling might miss. The third source is **automated labeling**: examples where the expected output was generated by a strong judge model or a rule-based system rather than a human expert. Automated labels are less reliable than expert labels, but they scale to thousands of examples at a fraction of the cost.

The quality standard for the silver tier is lower than for golden or super-golden — and that's by design. Not every silver example has been reviewed by a domain expert. Not every expected output is guaranteed correct. The silver set trades per-example accuracy for statistical breadth. A five percent error rate in ground truth labels across fifty thousand examples still produces aggregate quality metrics that are statistically meaningful, because the errors are distributed randomly rather than systematically biased. A five percent error rate in a two-hundred-example super-golden set would be catastrophic, because ten wrong ground truth answers could swing your release decision.

The silver set is updated continuously. New production samples flow in daily. Synthetic examples are generated when new edge cases or features need coverage. Automated labels are refreshed when the labeling model is updated. There's no formal monthly review of the entire silver set — at fifty thousand examples, a per-example review would be impractical. Instead, the silver set is maintained through statistical quality monitoring: tracking the agreement rate between silver set labels and golden set labels on overlapping examples, monitoring the distribution of silver set inputs against production traffic to ensure representativeness, and flagging silver examples where multiple evaluations produce inconsistent results.

## Why the Tiers Exist: The Rigor-Scale Tradeoff

You can't maintain fifty thousand examples at super-golden rigor. The expert review time alone would cost hundreds of thousands of dollars and take months. You can't detect subtle statistical trends with a hundred examples. A one-percentage-point quality regression requires thousands of evaluation samples to detect with confidence. Different evaluation purposes need different dataset sizes, and different sizes demand different quality levels. That's the fundamental reason the tiers exist.

The super-golden tier answers the question: "Is this model safe to deploy?" with the highest possible confidence on the most critical capabilities. The golden tier answers the question: "Has quality regressed on any measured dimension?" with enough statistical power to catch meaningful changes. The silver tier answers the question: "Are there emerging patterns, long-tail failures, or distribution shifts that the smaller datasets would miss?" with broad enough coverage to surface signals that fifty or even five thousand curated examples can't.

Each tier has a different tolerance for noise in the ground truth. Super-golden tolerates zero noise — every expected answer must be definitively correct. Golden tolerates minimal noise — a small number of debatable examples is acceptable as long as the aggregate signal is reliable. Silver tolerates moderate noise — the sheer volume of examples compensates for individual label inaccuracies, as long as the noise is random rather than systematic.

Trying to collapse these tiers into a single dataset forces you to make impossible compromises. A dataset maintained at super-golden rigor maxes out at a few hundred examples. A dataset large enough for trend detection can't receive expert review for every example. A dataset that's both large and rigorously maintained costs more to maintain than most teams can afford. Tiering is the admission that quality control is a finite resource, and the smart strategy is to allocate it where it has the most impact.

## How Examples Flow Between Tiers

The three tiers are not isolated silos. Examples flow upward through a promotion process that enriches the higher tiers with battle-tested data from below.

The upward flow starts in production. A real user interaction is logged and sampled into the silver tier. The automated evaluation scores it. A human reviewer spot-checks a sample of silver entries and validates the labels. If the example is particularly interesting — it represents a new edge case, a new failure mode, or a critical capability that the golden set lacks — the reviewer flags it for promotion. An eval team member reviews the flagged example, writes or verifies the expected output against the current quality standard, and adds it to the golden set. The golden set now has a new example that came from real production usage, with a human-verified ground truth.

The promotion from golden to super-golden is rarer and more deliberate. During the quarterly super-golden review, domain experts examine the golden set for examples that test genuinely critical capabilities — scenarios where a wrong answer has legal, financial, or safety consequences. They select the strongest candidates, subject them to the multi-expert review process, and if the example survives scrutiny, it enters the super-golden set.

Examples can also flow downward, though this is less common. When a super-golden example is retired because the capability it tests is no longer relevant, it may be demoted to the golden set as a historical regression check. When a golden example is flagged as having an ambiguous expected output, it may be demoted to the silver set until the ambiguity is resolved.

This bidirectional flow keeps the tiers connected. The silver set feeds realistic examples upward. The super-golden set's rigor standard trickles downward as reviewers internalize the quality bar and apply it during golden and silver reviews.

## When the Tiers Disagree

One of the most valuable signals in a multi-tier strategy is disagreement between tiers. When the silver set shows a quality regression but the golden set doesn't, or when the golden set passes but the super-golden set reveals a failure, the disagreement itself is diagnostic.

Silver regression without golden regression usually means the quality problem is in the long tail — categories, edge cases, or input types that the golden set doesn't cover. This is a coverage signal. The golden set needs updating to include the failing scenarios that the silver set caught. The regression is real, but the golden set's blind spot prevented it from showing up in the standard evaluation.

Golden regression without super-golden regression usually means the quality problem is in the moderate-difficulty range — capabilities that matter for routine operation but don't affect the critical scenarios the super-golden set tests. This is a severity signal. The regression is real, but it hasn't reached the critical capabilities yet. You may choose to deploy with a known golden-set regression if the super-golden set still passes — it depends on how much golden-set degradation your quality standards tolerate.

Super-golden regression is always a stop-the-line signal. If the most carefully curated, expert-reviewed examples show degradation, something fundamental has changed. This doesn't mean every super-golden failure blocks deployment — a single failing example on a one-hundred-fifty-example set might be within the noise margin. But a pattern of super-golden failures, or failures on the examples the team explicitly marked as the highest priority, demands investigation before deployment proceeds.

The disagreement pattern works because each tier plays a different role. Treating them as redundant measurements of the same thing misses the point. They measure different aspects of quality at different levels of rigor, and the relationship between their signals tells you more than any single tier's score alone.

## Organizational Investment at Each Tier

The three-tier strategy requires different organizational investment at each level. Understanding the cost and effort helps you budget and staff appropriately.

Super-golden maintenance requires senior domain experts. It requires their time for quarterly reviews, their judgment for resolving ambiguous cases, and their authority to define what the system must get right. For a typical team, the super-golden tier consumes about two full days of domain expert time per quarter — one day for the review itself, one day for follow-up on flagged examples and updates. This is non-negotiable time that domain experts must protect against other demands.

Golden maintenance requires the eval team's consistent attention. Monthly reviews, daily monitoring of golden set scores, integration of promoted examples, and coordination with domain experts for ground truth verification. A dedicated eval engineer should plan to spend twenty to thirty percent of their time on golden set maintenance at a mature organization. At smaller teams where the eval engineer wears multiple hats, golden set maintenance is the task most likely to be deferred — and the one most likely to cause problems when it is.

Silver maintenance is largely automated. Production sampling runs continuously. Synthetic generation runs on a scheduled cadence. Automated labeling scales with compute. The human investment is in building and maintaining the automation: the sampling logic, the synthetic generation pipelines, the label quality monitoring, and the statistical health checks that catch systematic problems. Building the silver tier infrastructure takes significant upfront engineering effort — typically two to four weeks for a first version — but once operational, the marginal cost of maintaining it is low.

## Getting Started: Building Tiers Incrementally

You don't need all three tiers on day one. Most teams build upward from a single golden set as scale demands it.

Start with a golden set of two hundred to five hundred examples. Maintain it at high quality with monthly reviews. Use it for regression testing and release gates. This single tier serves most early-stage products well.

When you start noticing coverage gaps — features without golden set representation, emerging failure modes the golden set misses — it's time to add the silver tier. Build a production sampling pipeline that feeds examples into a larger, less rigorously maintained dataset. Use the silver set for broad coverage testing and trend detection. Keep the golden set as your primary quality signal but use the silver set to find the blind spots.

When your product reaches the maturity stage where release decisions have significant financial or safety consequences — when a bad deployment costs real money or affects real people at scale — it's time to establish the super-golden tier. Extract the most critical examples from your golden set, subject them to the multi-expert review process, and create the smallest, most trusted dataset in your evaluation stack.

Each tier justifies itself through the failures it catches. The silver tier catches the long-tail regressions the golden set misses. The super-golden tier provides the confidence to make high-stakes deployment decisions. If a tier isn't catching failures the other tiers miss, it's not earning its maintenance cost. If it is, the maintenance cost is trivially cheap compared to the cost of the failures it prevents.

The tiers solve the rigor-scale tradeoff. But maintaining datasets at any tier requires tracking changes over time — knowing what changed, when, why, and by whom. The next subchapter covers dataset versioning at scale: why it differs fundamentally from code versioning, and what infrastructure you need to track the evolution of the data that judges your system.