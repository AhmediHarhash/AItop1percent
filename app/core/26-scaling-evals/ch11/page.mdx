# Chapter 11 — The Eval Operating Model: From Ad-Hoc Scripts to Continuous Quality Engineering

Infrastructure, tooling, and sampling strategies are the technical foundation. But evaluation at scale is ultimately an organizational problem — it requires cadences, ownership, governance, and a culture that treats evaluation not as a testing phase but as a continuous governing function. This chapter covers the operating model that makes scaled evaluation sustainable: the daily practices, weekly reviews, team structures, gaming defenses, compliance requirements, and maturity stages that separate teams running eval as a side project from teams running eval as a strategic control layer.

---

- **11.1** — Eval-Driven Development: Evaluation as a Continuous Governing Function
- **11.2** — The Eval Review Cadence: Daily Error Analysis, Weekly Reviews, Monthly Recalibration
- **11.3** — Eval Ownership Models: Centralized Platform Team vs Embedded Per-Product Evaluators
- **11.4** — The Error Analysis Practice: Twenty Outputs, Thirty Minutes, Every Sprint
- **11.5** — The Eval Gaming Problem: When Teams Optimize for Metrics Instead of Quality
- **11.6** — Eval Data Privacy and Compliance: PII Scrubbing, Retention Policies, and Access Control
- **11.7** — Scaling Safety Evaluation: Regulatory Cadences, Automated Red-Teaming, and Compliance Gates
- **11.8** — Multi-Team Eval Coordination: Shared Infrastructure with Local Autonomy
- **11.9** — Eval Governance at Scale: Approval Workflows, Change Control, and Red-Team Convergence
- **11.10** — The Eval Maturity Model: From Reactive to Predictive Quality Engineering

---

*This is the end of Section 26, but it is not the end of the scaling story. Section 27 takes the infrastructure lens wider — global deployment, Kubernetes orchestration, and the distributed systems engineering required to run AI at planetary scale.*
