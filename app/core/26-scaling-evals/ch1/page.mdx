# Chapter 1 — The Scaling Wall: Why Evaluation Breaks at Production Scale

Every eval system has a breaking point — a volume, a velocity, or a complexity threshold where the approach that worked beautifully at startup scale begins to crack. This chapter maps the terrain: what breaks, why it breaks, how much it costs when it does, and the maturity model that tells you where you stand and where you need to go.

---

- **1.1** — The Eval System That Worked at a Hundred and Collapsed at Ten Thousand
- **1.2** — The Four Scaling Dimensions: Volume, Variety, Velocity, and Veracity
- **1.3** — The Eval Tax: What Evaluation Costs as a Percentage of Your AI Spend
- **1.4** — The Eval Gap: Why Teams Monitor Everything But Evaluate Nothing
- **1.5** — The Eval Scaling Maturity Model: From Manual Spot-Checks to Autonomous Quality Systems
- **1.6** — When to Scale: The Volume Thresholds That Demand Eval Infrastructure
- **1.7** — The Eval Script vs the Eval Platform: When Code Becomes a System
- **1.8** — Eval Scaling Ownership: Who Builds, Operates, and Trusts the Platform

---

*With the scaling wall mapped, the next question is immediate and practical: when you cannot evaluate everything, how do you decide what to evaluate — and how do you ensure that what you skip does not quietly destroy quality?*
