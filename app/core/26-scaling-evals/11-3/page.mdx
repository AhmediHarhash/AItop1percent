# 11.3 — Eval Ownership Models: Centralized Platform Team vs Embedded Per-Product Evaluators

In early 2025, a B2B SaaS company with six AI-powered products decided to centralize all evaluation under a dedicated platform team. The reasoning was sound: one team, one platform, consistent methodology, shared cost. The platform team was staffed with three strong engineers who built excellent infrastructure — automated judge pipelines, golden set management, result dashboards, and a scheduling system that could handle thousands of evaluations per hour.

Within six months, the platform started falling behind. Product Team A needed a new quality dimension for regulatory compliance. Product Team B discovered a failure mode unique to their domain and wanted it added to the eval criteria. Product Team C needed their golden set restructured after a product pivot. Each request was reasonable. Together, they overwhelmed a three-person team that also had to maintain infrastructure, fix bugs, and keep the pipeline running. The backlog grew. Response times stretched from days to weeks.

Product teams started building workarounds. Informal eval scripts appeared in product repositories. One team set up a separate judge pipeline using their own API keys. Another team started doing manual reviews in a spreadsheet rather than waiting for the platform to support their new criteria. Within a year, the company had two parallel evaluation systems — one official, well-built, and increasingly outdated, and one unofficial, fragmented, but actually measuring what teams cared about. The centralized platform hadn't failed technically. It had failed organizationally.

## Three Models, Three Trade-offs

Section 1.8 introduced the three ownership models — centralized, embedded, and hybrid — and their fundamental trade-offs. This subchapter goes deeper into the operational reality of each: what staffing ratios work, what communication patterns prevent fragmentation, and how to avoid the "two systems" anti-pattern that the opening story illustrates.

The choice between ownership models is not a technical architecture decision. It is an organizational design decision that depends on how many AI products you operate, how specialized your evaluation requirements are, and how much your product teams' quality definitions diverge from each other.

## The Centralized Model in Practice

A centralized eval team owns everything: infrastructure, methodology, criteria design, golden set curation, judge calibration, and result interpretation. Product teams are consumers. They define what they want evaluated, and the platform team figures out how to evaluate it.

The centralized model works when two conditions hold. First, evaluation requirements across products are similar enough that a shared methodology covers at least seventy percent of each product team's needs. If all your AI products need accuracy, relevance, safety, and tone evaluation, and the differences are mostly in thresholds and golden set examples rather than fundamentally different quality dimensions, centralization is efficient. Second, the pace of product evolution is slow enough that the central team can keep up. If products ship major changes monthly rather than weekly, the central team has time to adapt criteria and golden sets between releases.

Staffing the centralized model is where most organizations underinvest. A common mistake is staffing the platform team for infrastructure only — enough engineers to build and run the pipeline, but nobody dedicated to evaluation methodology, judge calibration, or domain understanding. The result is a team that can run evaluations but can't design them. Product teams have to specify not just what they want evaluated but exactly how to evaluate it, reducing the platform team to an execution layer with no independent expertise.

A centralized team serving four to six product teams typically needs five to seven people: two to three infrastructure engineers for the pipeline, one to two evaluation methodology specialists who understand judge design and calibration, and one to two people who rotate across product teams to stay connected to domain context. Below that staffing level, the team becomes a bottleneck. Above it, the team often struggles to stay coordinated internally.

## The Embedded Model in Practice

Embedded eval engineers sit within product teams. They attend product meetings, understand the domain, and iterate on evaluation criteria as fast as the product evolves. The evaluation is always current because the evaluator is in the room when decisions are made.

The embedded model excels in organizations where evaluation requirements are highly specialized and diverge significantly across products. A healthcare AI product that needs clinical accuracy evaluation and a marketing copy AI that needs brand voice evaluation have so little overlap in their eval criteria that a centralized team would need deep expertise in both domains. Embedding evaluators solves this — each evaluator becomes a domain expert for their product.

The staffing question for embedded evaluation is how much dedicated capacity each product team needs. A product with stable evaluation requirements — criteria that change quarterly, a golden set that needs monthly refreshes, and automated judges that run without frequent recalibration — can be served by a half-time eval engineer. This is typically a product engineer who spends half their time on evaluation and half on feature work. A product with rapidly evolving evaluation requirements — new capabilities launching monthly, frequent model changes, high-stakes domain where quality criteria are complex and contested — needs a full-time dedicated eval engineer.

The failure mode you must actively prevent in the embedded model is isolation. When eval engineers are distributed across product teams with no connection to each other, three problems emerge.

Duplication: five product teams independently solve the same judge calibration problem, each spending a week on something that one team could have solved and shared. A mid-sized AI company tracked this in 2025 and found that embedded eval engineers across four teams spent a combined six weeks in one quarter solving problems that had already been solved by another team within the same company.

Inconsistency: each team defines "accuracy" differently, stores results in different formats, and uses different judge configurations. Cross-product quality comparisons become impossible. When an executive asks "which of our products has the highest quality?" the honest answer is "we can't tell you because each team measures quality differently."

Career stagnation: the embedded eval engineer has no peers, no community of practice, and no clear growth path. They are the "eval person" on a team full of product engineers. The best ones leave within eighteen months for organizations where evaluation is a recognized discipline. When they leave, the product team loses its entire evaluation capability because all the knowledge lived in one person's head.

## The Hybrid Model: Operational Detail

The hybrid model is where most organizations with three or more AI products converge, but "hybrid" is a vague label that covers a wide range of implementations. The operational details matter more than the label.

The hybrid model that works best in practice draws a clear boundary: the platform team owns everything below evaluation criteria, and product teams own everything at and above evaluation criteria. The platform team provides infrastructure — scheduling, storage, monitoring, cost tracking, judge execution, result aggregation, and a library of standard judge templates. Product teams configure evaluation criteria, maintain golden sets, set scoring thresholds, and interpret results.

The staffing ratio that scales is one platform eval engineer per three to four product teams for shared infrastructure, plus 0.5 to 1 dedicated eval engineer per product team for product-specific evaluation work. In concrete numbers: an organization with eight AI product teams needs two to three platform engineers and four to eight product-side eval champions. The platform engineers build and maintain shared infrastructure. The product-side eval champions — who may be full-time eval engineers or product engineers with dedicated eval responsibilities — configure the platform for their product's needs.

The boundary between platform and product responsibility must be explicit and documented. Without documentation, ambiguity accumulates. When a golden set needs updating, whose responsibility is it? When a judge starts producing inconsistent scores, who investigates — the platform team because it's a judge infrastructure issue, or the product team because the criteria may have drifted? The eval contract described in Section 1.8 formalizes these boundaries. In the hybrid model, the contract is not optional — it is the mechanism that prevents the model from collapsing into either pure centralization or pure embedding.

## Communication Patterns That Prevent Fragmentation

The hybrid model creates a distributed system of evaluation expertise. Like all distributed systems, it needs coordination mechanisms to stay coherent.

The most effective coordination mechanism is the eval community of practice — a regular gathering of all eval engineers across the organization, both platform and product-side. This is not a status meeting. It is a technical exchange where eval engineers share calibration techniques, discuss failure patterns, demonstrate tools they've built, and surface problems they haven't solved. Biweekly for thirty to forty-five minutes is the cadence that balances frequency with time investment. Monthly gatherings lose momentum. Weekly gatherings compete with too many other meetings.

The community of practice solves the isolation and duplication problems simultaneously. When a product-side eval engineer discovers that a particular judge prompt template produces more consistent scores for multi-turn conversations, they share it in the community and every other team benefits immediately. When the platform team plans infrastructure changes, they preview those changes in the community and get feedback from the people who will be affected. The community creates a shared context that would otherwise require individual conversations multiplied across every eval engineer in the organization.

A second coordination mechanism is the shared evaluation taxonomy. Every team should use the same vocabulary for error categories, quality dimensions, and severity levels. When Product Team A calls something a "factual error" and Product Team B calls the identical failure a "hallucination," cross-team communication breaks down and aggregate reporting becomes meaningless. The platform team should maintain the canonical taxonomy and ensure that all product teams map their evaluation criteria to shared definitions — even when their product-specific criteria go deeper than the shared definitions.

A third mechanism is the platform team's office hours. Dedicate two to three hours per week where product-side eval engineers can bring questions, request guidance, or get help with complex configurations. Office hours are more efficient than individual meetings because similar questions from different teams get answered once. They also build the platform team's understanding of what product teams need, which directly informs the platform roadmap.

## The Two-Systems Anti-Pattern and How to Kill It

**The Two-Systems Anti-Pattern** is the most common failure mode in eval ownership, and it always follows the same arc. A centralized or hybrid platform is built. It works well initially. As product teams' needs diverge or the platform team's response time increases, product teams start building supplementary evaluation outside the platform. These supplements are meant to be temporary. They become permanent. Within a year, the organization has two evaluation systems: the official one that produces the numbers leadership sees, and the unofficial one that produces the numbers teams actually trust.

The anti-pattern is deadly because it undermines both systems. The official system loses credibility with product teams, who know it doesn't measure what they care about. The unofficial system operates without governance, without monitoring, and without the reliability guarantees that the platform provides. The organization loses the ability to make consistent quality claims because different systems produce different numbers for the same product.

Prevention requires three practices. First, the platform must evolve at the speed of the fastest product team. This doesn't mean the platform team builds every feature every team wants. It means the platform provides extension points — the ability for product teams to add custom evaluation criteria, custom judges, and custom golden sets within the platform's infrastructure rather than outside it. The platform team should spend at least thirty percent of its capacity on extensibility and self-service features rather than custom work for individual teams.

Second, the platform team must monitor adoption. Track which product teams are using the platform, how frequently, and for what. When usage drops for a team, investigate immediately. The drop in usage almost always precedes the appearance of an unofficial eval system. Early intervention — understanding why the team stopped using the platform and addressing the gap — is dramatically cheaper than discovering two years later that half the company has abandoned the official evaluation system.

Third, make the platform the path of least resistance. If a product engineer can set up a custom eval script in thirty minutes but configuring the platform takes three days, the engineer will choose the script every time. Self-service configuration, templates for common evaluation patterns, and minimal bureaucracy for adding new eval criteria make the platform faster than the alternative, not just more principled.

## Escalation Paths for Eval Disagreements

In any ownership model, disagreements about evaluation arise. Product teams disagree with platform methodology. Different product teams define the same quality dimension differently. Eval scores don't match the product team's perception of quality. These disagreements are healthy — they indicate that people care about evaluation. But without a clear escalation path, they fester into resentment or get resolved by whoever shouts loudest.

The escalation path for eval disagreements has three levels. First, the eval champion and the platform team's methodology specialist discuss the disagreement directly. Most disagreements are resolved here — they stem from miscommunication about criteria definitions, misunderstanding of judge behavior, or differences in interpretation that can be reconciled through conversation.

Second, unresolved disagreements go to the weekly eval review (covered in 11.2) for the affected product team. The product manager provides the business context that helps determine which interpretation of quality is correct for this specific product.

Third, disagreements that affect multiple teams or involve fundamental methodology questions go to the monthly recalibration. This is where the organization makes strategic decisions about evaluation methodology — decisions that are too significant for a single product team to make unilaterally.

The worst outcome is no escalation path at all. Product teams that can't resolve eval disagreements either silently accept evaluation they don't trust — which means they stop using the platform as a decision tool — or build their own alternative, triggering the two-systems anti-pattern. Every ownership model needs an explicit, documented path from disagreement to resolution.

The ownership model determines who runs evaluation. The cadences determine when. But neither answers the most practical question in evaluation operations: what happens during a single error analysis session? What does a team actually do when they sit down with twenty outputs and thirty minutes? The next subchapter defines that practice in operational detail.
