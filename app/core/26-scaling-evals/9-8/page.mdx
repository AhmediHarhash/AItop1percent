# 9.8 — The Eval Observability Stack: Instrumentation, Storage, and Visualization

The eval observability stack has three layers: instrumentation that collects the data, storage that persists it efficiently, and visualization that makes it actionable. Each layer has specific requirements that differ from production application observability. Application observability answers "is the application healthy?" Eval observability answers "is the evaluation system healthy, and are the quality measurements it produces trustworthy?" This distinction matters because the design decisions for each layer — what to instrument, how to store it, how to display it — follow from the eval-specific requirements that general-purpose observability tools were not built to handle.

Teams that try to bolt eval observability onto their existing application monitoring stack inevitably hit friction. Application monitoring is optimized for request-level metrics — latency, error rates, throughput. Eval observability needs to track judgment-level data — scores with confidence intervals, judge model behavior, score distributions over time, and the relationship between evaluation results and the inputs that produced them. The data shapes are different, the query patterns are different, and the retention requirements are different. Getting the stack right from the beginning saves months of retrofitting.

## The Instrumentation Layer: What to Log

Eval instrumentation starts with a deceptively simple principle: log every eval result with its full context. In practice, this means every evaluation event should include the input that was evaluated, the output the model produced, every score assigned by every judge, the identity and version of each judge model, the confidence or probability associated with each score, the latency and cost of the evaluation, the timestamp, and a correlation identifier that links the evaluation to the original production request that triggered it.

This is substantially more data than application observability typically captures. A production request might generate a hundred bytes of metric data — latency, status code, endpoint. The corresponding eval event might generate ten kilobytes — the full input text, full output text, multiple judge scores with rationale, and metadata. At scale, this difference matters enormously for storage and cost.

The temptation is to log less. Only the scores. Only the aggregate pass/fail. Only the metadata without the actual input and output text. Every team that takes this shortcut regrets it within months, because the moment a quality investigation requires understanding why a specific test case failed, the team discovers that the context needed for diagnosis was never recorded. The solution is not to log less — it is to log everything at write time and manage the cost through intelligent retention and sampling, which the storage layer handles.

**Structured logging** is non-negotiable. Eval events should be emitted as structured records with consistent field names, consistent types, and consistent semantics across every eval pipeline in the organization. When one pipeline logs the judge score as "score" and another logs it as "quality_rating" and a third logs it as "judgment," building cross-pipeline dashboards and alerts becomes an exercise in field mapping rather than data analysis. Define a standard eval event schema and enforce it across all pipelines.

The schema should include a correlation identifier — typically a trace ID — that links the eval event to the production trace it originated from. If a user request generated a response, and that response was evaluated, the trace ID connecting the two is what enables the drill-down from "this eval dimension is degrading" to "here is the specific production request where the degradation occurred." Without trace correlation, eval data and production data exist in parallel universes, visible to each other only through aggregate metrics.

## Trace Correlation: Connecting Eval to Production

Trace correlation is the mechanism that makes eval observability actionable rather than merely informational. When an engineer sees that accuracy on the customer support bot dropped by 3% this morning, the next question is always "on which queries?" Trace correlation provides the answer by linking each eval result back to the production request that produced the output being evaluated.

The standard approach in 2026 uses OpenTelemetry-compatible trace propagation. The production system generates a trace ID for each request. When the output is sent to the eval pipeline, the trace ID travels with it. The eval pipeline emits its results tagged with the same trace ID. Downstream, any observability tool that ingests both production traces and eval events can join them on the trace ID, enabling queries like "show me all production requests from the last hour where the safety eval score was below 0.7" or "show me the eval scores for the requests that received negative user feedback."

Platforms like Langfuse, LangSmith, and Arize have built this trace correlation into their core architecture. Langfuse, for example, treats the production trace and the eval results as different spans within the same trace, allowing engineers to see the full lifecycle — user request, model response, evaluation scores, and any subsequent human review — in a single view. For teams building custom eval infrastructure, adopting the OpenTelemetry standard for trace propagation ensures compatibility with the broader observability ecosystem and avoids vendor lock-in.

## The Storage Layer: Time-Series, Documents, and Retention

Eval data has two distinct storage profiles, and most teams need both.

**Time-series data** captures metric values over time: accuracy scores, latency percentiles, cost per evaluation, coverage percentages, alert counts. This data powers trend dashboards, anomaly detection, and regression alerts. It is high volume but low cardinality — you have a finite number of metrics, each producing a new data point at a regular interval. Time-series databases like Prometheus, InfluxDB, VictoriaMetrics, or managed equivalents in cloud providers are the natural fit. They are optimized for the query patterns eval observability needs: "show me the accuracy trend for the last 30 days," "what was the 95th percentile eval latency last week," "how does the current cost per eval compare to the trailing average?"

**Document data** captures full eval records: the input, the output, the scores, the judge rationale, the metadata. This data powers diagnostic investigation — when an engineer needs to understand why a specific test case failed, they need the full record, not just the aggregate score. Document stores like Elasticsearch, MongoDB, or cloud-native equivalents handle this profile well. They support full-text search (find all eval records where the input contains a specific phrase), filtering (find all records where the safety score was below threshold), and aggregation (count records by score range, by eval dimension, by time period).

Trying to store both profiles in a single system creates problems. Time-series databases are not designed for full-text search over document content. Document stores are not optimized for high-frequency metric aggregation queries. The pragmatic approach is to emit eval data to both systems simultaneously: time-series metrics go to the metrics store, full eval records go to the document store, and both are linked by trace IDs so that a metric anomaly on the dashboard can be traced to the underlying eval records.

## Retention Policies: Balancing History and Cost

Eval data accumulates fast. A team running ten thousand evaluations per day, each generating a ten-kilobyte eval record, produces a hundred megabytes of document data per day — roughly three gigabytes per month. At enterprise scale with hundreds of thousands of daily evaluations across dozens of products, document storage reaches terabytes within months.

Retention policy design balances three needs. First, recent data must be available at full resolution for active debugging — the last 30 days of full eval records should be instantly queryable. Second, medium-term data supports trend analysis and seasonal comparison — the last 6 to 12 months of aggregate metrics should be available for dashboarding. Third, long-term data satisfies compliance and audit requirements — in regulated industries, eval records may need to be retained for years.

The standard approach uses tiered retention. Full eval records — input, output, scores, rationale, metadata — are stored in the document store for 30 to 90 days, depending on storage budget and regulatory requirements. After that, records are either archived to cold storage (cheaper but slower to query) or downsampled to retain only records that failed, records that were near threshold boundaries, and a random sample of passing records. Time-series metrics are retained at full resolution for 90 days, then downsampled to hourly or daily aggregates for up to two years.

The critical decision is which records to keep and which to discard during downsampling. Always retain failing records and near-threshold records, because these are the records you need for investigation when a currently-healthy metric starts declining and you want to understand historical failure patterns. A retention policy that discards all records older than 30 days creates an organization with no memory — when a quality problem recurs six months later, there is no historical data to compare against.

## The Visualization Layer: Dashboards, Comparisons, and Anomalies

The visualization layer transforms stored data into the dashboards described in the previous subchapter — engineering, product, and leadership views with progressive disclosure. But beyond dashboards, the visualization layer needs to support three specific eval-centric views that general-purpose dashboarding tools do not provide out of the box.

**Score distribution visualization** shows not just the mean or median score for an eval dimension, but the full distribution — how many evaluations scored in each range, where the distribution peaks, and how the distribution shape has changed over time. A mean accuracy of 87% looks identical whether it comes from a tight distribution (most scores between 85% and 89%) or a bimodal distribution (half the scores at 95% and half at 79%). The distribution shape reveals problems that aggregate metrics hide.

**Model comparison views** show two or more model versions side by side across all eval dimensions. When the team is deciding whether to ship a new model version, they need to see which dimensions improved, which degraded, and which were unchanged — ideally with statistical significance indicators showing whether the differences are larger than the measurement noise described in the previous subchapter. The comparison view should support filtering by test case category, difficulty level, and other metadata so that the team can understand not just "did quality change?" but "where did quality change?"

**Anomaly visualization** goes beyond simple threshold alerts to show detected anomalies in their metric context. When the anomaly detection system identifies an unusual pattern — a sudden shift in score distribution, a gradual drift in mean score, a change in the relationship between two correlated metrics — the visualization should show the anomaly highlighted against the metric's historical behavior. This context helps the engineer assess whether the anomaly is worth investigating or whether it falls within the metric's normal variation.

## Integration With Existing Observability Stacks

Most organizations already have an observability stack for production systems — Datadog, Grafana, New Relic, or cloud-native equivalents. The eval observability stack must integrate with these existing systems rather than existing in isolation.

Integration serves two purposes. First, it reduces the number of tools the team needs to learn and monitor. An engineer who already watches production health in Datadog should be able to see eval health in the same interface, not in a separate application they have to remember to check. Second, it enables correlation between production metrics and eval metrics — when response latency spikes in production and eval scores drop simultaneously, seeing both in the same tool makes the connection visible immediately.

Datadog released dedicated LLM observability capabilities in 2024-2025, including trace visualization, token accounting, and quality metric tracking. Grafana's ecosystem supports custom eval dashboards through its flexible data source model — you can connect a time-series database for eval metrics and a document store for eval records, building unified dashboards that show both. Arize, Langfuse, and LangSmith provide specialized eval observability with built-in integration points for exporting data to general-purpose observability platforms.

The integration architecture typically follows one of two patterns. In the embedded pattern, eval data is sent directly to the existing observability platform, and eval dashboards are built within that platform using its native tools. This works well when the observability platform supports the query patterns eval data needs — time-series queries, document search, distribution visualization. In the federated pattern, eval data stays in a specialized eval observability tool, and summary metrics are exported to the general observability platform. The engineer uses the general platform for high-level health monitoring and drills into the specialized tool for eval-specific investigation.

## Build Versus Integrate: The Decision Framework

Teams face a recurring decision: build custom eval observability tooling, adopt a specialized eval platform, or extend the existing observability stack. The right answer depends on the team's scale, requirements, and existing infrastructure.

At small scale — a single product with a handful of eval dimensions — extending the existing observability stack is usually sufficient. Export eval scores as custom metrics to Datadog or Grafana, build a simple dashboard, set threshold alerts. The overhead of adopting a specialized platform is not justified when the eval footprint is small.

At medium scale — multiple products with dozens of eval dimensions and several judge models — specialized eval platforms earn their cost. Langfuse, LangSmith, Arize, and Braintrust provide purpose-built eval observability with trace correlation, score distribution tracking, model comparison views, and team collaboration features that would take months to build from scratch. The integration cost is justified by the time saved on building and maintaining custom tooling.

At large scale — enterprise-wide eval infrastructure with hundreds of eval dimensions, custom judge models, regulatory retention requirements, and multiple teams consuming eval data — most organizations end up with a hybrid approach. A specialized eval platform handles the eval-specific capabilities, the general observability stack provides the unified monitoring experience, and custom tooling fills gaps that neither commercial option addresses. The build-versus-integrate decision becomes a continuous allocation problem rather than a one-time choice.

Regardless of the approach, the observability stack should be treated as a product with its own quality requirements, not as an afterthought bolted onto the eval pipeline. The observability system that monitors your eval system is itself a system that can fail, degrade, and produce misleading results. The teams that build the most reliable eval observability are the ones that apply the same engineering discipline to their observability infrastructure that they apply to the eval pipeline it monitors.

The eval observability stack provides the eyes and ears for your evaluation system. But the tools and platforms you choose to build that system — the eval tooling landscape itself — have consolidated significantly by 2026, with distinct tiers of capability and cost. The next chapter maps that landscape and helps you choose the right tools for your scale.
