# 3.1 — The LLM Judge as Your Largest Eval Cost Center

Most teams discover too late that their evaluation system costs more than their production system. The culprit is almost always the same: LLM judge calls that were cheap at prototype scale and ruinous at production volume. During development, nobody blinks at four cents per evaluation. At a hundred evaluations per day, it costs four dollars. At a thousand, forty dollars. The numbers feel harmless because the scale is harmless. Then the product launches, traffic grows, and the team wakes up to a monthly bill where evaluation spend exceeds the cost of actually serving users. The reaction is always the same — shock, followed by an emergency scramble to cut eval coverage, followed by months of operating blind because the team gutted its quality signal to survive financially.

This pattern is so common that it deserves a name and a framework. Understanding why LLM judges become your largest cost center — and instrumenting your system to see it coming — is the difference between teams that scale evaluation deliberately and teams that scale evaluation by accident, then retreat.

## Why Judges Cost More Than You Think

An LLM judge call is not a lightweight operation. It is a full inference request, often more expensive than the production call it evaluates. To understand why, consider what a judge actually consumes.

A typical production call in a customer support system might involve a five-hundred-token prompt and a three-hundred-token response. That's eight hundred tokens total. The judge call that evaluates this output needs to read the original prompt (five hundred tokens), read the production response (three hundred tokens), read the evaluation rubric (two hundred to eight hundred tokens depending on complexity), and generate a structured judgment with reasoning (two hundred to six hundred output tokens). The total for the judge call is twelve hundred to twenty-two hundred tokens — roughly twice the token count of the production call it's evaluating.

The cost implications compound when you account for model selection. Most teams use a stronger model as their judge than they use in production, because the judge needs to reliably assess quality in outputs from the production model. If your production system runs on GPT-5-mini or Claude Sonnet 4.5 at two to three cents per thousand tokens, but your judge runs on GPT-5 or Claude Opus 4.6 at five to fifteen cents per thousand tokens, the per-call cost gap is three to five times before you even account for the higher token count. A production call that costs $0.01 might be evaluated by a judge call that costs $0.04 to $0.06. The judge is four to six times more expensive per interaction than the system it's judging.

This asymmetry is invisible at small scale because both costs are trivially small. It becomes the dominant cost driver at production scale because evaluation costs scale linearly with coverage while production costs often benefit from caching, batching, and other efficiencies that judges cannot exploit.

## The Multi-Dimension Multiplier

The cost problem becomes dramatically worse when teams evaluate multiple quality dimensions — and most serious evaluation systems do. A single judge call that scores only "overall quality" is crude. Production evaluation typically needs to assess accuracy, tone, safety, completeness, and sometimes domain-specific criteria like regulatory compliance or citation quality. Each dimension requires its own evaluation.

Some teams attempt to consolidate dimensions into a single judge call: "evaluate this response on accuracy, tone, safety, and completeness, and return scores for each." This works at small scale but degrades at large scale. Multi-criteria judge calls produce longer prompts, consume more output tokens, and — critically — exhibit lower reliability per dimension than single-criterion calls. A judge asked to score four dimensions simultaneously is more likely to conflate them, produce inconsistent rubric application, or rush its reasoning on later dimensions because the context window is already cluttered with earlier assessments.

The result is that mature evaluation systems split multi-dimensional evaluation into separate judge calls. Accuracy gets one call. Tone gets another. Safety gets a third. Each call gets a focused rubric and produces a single, reliable score. But this means three judge calls per evaluated output. If each call costs four cents, a three-dimension evaluation costs twelve cents per output. A five-dimension evaluation costs twenty cents. This is **the eval cost multiplier** in its most aggressive form: the number of quality dimensions, times the per-judge-call cost, times the evaluation coverage rate.

Consider the arithmetic for a system processing a hundred thousand requests per day. If you evaluate ten percent of traffic across five quality dimensions with a judge that costs four cents per call, the daily evaluation cost is: a hundred thousand times ten percent times five dimensions times four cents. That's twenty thousand dollars per day. Six hundred thousand per month. For evaluation alone. Many teams' entire AI infrastructure budget — production inference, embeddings, storage, compute — is less than this.

## The Hidden Budget: Why Teams Don't See It Coming

The reason evaluation costs blindside teams is that they're almost always commingled with production costs in billing and accounting. Your LLM API bill shows total token consumption. It doesn't distinguish between tokens consumed by production inference and tokens consumed by evaluation inference. Your cloud GPU bill shows total compute hours. It doesn't distinguish between production serving and judge model serving. The evaluation system silently consumes an increasing share of a growing bill, and nobody notices because the total bill is growing for many reasons simultaneously — more users, more features, more model calls per interaction.

The first symptom is usually indirect. Someone asks why the AI infrastructure bill doubled when traffic only grew by fifty percent. An investigation reveals that the evaluation pipeline, which was added three months ago and configured for twenty percent coverage across four dimensions, is consuming forty percent of total API spend. The investigation takes a week. The remediation takes a month. The quality gap during remediation — the period when eval coverage is slashed while the team redesigns — lasts six to twelve weeks.

A logistics company experienced exactly this in mid-2025. Its production system routed delivery queries through Claude Sonnet 4.5 at roughly $0.008 per request. Its evaluation pipeline, added four months earlier, ran every tenth request through a Claude Opus 4.5 judge across three quality dimensions — accuracy of delivery estimates, tone of customer communication, and safety of address handling. Each judge call cost roughly $0.05. At ten percent coverage with three dimensions, evaluation cost $0.015 per production request — nearly twice the production cost per request. When traffic tripled after a partnership launch, the monthly eval bill jumped from nine thousand dollars to twenty-seven thousand. The total AI bill jumped from forty-two thousand to ninety-six thousand. The VP of Engineering demanded a breakdown. When the team finally separated eval costs from production costs, it discovered that evaluation was fifty-six percent of the increase. They had been scaling a cost center they didn't know they had.

## Instrument Eval Costs Separately — Day One

The fix is not complicated, but it requires deliberate action before the problem materializes. **Eval cost instrumentation** means tagging every LLM API call with its purpose — production serving, evaluation, development, experimentation — and tracking cost by purpose in your observability system. Every major LLM gateway and observability platform in 2026, including Langfuse, Helicone, and LangSmith, supports this tagging. The implementation is straightforward: add a metadata field to each API call that identifies whether it's a production call or an eval call, and which eval pipeline generated it.

Once instrumented, you should build three visibility layers. The first is a real-time cost dashboard that shows eval spend and production spend side by side, updated hourly. This dashboard is your early warning system. When eval spend starts climbing faster than production spend, you see it immediately rather than discovering it six weeks later in a billing investigation.

The second is a cost-per-evaluation metric that breaks down the all-in cost of evaluating a single output: judge model cost, any preprocessing or postprocessing compute, storage for evaluation results, and amortized infrastructure overhead. This metric tells you the true unit cost of your evaluation, which you need to calculate the total cost of any coverage rate change.

The third is a cost-per-quality-dimension breakdown. If you're running multi-dimension evaluation, you need to know what each dimension costs. It's common to discover that one dimension — often safety evaluation, which requires longer rubrics and more detailed reasoning — costs three times as much as another dimension. Armed with this visibility, you can make informed decisions about which dimensions to evaluate at what coverage rate, rather than applying a uniform rate across all dimensions.

## The Eval Cost Multiplier Framework

The cost of your evaluation system can be expressed as a single compound metric: **the Eval Cost Multiplier**. This is the ratio of your total evaluation cost to your total production inference cost, and it tells you how much you're spending on judging quality relative to how much you're spending on serving users.

The formula is intuitive: take the per-eval cost (the cost of one judge call), multiply by the number of quality dimensions per evaluation, multiply by the coverage rate (the fraction of production traffic you evaluate), and divide by the per-request production cost. The result is a multiplier. If your Eval Cost Multiplier is 0.2, you're spending twenty cents on evaluation for every dollar on production inference. If it's 1.0, you're spending dollar-for-dollar. If it's 2.0, evaluation costs twice what production does.

Most teams discover their multiplier is between 0.15 and 0.60 once they actually measure it. The teams at the lower end are typically running single-dimension evaluation with open-weight judges at low coverage rates. The teams at the upper end are running multi-dimension evaluation with frontier judges at moderate to high coverage rates. Neither end is inherently right or wrong — the appropriate multiplier depends on the value of quality information to your business, the risk tolerance for undetected quality failures, and the maturity of your evaluation program.

What is always wrong is not knowing your multiplier. A team that consciously chooses a 0.4 multiplier because quality information is critical to their regulated healthcare product is making a rational investment. A team that accidentally ends up at 0.6 because nobody tracked eval costs is wasting money. The multiplier framework turns an invisible cost into a visible, manageable, optimizable number.

## The Four Levers of Eval Cost Optimization

Once you can see your evaluation costs, you have exactly four levers to control them. Each lever has different trade-offs, and most teams need to pull more than one.

The first lever is **coverage rate**. Evaluating five percent of traffic costs half as much as evaluating ten percent. This is the bluntest lever and the one teams reach for first. Chapter 2 covered the mathematics of sampling: you can maintain strong quality signal at surprisingly low coverage rates if you sample intelligently. Reducing coverage is the fastest way to cut costs but carries the risk of missing localized quality issues, especially in low-volume segments.

The second lever is **judge model cost**. Switching from a frontier judge at five cents per eval to an open-weight self-hosted judge at half a cent per eval reduces per-eval cost by ninety percent. This is the highest-leverage single change and the subject of the next subchapter. The trade-off is potential accuracy loss, which must be measured and managed through calibration.

The third lever is **dimension count**. Evaluating three quality dimensions costs three times as much as evaluating one. The question is which dimensions deliver enough quality signal to justify their cost. Many teams find that one or two dimensions — typically accuracy and safety — provide eighty percent of the actionable signal, and additional dimensions add cost faster than they add insight. The discipline is to measure the marginal value of each dimension, not to assume that more dimensions always mean better evaluation.

The fourth lever is **prompt efficiency**. A judge prompt with two thousand tokens of rubric instructions costs twice as much in input tokens as a prompt with a thousand tokens. Prompt optimization — reducing the token count of your evaluation rubric without reducing agreement with human judgment — can cut per-eval cost by thirty to fifty percent. This is a subtler optimization and the subject of subchapter 3.3.

These four levers interact. Switching to a cheaper judge model might require a more detailed prompt to maintain accuracy, partially offsetting the per-call savings. Reducing coverage rate might justify using a more expensive but more reliable judge for the remaining evaluations, because the per-eval budget is now higher. Reducing dimension count might enable higher coverage rate at the same total cost, trading depth for breadth. Optimizing evaluation costs is a multi-dimensional problem that requires understanding all four levers simultaneously.

## From Cost Center to Investment

The goal of eval cost management is not to minimize evaluation spending. It is to maximize the quality information you get per dollar spent. A team that spends fifty thousand per month on evaluation and catches every quality issue before it reaches users is making a better investment than a team that spends five thousand per month and misses the regression that costs two hundred thousand in customer churn.

The Eval Cost Multiplier framework, combined with separated cost instrumentation, transforms evaluation from an invisible overhead into a visible investment with measurable returns. When you can say "we spent forty thousand on evaluation this month and caught seven quality regressions before they impacted users, with an estimated prevented revenue loss of three hundred thousand," evaluation stops being a cost center and starts being an insurance policy. The premium is high, but the payout justifies it — if you manage the premium deliberately rather than letting it grow unchecked.

The single highest-leverage decision in managing that premium is which model runs your evaluations. The difference between a frontier judge and an open-weight alternative is not a marginal optimization — it is often a ten-times cost reduction with a quality trade-off that is smaller than most teams expect. The next subchapter breaks down that decision in detail.
