# 2.2 — Random Sampling: The Baseline That Most Teams Never Outgrow

Random sampling is the most underrated technique in evaluation at scale. It is also the most dangerous when used as your only technique. This tension — between the genuine power of randomness and its blind spots — defines the first evaluation strategy every scaling team implements, and the one that most teams never evolve beyond.

The appeal is obvious. **Random sampling** means selecting a uniform random subset of production outputs for evaluation, where every output has an equal probability of being chosen. No classification logic. No risk scoring. No segment analysis. Just a coin flip — or, more precisely, a random number generator that includes each output with a fixed probability, say one in a hundred. The implementation is trivial: generate a random number between zero and one for each output, and if it's below your sampling rate, send that output to the eval pipeline. A junior engineer can build it in an afternoon. It runs in microseconds. It requires no domain knowledge, no product context, and no configuration beyond the sampling rate itself.

And it works. For a specific, important purpose, random sampling works remarkably well. The trouble starts when teams mistake that specific purpose for a general one.

## What Random Sampling Actually Tells You

Random sampling excels at one job: estimating aggregate quality. If you want to answer the question "what percentage of our outputs are acceptable?" a random sample is the mathematically optimal way to estimate that number. Because every output has an equal chance of being selected, the sample is an unbiased estimator of the population. A random sample of two thousand outputs from a system processing a million per day will give you a quality estimate within plus or minus two percentage points at ninety-five percent confidence. That estimate is as trustworthy as it gets without evaluating every output.

This makes random sampling ideal for trend monitoring. If your aggregate quality was ninety-one percent last week and your random sample this week shows eighty-eight percent, you have a statistically meaningful signal that something changed. You don't know what changed. You don't know which segment is affected. But you know the direction and the magnitude, and that's enough to trigger an investigation. For teams that previously had no production evaluation at all — and in 2026, that still describes a surprising number of teams — random sampling is a massive upgrade. It turns the lights on. Where you previously had no visibility into production quality, you now have a reliable aggregate signal that tells you whether things are getting better, getting worse, or staying the same.

Random sampling also provides a natural baseline against which more sophisticated strategies can be measured. When you later implement stratified, risk-based, or anomaly-triggered sampling, you'll compare their detection rates against what random sampling would have caught. This comparison is how you justify the additional complexity and cost of those strategies. Without a random baseline, you can't quantify whether your sophisticated sampling is actually better.

## The Coverage Illusion

Here is where the danger begins. A one-percent random sample of one million daily outputs gives you ten thousand evaluations per day. Ten thousand feels like a lot. You can build dashboards from ten thousand evaluations. You can compute confidence intervals. You can track trends. The numbers look impressive and statistically robust.

But consider what ten thousand evaluations from a one-percent random sample cannot do. If a critical failure mode affects 0.1 percent of your traffic — one thousand outputs per day — your random sample will contain roughly ten examples of that failure mode. Ten examples might be enough to notice that the failure exists, if someone is manually reviewing individual evaluation results. It is not enough to characterize the failure — to understand when it happens, which users it affects, what triggers it, or how severe it is. And if the failure mode affects 0.01 percent of traffic — still a hundred outputs per day reaching real users — your random sample will contain roughly one example. One. You're unlikely to notice a single anomaly in a dataset of ten thousand. The failure mode is effectively invisible.

This is the **coverage illusion**: the sample is large enough to produce statistically robust aggregate metrics while being far too sparse to detect or characterize rare but important failure modes. The dashboard shows overall quality at ninety percent with tight confidence intervals. Everything looks fine. Meanwhile, enterprise customers in a specific industry vertical are receiving hallucinated compliance advice three percent of the time, and the random sample simply didn't include enough of those outputs to surface the pattern.

A customer support AI company learned this lesson painfully in mid-2025. Their random sampling dashboard showed steady quality at eighty-nine percent for three consecutive months. But their enterprise account manager started receiving complaints from a healthcare client about incorrect insurance terminology in AI responses. When the team investigated, they found that healthcare-related queries — roughly four percent of total traffic — had a quality rate of sixty-one percent, not eighty-nine. The random sample had included healthcare queries proportional to their traffic share, which meant about four hundred healthcare evaluations per month. Those four hundred were flagging at a higher failure rate, but the signal was drowned in the nine-thousand-six-hundred evaluations from other categories where quality was fine. The aggregate number — eighty-nine percent — was correct. It was also useless for protecting the company's most important customer segment.

## The Rare Event Problem

Random sampling's blindness to rare events is not a minor limitation. It is a fundamental constraint that no amount of sampling rate increase can overcome cost-effectively. The mathematics are unforgiving.

To detect a failure mode that affects 0.1 percent of traffic with ninety-five percent confidence — meaning you want at least a ninety-five percent chance of seeing at least one instance in your sample — you need a minimum sample size of roughly three thousand from that traffic subset. If the affected subset is itself small — say one percent of total traffic — you need to sample three thousand from that one percent, which means your effective sampling rate for that category needs to be thirty percent or higher. But your random sampling rate is uniform. You can't tell a random sampler to sample healthcare queries at thirty percent and everything else at one percent, because the whole point of random sampling is that it doesn't distinguish between categories.

You could increase the overall random sampling rate to thirty percent to ensure coverage of that subset, but then you're evaluating thirty percent of all traffic — a thirty-fold cost increase — just to achieve adequate coverage of one small category. This is the brute-force approach to the rare event problem, and it scales horribly. Every additional rare category you want to cover requires another increase in the overall sampling rate, and the cost grows multiplicatively.

The rare event problem is particularly treacherous because the failures that damage your business most are almost always rare. The common outputs are the ones you've already optimized for. The edge cases, the unusual queries, the minority-language requests, the high-stakes professional questions — these are lower in volume and higher in consequence. Random sampling gives you the least visibility precisely where you need the most.

## Sizing Your Random Sample

Despite its limitations, random sampling is the correct baseline for every evaluation system, and sizing the sample correctly matters. Too small and you lose statistical power. Too large and you waste budget on marginal improvements in precision. The right size depends on three parameters: the smallest quality shift you want to detect, the confidence level you require, and the number of segments you plan to analyze.

For aggregate monitoring — tracking overall quality across all traffic — a sample of one thousand to three thousand evaluations per day is sufficient for most production systems. This gives you enough precision to detect a two-to-three-percentage-point quality shift within a single day. If quality drops from ninety percent to eighty-seven percent, you'll see it in one day's sample with high confidence. If you can tolerate a two-day detection window, you can halve the daily sample and aggregate across days.

For segment-level analysis — tracking quality within specific categories — you need the per-segment sample size to be independently sufficient. If you want to track quality in each of ten segments with the same precision, you need one thousand to three thousand evaluations per segment per day, which means ten thousand to thirty thousand total daily evaluations. At one percent sampling, this requires a daily traffic volume of one million to three million. If your traffic is lower, you have two options: increase the sampling rate, or accept lower precision at the segment level and rely on multi-day aggregation.

The practical rule that experienced teams converge on: set your random sampling rate so that you get at least five hundred evaluated outputs per meaningful segment per week. If you have ten segments and seven days, that's five thousand evaluations per week, or roughly seven hundred per day. At four cents per evaluation, that's twenty-eight dollars per day. For most teams, this is the cost of evaluating well below the cost of evaluating poorly.

## What Random Sampling Should Trigger

The most common mistake teams make with random sampling is treating it as a monitoring system rather than a triggering system. A monitoring system tells you the current state. A triggering system tells you when to take action. Random sampling is far more valuable as the latter.

Set up your random sampling pipeline with two kinds of thresholds. The first is a **level threshold**: an absolute quality floor below which the system triggers an alert. If your aggregate quality drops below eighty-five percent, someone gets paged. This catches acute degradation — a bad model deployment, a broken prompt template, a data pipeline failure. The second is a **trend threshold**: a quality change that exceeds the normal day-to-day variance. If quality drops by more than two standard deviations from the thirty-day rolling average, the system triggers an alert even if the absolute level is still above the floor. This catches slow degradation — the gradual erosion that happens when prompt quality drifts, training data goes stale, or user behavior shifts.

When either threshold fires, random sampling has done its job. It detected a signal. But random sampling cannot tell you what caused the problem or where to look. That's not its job. Its job is to ring the bell. The investigation that follows the bell requires different sampling strategies — stratified sampling to identify which segment is affected, risk-based sampling to prioritize high-stakes evaluation, anomaly-triggered sampling to deep-dive on unusual outputs. Random sampling is the early warning system. The response system requires more intelligence.

A well-designed eval system treats random sampling as the first stage of a cascade. Random sampling runs continuously, cheaply, at low sampling rates. When it detects a signal, it triggers intensified evaluation — higher sampling rates, targeted sampling of suspicious segments, deeper judge rubrics — until the problem is characterized and resolved. Then it returns to baseline rates. This surge-and-return pattern keeps costs low during stable periods and concentrates evaluation budget during the periods when it matters most.

## Background Sampling: The Named Pattern

Experienced evaluation teams refer to their random sampling layer as **background sampling** — the always-on, low-rate, unbiased quality signal that provides the baseline against which everything else is measured. The name is deliberate. Like background radiation in physics, it's always there, always measurable, and forms the baseline against which anomalies become detectable.

Background sampling has four properties that distinguish it from other sampling strategies. First, it is unbiased: every output has an equal probability of selection, so the resulting metrics are representative of overall quality. Second, it is continuous: it runs on every evaluation cycle, not triggered by events or conditions. Third, it is low-rate: typically between 0.5 and three percent of traffic, depending on volume and budget. Fourth, it is unconditional: it does not depend on risk classification, segment identification, or any other analysis of the output. These four properties make background sampling the foundation layer of any multi-strategy evaluation architecture.

The background sampling layer also serves as a calibration mechanism. When you implement stratified or risk-based sampling — which we'll cover in the next subchapters — the quality metrics from those targeted samples will differ from the background metrics, because targeted samples are not representative of overall traffic. Background sampling provides the unbiased reference point that lets you interpret targeted metrics correctly. If your risk-based sampling shows eighty-three percent quality in high-risk outputs, and your background sampling shows ninety-one percent overall, you know the risk-based sample is surfacing a genuinely lower-quality segment, not an artifact of biased sampling. Without the background reference, you can't distinguish between a real problem and a sampling effect.

## The Team That Never Outgrew Random Sampling

Most teams implement random sampling as their first evaluation strategy. This is correct. What's incorrect — and what happens far too often — is that random sampling remains their only evaluation strategy twelve months later.

The pattern is recognizable. A team implements random sampling during a quality crisis. The dashboard comes online. Aggregate metrics are visible. The immediate crisis passes. The team declares evaluation "done" and moves on to the next feature. Nobody builds stratified sampling because random sampling is "working fine." Nobody builds risk-based sampling because "we haven't had any more quality incidents." The absence of detected problems is interpreted as evidence that problems don't exist, when in reality it's evidence that the detection system can't see the problems that do exist.

A mid-sized e-commerce company operated this way for fourteen months. Random sampling showed aggregate quality hovering between eighty-seven and ninety percent, month after month. The team was satisfied. Then a product manager noticed, through manual review of support tickets, that German-language responses had a factual accuracy rate roughly twenty points below English responses. The random sample had been including German responses proportionally — about six percent of the sample, since German users were six percent of traffic — but the per-language sample was too small to surface the gap in the aggregate dashboard. German quality had been degrading for months, and the random sample had been technically capturing it but practically burying it.

This is not a failure of random sampling. Random sampling did exactly what it's designed to do: provide an unbiased estimate of overall quality. It is a failure of strategy — the failure to recognize that aggregate monitoring alone is not sufficient for a product serving multiple languages, multiple user segments, or multiple use cases. Random sampling is the floor of your evaluation architecture. It should never be the ceiling.

## Moving Beyond the Baseline

The path from random sampling to a mature evaluation strategy follows a predictable sequence, and understanding that sequence helps you plan the infrastructure investments before you need them. Random sampling is step one: turn on unbiased aggregate monitoring. Stratified sampling is step two: ensure every important segment gets adequate evaluation coverage. Risk-based sampling is step three: concentrate evaluation depth where failures cost most. Change-based sampling is step four: intensify evaluation during high-risk periods like deployments and model updates. Anomaly-triggered sampling is step five: route unusual outputs to deeper evaluation automatically.

Each step adds detection capability that the previous steps lack. Each step also adds implementation complexity, infrastructure requirements, and ongoing maintenance burden. The mistake is not implementing steps out of order. The mistake is stopping at step one and believing you're done.

Random sampling tells you the temperature of the room. But it can't tell you that the back corner is on fire while the front is comfortable. For that, you need to look at the room in pieces — ensuring every segment that matters gets its own quality measurement. That's the subject of the next subchapter: stratified sampling, and how it transforms one evaluation problem into many smaller, better-defined evaluation problems that random sampling alone can never solve.
