# 8.5 — Evaluating Compound AI Systems: Trace-Level Analysis and Component-vs-System Quality

The AI system that answers your user's question is not a single model. It is a pipeline — a retrieval component fetches relevant documents, a ranking component orders them, a generation model synthesizes an answer, a safety filter checks the output, and an orchestration layer coordinates the entire flow. When the final answer is wrong, which component failed? This question — trace-level evaluation of compound AI systems — is one of the hardest problems in production AI quality and one of the most consequential.

## Why Compound Systems Break Evaluation

Traditional evaluation treats the AI system as a black box: input goes in, output comes out, judge scores the output. This works when the system is a single model. It fails for compound systems because a good final output can mask component failures, and a bad final output can be caused by any component in the chain.

Consider a retrieval-augmented generation system. The retrieval component might fetch irrelevant documents, but the generation model might produce a plausible answer from its parametric knowledge alone, ignoring the bad retrieval. Output-level evaluation scores the answer as correct. But the system is fragile — it succeeded despite a component failure, not because of reliable component performance. The next query, where the model's parametric knowledge is insufficient, will fail because retrieval was never working properly.

Conversely, the retrieval component might fetch perfect documents, but the generation model might hallucinate an answer that contradicts the retrieved evidence. Output-level evaluation catches the hallucination, but without trace-level analysis, the team might blame retrieval when the problem is generation.

## Trace-Level Evaluation

**Trace-level evaluation** instruments every step in the compound system's pipeline and evaluates each step independently, in addition to evaluating the final output. For a RAG system, this means evaluating retrieval quality (did the right documents come back?), context relevance (were the retrieved documents useful for the query?), generation faithfulness (does the answer reflect the retrieved evidence?), and final output quality (is the answer correct and helpful?).

Each evaluation point in the trace requires its own criteria, its own judge configuration, and its own pass/fail thresholds. Retrieval is evaluated on recall and precision. Generation is evaluated on faithfulness and accuracy. The safety filter is evaluated on its false positive and false negative rates. The orchestration layer is evaluated on latency and error handling.

The 2026 state of the art in trace-level evaluation is built on platforms like LangSmith, Arize, and Maxim, which provide trace visualization and per-component scoring. LangSmith's agent trace analysis, for example, decomposes multi-step agent executions into individual actions, evaluates each action against defined criteria, and identifies the specific step where failures originate.

## Component-vs-System Quality

A critical insight in compound system evaluation is that system quality is not the average of component qualities. It is the product. If retrieval has 90% accuracy and generation has 90% faithfulness, the combined system has roughly 81% end-to-end accuracy — each component's error rate compounds. Add a third component with 90% reliability, and system accuracy drops to 73%.

This multiplicative relationship means that small degradations in individual components create disproportionate system-level impact. A 5% drop in retrieval accuracy doesn't cause a 5% drop in system quality — it causes a 5% increase in the failure rate at the retrieval stage, which cascades through downstream components. Trace-level evaluation catches these component degradations before they compound into system-level failures visible to users.

## The Blame Attribution Problem

When the final output fails, trace-level evaluation enables blame attribution — identifying which component caused the failure. But attribution is not always straightforward. In multi-component systems, failures cascade. A slow retrieval component causes a timeout, which causes the orchestrator to skip retrieval and fall back to parametric generation, which causes a hallucination. The root cause is retrieval latency, but the visible failure is generation quality. Without trace-level evaluation, the team would investigate the wrong component.

Blame attribution requires analyzing the full trace, not just the final output. The trace reveals the sequence of events: which components were called, what inputs each received, what outputs each produced, how long each took, and whether any error conditions occurred. This sequence is the forensic record that makes root-cause analysis possible.

## Practical Implementation

Implementing trace-level evaluation at scale requires three things. First, instrumentation — every component in the pipeline must emit structured trace events that can be collected and analyzed. Second, per-component evaluation criteria — each component needs its own quality definition and its own evaluation method, which may differ from the final output evaluation. Third, trace storage and analysis infrastructure — at production scale, trace data is voluminous and must be sampled, stored efficiently, and queryable for both real-time monitoring and retrospective investigation.

The cost of trace-level evaluation is higher than output-level evaluation because you're running multiple evaluations per request instead of one. The benefit is that you catch component failures before they become system failures, and you can optimize each component independently with precision that output-level evaluation cannot provide.

Beyond compound system internals, another frontier in outcome evaluation is the reliability of autonomous agents — systems that take multiple steps to achieve a goal, where success is measured not by any single output but by the trajectory of the entire execution.
