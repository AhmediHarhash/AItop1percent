# 5.3 — Async Eval Pipelines: Decoupling Evaluation from User-Facing Latency

The worst thing an evaluation system can do is slow down your product. The second worst thing is not running at all. Most teams who try to bolt evaluation onto their production request path discover both failures in sequence: first the evaluation adds latency that degrades user experience, then the team rips it out, and the product runs unmonitored for months until the next quality incident forces them to try again.

Async eval pipelines solve both problems by decoupling evaluation from the request-response path entirely. The production system serves users at full speed. Separately, independently, at its own pace, the evaluation system ingests copies of those production interactions and scores them. The user never waits. The evaluation never rushes. The two systems share data but share nothing else — not latency budgets, not failure domains, not scaling constraints. This decoupling is the architectural foundation of every production-grade quality monitoring system in 2026, and getting it right determines whether your evaluation system survives contact with real traffic.

## The Core Pattern: Produce, Queue, Consume

The async eval pattern has three actors and one buffer. The production system is the producer. It handles user requests, generates outputs, and — as its final step — publishes a copy of the interaction to a message queue or event stream. The queue is the buffer. It holds interaction records until the eval system is ready to process them. The eval workers are the consumers. They read interaction records from the queue, run evaluations, and write results to a separate data store.

The critical design principle is that the production system's obligation ends at the publish step. It does not wait for the evaluation to complete. It does not check whether the eval worker received the message. It does not retry if the publish fails unless the publish itself can be made effectively instantaneous — which, with modern message brokers, it can. The production system's latency budget includes the time to publish a message, which is typically one to five milliseconds. It does not include the time to evaluate, which is typically three to thirty seconds for an LLM judge call.

This separation means the production system and the evaluation system have completely independent scaling characteristics. Your production system scales based on user traffic. Your evaluation system scales based on evaluation throughput requirements. If traffic doubles overnight because of a marketing campaign, the production system scales up and the queue absorbs the increased message volume. The evaluation system scales up on its own schedule — perhaps the next morning when the ops team provisions additional eval workers — and processes the backlog. The user never notices. The evaluation never misses data. The queue is the shock absorber.

## Why Synchronous Evaluation Is Almost Always Wrong

Teams new to evaluation infrastructure often make the same mistake: they add a synchronous evaluation call to the production request handler. The request comes in, the model generates a response, the response is sent to a judge for scoring, and the score is logged before the response is returned to the user. It seems clean. It guarantees that every served response is evaluated. And it is almost always the wrong architecture.

Synchronous evaluation adds the full latency of the judge call to every user request. An LLM judge call that takes five seconds turns a one-second response into a six-second response. No user-facing product can absorb that latency penalty. Even a lighter evaluation — a heuristic check or a small classifier — adds ten to fifty milliseconds. At scale, those milliseconds compound across thousands of requests per second, consuming server resources and increasing tail latency.

Synchronous evaluation also creates a coupling that makes both systems fragile. If the judge API returns an error, what does the production system do? Serve the response without an eval score? That defeats the purpose of synchronous evaluation. Block the response until the judge recovers? That turns a judge outage into a production outage. Queue the evaluation and serve the response immediately? That's just async evaluation with extra steps. Every mitigation for synchronous evaluation's failure modes moves you closer to the async pattern.

The one legitimate use case for synchronous evaluation is real-time gating — the pattern where an output must pass an eval check before it reaches the user. Safety classifiers that block harmful outputs, compliance checks that filter regulated content, and format validators that reject malformed responses are all gating evaluations that must run synchronously. But these should be lightweight, purpose-built classifiers running locally on the serving infrastructure, not general-purpose LLM judge calls routed through an API. The real-time gating path is a separate concern from the quality monitoring path, and conflating them is how teams end up with evaluation systems that are too slow for gating and too tightly coupled for monitoring.

## Choosing Your Queue Infrastructure

The message queue sits between your production system and your evaluation pipeline, and the choice of queue technology shapes the reliability, scalability, and operational characteristics of your async eval system.

Apache Kafka is the most common choice for high-volume evaluation pipelines in 2026, and for good reason. Kafka provides durable, ordered, partitioned message storage with configurable retention periods. Messages aren't deleted when consumed — they're retained for a configurable window, typically seven to thirty days. This means you can replay the eval queue if you discover a bug in your evaluation logic. You can add a new evaluation type and backfill it against the last week of production traffic. You can investigate a quality incident by replaying the outputs from the affected time window. The replay capability alone makes Kafka the preferred choice for teams that treat evaluation as a first-class production concern.

Amazon SQS, Google Cloud Pub/Sub, and Azure Service Bus are simpler alternatives that work well for teams processing fewer than a million evaluations per day or teams that don't have existing Kafka infrastructure. These managed queue services handle provisioning, scaling, and fault tolerance automatically. The tradeoff is reduced replay capability — once a message is consumed and acknowledged, it's gone — and less control over partitioning and consumer group management. For many teams, the operational simplicity is worth the capability tradeoff.

Regardless of queue technology, you need to get three things right. First, the message format must include everything the eval worker needs to run the evaluation without querying external systems: the production input, the model output, the relevant metadata such as the model version, the timestamp, and the product surface identifier. If the eval worker has to query the production database to fetch context that should have been in the message, you've introduced a coupling between eval and production that the queue was supposed to eliminate.

Second, the queue must be sized for your peak-to-average traffic ratio. If your production system averages a hundred thousand outputs per hour but peaks at five hundred thousand during high-traffic periods, your queue must absorb the peak without dropping messages while your eval workers process at their sustainable rate. A queue that fills up and starts dropping messages during peak traffic is worse than having no queue — it creates gaps in your quality signal exactly when traffic is highest and quality risks are greatest.

Third, your queue monitoring must distinguish between healthy backlog and unhealthy backlog. A queue depth of fifty thousand messages is healthy if your eval workers process ten thousand per hour and the backlog was created by a temporary traffic spike — it will be cleared in five hours. The same queue depth is unhealthy if it's been growing steadily for three days, indicating that your eval throughput is permanently below your production volume. Queue depth without context is a meaningless metric. Queue depth trend — is it growing, stable, or shrinking? — is the metric that matters.

## Handling Backpressure

Backpressure is what happens when your evaluation pipeline can't keep up with your production system's output rate. The queue grows. If it grows without bound, you run out of storage. If you cap it and start dropping messages, you lose quality signal. Neither outcome is acceptable, so you need a backpressure strategy.

The first line of defense is **queue-level sampling**. Instead of evaluating every production output, you sample at the ingestion point. A sampling rate of five percent means the queue receives only five percent of production volume, reducing the sustained throughput requirement for your eval workers by ninety-five percent. When the queue depth starts growing, you can reduce the sampling rate — from five percent to two percent, or from two percent to one percent — to bring eval throughput back in line with production volume. The quality signal degrades slightly because you're evaluating fewer outputs, but the pipeline stays healthy and no data is lost from the sampled set.

The second line of defense is **priority-based consumption**. Not all evaluation work is equally urgent. A safety evaluation on a potentially harmful output is more important than a quality evaluation on a routine response. A production output from a new model version is more interesting than an output from the stable version. When backpressure builds, your eval workers should process high-priority evaluations first and defer low-priority ones. This requires priority metadata in the queue messages and a consumption strategy that respects priority ordering — either through separate priority queues or through a consumer that peeks at messages and selects the highest-priority one available.

The third line of defense is **autoscaling eval workers**. When queue depth exceeds a threshold — say, more than two hours of backlog — automatically provision additional eval workers to increase throughput. When the backlog clears and queue depth drops below a lower threshold — say, less than thirty minutes of backlog — scale workers back down. This requires your eval workers to be stateless and horizontally scalable, which they should be by design: each worker reads a message from the queue, runs an evaluation, writes the result, and acknowledges the message. No shared state between workers. No coordination required.

The combination of these three defenses — sampling, prioritization, and autoscaling — creates a system that handles sustained overload gracefully. In a traffic surge, the pipeline first autoscales to meet demand. If the surge exceeds autoscaling capacity, priority consumption ensures the most important evaluations complete first. If sustained overload persists beyond what autoscaling can handle, the sampling rate adjusts downward to match the evaluation throughput. At no point does the pipeline crash, drop data from the sampled set, or block the production system.

## The Latency Gap and Why It's Acceptable

Async evaluation introduces a fundamental latency gap: the time between when a production output is served and when its evaluation result is available. For an LLM judge evaluation in an async pipeline, this gap is typically five minutes to two hours, depending on queue depth, eval worker throughput, and evaluation complexity. For heuristic evaluations, it might be seconds. For human review in the evaluation loop, it might be days.

This latency gap makes async evaluation unsuitable for real-time gating decisions. You can't use an async eval result to block a response that was served an hour ago. But for quality monitoring, trend detection, reporting, and alerting, the latency gap is acceptable — and the benefits of decoupling far outweigh the delay.

Consider what you actually do with evaluation results. You look at dashboard averages that update every fifteen to thirty minutes. You review daily quality reports that aggregate thousands of evaluations. You investigate alerts that fire when rolling averages cross thresholds. None of these use cases require the evaluation result to be available at the instant the production output is served. They require the evaluation result to be available within the aggregation window — within fifteen minutes for real-time dashboards, within the same day for daily reports, within the same hour for time-sensitive alerts.

A well-configured async pipeline with adequate eval worker capacity maintains an end-to-end latency of five to fifteen minutes for the majority of evaluations. That means your real-time dashboards reflect quality conditions from fifteen minutes ago, not from right now. For most products, this is indistinguishable from real-time. The quality degradation that started ten minutes ago and will affect users for the next several hours is fully visible in the dashboard — it just shows up ten minutes after it begins rather than instantly. The practical impact of that ten-minute delay on your ability to detect and respond to quality issues is negligible.

The exception is high-stakes systems where even a ten-minute detection delay is unacceptable — healthcare, financial transactions, safety-critical applications. For these systems, the real-time gating path with synchronous lightweight classifiers is the appropriate pattern for immediate decisions, while the async pipeline provides the deeper, more comprehensive evaluation that supports investigation and reporting.

## Ordering Guarantees: How Much Do You Need?

Distributed queue systems offer varying levels of message ordering guarantees, and teams building async eval pipelines often worry about ordering more than they should.

For most evaluation workloads, strict ordering doesn't matter at all. Evaluating output A before output B produces the same quality scores as evaluating B before A. The evaluation of each output is independent — the judge scores an output based on the input and output alone, not based on what it scored previously. Your eval workers can process messages in any order, and the results are identical.

Where ordering becomes relevant is in aggregation and trend detection. If you're computing a rolling average quality score over thirty-minute windows, and your eval workers process messages significantly out of order, your thirty-minute windows might include evaluations from outputs served two hours apart. This makes the trend signal noisy. A quality drop that happened at 2:00 PM might show up as a gradual decline from 1:30 to 3:00 because the evaluations from that period were processed out of order and spread across multiple windows.

The practical solution is approximate ordering, not strict ordering. Kafka's partition-based ordering guarantees that messages within a single partition are processed in order. If you partition your eval queue by time bucket — all outputs from the same ten-minute window go to the same partition — you get ordering within each window, which is sufficient for trend detection. Messages from different windows may be processed concurrently and out of order, but each window's evaluations are internally ordered and complete before the window is used in aggregation.

For most teams, even this level of ordering is more than necessary. A simpler approach is to use the output's timestamp rather than the evaluation's completion timestamp for windowed aggregation. When aggregating quality scores into thirty-minute windows, group by when the output was served, not by when the evaluation completed. This produces correct trend signals regardless of evaluation processing order, because the window assignment is based on the production timestamp embedded in the message, not the evaluation timestamp that depends on processing order.

## Eventual Consistency: What Your Dashboards Must Account For

An async eval pipeline is eventually consistent. At any given moment, some recent production outputs have been evaluated and some haven't. Your dashboards, reports, and alerts must account for this fact, or they'll generate misleading signals.

The most common mistake is displaying the most recent thirty-minute window of evaluation results as if it represents complete data. If your eval pipeline has a fifteen-minute average latency, the most recent thirty-minute window is only about fifty percent evaluated. The quality score for that window is based on half the data, with significant sampling noise. A team that alerts on a quality drop in the most recent window will get frequent false alarms caused by incomplete data, not actual quality problems.

The fix is a **completeness threshold**. Your dashboard and alerting system should only consider a time window reliable when the number of evaluations in that window exceeds a configured fraction of the expected count. If you sample five percent of production traffic and your system serves ten thousand outputs per thirty-minute window, you expect roughly five hundred evaluations per window. If the most recent window has only two hundred evaluations, it's less than fifty percent complete — don't display it as a definitive quality measurement, and certainly don't alert on it. Display it with a visual indicator that the data is preliminary, and wait until it reaches the completeness threshold before treating it as final.

Similarly, your trend detection logic must use stable windows — windows old enough that their evaluations are complete — as the baseline for comparison. Comparing the current incomplete window against a stable historical window produces meaningless results. Compare the most recently completed window against the historical baseline, and display the current incomplete window as a projection rather than a measurement.

Teams that get eventual consistency right build trust in their evaluation dashboards. Teams that get it wrong train their engineers to ignore the dashboards, because the dashboards cry wolf with false alarms from incomplete data. Once engineers stop trusting the dashboard, they stop looking at it, and the entire investment in evaluation infrastructure is wasted.

## Monitoring the Async Pipeline Itself

The async eval pipeline is itself a production system that requires monitoring. The irony is not lost on experienced teams: the system you built to monitor quality needs its own monitoring to ensure it's working.

The key metrics for an async eval pipeline are queue depth, processing latency, evaluation throughput, error rate, and completeness ratio. Queue depth tells you whether the pipeline is keeping up with production volume. Processing latency tells you how long evaluations take from enqueue to result. Evaluation throughput tells you how many evaluations per minute the pipeline is completing. Error rate tells you what percentage of evaluations are failing. Completeness ratio tells you what fraction of expected evaluations in each time window have actually been completed.

These metrics should feed their own alerting rules, separate from the quality alerts that the evaluation results produce. A rising queue depth alert fires before the quality dashboard becomes stale, giving you time to scale up eval workers or adjust the sampling rate. An elevated error rate alert fires when the judge API is returning failures, before those failures manifest as missing data in your quality reports. A declining completeness ratio alert fires when a change to the production system — perhaps a new output format that the eval ingestion doesn't recognize — causes evaluations to silently stop processing a category of outputs.

The most dangerous failure mode for an async pipeline is the silent stop: the pipeline stops processing evaluations for a subset of traffic, but the queue depth doesn't grow because those messages are being consumed and discarded due to a parsing error or a routing misconfiguration. The quality dashboard shows data, so nobody notices that the data comes only from the unaffected traffic subset. The quality signal looks stable, but it's missing the segment where the actual degradation is happening.

The defense against silent stops is reconciliation. Periodically — daily for most teams, hourly for high-stakes systems — compare the number of production outputs in each category against the number of evaluation results in that category. Any category where evaluation coverage dropped significantly below the expected rate is a candidate for investigation. The production system logged ten thousand Spanish-language outputs yesterday, but the evaluation system has results for only two hundred? Something in the pipeline is dropping Spanish-language evaluations. Without reconciliation, you'd discover this the next time a Spanish-language quality incident slips through undetected.

## From Async to Parallel

The async pipeline decouples evaluation from production. It ensures that your users never wait for evaluation and that your eval system never crashes the product. But the async pipeline by itself doesn't guarantee fast evaluation. If you have one eval worker processing messages sequentially from a large queue, decoupling gives you reliability but not speed. The queue grows, the latency gap widens, and your dashboards show quality data from hours ago instead of minutes ago.

The next subchapter addresses the parallelization question directly: how to distribute evaluation work across many workers running simultaneously, how to avoid the bottlenecks and data races that emerge when hundreds of eval workers compete for shared resources, and how to scale horizontally without introducing the coordination overhead that eats your throughput gains.
