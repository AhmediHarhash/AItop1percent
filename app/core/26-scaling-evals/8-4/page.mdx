# 8.4 — Causal Inference vs Correlation in AI Evaluation

Every team that measures AI outcomes faces the same trap: the numbers show that users who interact with the AI have better outcomes than users who don't, and the team concludes that the AI is causing the improvement. This conclusion is often wrong. Users who engage with an AI feature may be more motivated, more tech-savvy, or more engaged with the product overall. The AI didn't cause the better outcomes — it correlated with them. The difference between these two interpretations is the difference between knowing your AI works and hoping it does.

## Why Correlation Deceives

Correlation between AI usage and positive outcomes can arise from three sources. The first is genuine causal impact — the AI actually helped. The second is selection bias — the users who choose to use the AI are systematically different from those who don't, and those differences, not the AI, explain the outcome difference. The third is confounding variables — other factors that correlate with both AI usage and outcomes, such as time of day, device type, or user experience level.

Without deliberate causal analysis, you cannot distinguish between these sources. And the distinction matters enormously for resource allocation. If the AI is genuinely improving outcomes, you should invest more in it. If selection bias is creating the appearance of impact, increasing AI adoption won't improve overall outcomes — it will dilute the effect as less-engaged users are included. One B2B SaaS company attributed a 22% improvement in customer retention to their AI onboarding assistant, based on comparing retention rates between users who interacted with the assistant and those who didn't. A subsequent holdout experiment revealed the actual causal impact was 6%. The remaining 16% was selection bias — customers who proactively engaged with onboarding tools were already less likely to churn, with or without AI.

## Randomized Holdouts: The Gold Standard

The strongest causal method is the **randomized holdout** — randomly assigning a small percentage of eligible users to a control group that doesn't receive the AI feature, and comparing outcomes between the AI group and the control group. Random assignment eliminates selection bias by design. Users in the holdout are statistically identical to users in the treatment group, so any difference in outcomes is attributable to the AI.

The holdout must be large enough to detect meaningful differences. Detecting a 5% improvement in conversion rate with 95% confidence when the baseline conversion rate is 10% requires roughly 3,000 users in each group. For rarer outcomes — like a 2% improvement in a metric that already has a 1% base rate — the required sample size climbs to tens of thousands. Underpowered holdouts produce ambiguous results that create more confusion than clarity.

The holdout should be truly random — assigned at the user level, not the session level, to prevent individual users from appearing in both groups. Session-level randomization creates contamination where the same user sometimes gets the AI and sometimes doesn't, muddying the comparison. User-level randomization is slightly more complex to implement but produces clean comparisons.

A practical implementation pattern is the "1% always-off" holdout: permanently assign 1% of users to never receive the AI feature. This small group provides a persistent baseline that you can compare against at any time. The experience cost is minimal — 99% of users get the full AI-powered experience — while the measurement value is substantial.

## Natural Experiments: Opportunistic Causal Evidence

When randomized holdouts aren't possible, **natural experiments** provide the next best source of causal evidence. These are situations where AI availability varied for reasons unrelated to user characteristics — creating quasi-random variation that approximates a controlled experiment.

Server outages are the most common natural experiment. When an outage disables the AI feature for some users but not others, the affected users become an unplanned control group. Comparing outcomes during and after the outage, between affected and unaffected users, provides causal evidence — as long as the outage didn't disproportionately affect certain user types. A content platform discovered this approach accidentally when a 14-hour outage disabled their AI recommendation engine for users in two regions. Conversion rates in those regions dropped 8% relative to unaffected regions during the outage period, providing stronger causal evidence than months of correlation-based analysis.

Phased rollouts create similar opportunities. When a new AI feature is launched region by region over several weeks, the early regions become the treatment group and the later regions serve as temporary controls. Geographic and demographic differences between regions introduce some confounding, but if the regions are reasonably similar, the phased rollout provides useful causal signal.

Feature flags that were enabled based on non-behavioral criteria — user ID parity, registration date, account tier — create another form of natural experiment. The assignment mechanism is unrelated to user engagement or propensity to convert, so the resulting comparison approximates random assignment.

## Statistical Controls: The Weakest but Most Accessible Method

When neither randomized holdouts nor natural experiments are available, **statistical controls** offer a partial solution. These methods use regression analysis or matching techniques to account for observable differences between AI users and non-users.

**Propensity score matching** is the most common technique. First, build a model that predicts the probability of a user engaging with the AI feature based on observable characteristics — account age, usage frequency, plan tier, device type. Then pair each AI user with a non-user who has a similar predicted probability of using the feature. Compare outcomes within matched pairs. Because matched users have similar observable characteristics, differences in outcomes are less likely to be driven by selection bias.

The critical limitation is the word "observable." Propensity matching controls for factors you can see and measure. It cannot control for unobservable factors — motivation, tech-savviness, intent — that may be the real drivers of both AI usage and outcomes. This means propensity-matched estimates are always potentially biased, and the direction and magnitude of the bias are unknown. Treat propensity-matched results as directional evidence, not definitive proof.

**Difference-in-differences** is another accessible method. Compare the change in outcomes before and after the AI was introduced for users who adopted it, versus the change over the same period for users who didn't. If both groups were trending similarly before the AI launched and then diverged afterward, the divergence is evidence of causal impact. This method controls for pre-existing differences between groups and for time trends that affect everyone, but it requires the parallel trends assumption — that both groups would have continued trending similarly in the absence of the AI.

## The Minimum Viable Experiment

Not every team needs full causal rigor for every decision. The **minimum viable experiment** provides enough causal evidence to make confident decisions without the overhead of a full experimental design.

The simplest version is a time-based on-off test. Disable the AI feature for all users for one week. Compare outcomes during the off week to the preceding and following on weeks. If outcomes drop during the off week and recover when the feature is restored, you have strong evidence of causal impact. This approach is crude — seasonal variation, day-of-week effects, and other time-varying factors introduce noise — but it's implementable in a day and provides more causal evidence than any amount of correlation analysis.

A slightly more rigorous version is the alternating-days experiment. Enable the AI on Monday, Wednesday, and Friday; disable it on Tuesday and Thursday. Compare outcomes between AI-on days and AI-off days, controlling for day-of-week effects by rotating the schedule across weeks. This requires two weeks of data and controls for weekly cyclical patterns.

The key principle behind minimum viable experiments is that imperfect causal evidence is dramatically more valuable than perfect correlation data. A crude holdout that shows the AI improves conversion by "somewhere between 3% and 9%" is more useful for decision-making than a correlation analysis that shows "users who interact with the AI convert at 22% higher rates" — because the first number is causal and the second is not.

## Getting Organizational Buy-In for Holdouts

The biggest barrier to running holdout experiments is rarely technical — it's organizational. Product managers resist withholding features from users. Executives worry about revenue impact. Legal teams raise concerns about treating users differently. Sales teams object if enterprise customers might land in the holdout group.

Address each objection directly. For product managers: the holdout is small, typically 1-3% of users, and without it, you are making million-dollar investment decisions based on data that might be 75% selection bias. The cost of the holdout is a few percentage points of potential impact for a small user group. The cost of bad decisions based on inflated impact numbers is orders of magnitude larger.

For executives: frame the holdout as de-risking, not withholding. Without causal evidence, the team cannot distinguish a 15% AI impact from a 3% AI impact inflated by selection bias. If the real impact is 3%, the company is overinvesting based on false signals. The holdout protects the company from allocating resources based on misleading metrics.

For legal teams: randomized holdouts are standard practice in technology companies and have been validated in regulatory contexts. A/B testing is not discriminatory when assignment is random. In regulated industries, frame the holdout as quality assurance — you are verifying that the AI is actually helping, which is a compliance obligation, not an optional experiment.

For sales teams: exclude enterprise customers from the holdout if necessary. The holdout provides aggregate population-level causal evidence, not per-customer evidence. Excluding a small segment from the control group reduces statistical power slightly but eliminates the political friction entirely.

## When Causal Rigor Matters Most

Causal rigor matters most when you're making high-stakes decisions based on outcome data — increasing investment in the AI, expanding it to new use cases, or reporting impact to executives and investors. For routine operational monitoring, correlation-based metrics are sufficient to detect trends and regressions. If outcome metrics are declining, the cause matters less than the fact that something needs investigation.

The danger zone is when correlation-based metrics are used to justify major investments or strategic decisions. "Our AI improved conversion by 15%" is a powerful claim that drives budget allocation, hiring decisions, and product strategy. If that 15% is actually 3% causal impact plus 12% selection bias, the decisions based on it are wrong by a factor of four. The holdout experiment that reveals the true 3% might feel deflating, but it prevents the team from making a $2 million investment based on a $500,000 reality.

Outcome attribution and causal inference apply to systems with single AI components. The next subchapter addresses the harder case — compound AI systems where multiple models, tools, and orchestration layers interact, and quality must be evaluated at the trace level, not just the output level.
