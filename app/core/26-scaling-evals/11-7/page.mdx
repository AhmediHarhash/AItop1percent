# 11.7 — Scaling Safety Evaluation: Regulatory Cadences, Automated Red-Teaming, and Compliance Gates

Safety evaluation at scale is no longer optional. The EU AI Act's August 2, 2026 deadline for high-risk AI system compliance means that organizations deploying general-purpose AI must demonstrate systematic safety evaluation — not as a one-time exercise, but as a continuous, documented, auditable process. The GPAI Code of Practice, finalized by the European AI Office in July 2025, spells out what "systematic" looks like: documented eval methodology, regular cadences of safety testing, evidence of adversarial testing, and records of identified risks along with the mitigations applied. Organizations that treat safety evaluation as a quarterly checkbox will find themselves non-compliant. Organizations that weave it into their operating rhythm will find that safety eval improves product quality, not just regulatory standing.

This is not only a European concern. The pattern of regulation following capability is global. California's AB 2013, effective January 2026, mandates transparency about training data including PII. China's generative AI regulations require safety assessments before public deployment. The direction is clear across every major jurisdiction: if you deploy AI systems, you must demonstrate that you evaluate their safety systematically. The question is not whether to scale safety evaluation. The question is how to do it without burying your engineering team under a compliance workload that crowds out product development.

## What Regulators Actually Expect

The gap between what engineering teams think regulators want and what regulators actually require is wider than most teams realize. Engineering teams imagine thick binders of documentation, rigid testing protocols, and formal verification of every output. What regulators actually demand is simpler and harder: evidence that you have a system, that the system runs regularly, that it catches real problems, and that you act on what it finds.

Under the EU AI Act's framework for general-purpose AI with systemic risk, providers must perform model evaluations including adversarial testing, conduct systemic risk analysis, document their methodology, and report serious incidents to the AI Office. The GPAI Code of Practice organizes these into three chapters — transparency, copyright, and safety and security — with the safety chapter applying specifically to models deemed to carry systemic risk. The key word in every requirement is "documented." It is not enough to run safety evals. You must demonstrate that you run them, show what they found, and prove that you responded to the findings.

For most AI products — those that are not themselves foundation models but rather applications built on top of them — the high-risk system requirements apply. These require a documented risk management process covering the entire AI lifecycle, from design through post-market monitoring. Your safety evaluation is a core artifact of that risk management system. The evaluation methodology, the test results, the identified risks, and the mitigations you applied are all compliance documentation. If you are running safety evals in Jupyter notebooks with results stored in an engineer's local folder, you have a compliance problem regardless of how thorough the evals themselves are.

## The Three-Tier Safety Eval Pipeline

Scaling safety evaluation requires the same tiered approach that scales quality evaluation — not every output needs the same level of scrutiny. The cost of running a comprehensive safety analysis on every single model output would make most AI products economically unviable. The solution is a pipeline with three tiers, each balancing depth against speed and cost.

The first tier is **fast safety screening**. This runs on every output, inline with the production pipeline. It uses lightweight heuristic classifiers — keyword filters, regex patterns, small fine-tuned safety models — to catch obvious safety violations: harmful content categories, known jailbreak patterns, explicit refusals that indicate the model recognized a dangerous request but a post-processing step stripped the refusal. Fast screening adds 10 to 30 milliseconds of latency per request. At that speed, it can run on 100% of production traffic. It catches the easy cases — roughly 80% of safety violations that follow recognizable patterns. Its purpose is not comprehensive safety assurance. Its purpose is to ensure that the most flagrant violations never reach the user.

The second tier is **thorough safety evaluation**. This runs on a sampled subset of production outputs, typically 1% to 5% of traffic, selected through a combination of random sampling and targeted sampling of high-risk interaction categories. Thorough evaluation uses LLM-as-judge safety scoring — a more capable model evaluating outputs against detailed safety rubrics that cover harm categories, bias dimensions, privacy leakage, misinformation, and contextual appropriateness. Each evaluation takes 2 to 5 seconds and costs a few cents in model inference. At 3% sampling of 100,000 daily requests, you are running 3,000 thorough safety evaluations per day at a cost of roughly $60 to $150 in judge inference. This tier catches the subtle cases: outputs that are not overtly harmful but are biased, misleading, or inappropriate in context.

The third tier is **comprehensive adversarial testing**. This runs on a scheduled cadence — before every deployment, weekly or bi-weekly between deployments, and on demand when the threat landscape changes. Comprehensive testing uses automated red-teaming: AI systems generating adversarial inputs designed to provoke unsafe outputs. The red-team system probes for jailbreak vulnerabilities, bias triggers, privacy leakage vectors, hallucination-inducing prompts, and edge cases specific to your domain. A comprehensive adversarial test run takes hours, not seconds, and costs hundreds to thousands of dollars in compute. It is the only tier that actively searches for unknown vulnerabilities rather than evaluating known traffic patterns.

## Automated Red-Teaming at Scale

Manual red-teaming — human experts crafting adversarial prompts and testing them against the system — was the state of practice through 2024. It remains valuable for its creativity and contextual understanding. A skilled red-teamer can devise attack scenarios that no automated system would generate, because they understand the social and cultural context in which harm occurs. But manual red-teaming does not scale. A team of five red-teamers working full-time can test perhaps 500 to 1,000 attack scenarios per week. An automated red-teaming system can generate and test 50,000 in the same period.

Automated red-teaming in 2026 uses a attacker-defender architecture. The attacker is a language model — often a capable model like Claude Opus 4.6 or GPT-5 — prompted to generate adversarial inputs that will cause the target system to produce unsafe outputs. The attacker model is given a taxonomy of harm categories, a set of known attack strategies (jailbreaks, role-playing exploits, gradual escalation, context injection), and instructions to generate novel variations. The defender is the target AI system under test. The evaluator is a separate model or set of models that scores the target's responses for safety violations.

The power of automated red-teaming comes from iteration. The attacker does not just generate a static list of adversarial prompts. It generates a batch, observes which prompts succeed in eliciting unsafe responses, analyzes the characteristics of successful attacks, and generates a new batch that exploits the same patterns more aggressively. Over hundreds of iterations, the attacker discovers attack surfaces that manual testing would take weeks to explore. Tools like Microsoft's PyRIT, NVIDIA's Garak, and dedicated platforms like Patronus and HiddenLayer's AutoRTAI have made this pattern accessible to teams that cannot afford to build their own red-teaming infrastructure from scratch.

The limitation of automated red-teaming is false positives and category saturation. The attacker model generates many prompts that trigger the safety evaluator without representing genuine safety risks — adversarial prompts so contrived that no real user would ever produce them. Filtering these false positives requires human review of the automated results, which partially undermines the scale advantage. The practical approach is to cluster the automated red-team findings, have humans review representative examples from each cluster, and focus engineering effort on the clusters that represent realistic attack vectors. This reduces the human review burden from 50,000 individual results to 200 to 300 cluster representatives.

## Regulatory Cadences: How Often to Run What

Regulators do not prescribe specific testing schedules — they require "regular" and "ongoing" evaluation. Translating that into an operational cadence is your responsibility, and the cadence you choose becomes part of your compliance documentation.

The cadence that has emerged as industry practice among organizations preparing for August 2026 compliance follows a three-rhythm pattern. The daily rhythm includes fast safety screening on all production traffic plus thorough safety evaluation on sampled traffic. Results feed into a safety dashboard that shows violation rates by category, trend lines over time, and any new categories of violation that have appeared. The weekly rhythm includes a review of the safety dashboard by the safety evaluation owner, analysis of any new violation patterns, and an update to the fast screening rules if new patterns are detected. The monthly rhythm includes a comprehensive automated red-team exercise, a review of the red-team findings by the safety team, updates to the adversarial testing library based on new attack techniques discovered in the broader AI security community, and a formal summary report that becomes part of the compliance documentation.

Before every deployment, an additional safety eval gate runs the full adversarial test suite against the new model or prompt configuration. No deployment proceeds without passing the safety gate. This deployment-triggered cadence overlays on top of the regular rhythm. If you deploy weekly, you run comprehensive testing weekly. If you deploy daily, the comprehensive test runs overnight and results are available before the morning deployment window.

## Integration with CI/CD Safety Gates

Safety evaluation at scale must be integrated into the same tiered gate model that governs quality evaluation. The principle is the same: the cost and depth of evaluation increases as a change moves from development toward production, and safety failures at any gate block forward progress until the failure is resolved.

The first gate — the pull request gate — runs a focused safety eval against the subset of use cases most likely to be affected by the change. If an engineer modifies a prompt template, the safety gate tests that prompt against a curated set of adversarial inputs specific to the prompt's function. This gate runs in minutes, not hours, and blocks the PR from being merged if safety regressions are detected.

The second gate — the staging gate — runs the full adversarial test suite against the staging environment. This is the comprehensive test that takes hours and covers the full taxonomy of harm categories. The staging gate is the last checkpoint before production. Passing it means the change has been tested against every known attack vector in your adversarial library.

The third gate — the production gate — is the continuous monitoring layer described in the three-tier pipeline. It runs on live traffic and catches safety violations that the pre-deployment gates missed — because the attack vector was new, because the real-world input distribution differed from the test distribution, or because the violation is context-dependent in a way that synthetic testing cannot reproduce.

When a production safety violation is detected, the response workflow follows severity tiers. Critical violations — outputs that could cause immediate harm, such as medical misinformation, instructions for dangerous activities, or exposure of private information — trigger automatic mitigation: the output is suppressed, the interaction is logged for investigation, and the safety team is alerted immediately. Moderate violations — biased outputs, inappropriate tone, mild misinformation — are logged, included in the next weekly review, and addressed in the next deployment cycle. Low-severity violations — formatting issues in safety disclaimers, overly cautious refusals — are tracked as metrics and addressed in batch.

## The Cost of Safety Evaluation

Safety evaluation is not free, and pretending it is leads to underfunded safety programs that satisfy neither regulators nor users. For teams deploying regulated AI products, safety evaluation typically represents 15% to 25% of total evaluation spend. Understanding where that budget goes lets you optimize for coverage rather than just spend.

The fast screening tier is the cheapest per-output cost — fractions of a cent per evaluation — but represents the highest total spend because it runs on every output. For a system processing 500,000 outputs per day, fast screening costs $50 to $150 daily. The thorough evaluation tier costs more per evaluation but runs on fewer outputs. At 3% sampling, the daily cost is $60 to $150 in judge inference. The comprehensive adversarial testing tier is the most expensive per run — $500 to $5,000 per full test cycle depending on the size of the adversarial library and the models used — but runs infrequently enough that the monthly cost is manageable.

The hidden cost is not inference. It is people. Safety evaluation requires human expertise to interpret results, update adversarial libraries, respond to findings, and maintain compliance documentation. A dedicated safety evaluator — someone who reviews automated results, manages the red-teaming program, and maintains regulatory documentation — costs $150,000 to $200,000 per year in fully loaded compensation. For large organizations, this scales to a safety evaluation team of three to five people. This human cost is the largest component of the safety evaluation budget, and it is the component that teams most frequently underestimate.

## Building the Compliance Record

Every safety evaluation run, every red-team exercise, every identified risk, and every mitigation applied must be documented in a format that an auditor can review. This documentation is not a burden added on top of safety evaluation. It is one of the primary outputs of the safety evaluation program.

The compliance record includes four elements. The methodology document describes how safety evaluation is conducted — the three-tier pipeline, the judge configurations, the adversarial testing approach, the cadences. This document is updated whenever the methodology changes, with a changelog that shows what changed and why. The results archive stores the output of every safety evaluation run — aggregate scores, violation counts by category, trend data, and representative examples of detected violations. The risk register lists every safety risk identified through evaluation, its severity, the mitigation applied, and its current status. The incident log records every safety incident — outputs that caused real harm or were flagged by users — along with the root cause analysis and the remediation taken.

Together, these four elements tell the story that regulators want to hear: you have a system, you run it regularly, you find real problems, you fix them, and you can prove it. The teams that build this documentation practice into their operating rhythm from the beginning spend two to three hours per week maintaining it. The teams that try to reconstruct it retroactively before an audit spend months.

## Safety Eval as a Quality Signal

The most sophisticated teams have stopped thinking of safety evaluation as a separate compliance function and started treating it as an integral quality dimension. When your safety eval detects a subtle bias pattern — the model consistently giving more cautious advice to users with certain names — that is not just a safety finding. It is a quality finding that affects user experience, trust, and retention. When your adversarial testing discovers that the model can be prompted to ignore its system instructions under certain conditions, that is not just a security vulnerability. It is a reliability problem that affects every user, not just adversarial ones.

Integrating safety evaluation into the broader quality evaluation framework means safety metrics appear on the same dashboards as accuracy, relevance, and tone metrics. Safety regressions trigger the same alert workflows as quality regressions. Safety findings feed into the same error analysis sessions as quality findings. This integration has a practical benefit: it prevents safety evaluation from becoming an isolated program that the safety team runs and nobody else pays attention to. When safety is a dimension on the quality dashboard that the entire product team reviews weekly, it gets the same attention and resources as every other quality dimension.

The next subchapter addresses the coordination challenge that emerges when multiple product teams need to share safety evaluation infrastructure, quality evaluation pipelines, and compliance processes while maintaining autonomy over their own product-specific criteria.
