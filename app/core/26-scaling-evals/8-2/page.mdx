# 8.2 — Outcome Attribution: Connecting AI Outputs to Business KPIs

The AI produced a product recommendation. The user clicked on it. The user bought the product three days later. Did the AI cause that purchase? The user might have bought it anyway. The user might have been influenced by an email campaign that ran the same week. The user might have seen the product on social media and the AI recommendation simply reminded them. Attribution — determining which outcomes the AI actually caused — is the central challenge of outcome-level evaluation, and most teams get it wrong in ways that either overstate or understate the AI's real impact.

## Why Attribution Is Hard

AI systems operate within complex environments where many factors influence outcomes simultaneously. A customer support AI resolves a ticket, but the customer's satisfaction depends on the resolution speed, the accuracy of the answer, the emotional tone of the interaction, the customer's prior experiences, and whether the underlying product issue was actually fixed. Isolating the AI's contribution from this web of factors requires analytical discipline that most teams don't invest in.

The attribution challenge has three dimensions. **Temporal distance** is the first — outcomes may occur hours, days, or weeks after the AI interaction, and the longer the gap, the more intervening factors muddy the relationship. **Multi-touch complexity** is the second — a user may interact with the AI multiple times, plus other channels, before the outcome occurs. **Counterfactual uncertainty** is the third — you can never observe what would have happened without the AI, only what did happen with it. Each dimension alone is solvable. Together, they create a measurement problem that has no perfect answer, only progressively better approximations.

## Attribution Model Selection

Three attribution approaches are practical for AI evaluation at scale, and choosing among them requires understanding what you're willing to trade off between accuracy, implementation effort, and organizational disruption.

**Last-touch attribution** credits the AI for outcomes that occur within a defined window after an AI interaction, regardless of other factors. If a user buys a product within 24 hours of an AI recommendation, the recommendation gets credit. This is the simplest approach to implement — you need only a timestamp join between AI interaction logs and outcome events. The weakness is systematic overcounting. Every outcome within the window is attributed to the AI, even when other factors were the primary driver. A user who received an AI recommendation and a promotional email on the same day generates an attributed conversion for both systems, and the sum of attributions across all channels exceeds actual conversions by 30-60% at most organizations.

Despite its crudeness, last-touch attribution is the right starting point for most teams because it answers the directional question — are outcomes happening after AI interactions? — without requiring cross-system integration. Teams that start with last-touch and graduate to more sophisticated models within six months get further than teams that delay all attribution measurement while planning a perfect multi-touch system.

**Multi-touch attribution** distributes credit across all touchpoints that contributed to an outcome. If a user saw an AI recommendation, then received an email, then purchased, credit is shared between the AI and the email campaign according to a weighting model. Linear attribution splits credit equally across all touchpoints. Time-decay attribution gives more credit to touchpoints closer to the conversion. Position-based attribution gives 40% credit to the first touch, 40% to the last touch, and distributes 20% among middle touches.

The accuracy improvement over last-touch is meaningful but comes at significant implementation cost. Multi-touch attribution requires integration with all channel tracking systems — marketing automation, CRM, email platforms, push notification services — so that every touchpoint is visible in a unified timeline. Most AI teams don't own these integrations, and building them requires partnership with marketing and analytics teams who have their own attribution needs and methodologies.

**Holdout attribution** randomly withholds the AI from a small percentage of eligible interactions and compares outcomes between the AI group and the holdout group. This is the most rigorous method, approximating a controlled experiment. The difference in outcome rates between the AI group and the holdout group is the AI's causal impact, free of selection bias and confounding. The challenge is organizational — deliberately delivering a worse experience to the holdout group requires executive buy-in, legal review in regulated industries, and product team alignment.

## Selection Criteria for Attribution Models

Choosing the right attribution model depends on four factors. First, decision stakes: if you're reporting AI impact to investors or using it to justify a $2 million infrastructure investment, you need holdout attribution or something similarly rigorous. If you're monitoring trends for weekly team reviews, last-touch is sufficient. Second, technical maturity: multi-touch attribution requires cross-system data integration that many teams lack. Don't choose a model your infrastructure can't support. Third, outcome time horizon: products with instant outcomes like click-through allow simpler attribution. Products with week-long conversion cycles need models that handle temporal complexity. Fourth, organizational appetite for holdouts: some organizations accept experimentation culture readily; others consider withholding features from users unacceptable.

The most common mistake is choosing too sophisticated a model too early. A team that spends four months building multi-touch attribution before they have any outcome measurement at all would have been better served by shipping last-touch attribution in two weeks and learning from the directional signal while building something better.

## Building the Attribution Pipeline

Outcome attribution requires a data pipeline that connects AI system logs to business event logs. The pipeline has four stages. First, every AI output is tagged with a unique interaction identifier that follows the user through subsequent events. Second, downstream systems — payment processors, CRM platforms, analytics tools — record business events tagged with the same identifier or a linkable user identifier. Third, a join process connects AI interactions to their downstream outcomes across the temporal gap. Fourth, an analysis layer computes attribution metrics.

This pipeline is not trivial to build. It requires cooperation between the AI team, the data engineering team, the analytics team, and the teams that own downstream business systems. Each team must agree on identifier schemas, event formats, and retention policies. The identifier consistency problem alone consumes weeks of engineering time — if the AI system uses session IDs and the payment system uses customer IDs and the CRM uses account IDs, the pipeline needs a resolution layer that maps between all three.

In practice, building the attribution pipeline is a three-to-six-month project for most organizations, and maintaining it is an ongoing operational commitment. Data formats change, new systems are onboarded, identifier mappings break when upstream teams update their schemas. The team that treats the attribution pipeline as a "build once and forget" project discovers within a quarter that their attribution data has gaps.

## Multi-Touch Complexity at Scale

Multi-touch attribution becomes exponentially harder as AI systems are embedded across more customer touchpoints. A user shopping on an e-commerce platform might encounter AI-powered search results, an AI chatbot that answers a product question, AI-generated product descriptions, and an AI recommendation widget — all in a single session. The user then receives an AI-personalized email the next day and returns to purchase. Five AI touchpoints, one conversion. How do you distribute credit?

The naive approach — divide equally — tells you nothing useful. The sophisticated approach — model the causal contribution of each touchpoint using behavioral data — requires enough conversion volume per touchpoint combination to estimate effects reliably. For a platform with five AI touchpoints, there are thirty-two possible combinations of touchpoints a user might encounter. Estimating the marginal contribution of each touchpoint within each combination requires sample sizes that only the largest platforms generate.

The practical middle ground is to define attribution at the system level rather than the touchpoint level. Instead of asking "how much credit does each AI touchpoint get for this conversion," ask "what is the overall lift from having AI in the customer journey versus not?" This system-level question can be answered with a holdout experiment that disables all AI features for a small user group, and the answer — overall AI lift — is the metric that matters most for investment decisions.

## Attribution Windows

The attribution window — how long after an AI interaction you continue looking for outcomes — depends on the product and the business cycle. For real-time products like search and customer support, windows are short: minutes to hours. For recommendation systems, windows extend to days or weeks. For B2B products where sales cycles are long, windows may stretch to months.

Setting the window too short misses real outcomes. Setting it too long introduces noise — outcomes attributed to the AI that were actually caused by other factors. The right window is determined empirically: analyze the distribution of time between AI interaction and outcome event, and set the window to capture 90-95% of genuine AI-influenced outcomes while minimizing false attribution from distant events. Most teams find that 80% of genuine post-AI outcomes cluster within a characteristic time window that is much shorter than the theoretical maximum. For e-commerce recommendations, 80% of AI-attributed purchases occur within 48 hours, even though occasional purchases trickle in for weeks afterward.

## From Attribution to Evaluation

Once you have attribution data, you can compute outcome-level evaluation metrics. AI-attributed conversion rate measures what percentage of AI interactions lead to a desired outcome within the attribution window. AI-attributed resolution rate measures what percentage of customer support interactions end in resolution without escalation. AI lift measures the improvement in outcome rates when AI is present compared to a holdout or historical baseline.

These metrics should be tracked at the same granularity as output quality metrics — by use case, by user segment, by model version, by time period. The goal is to detect not just whether the AI is driving good outcomes overall, but whether specific changes — model updates, prompt modifications, feature additions — improve or degrade outcomes. A model migration from Claude Opus 4.5 to Claude Opus 4.6 might improve output quality scores across the board while degrading outcomes for a specific user segment because the new model's response style doesn't match that segment's expectations. Without segmented outcome attribution, this regression is invisible.

## Attribution Maturity Levels

Most organizations progress through three maturity levels in attribution. Level one is presence attribution: tracking whether an AI interaction occurred before a desired outcome, with no attempt to separate causation from correlation. This takes days to implement and provides directional signal. Level two is controlled attribution: maintaining a holdout group to estimate causal impact, with last-touch or simple multi-touch models for per-interaction attribution. This takes weeks to months to implement and provides reliable aggregate impact measurement. Level three is modeled attribution: using statistical models to estimate per-interaction causal contribution in multi-touch journeys. This takes months to build and requires data science expertise, but provides the granularity needed for optimizing individual AI components.

Move through these levels sequentially. Skipping to level three without establishing level one first means you are building complex models without the foundational data infrastructure to feed them.

Not all outcomes are measurable immediately. The next subchapter covers the specific challenges of delayed outcome evaluation — measuring impact that manifests days or weeks after the AI interaction.
