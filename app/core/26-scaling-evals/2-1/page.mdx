# 2.1 — The Mathematics of Coverage: Why Evaluating Everything Is Economically Impossible

At a thousand requests per day, evaluating every output with an LLM judge costs roughly forty dollars. At a hundred thousand, it costs four thousand. At ten million, it costs four hundred thousand dollars — per day. These numbers are not hypothetical. They are the direct consequence of sending every production output through a frontier model judge at current 2026 pricing. And they represent the single most important economic reality that every team scaling evaluation must confront: **full-coverage evaluation** does not become expensive gradually. It becomes impossible suddenly, at a traffic threshold that arrives sooner than anyone expects.

The math is unforgiving. A GPT-5-class judge call that evaluates a single output — reading the prompt, reading the response, applying a rubric, producing a structured score — consumes roughly a thousand to two thousand tokens of input and two hundred to five hundred tokens of output. At current pricing in early 2026, that works out to somewhere between two and six cents per evaluation, depending on the model, the rubric complexity, and whether you use a frontier judge or a mid-tier one. Call it four cents as a reasonable average for a serious evaluation that checks multiple quality dimensions. Four cents is invisible at small scale. It is catastrophic at large scale. And the transition from invisible to catastrophic happens within a single order-of-magnitude traffic increase.

## The Linear Cost Trap

The fundamental problem is that evaluation cost scales linearly with traffic when you evaluate every output. Your production system may benefit from all kinds of efficiencies at scale — batching, caching, connection pooling, hardware amortization — but your evaluation system gets none of those benefits, because every output is unique and requires its own judge call. There is no way to batch-evaluate a thousand different customer support responses into a single judge call. Each one needs its own assessment.

This creates what experienced teams call **the linear cost trap**. Your product revenue might grow sublinearly with traffic — ten times the users doesn't mean ten times the revenue, because not all users convert or pay equally. Your infrastructure cost might grow sublinearly with traffic — thanks to caching, CDNs, and economies of scale in compute. But your evaluation cost grows linearly, or sometimes superlinearly if you're evaluating along multiple quality dimensions per output. The result is that evaluation's share of your total cost of operations increases as you scale, eventually becoming the dominant cost if you don't change your approach.

A B2B document processing company discovered this in late 2025 when it grew from processing fifty thousand documents per month to five hundred thousand after signing three enterprise contracts. At fifty thousand documents, their eval pipeline — which ran every output through a Claude Sonnet 4.5 judge checking four quality dimensions — cost roughly six thousand dollars per month. Manageable. At five hundred thousand documents, the same pipeline would have cost sixty thousand per month. The company's entire AI infrastructure budget, including model inference, was forty-five thousand per month. The evaluation would have cost more than the actual product. They paused the pipeline in a panic and spent six weeks redesigning their sampling strategy, during which time they had no systematic quality measurement for their highest-value customers.

## Why Full Coverage Is Not Just Expensive but Unnecessary

Here is the uncomfortable truth that makes the linear cost trap solvable: evaluating every output is not just economically impossible — it is statistically unnecessary. Most teams that insist on full coverage do so because of an intuition that feels right but is mathematically wrong. The intuition is: "If we only evaluate a sample, we might miss problems." The mathematical reality is: you can detect meaningful quality changes with a tiny fraction of your total traffic, as long as you sample correctly.

The core principle is statistical power. If your system produces ten million outputs per day and quality degrades by two percentage points — from ninety-two percent acceptable to ninety percent acceptable — you do not need to evaluate all ten million outputs to detect that degradation. You need roughly twenty-five hundred randomly sampled evaluations to detect a two-percentage-point shift with ninety-five percent confidence and eighty percent statistical power. Twenty-five hundred out of ten million is 0.025 percent. Not two percent. Not one percent. Twenty-five thousandths of one percent.

This is not an approximation. It comes from the mathematics of proportion testing, and the sample size depends on three variables: the size of the quality change you want to detect (your sensitivity), the confidence level you require (how sure you want to be that a detected change is real), and the baseline quality level (the closer you are to fifty percent, the larger the sample you need; the closer you are to ninety-five percent, the smaller). For most production AI systems with baseline quality above eighty-five percent, a sample of two thousand to five thousand evaluations per day is sufficient to detect any quality shift large enough to matter — regardless of whether your total traffic is a hundred thousand or a hundred million.

The implication is profound. At ten million daily requests, you don't need ten million evaluations at four cents each. You need five thousand evaluations at four cents each. That's two hundred dollars per day, not four hundred thousand. The gap between full coverage and intelligent sampling is not a matter of modest savings. It is the difference between evaluation being economically viable and evaluation being financially ruinous.

## The Coverage-Cost Curve

The relationship between evaluation coverage and quality signal follows a sharply diminishing curve. The first one percent of coverage captures roughly eighty percent of the quality signal you need for aggregate monitoring. The next nine percent captures another fifteen percent of signal. The remaining ninety percent of coverage — going from ten percent to full evaluation — captures the final five percent of signal while multiplying your cost by ten.

This curve has a name in operations research: **the Pareto frontier of evaluation**, and it explains why teams that evaluate intelligently at one percent often have better quality visibility than teams that evaluate everything. The team evaluating one percent can afford to run deeper evaluations — multiple quality dimensions, pairwise comparisons, multi-judge consensus — because they have budget headroom. The team evaluating everything has to cut corners on each individual evaluation to stay within budget, resulting in shallow assessments that catch fewer real problems despite examining every output.

A healthcare AI company illustrated this tradeoff in 2025. Its original pipeline evaluated one hundred percent of outputs with a single-dimension judge call — a pass-fail check on factual accuracy. Cost: twenty-two thousand dollars per month. After redesigning around strategic sampling, the company evaluated three percent of outputs with a five-dimension judge rubric covering accuracy, safety, completeness, citation quality, and tone. Cost: twelve thousand dollars per month. The three-percent pipeline caught more real quality issues per month than the one-hundred-percent pipeline had, because the deeper rubric surfaced problems that a single pass-fail check missed entirely. The team reduced cost by forty-five percent and improved detection capability simultaneously. This is the counterintuitive power of the coverage-cost curve: spending less on more can outperform spending more on less.

## The Aggregation Threshold

There is a minimum number of evaluations below which aggregate quality metrics become meaningless, and understanding this threshold prevents a common mistake: cutting your sample size too aggressively in pursuit of cost savings.

The threshold depends on what you're measuring and how you're measuring it. For a binary quality metric — acceptable versus unacceptable — you need roughly four hundred evaluations to estimate the true quality rate within plus or minus five percentage points at ninety-five percent confidence. That means if your true quality is ninety percent, four hundred evaluations will give you an estimate between eighty-five and ninety-five percent. If you need tighter precision — plus or minus two points — you need roughly twenty-five hundred evaluations. If you need to detect a two-point shift between today and yesterday, you need roughly twenty-five hundred evaluations on each day.

For continuous quality scores — like a one-to-five rubric rating — the sample size requirements are somewhat different but in the same order of magnitude. A few hundred evaluations give you a reliable average. A few thousand let you detect subtle shifts.

The practical rule of thumb for production systems is this: you need a minimum of five hundred to one thousand evaluations per segment per measurement period. If you slice your traffic into ten segments (by use case, user tier, or language) and measure quality daily, that means five thousand to ten thousand evaluations per day. At four cents per evaluation, that's two hundred to four hundred dollars per day. At most scales, this is a rounding error in your AI infrastructure budget. The trap is not that intelligent sampling is expensive. The trap is that full-coverage evaluation is so expensive that it prevents you from evaluating at all, which is far worse than evaluating a well-chosen sample.

## When Full Coverage Is Actually Required

There are narrow scenarios where evaluating every output is both necessary and justified, and pretending otherwise would be dishonest. The key variable is consequence asymmetry. When a single undetected bad output can cause catastrophic harm — regulatory penalty, patient injury, financial fraud — and when the cost of that harm vastly exceeds the cost of evaluation, full coverage makes economic sense.

Medical AI outputs that directly inform treatment decisions. Financial AI outputs that execute trades or authorize payments. Legal AI outputs that generate binding contract language. Safety-critical AI outputs that control physical systems. In these domains, the cost of a missed bad output is measured in lawsuits, regulatory actions, or physical harm — costs that dwarf even four hundred thousand dollars per day in evaluation spend.

But even in these domains, full coverage doesn't mean running a single monolithic judge over every output. It means building a tiered evaluation architecture. The first tier is a cheap, fast classifier — often a small fine-tuned model or a set of deterministic rules — that screens every output for obvious failures. This tier costs fractions of a cent per evaluation and catches the most egregious problems. The second tier is a mid-weight LLM judge that evaluates outputs the first tier flags as uncertain. The third tier is a full frontier-model evaluation, possibly with human review, for the highest-risk outputs. This tiered approach achieves functional full coverage while concentrating expensive evaluation where it matters most. We'll explore tiered architectures in depth in Chapter 3.

## The Mindset Shift: From Census to Survey

The transition from full-coverage evaluation to strategic sampling requires a mindset shift that many engineering teams resist. Engineers are trained to be thorough. Leaving ninety-nine percent of outputs unevaluated feels reckless, the same way a QA engineer would feel reckless shipping code with ninety-nine percent of test cases removed. But the analogy is misleading. In software testing, each test case exercises a different code path. In production evaluation, most outputs are routine — they exercise the same model capabilities on similar inputs and produce similar quality levels. Evaluating the ten-millionth customer support response about password resets tells you almost nothing you didn't already know from evaluating the first thousand.

The better analogy is public health surveillance. The CDC doesn't test every person in the country for the flu. It monitors a network of sentinel sites — hospitals, clinics, and labs that report cases from a representative sample of the population. That sample is tiny relative to the total population, but it's enough to detect outbreaks, track trends, and trigger responses. Your evaluation system should work the same way. You're not trying to grade every output. You're trying to detect quality shifts, characterize failure modes, and trigger interventions. Those goals require statistical rigor in your sampling, not brute-force coverage.

This mindset shift is what separates teams that scale evaluation successfully from teams that either bankrupt their eval budget trying to evaluate everything or give up and evaluate nothing. The teams that succeed recognize that the question is not "how do we evaluate every output?" but "how do we learn the most about our system's quality for every dollar we spend on evaluation?" That question leads to sampling strategies, not coverage maximization.

## The Economics of Strategic Ignorance

Strategic sampling means accepting that some individual outputs will go unevaluated. This is not a failure of your evaluation system. It is a feature. **Strategic ignorance** — deliberately not evaluating certain outputs because the expected cost of evaluation exceeds the expected value of the information — is a rational allocation of scarce evaluation resources.

The expected value of evaluating a single output is the probability that the evaluation reveals a problem, multiplied by the value of detecting that problem. For a routine output in a well-understood category with stable quality, the probability of revealing a new problem is low. The value of confirming what you already know — that routine outputs in this category are fine — is near zero. Evaluating that output spends real money to learn nothing. For an output in a high-risk category, from a new model version, or matching a pattern associated with previous failures, the probability of revealing a problem is higher and the value of detection is much greater. Evaluating that output is a high-return investment.

Strategic ignorance is not laziness. It is the discipline to allocate evaluation resources where they produce the highest return. Every eval dollar spent confirming that routine outputs are still routine is a dollar not spent detecting the rare failure in a high-stakes category. The teams that understand this principle build evaluation systems that are both cheaper and more effective than teams that pursue comprehensive coverage. They evaluate less, but they know more.

The shift from full coverage to strategic sampling opens a practical question that every team must answer: where do you start? The simplest, most immediately deployable sampling strategy is the one most teams should implement first and many teams never move beyond. The next subchapter covers random sampling — the baseline approach that provides surprisingly strong quality signal with almost no implementation complexity, and the hidden dangers of relying on it alone.
