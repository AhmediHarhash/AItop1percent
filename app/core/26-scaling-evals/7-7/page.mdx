# 7.7 — The Economics of Human Review: Cost Per Review, Budget Allocation, and ROI

Human review is the most expensive component of evaluation at scale, and the most underinvested. Most teams either spend too little — getting unreliable signal from undertrained reviewers — or spend too much — reviewing outputs that automated evaluation could handle. The economics of human review demand precision, not generosity.

## Cost Per Review

The cost of a single human review varies by an order of magnitude depending on the reviewer type and task complexity. External general annotation platforms charge $0.50 to $2.00 per review for simple binary tasks — "is this output safe or unsafe?" Internal trained reviewers performing multi-dimensional quality evaluation cost $3 to $8 per review when you account for salary, training, tooling, and management overhead. Internal domain experts evaluating complex outputs — medical advice, legal guidance, financial recommendations — cost $8 to $15 per review.

These costs are fully loaded. A reviewer earning $30 per hour who completes twenty-five reviews per hour has a direct labor cost of $1.20 per review. But add management time, tooling licenses, calibration sessions, QA of reviewer work, and the overhead of the review operations program, and the true cost doubles or triples. Teams that budget only for direct reviewer labor consistently underestimate total review program cost by 40-60%.

## Budget Allocation Framework

At maturity levels 3 and 4, human review typically consumes 15-25% of the total evaluation budget, with the remainder going to automated evaluation infrastructure — LLM judge API costs, compute, tooling, and engineering time. This ratio is not fixed. Teams with well-calibrated automated judges can push human review below 15%. Teams in highly regulated industries or with novel, poorly understood use cases may need 30% or more.

The human review budget divides into three categories. Calibration reviews — the work described in 7.6 that keeps automated judges accurate — should consume 40-50% of the human review budget. These are the highest-leverage reviews, and they should never be cut to save money. Quality reviews — direct assessment of output quality for reporting and incident detection — consume 30-40%. Edge case and escalation reviews — outputs that automated systems can't handle — consume the remaining 10-20%.

## The ROI Calculation

Human review ROI is not "cost per review divided by quality issues found." That metric incentivizes reviewers to find problems in every output, which distorts both routing and evaluation. The correct ROI framework accounts for two value streams.

The first is calibration value — the improvement in automated judge accuracy attributable to human review data. If your automated judges process 500,000 evaluations per month and human calibration data improves judge accuracy by 3 percentage points, those 15,000 additional correct judgments have a value that depends on what decisions they inform. For a product where each quality incident costs $500 in customer impact, 15,000 better judgments preventing even 1% of potential incidents yields $75,000 in avoided costs against a $15,000 review spend.

The second is direct detection value — quality issues found by human reviewers that automated evaluation missed entirely. These are typically edge cases, novel failure modes, or subtle quality issues that judges aren't yet calibrated for. Each detection has high individual value because it represents a blind spot in your automated system.

## Multimodal Review Economics

Human review of non-text outputs — images, audio, video — costs significantly more than text review. Image quality evaluation takes two to three times as long per output because reviewers must assess visual coherence, accuracy, cultural sensitivity, and brand alignment. Audio review requires real-time listening, making each review take at minimum the duration of the audio clip plus assessment time. Video combines both, making it the most expensive modality to review.

For multimodal products, human review costs three to five times more per output than text. This makes efficient routing even more critical for multimodal systems — you absolutely cannot afford to send routine multimodal outputs to human review. The review router must be more aggressive about filtering, and automated evaluation for non-text modalities must be good enough to handle routine quality assessment without human involvement.

## Cost Optimization Levers

Four levers control human review cost without reducing signal quality. Better routing reduces unnecessary reviews — every output that reaches human review but didn't need to is wasted budget. Better calibration reduces re-reviews — when reviewers are well-calibrated, you need fewer double-reviews for quality assurance of the reviews themselves. Better tooling reduces time per review — a well-designed review interface with pre-populated context, inline automated judge scores for reference, and streamlined input methods can reduce review time by 30-50%. And automation of simple binary decisions — tasks where a lightweight classifier achieves acceptable agreement with human judgment — removes entire review categories from the human queue.

The interaction between these levers matters. Investing $2,000 per month in better review tooling that saves fifteen seconds per review, across 5,000 monthly reviews, saves twenty-one hours of reviewer time — roughly $630 per month in direct labor savings. The payback period is about three months. These small efficiency gains compound across the review program.

The future of human review at scale is not more humans reviewing more outputs. It is a fundamentally different architecture — AI systems evaluating AI systems, with humans providing the calibration and oversight that keep the automated layer trustworthy.
