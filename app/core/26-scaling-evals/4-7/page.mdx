# 4.7 — The Feedback Loop: How Online Eval Results Refresh Offline Golden Sets

Your golden set is only as current as the last time you updated it. If your production traffic has evolved but your test set hasn't, you are testing against yesterday's reality. And testing against yesterday's reality produces a specific, dangerous outcome: models that score beautifully on your offline evaluation and fail silently in production. The golden set says the model is ready. The users say it isn't. The gap between those two verdicts is not a mystery — it is a maintenance failure.

Every golden set starts accurate. When a team first builds their evaluation suite, they draw examples from real production data, label them carefully with human reviewers, and validate that the golden set reflects the actual distribution of queries and quality expectations. But production doesn't stand still. Users find new ways to use the product. New features launch. Seasonal patterns shift the query distribution. Edge cases emerge that nobody anticipated during initial labeling. Within three to six months, a golden set that was perfectly representative becomes a historical artifact — still useful for catching the problems of six months ago, unable to detect the problems of today.

## The Staleness Problem

**Golden set staleness** is the divergence between the examples in your offline evaluation suite and the actual distribution of queries and outputs your system handles in production. It is universal, inevitable, and detectable — if you look for it. The teams that treat their golden set as a finished artifact discover staleness through failure. The teams that treat it as a living resource detect staleness before it causes harm.

Staleness manifests in three ways. The first is coverage gaps. Your product added a feature for multi-turn conversation summarization, but your golden set contains no summarization examples. Your model is being evaluated on the old use cases it already handles well, while the new use case — the one most likely to have quality issues — goes unmonitored.

The second manifestation is distribution mismatch. Your golden set has thirty percent of examples in the "product comparison" category because that was the most common query type when the set was built. But your user base has shifted, and product comparisons now account for only twelve percent of traffic, while "troubleshooting" queries have grown from eight percent to thirty-one percent. Your evaluation overweights a declining use case and underweights a growing one. The model could degrade on troubleshooting queries without moving your aggregate evaluation score.

The third manifestation is difficulty drift. The problems your users bring to your system tend to get harder over time. Early adopters ask simple questions. As the product matures, users trust it with more complex, nuanced, and ambiguous requests. If your golden set reflects the simpler queries of early adoption, it systematically overestimates the model's ability to handle today's harder traffic.

## The Golden Set Refresh Loop

**The Golden Set Refresh Loop** is a systematic process for using online evaluation results to continuously update your offline golden set. It works by treating online evaluation as a discovery engine — finding the examples that your golden set is missing and feeding them back into the offline suite.

The loop has five stages. First, online evaluation runs on production traffic, scoring a sample of outputs using your LLM judges, heuristic scorers, and any other real-time evaluation mechanisms. Second, the evaluation pipeline flags specific outputs for golden set consideration based on defined criteria. Third, human reviewers label the flagged outputs. Fourth, the labeled outputs are reviewed for inclusion in the golden set. Fifth, the golden set is versioned, updated, and validated. Then the cycle repeats.

The mechanism is straightforward. The design decisions — what to flag, what to include, how fast to grow, how to version — are where teams succeed or fail.

## What to Flag: The Four Signals

Not every production output belongs in your golden set. Flagging everything creates a labeling bottleneck and a bloated evaluation suite. Flagging nothing leaves your golden set static. The right approach flags outputs that carry the highest information value for your evaluation system.

The first signal is **judge disagreement**. When you run multiple judges or multiple scoring dimensions, outputs where judges disagree represent the boundary cases that your evaluation system is least certain about. If one judge scores an output as a four and another scores it as a two, that output is interesting — it lives in the ambiguous zone where quality is genuinely hard to assess. These boundary cases are precisely the ones that stress-test your golden set's coverage. If your golden set doesn't contain examples in this ambiguous zone, it can't validate whether the model handles them well.

The second signal is **novel patterns**. Online evaluation should track the query types and output patterns it encounters and compare them against the distribution in the current golden set. When a query arrives that doesn't fit any existing category — a new use case, a new query format, a new domain of questions — it should be flagged. These novel patterns represent the coverage gaps that staleness creates. A query type that didn't exist when the golden set was built needs to be represented now.

The third signal is **quality failures on new ground**. When the model produces an output that the online eval system scores as a failure, and the failure pattern doesn't match any existing failure mode in your golden set, that output is a candidate. It represents a failure mode that your offline evaluation can't detect because no examples of it exist in the test set. Adding it ensures that future offline evaluation can catch this problem if it recurs.

The fourth signal is **marginal passes**. Outputs that barely clear your quality threshold — scoring just above the pass/fail line — are often more informative than outputs that clearly pass or clearly fail. They represent the zone where your model is most vulnerable. A small degradation in model quality would push these marginal outputs from pass to fail. Including marginal passes in your golden set gives your offline evaluation sensitivity to subtle quality changes that would be invisible if the golden set only contained clear successes and clear failures.

## What Not to Add

The flip side of knowing what to add is knowing what to leave out. An undisciplined golden set refresh process produces a bloated evaluation suite that is expensive to run, slow to execute, and no more informative than a smaller, well-curated set.

Don't add outputs that duplicate existing test cases. If your golden set already has twelve examples of product comparison queries that score in the four-to-five range, adding a thirteenth similar example provides near-zero incremental value. The marginal information gain of another redundant example is negligible, but the marginal cost of labeling, storing, and evaluating it is real.

Don't add outputs where quality is unambiguous and well-represented. If the model clearly succeeded on a common query type, and your golden set already has ample coverage of that success pattern, the output tells you nothing you don't already know. Golden set space is a finite resource — not because of storage costs, but because of labeling costs and evaluation runtime.

Don't add outputs that are too specific to a single user interaction. An output that failed because a particular user provided highly unusual context — context that is unlikely to recur — is not a useful golden set example. Golden set examples should represent patterns, not individual incidents. If five different users trigger the same failure mode, that's a pattern worth capturing. If one user triggered a failure through a unique sequence of inputs, that's an incident worth investigating but not worth enshrining in the golden set.

The general principle is that every golden set addition should increase the evaluation suite's ability to detect problems across your real traffic distribution. If an addition only helps detect a problem that has already happened once and is unlikely to recur, it's not worth the maintenance cost.

## The Deliberate Growth Principle

Golden sets should grow slowly and deliberately, not rapidly and reactively. This is counterintuitive — if you're discovering gaps in your golden set, shouldn't you fix them as fast as possible? The answer is no, because golden set quality depends on curation, and curation takes time.

Every example added to the golden set needs human labels from multiple reviewers with high agreement. If you rush labeling — sending fifty examples per week to a single reviewer who labels them in thirty minutes — you get noisy labels that degrade your golden set's reliability. If the human labels on a golden set example are uncertain, the example can't serve as ground truth. It becomes a source of noise rather than signal.

The practical cadence that most mature teams converge on: flag candidates continuously from online evaluation results, batch them for review monthly, and execute a golden set refresh quarterly. The monthly review identifies which candidates have sufficient volume and variety to warrant inclusion. The quarterly refresh executes the actual golden set update — adding new examples, removing obsolete ones, and relabeling any existing examples where the quality criteria have evolved.

Between refreshes, the golden set is frozen. This gives you a stable baseline for evaluation. If you're continuously changing the golden set, you can't distinguish "the model changed" from "the test changed." Quarterly refreshes provide stability for comparison while ensuring the golden set doesn't fall more than ninety days behind production reality.

Teams at very high scale — those handling millions of requests per day with rapidly evolving products — may need monthly refreshes instead of quarterly. But even at high scale, the golden set shouldn't change more often than monthly. More frequent changes make it impossible to track evaluation trends, because every data point in the trend line was measured against a different test set.

## The Labeling Pipeline

Flagged outputs don't enter the golden set automatically. They pass through a labeling pipeline that ensures quality and consistency.

The pipeline starts with deduplication. Online evaluation may flag the same output pattern dozens of times — the same failure mode triggered by different users, the same novel query type from different sessions. Before human labeling begins, cluster the flagged outputs and select representative examples from each cluster. If twenty flagged outputs all represent the same "model hallucinates product specifications" failure mode, you don't need twenty human-labeled examples. You need three to five, chosen to represent the variety within the pattern.

After deduplication, the representative examples go to human reviewers. The labeling process must match the process used for the original golden set — same rubric, same scoring scale, same reviewer pool (or a pool calibrated against the original reviewers). Consistency between the original labels and the new labels is critical. If the new examples are labeled on a different rubric or by reviewers with different standards, the golden set loses internal consistency, and evaluation scores become unreliable.

Each example should be labeled by at least two reviewers independently, with a third reviewer resolving disagreements. This isn't overkill — it's the minimum for ground truth. A golden set example with a single reviewer's label carries that reviewer's individual biases and blind spots. Two agreeing reviewers dramatically reduce the chance of a biased label.

After labeling, a final review step compares the new examples against existing golden set examples in the same category. Do the new examples fill a genuine gap? Do they represent a pattern not already covered? Are the labels consistent with labels on similar existing examples? This review prevents the golden set from accumulating redundant examples over successive refresh cycles.

## Versioning and Diffing

Every golden set update creates a new version. This is not optional. Without versioning, you cannot track how your golden set has changed over time, compare evaluation results across different golden set versions, or roll back to a previous version if a refresh introduced problematic examples.

A golden set version should include a clear diff: which examples were added, which were removed, which were relabeled, and why. The "why" is as important as the "what." An example added because "online eval identified a new failure mode in multi-turn summarization" tells the team something useful about the state of their product and their evaluation coverage. An example added with no rationale is a maintenance burden for future teams who won't understand why it's there.

Removal is as important as addition. Each quarterly refresh should evaluate existing examples for continued relevance. If a use case has been deprecated, examples from that use case no longer serve the golden set. If a failure mode has been permanently fixed through a model change, the failure examples may still be useful as regression tests — but they should be tagged as "regression" so they don't distort the golden set's distribution. A golden set where thirty percent of examples test for a failure mode that was fixed a year ago overweights a historical problem and underweights current risks.

The versioning system should support running evaluation against any historical version. This lets you answer questions like "how does the current model score on the golden set from six months ago?" and "how would the old model have scored on today's golden set?" These cross-version comparisons are powerful diagnostic tools. If the current model scores well on today's golden set but poorly on last year's golden set, you may have lost capabilities that the old golden set tested. If the old model would have scored well on today's golden set, the quality improvements you thought the new model provided may be an artifact of golden set drift rather than genuine improvement.

## Closing the Loop: From Production to Test to Production

The golden set refresh loop creates a virtuous cycle when it works correctly. Online evaluation identifies quality issues and novel patterns in production. Those findings flow into the golden set. The updated golden set catches problems earlier in the development cycle — during offline evaluation before the model reaches production. Earlier detection means fewer production quality incidents. Fewer production incidents mean more stable online evaluation signals. The system gets better at finding problems, and finding problems earlier makes the system more stable.

The anti-pattern is the broken loop. Online evaluation identifies issues, but nobody reviews the flagged outputs. Or the outputs get reviewed but never make it into the golden set because the quarterly refresh gets deprioritized during a busy sprint. Or the golden set gets updated but the new version never gets deployed to the offline evaluation pipeline because the infrastructure team treats golden set updates as low-priority changes. Any break in the loop causes staleness to accumulate, and staleness causes the offline evaluation to diverge from production reality.

Ownership prevents the broken loop. One person — typically the evaluation engineer or the technical lead of the evaluation team — owns the golden set refresh cadence. That person is responsible for reviewing flagged outputs monthly, coordinating labeling, executing the quarterly refresh, and verifying that the updated golden set is deployed. Without this ownership, the loop breaks within two to three quarters, and the golden set quietly becomes a historical artifact that nobody trusts.

The teams that run this loop well report a specific, measurable benefit: their offline evaluation becomes a better predictor of production quality over time. When the golden set reflects current production reality, passing offline evaluation actually means the model is ready for production. When the golden set is stale, passing offline evaluation means the model handles the old traffic well — which tells you nothing about how it handles the new traffic. The difference is the difference between an evaluation system that prevents quality incidents and one that merely records them after the fact.

All of this online evaluation — the real-time scoring, the progressive rollouts, the golden set refresh, the feedback loops — generates cost. And at scale, that cost needs active management. The next subchapter covers the economics of online evaluation: how to score production traffic without spending more on evaluation than you spend on the production AI itself.
