# 1.2 — The Four Scaling Dimensions: Volume, Variety, Velocity, and Veracity

Eval systems don't break because of traffic alone. They break across four dimensions simultaneously, and teams that scale for only one dimension fail on the other three. A team that builds infrastructure to handle a million evaluations per day but runs them all against a single rubric designed for one use case hasn't scaled evaluation. They've scaled computation. The eval system that survives production growth is the one built to handle more traffic, more use cases, faster release cycles, and shifting definitions of quality — all at the same time, all without human intervention to keep the pieces in sync.

These four dimensions form what practitioners call **The Four V's of Eval Scale**: Volume, Variety, Velocity, and Veracity. Every scaling failure in evaluation can be traced to pressure along one or more of these axes. Understanding them separately is necessary. Understanding how they interact is what actually prepares you for growth.

## Volume: The Raw Throughput Problem

**Volume** is the most obvious dimension: how many evaluations you need to run per unit of time. At a hundred requests per day, you can evaluate every single output. At ten thousand, you start sampling. At a million, your sampling strategy becomes a statistical discipline. At a hundred million, your eval infrastructure is a distributed system in its own right, with its own compute requirements, its own latency SLAs, and its own failure modes.

The scale numbers are not hypothetical. By early 2026, Cursor serves billions of AI code completions daily. Large consumer AI products routinely handle tens of millions of requests per day. Enterprise platforms that serve thousands of business customers aggregate hundreds of millions of AI interactions per month. Even mid-sized B2B companies running AI features across their product suite see volumes in the low millions daily. These aren't edge cases. They're the normal operating scale of AI products that have found product-market fit.

Volume scaling isn't just about running more evaluations. It's about running them fast enough to be useful. An eval that takes six hours to process yesterday's traffic is already stale by the time it completes if your team ships prompt changes every morning. The eval needs to finish before the next change ships, or it's measuring a version of the system that no longer exists. This creates a throughput requirement that compounds: as volume grows, the window in which eval results are still relevant shrinks, because higher volume usually correlates with faster iteration.

The volume problem also creates a cost problem. If you're using an LLM judge — and by 2026 most teams are — every evaluation is an API call with a token cost. Evaluating one percent of a million daily requests is ten thousand judge calls per day. At even modest per-call costs, that adds up fast. Volume scaling forces you to think about eval economics long before you expected to. You need sampling strategies that are statistically valid at lower coverage rates. You need judge models that balance cost and accuracy. You need caching and deduplication to avoid evaluating the same type of output twice. Volume isn't just an infrastructure challenge. It's an economics challenge.

## Variety: The Use Case Explosion

**Variety** is the dimension that most teams underestimate. Your eval system started with one rubric for one use case. Then the product expanded. You added a second AI feature, then a third. You expanded into new languages. You started serving different customer segments with different quality expectations. You added a risk tier where outputs have regulatory implications. Each new variety doesn't just add one more thing to evaluate. It multiplies the evaluation surface.

Consider a B2B SaaS company that starts with an AI writing assistant for marketing copy. The eval rubric measures tone, factual accuracy, and brand alignment. Six months later, the product adds contract summarization for legal teams. The marketing rubric is useless for contracts. Legal summarization requires different criteria: completeness, accuracy of clause identification, preservation of legal meaning, no hallucinated terms. Now you need a second rubric, a second eval dataset, and a second set of human reviewers who understand legal language. Three months after that, the product adds multilingual support. Now every rubric needs language-specific variants. The marketing rubric needs to handle idiom and tone in French, German, and Japanese. The legal rubric needs to handle jurisdiction-specific terminology in each language. Your eval surface just went from one rubric to roughly a dozen, and your product only has two features.

Variety scaling is combinatorial. Use cases multiply by languages, which multiply by customer segments, which multiply by risk tiers. A product with five use cases, four languages, three customer segments, and two risk tiers has a theoretical eval surface of a hundred and twenty distinct evaluation contexts. You don't need to evaluate all of them independently — many share common quality criteria. But you do need to understand which combinations require distinct evaluation and which can share a rubric. That mapping is itself a non-trivial analytical task that most teams never do explicitly.

The organizational challenge of variety is that it requires domain expertise at scale. One person can hold the quality criteria for one use case in their head. Nobody can hold a hundred and twenty evaluation contexts in their head. Variety scaling forces you to externalize quality criteria into structured rubrics, version them, map them to traffic segments, and assign ownership. It forces evaluation to become a managed system rather than personal knowledge. Teams that don't make this transition hit a ceiling where the eval system covers their original use case well but barely touches everything they've added since.

## Velocity: The Release Cadence Problem

**Velocity** is the dimension that turns manageable scaling into a race. It measures how fast your system changes — new prompt versions, model updates, feature additions, configuration changes — and therefore how frequently evaluation must run to keep pace.

At a monthly release cadence, you have weeks to prepare evaluation, run it, analyze results, and gate the release. At a weekly cadence, that window compresses to days. At a daily cadence, evaluation must be automated enough to run without human intervention and fast enough to complete before the next change ships. By 2026, many teams deploy prompt changes multiple times per day. Some teams run continuous deployment where every committed change is automatically evaluated and promoted or blocked. At that velocity, evaluation isn't something a human triggers. It's a pipeline stage that runs automatically on every change, with clear pass/fail criteria and no manual gate.

Velocity creates a compounding pressure with volume. More frequent releases mean more evaluation runs. More evaluation runs mean more compute, more judge calls, more data storage. If your evaluation takes two hours and you release twice a day, you're spending four hours of compute daily just on evaluation. If each run involves ten thousand judge calls, that's twenty thousand calls per day. The cost and infrastructure requirements scale with the product of volume and velocity, not the sum. Teams that plan for each dimension independently get blindsided when the interaction between them drives costs and latency beyond expectations.

The more subtle velocity problem is rubric staleness. When you release daily, your evaluation criteria need to evolve almost as fast as your product. A prompt change that shifts the model's output format invalidates any rubric that checks for the old format. A new feature addition requires new eval criteria within days, not months. If your eval rubric update process is manual — someone writes new criteria, someone else reviews them, someone implements them in the pipeline — that process becomes a bottleneck at high velocity. The eval system becomes the slowest component in a fast deployment pipeline, and the team starts routing around it. This is how velocity kills eval systems: not by overwhelming them with requests, but by making them irrelevant before they can adapt.

High-velocity teams need eval systems that support rubric versioning, automatic rubric selection based on the change being evaluated, and fast iteration on eval criteria without full pipeline redeployment. They need the eval system to be as agile as the product itself. Any friction in the eval update process translates directly into evaluation gaps during periods of rapid change — which are precisely the periods when evaluation matters most.

## Veracity: The Shifting Ground Truth Problem

**Veracity** is the dimension that makes seasoned eval engineers lose sleep. It asks: is the thing you're measuring still the right thing to measure? Volume, variety, and velocity are infrastructure challenges. Veracity is an epistemological one.

Ground truth changes. What counted as a good output six months ago may not count as a good output today. User expectations shift as they become more sophisticated. Product requirements evolve as the business strategy changes. Regulatory requirements update as new guidance takes effect — the EU AI Act's GPAI Code of Practice, finalized in mid-2025, introduced evaluation requirements that didn't exist a year earlier. Competitive benchmarks move as competitors improve their own AI products. The definition of "good" is not a constant. It's a moving target, and your eval system must move with it or measure against an outdated standard.

Veracity degradation is the hardest scaling failure to detect because the eval system continues to produce high scores. The rubric says accuracy is ninety-three percent. The judge model agrees. The dashboard looks green. But the rubric is measuring accuracy against criteria defined eight months ago, before the product pivoted from generic summarization to financial-specific summarization, before users started expecting citation-level precision, before the regulatory landscape changed. The eval system is producing accurate measurements of an obsolete standard. It's precisely wrong — answering the wrong question with great confidence.

Maintaining veracity at scale requires systematic processes for reviewing and updating evaluation criteria. Quarterly rubric reviews at minimum. User feedback loops that surface changing expectations. Regulatory monitoring that flags when compliance standards shift. Competitive analysis that tracks whether your quality bar is keeping pace with the market. And perhaps most importantly, a cultural acceptance that eval criteria are not permanent. Teams that treat rubrics as fixed artifacts build systems that gradually lose touch with reality. Teams that treat rubrics as living documents build systems that maintain relevance through change.

The veracity problem also affects your eval datasets. A golden dataset that was representative of your traffic six months ago may no longer reflect current usage patterns. New user behaviors, new input types, seasonal shifts in query distribution — all of these erode the representativeness of static eval datasets. If your eval system runs every output against the same five hundred test cases from last year, it's testing the model's performance on a historical distribution, not on what users are actually sending today. Dynamic eval sets that are regularly refreshed from recent production traffic are essential for maintaining veracity, but they require infrastructure to sample, label, review, and integrate new examples on an ongoing basis.

## The Interaction Effects: Why Scaling One Dimension Isn't Enough

The four dimensions don't scale independently. They interact, and the interactions create challenges that are more than additive.

Volume multiplied by variety is the coverage explosion. If you have a million daily requests spread across ten use cases, you need to sample and evaluate across all ten. Your sampling strategy can't just take a random one percent of total traffic — it needs to ensure adequate coverage of each use case, including the low-volume ones that might represent your highest-risk interactions. A legal summarization feature that handles only two thousand requests per day but carries regulatory exposure needs proportionally more eval coverage than a high-volume, low-risk feature. Volume times variety forces you to build stratified sampling, not uniform sampling.

Variety multiplied by velocity is the rubric maintenance explosion. If you have twelve distinct eval rubrics and you release daily, every release potentially requires checking whether any of those twelve rubrics need updating. If you add a new language support every quarter, you add new rubric variants every quarter. If your product adds a new use case every month, you add new eval criteria every month. The maintenance burden grows as the product of how many rubrics you have and how often they need to change. Teams that handle this manually hit a ceiling around ten to fifteen rubrics, beyond which no human can keep the criteria current.

Velocity multiplied by veracity is the drift acceleration problem. Fast release cadence means ground truth can shift rapidly. A team that ships daily prompt changes may shift the output distribution enough in a single week that the rubric criteria are subtly misaligned by Friday. The faster you change the system, the faster the definition of "good" can drift, and the more frequently you need to verify that your eval criteria still match reality. High-velocity teams need automated mechanisms to detect when eval criteria have drifted from the actual product behavior — a second-order monitoring problem that most teams never build.

All four dimensions multiplied together — high volume, high variety, high velocity, and shifting veracity — is the full-scale challenge that enterprise AI teams face in 2026. It's why evaluation at scale is not a bigger version of evaluation at small scale. It's a fundamentally different engineering discipline, closer to building an observability platform than writing a test suite. The teams that treat it as infrastructure from the start are the teams that survive the scaling wall. The teams that treat it as a script that can grow are the teams that hit the wall at speed.

## Mapping Your Own Scaling Pressure

Before you can scale your eval system, you need to understand which dimensions are creating the most pressure for your team right now. Most teams feel the pain but haven't diagnosed which dimension is the root cause.

If your evals are timing out or backing up, you have a volume problem. The solution is infrastructure: distributed compute, parallel pipelines, faster judge models, better sampling. If your evals are covering some use cases well and ignoring others, you have a variety problem. The solution is organizational: rubric management, stratified sampling, per-use-case ownership. If your evals can't keep up with your release cadence, you have a velocity problem. The solution is automation: CI/CD integration, automatic rubric selection, fast eval pipelines that run on every change. If your evals produce high scores but users are still unhappy, you have a veracity problem. The solution is process: regular rubric reviews, ground truth refresh, user feedback integration.

In practice, most growing teams face pressure on all four dimensions at once. But the dimensions don't all reach critical severity at the same time. Understanding which one is your current bottleneck determines where to invest first. Building for all four from the start is ideal but expensive. Building for the one that's about to break first is pragmatic. The Four V's framework gives you the language to diagnose, prioritize, and communicate the scaling challenge to your team and your stakeholders.

Every dimension of eval scale has a cost. The next subchapter quantifies those costs and introduces the framework that tells you whether your evaluation spending is too low to catch problems, too high to sustain, or balanced for the risks your product actually carries.
