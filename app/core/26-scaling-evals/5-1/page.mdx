# 5.1 — The Eval Pipeline as a Distributed System: Architecture Patterns That Scale

An evaluation pipeline is a distributed system. The moment you accept that, you gain access to decades of distributed systems engineering — and you inherit all of its failure modes. Most teams don't start here. They start with a script: pull some outputs from a database, run them through a judge, write the scores back. The script works beautifully at fifty evaluations. It works fine at five hundred. At fifty thousand evaluations per day, the script is a bottleneck that blocks releases, loses results, and silently drops evaluations during peak load. At five hundred thousand, it is a liability. The transition from script to distributed system is not optional for any team that intends to evaluate at production scale. The only question is whether you design that system deliberately or discover its requirements through a series of painful outages.

This subchapter covers the architecture patterns that work for evaluation at scale. Not theoretical distributed systems — the specific patterns that evaluation pipelines demand, shaped by the unique characteristics of eval workloads: variable latency, mixed compute requirements, tolerance for eventual consistency, and the critical need to never lose a result once it's been computed.

## Why Eval Pipelines Become Distributed Systems

A single-machine evaluation script breaks down along four axes simultaneously, and understanding each axis tells you which distributed systems pattern you need.

The first axis is **data source diversity**. Your evaluation pipeline doesn't consume data from one place. It pulls production outputs from your serving layer, golden set examples from your dataset store, human annotations from your labeling platform, and configuration — which judge to use, what rubric to apply, what thresholds to enforce — from your eval management system. Each of these sources has its own availability characteristics, its own latency profile, and its own failure modes. When the labeling platform is down for maintenance, that shouldn't prevent production output evaluation from proceeding. When the dataset store is slow because someone is running a large export, that shouldn't block the nightly regression suite. A single-machine script that queries all four sources sequentially turns any one source's slowness into the entire pipeline's slowness.

The second axis is **evaluation type diversity**. Your pipeline doesn't run one kind of evaluation. It runs heuristic checks that complete in milliseconds, embedding similarity calculations that take seconds, LLM judge calls that take five to thirty seconds, and occasionally human review workflows that take hours. These evaluation types have radically different compute requirements, different latency profiles, and different failure characteristics. An LLM judge call might fail because of API rate limiting. A heuristic check will never fail for that reason. Treating these as a homogeneous workload — running them all through the same execution path with the same retry logic and the same timeout configuration — wastes resources on the fast evaluations and starves the slow ones.

The third axis is **compute target diversity**. Heuristic checks run on cheap CPU instances. Embedding evaluations run on GPU or call embedding APIs. LLM judge evaluations call external APIs with rate limits and variable latency. Self-hosted judge models run on your own GPU infrastructure with capacity constraints. A single-machine script cannot efficiently manage all of these compute targets because the optimal resource allocation for each is different, and contention between them degrades everything.

The fourth axis is **consumer diversity**. Evaluation results don't go to one place. They feed real-time dashboards, populate quality reports, trigger alerts, inform release gates, update trend databases, and flow into feedback loops that retrain evaluation models. Each consumer has different freshness requirements. The release gate needs results within minutes. The weekly report needs completeness more than speed. The trend database needs every result eventually but can tolerate hours of delay. A single pipeline that serves all consumers at the same speed and priority either over-serves the slow consumers or under-serves the fast ones.

When you look at these four axes together, you see a system with multiple inputs, multiple processing types, multiple compute backends, and multiple output destinations — each with independent requirements. That is the definition of a distributed system.

## Pattern One: The Centralized Orchestrator

The simplest distributed eval architecture puts a single orchestrator service at the center. The orchestrator receives evaluation requests, decides how to process each one, dispatches work to specialized workers, collects results, and routes them to consumers. Think of it as the air traffic controller for your evaluation pipeline.

The orchestrator maintains a job queue. When a new evaluation request arrives — a batch of production outputs to evaluate, a golden set run triggered by a deployment, a manual evaluation request from a team member — the orchestrator breaks it into individual evaluation tasks, assigns each task to the appropriate worker pool based on evaluation type, and tracks completion. Heuristic tasks go to the heuristic worker pool. LLM judge tasks go to the judge worker pool. Embedding tasks go to the embedding worker pool. Each pool scales independently based on its queue depth.

The centralized orchestrator pattern works exceptionally well for batch and CI/CD evaluation workloads. When your release pipeline triggers a golden set evaluation, the orchestrator receives one large job, fans it out to workers, monitors progress, handles retries for failed individual evaluations, and reports a single completion status back to the release pipeline. The orchestrator knows the overall state of every job. It can provide accurate progress reporting. It can implement priority logic — a release-blocking evaluation gets priority over a nightly quality report. It can enforce resource limits — no single evaluation job consumes more than forty percent of the judge worker pool, ensuring that other jobs can make progress.

The weakness of the centralized orchestrator is that the orchestrator itself is a single point of failure and a potential bottleneck. If the orchestrator goes down, no new evaluation work gets dispatched. If it falls behind on scheduling, the entire pipeline backs up. Teams that adopt this pattern must invest in orchestrator reliability — redundant instances, leader election, persistent job state that survives restarts. In practice, most teams implement the orchestrator on top of a workflow engine like Temporal, Apache Airflow, or Prefect, which provides the persistence, retry logic, and scheduling infrastructure that a custom implementation would need to build from scratch. By 2026, Temporal has become the most common choice for evaluation orchestration among teams processing more than a hundred thousand evaluations per day, because its durability guarantees and worker model map naturally onto the eval workload pattern.

## Pattern Two: The Event-Driven Pipeline

The event-driven pattern replaces the centralized orchestrator with a set of independent services that react to events. Instead of one service that knows about everything, each service watches for specific events and does its job when triggered. No single component has a global view of the pipeline. Coordination happens through the event stream.

The events that drive an eval pipeline map to natural triggers. A new model deployment triggers a golden set regression run. A production output being logged triggers a sampling decision. A sampling decision that selects an output for evaluation triggers an evaluation task. An evaluation result being stored triggers a threshold check. A threshold violation triggers an alert. Each of these is an independent service that subscribes to the relevant event, performs its function, and publishes the result as a new event.

This pattern shines for production monitoring workloads. When your production system logs millions of outputs per day, you don't want a centralized orchestrator deciding which ones to evaluate. Instead, the sampling service watches the output stream, selects outputs based on the configured sampling strategy, and publishes evaluation-request events for the selected outputs. The evaluation workers subscribe to evaluation-request events, run the evaluations, and publish evaluation-result events. The alerting service subscribes to evaluation-result events, compares scores against thresholds, and publishes alert events when quality drops. Each service scales independently. The sampling service is trivially cheap — it makes a random number decision per output. The evaluation workers are expensive and can be scaled based on the queue depth of pending evaluation requests. The alerting service is cheap and latency-sensitive.

The event-driven pattern's strength is resilience. If the alerting service goes down, evaluations still run — the results accumulate in the event stream, and when alerting recovers, it processes the backlog. If evaluation workers fall behind, the events queue up in the message broker without affecting the production logging or sampling services. No single failure takes down the whole pipeline. The pattern also makes it easy to add new consumers: a new reporting service simply subscribes to evaluation-result events without modifying any existing service.

The weakness is observability. No single component knows the end-to-end state of a particular evaluation. Tracing an output from production logging through sampling through evaluation through alerting requires correlating events across multiple services. Debugging failures means searching through distributed logs. Answering "how many outputs from yesterday's deployment have been evaluated?" requires querying multiple services or building a separate tracking layer. Teams that adopt event-driven eval pipelines almost always build a separate reconciliation service that watches the event stream and maintains end-to-end state for monitoring purposes.

## Pattern Three: The Streaming Pipeline

The streaming pattern treats evaluation as a continuous computation over a stream of production data. Rather than discrete jobs or discrete events, the pipeline processes a continuous flow using stream processing frameworks — Apache Kafka with Flink or Kafka Streams, or cloud-native equivalents like Amazon Kinesis or Google Dataflow.

In a streaming eval pipeline, production outputs flow into a topic or stream partition. Stream processors read from this stream and apply evaluations in real time. A sampling processor filters the stream to the configured sample rate. A routing processor examines each selected output and determines which evaluations to apply based on the output's characteristics — its product surface, language, query category, risk tier. Evaluation processors run the actual evaluations. Aggregation processors compute rolling averages, detect trend changes, and maintain the running quality statistics that feed dashboards.

Streaming excels when you need continuous, near-real-time quality scoring on high-volume production traffic. The stream processing framework handles partitioning, parallelism, and fault tolerance natively. You don't build retry logic — the framework handles it. You don't build parallelization — the framework scales by adding partitions. You don't build checkpointing — the framework persists processing state and resumes from the last checkpoint after a failure.

The streaming pattern is also the natural fit for windowed aggregation. Questions like "what is the average quality score over the last thirty minutes?" or "has the p95 quality score for Spanish-language outputs degraded compared to the last two hours?" are native stream processing operations. In an orchestrator or event-driven architecture, answering these questions requires separate aggregation infrastructure. In a streaming architecture, they're built-in.

The cost of streaming is operational complexity. Stream processing frameworks have a steep learning curve. Configuring partitioning, managing consumer groups, tuning processing parallelism, and debugging exactly-once semantics in the face of failures require specialized engineering skills. Teams that don't already run Kafka or Flink for other purposes face a significant infrastructure investment to adopt this pattern for evaluation alone. In 2026, the pragmatic guidance is that the streaming pattern is appropriate for teams already running stream infrastructure and processing more than a million production outputs per day. For smaller teams, the operational overhead of streaming outweighs its benefits.

## When Each Pattern Fits

The three patterns are not mutually exclusive. Most mature evaluation platforms use at least two of them for different workloads.

The centralized orchestrator handles batch workloads — golden set evaluations triggered by deployments, nightly regression suites, periodic deep evaluation runs against adversarial test sets. These are bounded jobs with known completion criteria: "evaluate these two thousand examples and report results." The orchestrator manages the job lifecycle, tracks progress, and provides a single completion status that the release pipeline can gate on.

The event-driven pipeline handles production monitoring — the continuous flow of sampling decisions, evaluation requests, and quality alerts that runs twenty-four hours a day alongside your production system. Events decouple each stage, ensuring that evaluation work doesn't create backpressure on production logging and that individual component failures don't cascade.

The streaming pipeline handles high-volume, low-latency quality scoring — the real-time computation of rolling quality metrics, the continuous aggregation that feeds live dashboards, and the windowed anomaly detection that spots degradation within minutes. This pattern is most valuable for teams with very high traffic and stringent detection latency requirements.

A team processing a hundred thousand outputs per day typically uses an orchestrator for batch and CI/CD evaluation and an event-driven pipeline for production monitoring. A team processing ten million outputs per day typically adds streaming for real-time aggregation on top of the event-driven pipeline. A team processing fewer than ten thousand outputs per day may not need distributed eval infrastructure at all — but should design with clean interfaces between the components so that distribution becomes possible when scale demands it.

## The Six Components of a Scaled Eval Pipeline

Regardless of which architecture pattern you choose, your evaluation pipeline contains six functional components. Understanding these components — what each does, where each can fail, and how they interact — is the foundation for building reliable eval infrastructure.

**Ingestion** is the entry point. This component collects the data to be evaluated: production outputs, golden set examples, or any other evaluation inputs. In an event-driven pipeline, ingestion subscribes to the production output stream and writes selected outputs into the eval pipeline. In an orchestrator pattern, ingestion receives a job specification that identifies the data source and fetches the data. The critical requirement for ingestion is completeness: if an output is selected for evaluation, ingestion must reliably capture it. A dropped output is an invisible gap in your quality signal.

**Routing** decides which evaluation to apply to each input. Not every output needs every evaluation. A safety-critical query might need a safety judge, a quality judge, and a compliance check. A routine informational query might need only a quality judge. Routing examines the output's metadata — product surface, language, risk tier, query category — and assigns the appropriate set of evaluations. Misconfigured routing is a silent failure mode: if routing stops assigning safety evaluations to a query category that should receive them, you lose safety coverage for that category with no error message.

**Execution** runs the evaluations themselves. This is the compute-intensive component and the one most affected by scaling decisions. Execution manages the worker pools for different evaluation types, handles API rate limits for external judge calls, manages GPU allocation for self-hosted models, and implements the timeout and retry logic that prevents individual evaluation failures from blocking the pipeline. Execution is where most of the infrastructure complexity lives.

**Aggregation** combines individual evaluation results into the metrics your consumers need. A single evaluation produces one score for one output on one dimension. Aggregation turns thousands of these individual scores into average quality by product surface, quality trends over time windows, score distributions by query category, and comparison metrics between model versions. This component is lightweight in compute but critical in correctness — an aggregation bug that miscalculates your average quality score can cause false alerts or, worse, mask a real quality regression.

**Storage** persists evaluation results for analysis, reporting, and auditing. At scale, the storage requirements are significant: every evaluation result includes the input, the output, the judge's reasoning, the score, and metadata about the evaluation — which judge model, which rubric version, when the evaluation ran. A team running a million evaluations per day generates substantial data. The storage component must support both fast writes during evaluation and fast reads for dashboard queries, which often means different storage solutions for ingestion and querying — a write-optimized store for real-time results and an analytical store for reporting.

**Alerting** monitors aggregated results and notifies teams when quality changes exceed configured thresholds. Alerting sits at the end of the pipeline and depends on every prior component working correctly. A failure anywhere upstream — ingestion dropping outputs, routing misconfigured, execution silently failing, aggregation miscalculating — manifests as either a false alert or, more dangerously, a missed alert. Alerting reliability is only as good as the weakest link in the chain that precedes it.

## The Data Flow: From Output to Decision

The end-to-end data flow through a scaled eval pipeline follows a predictable path, and each step in that path is a potential failure point that you must instrument and monitor.

A production system generates an output. The output, along with its input and relevant metadata, is logged to the production data store and simultaneously published to the eval ingestion layer. This is the first failure point: if the publish fails silently — dropped message, full queue, network timeout — the output is never evaluated. Your quality signal has a gap, and you don't know it. The mitigation is to monitor the eval ingestion rate against the production output rate and alert when they diverge beyond the expected sampling rate.

Ingestion passes the output to routing. Routing consults the evaluation configuration and determines the evaluation plan for this output: which evaluations to run, in what order, with what judge models and rubrics. This is the second failure point: a stale configuration, a miscategorized output, or a recently added evaluation that routing doesn't know about. The mitigation is configuration versioning and routing coverage reports that show which percentage of outputs received each evaluation type, compared to the intended distribution.

Routing dispatches evaluation tasks to the execution layer. Each task is queued for the appropriate worker pool. The worker picks up the task, executes the evaluation — calling an API, running a local model, applying a heuristic — and returns the result. This is the third failure point and the most common: API timeouts, rate limit errors, model loading failures, out-of-memory crashes. The mitigation is robust retry logic with exponential backoff, dead-letter queues for tasks that fail after all retries, and monitoring of the failure rate per worker pool.

Execution results flow to aggregation, which updates running metrics, and to storage, which persists the individual results. Aggregation updates feed dashboards and alerting in near-real-time. Storage feeds reporting and analysis on longer timescales. The failure point here is data loss or corruption between execution and storage — a result that was computed but never persisted is wasted compute and a quality signal gap.

The entire flow, from output generation to aggregated metric update, might take anywhere from seconds to hours depending on the architecture pattern and the evaluation type. For heuristic checks in a streaming pipeline, the end-to-end latency is seconds. For an LLM judge evaluation in a batch pipeline, it might be hours. For a human review workflow, it might be days. Your pipeline must handle all of these timescales simultaneously, routing fast evaluations through fast paths and slow evaluations through paths that don't block the fast ones.

## Traceability: Linking Results to Their Origins

One principle separates evaluation pipelines that scale from those that collapse under their own complexity: **traceability**. Every evaluation result must be linkable back to the exact production output it evaluated, the exact judge model and version that scored it, the exact rubric version that defined the criteria, and the exact configuration that determined how routing selected it. Without traceability, debugging is guesswork.

When a dashboard shows a quality drop, the first question is always "on which outputs?" Without traceability, you have aggregate scores and no way to drill down to the individual evaluations that drove the aggregate. You can see that average quality dropped from ninety-two to eighty-seven percent, but you can't identify which specific outputs scored poorly, which judge flagged them, or what the judge's reasoning was. The investigation stalls.

Traceability requires a correlation identifier that follows each output through every stage of the pipeline. The output ID, assigned at production time, travels through ingestion, routing, execution, aggregation, and storage. Every evaluation result is tagged with this ID. Every aggregation computation can be decomposed back to the individual results that contributed to it. Every alert can be traced to the specific evaluations that triggered it.

In practice, traceability also means versioning your evaluation configuration. If you changed the quality rubric on Tuesday and quality scores dropped on Wednesday, you need to know whether the drop reflects actual quality degradation or the new rubric scoring more harshly. Configuration versioning — tagging each evaluation result with the rubric version and judge model version in effect when it ran — makes this distinction possible. Without it, every quality change investigation starts with the question "did we change the eval, or did the model change?" and nobody has a fast answer.

## Starting Small, Designing for Distribution

You don't need to build a fully distributed evaluation pipeline on day one. But you do need to design your single-machine pipeline with the interfaces that make distribution possible later.

The most valuable early design decision is clean separation between the six components. Your ingestion logic should be separate from your routing logic, which should be separate from your execution logic. Even if all three run in the same process today, structuring them as distinct modules with well-defined inputs and outputs means you can move each to a separate service when scale demands it. The team that writes a monolithic evaluation script where data fetching, routing decisions, and judge calls are interleaved in a single function will have to rewrite the entire thing when distribution becomes necessary. The team that writes those functions separately, communicating through data structures rather than shared state, can extract each into its own service incrementally.

The second valuable early decision is external state. Your evaluation pipeline's state — which outputs have been evaluated, which are pending, which failed — should live in a database, not in memory. A single-machine pipeline with external state can be restarted without losing track of pending work. A distributed pipeline requires external state by definition. If you build with external state from the start, the transition to distribution is a deployment change, not an architecture change.

The eval pipeline is a distributed system. Understanding the patterns — orchestrator, event-driven, streaming — and the components — ingestion, routing, execution, aggregation, storage, alerting — gives you the architectural vocabulary to build infrastructure that grows with your evaluation needs. The first scaling lever within this architecture is how you batch work for throughput. The next subchapter covers batch processing optimization and the critical technique of prompt bucketing that turns naive sequential evaluation into efficient parallel throughput.
