# 1.6 — When to Scale: The Volume Thresholds That Demand Eval Infrastructure

The team had been evaluating every output by hand. It took forty-five minutes a day, and it caught real problems. Then the product launched to general availability, daily requests jumped from two hundred to twelve thousand, and the forty-five minutes became an impossible fourteen hours. Nobody made a deliberate decision to stop evaluating. The calendar just stopped cooperating. Reviews dropped from daily to twice a week. Then weekly. Then the team lead admitted in a standup that nobody had reviewed an output in eleven days. By the time they looked again, a prompt regression had been live for nine days, and customer satisfaction scores had already cratered.

This is the most common way eval systems die. Not a dramatic failure. Not a budgetary cut. Just a quiet gap between how fast traffic grows and how fast evaluation capacity can keep up. The gap starts as an inconvenience and becomes an operational blind spot. The question every AI team faces is not whether this gap will open, but when — and whether they will have the infrastructure to close it before quality degrades past the point of recovery.

## The Three Volume Thresholds

Not all scaling pressure is the same. There are three distinct volume thresholds, and each one demands a fundamentally different kind of eval infrastructure. Treating them as a smooth continuum is a mistake. Each threshold represents a phase transition — a point where the approach that worked in the previous phase does not merely slow down but actively breaks.

**Threshold One** sits between a few hundred and a few thousand daily evaluations. This is where manual review becomes impossible as a primary quality gate. A team of three engineers can review two hundred outputs per day with genuine attention — reading context, checking accuracy, assessing tone. At two thousand outputs per day, those same three engineers would need to spend their entire workday doing nothing but reviewing. At five thousand, even dedicated reviewers cannot keep pace without sacrificing depth. The failure mode at Threshold One is not catastrophic. It is erosion. Reviews become shallower. Sampling becomes ad hoc. The team shifts from "we review everything" to "we review what we happen to see," and the difference between those two approaches is the difference between systematic quality control and hope.

**Threshold Two** sits between tens of thousands and hundreds of thousands of daily evaluations. This is where single-machine eval pipelines become bottlenecks. Your automated eval scripts — the ones that replaced manual review at Threshold One — now take four hours to process a day's worth of traffic. Results arrive too late to inform decisions. The pipeline itself starts failing: out-of-memory errors on large batches, timeouts on LLM-as-judge calls, disk space consumed by result logs nobody has time to analyze. The failure mode at Threshold Two is infrastructure collapse. The eval system does not stop working — it starts working so slowly and unreliably that teams route around it. Engineers deploy without waiting for eval results because the pipeline is still processing yesterday's traffic.

**Threshold Three** sits at millions of daily evaluations. This is where eval cost becomes a significant line item in your AI budget. If you are running LLM-as-judge evaluations on even 10% of a million daily outputs, and each judge call costs a fraction of a cent, you are spending hundreds of dollars a day — tens of thousands a year — on evaluation alone. At this scale, you need cost optimization strategies like tiered evaluation, where cheap heuristics filter the majority and expensive judge calls only process the ambiguous cases. You need a dedicated platform team because the eval system is now production infrastructure with its own uptime requirements, its own capacity planning, and its own incident response. The failure mode at Threshold Three is budgetary. Eval costs grow linearly with traffic while eval budgets do not. Teams begin cutting corners — reducing sample rates, dropping expensive quality dimensions, skipping evaluations on "stable" use cases — and each cut creates a new blind spot.

## What Happens When You Do Not Invest at Each Threshold

The consequences of ignoring each threshold are distinct, and understanding them helps you argue for infrastructure investment before the damage accumulates.

At Threshold One, the consequence is undetected regression. A fintech startup in early 2025 kept manual review as its sole quality gate through a period where daily requests grew from three hundred to four thousand. The review team was sampling roughly 2% of traffic by the time they noticed a problem. A model update had degraded the accuracy of tax categorization outputs for small-business users — a segment representing 15% of their traffic. The degradation had been live for three weeks before a customer escalation triggered an investigation. The fix took two days. The trust repair took four months.

At Threshold Two, the consequence is eval system abandonment. When a pipeline takes six hours to return results, engineers stop waiting for it. They deploy based on spot-checks and gut feel. Within months, the eval system becomes a formality — it runs in the background, but nobody reads the results, nobody investigates failures, and nobody trusts the scores. A large e-commerce company discovered this pattern in mid-2025 when an audit revealed that their eval pipeline had been silently failing on 30% of evaluation jobs for two months. Nobody noticed because nobody was consuming the results. The pipeline had become infrastructure that existed to satisfy a compliance checklist rather than to inform engineering decisions.

At Threshold Three, the consequence is budget-driven quality erosion. A SaaS company serving enterprise customers crossed the million-daily-output mark in late 2025. Their eval costs were growing at $18,000 per month. Finance flagged the line item. The engineering team cut the eval sample rate from 5% to 1%. They dropped two of their four quality dimensions from automated evaluation. They stopped running weekend evaluations entirely. Within six weeks, a quality regression in one of the dropped dimensions — response completeness — caused a spike in support tickets from their highest-paying customer segment. The cost of the support incident exceeded the eval savings by an order of magnitude.

## The Velocity Threshold: When Shipping Speed Forces Automation

Volume is not the only pressure. Even at low volume, high deployment velocity forces eval automation. If your team ships model changes daily — new prompts, updated retrieval configurations, fine-tuned model versions — you cannot gate each deployment on manual review. The math breaks differently than it does with volume, but it breaks just as completely.

Consider a team processing only eight hundred outputs per day. That volume is well within manual review capacity. But the team ships prompt changes three times per week and model updates every two weeks. Each change requires evaluation against a golden set, regression testing against previous outputs, and quality validation across all supported use cases. If each evaluation cycle takes a full day of manual effort, the team spends three days per week on evaluation, leaving two days for everything else. Shipping faster requires either skipping evaluation — which is reckless — or automating it so that each evaluation cycle completes in minutes rather than hours.

The velocity threshold is deceptive because the per-evaluation effort feels manageable. Any single eval run is straightforward. The problem is frequency. A team that ships once per month can afford manual evaluation. A team that ships daily cannot. The investment in automation pays for itself not through volume savings but through velocity savings — enabling the team to maintain its shipping cadence without sacrificing quality gates.

## The Variety Threshold: When Use Case Count Forces Stratification

The third pressure axis is variety. Even at modest volume and moderate velocity, supporting many distinct use cases forces a fundamentally different evaluation approach. An AI platform supporting twelve different use cases — customer support, document summarization, code review, data extraction, content moderation, and seven others — cannot evaluate all use cases with a single set of criteria. Each use case has its own quality dimensions, its own failure modes, and its own golden sets.

A team evaluating a single use case might maintain one golden set of 200 examples and one evaluation rubric. A team evaluating twelve use cases needs twelve golden sets and twelve rubrics. Maintaining those golden sets requires ongoing effort — examples go stale, edge cases accumulate, product changes invalidate existing test cases. At twelve use cases, golden set maintenance alone becomes a part-time job. At twenty use cases, it requires dedicated headcount.

The variety threshold also creates a coverage problem. If your eval pipeline runs sequentially through all use cases, twelve use cases take twelve times as long. If each use case evaluation takes thirty minutes, a full evaluation run takes six hours. By the time the pipeline finishes, the results for the first use case are already half a day old. Stratified evaluation — running use cases in parallel, prioritizing high-risk use cases, and adjusting sample rates per use case based on recent stability — is not a luxury at this scale. It is the only approach that produces timely results.

## The Compound Threshold: Volume Times Variety Times Velocity

The hardest scaling challenge is not any single dimension in isolation. It is the compound pressure of all three dimensions multiplying together. A team processing fifty thousand requests per day across fifteen use cases while shipping daily faces a combinatorial explosion. Each deployment needs evaluation across each use case, with sufficient sample size per use case to detect meaningful regressions. Fifteen use cases at five hundred samples each requires seventy-five hundred evaluations per deployment cycle. Three deployments per week means twenty-two thousand five hundred evaluations per week just for release gating — before continuous production monitoring even begins.

This is **the compound threshold**, and it is where most organizations discover that their evaluation approach is fundamentally inadequate. The response to compound pressure cannot be incremental. You cannot solve it by hiring one more reviewer or adding one more eval script. The compound threshold demands a platform — a system that manages evaluation jobs across use cases, allocates compute resources dynamically, tracks results over time, and surfaces the most important findings without requiring a human to read every score.

Organizations that hit the compound threshold without platform-level infrastructure typically respond in one of two ways. They either slow down — reducing deployment frequency, consolidating use cases, capping traffic — or they reduce evaluation scope until it covers only the highest-risk use cases. Both responses are retreats. The first sacrifices velocity. The second sacrifices coverage. Neither is sustainable as the product continues to grow.

## How to Recognize You Have Crossed a Threshold

The thresholds are not neatly labeled in your monitoring dashboard. You recognize them through operational signals — patterns in how your team works, what they complain about, and what starts failing.

You have crossed Threshold One when manual reviewers start skipping days. When the review queue grows faster than the team can process it. When engineers ask whether they can just "spot-check a few" instead of reviewing the full set. When someone proposes sampling without a formal sampling strategy. When you discover a regression through a customer complaint rather than through review.

You have crossed Threshold Two when your eval pipeline takes longer to run than the interval between deployments. When eval scripts fail and nobody notices for a day or more. When results are stored in files that only one engineer knows how to interpret. When a new engineer asks how to run the eval suite and the answer takes more than five minutes to explain. When you have eval results from last week but cannot confidently compare them to this week because the pipeline configuration changed.

You have crossed Threshold Three when your monthly eval compute costs exceed the cost of one engineer's salary. When finance starts asking what the evaluation budget is and why it is growing. When your team debates whether to evaluate more use cases or keep the budget stable. When you need a dashboard to track eval costs by use case, by model, and by judge type. When eval cost optimization becomes a standing agenda item in weekly planning.

The operational signals are unmistakable once you know what to look for. The danger is that teams normalize them. A reviewer skipping a day feels like a scheduling conflict, not a threshold crossing. A slow pipeline feels like a temporary inconvenience, not an architectural limitation. The team adapts to each degradation until the cumulative effect is that evaluation has effectively stopped — and nobody made a conscious decision to stop it.

## The Decision Framework: Invest or Accept the Risk

Recognizing a threshold crossing creates a decision point. You can invest in the next level of eval infrastructure, or you can accept the quality risk that comes with operating beyond your current evaluation capacity. Both are legitimate choices, but only if they are conscious choices.

The investment case is straightforward to calculate. Estimate the cost of building and operating the next level of infrastructure. Estimate the cost of the quality incidents that will occur without it, using your historical incident rate and the cost per incident. If the infrastructure cost is less than two to three quality incidents per year, the investment pays for itself. Most teams find that a single production quality incident — customer trust damage, compliance exposure, engineering time to investigate and remediate — costs far more than the infrastructure that would have prevented it.

The risk acceptance case has its own logic. If your product is in early beta with a small, forgiving user base, the cost of quality incidents is low. If your use case has minimal safety or compliance implications, the consequences of undetected regression are manageable. If your team is capacity-constrained and the alternative to eval infrastructure is delaying product features, the trade-off might favor shipping. The key is making this trade-off explicitly. Write it down. Share it with stakeholders. Revisit it quarterly. A conscious decision to accept risk is engineering judgment. An unconscious drift into inadequate evaluation is negligence.

Once you recognize that you have crossed a threshold and decide to invest, the next decision is the most consequential one: do you need a better script, or do you need a platform?

---

The difference between an eval script and an eval platform is not a matter of ambition. It is a matter of operational reality — and the next subchapter maps when code needs to become a system.
