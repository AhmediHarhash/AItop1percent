# 7.5 — Review Queue Management: Prioritization, Load Balancing, and Turnaround SLAs

The review queue shows 4,200 pending items. The oldest has been waiting eleven days. Three reviewers are idle because their specialization doesn't match the queue contents. Meanwhile, a safety-flagged output from a model deployed yesterday is buried at position 3,800, behind three thousand routine quality checks.

## The Queue Management Problem

At scale, the review queue is a system with its own dynamics — inflow rate, processing rate, priority levels, aging, and staleness. Managing it well means every reviewer is working on the most valuable item available at any moment, and every time-sensitive item gets reviewed within its acceptable window. Managing it poorly means critical reviews wait behind routine ones, reviewer expertise goes unused, and the entire human review program generates less signal than its budget deserves.

The inflow rate depends on how many outputs the review router (from 7.2) sends to human review. The processing rate depends on how many reviewers are active and how fast they work. When inflow exceeds processing capacity, the queue grows. When it grows unchecked, turnaround times stretch, and the value of the reviews degrades because stale reviews of old outputs provide less actionable information than fresh reviews of recent outputs.

## Priority-Based Queuing

Not all reviews are equally urgent. A priority system ensures that the most important reviews are completed first, regardless of when they entered the queue. At minimum, four priority tiers work well for most organizations.

Critical priority is reserved for safety-related outputs, outputs that triggered compliance flags, and outputs involved in active customer escalations. These should be reviewed within four hours. High priority covers outputs from newly deployed model versions during the post-deploy surge, outputs where automated judges strongly disagree, and outputs from enterprise customers. Target turnaround is twenty-four hours. Standard priority includes routine quality checks, calibration reviews, and golden set maintenance reviews. Target turnaround is seventy-two hours. Low priority covers background sampling reviews and silver-tier data validation. Target turnaround is one week, and these items are the first to be dropped if capacity is constrained.

## The Review SLA

**The Review SLA** is a turnaround time guarantee for each priority tier. It is not a suggestion — it is a measurable commitment that the review team is accountable for. SLA compliance rate (the percentage of reviews completed within their tier's time limit) is the primary operational metric for the review program.

Setting SLAs requires balancing ambition with capacity. SLAs that are too tight create constant firefighting and reviewer burnout. SLAs that are too loose provide no operational discipline. The right SLAs are tight enough that teams feel urgency on critical reviews and loose enough that standard reviews can be completed during normal working hours without overtime.

## Load Balancing Across Reviewers

Effective load balancing matches reviews to reviewers based on two dimensions: expertise and availability. Domain routing sends medical outputs to reviewers with medical training, legal outputs to reviewers with legal background, and general outputs to generalists. Availability routing distributes work evenly across active reviewers, accounting for timezone coverage and scheduled absences.

The worst anti-pattern is random assignment without expertise matching. When a complex financial compliance review lands on a generalist reviewer, the review either takes three times longer than it should or produces a judgment that lacks the depth to be useful. Expertise-matched routing improves both speed and quality simultaneously.

Load balancing must also handle the reviewer concentration problem. If one reviewer handles 40% of all reviews in a week, their personal biases — however subtle — distort the overall data. Distributing reviews so no single reviewer exceeds 15-20% of total volume provides natural debiasing.

## Queue Health Metrics

Four metrics define queue health. Queue depth measures total pending items — a growing trend signals capacity problems. Average wait time measures how long items sit before being picked up — this should stay well below SLA thresholds. SLA compliance rate tracks the percentage of reviews completed within their tier's time limit — the target is 95% or above. Aging distribution shows how many items are approaching or past SLA, providing an early warning of impending SLA breaches.

These metrics should be monitored daily by the review operations lead. Weekly trends should be reviewed by the team responsible for review capacity planning. If queue depth grows consistently over two or more weeks, either routing volume needs to decrease (by raising the routing threshold) or review capacity needs to increase (by adding reviewers or extending hours).

## The Backlog Problem

When inflow exceeds capacity for an extended period — during a major deployment, a traffic spike, or reviewer shortage — the queue develops a backlog. Backlogs are dangerous not because of their size but because of their age. Items that have been waiting for days are less valuable than items that arrived today, because older outputs represent a quality state that may have already changed.

Three strategies manage backlogs. Temporary sampling reduction tells the review router to send fewer items during the backlog period, accepting a short-term coverage gap to prevent the queue from growing further. Surge capacity brings on temporary reviewers — either internal volunteers or external vendor reviewers — to work through the backlog. Auto-triage reviews the backlog and routes items that have aged past their SLA back to automated evaluation, acknowledging that a late human review is less valuable than no human review for those specific items.

## The Stale Review Problem

A review completed three days after the output was generated provides less value than a review completed three hours after. The output may have been produced by a model version that has already been updated. The context that produced the output may have changed. And the operational decisions that the review should inform may have already been made without the review data.

Freshness windows define the maximum age at which a review is still considered valuable. For safety-critical reviews, the window is typically four to eight hours. For quality reviews of new deployments, twenty-four to forty-eight hours. For routine quality monitoring, up to one week. Items that age past their freshness window should be automatically expired from the queue rather than reviewed, freeing reviewer capacity for current items.

Review queues generate the raw material for one of the most valuable processes in scaled evaluation — using human judgments to calibrate and improve automated judges.
