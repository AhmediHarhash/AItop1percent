# 5.5 — Eval Result Caching: Avoiding Redundant Evaluation of Identical or Near-Identical Outputs

If your model produces the same output for the same input that it produced yesterday, and the judge model hasn't changed, and the eval criteria haven't changed — why would you evaluate it again? The score will be identical. The tokens consumed by the judge are wasted. The latency added to the pipeline is pure overhead. And yet, most evaluation pipelines do exactly this. They treat every evaluation as a fresh job, regardless of whether the same work was already done an hour ago, a day ago, or a week ago.

This is not a minor inefficiency. In production eval systems processing tens of thousands of evaluations per day, redundant evaluations account for twenty to forty percent of total eval volume — and therefore twenty to forty percent of the API cost, the compute cost, and the pipeline latency. For teams running continuous evaluation on high-traffic systems, where many users ask similar questions and receive similar answers, the redundancy is even higher. A customer support system might generate functionally identical responses to the same question hundreds of times per day. Evaluating each of those identical responses as if it were a novel output is waste, pure and simple.

Caching eval results is the single highest-leverage optimization most teams haven't implemented. It's conceptually simple, architecturally straightforward, and the return on investment is immediate. The challenge — and the reason this subchapter exists — is that getting caching right requires understanding what makes two evaluations equivalent, when a cached result becomes stale, and how to prevent the cache from introducing subtle errors that are worse than the waste it eliminates.

## The Cache Key: What Makes Two Evaluations Equivalent

An eval result is valid if and only if the conditions that produced it haven't changed. The cache key must encode every input that influences the score. Miss one input, and you'll serve stale results. Include unnecessary inputs, and your cache hit rate drops to near zero.

The minimum cache key for an LLM judge evaluation contains four components. First, the input to the production model — the user query or prompt that triggered the output. Second, the production model's output — the actual text being evaluated. Third, the judge model identifier — the specific model name and version used for scoring. Fourth, the eval criteria — the judge prompt, rubric, scoring dimensions, and any configuration that shapes how the judge evaluates the output.

Hashing these four components into a single key gives you a deterministic identifier for the evaluation. If all four match a cached entry, the cached score is valid. If any one differs, the evaluation must be re-run.

The implementation is straightforward. Concatenate the input, output, judge model ID, and eval prompt into a single string, then apply a collision-resistant hash function like SHA-256 to produce the cache key. Store the key alongside the eval result — the scores, the judge's reasoning, the timestamp, and the cache metadata. On each new evaluation, compute the key first and check the cache. A hit returns the stored result immediately, skipping the judge call entirely.

The subtlety is in what you include versus what you omit. The judge model ID must be precise enough to capture version changes. "GPT-5" is not sufficient if the provider silently updates the model's weights — as major providers routinely do between named version releases. If your provider offers version-pinned endpoints, use the pinned version in the key. If they don't, include the date of the last known model update as a proxy.

The eval prompt must be included in its entirety. A single-word change to the rubric can shift scores across the board. Teams that hash the eval prompt separately and include the hash in the cache key can quickly invalidate all cached results when the prompt changes, without needing to inspect individual entries.

## Exact-Match Caching in Practice

Exact-match caching — where the cached result is returned only when every byte of the input-output pair is identical — is the simplest and safest form of eval caching. It has zero risk of returning an incorrect result, because the evaluation conditions are exactly reproduced. The only question is whether the hit rate is high enough to justify the cache infrastructure.

For systems with high output determinism — where the same input reliably produces the same output — exact-match caching performs well. Classification systems, structured extraction pipelines, and template-driven generation often produce byte-identical outputs for identical inputs. A customer service chatbot that retrieves answers from a knowledge base will return the same response verbatim for the same question until the knowledge base changes. In these scenarios, exact-match cache hit rates of thirty to fifty percent are common, meaning a third to half of all evaluations can be skipped.

For systems with high output variance — creative writing, open-ended conversation, brainstorming — exact-match caching provides little benefit. Even with temperature set to zero, model outputs contain enough non-determinism in token selection that identical inputs rarely produce identical outputs across separate inference calls. Cache hit rates for these systems are typically five to ten percent — still worth having, since even a five percent reduction in eval volume is free, but not transformative.

The cache storage requirements are modest. Each cached entry stores the cache key (a hash, thirty-two to sixty-four bytes), the evaluation result (scores and optional reasoning, typically five hundred to two thousand bytes), and metadata (timestamp, judge model ID, eval prompt hash, another hundred bytes). A cache of one million entries consumes approximately two to three gigabytes — easily served from an in-memory store like Redis, or from a fast key-value database. The lookup time is sub-millisecond, adding negligible overhead to the pipeline.

## Near-Duplicate Detection: Caching Beyond Exact Matches

The real power of eval caching emerges when you extend it beyond exact matches to semantically equivalent outputs. Two outputs that differ by whitespace, punctuation, a synonym, or a rephrased clause are almost certainly going to receive the same eval score. Treating them as completely different evaluations wastes money for the same reason that exact duplicates do — the judge will return the same assessment.

**Near-duplicate detection** identifies outputs that are semantically similar enough that their eval scores can be safely reused. The mechanism relies on embedding similarity. When a new output arrives for evaluation, the system computes its embedding using a lightweight embedding model. It then compares this embedding against the embeddings of recently cached outputs using cosine similarity. If a cached output exceeds a configurable similarity threshold — typically 0.95 to 0.98 — the cached score is returned instead of running the judge.

The threshold is the critical parameter. Set it too low (0.90), and you'll serve cached scores for outputs that are meaningfully different — a response that includes a critical safety caveat and a response that omits it might still have 0.92 similarity. Set it too high (0.99), and the near-duplicate cache rarely triggers, adding the overhead of embedding computation without the benefit of cache hits. The right threshold depends on your evaluation granularity. For binary pass/fail evaluations, where small output differences rarely change the verdict, a threshold of 0.95 works well. For fine-grained numeric scoring on a ten-point scale, where small output changes might shift the score by a point, a threshold of 0.98 is safer.

The embedding model must be fast and cheap, because it runs on every evaluation candidate. Frontier embedding models are unnecessary — a small, efficient model with 384 or 768 dimensions provides sufficient similarity discrimination for cache matching. The embedding computation adds five to twenty milliseconds per output, which is negligible compared to the one to four seconds you save on every cache hit. The break-even point is a cache hit rate of roughly one to two percent — if even one in fifty near-duplicate checks results in a cache hit, the embedding computation has paid for itself.

The near-duplicate cache requires a vector search index to efficiently find similar cached entries. A brute-force comparison against every cached embedding is viable for small caches (under ten thousand entries) but becomes expensive at scale. For larger caches, an approximate nearest-neighbor index — using libraries like FAISS, Annoy, or a managed vector database — provides sub-millisecond lookups even for millions of cached entries. The index adds operational complexity but enables near-duplicate caching at scale.

Research from 2025 on semantic caching systems, including ensemble embedding approaches that combine multiple embedding models through a trained meta-encoder, has pushed cache hit accuracy above ninety percent for semantically equivalent queries while maintaining eighty-five percent accuracy in rejecting non-equivalent ones. These techniques, originally developed for inference caching, transfer directly to eval result caching — the same principle applies when the question is "have we evaluated something equivalent?" rather than "have we answered something equivalent?"

## Cache Invalidation: The Hard Problem

Phil Karlton's famous observation that the two hardest problems in computer science are cache invalidation and naming things applies with full force to eval result caching. A stale cache entry that serves an outdated score is worse than no cache at all, because it silently reports incorrect quality data. Your dashboard shows green. Reality is red. The cache made you blind.

There are four events that must trigger cache invalidation. The first is a **judge model update**. When your judge model changes — a new version release, a provider-side update, or a switch to a different model — every cached result scored by the old model is potentially invalid. The new model may score the same outputs differently. Invalidating the entire cache on model change is the safe default. If your cache is keyed on the judge model ID and you update the ID when the model changes, invalidation happens automatically — old entries simply stop matching new lookups.

The second is an **eval criteria change**. When you modify the judge prompt, the rubric, the scoring dimensions, or the scoring scale, cached results from the old criteria are invalid. As with model changes, including the eval prompt hash in the cache key handles this automatically. Any change to the prompt produces a different hash, which produces different cache keys, which means old entries don't match.

The third is a **production model change**. When the model being evaluated changes — a fine-tuning iteration, a model swap, a configuration change — the same input may produce different outputs. Cached eval results for the old model's outputs don't apply to the new model's outputs. This invalidation typically happens naturally because the new model produces different outputs that generate different cache keys. But if the new model happens to produce identical output for some inputs — which is common when models are closely related — cached scores from the old eval context might be served. Including the production model version in the cache key prevents this edge case.

The fourth is **time-based expiry**. Even when no explicit change triggers invalidation, cached results should expire after a configurable window. A twenty-four to seventy-two hour time-to-live is typical for production eval caches. This is a safety net rather than a primary invalidation mechanism. If a judge model update happens silently — the provider updates weights without changing the version identifier — time-based expiry ensures that stale results eventually cycle out. Without this backstop, a silent model update could leave your cache serving outdated scores indefinitely.

## Cache Architecture Patterns

The simplest cache architecture is a **write-through cache** collocated with the eval pipeline. When the pipeline evaluates an output, it computes the cache key, checks the cache, and either returns the cached result or runs the judge and writes the result to the cache before returning it. The cache is a single shared store — Redis for in-memory speed, DynamoDB or a similar key-value store for persistence across restarts.

For distributed pipelines with multiple workers, the cache must be accessible from all workers simultaneously. A shared Redis instance or a managed cache service handles this naturally. The cache lookup and write operations should be atomic — or at least idempotent — to prevent race conditions where two workers evaluate the same output concurrently, both miss the cache, both call the judge, and both write a result. With idempotent writes, both workers write the same key with the same value, and the second write is a harmless no-op. With non-idempotent writes, the second write might overwrite the first with a slightly different score — possible because LLM judges have inherent non-determinism even for identical inputs. The practical solution is first-write-wins: check the cache again after the judge returns, and write only if the key doesn't already exist.

For teams running eval pipelines across multiple environments — staging, canary, production — the cache should be scoped per environment or shared with care. A production eval cache contains scores for production outputs, which may differ from staging outputs even for the same input. Sharing a cache across environments risks cross-contamination: a staging score served for a production output, or vice versa. The simplest mitigation is to include the environment identifier in the cache key, creating effectively separate caches within a shared store.

## Measuring Cache Effectiveness

A cache you don't monitor is a cache you can't trust. Three metrics tell you whether your eval cache is working as intended.

**Cache hit rate** is the percentage of eval requests served from the cache rather than sent to the judge. Track this daily. A hit rate below five percent suggests the cache is not providing meaningful savings — either your outputs are too diverse for caching or your cache keys are too specific. A hit rate above fifty percent is excellent and means the cache is saving you half your judge costs. A suddenly dropping hit rate indicates a change in output distribution, a model update that invalidated the cache, or a criteria change. A suddenly rising hit rate might indicate that the production model has started producing more homogeneous outputs — which could be a quality concern worth investigating independent of the cache.

**Cache staleness rate** is the percentage of cached results that would differ from a fresh evaluation if re-run. You can't measure this directly without re-running evaluations, but you can estimate it by periodically running a calibration check: take a random sample of cached results, re-evaluate them fresh, and compare. If more than five to ten percent of re-evaluated scores differ materially from the cached scores, your invalidation strategy is too lenient. Tighten the time-to-live, include more components in the cache key, or raise the near-duplicate similarity threshold.

**Cost savings** is the total judge API cost avoided by cache hits. Multiply the cache hit count by the average cost per evaluation to get the dollar savings. This number justifies the cache infrastructure cost. A team running one hundred thousand evaluations per day at 0.005 dollars per evaluation, with a thirty percent cache hit rate, saves thirty thousand evaluations times 0.005 dollars — one hundred fifty dollars per day, or forty-five hundred dollars per month. The Redis instance hosting the cache costs perhaps fifty to one hundred dollars per month. The return on investment is immediate and substantial.

## The Semantic Cache Pitfall

Near-duplicate caching introduces a risk that exact-match caching avoids: false positives. A false positive occurs when the cache serves a score for an output that is similar to — but meaningfully different from — the cached output. The similarity threshold was met, but the quality-relevant difference was in the details that the embedding didn't capture.

Consider a medical information system. Two outputs might be ninety-six percent similar by embedding distance, but one includes a dosage recommendation and the other omits it. The omission is a critical quality failure, but the embedding treats it as a minor difference because the overall semantic content is nearly identical. If the cache serves the passing score from the complete output to the incomplete output, a safety-critical failure is masked.

The mitigation is domain-aware cache validation. For high-stakes evaluation dimensions — safety, accuracy of factual claims, inclusion of required disclaimers — disable near-duplicate caching entirely and require exact matches. For lower-stakes dimensions — tone, fluency, formatting — near-duplicate caching is safe because small output variations don't change the score materially. This hybrid approach captures most of the cost savings from near-duplicate caching while protecting the evaluation dimensions where false positives are dangerous.

An additional safeguard is periodic cache auditing. Monthly, pull a random sample of near-duplicate cache hits and have a human reviewer compare the cached output with the output that received the cached score. If the reviewer identifies cases where the score should have differed, those cases inform threshold tuning and identify evaluation dimensions where near-duplicate caching is too aggressive.

## Cache Warming and Cold-Start Strategies

When you deploy a new eval pipeline or invalidate the entire cache — after a judge model update, for instance — the cache starts cold. Every evaluation misses the cache and calls the judge, producing the same throughput and cost as an uncached pipeline. For teams that depend on cache savings to stay within budget, a cold cache means a temporary spike in eval costs.

**Cache warming** mitigates the cold start by pre-populating the cache before the pipeline begins processing production evaluations. The approach depends on your output distribution. If you have historical eval data from the previous cache period — the outputs, the scores, and the criteria — you can re-evaluate a representative sample and populate the new cache with fresh results. The sample should prioritize high-frequency outputs, since those are the most likely to produce cache hits.

For teams with predictable output distributions, a nightly warm-up job that evaluates the top one thousand most-common outputs can pre-populate thirty to fifty percent of the next day's cache hits. This warm-up job runs during off-peak hours, consumes minimal rate budget, and ensures that when the production pipeline starts the next morning, the most common evaluations are already cached.

The alternative to cache warming is graceful degradation. Accept that the first few hours after a cache invalidation will have higher eval costs and lower throughput. Configure the pipeline's budget cap (discussed in the previous chapter) to accommodate occasional cold-start periods without triggering aggressive cost-cutting measures. This is simpler than cache warming and appropriate for teams whose eval cost spikes are tolerable as long as they're infrequent and short-lived.

Caching reduces the number of evaluations your pipeline runs. But the evaluations that remain still consume compute — API calls, GPU cycles, CPU time — and that compute costs money. The next subchapter addresses how to allocate your eval compute budget across the different types of evaluation in your pipeline, ensuring that every dollar spent on evaluation infrastructure produces the maximum quality signal.
