# 9.7 — Eval Dashboards for Different Audiences: Engineering, Product, and Leadership

The dashboard that helps an engineer debug a quality regression is useless to a VP making budget decisions. The dashboard that helps a VP understand trends is useless to an engineer who needs to find the specific test case that's failing. And the dashboard that tries to serve both audiences simultaneously serves neither — it overwhelms the VP with detail they don't need and buries the engineer's diagnostic data under layers of summarization they have to click through to reach anything actionable.

This is not an aesthetic problem. It is an operational one. When leadership lacks visibility into eval health, they make uninformed decisions about AI investment. When engineering lacks diagnostic depth, they spend hours finding what a well-designed dashboard would surface in seconds. When product lacks quality trend data, they ship features without understanding the quality trajectory of the models behind them. Each audience has different questions, different time horizons, and different decision types — and the dashboard design must reflect those differences.

## Three Audiences, Three Sets of Questions

The fundamental insight is that these three audiences do not need the same data filtered differently. They need different data entirely.

Engineers ask: What broke? Where did it break? When did it start breaking? What changed? Their time horizon is minutes to hours. They need per-test-case results, per-dimension scores, trace-level debugging, model version comparisons, and the ability to drill from an aggregate anomaly down to the specific input that triggered it. An engineer investigating a quality regression needs to see which eval dimensions degraded, which test cases flipped from pass to fail, what the model's actual outputs were on those failing cases, and how those outputs differ from the previous model version. They need this within five minutes of starting the investigation, not after thirty minutes of clicking through a general-purpose dashboard.

Product managers ask: Is quality getting better or worse? Are our users' most important features covered? How does quality compare across features? What's the quality impact of the experiment we're running? Their time horizon is days to weeks. They need quality trend lines, per-feature quality breakdowns, experiment result summaries, and the relationship between quality metrics and user outcomes. A product manager evaluating whether to expand a feature to new markets needs to see how quality scores differ across user segments, how quality has trended since the last model update, and whether the quality threshold is met for the expansion criteria. They do not need to see individual test cases.

Leadership asks: Are we safe? Are we improving? Are we spending the right amount? How do we compare to competitors? Their time horizon is weeks to quarters. They need aggregate health indicators, cost-quality ratios, quality risk exposure, trend direction across the portfolio, and enough context to make investment decisions without having to understand the evaluation methodology in detail. A VP deciding whether to approve the budget for a model upgrade needs to see the current quality level, the projected improvement, the cost delta, and the risk profile — not the per-dimension score breakdown across four hundred test cases.

## Engineering Dashboards: The Diagnostic Layer

The engineering dashboard is the most data-dense and the most interactive. Its primary purpose is diagnosis: given that something went wrong, help the engineer find the root cause as fast as possible.

The entry point is an overview showing all eval dimensions with their current scores, their recent trend direction, and a status indicator — green for within normal range, yellow for approaching threshold, red for in violation. This overview lets the engineer scan hundreds of dimensions in seconds and focus on the ones that need attention.

Clicking into any dimension opens a detail view showing the score history over time with alert threshold lines, the distribution of individual test case scores (not just the aggregate), and a comparison panel that shows the current run side by side with the previous run or a selected baseline. The comparison panel is critical — most engineering investigations start with "what changed?" and the answer is usually visible in a diff between the current and previous evaluation results.

The deepest layer is trace-level inspection. For any individual test case, the engineer can see the full evaluation trace: the input, the model's output, the judge's score, the judge's reasoning (if using an LLM judge that provides rationale), and the expected result from the golden set. For compound systems, this includes the full pipeline trace — retrieval results, intermediate processing steps, and the final generation.

A design principle that separates good engineering dashboards from adequate ones is **contextual comparison**. When an engineer is looking at a failing test case, the dashboard should automatically show the same test case from the last passing run, the same test case from the current production model, and similar test cases that are currently passing. This context turns a failed test case from a mystery into a comparison problem, dramatically reducing investigation time.

Model version comparison deserves its own view. When a new model version is being evaluated, the engineer needs to see which test cases improved, which degraded, and which are unchanged — sorted by impact severity, filterable by dimension, and exportable for deeper analysis. The best engineering dashboards make it possible to answer "should we ship this model version?" within ten minutes of the evaluation run completing.

## Product Dashboards: The Trend Layer

The product dashboard trades granularity for context. Its primary purpose is to show quality trajectory and help product decisions.

The entry point is a quality health summary organized by product feature, not by eval dimension. An engineer thinks in terms of "accuracy on summarization" or "safety for code generation." A product manager thinks in terms of "the search feature" or "the customer support bot." The product dashboard maps eval dimensions to features so that the product manager sees "search quality: 91%, trending up 2% this month" rather than "retrieval precision: 93%, generation faithfulness: 89%, relevance scoring: 91%."

Quality trends are shown over longer time horizons — weekly and monthly views rather than the hourly views engineers need. The trend lines show where quality is heading, whether recent model updates improved or degraded each feature, and how quality compares across user segments. A product manager making localization decisions needs to see that quality for English users is 93% while quality for Japanese users is 81%, and that the gap has widened over the last two months.

Experiment integration is essential for product dashboards. When the team is running an A/B test comparing two model configurations, the product dashboard should show the quality difference between the two arms alongside the business metrics — conversion rate, satisfaction score, task completion rate. The product manager's question is not "which model has higher eval scores?" but "which model produces better business outcomes?" Quality scores are one input to that decision, not the entirety of it.

The product dashboard should also surface **quality risk exposure** — features where quality is close to the alert threshold, features where quality has been declining steadily, and features where eval coverage is low relative to user traffic. This forward-looking view helps product managers prioritize engineering investment before quality issues reach users, rather than reacting after a quality incident.

## Leadership Dashboards: The Decision Layer

The leadership dashboard is the most abstracted and the most opinionated. Its primary purpose is to inform investment and risk decisions.

The entry point is a single-screen summary showing the overall health of the AI portfolio. Think of it as the vital signs monitor in a hospital — a small number of indicators that tell you whether the patient is stable, improving, or in trouble. For an AI system, the vital signs are: aggregate quality health (are we meeting our quality targets across all features?), quality trend direction (is quality improving, stable, or declining?), cost-quality ratio (how much are we spending per unit of quality, and is that improving?), and risk exposure (how many features are close to quality thresholds or have declining trends?).

Each vital sign should be a color-coded indicator with a one-sentence explanation. Green with "all features meeting quality targets, quality improving 3% month-over-month." Yellow with "two features approaching quality thresholds, investigation in progress." Red with "customer support bot quality below target for three consecutive weeks, remediation plan in review." A VP should be able to assess the AI program's health status in thirty seconds by reading this summary.

The leadership dashboard should show cost-quality trends over time — not individual model costs, but the relationship between quality investment (eval compute, judge costs, human review costs) and quality outcomes. Is more investment producing proportionally more quality? Are there diminishing returns suggesting over-investment in evaluation? Is quality declining despite stable investment, suggesting an efficiency problem? These are the questions leadership needs answered for budget planning.

Competitive positioning, where available, belongs on the leadership dashboard. If your team benchmarks against publicly available quality standards or tracks relative performance against competitors' publicly stated metrics, showing this context helps leadership understand where the organization stands in the market.

## Progressive Disclosure: The Unifying Design Principle

The three dashboard tiers should not be three disconnected applications. They should be one system with **progressive disclosure** — the ability to start at any level and drill down or roll up seamlessly. A VP who sees "customer support bot quality: red" should be able to click into that indicator and see the product-level view showing which quality dimensions are degrading. A product manager who sees "response accuracy trending down" should be able to click into that trend and see the engineering-level view showing which test cases are failing.

Progressive disclosure means that every number on every dashboard is clickable. Every aggregate links to its components. Every trend links to the data points that compose it. Every alert links to the underlying evaluation results that triggered it. The VP never needs to use the engineering dashboard directly, but they can reach any level of detail from their dashboard if they choose to investigate.

This design principle has an organizational benefit beyond convenience. When a VP asks "why is quality declining?" and the product manager can drill down from the leadership view to the product view to find the answer, and the engineer can drill from there to the specific failing test cases, the entire organization is looking at the same data through different lenses. There is one source of truth with three presentation layers, not three disconnected systems that might tell conflicting stories.

## Consistent Time Ranges and Refresh Cadences

A subtle but critical design requirement is consistent time alignment across dashboards. If the engineering dashboard shows data as of 14:00 and the leadership dashboard shows data as of 12:00 because it refreshes less frequently, a VP and an engineer having a conversation about quality will be looking at different numbers. This creates confusion, erodes trust in the data, and wastes time reconciling discrepancies.

The solution is to share a common data pipeline with different aggregation windows. All dashboards read from the same evaluation results store. The engineering dashboard shows the most recent data with minimal aggregation. The product dashboard aggregates the same data into daily or weekly views. The leadership dashboard aggregates further into weekly or monthly views. But when the VP drills down to hourly data, they see the same numbers the engineer sees, because the underlying data is identical.

Refresh cadences can differ — the engineering dashboard might update every few minutes while the leadership dashboard refreshes hourly — but the data for any given time period should be identical across all three views. This is a data architecture requirement, not a dashboard design requirement, and it needs to be built into the evaluation data pipeline from the beginning.

## Clear Ownership of Every Metric

Every metric on every dashboard should have a named owner — a team or individual responsible for that metric's health. When accuracy on the customer support bot drops below threshold, who gets paged? When quality for the search feature has been declining for two weeks, who is responsible for the remediation plan? When the cost-quality ratio for the recommendation engine is above target, who decides whether to invest in optimization?

Ownership should be visible on the dashboard itself. Next to each metric or feature section, show the owning team. This turns the dashboard from a passive display into an accountability tool. When a VP sees a red indicator with "Owner: Search Quality Team" and clicks to see that it has been red for two weeks with no remediation plan, the organizational conversation happens naturally.

The ownership model also determines alert routing. Engineering alerts for a specific eval dimension go to the team that owns that dimension. Product-level quality alerts go to the product manager responsible for that feature. Leadership-level risk alerts go to the engineering director or VP responsible for the AI program. The dashboard design and the alert routing should use the same ownership map.

Building the dashboards described here requires a foundation — the instrumentation, storage, and visualization infrastructure that collects eval data, persists it at scale, and renders it into the views each audience needs. The next subchapter defines that eval observability stack from the ground up.
