# 11.5 — The Eval Gaming Problem: When Teams Optimize for Metrics Instead of Quality

The product team's eval scores have been climbing steadily for three months. Every sprint shows improvement. Accuracy is up four points. Relevance crossed the 90% threshold. Safety scores are near-perfect. The team is celebrated at the company all-hands. Engineering leadership points to the dashboard as proof that investment in AI quality is paying off.

But customer satisfaction surveys tell a different story. Satisfaction has been flat for three months — the same three months that eval scores were climbing. Support ticket volume for the AI product hasn't decreased. User retention in AI-powered features hasn't improved. The eval dashboard and the business metrics are looking at the same product and seeing different realities.

The team hasn't been improving quality. They've been gaming the eval.

## What Eval Gaming Is

**Eval Gaming** is the phenomenon where teams optimize for evaluation metrics rather than the actual quality those metrics are supposed to represent. It is not fraud. It is not intentional deception — at least not usually. It is the predictable consequence of a fundamental law of measurement: when a metric becomes a target, it ceases to be a good metric. Charles Goodhart articulated this in economics. Campbell's Law expressed the same idea in social science. In AI engineering, the pattern plays out with remarkable speed because the feedback loops are tight and the optimization surfaces are vast.

Gaming happens because the map is not the territory. Eval metrics are a map of quality — a simplified, measurable representation of a complex, multidimensional concept. When teams are rewarded for improving the map, they find it is far easier to change the map than to change the territory. A prompt tweak that inflates judge scores by three points takes an afternoon. A genuine quality improvement that users would notice takes weeks of investigation, data work, and iteration. The incentive structure makes gaming not just possible but rational.

## How Gaming Manifests

Gaming takes several forms, and the subtler forms are more dangerous than the obvious ones because they are harder to detect and easier to rationalize.

**Golden set leakage** is the most common form. The team reviews the golden set examples, identifies patterns in what the judge rewards, and tailors their prompts to match those patterns. If the golden set is heavy on customer support queries about billing, the team optimizes for billing-related performance. Performance on billing queries improves — eval scores go up. Performance on product troubleshooting, which the golden set underrepresents, stays the same or degrades. But the dashboard shows improvement because the golden set is the dashboard's only lens.

The insidious version of golden set leakage is not even conscious. Engineers running evaluations repeatedly see the same golden set examples. Their intuitions about what "good" looks like become calibrated to the golden set rather than to the full range of production queries. They make prompt changes that feel like genuine improvements because the outputs look better on the examples they've been staring at for months. The gaming isn't deliberate — it's a cognitive bias built into the workflow.

**Dimension selection bias** is the second form. Teams choose to report eval dimensions where performance is strong and de-emphasize or drop dimensions where performance is weak. If the system scores 93% on relevance but 71% on completeness, the team's sprint review shows the relevance chart prominently and mentions completeness in a footnote. Over time, the dimensions that look good become the dimensions that matter, and the dimensions that look bad disappear from the conversation.

In some organizations, this extends to dropping entire eval dimensions. A team discovers that adding a "conciseness" dimension to their eval pulls their overall score down. Instead of improving conciseness, they argue that conciseness isn't important for their use case and remove the dimension. The argument may even be correct. But the motivation was score improvement, not genuine reassessment of quality priorities.

**Judge prompt engineering for inflation** is the third form, and it's the most technically sophisticated. Teams discover that small changes to judge prompts can shift scores without changing the outputs being evaluated. A judge prompt that says "evaluate whether the response is accurate" might score 85%. The same outputs evaluated by a judge prompt that says "evaluate whether the response contains any factual errors" might score 92% — not because accuracy and error-free mean different things, but because the framing changes how the judge model weights borderline cases. Teams that experiment with judge prompt phrasing will — often without intending to — converge on phrasings that produce higher scores.

**Hard case avoidance** is the fourth form. Teams discover that certain types of inputs reliably produce lower eval scores — ambiguous queries, multi-step reasoning tasks, emotionally charged conversations, edge cases with no clear correct answer. Rather than improving the system's handling of these inputs, the team adjusts input routing to avoid them — redirecting hard cases to human agents, adding disclaimers that preempt the eval criteria, or filtering out query types that drag scores down. The remaining traffic scores higher because the difficult cases have been removed from the evaluation pool.

## Why Gaming Is Rational

Understanding why gaming happens is essential to preventing it. Gaming is not a character flaw. It is a system design problem.

Teams are rewarded for eval scores. Sprint reviews feature eval dashboards. Performance reviews reference quality metrics. Promotion cases cite "improved model quality by X%." When the organizational incentive structure ties career advancement to eval metrics, engineers will — rationally, predictably — optimize for those metrics.

The optimization is often invisible even to the people doing it. An engineer who spends three days experimenting with prompt variations and selects the one that scores highest is not gaming in any malicious sense. They are doing their job as the incentive structure defines it. The problem is not the engineer. The problem is a system that rewards metric improvement without verifying that the improvement reflects genuine quality change.

Gaming also persists because it is difficult to distinguish from real improvement. When eval scores go up, the default assumption is that quality improved. Proving that the improvement is an artifact of gaming requires a counter-evaluation that measures quality independently of the metrics being gamed — and building that counter-evaluation means someone has to believe gaming is happening before there is evidence that it is. This creates a detection problem: the evidence of gaming only appears if you look for it, and you only look for it if you already suspect it.

## Detecting Gaming

Detection starts with one principle: eval scores should correlate with external measures of quality that the team does not control.

The most powerful detection method is the correlation check between eval metrics and business outcomes, described in the monthly recalibration (Section 11.2). When eval scores improve but customer satisfaction, task completion rate, user retention, and support ticket volume remain flat, gaming is the most likely explanation. Genuine quality improvement shows up in both evaluation metrics and business outcomes. Gaming shows up only in evaluation metrics.

**Golden set rotation** is the second detection method. Replace a portion of the golden set each month — twenty to thirty percent of examples — with new examples drawn from recent production traffic. If eval scores drop when the golden set changes and recover within a sprint, the team was optimizing for the old golden set rather than for general quality. Golden set rotation is the single most effective structural defense against golden set leakage. It disrupts the feedback loop between golden set familiarity and prompt optimization.

**Blind evaluation on unseen data** is the third method. Periodically evaluate the system on a test set that the product team has never seen and doesn't know exists. Compare performance on the blind set to performance on the standard golden set. A significant gap — high scores on the golden set, lower scores on the blind set — is a strong signal that the team has overfitted to the golden set. The blind set should be curated by someone outside the product team — the platform eval team, a quality assurance specialist, or a rotating evaluation committee.

**Judge prompt auditing** is the fourth method. Review the judge prompts in use and compare them to the originals. If judge prompts have been modified — particularly if the modifications are consistently in the direction of higher scores — investigate whether the changes reflect genuine refinement of evaluation criteria or inflation. Track judge prompt changes over time with version control. Require review for judge prompt modifications the same way you require review for production code changes.

## Preventing Gaming

Detection catches gaming after it happens. Prevention addresses the structural conditions that make gaming rational.

The most effective prevention is separating the people who set eval criteria from the people who optimize for them. When the same team defines what "good" means and is rewarded for achieving it, the definition inevitably drifts toward what the team can achieve easily. When an independent team — the platform eval team, a quality assurance function, or a cross-functional eval committee — sets and maintains eval criteria, the criteria reflect quality standards rather than team convenience.

This separation doesn't mean product teams have no voice in defining eval criteria. They should actively participate in criteria design, because they understand the product best. But the final authority over criteria changes should sit with someone whose incentives are aligned with quality rather than with metric improvement. In practice, this often means the platform eval team approves criteria changes proposed by product teams, ensuring that criteria evolve based on legitimate quality reassessments rather than score optimization.

A second prevention mechanism is multi-signal evaluation. Instead of relying on a single eval pipeline with a single set of judges, evaluate quality through multiple independent signals: automated judge scores, human reviewer ratings, user feedback metrics, and business outcome correlations. Gaming one signal is straightforward. Gaming four independent signals simultaneously is nearly impossible, because each signal measures quality from a different angle with different biases.

A third mechanism is including "surprise" evaluation dimensions — quality criteria that are evaluated but not reported to the product team until the monthly recalibration. If the team doesn't know that conciseness and emotional calibration are being measured, they can't optimize for those dimensions. When the surprise dimensions are revealed during monthly recalibration and compared to the team's standard metrics, any divergence highlights areas where the team's optimization narrowed quality rather than broadened it.

A fourth mechanism is rewarding quality outcomes rather than quality scores. Instead of celebrating "accuracy improved from 88% to 93%," celebrate "customer satisfaction for AI-resolved tickets improved from 72% to 79%." Instead of tracking eval score improvement as a team metric, track the correlation between eval scores and business outcomes. When the organizational reward structure is anchored to outcomes rather than eval metrics, the incentive to game eval metrics diminishes because gaming doesn't produce the outcomes that matter.

## The Goodhart Spiral

When gaming goes undetected, it creates a self-reinforcing cycle that experienced evaluation engineers call **The Goodhart Spiral**. Eval scores improve due to gaming. Leadership sees the improvement and raises the target. The team games harder to hit the new target. Scores improve again. Targets rise again. Each cycle increases the gap between eval metrics and actual quality while increasing the organizational confidence in those metrics.

The spiral breaks in one of two ways. Either someone detects the gap — usually through a business outcome analysis, a customer complaint investigation, or an independent audit — and the organization experiences a trust crisis in its evaluation system. Or the gap eventually becomes large enough that a quality failure reaches users at a scale that cannot be ignored, and the investigation traces the failure back to evaluation metrics that had long since stopped reflecting reality.

Breaking the spiral early requires the monthly recalibration practice described in 11.2. The correlation check between eval metrics and business outcomes is the circuit breaker that detects Goodhart dynamics before they reach crisis levels. Organizations that skip the monthly recalibration are the ones most vulnerable to the spiral — because they have no mechanism for asking the question that matters: are our improving eval scores actually improving our product?

## Gaming as Organizational Feedback

There is one more perspective on eval gaming that experienced teams learn to hold: gaming is information. When a team games eval metrics, it reveals something about the incentive structure, the metric design, or the organizational culture that is worth understanding.

If teams game because eval improvement is the only way to get recognition for quality work, the problem is the recognition system, not the team. Fix the incentive structure so that teams can be recognized for genuine quality improvements that don't show up as simple score increases — improving handling of edge cases, expanding coverage to new input types, reducing the variance in output quality.

If teams game because the eval criteria don't measure what actually matters to users, the problem is the criteria design, not the team. Fix the criteria so that optimizing for eval scores and optimizing for user experience are the same thing. When your eval metrics perfectly capture what users value, gaming becomes impossible because there is no gap between the metric and the reality.

If teams game because they don't have the time, resources, or expertise to make genuine quality improvements, the problem is resource allocation, not the team. Genuine quality improvement is harder and slower than metric manipulation. If the team's capacity only allows for the fast, surface-level optimization, they will take it.

Every instance of eval gaming is a diagnostic signal. The question is not just "how do we stop this team from gaming?" It is "what does this gaming behavior tell us about our evaluation system, our incentive structure, and our organizational design?"

Evaluation governance does not stop at gaming prevention. As AI systems handle sensitive data and operate under regulatory requirements, the eval system itself must comply with privacy, retention, and access control constraints that most engineering teams don't think about until an audit arrives. The next subchapter covers eval data privacy and compliance.
