# 2.7 — Adaptive Sampling: Adjusting Rates Based on Observed Quality Trends

Every sampling strategy discussed so far uses rates set by policy — a fixed percentage, a risk multiplier, a post-deploy surge. You configure the rate, deploy it, and the system evaluates at that rate until someone manually changes it. Adaptive sampling goes further. It observes the results of recent evaluations and adjusts sampling rates in real time based on what the system is actually seeing. When quality drops in a segment, sampling increases there automatically. When quality is stable for weeks, sampling decreases to save budget. The system learns where to look harder and where to look less, without waiting for a human to notice the trend and file a configuration change.

This is the difference between a security camera on a fixed timer and a security camera that tracks motion. Both record footage. One records what matters.

## The Core Idea: Feedback-Driven Sampling

Traditional sampling strategies are open-loop. You set a rate, and the rate stays the same regardless of what the evaluations find. Adaptive sampling is closed-loop. Evaluation results feed back into the sampling decision, creating a continuous cycle where the system's own quality measurements determine where the next evaluation budget is spent.

The concept is borrowed from industrial quality control, where adaptive inspection rates have been standard practice for decades. A manufacturing line that produces zero defects for a thousand units in a row reduces its inspection frequency. The moment a defect appears, inspection frequency jumps back up. The logic is straightforward: stable processes need less monitoring, unstable processes need more. The same principle applies to AI evaluation at scale, but with an important difference. Manufacturing defects are usually binary — the part is within tolerance or it is not. AI quality exists on a spectrum, degrades gradually, and can shift differently across segments of your traffic. Adaptive sampling for AI systems must handle continuous quality scores, per-segment trends, and the ambiguity of what "declining quality" actually means for different use cases.

The practical benefit is substantial. A team running fixed-rate sampling at 2% across all segments spends the same evaluation budget on a segment that has been perfectly stable for three months as it does on a segment where quality dropped 15% last week. Adaptive sampling reallocates that budget automatically — pulling evaluation capacity away from the stable segment and concentrating it on the segment that needs attention. Industry teams that have implemented adaptive sampling report 30 to 50 percent reductions in total eval spend compared to fixed-rate strategies, while maintaining or improving their ability to detect quality issues. The savings come not from evaluating less overall, but from evaluating smarter — putting budget where the signal is, not where the silence is.

## How the Sampling Controller Works

**The Sampling Controller** is the component that implements adaptive sampling. Think of it as a small feedback loop that sits between your evaluation results and your sampling configuration. It runs on a schedule — typically hourly or daily, depending on your traffic volume — and performs three operations each cycle.

First, it reads recent evaluation results for each segment. A segment can be a product feature, a user cohort, a language, a risk tier, or any other dimension you have defined in your stratified sampling strategy. For each segment, the controller computes quality trend indicators from the most recent evaluation window — typically the last 24 to 72 hours, though the window size is configurable and should match the cadence at which your system's quality realistically shifts.

Second, it compares those trend indicators against historical baselines. The baseline is the segment's typical quality level over a longer period — the last 30 to 90 days. The comparison produces a signal: is this segment's quality stable, improving, or declining relative to its own history? The comparison is per-segment, not global. A segment with a baseline pass rate of 92% that drops to 87% is in trouble, even if another segment with a baseline of 98% just rose to 99%.

Third, the controller adjusts the sampling rate for each segment based on that signal. Declining quality increases the sampling rate — you want more data to understand the problem. Stable quality reduces the sampling rate — you have enough confidence. Improving quality can also reduce the rate, though some teams keep elevated rates during improvement to confirm the trend is real rather than noise.

The adjusted rates are written back to the sampling configuration, and the next batch of requests is sampled at the new rates. The cycle repeats. Over time, the system converges on a sampling allocation that concentrates budget on the segments that need it most while minimizing spend on segments that are running well.

## Quality Trend Indicators

The Sampling Controller needs signals to make its decisions. Raw evaluation scores are too noisy — individual evaluations vary, judge models have their own variance, and a single low score might be an outlier rather than a trend. The controller works with aggregate trend indicators that smooth out noise and surface genuine shifts.

**Rolling pass rate** is the most straightforward indicator. For each segment, compute the percentage of evaluated outputs that met or exceeded the quality threshold over the recent window. If your threshold for the customer support segment is a judge score of 0.8, and 340 out of 400 recently evaluated outputs scored 0.8 or higher, the rolling pass rate is 85%. Compare this to the 30-day baseline. If the baseline pass rate is 91%, the segment is underperforming. The controller increases sampling.

**Rolling score distribution** captures more nuance than pass rate alone. A segment can maintain the same pass rate while its score distribution shifts. If outputs that used to score 0.95 are now scoring 0.82 — still passing, but barely — the pass rate looks fine while quality is actually eroding. Tracking the median score, the 25th percentile, and the standard deviation of the score distribution reveals these shifts before they cross the pass-rate threshold. A widening standard deviation, in particular, signals increasing inconsistency — some outputs are still excellent while others are degrading. This pattern often precedes a broader quality collapse.

**Rate of change** matters as much as absolute level. A segment that dropped from 94% to 91% pass rate over three days is more concerning than a segment that has been stable at 88% for months. The first is declining. The second is consistently below target but not worsening. The controller should react more aggressively to rapid declines than to stable underperformance. Rapid declines suggest something changed — a model update, a prompt regression, a shift in input distribution — and the faster you detect the cause, the faster you can fix it.

**Comparison to historical baselines** prevents the controller from treating natural variance as a signal. Quality scores fluctuate. A segment that bounces between 90% and 93% pass rate week over week is not declining when it hits 90% — that is within its normal range. The baseline establishes what "normal" looks like for each segment, so the controller only reacts when quality moves outside the expected range. Setting the sensitivity threshold is a tuning decision. Too sensitive, and the controller chases noise, constantly adjusting rates based on random fluctuation. Too lenient, and it misses real degradation. Most teams start with a threshold of two standard deviations from the baseline mean and adjust based on operational experience.

## Guardrails: Bounding the Controller

An adaptive system without guardrails is an adaptive system that will eventually do something catastrophic. The Sampling Controller needs hard limits on how far it can adjust rates, in both directions.

The minimum sampling rate per segment ensures that no segment ever drops to zero evaluation. Even the most stable, highest-quality segment should maintain a baseline level of monitoring. The floor serves two purposes. First, it provides ongoing evidence that the segment is actually stable — you need fresh evaluations to confirm the trend, not just the absence of evaluations. Second, it protects against silent failures that the controller itself would not detect. If a segment's quality collapses so suddenly that no evaluations happen between the collapse and the next controller cycle, the controller has no data to react to. The minimum rate ensures there is always data. A floor of 0.1% to 0.5% of traffic works for most segments. For high-risk segments — healthcare, financial, legal — set the floor higher, typically 1% to 2%.

The maximum sampling rate per segment prevents the controller from consuming the entire eval budget on a single problematic segment. If one segment's quality drops sharply, the controller will try to increase sampling dramatically. Without a ceiling, it could allocate 80% of your eval budget to one segment, starving everything else. Set maximum rates per risk tier — perhaps 5% for low-risk segments, 10% for medium, and 20% for high-risk. Critical-risk segments might go higher during active incidents, but this should require human approval rather than automatic controller action.

The global budget cap is the ultimate guardrail. The controller can redistribute budget between segments, but it cannot create new budget. If total eval capacity is 10,000 evaluations per day, the controller can shift 3,000 from stable segments to declining segments, but it cannot increase the total to 15,000. When all segments are declining simultaneously — which happens during major model updates or infrastructure incidents — the controller hits the budget cap and must prioritize. The prioritization logic should follow the risk-tier hierarchy established in your risk-based sampling strategy: critical segments get budget first, then high-risk, then the rest.

Rate change velocity limits prevent the controller from making large adjustments in a single cycle. A segment should not jump from 0.5% sampling to 15% sampling in one hour. Gradual adjustments — capping changes at 2x per cycle, for example — give the system time to accumulate data at the new rate before deciding whether to adjust further. Rapid oscillation between high and low rates wastes budget and produces noisy data.

## The Efficiency Gain in Practice

A B2B SaaS company operating four AI-powered products implemented adaptive sampling in late 2025 after running fixed-rate stratified sampling for eight months. Their fixed-rate strategy evaluated 2% of all traffic across every segment, costing approximately $22,000 per month in judge API calls. Their evaluation team knew the spend was inefficient — three of their twelve segments had been perfectly stable for months, consuming budget without producing actionable insights — but manually adjusting rates required engineering time they did not have.

After deploying a Sampling Controller with hourly cycles, the system converged within two weeks. Stable segments dropped to their configured minimum rates of 0.3%. Two segments that had been showing gradual quality drift — a multilingual feature and a document summarization product — increased to 4% and 6% respectively. Total eval spend dropped to $13,500 per month, a 39% reduction. More importantly, the elevated sampling on the drifting segments caught a quality regression in the summarization product three days earlier than the fixed-rate strategy would have detected it, based on retrospective analysis. The faster detection saved an estimated $40,000 in customer-facing impact by enabling a quicker rollback.

The efficiency gain is not uniform across organizations. Teams with highly variable quality across segments see the largest savings, because adaptive sampling concentrates budget where it matters most. Teams with uniformly stable quality see smaller gains — there is less waste to eliminate. Teams with uniformly unstable quality see almost no gains, because the controller wants to increase sampling everywhere and quickly hits the budget cap. The sweet spot is an organization with a mix of stable and unstable segments, which is where most production AI systems land.

## The Complexity Cost

Adaptive sampling is not free infrastructure. It introduces moving parts that need their own monitoring, their own debugging, and their own operational discipline.

The controller itself can malfunction. A bug in the trend computation logic, a stale baseline that no longer reflects the segment's true quality, or a misconfigured sensitivity threshold can cause the controller to make bad decisions systematically. When the controller under-samples a declining segment because its baseline is wrong, you lose the very protection the adaptive system was supposed to provide. This means you need eval-of-eval monitoring specifically for the controller — alerts when sampling rates change dramatically, dashboards showing the controller's decisions and the data that drove them, and periodic human review of whether the controller's behavior matches what an experienced engineer would do.

Debugging becomes harder. When someone asks "why did we miss that quality issue in the translation segment?", the answer might be "because the controller reduced sampling there two weeks ago based on stable trends, and the quality shift happened in the low-sampling window." Tracing that chain of causation requires logging every controller decision, every trend indicator computation, and every rate adjustment. Without that audit trail, adaptive sampling is a black box that sometimes produces surprising gaps in coverage.

The feedback loop can create perverse incentives if not carefully designed. If the controller increases sampling when quality drops, and increased sampling catches more low-quality outputs, the metrics might show quality dropping further simply because you are now measuring more of the distribution. This creates a positive feedback loop — quality looks worse, sampling increases, quality looks even worse. The fix is to compare quality metrics at a consistent sampling rate, not the adaptive rate. Use the adaptive rate for operational sampling but compute trend indicators using a statistically consistent comparison method.

Team trust is another cost. Engineers who are accustomed to fixed-rate sampling understand intuitively how much of their traffic is being evaluated. With adaptive sampling, the rate is constantly changing, and engineers need to trust that the controller is making good decisions. Building that trust requires transparency — dashboards that show current rates per segment, trend indicators driving each decision, and historical accuracy of the controller's adjustments.

## When Adaptive Sampling Is Overkill

Not every organization needs adaptive sampling. The infrastructure cost, operational complexity, and debugging overhead are justified only when the efficiency gains are large enough to matter.

Below 100,000 daily requests, fixed-rate sampling strategies are usually sufficient. At this volume, even a generous 5% sampling rate produces only 5,000 evaluations per day — manageable both in cost and in result review. The potential savings from adaptive sampling at this scale might be a few hundred dollars per month, which does not justify the engineering investment to build and maintain the controller.

Between 100,000 and 1 million daily requests, adaptive sampling becomes attractive if you have significant segment diversity. If most of your traffic flows through two or three segments with similar quality characteristics, fixed-rate sampling with occasional manual adjustment works fine. If you have a dozen or more segments with varying quality levels, stability patterns, and risk profiles, the manual adjustment burden becomes real and adaptive sampling starts to pay for itself.

Above 1 million daily requests, adaptive sampling is nearly always worth the investment. At this volume, even small percentage-point improvements in sampling efficiency translate to thousands of dollars per month in eval savings. The operational complexity of manually managing sampling rates across dozens of segments at this scale is prohibitive. The controller does in minutes what would take an engineer hours of analysis, and it does it every cycle without fatigue, without forgetting a segment, and without waiting for someone to notice that the configuration needs updating.

The other factor is rate of change. If your system's quality profile is stable — same models, same prompts, same user distribution, low deployment frequency — adaptive sampling adds complexity without proportionate benefit. If your system changes frequently — regular model updates, prompt iterations, shifting user demographics, expanding to new markets — adaptive sampling's ability to respond to change without human intervention becomes essential.

## Monitoring the Monitor

The most dangerous failure mode in adaptive sampling is a malfunctioning controller that silently reduces sampling on a segment that is actually degrading. To guard against this, you need a separate monitoring layer that watches the controller itself.

Track the controller's decision accuracy over time. Periodically run a "shadow evaluation" at a fixed rate on a random subset of segments, regardless of the controller's adaptive rate. Compare the quality metrics from the shadow evaluation to the metrics the controller is using to make its decisions. If they diverge significantly, the controller's inputs are wrong — perhaps its baselines are stale, its trend computation is buggy, or its data pipeline has a gap.

Monitor for coverage blind spots. If any segment has not received a meaningful evaluation in more than 48 hours, that is an alert condition regardless of how stable the segment appeared. The minimum sampling rate should prevent this in theory, but in practice, low-volume segments with low sampling rates can go days without a single sampled request during traffic dips.

Review the controller's rate change history monthly. An experienced engineer should look at every significant rate change the controller made — every time it doubled or halved a segment's rate — and evaluate whether the decision made sense given the data. This manual review catches systematic biases that automated monitoring might miss. A controller that consistently over-reacts to normal variance, or one that consistently under-reacts to genuine shifts, needs its sensitivity thresholds recalibrated.

Log everything. Every trend indicator computation, every baseline comparison, every rate adjustment, every guardrail activation. When something goes wrong — and it will — the logs are how you figure out what the controller saw, what it decided, and why the decision was wrong. Without comprehensive logging, debugging the controller is guesswork.

---

All the sampling strategies in this chapter — fixed-rate, stratified, risk-based, change-based, anomaly-triggered, and adaptive — share one constraint they cannot escape: a finite evaluation budget. The final question is how to allocate that budget when multiple products, features, and risk tiers all compete for the same evaluation capacity.
