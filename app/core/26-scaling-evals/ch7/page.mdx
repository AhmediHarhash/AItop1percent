# Chapter 7 — Human Review at Scale: The Collapse and Reinvention of Human-in-the-Loop

Human-in-the-loop evaluation hit a wall in 2025. The volume of AI outputs outpaced the capacity of human reviewers by orders of magnitude, and teams that tried to scale review linearly with traffic either went bankrupt or abandoned review entirely. The solution is not more humans — it is a fundamentally different role for humans. This chapter covers the transition from humans reviewing outputs to humans calibrating the systems that review outputs, and the operational infrastructure required to make that transition work.

---

- **7.1** — Why Human-in-the-Loop Hit a Wall in 2025
- **7.2** — The Routing Strategy: What Gets Human Eyes and What Does Not
- **7.3** — Reviewer Workforce Design: Internal vs External, Specialists vs Generalists
- **7.4** — Reviewer Calibration and Inter-Annotator Agreement Across Large Teams
- **7.5** — Review Queue Management: Prioritization, Load Balancing, and Turnaround SLAs
- **7.6** — Human Review as Calibration Signal: Training Your Automated Judges with Human Feedback
- **7.7** — The Economics of Human Review: Cost Per Review, Budget Allocation, and ROI
- **7.8** — The AI-Oversees-AI Pattern: Automated Triage with Human Escalation

---

*Human review and automated evaluation together measure output quality — whether the response is good. But output quality is not the same as outcome quality. The next chapter bridges that gap, connecting what the model says to what the business achieves.*
