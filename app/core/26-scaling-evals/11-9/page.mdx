# 11.9 — Eval Governance at Scale: Approval Workflows, Change Control, and Red-Team Convergence

Governance is what prevents your evaluation system from becoming a lawless frontier where anyone can change eval criteria, modify golden sets, or adjust pass thresholds without oversight. At scale, governance is not bureaucracy — it is the immune system that protects eval integrity. Without it, every well-intentioned optimization risks introducing bias, every quick fix risks breaking a quality gate that another team depends on, and every undocumented change creates an audit gap that compounds silently until an auditor or a production incident surfaces it.

A mid-sized SaaS company experienced this in the second half of 2025. Three product teams shared eval infrastructure but operated without formal change control. Over a four-month period, seventeen changes were made to shared judge configurations, golden set contents, and pass thresholds. None were formally reviewed. None were documented beyond the git commit message. When the company's quality metrics showed a slow but steady decline that nobody could explain, it took two engineers three weeks of forensic analysis to trace the degradation to a single threshold change made by an engineer on a different team — a change that was reasonable for that team's product but had cascading effects on the shared safety evaluation pipeline. The fix took an hour. Finding it took 240 engineering hours.

## Why Eval Changes Need Governance

Code changes go through code review. Infrastructure changes go through change management. Database schema changes go through migration review. But evaluation changes — modifications to the criteria, thresholds, golden sets, and judge configurations that determine whether your AI product is good enough to ship — frequently bypass any review process at all.

This happens because teams think of eval changes as configuration rather than code. Adjusting a pass threshold from 0.85 to 0.80 feels like changing a setting, not changing a system. Adding ten examples to a golden set feels like routine maintenance, not a structural modification. Updating a judge prompt feels like tweaking a tool, not altering a measurement instrument. But each of these changes affects what your evaluation system measures, how it measures, and what it considers acceptable. A threshold change can turn a failing deployment into a passing one. A golden set change can shift eval coverage away from a critical use case. A judge prompt change can recalibrate how harshly or leniently the entire system scores quality.

These changes are invisible to the rest of the organization. Nobody gets an alert when a threshold is adjusted. Nobody sees a dashboard indicator when golden set coverage shifts. The quality metrics might change — slightly better numbers one week, slightly worse the next — but nobody connects the metric movement to the eval configuration change because nobody knew the change happened. This invisibility is what makes ungoverned eval changes so dangerous. The system appears stable when in fact its measurement foundation is shifting.

## The Eval Change Control Process

The governance process for eval changes should mirror the code review process — because eval changes are, in effect, changes to the definition of quality. The process has four steps: propose, review, approve, deploy.

**Propose.** Any eval change — a threshold adjustment, a golden set modification, a judge prompt update, a new eval dimension, a change to the safety evaluation pipeline — starts as a formal proposal. The proposal states what is being changed, why, what the expected impact is, and what evidence supports the change. "Lowering the accuracy threshold from 0.88 to 0.84 because our new product feature introduces a use case where current accuracy measurement overstates failures" is a proposal. "Tweaking the threshold because we keep failing the gate" is not.

**Review.** The proposal is reviewed by at least one person who was not involved in the change. For changes to shared infrastructure — safety evaluation pipelines, shared judge configurations, cross-team reporting standards — the reviewer should be from the platform team or a different product team. For changes to product-specific criteria, the reviewer can be from the same product team but should not be the person who proposed the change. The review checks three things. First, is the justification sound? Does the evidence support the change? Second, what is the blast radius? Does this change affect other teams, other evaluations, or other quality gates? Third, is the change reversible? If the change has unintended consequences, can it be rolled back quickly?

**Approve.** Approval authority depends on the scope of the change. Changes to product-specific eval criteria — a team adjusting its own thresholds or updating its own golden set — require approval from the product team's eval owner. Changes to shared infrastructure — judge configurations, pipeline settings, safety evaluation rules — require approval from the platform team lead and notification to all affected product teams. Changes to compliance-related evaluation — safety thresholds, regulatory testing cadences, audit documentation — require approval from the compliance or legal function.

**Deploy.** The change is implemented through the same deployment pipeline as any other infrastructure change, with a rollback plan documented and tested before deployment. Post-deployment, the change is verified by comparing eval metrics before and after to confirm the expected impact materialized and no unintended side effects appeared.

## The Eval Configuration as Versioned Code

Eval governance becomes practical when eval configuration is treated as code — versioned, reviewed, diffable, and auditable. Every component of the eval system that affects measurement should live in a version-controlled repository: judge prompts, pass thresholds, golden set manifests, safety evaluation rules, sampling configurations, alert thresholds, and pipeline settings.

Version control provides three capabilities that governance requires. The first is auditability. At any point, you can see the complete history of every eval configuration change — who made it, when, why, and what the configuration looked like before and after. This is not just good practice. It is a regulatory requirement under the EU AI Act, which mandates documented risk management processes and the ability to demonstrate how evaluation methodology has evolved.

The second is diff-ability. When quality metrics shift unexpectedly, you can compare the current eval configuration against the configuration from a week ago, a month ago, or any specific date. The diff shows exactly what changed. The three-week forensic investigation described in the opening story would have taken thirty minutes with versioned eval configuration — pull the diffs for the period in question, identify which changes coincided with the metric shift, investigate those changes.

The third is rollback. When a change produces unintended consequences, you can revert to the previous configuration instantly. No manual reconstruction of previous settings. No guessing what the judge prompt used to say. The version control system stores the complete history, and reverting is a single operation.

The practical implementation is straightforward. Eval configurations live in a git repository or a dedicated configuration management system. Changes are submitted as pull requests. Reviewers comment, request modifications, or approve. Approved changes are merged and deployed through the standard pipeline. The same workflow that governs application code governs eval configuration.

## Red-Team Convergence

Organizations that reach a certain scale find themselves running adversarial testing in multiple places simultaneously: the safety evaluation program conducts automated red-teaming on a regular cadence, the security team conducts penetration testing that includes AI-specific attack vectors, the compliance team commissions third-party audits that include adversarial testing, and individual product teams run their own domain-specific red-team exercises. Without governance, these activities operate independently, producing overlapping findings, conflicting severity assessments, and inconsistent remediation timelines.

**Red-team convergence** is the governance process that unifies these parallel adversarial testing streams into a single, coherent view of your system's vulnerability landscape. It has three components.

The first is a shared vulnerability taxonomy. Every adversarial testing activity — regardless of which team conducts it — classifies its findings using the same taxonomy of vulnerability types. Jailbreak vulnerabilities, bias manifestations, privacy leakage vectors, hallucination-inducing inputs, safety bypass techniques — each has a defined category, severity scale, and required response timeline. Without a shared taxonomy, one team's "critical" is another team's "moderate," and prioritization becomes political rather than risk-based.

The second is a unified findings register. Every adversarial finding from every source — automated red-teaming, manual red-teaming, security testing, compliance audits, production incidents — is recorded in a single register. The register tracks the finding, its severity, the affected products, the assigned owner, the remediation status, and the verification that the remediation was effective. This register is the single source of truth for "what vulnerabilities do we know about and what are we doing about them." Without it, findings discovered by the security team may never reach the product team whose system is vulnerable.

The third is a convergence review. Monthly or quarterly, the stakeholders from all adversarial testing streams meet to review the unified findings register, identify patterns across findings, and assess whether the organization's overall vulnerability posture is improving or degrading. This review is not a status meeting where each team reports independently. It is a synthesis meeting where the participants look for systemic patterns that individual testing streams cannot see. When the automated red-team finds that jailbreak vulnerability X was patched in Product A but still exists in Product B, the convergence review catches it. When the compliance audit finds that a remediation claimed as "complete" does not actually prevent the vulnerability in production, the convergence review surfaces the gap.

## Periodic Eval Governance Reviews

Beyond change-level governance, the evaluation system itself needs periodic review at a higher level of abstraction. Are we evaluating the right things? Are our thresholds set at appropriate levels? Are our golden sets representative of current production traffic? Are our judges still calibrated? These questions cannot be answered by reviewing individual changes. They require stepping back and assessing the evaluation system as a whole.

The **quarterly eval governance review** serves this function. It examines four questions across the entire eval program.

The first question is coverage. Are there production use cases, user segments, or failure modes that the evaluation system does not cover? Coverage gaps accumulate silently as products evolve. A use case that launched three months ago may not have eval coverage because nobody added it to the golden set. A user segment that shifted behavior may not be represented in the evaluation samples. The quarterly review identifies these gaps by comparing the eval system's coverage against current production traffic patterns.

The second question is calibration. Are the evaluation judges producing scores that correlate with human judgment? Judge drift — the gradual divergence between automated scores and human quality assessments — is a slow-moving threat that is easy to miss on a daily basis but obvious on a quarterly scale. The quarterly review compares a sample of recent automated scores against fresh human evaluations of the same outputs. If the correlation has dropped, the judges need recalibration.

The third question is threshold appropriateness. Are the pass thresholds still set at the right levels? Thresholds that were appropriate six months ago may be too lenient if user expectations have risen, or too strict if a new use case has legitimate lower-quality outputs that the threshold was never designed to accommodate. The quarterly review examines threshold violation rates. If a threshold is never violated, it may be too lenient. If it is frequently violated without any corresponding user complaints, it may be too strict.

The fourth question is impact. Have the evaluation findings led to actual quality improvements? An evaluation system that identifies problems but does not lead to fixes is a measurement system without teeth. The quarterly review tracks the full cycle: finding identified, remediation planned, remediation implemented, verification completed. If the conversion rate from finding to verified fix is low, the governance process is producing documentation without producing improvement.

## The Governance Overhead Problem

The objection to eval governance is always the same: it slows things down. And the objection is not wrong. A change that previously took five minutes — update a threshold, commit, push — now takes two days to move through proposal, review, approval, and deployment. Engineers chafe at the process. Product managers worry about iteration speed. Teams that operated independently for months resist the introduction of oversight.

The response to this objection is not to deny the overhead. It is to make the overhead proportional to the risk. Not every eval change carries the same risk, and the governance process should reflect that distinction.

Low-risk changes — adding a few examples to a product-specific golden set, adjusting a formatting criterion, updating a judge prompt for a non-safety dimension — go through lightweight review. One reviewer, same-day turnaround, asynchronous approval. The friction is minimal.

Medium-risk changes — adjusting pass thresholds, modifying judge scoring rubrics, changing sampling rates — go through standard review. Two reviewers, one from outside the team, 48-hour turnaround. The friction is noticeable but manageable.

High-risk changes — modifying safety evaluation criteria, changing compliance-related thresholds, altering the adversarial testing methodology — go through full review. Multiple reviewers including compliance, explicit approval from the platform team lead, verified deployment with post-change monitoring. The friction is significant and intentional.

Calibrating the governance process to the risk level prevents the worst outcome: teams abandoning the process entirely because it treats every change as high-risk. A governance process that nobody follows is worse than no governance process at all, because it creates the illusion of oversight without the reality.

## Building a Governance Culture

Governance is ultimately a cultural practice, not a technical system. The tools — version control, review workflows, approval gates — are necessary but not sufficient. What makes governance work is a team culture that views eval configuration as important enough to protect. An engineer who considers adjusting a pass threshold to be as consequential as changing a database schema will follow the review process without being forced. An engineer who views eval thresholds as arbitrary settings will route around any governance process you build.

Building this culture starts with making the consequences visible. When the quarterly governance review traces a quality incident back to an ungoverned eval change, share that story with the engineering organization. When a governed change process catches a mistake that would have caused a regression, celebrate the catch. When the audit trail enables a two-hour investigation instead of a three-week forensic analysis, quantify the time saved and make it known.

The goal is not to make engineers afraid of changing eval configurations. The goal is to make them thoughtful about it — to treat eval changes with the same care they apply to database migrations, API contract changes, and production infrastructure modifications. Eval governance at scale is what allows the evaluation system to evolve without degrading, to improve without introducing blind spots, and to scale across teams without fragmenting into inconsistency.

The final subchapter of this chapter — and of this entire section — synthesizes everything taught across all eleven chapters into a single framework: the eval maturity model, revisited with the operational depth to make each level actionable.
