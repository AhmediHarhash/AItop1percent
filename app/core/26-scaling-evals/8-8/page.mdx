# 8.8 — The Outcome Feedback Loop: From Business Metrics Back to Eval Criteria

Evaluation criteria are hypotheses. When you define "a good output is accurate, relevant, and well-formatted," you're hypothesizing that outputs meeting those criteria will produce good business outcomes. The outcome feedback loop tests that hypothesis continuously — and revises the criteria when reality disagrees.

## Why Eval Criteria Must Evolve

Evaluation criteria are set by humans based on their best understanding of what quality means for the product. But human understanding is imperfect, and it doesn't update automatically. The team that defined "helpfulness" as the primary quality dimension for a customer support AI may not realize that "resolution speed" — how quickly the response gets the user to a solution — matters more for customer satisfaction than how comprehensive the answer is.

Without outcome data flowing back into eval criteria, evaluation drifts away from business reality. The judges score outputs against criteria that were correct at launch but are no longer aligned with what actually matters to users and the business. The eval system reports health while business metrics decline, because the eval system is measuring the wrong thing.

## The Feedback Loop Structure

**The Outcome Feedback Loop** connects business outcome data back to evaluation criteria through a structured process. Quarterly — or whenever outcome data reveals a significant disconnect — the team reviews the correlation between output quality scores and business outcomes. Dimensions where judge scores predict business outcomes well are validated. Dimensions where scores and outcomes diverge are investigated.

Investigation follows a specific pattern. First, identify the gap: which output quality dimension is disconnected from outcomes? Second, analyze examples: pull outputs that scored high on quality but led to poor outcomes, and outputs that scored low but led to good outcomes. Third, diagnose the cause: is the dimension wrong (measuring the wrong thing), miscalibrated (measuring the right thing at the wrong threshold), or incomplete (missing an aspect that matters)?

The diagnosis drives revision. If the dimension is wrong, replace it. If it's miscalibrated, adjust thresholds. If it's incomplete, add the missing aspect. Then update the judge configuration, recalibrate against human judgment, and monitor whether the revised criteria better predict outcomes.

## Discovering New Quality Dimensions

The most valuable outcome of the feedback loop is the discovery of quality dimensions that the team hadn't considered. These emerge from outcome data that defies existing evaluation criteria. When outputs that score well on all existing dimensions consistently produce poor outcomes in a specific context, the missing dimension is usually the explanation.

A real pattern in 2025-2026: teams building AI assistants discovered that "actionability" — whether the response tells the user exactly what to do next — predicted customer satisfaction better than "accuracy" or "completeness." Accurate, complete responses that left the user uncertain about the next step produced worse outcomes than slightly less complete responses that ended with a clear call to action. This dimension wasn't in the original evaluation criteria because it's not obvious from the output alone — it only became visible through outcome data.

## Closing the Loop Operationally

The feedback loop requires three organizational commitments. First, regular access to outcome data. The AI team must have visibility into business metrics — not just AI metrics — on a cadence that allows correlation analysis. This often requires data sharing agreements with product, analytics, and business teams.

Second, a review cadence. Quarterly outcome review sessions where the AI team examines the relationship between eval scores and business outcomes, identifies disconnects, and proposes criteria revisions. These sessions should include representatives from product and business teams who can provide context about what outcomes matter and why.

Third, eval criteria change management. Revising evaluation criteria is not a casual decision — it changes the measurement standard that governs model development, release gating, and quality reporting. Changes should be documented, justified by outcome data, validated through human review, and communicated to all stakeholders.

## The Virtuous Cycle

When the outcome feedback loop operates well, it creates a virtuous cycle. Better eval criteria lead to more targeted model improvements. More targeted improvements lead to better business outcomes. Better outcomes generate more informative outcome data. More informative data reveals further opportunities for eval criteria refinement. Each cycle tightens the alignment between what your evaluation system measures and what your business actually needs.

The teams that achieve this virtuous cycle — where evaluation criteria are continuously calibrated against business outcomes — are the teams that extract the most value from their AI investments. They avoid the trap of optimizing for proxy metrics that have drifted away from reality, and they catch quality regressions that matter to the business rather than quality regressions that matter only to the eval system.

Outcome evaluation measures what the AI system achieves. The next chapter turns the lens inward — monitoring the health, reliability, and trustworthiness of the evaluation systems themselves.
