# 5.2 — Batch Processing for Evaluation: Throughput Optimization and Prompt Bucketing

Most evaluation at scale runs in batches. Not real-time, not streaming, not per-request — batches. Your nightly quality report evaluates the day's sampled outputs in a single run. Your release gate evaluates the golden set in one sweep before the deploy decision. Your weekly adversarial audit processes the entire adversarial library in a scheduled window. Understanding how to optimize batch throughput — and specifically how to bucket evaluation prompts for maximum efficiency — is the difference between an eval pipeline that keeps up with your release cadence and one that becomes a perpetual bottleneck your team learns to route around.

The teams that skip batch optimization pay for it in one of two ways. Either their evaluation runs take so long that engineers stop waiting for results and ship without them, or they reduce evaluation coverage to fit within the time window, trading quality signal for speed. Both outcomes defeat the purpose of building an evaluation system in the first place. The eval that never runs is worse than no eval at all, because no eval creates urgency to build one, while a slow eval creates the illusion that coverage exists.

## Why Batch Processing Dominates Eval Workloads

The vast majority of evaluation work doesn't need real-time results. The nightly quality report that summarizes yesterday's production quality can take three hours to generate without any business impact, as long as it's ready for the morning standup. The golden set evaluation that gates a release can take forty-five minutes without delaying the deploy, as long as the CI pipeline budgets for it. The weekly trend analysis that compares this week's quality to the last four weeks can run over the weekend. These workloads are naturally batch workloads — they process a bounded set of inputs, produce a bounded set of results, and have deadlines measured in hours, not milliseconds.

Batch processing also offers concrete cost advantages. Major LLM API providers — OpenAI, Anthropic, Google — offer batch API pricing that runs thirty to fifty percent cheaper than synchronous API pricing, in exchange for accepting higher latency. As of early 2026, OpenAI's batch API offers fifty percent discount with a twenty-four-hour completion window. Anthropic's message batches API offers similar economics. If your evaluation workload can tolerate batch latency — and most scheduled evaluation workloads can — batch pricing alone can cut your judge costs nearly in half, before any other optimization.

Even without batch API discounts, processing evaluations in batches rather than individually reduces overhead. Each individual API call carries connection setup cost, authentication overhead, and network round-trip latency. When you send a thousand evaluations individually, you pay that overhead a thousand times. When you batch them into groups of fifty, you pay it twenty times. The per-evaluation overhead drops, and total throughput increases.

The exception is real-time evaluation gating — the synchronous scoring path discussed in chapter four — where evaluation must complete before the response reaches the user. That workload demands the async pipeline patterns covered in the next subchapter. Everything else is a candidate for batch optimization.

## Throughput Optimization Fundamentals

Batch evaluation throughput is governed by a simple relationship: how many evaluations you can complete per unit of time, subject to the constraints of API rate limits, compute capacity, and error rates. Optimizing throughput means attacking each of these constraints.

The first constraint is **concurrent request capacity**. If you send evaluation requests one at a time, waiting for each to complete before sending the next, your throughput is limited by the round-trip latency of a single evaluation. An LLM judge call that takes three seconds means you process twenty evaluations per minute. Twenty evaluations per minute means it takes thirty-five hours to evaluate a golden set of forty-two thousand examples. That's not acceptable for a release gate.

Concurrent requests change the math entirely. If you send fifty evaluation requests simultaneously, maintaining fifty in-flight requests at all times, and each completes in three seconds on average, you process roughly a thousand evaluations per minute. The same golden set now completes in forty-two minutes. The throughput improvement is linear with concurrency, up to the point where you hit the next constraint.

That next constraint is **API rate limits**. Every LLM provider imposes rate limits — typically measured in requests per minute and tokens per minute. As of 2026, a standard enterprise tier with a major provider might allow three thousand to ten thousand requests per minute, with token limits of several million tokens per minute. Your concurrent request level must stay within these limits. If you push fifty concurrent requests at three seconds each, you're generating roughly a thousand requests per minute, which is within most enterprise rate limits. If you push five hundred concurrent, you're generating ten thousand requests per minute — and you may be hitting the ceiling.

The practical approach is to implement a rate-limiting client that maintains maximum concurrency within the provider's limits. The client tracks in-flight requests and pending requests, dispatches new requests as in-flight ones complete, and backs off when rate limit errors are returned. Most teams implement this as a semaphore-based dispatcher: a configurable maximum concurrency level, a queue of pending evaluation tasks, and a worker loop that dequeues and dispatches as slots become available.

The second throughput optimization is **minimizing idle time between batches**. Naive batch processing divides evaluation work into fixed-size batches, processes each batch to completion, then starts the next. This creates idle time at the end of each batch: the last few evaluations in a batch complete while the next batch hasn't started yet. The fix is to switch from fixed-batch processing to a continuous feed model. Instead of processing discrete batches, maintain a queue of evaluation tasks and a pool of workers that continuously pull from the queue. When a worker completes one evaluation, it immediately starts the next, with no batch boundary idle time. The batch concept still exists at the input level — you collect a day's outputs, stage them in the queue — but execution is continuous, not chunked.

The third optimization is **handling partial failures without restarting**. In a batch of ten thousand evaluations, some will fail: API timeouts, rate limit responses, malformed outputs that the judge can't parse. A naive implementation that restarts the entire batch on failure wastes all the work already completed. A robust implementation tracks the completion status of each individual evaluation, marks failures for retry, and runs a retry pass over only the failed items after the main pass completes. This requires persistent tracking of evaluation state — an evaluation task table that records each task's status as pending, in-progress, completed, or failed — so that a pipeline restart doesn't lose track of what's been done.

## Prompt Bucketing: The Throughput Multiplier

**Prompt bucketing** is the practice of grouping evaluation prompts by shared characteristics before submitting them for scoring, and it is one of the highest-leverage optimizations available for batch evaluation throughput. The concept is straightforward: evaluations that share similar properties are more efficient to process together than evaluations drawn at random from the full workload.

The most impactful bucketing dimension is prompt length. LLM inference time scales with total token count — the sum of input tokens and output tokens. A judge evaluation of a two-hundred-token output completes faster than a judge evaluation of a two-thousand-token output. If you mix short and long evaluations randomly in your concurrent request pool, your throughput is governed by the long evaluations: the short evaluations complete and sit idle while the long evaluations finish, wasting concurrency slots. If you bucket evaluations by length — all short evaluations processed together, then all medium, then all long — each batch's evaluations complete at roughly the same time, maximizing utilization of your concurrent request slots.

In practice, this means sorting your evaluation queue by total token count before processing. You don't need precise token counts — approximate length categories are sufficient. Outputs under five hundred tokens go in the short bucket. Outputs between five hundred and two thousand tokens go in the medium bucket. Outputs over two thousand tokens go in the long bucket. Process the short bucket first (fastest throughput, gives you early results), then medium, then long. The overall completion time can improve by twenty to forty percent compared to random ordering, simply because you eliminate the idle time caused by length variance within concurrent batches.

The second bucketing dimension is evaluation criteria. If you're running multiple evaluation types — safety, quality, accuracy — across the same set of outputs, you can bucket by evaluation type. All safety evaluations run together with the safety rubric. All quality evaluations run together with the quality rubric. This seems like the obvious approach, but many teams instead process all evaluations for a given output together — safety, quality, and accuracy for output one, then safety, quality, and accuracy for output two. The per-output approach feels logical but is less efficient because it mixes rubric complexity levels and prevents the judge from benefiting from rubric consistency across sequential calls.

When your judge processes a batch of safety evaluations using the same safety rubric, some providers optimize for shared prompt prefixes. The rubric instructions — which are identical across all evaluations in the bucket — can be cached in the provider's key-value cache, reducing the per-evaluation processing cost. This is the principle behind prefix caching, which OpenAI, Anthropic, and Google all support in their batch APIs as of 2026. When the first thousand tokens of your prompt are identical across hundreds of evaluations, the provider caches the prefix computation and applies it to all subsequent evaluations in the batch. The cost savings can be thirty to fifty percent on input tokens for evaluations with long shared rubrics.

The third bucketing dimension is judge model. If your evaluation system uses different judge models for different evaluation types — a frontier model for nuanced quality assessment and a smaller model for straightforward safety checks — bucket evaluations by target judge model. This ensures that your rate limit management is per-model rather than aggregate, prevents cross-model contention, and simplifies the retry logic because failures from one judge don't delay evaluations destined for a different judge.

## The Batch Size Sweet Spot

Batch size — the number of evaluations you group into a single processing unit — has a meaningful impact on throughput, reliability, and latency to first result. The sweet spot depends on your workload characteristics, but the principles are consistent.

Too-small batches carry high overhead. Each batch has setup cost: authenticating with the API, fetching evaluation configuration, initializing the result tracking system. If you process evaluations in batches of ten, and you have fifty thousand evaluations to run, you're paying setup cost five thousand times. For API-based evaluation, the setup cost is small per batch but measurable at thousands of batches. For self-hosted evaluation where model loading is involved, small batches can be catastrophic — loading the judge model for every ten evaluations wastes GPU time on model initialization that should be spent on inference.

Too-large batches create different problems. A batch of fifty thousand evaluations takes hours to complete. During that time, you have no results — the entire golden set evaluation is in progress, and the release pipeline is waiting. If the batch fails at evaluation forty-eight thousand due to a transient infrastructure issue, you face a choice between retrying the entire batch (wasting the work already done) or implementing complex partial-completion recovery (adding engineering complexity). Large batches also concentrate risk: a bug in your evaluation configuration affects all fifty thousand evaluations in the batch, rather than being caught after the first smaller batch completes.

The practical sweet spot for most API-based evaluation workloads falls between one hundred and one thousand evaluations per batch. At this size, each batch completes in minutes rather than hours, giving you frequent result checkpoints. Batch failures affect a manageable number of evaluations that can be retried quickly. You see early results fast enough to spot configuration errors before they contaminate the full run. And the per-batch overhead is amortized across enough evaluations to be negligible.

For self-hosted evaluation on GPU infrastructure, batch sizes should align with the GPU's optimal batch inference capacity — typically thirty-two to two hundred fifty-six, depending on model size and available memory. Underfilling the GPU wastes compute capacity. Overfilling it causes out-of-memory failures or forces the inference engine to split internally, adding latency. The right batch size for self-hosted evaluation is a hardware question as much as a workflow question.

## Rate Limit Management at Scale

Rate limits are the binding constraint for most API-based evaluation workloads, and managing them well is the difference between hitting your throughput ceiling at thirty percent of theoretical capacity and hitting it at ninety percent.

The naive approach to rate limits is to set a fixed concurrency level and hope for the best. If your rate limit is five thousand requests per minute, you set fifty concurrent workers each making one request every 0.6 seconds. This works until it doesn't — until a burst of slow responses shifts the request timing, or a transient rate limit reduction from the provider drops your allowed throughput, or a retry burst from a failed batch pushes you over the limit. The resulting 429 rate-limit responses trigger retries, which add more load, which triggers more rate-limit responses. The feedback loop can reduce your effective throughput to a fraction of the limit.

The robust approach is adaptive rate limiting. Your eval pipeline tracks the actual response times and error rates from the API provider, calculates its current effective throughput, and adjusts its request rate continuously. When responses come back fast and without errors, the pipeline ramps up toward the rate limit. When rate limit errors appear, the pipeline backs off exponentially and recovers gradually. This adaptive approach requires instrumenting every API call with timing data and building a feedback loop between the response metrics and the dispatch rate.

For teams running evaluation across multiple API providers — a common pattern when different evaluation types use different judge models from different providers — rate limits must be managed per-provider. Your safety evaluations running against Anthropic's API have a separate rate limit from your quality evaluations running against OpenAI's API. Managing them as a single pool wastes capacity on the provider with room and overloads the provider at its limit. The dispatch system needs per-provider rate tracking and per-provider backoff logic.

A strategy that many high-volume teams adopt is maintaining accounts with multiple providers and routing evaluations based on current rate limit headroom. If your primary judge provider is rate-limited because of a surge in your quality evaluations, your safety evaluations — which can run against a different provider's model with equivalent accuracy — route to the provider with available capacity. This multi-provider routing adds complexity but can double or triple effective throughput for teams pushing the limits of any single provider.

## Retry and Idempotency

Every evaluation in a batch can fail. API timeouts, rate limit responses, malformed judge outputs, connection resets — at scale, these aren't edge cases. They're a constant background noise affecting one to five percent of evaluations in every batch. Your retry strategy determines whether these failures become missing data or just slow data.

The first principle is **idempotency**: running the same evaluation twice must produce a result, not a duplicate. If an evaluation fails and is retried, the retry must not create a second evaluation result for the same output. This requires a unique evaluation identifier — typically a combination of the output ID, the evaluation type, and the rubric version — and a deduplication check before persisting any result. If a result already exists for this identifier, the retry's result replaces it rather than appending alongside it.

Idempotency sounds simple until you consider timing. What happens if the original evaluation didn't actually fail — it just took longer than your timeout? The evaluation completes and writes a result. Meanwhile, the retry also completes and writes a result. You now have two results for the same evaluation. The deduplication check must handle this race condition, typically by using an upsert operation that keeps the most recent result or the result with the highest confidence, rather than an insert that fails on conflict.

The second principle is progressive retry with decay. The first retry should happen quickly — within seconds — because many failures are transient network issues. The second retry should wait longer — thirty seconds to a minute — to allow any temporary provider issues to resolve. The third retry should wait longer still. After three to five retries, the evaluation should be moved to a dead-letter queue for manual investigation rather than retried indefinitely. Infinite retries on a persistently failing evaluation waste API budget and can trigger rate-limit responses that degrade throughput for healthy evaluations.

The third principle is batch-level monitoring of failure rates. If more than five percent of evaluations in a batch are failing, something systemic is wrong — a provider outage, an expired API key, a malformed rubric that the judge can't parse. In this case, continuing to retry individual evaluations is futile. The pipeline should pause the batch, alert the team, and wait for manual intervention. A five percent failure rate that triggers thousands of retries can consume more API budget in retries than the successful evaluations consume in primary calls.

## The Batch Pipeline in Practice

A well-optimized batch evaluation pipeline follows a predictable workflow. The trigger — a deployment event, a cron schedule, a manual initiation — creates a batch job. The job fetches the evaluation inputs: golden set examples, sampled production outputs, or whatever the job specification defines. The inputs are sorted and bucketed by prompt length, evaluation type, and target judge model.

Processing begins with the shortest prompts in the highest-priority evaluation type. Workers maintain maximum concurrency within per-provider rate limits. Results flow to persistent storage as they complete — no waiting for the entire batch. A monitoring dashboard shows batch progress in real time: completed evaluations, pending evaluations, failed evaluations, estimated time to completion. A failure rate exceeding the configured threshold pauses the batch and triggers an alert.

After the main pass completes, a retry pass processes all failed evaluations with progressive backoff. Evaluations that fail the retry pass are logged to the dead-letter queue with their failure reason. The batch completes when all evaluations have either succeeded or been dead-lettered. A completion report summarizes the results: total evaluations, pass rate, failure rate, evaluation duration, cost, and any anomalies detected.

This workflow is not exotic. It is the standard operating procedure for any team running evaluation at scale in 2026. The teams that implement it spend less, finish faster, and get more reliable results than teams that treat batch evaluation as a script to be run and monitored manually.

Batch processing handles the scheduled evaluation workloads — the golden set runs, the nightly reports, the periodic audits. But production traffic doesn't stop flowing while batches run. The continuous stream of user interactions needs its own evaluation path, one that runs independently of batch schedules and doesn't wait for batch windows. The next subchapter covers async evaluation pipelines: the architecture that decouples evaluation from the user-facing request path entirely, letting you evaluate production traffic at your own pace without touching production latency.
