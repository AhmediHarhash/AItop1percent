# Chapter 2 — Sampling Strategies: Evaluating What Matters When You Cannot Evaluate Everything

At production scale, evaluating every output is economically impossible. The difference between teams that maintain quality and teams that lose control is not budget — it is sampling strategy. This chapter teaches you how to evaluate selectively while maintaining statistical confidence, risk coverage, and the ability to catch problems that affect a fraction of one percent of your traffic.

---

- **2.1** — The Mathematics of Coverage: Why Evaluating Everything Is Economically Impossible
- **2.2** — Random Sampling: The Baseline That Most Teams Never Outgrow
- **2.3** — Stratified Sampling: Ensuring Coverage Across Use Cases, User Segments, and Edge Cases
- **2.4** — Risk-Based Sampling: Prioritizing High-Stakes Outputs for Deeper Evaluation
- **2.5** — Change-Based Sampling: Intensifying Evaluation After Deployments and Model Updates
- **2.6** — Anomaly-Triggered Sampling: Routing Unusual Outputs to Deeper Eval Automatically
- **2.7** — Adaptive Sampling: Adjusting Rates Based on Observed Quality Trends
- **2.8** — The Sampling Budget: Allocating Eval Capacity Across Products, Features, and Risk Tiers

---

*Sampling tells you what to evaluate. The next chapter tackles the evaluator itself — because at scale, LLM-as-judge becomes your largest eval cost center, and the economics of judge model selection can make or break your entire evaluation budget.*
