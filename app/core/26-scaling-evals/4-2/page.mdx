# 4.2 — Why Traditional A/B Testing Breaks for AI Systems

A/B testing is the gold standard for web experimentation. For AI systems, it is broken. Not slightly suboptimal — fundamentally broken. Teams that apply classical A/B testing methodology to AI model comparison get answers that are wrong in specific and dangerous ways. They conclude that Model B is better than Model A when the difference is noise. They conclude that there is no difference when Model B is meaningfully worse on the queries that matter most. They run tests for weeks that should have taken days, or declare results in days when the test needed weeks. The methodology that revolutionized web product development leads AI teams into confident, data-backed wrong decisions.

This is not because A/B testing is a bad idea. The principle — compare two treatments on real users and measure the difference — is sound. The problem is that classical A/B testing was designed for a world that AI systems do not inhabit. Understanding exactly why it breaks is the first step toward building experimentation methods that actually work for model comparison.

## The Assumptions That Made A/B Testing Work

To understand why A/B testing breaks for AI, you need to understand the assumptions that made it succeed for web products. Classical A/B testing, as practiced by web companies since the mid-2000s, rests on a set of assumptions that are so consistently true for web products that most practitioners have forgotten they are assumptions at all.

The first assumption is **deterministic treatments**. When you A/B test a blue button versus a green button, every user in group A sees the exact same blue button, and every user in group B sees the exact same green button. The treatment is stable. It does not vary from user to user or from moment to moment. This stability is what makes comparison meaningful — any difference in outcomes between groups must be caused by the treatment, because the treatment is the only thing that differs.

The second assumption is **binary or simple metrics**. Web A/B tests typically measure click-through rate, conversion rate, sign-up rate, or revenue per user. These are single numbers. They go up or they go down. Statistical tests are designed to detect differences in a single metric between two groups, and they do this reliably when the metric is well-defined and consistently measured.

The third assumption is **stable populations over the test period**. A two-week A/B test assumes that the users arriving in week one are statistically similar to the users arriving in week two. For most web products, this is true — the user base shifts slowly, and daily variations in traffic composition are small relative to the overall population.

The fourth assumption is **independence between observations**. Each user's click decision is assumed to be independent of every other user's click decision. This makes the statistics clean. If user 147 clicks, that tells you nothing about whether user 148 will click.

Every one of these assumptions fails for AI systems, and they fail in ways that produce quietly wrong results rather than obvious errors.

## Non-Determinism Kills Treatment Stability

The most fundamental break is that AI model outputs are not deterministic. Send the same prompt to the same model twice and you may get different responses. The model samples from a probability distribution over tokens. With temperature above zero — which is the default for nearly every production deployment — two calls with identical inputs produce different outputs. This means the "treatment" in your A/B test is not stable.

In a web A/B test, every user in group B sees the green button. In an AI A/B test, every user in group B gets a response from Model B, but each response is different. User 1 might get a brilliant response. User 2, with the exact same query, might get a mediocre one. The variance within each treatment group is enormous compared to a web test, where the variance within each group comes only from user behavior differences, not from treatment differences.

This inflated variance destroys statistical power. The statistical tests used in A/B testing calculate whether the difference between groups is larger than the variation within groups. When within-group variation is small — as with deterministic treatments — modest between-group differences are detectable with reasonable sample sizes. When within-group variation is large — as with non-deterministic AI outputs — you need dramatically larger sample sizes to detect the same difference. A web test that reaches significance in ten thousand users might need a hundred thousand users for an AI comparison, and most teams don't realize this because they're using sample size calculators designed for deterministic treatments.

The practical consequence is undertested experiments. A team runs an A/B test comparing two AI models for two weeks. The sample size calculator, designed for web tests, told them two weeks would be sufficient. The test concludes with no statistically significant difference. The team concludes that the models are equivalent and picks the cheaper one. But the test never had enough power to detect the real difference — a five-percent quality gap that would have been obvious with a properly calibrated sample size estimate. The team made a data-backed decision. The decision was wrong.

## Infinite Variant Space Breaks Effect Size Estimation

A web A/B test has a finite, enumerable variant space. Button A is blue. Button B is green. There are exactly two variants. The effect of seeing a green button versus a blue button is a single, stable quantity that the test estimates.

An AI model produces effectively infinite variants. Model B doesn't generate one response — it generates a different response for every input, and a different response for the same input across runs. The "effect" of Model B is not a single quantity. It is a distribution of effects across the input space. Model B might be significantly better than Model A on simple factual questions, equivalent on complex reasoning tasks, and significantly worse on tasks requiring creative output. The overall average might show no difference, but the average conceals critical variation.

Classical A/B tests estimate average treatment effects. For AI, the average is often meaningless. A model that is ten percent better on eighty percent of queries and thirty percent worse on twenty percent of queries has an average improvement of two percent. A classical test will either miss this entirely or report a modest positive result. Meanwhile, the twenty percent of queries where quality dropped — which might be your highest-value queries — are silently degrading the user experience.

The fix requires segment-level analysis: measuring the treatment effect for different query types, complexity levels, user segments, and input characteristics separately. But this is not classical A/B testing anymore. It is a fundamentally different experimental design that most A/B testing frameworks do not support out of the box.

## Rapid Iteration Invalidates Long-Running Tests

Classical A/B tests are designed for treatments that remain stable throughout the test period. You deploy button A and button B, run the test for two weeks, and analyze the results. The buttons don't change during the test.

AI models iterate rapidly. In the time it takes a traditional A/B test to accumulate sufficient data for significance — especially given the inflated variance from non-determinism — the model has often been updated. A team starts testing Model B against Model A on Monday. By Wednesday, the engineering team has a new prompt template for Model B that they believe improves quality. Do they restart the test? Apply the new template mid-test, invalidating the first two days of data? Continue with the old template even though they know a better version exists?

In practice, most teams face this tension every week. AI development cycles are measured in days, not quarters. The prompt is updated, the system instructions are revised, a new retrieval strategy is tested, the temperature is tuned. Each change is potentially meaningful. Classical A/B testing requires holding the treatment constant for the duration of the test. AI development requires changing the treatment continuously. These two requirements are in direct conflict.

The result is either stale tests — running a two-week experiment on a treatment that was already outdated by day four — or contaminated tests where mid-experiment changes invalidate the statistical analysis. Both produce unreliable results. And both happen routinely at teams that try to force classical A/B methodology onto AI experimentation.

## Multi-Dimensional Quality Defies Single Metrics

The simplicity of web A/B test metrics is a feature, not a limitation. Click-through rate is one number. Conversion rate is one number. Revenue per user is one number. You can detect a change in one number with a straightforward statistical test and a well-defined decision rule: if the new variant improves the metric by a statistically significant amount, ship it.

AI quality is not one number. A customer support model's quality includes accuracy — did it answer the question correctly? Helpfulness — did the response actually help the user? Tone — was the communication professional and empathetic? Safety — did the response avoid harmful content? Completeness — did the response address all parts of the user's question? Conciseness — was the response appropriately brief, or did it waste the user's time with unnecessary detail?

A model change might improve accuracy by three percent, degrade tone slightly, maintain safety, and reduce completeness. Is the model better? That depends on which dimension you care about most, and the answer differs by use case, by user segment, and by stakeholder. Classical A/B testing has no framework for multi-dimensional quality comparison. You can run a separate test for each dimension, but then you face the multiple testing problem — the more tests you run, the more likely you are to find a spurious significant result. You can combine dimensions into a composite score, but then you've made an implicit weighting decision that hides the trade-offs. Either approach produces misleading results.

The 2026 state of practice treats model comparison as a multi-dimensional evaluation problem, not a single-metric hypothesis test. Instead of asking "is Model B better than Model A?" you ask "on which quality dimensions does Model B differ from Model A, by how much, and does the trade-off profile match our priorities?" This is a richer, more honest assessment — but it is not what classical A/B testing does.

## Personalization Effects Destroy Group Comparability

Web A/B tests randomly assign users to groups and assume that the groups are comparable. With large enough samples, this assumption holds well — the distribution of user characteristics is roughly equal between groups.

But AI systems with personalization, context adaptation, or session history create a different problem. The model's output depends on the context — the user's previous interactions, retrieved documents, session state. This means Model A and Model B are not producing outputs in a vacuum. They are producing outputs conditioned on context that varies for every user and every interaction.

The "treatment" for User 1 is Model B given User 1's conversation history, retrieved documents, and session context. The "treatment" for User 2 is Model B given User 2's completely different context. These are effectively different treatments wearing the same label. The variance this introduces is enormous, far larger than the group-assignment variance that traditional randomization eliminates. Two users "receiving the same model" may have quality experiences that differ by an order of magnitude because their contexts differ dramatically.

This makes it extremely difficult to isolate the model's contribution to quality from the context's contribution. A model that appears better in an A/B test might simply have been assigned users with contexts that are easier to handle. A model that appears worse might have drawn a disproportionate share of complex contexts by chance. With deterministic web treatments this kind of confounding is minimal — the treatment is the same for everyone. With context-dependent AI outputs, it is the dominant source of noise.

## What Teams Get Wrong in Practice

Armed with these misunderstandings, teams make predictable mistakes. The most common is using an off-the-shelf A/B testing platform designed for web experiments and applying it directly to AI model comparison. These platforms calculate sample sizes assuming deterministic treatments and single metrics. They report significance using tests designed for binary outcomes. They assume treatments are stable over the test period. Every assumption is violated, and the platform reports results as if they are valid.

Another common mistake is reducing multi-dimensional quality to a single "quality score" by averaging across dimensions, then A/B testing that average. This hides the trade-offs that matter most. A model that improved safety by ten percent and degraded accuracy by eight percent looks like a net positive on the composite score. Ship it, says the A/B test. Three weeks later, users are abandoning conversations because the model can't answer their questions, even though it's very safe while not answering them.

A third mistake is testing for too long during rapid iteration. A team insists on reaching classical significance before making a model decision. The test runs for three weeks. During that time, two prompt revisions and a temperature adjustment are applied to the candidate model. The final significance result is an average over a treatment that changed twice during the test. What exactly did the team prove?

The fourth mistake is testing for too short a period and declaring results before the test has seen the full range of production traffic patterns. A test that starts Monday morning and concludes Wednesday evening has never seen weekend traffic, which might have a completely different query distribution. The model appears to pass, but it fails on the traffic patterns it was never exposed to.

## The 2026 Replacement: Eval-Driven Online Experimentation

The replacement for classical A/B testing in AI systems is not a minor tweak to the methodology. It is a fundamentally different approach: **eval-driven online experimentation**. Instead of binary outcome metrics, you use continuous quality scoring across multiple dimensions. Instead of waiting for statistical significance on a single metric, you accumulate multi-dimensional quality evidence and make decisions based on quality profiles rather than p-values. Instead of requiring stable treatments for weeks, you design experiments that accommodate rapid iteration — comparing quality trajectories rather than fixed snapshots.

The core ideas are straightforward. First, evaluate every production output — or a representative sample — across your quality dimensions using real-time eval scoring. Second, compare models on each dimension independently and look at the full trade-off profile, not a blended average. Third, use shadow deployments, interleaving experiments, and progressive rollouts with eval gates instead of classical random assignment to get valid comparisons despite non-determinism and context dependence. Fourth, design experiments that produce usable results in hours or days, not weeks, to match the speed of AI iteration.

This approach doesn't abandon experimentation. It evolves experimentation from a tool designed for deterministic, single-metric web products into one that matches the complexity of non-deterministic, multi-dimensional AI systems. The remaining subchapters in this chapter cover the specific techniques — shadow deployments, interleaving experiments, real-time scoring, progressive rollouts — that make eval-driven online experimentation work in practice.

## The Shadow Alternative

The first alternative that works well for AI model comparison is the shadow deployment — a technique that lets you test a new model on production traffic without exposing any user to its output. Shadow deployments solve the non-determinism problem by comparing models on the exact same inputs. They solve the user-risk problem by serving only the established model's output. And they produce comparison data fast enough to match AI iteration speed. The next subchapter covers exactly how shadow deployments work, what they cost, and when they're the right choice.
