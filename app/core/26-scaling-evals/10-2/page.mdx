# 10.2 — Braintrust, LangSmith, and Langfuse: The Full-Stack Platform Tier

A full-stack eval platform handles the entire evaluation lifecycle — from dataset management through scoring to reporting — in one integrated system. The value is workflow integration. The risk is lock-in. Understanding the difference between the three dominant players in this tier is not about reading feature comparison tables. It is about understanding three fundamentally different philosophies of what evaluation should be, and choosing the one that matches how your team actually works.

## What Defines the Full-Stack Tier

The full-stack tier is defined by a single property: these platforms cover enough of the evaluation workflow that many teams can use them as their only eval tool. They provide dataset creation and versioning, prompt management and experimentation, scoring with both automated and human judges, tracing and observability, reporting and visualization, and increasingly, CI/CD integration for automated deployment gating. The promise is that you log into one interface, and you can run an eval, review the results, compare it against previous runs, investigate individual failures, adjust your prompt, and run the eval again — without switching tools.

This breadth comes at a cost. No full-stack platform does every individual function as well as the best specialized tool for that function. Arize does observability better than any full-stack platform. Patronus does safety evaluation with more depth. DeepEval offers more metric flexibility for teams willing to write code. The full-stack value proposition is not "best at everything" — it is "good enough at everything, integrated in ways that save your team hours per week." For most teams at most stages, that trade-off is correct.

The three dominant full-stack platforms in 2026 — Braintrust, LangSmith, and Langfuse — each approach this integration challenge differently, and those differences matter far more than any feature checklist.

## Braintrust: Eval as Experimentation

Braintrust's core thesis is that evaluation is experimentation, not monitoring. The platform was designed from the ground up around the idea that the primary job of an eval system is to help you compare variants — different prompts, different models, different configurations — against the same dataset and decide which one to deploy. This philosophy shapes everything about the product.

The central workflow in Braintrust is the experiment. You define a dataset of representative inputs, run multiple model configurations against that dataset, and compare scored results side by side. The platform provides statistical significance testing to tell you whether observed differences are real or noise. It supports LLM-as-judge scoring, deterministic checks, and custom scoring functions. The experiment-centric design means that every evaluation is implicitly a comparison — not "how good is this prompt?" but "is this prompt better than what we have today?"

This philosophy gives Braintrust a particular strength in pre-deployment evaluation. Teams that use Braintrust heavily tend to run experiments before every prompt change, before every model swap, and before every pipeline modification. The platform's CI/CD integration extends this to automation: a GitHub Action runs experiments against pull requests and posts eval results directly to the PR, blocking merges when quality degrades. For teams that want evaluation to function as a deployment gate — the same way unit tests gate code changes — Braintrust's architecture is a natural fit.

Where Braintrust's thesis creates friction is in production monitoring. The experiment model assumes you know what inputs to test against. In production, you don't control the inputs — users send whatever they send. Braintrust has added online scoring and production monitoring capabilities, but these feel grafted onto the experimentation core rather than native to it. Teams that need deep production observability as their primary eval capability sometimes find Braintrust's monitoring less mature than its experimentation.

Braintrust's pricing reflects its target market of teams doing frequent experimentation. The free tier includes one million trace spans and ten thousand scores with unlimited users, which is generous enough for most startups to get meaningful value before hitting a paywall. The Pro tier at around two hundred fifty dollars per month removes those limits and adds advanced features. Enterprise pricing adds self-hosting options and dedicated support. The transparent pricing is a deliberate contrast to platforms that require sales calls for basic cost information.

## LangSmith: Trace-First Debugging

LangSmith approaches evaluation from a different starting point: the trace. Built by the LangChain team, LangSmith was originally designed as a debugging tool for LangChain applications — a way to see exactly what happened inside a chain of model calls, retrievals, and tool invocations. This trace-first heritage shapes the product even as it has expanded far beyond the LangChain ecosystem.

The defining strength of LangSmith is its ability to decompose complex AI system behavior into inspectable traces. When an agent takes seven steps, calls three tools, retrieves from two knowledge bases, and produces a final answer, LangSmith shows you every step — the prompt that went into each model call, the response that came back, the retrieval query, the documents returned, the tool inputs and outputs. This granularity is invaluable for debugging agentic systems, where the failure mode is rarely "the model was bad" and is much more often "step four received bad context because step two's retrieval query was poorly formed."

LangSmith's evaluation capabilities have matured significantly since 2024. The platform now supports offline evaluation against curated datasets, online evaluation against production traffic in near-real-time, pairwise comparisons, multi-turn conversation scoring, and LLM-as-judge evaluation with customizable criteria. A recent addition, the Insights Agent, automatically categorizes production threads into usage patterns and scores entire conversations, not just individual turns. This is particularly useful for chatbot and agent applications where quality depends on the full interaction, not any single response.

The platform's relationship with LangChain is both its greatest strength and its most significant liability. Teams using LangChain get instrumentation essentially for free — traces flow into LangSmith with minimal setup. But LangSmith now supports framework-agnostic instrumentation through OpenTelemetry and direct SDK integration, making it usable without LangChain. The perception of LangChain dependency is stronger than the reality, but it still deters some teams who want to avoid any association with a specific orchestration framework.

LangSmith's pricing is seat-based for some features and usage-based for others, which can create unpredictable costs at scale. Teams with large engineering organizations that all need dashboard access sometimes find the per-seat costs add up faster than expected. The platform's strength in agent debugging makes it particularly attractive to teams building complex agentic systems, while teams with simpler prompt-and-response architectures may find they are paying for sophistication they don't need.

## Langfuse: Open-Source Core, Developer Ergonomics

Langfuse takes the third path: open-source core with optional commercial cloud. Licensed under MIT, Langfuse can be self-hosted without restrictions, giving teams complete control over their data and infrastructure. This makes it the default choice for organizations where data cannot leave their infrastructure — healthcare companies bound by HIPAA, financial institutions with strict data residency requirements, government agencies, and any team that simply prefers to own its stack.

The platform's architecture is built around ClickHouse for high-performance analytics and PostgreSQL for relational data, with Redis for caching and S3-compatible object storage for blob data. This stack is designed for teams that understand infrastructure: you can deploy it on Docker, Kubernetes, or bare VMs, and you can scale each component independently. For infrastructure-savvy teams, this is empowering. For teams without strong DevOps capacity, it is a meaningful maintenance burden.

Langfuse's evaluation features include LLM-as-judge scoring, custom evaluation functions, human annotation workflows, and dataset management with versioning. The platform supports session-level tracing for multi-turn conversations and agent workflows, with visual agent graphs that illustrate the flow of complex interactions. Prompt management with a built-in playground allows teams to iterate on prompts and compare variants, though the experimentation workflow is less structured than Braintrust's.

What sets Langfuse apart beyond the open-source licensing is developer ergonomics. The platform provides native SDKs for Python and JavaScript, with connectors for over fifty frameworks including LangChain, LlamaIndex, and the major LLM provider SDKs. The integration experience is consistently praised by developers: instrumenting an existing application typically takes minutes, not days. The API design favors simplicity and predictability, which matters when you are instrumenting thousands of model calls and need the tracing overhead to be negligible.

Langfuse Cloud offers a managed version for teams that want the Langfuse experience without the infrastructure responsibility. The cloud version adds features like team management, advanced access controls, and managed scaling. The pricing for cloud is competitive with commercial alternatives, but the existence of the self-hosted option means teams always have an exit path — a significant advantage in a market where vendor lock-in is a legitimate concern.

The trade-off with Langfuse is feature velocity versus control. Commercial platforms with dedicated product teams ship features faster. Langfuse's open-source community is active — the project has over nineteen thousand GitHub stars and a large contributor base — but the pace of feature development does not match a well-funded commercial product team working on a single priority. Teams that need cutting-edge capabilities on day one may find Langfuse lags behind. Teams that value stability, transparency, and data ownership may find that trade-off perfectly acceptable.

## Strengths and Weaknesses of the Tier

The full-stack tier's greatest strength is reduced integration overhead. When your dataset management, scoring, tracing, and reporting all live in one system, you avoid the glue code, data pipeline plumbing, and format conversion that consumes engineering time when stitching together specialized tools. A single data model means your eval results can reference the same traces your observability dashboard uses. A single access control model means you don't manage permissions across five different tools.

The tier's greatest weakness is the compromise inherent in breadth. If your primary challenge is safety evaluation, a full-stack platform's safety features will likely be shallower than Patronus's. If your primary challenge is production drift detection, Arize's capabilities in that area are deeper. If your primary challenge is RAG-specific metrics, Ragas provides more specialized evaluation logic. The full-stack platform gives you "good enough" on everything, which is genuinely good enough for most teams, but teams with extreme depth requirements in one area should honestly evaluate whether the full-stack platform's coverage meets their needs or whether they need a specialized tool layered on top.

Another weakness is platform assumptions about workflow. Each full-stack platform has opinions about how evaluation should work. Braintrust assumes you want to run experiments. LangSmith assumes you want to start from traces. Langfuse assumes you want to build on open infrastructure. If your team's natural workflow matches the platform's assumptions, the platform accelerates you. If it does not, you spend time working around the platform's opinions instead of working within them. The best way to evaluate a full-stack platform is not to compare feature lists but to run your actual evaluation workflow through each platform for a week and see which one creates the least friction.

## Cost at Scale

Pricing models in the full-stack tier vary significantly, and the differences compound at scale. Braintrust charges primarily on span and score volume, which creates predictable costs that scale linearly with usage. LangSmith combines seat-based pricing with usage-based pricing, which can create step-function cost increases as new team members need access. Langfuse self-hosted eliminates licensing costs entirely but introduces infrastructure costs — ClickHouse clusters, storage, compute — that can be substantial at high volumes. Langfuse Cloud charges usage-based fees that are competitive but not free.

At startup scale — under a hundred thousand evaluations per month — all three platforms are affordable, and the free tiers often suffice. At growth scale — one million to ten million evaluations per month — the cost differences become material. At enterprise scale — tens of millions of evaluations across multiple teams and products — the cost difference between platforms can be six figures annually, and the choice between self-hosted Langfuse and commercial alternatives becomes a genuine financial decision, not just a technical one.

The hidden cost that most teams underestimate is switching cost. Every month you spend on a platform, your team builds workflows, dashboards, integrations, and institutional knowledge around that platform's data model and interface. Switching platforms after twelve months of usage is not a weekend project. It is a quarter-long migration that disrupts every team that depends on eval results. This switching cost is the real lock-in, more than any contractual obligation or data format. Choose deliberately at the beginning, because switching later is expensive.

The next subchapter examines the specialized and emerging tier — Arize, Patronus, and Maxim — where depth in a single domain trades against the breadth of a full-stack platform.
