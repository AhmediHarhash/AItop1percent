# 11.2 — The Eval Review Cadence: Daily Error Analysis, Weekly Reviews, Monthly Recalibration

Three cadences define a healthy eval operating model. Daily error analysis catches immediate problems. Weekly reviews track trends. Monthly recalibration ensures the evaluation system itself stays aligned with business reality. Skip any one of the three, and the operating model develops a blind spot that compounds over time. Skip daily review, and acute failures go undetected for a week. Skip weekly review, and slow trends become invisible until they're crises. Skip monthly recalibration, and the evaluation system gradually decouples from the business outcomes it is supposed to predict.

These are not arbitrary time intervals. Each cadence operates on a different signal-to-noise ratio and serves a different organizational function. Getting the cadences right is as important as getting the eval criteria right — because criteria that nobody reviews at the right frequency are criteria that don't govern anything.

## The Daily Practice: Twenty Minutes, Fifty Outputs, One Engineer

The daily practice is the fastest feedback loop in the eval operating model. One engineer — usually the on-call engineer or the team's eval champion — spends fifteen to twenty minutes reviewing the most recent batch of flagged outputs. These are the outputs that automated evaluation identified as potential failures: low judge scores, safety flags, confidence scores below threshold, or outputs flagged by user feedback.

The goal is not comprehensive analysis. The goal is detection. You are looking for new failure modes that didn't exist yesterday, sudden spikes in a particular error category, or patterns that suggest a systemic change rather than isolated noise. A single hallucinated medical claim in a health advice product might be noise. Five hallucinated medical claims with the same structure — all citing a specific study that doesn't exist — is a pattern that warrants immediate investigation.

The daily review requires a lightweight incident tracker, not a sophisticated analytics platform. A shared document or a simple issue board where the reviewer logs what they found, how many outputs were reviewed, which error categories appeared, and whether any pattern warranted escalation. The value is not in the tool — it is in the discipline of looking at real outputs every single day. Teams that skip the daily review for even a week consistently report that when they return, they find failure patterns that had been accumulating silently for days.

The daily practice also serves a subtler purpose: it keeps the engineering team connected to what the system actually produces. Engineers who spend all their time on infrastructure, prompt engineering, and model configuration develop an abstract understanding of their system's behavior. Engineers who look at fifty real outputs every day develop an intuitive understanding that no dashboard can provide. They start recognizing failure signatures by feel, noticing when the model's tone shifts slightly, catching edge cases that formal evaluation criteria don't cover. This intuitive calibration is one of the most valuable assets an AI team can develop, and it only comes from consistent, direct exposure to outputs.

## What to Do When Daily Review Finds Something

Not every finding from the daily review requires immediate action. The review's primary function is triage. Classify each finding into one of three categories.

First: immediate action required. The system is producing outputs that are dangerous, legally risky, or severely degraded. A safety failure, a data leak, a hallucination rate spike in a high-stakes domain. These require same-day investigation and may require temporarily tightening safety filters or even rolling back a recent change. The daily reviewer should have the authority and the playbook to escalate immediately without waiting for the weekly review.

Second: pattern to track. The finding is concerning but not acute. A new error category that appeared for the first time. A quality dimension that dropped slightly. An increase in user-flagged outputs in one product area. These go into the weekly review agenda with enough context for the team to investigate whether the pattern is growing, stable, or was a one-day anomaly.

Third: noise. A single unusual output with no pattern, no recurrence, and no safety implication. Log it, note the error category, and move on. Not every bad output signals a systemic problem. Overreacting to noise is as damaging as underreacting to signals — it teaches the team to ignore daily review findings because they always turn out to be false alarms.

The triage discipline is what keeps the daily review manageable. Without it, the daily review either drowns the team in investigation tasks or becomes a ritual that produces logs nobody reads. With it, the daily review feeds a continuous stream of calibrated signal into the weekly review, where the team can assess patterns across the full week.

## The Weekly Review: Thirty to Sixty Minutes, Cross-Functional, Decision-Oriented

The weekly review is the operating model's primary decision-making forum. This is where the team looks at quality trends across the past seven days, discusses the patterns flagged in daily reviews, and makes decisions about eval improvements, investigation priorities, and quality interventions.

Participants matter. The weekly review is not an engineering-only meeting. It includes the product manager who understands user impact, the eval champion or engineer who ran the daily reviews, and ideally a domain expert who can assess whether quality trends reflect genuine product issues or evaluation artifacts. Keeping the meeting small — three to five people — prevents it from becoming a status update and keeps it focused on decisions.

The weekly review has a standard agenda with three parts, and the discipline is in sticking to it even when one part threatens to consume the entire meeting.

Part one: trend review. Look at the weekly quality metrics across all products the team owns. Are scores stable, improving, or declining? Are there sudden shifts that correlate with a specific deployment, model update, or input distribution change? The trend review should take no more than ten minutes. If the trends are stable, acknowledge that and move on. The temptation to spend thirty minutes discussing stable trends — confirming why things are good, speculating about whether they'll stay good — is strong and must be resisted. Stable trends don't need discussion. They need acknowledgment.

Part two: pattern discussion. Review the patterns flagged during daily reviews. Which new error categories emerged? Are previously flagged patterns growing or shrinking? Does any pattern suggest a root cause that the team should investigate this week? This is where the daily review data compounds — a single finding that seemed like noise on Tuesday looks different when the same pattern appeared again on Thursday and Friday. The pattern discussion is where the team decides what warrants investigation and who will do it.

Part three: eval improvement priorities. Based on trend review and pattern discussion, what should change about the eval system itself? Does the golden set need new examples to cover an emerging failure mode? Does a judge prompt need recalibration because it's consistently scoring a particular category too high or too low? Does a new quality dimension need to be added? Eval improvement priorities carry directly into the sprint plan. They are not backlog wishes — they are quality governance actions.

## The Anti-Patterns of Weekly Reviews

Two failure modes kill weekly reviews. The first is expansion — the review gradually absorbs more topics, more stakeholders, and more status updates until it becomes a sixty-minute meeting where evaluation gets fifteen minutes. Protect the weekly review's scope ruthlessly. It is about evaluation quality, evaluation trends, and evaluation improvements. Product roadmap discussions, model architecture debates, and infrastructure planning belong elsewhere.

The second failure mode is irregularity. The weekly review gets canceled because the team is busy, pushed because someone has a conflict, or skipped because "nothing happened this week." Evaluation trends are only visible with consistent data points. A monthly review that calls itself weekly provides no trend visibility. If the review happens on inconsistent schedules, the team can't distinguish between a genuine trend and the artifact of uneven observation intervals. Make the weekly review a recurring calendar event that only gets canceled for genuine emergencies. Three canceled weekly reviews in a row is an organizational signal that the team doesn't take evaluation governance seriously, regardless of what the team says about evaluation priorities.

## The Monthly Recalibration: Ninety Minutes, Strategic, System-Level

The monthly recalibration is the most important and most neglected cadence. This is where the team steps back from operational concerns and asks: is the evaluation system itself still measuring what matters?

The question sounds abstract until you consider how quickly the gap between evaluation metrics and business reality can grow. A customer support AI might have been evaluated on response accuracy, empathy, and resolution guidance when it launched. Three months later, the product has evolved to handle billing disputes, account modifications, and multi-step troubleshooting. The original eval criteria still run, still produce scores, and still look healthy — but they don't cover the new capabilities that represent half the product's usage. The eval system is measuring the old product while the team ships the new one.

Monthly recalibration has four components.

The first component is the correlation check. Compare eval scores over the past month to actual business outcomes — customer satisfaction scores, task completion rates, error ticket volumes, user retention metrics. If eval scores went up and business outcomes went up, the eval system is reasonably well-calibrated as a quality proxy. If they diverged — eval improved but business outcomes stayed flat or declined — the eval system has a blind spot. Identifying and investigating that blind spot is the single most valuable outcome of monthly recalibration.

The second component is judge calibration review. Pull the calibration data for every automated judge in the pipeline. Compare judge scores to human reviewer scores on the same outputs. Check inter-judge agreement where multiple judges score the same dimension. Look for drift — a judge that agreed with human reviewers 89% of the time three months ago and now agrees only 78% of the time has drifted, and the evaluations it produced during that drift period are suspect. Judge recalibration is not optional maintenance — it is the quality control layer for the quality control system.

The third component is criteria refresh. Review the evaluation criteria in light of product changes, customer feedback, and regulatory updates. Have new capabilities been added that aren't covered by evaluation? Have quality expectations changed? Has a regulatory requirement been clarified or updated? The EU AI Act's GPAI Code of Practice, for example, has been rolling out compliance expectations through 2025 and 2026 that may require new evaluation dimensions for safety, transparency, or bias detection. Criteria that were complete in January may be incomplete by April.

The fourth component is golden set health check. Review the golden sets used across the evaluation pipeline. Are the examples still representative of current production traffic? Has the distribution of inputs shifted in ways that make old golden set examples irrelevant? Are there enough examples in each difficulty tier — easy cases, moderate cases, and hard edge cases? A golden set that was representative six months ago may now overrepresent query types that have become less common while underrepresenting query types that have surged.

## Who Participates in Each Cadence

The cadences require different participants and different levels of organizational authority.

Daily review is a single engineer activity. It requires someone with enough domain knowledge to recognize unusual outputs and enough technical context to classify errors accurately. This is typically the eval champion on the product team or the on-call engineer. No meetings, no coordination — just fifteen to twenty minutes of structured observation.

Weekly review is a small-group activity. The core participants are the eval champion, the product manager, and one additional engineer. Domain experts attend when specific patterns require their expertise. The meeting owner — usually the eval champion — prepares a one-page summary of daily review findings and weekly trends before the meeting. Preparation matters: a weekly review where participants see the data for the first time during the meeting spends half its time on comprehension rather than decision-making.

Monthly recalibration is a cross-functional activity. It requires the eval champion, the product manager, a data analyst who can pull business outcome correlations, and ideally a member of the platform eval team who can assess judge calibration data. The meeting owner prepares a more substantial brief: correlation analysis between eval scores and business outcomes, judge calibration summaries, and a draft assessment of criteria completeness. Monthly recalibration is where strategic evaluation decisions are made, and it needs strategic participants.

## The Data Infrastructure Behind the Cadences

None of these cadences work without the right data at the right granularity.

Daily review requires a feed of flagged outputs from the past twenty-four hours, sortable by error category and severity. The automated eval pipeline should produce this feed as a standard output — not as a custom query the reviewer has to run each morning.

Weekly review requires trend charts for all tracked quality dimensions, broken down by product, by time period, and by error category. These need to be pre-computed and accessible in a dashboard that loads in seconds. If the weekly review spends its first ten minutes waiting for queries to finish, the meeting burns goodwill fast.

Monthly recalibration requires correlated data: eval scores alongside business metrics for the same time period, judge calibration data with historical comparison, golden set coverage analysis against current production traffic distribution, and criteria completeness assessment. This data often lives in different systems — eval results in the eval platform, business metrics in the analytics warehouse, judge calibration data in the eval monitoring system. The monthly recalibration prep involves pulling data from multiple sources and synthesizing it into a single view. Organizations that automate this synthesis invest sixty to eighty hours upfront but save hundreds of hours per year in manual preparation.

## When the Cadences Break Down

Every team hits periods where one or more cadences lapse. A major product launch consumes all engineering attention. A critical incident displaces the weekly review. A quarter-end crunch pushes the monthly recalibration into "next month."

The recovery discipline is as important as the steady-state practice. When a cadence lapses, don't try to retroactively cover the gap. You can't productively review three weeks of daily findings in a single session. Instead, acknowledge the gap, note it in the evaluation record, and restart the cadence with an emphasis on whether the gap period introduced any undetected changes. Run a one-time expanded review — look at a broader sample of recent outputs, check for quality shifts during the gap, and verify that the eval system itself didn't develop issues while nobody was watching.

The cadences are the heartbeat of the eval operating model. They turn infrastructure into governance and data into decisions. But cadences without clear ownership create a diffuse responsibility that nobody fulfills. The next subchapter examines the ownership models that determine who runs these cadences, who acts on their findings, and who is accountable when the system fails.
