# 3.4 — Multi-Judge Ensembles and Consensus Protocols at Scale

A single judge gives you a score. Multiple judges give you confidence. The difference matters most at scale, where the cost of a wrong evaluation compounds across millions of decisions. A single LLM judge — no matter how capable — has an error rate. On straightforward tasks like detecting whether a response is in the wrong language, that error rate might be two or three percent. On complex tasks like assessing whether a legal summary accurately captures the nuances of a contract, it climbs to ten, fifteen, sometimes twenty percent. At a hundred evaluations per day, a fifteen percent error rate means fifteen wrong scores. Annoying but manageable. At a hundred thousand evaluations per day, that same error rate means fifteen thousand wrong scores — fifteen thousand outputs that your system classifies as good when they are bad, or bad when they are good. The quality signal you depend on for every downstream decision — model selection, prompt iteration, regression detection, release gating — is contaminated by noise that no amount of dashboard design can fix.

Multi-judge ensembles exist to reduce that noise. The core insight is straightforward: if two independent judges make uncorrelated errors, the probability that both are wrong on the same output drops dramatically. A single judge with an 85% accuracy rate is wrong on 15% of evaluations. Two independent judges with 85% accuracy, using majority voting, are both wrong on roughly 2.25% of evaluations — because both need to be wrong simultaneously for the ensemble to produce a wrong answer. Add a third judge and the probability that two out of three are all wrong falls further. The math is clean. The economics are not.

## The Cost Multiplier Problem

Every additional judge multiplies your evaluation cost. Two judges cost twice as much. Three judges cost three times as much. For a team that already spends $15,000 per month on judge calls, tripling that to $45,000 for a three-judge ensemble is a budget conversation, not a technical one. And the reliability gain from each additional judge follows diminishing returns. Going from one judge to two buys you the largest error reduction. Going from two to three buys less. Going from three to five buys marginal improvement at substantial cost. Very few production systems run more than three judges, because the economics simply don't justify it beyond that point.

This means the decision to use an ensemble is not a universal one — it's a segmented one. You don't run three judges on every evaluation. You run three judges on the evaluations where the cost of a wrong score is high enough to justify the spend. A customer support chatbot where a single wrong eval barely matters? One judge is fine. A medical triage system where a misclassified response could influence patient care decisions? Three judges with majority voting is the minimum. The art of ensemble design at scale is not choosing how many judges to run — it's choosing where to run how many.

Teams that apply ensembles uniformly across all evaluations are making the same mistake as teams that evaluate every output instead of sampling. They're spending the budget linearly when the value curve is exponential. The first judge gives you the score. The second judge tells you whether to trust it. The third judge breaks ties. Each has a role, but not every evaluation needs all three.

## Consensus Protocols: From Majority Voting to Deliberation

**Majority voting** is the simplest consensus protocol and the one most teams start with. Each judge independently scores the output. The final score is the one that appears most often. With three judges, you need two to agree. With five, you need three. Majority voting is fast to implement, easy to reason about, and effective at reducing random errors. Its weakness is that it assumes judges make independent errors — which is often not true. Two frontier models from the same provider may share systematic biases, like inflating scores for verbose responses or penalizing outputs that are concise but correct. When judges share a bias, majority voting amplifies it instead of correcting it.

**Weighted voting** addresses part of this problem by assigning different influence to different judges based on their historical accuracy. If you've calibrated each judge against human labels and know that Judge A agrees with humans 90% of the time while Judge B agrees 78% of the time, you give Judge A's vote more weight. Weighted voting requires a calibration dataset — a set of outputs with known-good human labels against which each judge's accuracy is measured. This adds setup cost, but for any team running ensembles at scale, you should already have this calibration data. The weight update process needs to happen regularly, because judge accuracy shifts as models are updated and as your evaluation criteria evolve. Quarterly recalibration is a reasonable starting cadence. Monthly is better for high-volume systems.

**Unanimous agreement** raises the bar. Instead of requiring a majority, you require all judges to agree. This is appropriate for high-stakes binary decisions — blocking a response from being served, flagging an output for mandatory human review, triggering an automated rollback. Unanimous agreement has very low false positive rates, meaning you almost never take drastic action on a correct output. But it has higher false negative rates, meaning some bad outputs slip through because one judge out of three didn't flag them. The tradeoff is intentional: for actions with high cost of false positives — like pulling a model from production — you want near-certainty before acting, even if that means catching fewer problems automatically.

**Deliberation protocols** represent the newest and most expensive approach. Instead of each judge scoring independently, the judges operate in rounds. In the first round, each judge produces a score and a written rationale. In the second round, each judge receives the other judges' rationales and re-evaluates, potentially changing its score based on the arguments it has seen. This mimics how human expert panels work — initial independent assessment followed by discussion and convergence. Deliberation can resolve cases where independent voting would deadlock, and it can surface reasoning flaws that a single judge wouldn't catch. The cost is substantial: deliberation requires at least two full judge calls per judge, and often three, multiplying cost by the number of rounds as well as the number of judges. A three-judge, two-round deliberation costs six judge calls per evaluation — six times the cost of a single judge. Very few teams use deliberation at full scale. It's typically reserved for offline evaluation of high-stakes quality dimensions where accuracy matters more than cost, or for the final tier of a cascade system that handles only the hardest cases.

## The Confidence Threshold: When to Escalate

The most powerful cost optimization for multi-judge systems is not running fewer judges everywhere — it's running more judges only where you need them. This is the principle behind **the Confidence Threshold**, and it transforms ensemble economics from multiplicative to selective.

The idea is straightforward. Your primary judge — typically the cheapest one that meets minimum accuracy requirements — evaluates every output. Along with its score, it produces a confidence signal. This might be an explicit confidence score if the judge is prompted to report one, a calibrated probability derived from logprob analysis, or a heuristic based on score magnitude — outputs scored near the extremes of the rubric are typically higher confidence than outputs scored in the ambiguous middle. When the primary judge's confidence is above the threshold, you accept its score as final. When confidence falls below the threshold, you escalate to additional judges.

Research published at ICLR 2025 formalized this approach under the name "Trust or Escalate," demonstrating that cascaded selective evaluation — where a cheap model judges first and a more capable model judges only uncertain cases — can guarantee human-level agreement rates while reducing API costs by 40% compared to running a frontier model on everything. The key insight is that the majority of evaluations are easy. A response that is clearly excellent or clearly terrible doesn't need three frontier models to assess it. A single mid-tier model identifies these cases reliably. The hard cases — the responses in the ambiguous middle where quality is debatable — are where ensemble power matters, and those cases are typically a minority of total volume.

The threshold itself requires calibration. Set it too high and you escalate too many evaluations, eroding cost savings. Set it too low and you accept too many wrong single-judge scores, undermining reliability. The calibration process mirrors what you do for any classification threshold: use a labeled validation set where you know the correct answers, sweep the threshold across values, and find the point that gives you your target accuracy at your target cost. For most teams, the sweet spot is a threshold that sends 60-75% of evaluations through a single judge and escalates 25-40% to the ensemble.

## The Cascade Pattern at Production Scale

The Confidence Threshold is the simplest form of a broader pattern that dominates LLM judge architecture at scale in 2026: **the judge cascade**. A cascade arranges judges in tiers of increasing cost and capability, with each tier handling only the cases that the previous tier couldn't resolve confidently.

Tier one is a fast, cheap judge. In many production systems, this is an open-weight model running on your own infrastructure — a fine-tuned Llama 4 or Mistral Small 3.1 — with near-zero marginal cost per evaluation. It handles the easy cases: outputs that are clearly correct, clearly wrong, or clearly out of scope. At typical production distributions, this tier resolves 60-70% of evaluations with acceptable accuracy.

Tier two is a mid-tier judge, typically a frontier model like Claude Sonnet 4.5 or GPT-5-mini, invoked via API for the cases that tier one couldn't resolve confidently. These are the ambiguous outputs — quality that could go either way, edge cases the rubric doesn't cover cleanly, responses that are partially correct. Tier two resolves another 20-30% of evaluations.

Tier three is the full ensemble — multiple frontier judges with majority voting or deliberation, sometimes combined with human review — reserved for the 5-10% of evaluations where tiers one and two disagree or where the stakes of a wrong score justify maximum reliability.

The cost savings from this architecture are dramatic. Assume your evaluation volume is 100,000 outputs per day. A single frontier judge at four cents per eval costs $4,000 daily. A three-judge frontier ensemble costs $12,000. A cascade where 65% are handled by an open-weight judge at near-zero cost, 28% by a single frontier judge at four cents, and 7% by a three-judge ensemble at twelve cents costs roughly $1,960 — half the cost of the single-judge approach and one-sixth the cost of the full ensemble, while achieving ensemble-level accuracy on the cases that actually need it.

## Designing for Judge Independence

Ensemble accuracy depends on judges making independent errors. If all three judges share the same blind spot — inflating scores for confident-sounding responses, for example — three wrong answers don't become right through voting. They become wrong with false confidence.

Judge independence comes from diversity across three dimensions. First, model diversity: use judges from different providers or different model families. A Claude judge and a GPT judge make less correlated errors than two different Claude models. An open-weight judge fine-tuned on your specific rubric makes different errors than either frontier model. Second, prompt diversity: even with the same underlying model, different judge prompts elicit different evaluation behaviors. One prompt might emphasize factual accuracy, another might weight completeness, a third might focus on safety. Scoring the same quality dimension from different angles reduces shared blind spots. Third, perspective diversity: some teams use a technique where each judge is assigned a specific persona — "evaluate as a domain expert," "evaluate as a new user," "evaluate as a safety reviewer" — producing naturally different scoring tendencies that, when aggregated, approximate a broader range of human judgment than any single perspective.

The practical challenge is that achieving true independence at scale requires maintaining multiple judge configurations — different models, different prompts, different infrastructure. This is operational overhead. Each judge needs its own latency monitoring, its own cost tracking, its own calibration loop. Teams that run three judges often find they're maintaining three parallel evaluation subsystems. The complexity is justified for high-stakes evaluations but excessive for routine quality monitoring. This is yet another reason why cascading matters: you only need to maintain the full ensemble infrastructure for the small percentage of evaluations that reach the top tier.

## Aggregation Beyond Simple Voting

Majority voting treats all judges as producing the same kind of signal: a score on the same scale. But in practice, different judges in an ensemble often evaluate subtly different things, even when given the same rubric. One judge might be more sensitive to factual errors. Another might weight stylistic quality more heavily. A third might be stricter on safety. Reducing these diverse signals to a simple vote discards information.

More sophisticated aggregation strategies extract value from this diversity. **Score averaging** — taking the mean of all judges' numerical scores rather than voting on pass-fail — preserves the continuous signal and lets you detect cases where judges mildly disagree (scores of 3, 4, and 4) versus cases where they fundamentally disagree (scores of 1, 4, and 5). **Dimension-specific routing** assigns different quality dimensions to different judges based on each judge's strength — if Judge A has the highest calibrated accuracy for factual correctness and Judge B is best at tone assessment, each judges only its strongest dimension, and the results are combined. This avoids paying for three judges on every dimension when each dimension has a clear best judge. **Disagreement flagging** doesn't try to resolve disagreements automatically at all — it routes disagreements to human review, treating the ensemble as a triage system rather than a resolution system.

The choice of aggregation strategy depends on your downstream use of the scores. If you're using eval scores for binary release gating — ship or don't ship — majority voting is appropriate because you need a clear decision. If you're using eval scores to track quality trends over time, score averaging gives you more sensitive trend detection. If you're using eval scores to identify specific outputs for human review, disagreement flagging is most useful because it highlights exactly the cases where automated judgment is least reliable.

## Scaling Ensemble Infrastructure

Running multiple judges at scale introduces infrastructure requirements that don't exist for single-judge systems. The most immediate is latency management. If your three judges run sequentially, your evaluation latency triples. If they run in parallel, you need infrastructure that can dispatch three concurrent API calls per evaluation, collect all responses, handle partial failures when one judge times out, and aggregate results — all within your latency budget.

Parallel execution is the standard approach, but it introduces its own challenges. You need to handle the case where one judge fails while two succeed. Do you proceed with two judges? Do you retry the failed judge? Do you substitute a backup judge? The answer depends on your consensus protocol. For majority voting with three judges, two successful responses still give you a majority — but now a "majority" is unanimous agreement between the two survivors, which is a different statistical guarantee than two-of-three. Your system needs to be aware of these edge cases and either adjust its confidence reporting or escalate to a higher tier when it gets fewer responses than expected.

Retry logic for judge calls in an ensemble is different from retry logic for inference calls. If a production model inference fails, you retry immediately because the user is waiting. If a judge call fails and you have results from the other judges, the evaluation can often proceed without retrying, because the cost of retrying — added latency, added expense — may exceed the cost of operating with a reduced ensemble for that single evaluation. Defining your retry and fallback policies per tier of the cascade is an infrastructure decision that teams frequently overlook until they hit their first sustained judge API outage and discover that their evaluation pipeline grinds to a halt because it waits indefinitely for a response that never comes.

## When Ensembles Are Not Worth It

Not every evaluation benefits from an ensemble. If your single judge has 95% agreement with human labels on a given quality dimension, adding a second judge might push that to 97%. That two-point improvement costs double. For many evaluation dimensions, 95% is more than sufficient, and the money is better spent improving coverage, adding new evaluation dimensions, or funding more human calibration reviews.

The ensemble decision should be driven by two factors: the error rate of your current single judge on the specific dimension you're evaluating, and the cost of a wrong evaluation in that context. If the error rate is below 8-10% and the consequences of errors are modest, a single judge is sufficient. If the error rate exceeds 15% or the consequences of errors are severe, ensembles become necessary. The uncomfortable middle ground between 10% and 15% is where judgment calls happen, and the right answer depends on your specific risk tolerance and budget constraints.

One useful heuristic: calculate the expected cost of wrong evaluations per month. If your single judge evaluates 50,000 outputs per month with a 12% error rate, that's 6,000 wrong evaluations. If each wrong evaluation leads to a downstream cost — a bad output served to a user, a missed quality regression, a false alarm that wastes human reviewer time — estimate that cost. If 6,000 wrong evaluations cost your organization $3,000 per month in downstream impact, and adding a second judge at $2,000 per month would reduce wrong evaluations to 2,000 (saving roughly $2,000 in downstream impact), the second judge pays for itself. If the downstream cost of wrong evaluations is only $500 per month, the second judge is a net loss. This is the kind of calculation that separates teams who deploy ensembles strategically from teams who deploy them because they assume more judges are always better.

Ensembles improve reliability, but they are only as good as the underlying judges' alignment with human judgment. If all three judges are miscalibrated in the same direction — consistently scoring outputs higher than humans would — voting among them produces a confident but wrong consensus. This is why calibration is the deeper problem, and why the next subchapter addresses the recurring process of keeping your judges aligned with human judgment as your system, your product, and your models evolve over time.