# 6.3 — Dataset Versioning at Scale: Why It Differs Fundamentally from Code Versioning

You version your code. You version your models. If you don't version your evaluation datasets with the same rigor, you have no way to know whether a quality change is real or an artifact of changing your test set. A team that updates their golden set on Tuesday and sees a quality drop on Wednesday faces a question they cannot answer: did the model get worse, or did the test get harder? Without dataset versioning, this question has no resolution. The investigation stalls. The team either rolls back the model change unnecessarily or ignores the quality drop dangerously. Both outcomes are failures of infrastructure, not failures of judgment.

Dataset versioning is the discipline of tracking every change to your evaluation data — what was added, what was removed, what was modified, when, by whom, and why — so that every evaluation result can be traced back to the exact dataset state it was measured against. This subchapter covers why dataset versioning matters, how it differs from the code versioning workflows you already know, and the infrastructure strategies that work at scale.

## Why Dataset Versioning Is Non-Negotiable

Three capabilities break completely without dataset versioning, and each one is essential for an evaluation system that teams can trust.

The first is **reproducibility**. When someone asks "What was our quality score on March fifteenth?" you need to be able to answer not just with a number but with the ability to rerun that exact evaluation and get the same result. That requires the exact model version, the exact eval configuration, and the exact dataset version. If you've been modifying your golden set in place — adding examples, correcting labels, removing outdated entries — you can't recover the March fifteenth version. The dataset that produced that score no longer exists. You have a number in a database with no way to verify it, challenge it, or understand what drove it.

The second is **attribution**. When quality scores change, you need to know what caused the change. In a system with multiple moving parts — model weights, prompt templates, eval rubrics, eval datasets — a score change could be caused by any of them. Dataset versioning lets you hold the dataset constant while varying the model, or hold the model constant while varying the dataset. Without it, every quality investigation starts with the same unanswerable question: did the thing we're measuring change, or did our measuring instrument change?

The third is **auditing**. In regulated industries — healthcare, finance, legal — you may need to demonstrate that your evaluation standards met specific requirements at specific points in time. A regulator who asks "How did you evaluate your system before the June deployment?" expects an answer that includes the evaluation criteria and the data that was tested. If you can't reconstruct the evaluation dataset that was in effect during the June evaluation, you can't satisfy that audit. In the era of the EU AI Act's GPAI compliance requirements, which began enforcement for general-purpose AI models in 2025, this is not a theoretical concern.

## How Dataset Versioning Differs from Code Versioning

Teams accustomed to Git often assume that dataset versioning is just "Git for data." It isn't. The differences are fundamental, and treating dataset versioning as a trivial extension of code versioning leads to infrastructure that breaks at scale.

The first difference is **size**. A typical code repository is megabytes. A typical evaluation dataset — especially a silver tier set with tens of thousands of examples including inputs, expected outputs, and metadata — is gigabytes. Git was designed for small text files. It stores full copies of every version and computes character-level diffs. At gigabyte scale, this means your repository balloons in size, clones take minutes to hours, and every operation touches more data than Git was optimized for. Teams that naively check their evaluation datasets into Git discover this when their CI/CD pipeline takes forty-five minutes to clone the repo.

The second difference is **diff semantics**. In code, a diff is textual: a line was added, a line was removed, a character was changed. In a dataset, the meaningful unit of change is the example, not the line. Adding a new evaluation example is one logical change, even though it might span multiple lines across multiple fields. Removing an outdated example is one logical change. Correcting a ground truth label is one logical change that might look like a minor text edit but has enormous evaluation implications — it changes what the system considers "correct." A textual diff that shows "changed 'Paris' to 'Lyon'" doesn't convey the evaluation impact: every model output that said "Paris" was being scored as correct, and now it will be scored as wrong. Code diffs tell you what changed syntactically. Dataset diffs need to tell you what changed semantically.

The third difference is **merge conflicts**. In code, merge conflicts are syntactic: two people edited the same line. In datasets, merge conflicts are conceptual: two people added examples that test the same capability, or one person added an example while another person changed the quality standards that the example should meet, or one branch added a hundred synthetic examples while another branch reorganized the dataset schema. These conflicts can't be resolved by choosing "ours" or "theirs" on a line-by-line basis. They require human judgment about dataset composition, coverage, and quality standards.

The fourth difference is **review process**. Code reviews focus on logic, readability, and correctness of implementation. Dataset reviews focus on representativeness, label accuracy, and coverage impact. A code reviewer asks "Does this function do what it should?" A dataset reviewer asks "Is this expected output actually correct? Does this example add coverage we need? Does removing this example create a blind spot?" The skills, tools, and time required for dataset review are different from code review, and using code review tools for dataset review produces a process that's awkward for reviewers and misses the questions that matter.

## What a Dataset Version Includes

A complete dataset version is more than a snapshot of the data. It's a package of information that makes the version interpretable and reproducible.

The **data itself** is the core: every input-output pair in the evaluation set, in whatever format the evaluation pipeline consumes. For a golden set, this might be a few thousand rows. For a silver set, it might be millions. The data must be stored in a format that is both machine-readable for the eval pipeline and human-inspectable for reviewers. Structured formats like Parquet, JSONL, or CSV with consistent column ordering serve both purposes. Binary formats that require specialized tooling to inspect make review difficult and reduce the likelihood that anyone will actually review changes.

The **metadata** tells you the context of this version: when it was created, who approved it, what changed from the previous version, and why. The "why" is critical and almost always missing. A changelog that says "Added 47 examples" is barely useful. A changelog that says "Added 47 examples covering the new document summarization feature launched on January 12th, to close the coverage gap identified in the December golden set audit" tells you why the change was made, what problem it solves, and how to evaluate whether it was the right change. Without the "why," future team members who inherit the dataset can see what happened but not why, making it impossible to judge whether the decisions were sound.

The **schema** defines the structure of each example: what fields exist, what format each field follows, and what constraints each field must satisfy. When the schema changes — adding a new metadata field, changing the format of the expected output, splitting a single output field into separate fields for different quality dimensions — that change must be captured as part of the version. A schema change without a corresponding version bump means downstream consumers of the dataset may break silently when they encounter the new format.

The **provenance** records where each example came from. Was it sampled from production traffic? Was it synthetically generated? Was it hand-crafted by a domain expert? Was it promoted from the silver tier? Provenance matters because it affects how much you should trust the example's label and how you should interpret evaluation results that depend on it. A dataset where ninety percent of examples are expert-crafted has different reliability characteristics than one where ninety percent are synthetically generated with automated labels.

## Versioning Strategies That Work at Scale

Three versioning strategies have proven effective for evaluation datasets, each with different tradeoffs in complexity, storage cost, and operational simplicity.

**Immutable snapshots** is the simplest strategy. Every time the dataset changes, you create a complete new copy with a new version identifier. Version 1 is the original dataset. Version 2 is the dataset after the January update. Version 3 is the dataset after the February update. Each version is a self-contained, frozen artifact that never changes. To reproduce an evaluation from version 2, you fetch version 2 and run against it. There's no assembly, no delta computation, no risk of getting a different dataset than what you expected.

Immutable snapshots are easy to implement, easy to reason about, and reliable. The downside is storage cost. If your golden set is one gigabyte and you create a new version monthly, you're storing twelve gigabytes per year — and that's just the golden tier. For a silver set at fifty gigabytes with weekly updates, the storage costs become significant. In practice, most teams mitigate this with object storage — Amazon S3, Google Cloud Storage, Azure Blob Storage — where per-gigabyte costs are low enough that storing a few terabytes of dataset versions is cheaper than the engineering effort to implement a more sophisticated strategy.

**Delta-based versioning** stores the initial version in full and subsequent versions as diffs from the previous version. To reconstruct version 5, you start from version 1 and apply the diffs for versions 2, 3, 4, and 5 in sequence. This saves storage because most updates touch a small fraction of the total dataset — adding fifty examples to a five-thousand-example golden set is a one-percent change. The diffs are small, and storage grows slowly.

The cost of delta-based versioning is reconstruction complexity. Recovering an old version requires applying a chain of diffs, which is slow for old versions with many intervening changes and fragile if any intermediate diff is corrupted or missing. Some teams mitigate this with periodic checkpoints — storing a full snapshot every tenth version and computing diffs relative to the nearest checkpoint. This bounds the maximum reconstruction cost.

**Content-addressable storage** is the approach used by tools like DVC and lakeFS — and since lakeFS acquired DVC in late 2025, the ecosystem has consolidated around a unified philosophy. In this approach, each file or dataset artifact is stored by its content hash. A version is a manifest that maps logical file names to content hashes. When an example is added, only the new example is stored; the version manifest points to the existing hashes for unchanged examples and the new hash for the addition. This provides the storage efficiency of delta-based versioning with the reconstruction simplicity of snapshots — recovering any version requires reading the manifest and fetching the referenced content, with no sequential diff application.

Content-addressable storage also gives you deduplication for free. If two versions of the dataset share ninety-nine percent of their examples, the shared examples are stored once and referenced by both versions. For large silver-tier datasets where weekly updates touch a small fraction of the data, the storage savings are substantial.

## The Minimum Viable Dataset Version

If you're not ready to invest in sophisticated versioning infrastructure, the minimum viable approach is a timestamped snapshot with a changelog. Every time someone modifies the evaluation dataset, they create a copy named with the current date — golden-set-2026-02-14 — and write a text file describing what changed and why. The copy goes into cloud storage or a shared drive. The changelog goes into a shared document or wiki.

This approach is crude but better than nothing. It gives you reproducibility: you can rerun any past evaluation by fetching the snapshot from that date. It gives you basic attribution: the changelog tells you what changed. It doesn't give you efficient storage, semantic diffs, or automated provenance tracking. But it lets a team of three get started with dataset versioning in an afternoon, and it's infinitely better than the alternative of modifying the golden set in place with no record of what changed.

The minimum viable version fails at scale — fifty timestamped copies of a fifty-gigabyte silver set consume two and a half terabytes and take forever to browse — but it buys you time to implement a proper solution while establishing the discipline of versioning from day one.

## Cross-Referencing: Tying Eval Results to Dataset Versions

The most important rule of dataset versioning is this: every evaluation result must reference the exact dataset version it was evaluated against. Without this cross-reference, historical evaluation results are uninterpretable.

Imagine your dashboard shows that quality dropped from ninety-three percent to eighty-nine percent between Monday and Friday. Was it a real regression? Or did someone update the golden set on Wednesday, adding harder examples and correcting two ground truth labels? Without the dataset version attached to each eval run, you can't tell. The Monday result was computed against dataset version 14. The Friday result was computed against dataset version 15. If you compare them as if they measured the same thing, you're comparing measurements taken with different instruments and treating the difference as signal.

In practice, this means your eval runner must record the dataset version alongside every result. When the evaluation pipeline pulls the golden set to run a regression suite, it records not just "golden set" but "golden-set-v15, content hash abc123, created 2026-02-12." When the result is stored, the dataset version is stored with it. When the result is displayed on a dashboard, the dataset version is available as metadata. When someone queries "show me the quality trend over the last month," the system can either normalize for dataset version changes or flag the points where the dataset version changed so the viewer knows to interpret the trend carefully.

Sophisticated evaluation platforms go further. They compute **dataset-adjusted scores** that account for changes in dataset difficulty between versions. If version 15 added twenty hard examples that version 14 didn't have, the raw score drop between the two versions is partially attributable to the harder test, not the model. By running the new model against both the old and new dataset versions, you can decompose the score change into the component caused by the model change and the component caused by the dataset change. This decomposition is the gold standard for evaluation attribution, and it requires the cross-referencing infrastructure that dataset versioning makes possible.

## The Organizational Discipline of Dataset Versioning

Dataset versioning is as much an organizational challenge as a technical one. The infrastructure for creating versions, storing snapshots, and tracking metadata is the easy part. The hard part is ensuring that people actually use it — that every dataset change goes through the versioning system, that every changelog is written with enough detail to be useful, and that nobody modifies the live evaluation dataset directly without creating a new version.

The most effective teams treat dataset changes like code changes. A dataset modification starts as a proposal: "I want to add thirty-five examples covering the new regulatory disclosure requirement." The proposal includes the examples themselves, the provenance of each example, and the reason for the change. A reviewer — typically the dataset owner or a domain expert — examines the proposal, checks the ground truth labels, evaluates the coverage impact, and approves or requests changes. Once approved, the change is applied, a new version is created automatically, and the changelog is generated from the proposal description. This process can be implemented in Git with data stored in a separate artifact system, in a purpose-built tool like lakeFS, or even in a shared spreadsheet with a manual versioning process. The tool matters less than the discipline.

The hardest habit to establish is writing useful changelogs. Engineers resist it because it feels like bureaucracy. But a changelog that says "Routine update" provides zero value to the future engineer who needs to understand why the evaluation scores shifted after this version. Requiring structured changelog entries — what changed, how many examples were affected, which quality dimensions or product features are impacted, and why the change was necessary — transforms the changelog from an administrative burden into a diagnostic resource that pays for itself the first time someone investigates a suspicious quality trend.

## When Dataset Versioning Fails

Even teams with versioning infrastructure encounter failure modes that undermine the system's value.

The most common failure is **version skipping**. Someone makes a "quick fix" to the live dataset — correcting an obviously wrong label, removing a duplicate example — without creating a new version. The fix is reasonable. The label was wrong, and leaving it wrong would produce incorrect evaluation results. But by fixing it in place, the team has broken the connection between past evaluation results and the dataset they were computed against. If someone tries to rerun last week's evaluation, they'll get a different result because the dataset has been silently modified. Over time, accumulated quick fixes create a gap between the nominal version — the version recorded in the evaluation results — and the actual state of the dataset.

The mitigation is to make versioning the path of least resistance. If creating a new version requires five steps and a manual approval, people will skip it for small changes. If creating a new version requires one command that automatically snapshots, generates a diff, and prompts for a changelog description, people will use it because it's faster than the alternative. Good tooling makes good discipline easy.

The second common failure is **version proliferation without pruning**. A team that creates a new version for every change accumulates hundreds of versions. Most are never referenced again. The version list becomes unnavigable. Nobody can find the version they need. The storage costs grow unchecked. The mitigation is a retention policy: keep every version that was used for a release gate evaluation, keep the most recent twelve monthly snapshots, and archive everything else. Archived versions are available for recovery but not listed in the default view. This keeps the active version list manageable while preserving the ability to reproduce any evaluation that led to a deployment decision.

The third common failure is **changelog decay**. The first ten versions have detailed, useful changelogs. By version thirty, the changelogs say "Updated." By version fifty, there are no changelogs at all. The mitigation is automation and enforcement. If the versioning tool won't create a new version without a changelog entry of at least fifty characters, the changelog quality stays above a usable floor. It's a crude heuristic, but fifty characters forces the author to write at least one meaningful sentence, which is infinitely more useful than a blank field.

## From Versioning to Freshness

Versioning tells you what changed and when. It gives you the infrastructure to reproduce past evaluations, attribute quality changes, and maintain audit trails. But versioning alone doesn't tell you whether your dataset needs to change — whether the ground truth it represents still matches the reality it's supposed to measure. A perfectly versioned dataset can still be fatally stale if the world has moved on and the dataset hasn't kept up. The next subchapter covers staleness tracking and refresh cadence: the signals that tell you when your ground truth has drifted from reality, and the maintenance rhythms that keep your evaluation data trustworthy.