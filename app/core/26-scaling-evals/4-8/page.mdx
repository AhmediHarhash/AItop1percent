# 4.8 — Cost Management for Online Evaluation: Scoring at Scale Without Breaking the Budget

In early 2025, a media company launched real-time eval scoring on their content recommendation engine. The system ran an LLM judge on every production output — twenty million recommendations per day — scoring each for relevance, tone, and content safety. The quality signal was excellent. The team caught subtle degradation patterns within hours, flagged safety issues before they reached more than a few hundred users, and built a real-time quality dashboard that became the centerpiece of their weekly review meetings. Then the first monthly invoice arrived. The eval system had cost $840,000 — more than three times the cost of the production AI itself. The CTO killed the program the same afternoon. The company reverted to no online evaluation at all, losing not just the cost but the quality visibility they had built.

The tragedy isn't that they spent too much. It's that they had no cost management strategy. A well-designed online evaluation system can deliver ninety percent of the quality signal at five to ten percent of the cost. The media company didn't need to evaluate every output with a frontier LLM judge. They needed to evaluate the right outputs with the right tools at the right frequency. Without cost management, online evaluation follows a predictable path: launch, scale, shock, shutdown. With cost management, it follows a better path: launch, tune, sustain, improve.

## Why Online Eval Costs Spiral

Offline evaluation runs on a fixed schedule against a fixed test set. You control the volume. If your golden set has two thousand examples and you evaluate nightly, you know exactly how many evaluations you'll run and what they'll cost. The cost is predictable and bounded.

Online evaluation is the opposite. It scales with production traffic. Every user request is a potential evaluation. Every new feature, every marketing push, every viral moment increases the number of outputs eligible for scoring. If your online evaluation system evaluates a fixed percentage of traffic, your evaluation cost is a direct function of your production volume — and production volume is the one variable your business wants to increase without bound.

This creates a vicious dynamic. Success makes evaluation more expensive. A product that doubles its user base doubles its online evaluation cost. A feature launch that increases traffic by forty percent increases evaluation cost by forty percent. The cost grows at the same rate as revenue in the best case, but evaluation cost has no natural ceiling in the way that inference cost does — inference cost is bounded by the price of the model per token, while evaluation cost adds a second model call (the judge) on top of the first.

The compound effect is what catches teams off guard. A single LLM judge call on a single output might cost a fraction of a cent. At twenty million outputs per day, fractions of a cent become hundreds of thousands of dollars per month. The per-unit cost feels trivial. The aggregate cost is catastrophic. Teams that plan online evaluation by looking at per-unit cost and multiplying by expected traffic volume often discover that the result is a number their budget cannot absorb.

## The Six Cost Levers

Online evaluation cost is not a single number you accept or reject. It is the product of six variables, each of which you can tune independently. Understanding these levers is the difference between "we can't afford online evaluation" and "we can afford exactly the online evaluation we need."

**Sampling rate** is the most powerful lever. You don't need to evaluate every output. You need to evaluate enough outputs to detect the quality changes you care about, at the statistical confidence level you require. For most systems, evaluating one to five percent of production traffic provides sufficient signal to detect quality degradations of two or more percentage points within hours. A team running twenty million outputs per day can reduce their evaluation volume from twenty million to two hundred thousand by sampling at one percent — cutting costs by ninety-nine percent while retaining the ability to detect meaningful quality shifts.

The key insight about sampling is that detection sensitivity degrades gracefully, not catastrophically. At one hundred percent evaluation, you detect a two-point quality degradation in fifteen minutes. At five percent evaluation, you detect the same degradation in two hours. At one percent, perhaps six hours. The question is whether detecting a quality problem in six hours instead of fifteen minutes is acceptable for your use case. For most products, it is. For life-critical systems, it may not be. Match the sampling rate to the detection speed your use case requires.

**Judge model selection** is the second lever. Not every evaluation needs a frontier model. Frontier LLM judges — GPT-5, Claude Opus 4.6, Gemini 3 Pro — cost five to twenty times more per evaluation than mid-tier models. But mid-tier and small models — GPT-5-mini, Claude Haiku 4.5, Gemini 3 Flash — produce evaluation scores that correlate at eighty-five to ninety-two percent with frontier judge scores for most evaluation tasks. For routine quality monitoring, where you need to detect significant degradation rather than make fine-grained quality distinctions, a smaller judge model is often sufficient.

The practical approach is to calibrate your smaller judge against your frontier judge on your specific evaluation tasks. Run both judges on a thousand production outputs. Measure the correlation. If the smaller judge's scores predict the frontier judge's pass/fail decisions at ninety percent accuracy or above, the smaller judge is a viable replacement for routine online evaluation. Reserve the frontier judge for the cases that matter most: close-call outputs near the quality threshold, safety-critical evaluations, and periodic calibration runs.

**Prompt efficiency** is the third lever. LLM judge costs are proportional to the number of tokens processed. A judge prompt that includes the full system prompt, the user's entire conversation history, the model's output, a detailed rubric, and ten scoring examples can easily consume five thousand to eight thousand tokens per evaluation. A streamlined prompt that includes only the user's query, the model's output, and a concise rubric might consume one thousand to two thousand tokens. The cost difference is three to five times per evaluation.

Streamlining judge prompts requires care. Removing context can degrade judge accuracy. The solution is to test prompt variants systematically: start with your full prompt, then progressively remove components and measure the impact on judge accuracy. Many teams discover that the ten scoring examples in their judge prompt contribute only a marginal improvement over three well-chosen examples, and that the full conversation history can be replaced with a one-sentence summary of context without meaningful accuracy loss. Each simplification reduces cost per evaluation.

**Caching** is the fourth lever. Production traffic contains duplicates and near-duplicates. A customer support system might receive the same question — or minor variations of the same question — hundreds of times per day. If the model produces identical or near-identical outputs for identical inputs, the eval score will be the same. Caching eval results for identical or near-identical input-output pairs eliminates redundant scoring.

The cache key is typically a hash of the input query and the model's output. When a new output arrives for evaluation, the system checks the cache. If a matching entry exists and was scored within the cache's time-to-live window — typically twenty-four to seventy-two hours — the cached score is returned without calling the judge. If no match exists, the judge is called and the result is cached. For systems with significant traffic repetition, caching can reduce judge call volume by thirty to sixty percent on top of whatever reduction sampling provides.

Near-duplicate detection extends caching further. Two queries that differ only in a customer name or a product ID will likely receive the same quality score. Normalizing inputs before hashing — stripping personally identifiable information, replacing specific entities with generic placeholders — broadens the cache hit rate. This requires careful validation to ensure that normalization doesn't collapse meaningfully different queries into the same cache entry, but when done correctly, it can double the effective cache hit rate.

**Tiered scoring** is the fifth lever. Not every output needs the same depth of evaluation. A tiered approach applies fast, cheap heuristic checks to every output and reserves expensive LLM judge evaluations for the outputs where heuristics can't make a confident determination.

The first tier is heuristic scoring: regex-based safety checks, response length validation, format compliance checks, keyword detection. These run in milliseconds at negligible cost. Any output that clearly passes all heuristic checks — a well-formatted response of appropriate length with no safety keywords — gets a "heuristic pass" and is not sent to the LLM judge. Any output that clearly fails a heuristic check — a response containing blocked content, an empty response, a response that dramatically exceeds length limits — gets a "heuristic fail" and is not sent to the judge either.

The second tier is the LLM judge, which evaluates only the outputs that fall in the ambiguous zone: outputs that didn't clearly pass or clearly fail the heuristics. Depending on the product and the heuristic design, this ambiguous zone might be twenty to fifty percent of traffic. By filtering the obvious cases with cheap heuristics, you reduce judge volume by fifty to eighty percent — and the evaluations you do run are on the outputs that most benefit from nuanced assessment.

**Batching** is the sixth lever. Some online evaluation doesn't need to be truly real-time. If your use case requires quality monitoring for trend detection rather than per-output gating, you can batch evaluation calls. Instead of sending each output to the judge individually, accumulate outputs for five or ten minutes and send them in a batch. Batch API pricing from major providers is thirty to fifty percent cheaper than synchronous API pricing. The tradeoff is latency — you lose the ability to react to individual output quality in real time. But for monitoring use cases where you're tracking aggregate quality trends, batch latency is acceptable and the cost savings are significant.

## The Online Eval Budget Cap

**The Online Eval Budget Cap** is a hard dollar limit on online evaluation spend per day, week, or month, with automatic adjustments to sampling rate and judge selection when the cap is approached. It is the single most important cost management mechanism for online evaluation, because it prevents the spiral that killed the media company's program.

The budget cap works as a feedback loop. You set a monthly budget — say, $15,000 for online evaluation. The system tracks cumulative spend daily. When cumulative spend is on pace to finish the month under budget, the system operates at its configured sampling rate and judge model selection. When cumulative spend is on pace to exceed the budget, the system automatically reduces sampling rate or switches from a frontier judge to a cheaper alternative. The quality signal degrades slightly, but the cost stays within bounds.

Implementing the budget cap requires real-time cost tracking. Every judge call has a known cost — the input token count times the per-token price, plus the output token count times the per-token price. The system logs these costs as they occur and maintains a running total. A daily cost projection — current month-to-date spend divided by days elapsed, multiplied by days in the month — provides the forecast that triggers adjustments.

The adjustment logic should be graduated, not binary. If the projection is five percent over budget, reduce sampling rate by ten percent. If the projection is twenty percent over budget, switch routine evaluations to a cheaper judge model. If the projection is fifty percent over budget, reduce sampling to the minimum viable rate — the rate below which quality signal becomes unreliable. These graduated responses ensure that the system degrades gracefully rather than swinging between full evaluation and no evaluation.

The budget cap also serves an organizational function. It forces the conversation about evaluation investment. When the evaluation team says "we need $15,000 per month for online evaluation," leadership can evaluate that against the cost of quality incidents the evaluation would prevent. When online eval costs rise because production traffic grew, the budget cap surfaces the decision: increase the budget, or accept reduced evaluation coverage. This is better than the alternative, which is uncapped evaluation spend that grows silently until someone notices and shuts the entire program down.

## Budget Allocation: Online vs Offline

Teams building their evaluation budget for the first time often ask what percentage should go to online evaluation versus offline evaluation. The answer depends on the maturity and risk profile of the system, but a useful starting framework exists.

For early-stage products with low traffic and rapidly changing models, invest seventy to eighty percent of your evaluation budget in offline evaluation and twenty to thirty percent in online. At this stage, you're iterating fast, the model changes frequently, and offline evaluation is where most quality decisions are made. Online evaluation provides a safety net but doesn't need to be comprehensive.

For mature products with high traffic and stable models, the balance shifts. Invest fifty to sixty percent in offline and forty to fifty percent in online. At this stage, the model changes less frequently, but the traffic volume means that even rare quality issues affect many users. Online evaluation becomes the primary quality signal because production traffic is where the novel problems emerge. Offline evaluation remains important for regression testing and pre-deployment validation, but the critical quality decisions happen in production.

For high-stakes products — healthcare, financial advice, legal — invest the most in online evaluation that your budget allows, potentially sixty to seventy percent of the total evaluation budget. The consequences of a quality failure reaching users are severe enough that real-time detection justifies the higher cost. These systems should also maintain robust offline evaluation, but the cost of missing a production quality issue outweighs the cost of extensive online scoring.

## The Minimum Viable Online Eval

For teams that can't afford comprehensive online evaluation — startups, small teams, products in early stages — the minimum viable approach costs very little and still provides meaningful quality signal.

The minimum viable online eval has three components. First, heuristic checks on one hundred percent of traffic: basic safety keywords, response length bounds, format validation. These are nearly free to run — they're string operations, not model calls. They catch the catastrophic failures: empty responses, safety violations, format breakdowns.

Second, an LLM judge on a one-percent random sample, using a mid-tier model like GPT-5-mini or Claude Haiku 4.5. At one million requests per day, a one-percent sample is ten thousand evaluations. At roughly 0.002 to 0.005 dollars per evaluation with a small model and a concise prompt, that's twenty to fifty dollars per day. This provides a continuous quality signal sufficient to detect degradations of two or more points within a few hours.

Third, a daily summary report that aggregates the sampled evaluation scores and compares them against the previous seven-day average. Any drop greater than one standard deviation triggers an alert for human review. This report costs nothing beyond the compute to generate it and provides the trend visibility that prevents slow degradation from going unnoticed.

Total cost: twenty to fifty dollars per day, or six hundred to fifteen hundred dollars per month. For the vast majority of production AI systems, this is a trivial addition to the operating budget and provides quality visibility that would otherwise require manual review — which is far more expensive in labor costs.

## The Cost Monitoring Loop

Online evaluation cost management is not a one-time configuration. It requires ongoing monitoring because the inputs change: traffic volume fluctuates, output distributions shift, cache hit rates vary, and judge model pricing changes as providers update their rates.

The cost monitoring loop tracks three metrics daily. First, total online eval spend — the actual dollars spent on judge calls, computed from token counts and per-token prices. Second, cost per evaluated output — the average cost of a single evaluation, which changes when you switch judge models, modify prompts, or change scoring depth. Third, the evaluation-to-inference cost ratio — what percentage of your total AI spend goes to evaluation versus production inference. If evaluation costs rise above twenty to twenty-five percent of inference costs, something is misconfigured.

Weekly, compare these metrics against your budget and against the previous week. Rising cost per evaluated output suggests your judge prompts have grown or you've switched to a more expensive model. Rising total spend with stable cost per output suggests traffic growth. A declining evaluation-to-inference ratio during traffic growth is healthy — it means your sampling and caching are absorbing the growth. A rising ratio is a warning sign that requires intervention.

Monthly, review the cost levers. Is the sampling rate still appropriate? Has the cache hit rate changed? Could you switch routine evaluations to a cheaper model without meaningful accuracy loss? Has the traffic distribution shifted in ways that change the economics? This monthly review prevents cost drift — the gradual accumulation of small cost increases that individually seem insignificant but collectively push spending above budget.

## When to Spend More

Cost management is not synonymous with cost minimization. There are moments when increasing online evaluation spend is the correct decision.

After a major model change, increase your sampling rate temporarily — from one percent to ten or twenty percent for the first forty-eight hours. The higher cost is justified by the higher risk. A new model is the most likely time for quality surprises, and early detection at higher sampling rates prevents those surprises from reaching your full user base.

When online evaluation detects an anomaly — a quality drop, an unexpected behavioral shift, a safety signal — increase the evaluation depth on the affected query types. Switch from the routine judge to a frontier judge. Expand the evaluation prompt to include more context. These actions increase cost, but they also increase the accuracy of the quality signal at the moment when accuracy matters most.

When you're running a progressive rollout with eval gates, the canary traffic should be evaluated at a higher rate than normal production traffic. The canary represents your highest-risk traffic — it's the first users to see an untested model. Evaluating fifty percent or even one hundred percent of canary traffic for the duration of the rollout is a sound investment. The canary period is short — hours to days — and the cost of a quality failure that slips through during the rollout far exceeds the cost of comprehensive evaluation during the canary window.

The principle is straightforward: spend more when risk is higher, spend less when risk is lower. Cost management means matching your evaluation investment to your risk profile at any given moment, not maintaining a flat budget regardless of circumstances.

Online and offline evaluation both generate data — quality scores, flagged outputs, behavioral signals, cost metrics — that must flow through infrastructure to reach the teams and systems that act on it. The next chapter covers the eval pipeline as a distributed system: the compute allocation, reliability engineering, and architectural patterns required to run evaluation as production-grade infrastructure that scales with your product.
