# 11.6 — Eval Data Privacy and Compliance: PII Scrubbing, Retention Policies, and Access Control

A fintech company's eval pipeline stored full customer conversations — including account numbers, social security numbers, and transaction histories — in an eval results database that every engineer on the team could access. The database was not encrypted at rest. It had no access logging. It had been running this way for fourteen months. When an auditor discovered this during a SOC 2 review in early 2025, the remediation took four months and cost $320,000 in engineering time, legal fees, and audit re-engagement. The company also had to notify 23,000 customers whose financial data had been stored in an unprotected system, triggering a wave of support tickets that consumed the customer success team for six weeks.

The irony is that nobody intended to build a privacy violation. The eval pipeline was doing exactly what it was designed to do: capture production outputs and store them for quality analysis. The problem was that production outputs contain production data, and production data contains the most sensitive information your users entrust to your system. The eval team thought of their pipeline as a testing tool. The auditor saw it as an uncontrolled data store with personally identifiable information accessible to dozens of employees who had no business need to see it.

## Why Eval Data Is a Privacy Risk

Evaluation data is not synthetic. At scale, the overwhelming majority of eval data originates from real production interactions — real user queries, real model responses, real context documents retrieved from your knowledge base. This means your eval data contains everything your production data contains: names, addresses, email addresses, phone numbers, financial account details, medical information, legal documents, proprietary business content, and whatever else your users submit to your AI system.

Most teams understand that their production database needs access controls, encryption, and retention policies. Few teams apply the same rigor to their eval data stores. The eval database is treated as a developer tool rather than a regulated data store. Engineers query it freely for debugging. Eval results are shared in Slack channels with full input-output pairs attached. Golden sets are stored in git repositories accessible to every contributor. Sampled production outputs sit in cloud storage buckets with default permissions.

This gap between how teams protect production data and how they protect eval data creates what privacy professionals call a **shadow data problem**. The data exists in a location that the organization's data governance framework does not cover. It falls outside the scope of data access reviews, outside the retention schedules applied to production databases, and outside the monitoring that detects unauthorized access. The shadow eval data store is not invisible — everyone on the team knows it exists. It is simply ungoverned, which is worse.

## The Three Privacy Requirements

Securing eval data requires addressing three distinct requirements, each with its own technical implementation and organizational process. Treating them as a single problem leads to solutions that address one dimension while leaving the others exposed.

**PII scrubbing** is the first requirement. Before eval data is stored, personally identifiable information must be detected and masked. This means running every production output through a PII detection pipeline before it enters the eval data store. The pipeline identifies names, email addresses, phone numbers, social security numbers, financial account numbers, medical record numbers, and other sensitive identifiers. Once detected, PII is replaced with consistent pseudonyms — so "John Smith" becomes "Person-4721" everywhere it appears in that interaction, preserving the structure of the conversation without preserving the identity. Tools like Microsoft Presidio, Private AI, and spaCy-based NER pipelines handle detection. The challenge is not the technology — it is running detection at eval pipeline speed without adding latency that delays eval results.

**Retention policies** are the second requirement. Eval data does not need to exist forever. A production interaction from eight months ago has minimal value for evaluating today's model, but it still carries the same privacy risk it carried on day one. Retention policies define how long eval data is kept, what happens when it expires, and which data classes have shorter or longer retention windows. Safety-flagged interactions might be retained for twelve months to support audit trails. Routine eval samples might be retained for 90 days. Golden set examples might be retained indefinitely but must be re-anonymized whenever the scrubbing rules are updated. Without explicit retention policies, eval data accumulates indefinitely, growing the attack surface and the compliance exposure with every passing month.

**Access control** is the third requirement. Not everyone who works with eval data needs to see the same level of detail. The eval platform engineer who maintains the pipeline infrastructure needs access to metadata and aggregate metrics — not to raw user conversations. The quality analyst who reviews individual outputs needs access to scrubbed versions. The safety reviewer investigating a flagged interaction might need access to the full unscrubbed record, but only for that specific interaction, only for a limited time, and only with an audit trail. Role-based access with tiered visibility — aggregate metrics, scrubbed records, full records — matches data access to business need.

## Regulatory Requirements That Apply to Eval Data

Eval data is not exempt from data protection regulation. Teams that assume GDPR, HIPAA, or sector-specific privacy rules apply only to their production database are making a legal error that regulators will not overlook.

Under GDPR, the right to erasure applies to any data store that contains personal data. If a user requests deletion of their data, you must delete it from your eval data store, your golden sets, your error analysis logs, and any cached eval results that contain their information. This is technically difficult when PII has been embedded in training sets, eval examples, and comparative analyses. It becomes practically impossible when you do not know which eval records contain which user's data because the mapping was never maintained. Teams that implement PII scrubbing before storage sidestep most of this complexity — if the PII was never stored, there is nothing to delete.

HIPAA applies to any system that processes protected health information. If your AI system handles medical data and your eval pipeline stores the outputs, those eval records are PHI. They must be stored in HIPAA-compliant infrastructure with access logging, encryption at rest and in transit, and business associate agreements covering any third-party tools in the eval pipeline. A healthcare AI company that sends eval data through a commercial eval platform without a BAA in place is in violation the moment the first record is transmitted.

The EU AI Act adds a new layer. Under the GPAI Code of Practice finalized in July 2025, providers of general-purpose AI must document their evaluation methodology and the data used for evaluation. This documentation requirement means your eval data handling is not just a privacy concern — it is a compliance artifact that regulators may request. If your evaluation records show uncontrolled access to personal data, you are demonstrating a governance failure in a document designed to prove governance maturity.

## PII Scrubbing at Pipeline Speed

The operational challenge of PII scrubbing is performance. Your eval pipeline processes thousands of outputs per hour. Adding PII detection and masking to every output adds latency. If the scrubbing step takes 200 milliseconds per record and you are processing 10,000 records per hour, you have added over 30 minutes of total processing time to your eval pipeline. At 100,000 records per hour, you have added over five hours.

The solution is tiered scrubbing. Fast heuristic scrubbing — regex-based detection of common PII patterns like social security numbers, email addresses, phone numbers, and credit card numbers — runs inline at pipeline speed. It catches the structured PII formats that account for roughly 70% of identifiable information. Slower model-based scrubbing — NER models that detect names, addresses, and contextual identifiers — runs asynchronously on stored records before they become queryable. The eval result is stored immediately with heuristic scrubbing applied, then upgraded to full scrubbing within minutes.

This two-pass approach means there is a brief window during which partially scrubbed records exist in the eval data store. That window must be documented and access-controlled. During the window, only the pipeline service account has read access. After full scrubbing completes, records become available to authorized human reviewers. The window is a known trade-off, not an ignored risk.

False negatives — PII that the scrubbing pipeline misses — are inevitable. No detection system catches 100% of identifiable information. A customer name that is also a common English word ("Grace," "Rose," "Mark") may pass through name detection undetected. An account number in an unusual format may not match the expected pattern. The defense against false negatives is layered: heuristic scrubbing catches structured patterns, model-based scrubbing catches contextual PII, and periodic human audits of scrubbed records catch what the automated layers missed. The audit cadence matters — quarterly is common, monthly is better for high-sensitivity domains like healthcare or financial services.

## Retention Policy Design

Retention policies for eval data need to balance three competing forces. Longer retention provides more historical data for trend analysis and regression detection. Shorter retention reduces privacy risk and storage costs. Regulatory requirements impose minimum retention windows for certain data types regardless of your operational preference.

The practical approach is to define retention tiers based on data sensitivity and operational value. The first tier is routine eval samples — production outputs sampled for ongoing quality monitoring. These records have the shortest retention window, typically 30 to 90 days. After that window, the records are deleted. The quality metrics computed from those records — aggregate scores, trend data, dimensional breakdowns — are retained indefinitely because they contain no PII.

The second tier is flagged eval records — outputs that triggered a safety alert, a quality regression, or a manual review escalation. These records have a longer retention window, typically six to twelve months, because they serve as evidence for incident investigations and audit trails. They are stored with full scrubbing applied and access restricted to the safety and quality review teams.

The third tier is golden set examples — curated inputs and outputs used as evaluation benchmarks. These are retained indefinitely but require special handling. Because golden sets are reused across many eval runs, they tend to accumulate stale PII patterns. A golden set example created twelve months ago may contain PII that was adequately scrubbed at the time but would not meet today's scrubbing standards because new pattern detectors have been added. Golden sets must be periodically re-scrubbed — run through the current scrubbing pipeline to catch identifiers that earlier versions missed.

Retention enforcement must be automated. Manual deletion schedules are forgotten. A scheduled job that enforces retention policies — scanning the eval data store for records past their retention window and deleting them — runs daily. The job logs what it deletes, when, and why. The logs themselves are retained as compliance evidence.

## Access Control Architecture

Access control for eval data follows the principle of least privilege, implemented through role-based tiers that match data visibility to business need.

The first tier is aggregate access. Dashboard users, executives, and cross-functional stakeholders see aggregate metrics: average quality scores, trend lines, dimensional breakdowns, regression alerts. They never see individual eval records. Their dashboards pull from a metrics database that contains no PII by design — it stores only computed scores and statistical summaries.

The second tier is scrubbed record access. Quality analysts, eval reviewers, and product managers who conduct error analysis see individual eval records with PII removed. They can read the full conversation between user and model, but all identifying information has been replaced with pseudonyms. This is sufficient for quality assessment — you can evaluate whether a model response is accurate, relevant, and well-formatted without knowing the user's real name or account number.

The third tier is full record access. Safety investigators, compliance officers, and designated incident responders can access unscrubbed records when investigating specific flagged interactions. This access is time-limited, purpose-limited, and logged. The reviewer requests access to a specific set of records, provides a justification tied to an incident or audit, receives temporary access for a defined window (typically 24 to 72 hours), and the access is logged with their identity, the records accessed, and the stated justification.

Implementing these tiers requires that your eval data store supports row-level or field-level access control — not just database-level permissions. Most teams implement this by maintaining two copies of each eval record: a scrubbed version accessible to Tier 2 users and an unscrubbed version in a restricted store accessible only through the Tier 3 request process. The unscrubbed version is encrypted with a key that requires multi-party authorization to access. This architecture adds storage cost and complexity, but it is the only way to provide both everyday analytical access and emergency investigative access without compromising the access control model.

## The Audit Trail

Every access to eval data — who accessed what, when, why, and what they did with it — must be logged in an immutable audit trail. This is not optional under GDPR, HIPAA, or the EU AI Act. It is also not optional under SOC 2, ISO 27001, or any enterprise compliance framework your customers are likely to require.

The audit trail covers three categories of events. Access events record when someone queries the eval data store, which records they accessed, and through which access tier. Modification events record when eval data is created, updated, scrubbed, or deleted, and by which system or person. Policy events record when retention policies are changed, access roles are modified, or scrubbing rules are updated. Together, these three categories provide a complete provenance chain for every piece of eval data in your system.

The audit trail itself is stored separately from the eval data, with its own retention policy that exceeds the eval data retention policy. When an eval record is deleted after 90 days, the audit log recording its creation, access history, and deletion is retained for at least two years. This ensures that even after the data is gone, you can demonstrate to an auditor that it was handled properly during its lifetime.

## When Privacy Breaks the Eval Workflow

Privacy controls add friction. Engineers accustomed to querying raw production data during debugging sessions now face scrubbed records with pseudonymized identifiers. When a user reports a specific problem, the support engineer cannot simply search the eval database for that user's interactions — the user's identity has been scrubbed. Error analysis becomes harder when the context surrounding a failure is partially masked.

These friction points are real, and dismissing them as "the cost of compliance" is a recipe for teams finding workarounds. The effective response is to design privacy-aware workflows that provide the information people need without exposing the information they should not see. For debugging, provide a lookup service that maps a user's support ticket to the pseudonym used in the eval data store, so the engineer can find the relevant records without having access to a PII-to-pseudonym mapping table. For error analysis, ensure that scrubbing preserves the semantic structure of the interaction — replacing a real address with a realistic fake address rather than with a meaningless token — so the analyst can evaluate quality in a realistic context.

The teams that handle eval data privacy well do not treat it as a compliance checkbox. They treat it as a design constraint that shapes how the eval system is built from the beginning. Adding privacy controls to an existing eval pipeline is a four-month remediation project that costs $320,000. Building them in from the start adds two to three weeks of initial development time. The math is not close.

The next subchapter addresses a privacy-adjacent concern that has become equally urgent: scaling safety evaluation to meet the regulatory cadences imposed by the EU AI Act and similar frameworks.
