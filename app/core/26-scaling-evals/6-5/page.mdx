# 6.5 — Synthetic Data for Scaling Eval Coverage: LLM-Generated Test Cases That Actually Work

What do you do when your golden set covers fifty use cases but your product serves three hundred? You can't hire enough domain experts to hand-craft examples for every scenario. You can't wait six months for production data to accumulate organically. And you can't accept the risk of flying blind on two hundred and fifty use cases that your eval system doesn't measure at all. The answer, increasingly, is synthetic generation — using strong LLMs to create evaluation test cases at volumes that manual curation can't match. But "generate synthetic data" is not a strategy. It's a starting point. The difference between synthetic eval data that strengthens your quality signal and synthetic eval data that pollutes it comes down to how you generate, validate, and integrate those examples into your dataset hierarchy.

## The Role of Synthetic Data in Evaluation

Synthetic eval data exists to fill coverage gaps, not to replace human-curated examples. This distinction is fundamental and gets muddled constantly. Your golden set — the hand-crafted, expert-reviewed, high-confidence examples that anchor your quality measurement — should remain human-curated. The role of synthetic data is everything around that golden core: the long tail of use cases your experts haven't reached yet, the edge cases that are too numerous to hand-craft, the stress tests that exercise specific failure modes, and the broad-coverage silver tier datasets that verify your model doesn't fall apart in scenarios outside the golden set's scope.

Think of it as the relationship between a building's structural beams and its insulation. The golden set is the steel frame — you don't want that generated by a machine with no human oversight. The synthetic data is the insulation — it fills every gap, covers every surface, and protects against problems that the frame alone doesn't address. Both are necessary. Neither replaces the other.

The practical advantage is throughput. A team with two domain experts and an LLM generation pipeline can produce and review one hundred to five hundred useful synthetic eval examples per day. The same two experts working without a generation pipeline produce twenty to fifty hand-crafted examples per day. The five-to-ten-times throughput multiplier is what makes it feasible to cover three hundred use cases instead of fifty. But the multiplier only holds if the generated examples are actually good — which brings us to the quality problem.

## How LLM-Generated Test Cases Work

The basic process is straightforward: you prompt a strong model to generate input-output pairs that exercise specific evaluation scenarios. The prompt describes the scenario, the expected difficulty level, the quality dimension being tested, and any constraints on the format. The model generates candidate test cases that you then review, filter, and integrate into your eval dataset.

For a customer support evaluation, the generation prompt might describe a scenario where a user asks about a product return policy for an item purchased internationally, with specific constraints: the user is frustrated, the return window has expired, and the correct response should acknowledge the policy while offering an alternative. The generating model produces a user query and a reference response. That pair becomes a candidate test case.

The choice of generating model matters. You want the strongest, most capable model available — GPT-5, Claude Opus 4.6, Gemini 3 Pro — because the generating model needs to understand subtle scenario distinctions, produce realistic user queries, and craft reference responses that genuinely represent high quality. Using a weaker model for generation produces test cases that are superficially plausible but miss the nuances that make evaluation challenging. A weak model generates obvious test cases that every production model handles easily. A strong model generates test cases that probe the boundaries where models actually differentiate.

There's a critical constraint: the model you use to generate eval data should not be the same model you evaluate. If you use Claude Opus 4.6 to generate test cases and then evaluate Claude Opus 4.6's production outputs against those test cases, you've created a circular dependency. The generating model's biases, blind spots, and style preferences are baked into the eval data, and the same model will naturally conform to its own preferences. Use a different model family for generation than for production evaluation, or at minimum a different model version.

## The Quality Hierarchy for Synthetic Eval Data

Not all synthetic data is created equal, and pretending otherwise is how teams end up with eval datasets that measure nothing useful.

**Human-curated examples** remain at the top of the quality hierarchy. These are crafted by domain experts who understand the real-world context, the subtle variations in correctness, and the edge cases that matter. They belong in your super-golden and golden tiers. No synthetic process replaces this.

**Human-reviewed synthetic examples** sit at the second tier. These are generated by an LLM and then reviewed, edited, and approved by a domain expert. The expert verifies that the scenario is realistic, the reference response is correct, and the difficulty level is appropriate. The generation step is a productivity multiplier — the expert spends their time reviewing and refining instead of creating from scratch. This tier is appropriate for golden-level datasets when you need to expand coverage faster than pure manual curation allows.

**Automatically filtered synthetic examples** sit at the third tier. These are generated by an LLM and filtered by automated quality checks — embedding-based diversity analysis, difficulty calibration, format validation — but not individually reviewed by a human. A human reviews a sample to validate the filtering process, but not every example. This tier is appropriate for silver-level datasets where broad coverage is more important than per-example perfection.

**Unfiltered synthetic examples** belong nowhere in a production eval system. Raw LLM output used directly as eval data introduces noise, redundancy, and bias that degrades your evaluation signal. Every synthetic example must go through at minimum automated quality filtering. Teams that skip this step usually discover the problem when their eval suite grows to ten thousand examples that mostly test the same thing in slightly different words.

## The Copy-Paste Trap

**The Copy-Paste Trap** is the most common failure mode in synthetic eval data generation. It looks like this: you prompt the generating model to create fifty test cases for "handling angry customers." The model produces fifty examples that are lexically diverse — different words, different product names, different complaint specifics — but structurally identical. Every example follows the same arc: customer states frustration, describes problem, asks for resolution. The reference responses all follow the same pattern: acknowledge emotion, apologize, offer solution, confirm satisfaction.

If you measure coverage by counting distinct examples, you have fifty test cases. If you measure coverage by the range of scenarios actually tested, you have one test case repeated fifty times with different surface details. Your model could handle the single pattern perfectly and fail catastrophically on a genuinely different scenario — an angry customer who doesn't want resolution but wants to escalate, or one who is angry because the previous agent gave wrong information, or one whose frustration is cultural rather than transactional — and your eval suite wouldn't detect it.

Avoiding the Copy-Paste Trap requires structural diversity, not just lexical diversity. Before generating, define the axes of variation that matter for the quality dimension you're testing. For angry customer handling, those axes might include: the source of anger (product failure, billing error, service delay, previous agent error, policy disagreement), the customer's goal (refund, replacement, escalation, apology, information), the customer's communication style (direct confrontation, passive aggression, emotional venting, factual recitation), and the correct resolution complexity (simple fix, escalation required, no resolution available). Generate test cases that systematically cover combinations of these axes, not just variations within a single combination.

The practical test for the Copy-Paste Trap is embedding-based clustering. Embed all your synthetic examples into a vector space and cluster them. If the clusters are few and tight — most examples falling into a small number of clusters — you have a diversity problem regardless of how different the examples look on the surface. If the clusters are many and well-separated, your synthetic data is testing genuinely different scenarios.

## Quality Control for Synthetic Eval Data

Generating synthetic data is the first fifteen minutes. Quality control is the next eight hours. Without rigorous filtering and validation, synthetic data degrades your eval system faster than it improves it.

The first quality gate is **discriminative power testing**. A useful eval example must distinguish between good and bad model outputs. If both a strong model (GPT-5, Claude Opus 4.6) and a weak model (a much smaller or untuned model) produce outputs that score equally well against the synthetic test case, the test case is not testing anything meaningful. Run each candidate synthetic example against two models of different quality levels. Keep only the examples where the strong model scores significantly better than the weak model. These are the examples that actually differentiate — the ones that make your eval suite a real quality signal rather than a rubber stamp.

The second quality gate is **diversity verification**. After generating a batch of synthetic examples, embed them alongside your existing eval dataset. Check that the synthetic examples occupy regions of the embedding space that the existing dataset doesn't cover. If the synthetic examples cluster on top of existing examples, they're adding volume but not coverage. Retire the duplicates and generate examples specifically targeting the uncovered regions.

The third quality gate is **difficulty calibration**. A useful eval suite contains a range of difficulties: easy cases that even weak models handle, medium cases that separate adequate from good models, and hard cases that only the strongest models get right. If all your synthetic examples are medium difficulty — which is the default output of most generation prompts, because LLMs tend to generate "typical" examples — your eval suite has poor resolution at both ends of the quality spectrum. You miss the floor (can the model handle basic cases?) and the ceiling (how does the model perform on the hardest realistic scenarios?). Explicitly prompt for difficulty levels during generation, and verify the difficulty distribution of the result.

The fourth quality gate is **factual correctness verification**. The generating model can produce reference responses that are confident, well-structured, and wrong. This is especially dangerous in domains where the generating model lacks specialized knowledge — medical, legal, financial, highly technical. For these domains, every synthetic reference response must be reviewed by a domain expert, regardless of how convincing it reads. The cost of expert review is high. The cost of evaluating your model against wrong answers is higher.

## Practical Throughput and Team Structure

A realistic synthetic data pipeline for eval coverage involves three roles: a generation engineer who designs prompts and manages the generation pipeline, domain reviewers who validate generated examples, and an eval engineer who integrates validated examples into the dataset hierarchy.

The generation engineer spends most of their time on prompt engineering — not writing individual test cases but designing generation prompts that produce diverse, well-calibrated, domain-appropriate examples. A good generation prompt is iterated over dozens of versions. The engineer generates a batch, analyzes the results for diversity, difficulty distribution, and correctness, adjusts the prompt, and generates again. This iteration cycle takes one to two weeks per major domain area, but once the prompt is dialed in, it can produce high-quality candidates at volume.

Domain reviewers process the generated candidates. Depending on the domain complexity, a reviewer can validate thirty to eighty synthetic examples per hour. The reviewer is checking for scenario realism, reference response correctness, appropriate difficulty, and anything that feels off — the kind of judgment that automated filters can't replicate. For a golden-tier integration, every example gets reviewed. For a silver-tier integration, a statistically significant sample gets reviewed and the rest are accepted based on automated filters.

At these rates, a team of one generation engineer and two part-time domain reviewers can produce three hundred to six hundred validated synthetic eval examples per week. Over a quarter, that's four thousand to eight thousand new eval examples — enough to meaningfully expand coverage across dozens of new use cases. The key bottleneck is not generation speed but review throughput. Teams that try to skip review or automate it entirely end up with synthetic datasets that look impressive by volume but contribute noise rather than signal.

## When Synthetic Data Fails

Synthetic eval data is not a universal solution. There are domains and quality dimensions where it fails, and recognizing those boundaries prevents you from investing in a pipeline that can't deliver.

The first failure domain is **highly specialized expertise**. If the generating model doesn't have deep knowledge in the domain, it produces test cases that are superficially plausible but miss the subtleties that matter. A synthetic test case for radiology diagnosis review might describe a scenario that no actual radiologist would encounter, or reference an imaging finding that doesn't match the described condition. Domain experts spot these instantly, but if your pipeline is designed to minimize expert involvement, the flawed examples slip through.

The second failure domain is **subjective quality dimensions**. For quality dimensions like tone, empathy, creativity, and cultural appropriateness, there is often no single correct answer — reasonable reviewers disagree on what "good" looks like. Synthetic generation amplifies one model's opinion about what "good" means and bakes it into the eval data. If the generating model has a different sense of appropriate tone than your users, every synthetic eval example reinforces the wrong standard.

The third failure domain is **adversarial test cases**. Synthetic adversarial examples — jailbreak attempts, prompt injections, social engineering vectors — tend to have tells that make them easy for models to detect. An LLM generating an adversarial prompt produces something that looks adversarial to a human but follows patterns that another LLM can easily recognize and reject. Real adversarial attacks are crafted by humans who understand model internals, exploit unexpected behaviors, and iterate until the attack works. Synthetic adversarial examples often test whether the model can detect obvious attacks, not whether it can withstand sophisticated ones.

The fourth failure domain is **temporal specificity**. Synthetic examples are generated based on the generating model's knowledge, which has a training cutoff. If you need eval examples about events, products, or regulations that post-date the model's training data, the generated examples will either be wrong or will be fabricated — the model will produce confident, specific test cases about things that don't exist. For temporally sensitive domains, always verify synthetic examples against current real-world conditions.

## Integrating Synthetic Data into the Dataset Hierarchy

Synthetic examples don't just get dumped into a folder. They flow into the multi-tier dataset structure covered in subchapter 6.2, with the integration path depending on the quality tier.

For golden-tier integration, every synthetic example goes through full human review and is held to the same quality standard as hand-crafted examples. The synthetic origin is metadata — it's tracked for analysis purposes, but once reviewed and approved, the example is treated identically to a human-crafted one. Golden-tier integration is for filling specific identified gaps: "we have no golden examples for multilingual code-switching" or "our golden set doesn't cover the new pricing plan."

For silver-tier integration, synthetic examples go through automated quality filtering and sample-based human review. The automated filters check format, diversity, and discriminative power. The human review sample validates that the filters are working — if the sample shows problems, the entire batch is rejected and the generation prompt is revised. Silver-tier integration is for broad coverage expansion: "we need test cases across two hundred product categories" or "we need stress tests for every supported language."

Regardless of tier, synthetic examples get a generation metadata stamp: which model generated them, which prompt version was used, which quality gates they passed, and when they were created. This metadata lets you analyze the contribution of synthetic data to your overall eval signal. If you find that your eval suite's quality scores are driven primarily by synthetic examples rather than human-curated ones, that's a sign that the synthetic data is dominating the signal — which may or may not be what you want, but you need to know.

Synthetic data fills coverage gaps for the scenarios your experts haven't had time to hand-craft and for the standard quality dimensions where a correct answer can be objectively defined. But there's one category of eval data that synthetic generation can't handle on its own — the adversarial dataset. Attack libraries that test your model's defenses require a fundamentally different approach to creation, maintenance, and evolution. The next subchapter covers how adversarial datasets work and why they demand their own lifecycle.