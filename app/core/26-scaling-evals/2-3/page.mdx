# 2.3 — Stratified Sampling: Ensuring Coverage Across Use Cases, User Segments, and Edge Cases

In early 2025, a workforce management company rolled out an AI assistant for employee questions — policy lookup, benefits inquiries, PTO calculations, and onboarding guidance. The product served eighty thousand employees across four countries. The evaluation team had implemented random sampling six months earlier and was seeing aggregate quality scores between eighty-eight and ninety-one percent, stable week over week. Every dashboard was green. Every report to leadership was positive.

Then the company's largest client, a German automotive manufacturer representing three percent of total traffic but forty percent of annual contract revenue, escalated. Their HR director reported that employees were receiving incorrect information about German parental leave entitlements — the AI was citing EU-wide defaults instead of Germany-specific regulations that provided significantly more generous benefits. Employees were making family planning decisions based on wrong information. The client threatened contract termination.

When the evaluation team investigated, the root cause was devastating in its simplicity. German-language policy queries were three percent of total traffic. The one-percent random sample included roughly twenty-four German policy evaluations per day. Of those twenty-four, perhaps two or three touched parental leave. The evaluation rubric was catching some of these as incorrect, but two or three failures per day in a dataset of eight hundred total daily evaluations didn't register as a pattern on any dashboard. The aggregate quality number was right. The segment-level quality for the company's most valuable customer was catastrophically wrong. And the random sampling strategy, by design, had no mechanism to surface it.

The company kept its client — barely — after a $280,000 emergency remediation effort. What it actually needed was a sampling strategy that guaranteed adequate evaluation coverage for every segment that mattered, regardless of that segment's share of total traffic. That strategy is **stratified sampling**.

## What Stratified Sampling Actually Does

Stratified sampling divides your production traffic into meaningful groups — called **strata** — and then samples from each stratum independently. Instead of one big random sample from all traffic, you draw many smaller random samples, one per stratum. Each stratum gets its own sampling rate, its own sample size, and its own quality metrics.

The conceptual shift is subtle but powerful. Random sampling treats your traffic as a single homogeneous population. Stratified sampling treats it as a collection of distinct sub-populations, each of which may have different quality characteristics, different risk profiles, and different business importance. By sampling each sub-population independently, you guarantee that every group receives enough evaluation to produce meaningful quality metrics — even groups that represent a tiny fraction of total traffic.

Think of it this way. Random sampling answers the question "what is the overall quality of our system?" Stratified sampling answers the question "what is the quality of our system for each group of users, each type of query, and each use case?" The first question is useful for executive dashboards. The second question is useful for actually finding and fixing problems.

The mathematical advantage is straightforward. In the workforce management example, German policy queries were three percent of traffic. A one-percent random sample of all traffic yields roughly 0.03 percent of German queries — a handful per day. But a stratified sample that allocates, say, five percent sampling to the German-language stratum yields five percent of German queries — roughly a hundred twenty evaluations per day. That's enough to detect a ten-percentage-point quality gap within a single day with high confidence. Same total evaluation budget, radically different visibility.

## Choosing Your Strata

The choice of stratification dimensions is the most consequential design decision in stratified sampling, and it requires product knowledge that no algorithm can provide. Strata should reflect the dimensions along which quality can vary independently. If quality can be excellent for English and terrible for German, then language is a stratification dimension. If quality can be excellent for basic questions and terrible for complex multi-step reasoning, then query complexity is a stratification dimension.

The most common and most useful stratification dimensions for AI products in 2026 fall into six categories.

The first is **use case or feature type**. A customer support bot handles returns, billing disputes, account changes, and general inquiries. Each feature exercises different model capabilities. Returns are template-heavy and low-risk. Billing disputes require accurate calculation and regulatory language. Quality can — and does — differ dramatically across features. Stratifying by feature ensures each one gets evaluated on its own terms.

The second is **user segment or tier**. Enterprise customers, small business customers, and free-tier users often interact with the same AI but through different contexts, with different expectations, and with different consequences when quality fails. Enterprise users may submit more complex queries, expect higher accuracy, and represent disproportionate revenue. Stratifying by user tier ensures that your most valuable customers' experience is measured separately from the overall average.

The third is **language or locale**. Multilingual AI products almost always exhibit quality variation across languages. English outputs benefit from the largest training data, the most eval benchmarks, and the most attention during development. Other languages — especially lower-resource ones — receive less development attention and exhibit lower quality. Random sampling underrepresents minority languages by definition. Stratification guarantees each language gets a meaningful sample.

The fourth is **risk level**. If your system classifies outputs into risk tiers — as described in the risk-based sampling approach covered in subchapter 2.4 — each risk tier is a natural stratum. High-risk outputs deserve more evaluation regardless of volume. Stratifying by risk tier and allocating higher sampling rates to high-risk strata achieves this without requiring a separate sampling mechanism.

The fifth is **model version or prompt template**. When multiple model versions or prompt templates are in production simultaneously — during A/B tests, canary deployments, or gradual rollouts — each version is a stratum. You want independent quality metrics for each version to make informed rollout decisions. Random sampling from the combined traffic may over-represent the majority version and under-represent the minority one.

The sixth is **input characteristics**. Query length, topic, complexity, the presence of specific keywords, or whether the query includes attached documents all affect model performance differently. A document analysis AI may handle short contracts well but struggle with hundred-page regulatory filings. Stratifying by input characteristics ensures that rare but important input types receive adequate evaluation.

## Setting Sampling Rates Per Stratum

Not every stratum deserves the same sampling rate. The power of stratified sampling comes precisely from the ability to allocate more evaluation to the strata that need it most. Three factors determine the sampling rate for each stratum: the stratum's business importance, its quality stability, and its volume.

High-importance, low-volume strata get the highest sampling rates. The German policy queries in the workforce management example were high-importance (forty percent of revenue) and low-volume (three percent of traffic). A sampling rate of ten to twenty percent in that stratum — ten to forty times the baseline random rate — ensures adequate coverage despite low volume. The absolute cost is modest because the volume is low.

High-importance, high-volume strata get moderate sampling rates. If your enterprise tier represents thirty percent of traffic and sixty percent of revenue, a sampling rate of three to five percent gives you thousands of evaluations per day. You don't need a high rate because the volume does the work — three percent of a large number is still a large number.

Low-importance, high-volume strata get the lowest sampling rates. General inquiries from free-tier users may represent fifty percent of traffic but five percent of revenue and minimal regulatory risk. A sampling rate of 0.5 to one percent is sufficient for aggregate monitoring. Failures in this stratum are less costly, and the high volume means even a low rate produces a statistically adequate sample.

Low-importance, low-volume strata are a judgment call. Evaluation resources are finite, and some strata may be too small and too low-impact to justify dedicated sampling. The key question is: "If quality in this stratum degraded to zero, what would the business impact be?" If the answer is negligible, you can rely on background random sampling to provide incidental coverage. If the answer is "we'd lose a customer" or "we'd face regulatory risk," the stratum deserves its own allocation.

## The Over-Sampling Principle

The most counterintuitive aspect of stratified sampling is that it requires you to intentionally over-sample rare strata relative to their traffic share. A stratum that represents one percent of traffic might receive five to fifty times its proportional share of evaluations. This feels like waste if you think of evaluation as auditing traffic. It makes perfect sense if you think of evaluation as managing risk.

The **over-sampling principle** states: rare segments that carry disproportionate business risk should be sampled at rates that guarantee a minimum number of evaluations per time period, regardless of volume. The minimum is determined by statistical requirements, not by proportionality. If you need five hundred evaluations per week to detect a five-point quality shift, and the stratum only generates a thousand outputs per week, you need a fifty percent sampling rate for that stratum. The fact that the stratum is tiny compared to total traffic is irrelevant. The statistical requirement is the constraint.

This principle creates sampling rates that look wildly disproportionate. A production system might sample free-tier English queries at 0.5 percent and enterprise German regulatory queries at eighty percent. The ratio between these rates — a hundred sixty to one — seems absurd until you realize that the first category is high-volume, low-risk, and well-understood, while the second is low-volume, high-risk, and poorly characterized. Over-sampling the second category costs very little in absolute terms (because the volume is low) and provides enormous value (because the risk is high). Under-sampling it costs very little in eval budget savings and creates enormous exposure.

## The Segment Coverage Guarantee

Over-sampling leads to a formalized concept that mature evaluation teams implement as a hard constraint: the **segment coverage guarantee**. This is the minimum number of evaluated examples per stratum per time period, specified as a non-negotiable requirement that the sampling system must meet regardless of traffic fluctuations.

A segment coverage guarantee might specify: "Every stratum must have at least three hundred evaluated examples per week." If a stratum generates sixty thousand outputs per week, a 0.5 percent sampling rate yields three hundred evaluations — guarantee met. If a different stratum generates five hundred outputs per week, meeting the guarantee requires a sixty percent sampling rate. If a third stratum generates only a hundred outputs per week, meeting the guarantee is mathematically impossible through sampling alone — and the guarantee tells you that this stratum needs one hundred percent evaluation, or that the time window needs to extend to two weeks.

The guarantee works as both a floor and a constraint. As a floor, it prevents any stratum from receiving so few evaluations that its quality metrics are meaningless. As a constraint, it forces the team to confront trade-offs when the total evaluation budget isn't large enough to meet guarantees for all strata simultaneously. When budget is tight, the guarantee framework forces a prioritization conversation: which strata can tolerate a lower guarantee, and which are non-negotiable? That conversation — which strata matter most — is precisely the conversation the team should be having.

Segment coverage guarantees also make evaluation health auditable. When a post-mortem reveals a quality failure that went undetected, the first question is: "Was the affected segment meeting its coverage guarantee?" If yes, the problem was in the evaluation criteria, not the sampling. If no, the sampling system failed to deliver adequate coverage, and the fix is a sampling adjustment. The guarantee turns an ambiguous "our evaluation didn't catch this" into a precise diagnosis.

## The Stratum Maintenance Problem

Stratified sampling is not a set-and-forget configuration. Strata evolve as your product evolves, and failing to maintain your stratification scheme creates the same blindness that stratified sampling was designed to prevent.

New strata emerge when your product adds new features, enters new markets, or serves new user segments. A product that launches in Japan creates a Japanese-language stratum that didn't exist before. If nobody adds it to the stratification scheme, Japanese outputs are evaluated only through background random sampling — exactly the under-coverage problem you built stratification to solve. The most disciplined teams tie stratum creation to product feature launches. Every new feature, market, or user segment comes with a stratum definition and a sampling rate allocation as part of the launch checklist.

Existing strata shift when traffic patterns change. A stratum that represented ten percent of traffic six months ago might represent two percent now because user behavior shifted. If the sampling rate hasn't been adjusted, the stratum is now producing far fewer evaluations than intended, and the coverage guarantee may be breached. Periodic audits — monthly at minimum, weekly for fast-changing products — compare actual evaluation volume per stratum against the coverage guarantee and flag any stratum that has fallen below its minimum.

Strata can also become obsolete. A prompt template that was retired three months ago may still be defined as a stratum in your sampling configuration, consuming evaluation budget on outputs that no longer exist. Or a user segment that was split into two sub-segments may still be treated as one, hiding quality differences within the combined stratum. Stale strata waste budget and mask problems — both of which are exactly what stratification is supposed to prevent.

The maintenance burden is real, and it's the primary reason some teams resist stratified sampling. The response to that resistance is not to skip stratification. It is to build the maintenance process into the team's operational cadence. Stratum review should be a monthly agenda item, not an annual project. The team that reviews its strata monthly spends fifteen minutes per review. The team that ignores strata for a year spends a week rebuilding after a quality incident it should have caught.

## Practical Challenges and Hard Trade-offs

Stratified sampling introduces complexity that random sampling avoids, and that complexity creates its own failure modes. The most common is **stratum explosion**: the tendency to create too many strata, dividing traffic so finely that each stratum is too small to produce meaningful evaluations. If you stratify by language (ten strata), by use case (eight strata), by user tier (three strata), and by risk level (four strata), you have nine hundred sixty possible combinations. Most of those combinations will have near-zero traffic. Your sampling system will spend its budget evaluating hundreds of near-empty strata while the important strata are under-resourced.

The solution is hierarchical stratification: pick a primary dimension (use case or user tier) and create strata along that dimension. Then, within the highest-priority strata, add a secondary dimension (language or risk level). Don't try to cover every possible combination. Cover the combinations that matter — the ones where quality failure has business consequences.

Another challenge is the cold start problem. When a new stratum is created — a new feature launches, a new market opens — there's no historical data to inform the sampling rate. How do you set the initial rate? The conservative approach is to start high — twenty to thirty percent — and reduce over time as quality data accumulates and quality proves stable. A new stratum is, by definition, uncharacterized. Over-evaluation during the characterization period is cheap insurance against the discovery, months later, that a new feature has been producing low-quality outputs since launch with nobody noticing.

The deepest challenge, and the one that separates good implementations from great ones, is aligning strata with business categories rather than technical categories. It's tempting to stratify by API endpoint, prompt template, or model version, because those are the dimensions your engineering system knows about. But business-meaningful strata often cut across technical boundaries. "Enterprise healthcare customers asking compliance questions" might involve three different API endpoints, two prompt templates, and outputs from two different model versions. The stratum that matters is defined by the business context, not the technical pathway. Building strata that match business categories requires product team input, not just engineering instrumentation.

## The Compound Effect of Good Stratification

When stratified sampling is well-designed and well-maintained, it produces a compound benefit that goes beyond detection. It creates a quality map of your product — a segment-by-segment picture of where your AI works well, where it struggles, and how quality is changing over time in each area.

This map becomes a product strategy input. If German-language quality is consistently fifteen points below English, that's not just an evaluation finding — it's a product decision about whether to invest in German language improvement, reduce German-market commitments, or route German queries through a different model. If enterprise-tier quality is two points higher than free-tier quality, that's evidence that your prompts are well-tuned for enterprise use cases but may need work on the consumer side. If a newly launched feature is consistently eight points below mature features, that's expected — but the trend line matters. Is it improving week over week as the team iterates on prompts? Or is it stuck, suggesting a deeper capability gap?

None of these insights are possible with random sampling alone. Random sampling tells you one number: overall quality. Stratified sampling tells you a matrix of numbers that corresponds to how your business actually works, how your users actually experience the product, and where your engineering effort will have the highest impact. The team that operates from a segment-level quality map makes better decisions than the team that operates from a single aggregate metric, in the same way that a doctor who reads a full blood panel makes better decisions than a doctor who checks only temperature.

Stratified sampling ensures your evaluation covers every corner of your product. But breadth alone is not enough. Some outputs are more dangerous than others, and a wrong answer on a high-stakes query costs exponentially more than a wrong answer on a routine one. The next subchapter introduces risk-based sampling — the practice of concentrating evaluation depth not on where traffic is highest, but on where failure costs most.
