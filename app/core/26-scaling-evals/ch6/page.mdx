# Chapter 6 — Dataset Management and Benchmark Integrity

Your evaluation is only as trustworthy as the data it runs against. At scale, eval datasets become living systems that must be versioned, refreshed, audited, and protected from contamination. A golden set that was accurate six months ago may be misleading today. A benchmark that was clean at launch may have leaked into training data. This chapter covers the infrastructure and discipline required to keep your eval data trustworthy as everything around it changes.

---

- **6.1** — The Golden Dataset as a Living System, Not a Static File
- **6.2** — Golden, Silver, and Super-Golden: The Multi-Tier Dataset Strategy
- **6.3** — Dataset Versioning at Scale: Why It Differs Fundamentally from Code Versioning
- **6.4** — Staleness and Refresh Cadence: When Your Ground Truth Stops Being True
- **6.5** — Synthetic Data for Scaling Eval Coverage: LLM-Generated Test Cases That Actually Work
- **6.6** — Adversarial Dataset Maintenance: Attack Libraries That Evolve with Threats
- **6.7** — Dataset Quality Assurance: Auditing the Data That Judges Your System
- **6.8** — Red-Team Integration: How Adversarial Findings Flow Into Eval Datasets and Sampling
- **6.9** — Benchmark Contamination: How Eval Data Leaks Into Training Data and Destroys Your Signal
- **6.10** — Living Benchmarks and Eval Freshness: Staying Ahead of Contamination at Scale

---

*Automated evaluation handles volume. But some outputs still need human eyes — and at scale, the way you deploy human reviewers changes fundamentally. The next chapter covers the collapse and reinvention of human-in-the-loop evaluation.*
