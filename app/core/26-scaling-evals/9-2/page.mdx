# 9.2 — Eval System Health Metrics: Pass Rates, Latency, Coverage, and Cost Per Eval

Four metrics define whether your eval system is healthy: pass rate distribution, eval latency, coverage rate, and cost per evaluation. Every other eval observability signal derives from or decomposes into these four. If you monitor nothing else about your eval infrastructure, monitor these. They tell you whether evaluations are producing stable results, finishing fast enough to be useful, covering the traffic they should be covering, and staying within the budget that makes the whole system sustainable.

## Pass Rate Distribution: Stability Over Absolute Value

**Pass rate distribution** measures how evaluation scores are distributed across your scoring dimensions and how that distribution changes over time. The absolute pass rate — what percentage of outputs score above threshold — matters, but the distribution's stability matters more for eval health monitoring.

A healthy pass rate distribution is stable over periods where no intentional changes have been made to the model, prompts, or eval criteria. If your relevance judge gives 87% of outputs a passing score this week and 86% next week, that is normal variance. If it gives 87% this week and 72% next week with no system changes, something is wrong — either with your AI system or with your eval system, and you need to determine which.

The distinction between "AI got worse" and "eval system broke" is the central diagnostic challenge of pass rate monitoring. A sudden drop can mean the model degraded, or it can mean the judge model was updated by its provider, or the eval prompt was inadvertently modified, or a data pipeline change altered the format of inputs to the judge. A sudden improvement can mean the model got better, or it can mean the judge became more lenient, or a coverage gap means you are now evaluating only the easiest cases.

The diagnostic protocol is to check eval system health metrics first, AI system metrics second. When pass rates shift, start by asking: did coverage change? Did judge latency change? Did the judge model version change? Did the eval pipeline process the same volume as the previous period? Only after ruling out eval system causes should you investigate the AI system itself.

## Tracking Pass Rate Anomalies

Two types of anomalies require different responses. **Step changes** are sudden shifts — a metric moves from one stable level to another between two consecutive measurement periods. Step changes typically have a single identifiable cause: a deployment, a configuration change, a judge model update. They are relatively easy to diagnose because you can correlate the timing of the shift with the timing of known changes.

**Trend changes** are gradual movements — a metric drifts one or two percentage points per week over six weeks. Trend changes are harder to diagnose because no single event caused them. They typically result from compounding factors: slow distribution shift in production traffic, gradual judge drift, incremental changes in user behavior that push more edge cases to the model. By the time a trend change crosses an alert threshold, the root cause may be weeks old.

For step changes, set thresholds at two to three standard deviations from the trailing mean. For trend changes, use linear regression over a rolling window and alert when the slope exceeds a predefined rate. A relevance pass rate declining 0.5 points per week may not trigger a step-change alert on any given week, but the trend alert catches the steady erosion before it compounds into a serious problem.

## Eval Latency: The Freshness Problem

**Eval latency** measures the elapsed time from when an AI system produces an output to when that output has a completed evaluation. This metric is not about computational speed — it is about decision-making freshness. An eval result that arrives six hours after the output was generated is less useful than one that arrives in ninety seconds, because the decisions the eval result should inform may have already been made.

Healthy eval latency depends on your operational cadence. If you deploy weekly and review eval trends before each deployment, eval latency under twenty-four hours is probably acceptable. If you run continuous deployment with automated gates, you need eval latency under five minutes or the gate becomes a bottleneck. If you operate safety-critical systems where flagged outputs require immediate human review, eval latency needs to be under sixty seconds.

Three components make up total eval latency: queue wait time, judge execution time, and result processing time. Queue wait time is how long an output sits in the evaluation queue before a judge picks it up — this is a capacity problem. Judge execution time is how long the judge takes to produce a score — this is a compute and model problem. Result processing time is how long it takes to aggregate, store, and surface the score — this is a pipeline engineering problem. When eval latency spikes, decomposing it into these three components tells you where to focus.

The dangerous pattern is creeping queue wait time. When eval volume grows gradually — a few percent more outputs each week as traffic increases — queue wait time grows with it. Each week the increase is small enough to ignore. After two months, eval results that used to arrive in four minutes now arrive in forty-five, and the team has not noticed because the degradation was never sharp enough to trigger an alert. Time-based trend monitoring on queue wait time catches this drift before it makes your evaluations operationally irrelevant.

## Coverage Rate: The Silent Dropout

**Coverage rate** measures the percentage of eligible AI outputs that actually receive an evaluation. If your system generates ten thousand outputs per day and your eval pipeline should evaluate all of them, but only seven thousand receive eval scores, your coverage rate is 70%. The missing 30% is invisible in your eval results — your dashboard reflects the quality of the outputs that were evaluated, not the outputs that were skipped.

Coverage gaps are almost always non-random. The outputs that get dropped tend to share characteristics — they are the longest outputs that cause judge timeouts, the outputs from the highest-traffic period when the eval queue was saturated, the outputs that triggered parsing errors because they contained unexpected formatting. Each of these patterns means the dropped outputs are systematically different from the evaluated ones. Your eval results describe a biased sample of your actual production traffic.

Healthy coverage rate depends on your eval design. If you sample 10% of production traffic for evaluation, your coverage of the sampled set should be at or above 99% — dropped items from an already-small sample create severe selection bias. If you evaluate all production traffic, coverage above 95% is the minimum viable target. Below 90%, your eval results should carry a visible warning in any dashboard or report, because the sample is no longer representative enough to trust.

Monitor coverage rate broken down by output characteristics. Overall coverage of 96% is healthy. But if that 96% breaks down as 99% coverage for short outputs and 78% coverage for long outputs, you have a systematic blind spot in exactly the category where quality issues are most likely to occur. Segment-level coverage monitoring catches biases that aggregate metrics hide.

## Cost Per Eval: The Sustainability Metric

**Cost per evaluation** measures the total cost of evaluating one output — including judge model inference costs, compute infrastructure, data storage, and the amortized cost of building and maintaining the eval pipeline. This is the metric that determines whether your eval system is financially sustainable as you scale.

In 2026, the dominant cost component for most eval systems is judge model inference. If you use Claude Opus 4.6 or GPT-5 as a judge for every evaluation, and each evaluation consumes several thousand tokens for the prompt and rubric, the per-eval cost adds up fast. At ten thousand evaluations per day, even modest per-call costs compound into significant monthly spend. At one hundred thousand evaluations per day, judge model costs can rival the cost of the production AI system being evaluated.

Healthy cost per eval ranges vary enormously by judge strategy. Systems using lightweight embedding-based classifiers might spend fractions of a cent per evaluation. Systems using frontier LLM judges might spend five to fifteen cents per evaluation. Systems using multi-judge ensembles with frontier models can easily exceed fifty cents per evaluation. The right number depends on the value of the evaluation signal — if a single missed quality issue costs your business ten thousand dollars, spending fifty cents per eval is a bargain. If the evaluation is routine quality sampling, a lower-cost approach is likely appropriate.

Three cost trends should trigger investigation. First, cost per eval increasing over time with no change in eval design — this usually means judge prompts have grown longer, judge outputs have become more verbose, or a provider has increased pricing. Second, total eval cost growing faster than production traffic — this means either coverage expanded (which is good) or per-eval costs crept up (which needs diagnosis). Third, cost per eval varying dramatically across evaluation dimensions — if your safety eval costs eight times more than your relevance eval, you may need a different judging strategy for the expensive dimension.

## How the Four Metrics Connect

These four metrics are interdependent, and understanding their connections prevents you from optimizing one at the expense of another. Reducing eval latency by adding more judge instances increases cost per eval. Increasing coverage rate by evaluating more outputs increases total cost and may increase latency if capacity doesn't scale proportionally. Improving pass rate stability by using more sophisticated judges increases per-eval cost and latency.

The most dangerous trade-off is the one teams make unconsciously: letting coverage rate drop to control costs. When the eval budget is fixed and traffic grows, the pipeline naturally drops the overflow. Coverage silently decreases while cost per eval remains stable and total spend stays within budget. The team sees flat cost metrics and assumes the system is healthy. In reality, they are evaluating a shrinking fraction of their traffic, and their eval results are becoming less representative every week.

A healthy eval system maintains explicit targets for all four metrics simultaneously. When any one metric drifts outside its target range, the team makes a deliberate decision about which trade-off to accept — not an unconscious one forced by capacity constraints.

The next subchapter addresses what happens when you monitor not just four aggregate metrics but dozens or hundreds of evaluation dimensions — and how drift across many dimensions creates problems that no single-dimension monitor can detect.
