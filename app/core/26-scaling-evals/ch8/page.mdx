# Chapter 8 — Outcome-Level Evaluation: From Output Quality to Business Impact

A response can be fluent, accurate, well-formatted, and completely useless. Output-level evaluation measures whether the AI said the right thing. Outcome-level evaluation measures whether the right thing actually worked — whether the user completed the task, whether the recommendation converted, whether the agent achieved its goal. This chapter covers the techniques for connecting AI outputs to business results, including delayed measurement, causal inference, compound system evaluation, and the feedback loops that turn business metrics into evaluation criteria.

---

- **8.1** — Why Output Quality Is Not Enough: The Gap Between Good Responses and Good Outcomes
- **8.2** — Outcome Attribution: Connecting AI Outputs to Business KPIs
- **8.3** — Delayed Outcome Evaluation: Measuring Impact Days and Weeks After the Response
- **8.4** — Causal Inference vs Correlation in AI Evaluation
- **8.5** — Evaluating Compound AI Systems: Trace-Level Analysis and Component-vs-System Quality
- **8.6** — Agentic Reliability Metrics: pass at k, Trajectory Analysis, and Multi-Step Reliability
- **8.7** — Long-Tail Outcome Measurement: Rare Events That Determine System Trust
- **8.8** — The Outcome Feedback Loop: From Business Metrics Back to Eval Criteria

---

*Outcome evaluation measures what matters. But who measures the evaluation system itself? The next chapter turns the lens inward — monitoring the health, reliability, and trustworthiness of the systems that monitor your AI.*
