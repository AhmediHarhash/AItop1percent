# 4.4 — Interleaving Experiments: Comparing Models Within a Single User Session

A search company ran a standard A/B test comparing two ranking models in early 2025. After two weeks and two million queries, the results showed no statistically significant difference. The product team was ready to declare the models equivalent and kill the project when a senior engineer suggested running an interleaving experiment instead. Within three days and two hundred thousand queries, the interleaving results clearly showed that Model B produced better rankings — with an effect size that the A/B test would have taken another four weeks to detect. The difference between "no signal" and "clear winner" wasn't in the models. It was in the measurement method. The A/B test drowned in between-user variance. The interleaving experiment eliminated it.

This is the core advantage of interleaving, and it matters enormously for teams that need to compare models quickly without burning weeks of traffic on inconclusive experiments. When your iteration cycle is measured in days, not quarters, you cannot afford a measurement technique that needs millions of samples to detect real quality differences.

## What Interleaving Actually Is

In a traditional A/B test, you split your users into two groups. Group A sees outputs from Model A. Group B sees outputs from Model B. You measure an outcome metric for each group and compare. The problem is that users in Group A and Group B are different people with different preferences, different query patterns, different tolerance for quality variation, and different baseline engagement. These differences between users introduce enormous noise into your comparison. Most of the variance in your outcome metric comes from who the users are, not which model served them.

**Interleaving** eliminates this problem by showing each user results from both models within the same session. Instead of asking "do users who see Model A behave differently from users who see Model B?" you ask "when a single user sees outputs from both models side by side, which model's outputs does the user prefer?" The comparison happens within the same person, at the same moment, under the same conditions. Between-user variance — the dominant source of noise in A/B tests — drops to zero.

The technique originated in information retrieval research and was popularized by Netflix, Airbnb, and major search engines that needed to compare ranking algorithms at scale. Netflix pioneered a two-stage experiment pipeline: interleaving as a fast first stage to prune unpromising candidates, followed by traditional A/B testing on only the survivors to measure longer-term behavioral impact. Airbnb published research in 2025 showing that their interleaving framework achieves up to fifty times the sensitivity of A/B testing — meaning they need fifty times fewer queries to detect the same effect size. These are not marginal improvements. They are order-of-magnitude reductions in experiment duration.

## How Interleaving Works in Practice

The most widely used interleaving method is **team-draft interleaving**, and it works like a schoolyard team selection. Imagine two captains — Model A and Model B — each with a ranked list of outputs they want to show the user. The algorithm alternates turns. On each turn, the model whose team is currently smaller picks its highest-ranked output that hasn't been selected yet. If the teams are tied, the model that goes first is chosen randomly. The result is a single interleaved list where outputs from both models are mixed together, with roughly equal representation.

The user sees this blended list and interacts with it naturally — clicking, copying, expanding, following up. The user doesn't know which outputs came from which model. They just interact with whatever looks best to them. After the session, the system counts how many of the user's interactions fell on outputs from Model A versus Model B. If the user consistently engaged with Model B's outputs more than Model A's, Model B wins that session.

Across thousands of sessions, the aggregate preference signal tells you which model produces outputs that users actually prefer. Because every session contains outputs from both models, the comparison is perfectly controlled for user-level differences. A user who engages heavily contributes more signal, but their heavy engagement benefits both models equally. A user with unusual query patterns still compares both models on those same unusual queries. The within-user comparison neutralizes every source of between-user noise that plagues A/B tests.

## The Sensitivity Advantage: Why It Needs So Much Less Data

The power of interleaving comes from a statistical principle that is easy to state and hard to overstate: paired comparisons are dramatically more sensitive than unpaired comparisons. When you compare two groups of different users, you need enough data to separate the real signal (the model difference) from the noise (the user differences). When you compare two models within the same user, the noise floor drops because user differences cancel out.

Consider a concrete example. Suppose Model B is genuinely three percent better than Model A at producing relevant search results. In a standard A/B test, this three-percent difference is swimming in a sea of between-user variance — some users have easy queries, some have hard ones, some click on everything, some click on nothing. Detecting a three-percent signal in that noise might require a million users per group and three weeks of traffic. In an interleaving experiment, the three-percent difference shows up within each session. A user who would have clicked on ten results now clicks on slightly more of Model B's results. Multiply across thousands of users and the signal emerges quickly. The same three-percent difference that requires millions of samples in an A/B test can be detected in tens of thousands of interleaved sessions — a reduction of one to two orders of magnitude.

This is why Netflix uses interleaving as a fast pruning step. They can test dozens of ranking algorithm candidates in parallel using interleaving, identify the top two or three within days, and then commit their A/B testing traffic budget only to those survivors. Without interleaving, testing twenty candidates through sequential A/B tests would take months. With interleaving as a first pass, it takes weeks.

## When Interleaving Works for AI Systems

Interleaving works anywhere users see multiple outputs and make implicit or explicit choices among them. The classic use cases are search results and recommendation lists, where the output is inherently a ranked set and the user's click behavior provides a natural preference signal. But the technique extends to any AI system that presents options.

Autocomplete and suggestion systems are natural candidates. When your AI offers three to five code completions, email suggestions, or search query refinements, you can source some from Model A and some from Model B, randomize the ordering, and measure which model's suggestions the user accepts. This is interleaving in its purest form — multiple outputs, single user, implicit preference.

Multi-document summarization systems work too. If your system generates a summary with five key points, you can source some points from Model A and some from Model B, present them as a unified summary, and track which points the user highlights, copies, or asks follow-up questions about. The attribution is trickier here because the points interact with each other — a strong point from Model A might make a mediocre point from Model B look worse by contrast — but the aggregate signal across many sessions still reveals which model produces more useful content.

Any system that generates a list, a set of options, or a multi-part output can potentially use interleaving. The requirement is that the output format naturally supports blending results from two sources without the blend being obvious to the user.

## When Interleaving Does Not Work

Interleaving has a fundamental limitation: it requires outputs that can be mixed. Single-response systems — a chatbot that produces one answer to a question, a content generator that writes one article — cannot interleave because there is nothing to blend. You cannot show the user half of Model A's answer and half of Model B's answer. The output is atomic, and the user experiences exactly one version of it.

This rules out the most common AI interaction pattern: a user asks a question and gets a single response. For these systems, A/B testing, shadow deployments, and progressive rollouts are the appropriate comparison methods. Interleaving simply doesn't apply.

Interleaving also fails when mixing outputs from different models would confuse or degrade the user experience. Consider a customer support chatbot that maintains a consistent tone throughout a conversation. If some responses come from Model A — which tends toward formal language — and others from Model B — which tends toward casual language — the user experiences jarring tone shifts within a single conversation. The models are not being compared on a level playing field because the inconsistency itself degrades the experience. The user isn't choosing between two models; they're reacting to the disorientation of talking to two different personalities.

Similarly, interleaving breaks down when output order matters beyond relevance. In a ranked list, the user expects the best results near the top. Interleaving preserves this roughly — the team-draft method alternates between models' top-ranked items — but in scenarios where strict ordering is critical, the interleaving itself introduces noise that can contaminate the preference signal.

The honest assessment is that interleaving is a powerful but narrow tool. It works spectacularly well for ranking, recommendation, and multi-option systems. It doesn't work at all for single-output systems. Know which kind of system you have before investing in interleaving infrastructure.

## Position Bias and How to Handle It

Even in systems where interleaving is a natural fit, there is a persistent confound: **position bias**. Users tend to interact more with items at the top of a list regardless of quality. The first search result gets more clicks than the third, even if the third is more relevant. The first suggestion in an autocomplete dropdown gets accepted more often than the fourth. This is not a preference for the model that produced the top item — it's a preference for the top position.

Position bias can corrupt interleaving results if it's not controlled. If Model A's outputs happen to land in higher positions more often than Model B's — which can happen depending on the interleaving algorithm and the models' original rankings — the preference signal will be biased toward Model A even if Model B's outputs are genuinely better.

The team-draft interleaving method partially addresses this by alternating which model picks first, ensuring roughly equal position distribution across sessions. But "roughly equal" is not "exactly equal," and for small effect sizes, the residual position bias can still distort results.

More sophisticated interleaving methods address position bias directly. **Optimized interleaving** assigns outputs to positions in a way that explicitly equalizes each model's expected position distribution, not just approximately. **Probabilistic interleaving** introduces randomization that allows unbiased estimation of model quality even when position effects are strong. These methods are more complex to implement but produce cleaner preference estimates, which matters when you're trying to detect small quality differences between two strong models.

The practical recommendation is to start with team-draft interleaving — it's simple to implement and sufficient for detecting moderate to large quality differences. Move to optimized or probabilistic interleaving when you need to detect smaller differences or when your user interface has particularly strong position effects, such as a mobile screen where only the first one or two results are visible without scrolling.

## Attribution: Knowing Which Model Won

After an interleaving session, you need to determine which model's outputs the user preferred. This is the attribution problem, and it's more nuanced than it appears.

The simplest attribution method is **click credit**. Each time a user clicks on an interleaved result, the model that produced that result gets one credit. At the end of the session, the model with more credits wins. Across thousands of sessions, the model that wins more sessions is the better model. This is clean, intuitive, and works well when clicks are the primary user signal.

But clicks are not the only signal, and in some AI systems they're not the best one. A user might read a search result without clicking it — the snippet was enough. A user might click a result and immediately bounce back — the content didn't match the snippet. A user might copy an autocomplete suggestion without clicking anything. A user might dwell on one result for thirty seconds and skim another in two seconds. Each of these behaviors carries preference information that pure click credit misses.

Richer attribution methods weight interactions by their strength. A click is a weak preference signal. A click followed by thirty seconds of dwell time is a strong one. A click followed by a two-second bounce is a negative signal — the user was deceived by the snippet. A copy action is a strong positive signal. A follow-up query that refines the user's search is a negative signal for the results they just saw. Weighted attribution assigns different credit values to different interaction types, producing a more accurate preference estimate.

The challenge is calibrating these weights. What's the right ratio between a click and a copy? How much negative credit does a bounce deserve? These weights are empirical — you determine them by analyzing historical interaction data and measuring which weighting scheme best predicts external quality measures. There is no universal answer. A search engine's optimal weights differ from a code completion system's optimal weights. You need to calibrate attribution for your specific product and user population.

## The Two-Stage Experiment Pipeline

The most powerful application of interleaving is not as a replacement for A/B testing but as its accelerator. The two-stage pipeline, refined by Netflix and adopted by Airbnb and other major platforms, uses interleaving to compress the experimentation cycle from months to weeks.

Stage one: interleaving. Run interleaving experiments on a small fraction of traffic — as little as one to five percent — with many candidate models simultaneously. Because interleaving is so sensitive, you can test five to ten candidates in parallel, each getting a fraction of the interleaving traffic, and identify the top performers within days. This stage answers the question "which candidates are worth further investigation?" It doesn't answer "what will the business impact be?" because interleaving measures preference, not downstream behavior.

Stage two: A/B testing. Take the one to three winners from interleaving and run traditional A/B tests with larger traffic allocations and longer durations. This stage measures what interleaving cannot: long-term user engagement, retention, conversion, revenue impact, and other business metrics that require weeks of observation. The A/B test answers the question "what happens to the business if we deploy this model?"

The efficiency gain is dramatic. Without interleaving, testing ten candidates through A/B tests requires either running them sequentially — ten experiments times three weeks each equals thirty weeks — or splitting traffic into ten thin slices that lack statistical power. With the two-stage pipeline, interleaving prunes ten candidates to two in one week, and the A/B test validates the finalists in three weeks. Total time: four weeks instead of thirty. Total traffic consumed: a fraction of the sequential approach.

For AI teams iterating on models, prompts, or retrieval pipelines, this acceleration changes what's possible. You can test bold, speculative changes alongside conservative incremental ones, because the cost of testing a candidate that turns out to be bad is measured in days of interleaving traffic, not weeks of A/B testing traffic. The two-stage pipeline makes experimentation cheap enough to be ambitious.

## Implementing Interleaving at Scale

Building an interleaving system requires four components that must work together cleanly.

First, you need a blending layer that takes ranked outputs from two models and produces a single interleaved list. This layer implements the interleaving algorithm — team-draft, optimized, or probabilistic — and tags each output in the blended list with its source model. The tags must be invisible to the user but available to the analytics pipeline. The blending layer must be fast, adding no more than a few milliseconds to response latency, because it sits in the critical path between model inference and user display.

Second, you need an interaction tracker that records every user interaction with the interleaved results and attributes it to the source model. Clicks, dwells, copies, bounces, conversions — every signal that your attribution method uses must be captured with the model tag attached. This tracker must be reliable. A missed interaction is a lost preference signal. At scale, even a one-percent loss rate degrades the quality of your preference estimates.

Third, you need an analysis engine that aggregates preference signals across sessions, computes win rates for each model, and applies statistical tests to determine when the results are conclusive. The standard test for interleaving is a paired sign test or a paired t-test, depending on whether you're working with binary win-loss outcomes or continuous preference scores. The analysis engine should report confidence intervals and estimated sample size to conclusion, so the team knows both the current result and how much longer the experiment needs to run.

Fourth, you need an experiment management layer that configures which models are being compared, what percentage of traffic is routed to interleaving, and when to start and stop experiments. This layer should support running multiple interleaving experiments simultaneously — comparing Model A versus Model B on one traffic slice and Model C versus Model D on another — without interference.

The implementation complexity is moderate. Teams with existing A/B testing infrastructure can often build an interleaving system by extending their existing experimentation framework. The blending layer is the only truly new component. The interaction tracker, analysis engine, and experiment management layer are variations on infrastructure that most experimentation platforms already provide.

## What Interleaving Cannot Tell You

Interleaving is extraordinarily good at answering one question: which model produces outputs that users prefer? But it cannot answer several questions that matter for deployment decisions.

It cannot tell you about long-term effects. A model that produces slightly more engaging results might also produce results that are less informative, leading to more repeat queries and lower long-term satisfaction. Interleaving measures immediate preference, not sustained impact. This is why the second stage of the pipeline — A/B testing for longer-term metrics — remains essential.

It cannot tell you about safety. A model that users prefer might be preferred precisely because it's less cautious — it gives confident-sounding answers even when uncertain, it avoids the hedging language that makes careful responses less appealing. User preference and safety can be inversely correlated, which means interleaving preference can steer you toward models that are more engaging but less reliable. Safety evaluation must happen through a separate process, not through user preference signals.

It cannot tell you about cost. Users might prefer Model B's outputs, but if Model B costs four times as much to serve, the business case for switching may not hold. Interleaving measures quality preference in isolation from operational constraints. The deployment decision integrates quality preference with cost, latency, reliability, and other operational factors.

Interleaving gives you one powerful piece of information — relative quality preference under controlled conditions — and it gives it to you fast. Use it for what it's good at and don't ask it to replace the other evaluation methods that cover the dimensions it misses.

Interleaving compares models by letting users choose between blended outputs. But there are many scenarios — single-response systems, safety-critical applications, quality monitoring across all traffic — where you don't need users to choose. You need to score individual outputs automatically, in real time, as they flow through your production system. That's the domain of real-time eval scoring, and it transforms how you operate an AI system at scale.