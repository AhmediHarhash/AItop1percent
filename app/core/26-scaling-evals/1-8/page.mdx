# 1.8 — Eval Scaling Ownership: Who Builds, Operates, and Trusts the Platform

An eval platform without clear ownership is a platform that will be abandoned within six months. The most technically sophisticated eval infrastructure in the world is worthless if nobody is accountable for keeping it running, keeping it calibrated, and keeping it trusted. This is not a theoretical warning. It is the most common outcome for eval platforms built without an explicit ownership model. The platform launches with enthusiasm. Early adopters run evaluations. Results look useful. Then nobody fixes the first bug. Nobody updates the judge prompts when the product changes. Nobody investigates when scores drift. The platform becomes stale, and stale platforms get abandoned — quietly, without a formal decision, the same way eval scripts die when the volume threshold crosses.

The ownership question is not "who built it?" It is three separate questions that demand three separate answers. Who is responsible for the platform's availability and performance? Who is responsible for the evaluation criteria being correct and current? Who is responsible for acting on the results? Conflating these questions is how organizations end up with platforms that are technically running but operationally useless.

## The Three Ownership Models

Organizations converge on one of three ownership models, each with distinct strengths and failure modes. The right choice depends on your company's size, the number of AI-powered products you operate, and how specialized your evaluation requirements are.

**The centralized model** places a dedicated platform team in charge of the entire evaluation infrastructure. This team builds the platform, operates it, defines the evaluation framework, and often designs the evaluation criteria for each product team. The centralized team owns uptime, performance, result storage, and eval methodology. Product teams are consumers. They submit evaluation requests and receive results. They may provide golden set examples, but the centralized team manages the pipeline end to end.

Centralized ownership works well when evaluation requirements are relatively consistent across products. If your company runs five AI features and all of them need the same core quality dimensions — accuracy, relevance, safety, tone — a centralized team can standardize evaluation methodology, share judge prompts across products, and maintain a single result schema that enables cross-product quality comparisons. The efficiency gains are real. One team maintaining one platform is cheaper than five teams maintaining five separate eval systems. The centralized team accumulates deep expertise in evaluation methodology that no individual product team could develop alone.

The failure mode of centralized ownership is distance from product context. The centralized team does not sit in product meetings. They do not hear customer complaints firsthand. They do not understand why a particular edge case matters more than the eval scores suggest. When a product team requests a new quality dimension or a change to scoring criteria, the request enters the centralized team's backlog and competes with requests from every other product team. Response time slows. Product teams start working around the platform — running their own ad hoc evaluations because the official pipeline does not measure what they need it to measure. The platform remains technically healthy while becoming practically irrelevant.

## Embedded Ownership and Its Trade-offs

**The embedded model** places eval engineers directly within each product team. These engineers build and operate evaluation infrastructure specific to their product. They sit in product meetings, understand the domain, and can iterate on eval criteria as fast as the product evolves. When the product team ships a new feature, the eval engineer has already updated the golden set and added new quality dimensions. There is no ticket, no backlog, no waiting.

Embedded ownership excels at relevance. The evaluations measure exactly what the product team cares about. The golden sets reflect real customer scenarios. The scoring criteria match the product's definition of quality. When a customer complaint reveals a new failure mode, the eval engineer adds it to the eval suite the same day. The feedback loop between product experience and evaluation coverage is tight.

The failure mode of embedded ownership is duplication and inconsistency. Five product teams with five embedded eval engineers build five different evaluation pipelines. Each pipeline stores results in a different format. Each uses a different judge prompt template. Each defines "accuracy" slightly differently. There is no way to compare quality across products. There is no shared learning — when one team discovers that a particular judge prompt calibration technique works well, the other four teams do not benefit. Infrastructure costs multiply because each team provisions its own compute, its own storage, and its own monitoring. The company pays five times for infrastructure that could have been shared.

Embedded ownership also creates a career development problem. An eval engineer embedded in a product team often becomes the "eval person" — responsible for everything evaluation-related while the rest of the team focuses on features. This is isolating. The eval engineer has no peers to learn from, no community of practice, and no obvious career ladder. The best eval engineers leave for organizations where evaluation is a first-class discipline with its own team, its own leadership, and its own growth path. When they leave, the product team loses its entire evaluation capability, because the knowledge was in one person's head.

## The Hybrid Model: Where Mature Organizations Converge

**The hybrid model** is where most organizations with three or more AI products eventually land. A central platform team builds and operates shared infrastructure — the scheduling system, result storage, monitoring, cost tracking, and a library of standard judge prompts and evaluation templates. Product teams configure and extend this infrastructure for their specific needs — custom quality dimensions, domain-specific golden sets, product-specific scoring thresholds.

The boundary between platform and product is the critical design decision in the hybrid model. Draw it wrong, and you get the worst of both worlds — a centralized team that cannot keep up with product-specific requests, and product teams that cannot modify the platform to meet their needs. Draw it right, and you get shared efficiency with product-level relevance.

The boundary that works for most organizations is this: the platform team owns everything below the evaluation criteria, and the product team owns everything at and above the evaluation criteria. The platform team provides the infrastructure to define evaluation criteria, store golden sets, run judge prompts, schedule evaluation jobs, store results, and monitor pipeline health. The product team defines what criteria to evaluate, which golden sets to use, what scoring thresholds trigger alerts, and how to interpret results. The platform team provides the engine. The product team provides the fuel.

This boundary creates clear accountability. When the eval pipeline goes down at 3am, the platform team is responsible. When evaluation scores drop because the golden set is stale, the product team is responsible. When a new quality dimension needs to be added, the product team defines it and the platform team ensures the infrastructure supports it. Neither team can blame the other for failures in the other's domain.

A mid-sized AI company in 2025 implemented this hybrid model after eighteen months of struggling with fully embedded eval engineers. Their platform team of four engineers built a shared eval platform that served seven product teams. Each product team had one "eval champion" — not a full-time eval engineer, but a product engineer who was designated as the team's evaluation point of contact. The eval champion maintained the team's golden sets, configured evaluation criteria in the platform, and reviewed evaluation results weekly. The platform team handled infrastructure, scaling, and methodology. In the first quarter after migration, the company reduced its total eval infrastructure costs by 40% while increasing evaluation coverage from three products to seven.

## The Trust Problem: Why Teams Reject Good Platforms

Building a platform is the easy part. Getting teams to trust it is the hard part. Engineering teams will not use an eval platform they do not trust, and trust in evaluation infrastructure requires three properties that many platforms fail to provide.

The first property is **transparency**. Engineers need to understand exactly how scores are computed. A platform that produces a quality score of 0.82 without explaining what judge prompt generated that score, what model served as judge, what scoring rubric was applied, and how the final number was aggregated from individual evaluations is a black box. Engineers do not trust black boxes. They especially do not trust black boxes that claim to measure the quality of their work. Every score the platform produces must be traceable back to its inputs — the specific output that was evaluated, the specific criteria that were applied, the specific judge response that generated the score. When an engineer questions a result, they should be able to inspect every step of the evaluation chain within minutes.

The second property is **reliability**. A platform that goes down during a critical release, that produces inconsistent results across identical runs, or that loses evaluation data creates distrust that takes months to rebuild. Eval platform reliability needs to be treated with the same seriousness as production service reliability. Uptime targets, incident response, and post-mortems for evaluation outages signal to product teams that the organization takes eval infrastructure seriously. If the platform team treats outages casually, product teams treat the platform casually.

The third property is **relevance**. Product teams will not use evaluations that do not measure what they care about. If the platform evaluates generic quality dimensions — coherence, fluency, relevance — but the product team's actual quality concerns are domain-specific — clinical accuracy, regulatory compliance, brand voice adherence — the platform's scores are irrelevant to the product team's decisions. Relevance requires that product teams can define their own evaluation criteria within the platform. A platform that forces all products into the same evaluation rubric will be abandoned by any team whose quality requirements diverge from the default.

Trust is also built through small wins. A platform team that delivers one feature a product team requested, resolves one reliability issue quickly, or helps debug one confusing evaluation result earns more trust than a perfect architecture deck. Trust accumulates through demonstrated competence, not through promises.

## The Accountability Structure

Clear ownership requires clear accountability. Three questions define the accountability structure for any eval platform, and leaving any of them unanswered is a guarantee of future conflict.

First: who gets paged when the eval platform fails? This is the operational accountability question. If the platform goes down at 3am, whose phone rings? If nobody's phone rings, the platform does not have operational ownership, regardless of what the organizational chart says. The platform team should own this on-call responsibility, with escalation paths to product teams when failures are caused by product-specific configuration rather than platform infrastructure.

Second: who approves changes to evaluation criteria? This is the quality accountability question. When a product team wants to change a scoring threshold, add a new quality dimension, or retire an old golden set, who reviews that change? Product teams should own their evaluation criteria, but changes should be reviewed — either by the eval champion's peers or by the platform team's methodology experts. Unreviewd changes to evaluation criteria are as dangerous as unreviewed changes to production code. A lowered scoring threshold can mask quality degradation. A poorly designed golden set can create false confidence. Evaluation criteria deserve the same rigor as the systems they evaluate.

Third: who reviews eval results regularly? This is the insight accountability question. Results that nobody reads are results that do not exist. Every product team should have a designated person who reviews evaluation results at least weekly — not just the summary dashboard, but the failure cases, the edge cases, and the trend lines. When nobody reviews results, the eval platform becomes a data warehouse rather than a quality system. The difference between a data warehouse and a quality system is that someone acts on the quality system's outputs.

The accountability structure should be documented and visible. Not in a policy document that nobody reads, but in the platform itself — dashboards that show which teams reviewed results this week, alerts that notify team leads when results have not been reviewed, and escalation paths that activate when critical quality thresholds are breached and nobody responds.

## The Eval Contract

The relationship between the platform team and product teams works best when it is formalized in what experienced organizations call **the eval contract**. This is not a legal document. It is an explicit agreement about mutual expectations — what the platform guarantees, what the product team must provide, and what happens when either side fails to deliver.

The platform side of the contract specifies uptime guarantees for the evaluation pipeline, latency targets for evaluation results, storage retention policies for historical results, cost transparency and alerting thresholds, and response time commitments for platform bugs and feature requests. These guarantees give product teams confidence that they can depend on the platform as production infrastructure. Without them, product teams maintain backup evaluation scripts "just in case," and those backup scripts eventually become the primary eval system because they are under the product team's control.

The product team side of the contract specifies golden set maintenance cadence — how frequently the team reviews and updates its evaluation examples. It specifies result review cadence — how often someone looks at evaluation outputs and investigates anomalies. It specifies criteria update responsibility — who is accountable for keeping evaluation criteria current as the product evolves. It specifies escalation behavior — what the team does when evaluation scores breach critical thresholds.

The contract is valuable not because it creates legal obligation but because it makes expectations explicit. When a product team's golden set has not been updated in four months and evaluation scores become unreliable, the platform team can point to the contract rather than having a subjective argument about whose fault it is. When the platform experiences an outage that blocks a product release, the product team can reference the uptime guarantee rather than making an emotional case for prioritization.

Organizations that operate without an eval contract discover the consequences during incidents. The platform goes down, and nobody knows the expected response time. Golden sets go stale, and the platform team and product team each believe the other is responsible. Evaluation results show a quality regression, and nobody is sure who should investigate. The eval contract prevents these ambiguities from becoming conflicts. It turns organizational friction into engineering process.

## The Maturity Journey: From Ownership Confusion to Operating Discipline

Eval ownership matures through recognizable stages. In the earliest stage, nobody owns evaluation — it happens ad hoc, driven by individual engineers who care about quality. In the second stage, one engineer takes ownership, and evaluation becomes that engineer's side project. In the third stage, eval ownership is formalized — either in a central team, embedded engineers, or a hybrid structure. In the fourth stage, the ownership model includes the eval contract, regular accountability reviews, and platform-level operational discipline.

Most organizations in 2026 are somewhere between the second and third stages. They have outgrown ad hoc evaluation but have not yet formalized ownership. The common pattern is that a senior engineer recognizes the need, builds something useful, and becomes the de facto eval owner. The organization depends on that engineer without acknowledging the dependency. When the engineer transfers to another team or leaves the company, the organization discovers how fragile its eval infrastructure was.

The path from the second stage to the fourth stage does not require a massive reorganization. It requires three explicit decisions. First, decide on the ownership model — centralized, embedded, or hybrid — and staff it accordingly. Second, write the eval contract that defines mutual expectations. Third, establish the accountability structure that ensures results are reviewed, criteria are maintained, and the platform is operated as production infrastructure.

These decisions are not technical decisions. They are leadership decisions. They require someone with organizational authority to say: evaluation is production infrastructure, it needs a team, and that team needs a mandate. Without that mandate, the best eval platform in the world will slowly decay into another abandoned internal tool.

---

With the scaling wall mapped and ownership established, the next question is the most fundamental operational decision in evaluation at scale: when you cannot evaluate everything, how do you decide what to evaluate — and how do you ensure that what you skip does not quietly destroy quality? That question opens Chapter 2.
