# Chapter 4 — Online Evaluation and Experimentation: Real-Time Quality Measurement in Production

Offline evaluation tells you how a model performs on your test set. Online evaluation tells you how it performs on your users. The gap between these two is where most quality failures hide. Traditional A/B testing, the standard for web experimentation, breaks down for AI systems — non-deterministic outputs, infinite variant spaces, and rapid iteration cycles make classical approaches inadequate. This chapter covers the 2026 replacement: eval-driven online experimentation that measures quality in production continuously, not just at launch.

---

- **4.1** — The Offline-Online Evaluation Spectrum: Why You Need Both
- **4.2** — Why Traditional A/B Testing Breaks for AI Systems
- **4.3** — Shadow Deployments: Testing Models on Live Traffic Without User Impact
- **4.4** — Interleaving Experiments: Comparing Models Within a Single User Session
- **4.5** — Real-Time Eval Scoring: Grading Production Outputs as They Flow
- **4.6** — Progressive Rollouts with Eval Gates: From Canary to Full Deployment
- **4.7** — The Feedback Loop: How Online Eval Results Refresh Offline Golden Sets
- **4.8** — Cost Management for Online Evaluation: Scoring at Scale Without Breaking the Budget

---

*Online evaluation generates the signal. The next chapter covers the infrastructure that carries that signal — the distributed pipelines, compute allocation, and reliability engineering required to run evaluation as a production-grade system.*
