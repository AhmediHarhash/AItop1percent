# 10.7 — Vendor Lock-In and Portability: Keeping Your Eval Data Yours

Your eval data is the most valuable dataset your AI team produces. It contains the ground truth about your system's quality, the history of your quality evolution, and the calibration signal for every judge you'll ever build. It is the artifact that tells you what "good" looked like six months ago, what it looks like today, and how it changed in between. Losing access to it because you switched vendors is unacceptable. Yet teams lose access to eval data during vendor transitions with alarming regularity — not because the vendor refuses to export it, but because the data was stored in formats, schemas, and relationships that only that vendor's tooling can interpret. The lock-in is rarely contractual. It is structural.

## The Four Types of Lock-In

Lock-in in eval tooling is not a single problem. It manifests in four distinct forms, each with different consequences and different mitigations.

**Data format lock-in** is the most visible. Your eval results, datasets, annotations, and judge outputs are stored in the platform's proprietary schema. The schema encodes relationships — which judge produced which score, which dataset was used for which evaluation run, which annotation corresponds to which output — in a format that the platform's tools understand and nothing else does. You can export the raw data, but without the relationships, the export is a pile of disconnected records rather than a usable evaluation history.

**Workflow lock-in** is subtler. Over months of using a platform, your team develops workflows that depend on the platform's specific capabilities. Your reviewers know how to use the platform's annotation interface. Your engineers know how to configure its judge pipelines. Your CI/CD integration calls its specific API endpoints. Switching platforms means retraining reviewers, reconfiguring pipelines, and rewriting integrations — even if the new platform has equivalent capabilities, the migration cost is real.

**Integration lock-in** compounds workflow lock-in. Every system connected to the eval platform — model serving, CI/CD, dashboards, alerting — has an integration that depends on the platform's API contract, event format, and authentication mechanism. Switching the eval platform means rewiring every integration. The more deeply integrated the platform, the higher the switching cost. This creates a perverse incentive: the better you integrate a platform, the harder it becomes to leave.

**Institutional knowledge lock-in** is the most insidious and the hardest to mitigate. Over time, your team's understanding of your evaluation quality is expressed in the platform's terms. "Our accuracy score is 87%" means the platform's accuracy metric, computed by the platform's judge, on the platform's dataset. If you switch platforms and the new platform computes accuracy differently — different tokenization, different scoring rubric, different sampling — your historical benchmarks become meaningless. You can't compare your new scores to your old ones because the measuring instruments changed.

## Data Portability Requirements

Portability starts with a set of non-negotiable requirements that you should evaluate before adopting any platform, not after you've been using it for two years.

All eval results must be exportable in standard, documented formats. Every score, every judge output, every metadata field, every timestamp. The export must preserve the relationships between results — which eval run produced which scores, which model version was being evaluated, which dataset was used. A flat CSV of scores without context is not a portable export. A structured export with full provenance is.

All datasets must be exportable with full version history. Your golden sets, regression suites, and production samples represent months or years of curation. The export must include not just the current version of each dataset but its evolution over time — what was added, what was removed, what was modified, and when. Without version history, you lose the ability to understand how your evaluation standards evolved and to reproduce historical evaluations.

All judge configurations must be exportable in a format that another system can consume. Your judge prompts, scoring rubrics, threshold configurations, and routing rules represent substantial intellectual investment. If they exist only as configuration within the platform's UI, they are locked in. If they are exportable as structured configuration files that you can version-control and deploy to another system, they are portable.

All annotation data must be exportable with annotator provenance. Who annotated what, when, with what inter-annotator agreement, using which version of the annotation guidelines. Annotation data is training data for future judges. Losing the provenance — who made each judgment and how consistent they were — degrades the value of the annotations from calibrated ground truth to unlabeled opinions.

## The Abstraction Layer Strategy

The most effective portability strategy is to never let your evaluation logic depend directly on any single platform's interface. Instead, build an **abstraction layer** — a thin internal interface that defines how your code interacts with eval infrastructure. Your application code, CI/CD pipelines, and custom judges all communicate through this abstraction. The abstraction communicates with the underlying platform.

The abstraction defines operations like "submit an output for evaluation," "retrieve eval results for a model version," "update a golden set," and "configure a judge." Each operation has a platform-independent contract. The implementation behind the abstraction translates those operations into whichever platform's API you're currently using. When you switch platforms, you write a new implementation for the abstraction layer. Everything above the abstraction — your application code, your CI/CD integration, your custom judges — remains unchanged.

This strategy costs more up front. You're building an internal API layer that the platform's SDK already provides. But the cost is modest — typically a few weeks of engineering for a team that understands the eval workflow well — and the payback is substantial. You can evaluate alternative platforms without rewriting application code. You can run two platforms in parallel during migration. You can even use different platforms for different evaluation types if that turns out to be the best architectural choice.

## The Dual-Write Strategy

A complementary strategy is **dual-writing**: every eval result is written to the platform's storage and simultaneously to your own internal data store. The platform provides the dashboards, alerting, and workflow management. Your internal store provides the portable backup, the historical record, and the insurance against platform lock-in.

The internal store should use a schema you control — not the platform's schema mirrored, but a schema designed around your evaluation needs. A relational database works for most teams. PostgreSQL is the pragmatic choice in 2026 for teams that want reliability without operational complexity. The schema captures what matters: the output being evaluated, the judge that evaluated it, the score produced, the model and prompt versions, the dataset used, the timestamp, and any metadata your team needs for analysis.

Dual-writing adds operational complexity. You need to keep the two stores consistent, handle failures in either store without losing data, and maintain the schema as your evaluation needs evolve. But the complexity is manageable, and the alternative — trusting a single vendor to hold the only copy of your eval history — is a risk that grows every month you use the platform. The longer you wait to implement dual-writing, the more history you've accumulated exclusively in the vendor's system and the harder migration becomes.

## Standard Schemas for Eval Data

Portability is easier when your eval data follows standard schemas rather than vendor-specific formats. The eval tooling ecosystem in 2026 has not converged on a universal standard the way observability converged on OpenTelemetry, but common patterns have emerged.

The minimum standard schema for an eval result includes the evaluated output and its input, the model version and prompt version that produced it, the judge identifier and version, the score or scores produced, the timestamp, and a unique identifier that links the result to the broader eval run. Teams that adopt this schema from day one — regardless of which platform they use — find migration dramatically simpler because the core data translates directly between systems.

For datasets, the minimum standard includes the example content, the expected output or annotation if applicable, the dataset version, the date added, and the provenance — where did this example come from, why was it added, who approved it. For annotations, the standard includes the annotator identifier, the annotation value, the timestamp, the annotation guideline version, and any inter-annotator agreement metrics computed at annotation time.

Defining these schemas before you adopt a platform is a thirty-minute exercise that saves months during migration. Maintaining them as your evaluation evolves requires discipline — every new field, every new eval type, every new judge output needs to be reflected in the standard schema, not just in the platform's native format.

## The Cost of Migration and How to Minimize It

Migration cost has three components, and teams routinely underestimate the second and third.

The first component is technical migration: exporting data from the old platform, transforming it to the new platform's format, importing it, verifying completeness. This is the component teams think about, and it is usually the smallest. If you've maintained dual-writes and standard schemas, technical migration is straightforward — the data is already in your control.

The second component is workflow migration: retraining every person who interacts with the eval system — reviewers, engineers, product managers, executives who read dashboards. Each person has built habits around the old platform's interface. They know where to click, what the numbers mean in that platform's context, how to investigate anomalies. The new platform may have equivalent capabilities, but the interface is different, the navigation is different, and the learning curve is real. Teams that underestimate workflow migration end up with a technically complete migration and an organizationally dysfunctional one — the new platform is running but nobody trusts it because they don't understand it yet.

The third component is calibration migration: establishing that the new system's measurements are comparable to the old system's. If your team has been tracking "accuracy at 87%" for a year, and the new platform computes accuracy at 83% on the same data because of different implementation details, you have a calibration discontinuity. Historical trends become uninterpretable. Quality targets need recalibration. Executive dashboards need explanation. The work to bridge this discontinuity — running parallel evaluations on both platforms, computing conversion factors, re-establishing baselines — is often the most time-consuming part of migration.

The mitigation for all three components is the same: plan for portability from day one, not from the day you decide to leave. Abstraction layers, dual-writes, standard schemas, and documented workflows are investments that cost little when implemented early and save enormously when migration becomes necessary.

Your eval data is yours. Your eval history is yours. Your quality definitions are yours. No platform decision should change that, and the subchapter that follows covers the single most impactful tooling investment you can make: the custom annotation tool that bridges human judgment and automated evaluation.
