# 5.6 — Eval Compute Allocation: GPU, CPU, and API Budget Distribution Across Eval Types

A fintech company spent $45,000 per month on eval infrastructure. When they audited the spend, they discovered that sixty percent was going to a single eval type — a computationally expensive embedding-based similarity check that ran on every production output. The check compared each output against a reference corpus of five thousand approved responses using pairwise cosine similarity, requiring both embedding generation and nearest-neighbor search across the full corpus. It had been the primary quality gate during the product's beta phase, when the team had no LLM judge and relied on semantic similarity as a proxy for correctness. Nine months later, the team had built a comprehensive LLM judge pipeline that evaluated accuracy, tone, safety, and compliance — making the similarity check redundant for quality gating. But nobody had turned it off. The similarity eval ran on every output, consumed three GPUs full-time, and contributed nothing that the judge pipeline didn't already cover. Twenty-seven thousand dollars per month, burned on inertia.

This story repeats itself across the industry. Evaluation pipelines accumulate eval types over time — each one added for a good reason at the time of creation — and the total compute bill grows monotonically because nobody audits which evaluations are still earning their keep. Cost allocation is not just about choosing the cheapest compute for each eval. It is about ensuring that every eval in your pipeline has a documented purpose, a justified compute tier, and a periodic review that confirms it still belongs.

## The Three Compute Categories

Every evaluation in your pipeline falls into one of three compute categories, and each category has a fundamentally different cost structure. Understanding which category an eval belongs to determines how you budget for it, how you scale it, and when you should consider replacing it.

**CPU-based evaluations** are the cheapest. They run on standard compute without specialized hardware. Heuristic checks — regex pattern matching, response length validation, format compliance, keyword detection, JSON schema validation — consume negligible compute. A single CPU core can process thousands of these checks per second. Statistical evaluations — computing BLEU scores, ROUGE scores, exact-match rates, or string distance metrics — are similarly inexpensive. CPU-based evals cost fractions of a cent per thousand evaluations. Their operational overhead is minimal: they run reliably, they don't require API keys, they don't hit rate limits, and they don't fail due to provider outages. The weakness is expressiveness. CPU-based checks catch structural problems — the output is the wrong format, the wrong length, contains blocked words — but they can't assess semantic quality. They tell you whether the output looks right, not whether it is right.

**GPU-based evaluations** occupy the middle tier. They require GPU hardware but don't involve external API calls. Embedding computation — generating vector representations of outputs for similarity scoring, clustering, or drift detection — is the most common GPU-based eval. Self-hosted judge models — open-weight LLMs running on your own infrastructure for scoring — are another. Classifier-based evaluations — running a fine-tuned BERT or similar model to detect toxicity, sentiment, or topic — also fall here. GPU-based evals cost more than CPU checks but less than API calls at scale. The per-eval cost depends on the model size, the hardware, and the utilization rate. A small classifier on a T4 GPU might cost 0.0001 dollars per evaluation. A seven-billion-parameter judge model on an A100 might cost 0.001 to 0.005 dollars per evaluation. The advantage is control: no rate limits, no provider dependencies, predictable latency. The disadvantage is capital: you pay for the GPU whether or not the eval pipeline is running.

**API-based evaluations** are the most expensive per unit but require no infrastructure. LLM judge calls to frontier providers — GPT-5, Claude Opus 4.6, Gemini 3 Pro — cost 0.003 to 0.02 dollars per evaluation depending on prompt length, model tier, and output size. Mid-tier models — GPT-5-mini, Claude Haiku 4.5, Gemini 3 Flash — cost 0.0005 to 0.003 dollars per evaluation. API-based evals are the most expressive: they can assess nuance, context, reasoning quality, and subjective dimensions that no heuristic or classifier can capture. They're also the most variable in cost: a complex multi-dimension evaluation with a long rubric costs five to ten times more than a simple pass/fail check with a concise prompt. And they're subject to external constraints — rate limits, provider outages, pricing changes — that you don't control.

## Matching Eval Types to Compute Tiers

The allocation principle is straightforward: run each evaluation on the cheapest compute tier that produces an acceptable quality of signal. Don't use an API-based LLM judge to check whether a response is valid JSON — a CPU regex check does that better, faster, and for free. Don't use a CPU heuristic to assess whether a response is empathetic — that requires semantic understanding that heuristics can't provide.

The matching process starts with your eval catalog — the complete list of evaluation types your pipeline runs. For each eval type, answer three questions. What is the evaluation actually measuring? What is the minimum signal quality needed for that measurement to be useful? And what is the cheapest compute tier that can deliver that signal quality?

Format validation, length checks, keyword presence, response structure, and schema compliance all belong on CPU. These are deterministic checks with binary outcomes — the output either passes or it doesn't. There's no ambiguity, no need for judgment, and no benefit from using a more expensive compute tier.

Embedding similarity, topic classification, sentiment analysis, toxicity scoring, and drift detection belong on GPU. These require model inference but not the full reasoning capability of a frontier LLM. Small, efficient models — fine-tuned classifiers, sentence transformers, compact embedding models — handle them well. A fine-tuned DistilBERT toxicity classifier produces safety classifications that agree with human reviewers at ninety-three to ninety-five percent, comparable to frontier LLM judges, at a fraction of the cost.

Accuracy assessment, rubric-based quality scoring, multi-dimension evaluation, nuanced safety checks that require understanding context, and any evaluation that involves comparing an output against complex criteria belong on API. These evaluations require the general reasoning capability that only large language models provide. Attempts to replace them with cheaper alternatives — classifiers, heuristics, smaller models — consistently lose the nuanced signal that makes the evaluation useful.

## The Tiered Evaluation Pyramid

The most cost-effective eval architecture is a pyramid: broad coverage at the bottom with cheap checks, progressively narrower coverage at each higher tier, and expensive LLM judge evaluations only at the top for the outputs that need them.

The base of the pyramid is CPU-based checks on one hundred percent of production outputs. Every output passes through format validation, length checks, safety keyword detection, and any heuristic rules specific to your product. These checks cost essentially nothing and catch the catastrophic failures — empty responses, format breakdowns, obvious safety violations. Any output that fails a CPU check is flagged immediately without consuming GPU or API resources.

The middle layer is GPU-based evaluation on a large sample — twenty to fifty percent of outputs that passed the CPU checks. Embedding-based similarity scoring can detect drift, cluster anomalies, and identify outputs that fall outside the normal distribution. Classifier-based checks can flag borderline safety cases or off-topic responses. These evaluations are cheap enough to run on a substantial fraction of traffic without breaking the budget.

The top of the pyramid is API-based LLM judge evaluation on a targeted sample — one to ten percent of outputs, selected based on the results of the lower tiers. Outputs that the GPU-based checks flagged as borderline get priority for judge evaluation. Outputs that the heuristic checks couldn't confidently categorize — outputs near the quality threshold — get bumped up for nuanced assessment. Random samples fill the remaining budget to maintain statistical coverage.

This pyramid structure means that your most expensive evaluation resource — the LLM judge — is used only on the outputs that most benefit from it. The cheap checks handle the easy cases. The mid-tier checks handle the medium cases. The judge handles the hard cases. The total cost is a fraction of what you'd spend running the judge on everything, and the quality signal is nearly as good because the easy cases that the judge would have evaluated were obvious passes or fails that didn't need nuanced assessment.

## Cost Tracking by Eval Type

You cannot optimize what you don't measure. Every eval pipeline should instrument per-eval-type cost tracking from day one — and "day one" means before the pipeline runs in production, not after the first budget overrun.

Per-eval-type cost tracking requires three data points for each evaluation executed: the eval type identifier, the compute tier used, and the cost. For API-based evals, the cost is the token count multiplied by the per-token price — both input and output tokens, which you can compute from the API response metadata. For GPU-based evals, the cost is the GPU-seconds consumed multiplied by the per-second cost of the GPU instance, divided by the batch size if the eval uses batching. For CPU-based evals, the cost is typically so low that tracking it at all is optional, but including it for completeness helps identify cases where "cheap" CPU checks have been replaced by expensive alternatives without anyone noticing.

Aggregate these costs daily by eval type and review the breakdown monthly. The review answers three questions. Which eval types consume the most budget? Has the cost distribution shifted since last month? Are there eval types consuming meaningful budget that no longer provide unique value?

The monthly review is where you catch the fintech company's problem. An eval type that was essential during development but is now redundant with a better alternative. An eval that was configured for full-traffic coverage but should have been downgraded to sampling after the product stabilized. A GPU-based eval running on an expensive A100 that could run equally well on a cheaper T4. These optimizations are invisible without per-type cost tracking and routine reviews.

## Zombie Evaluations: The Silent Budget Drain

**Zombie Evaluations** are eval jobs that continue running and consuming budget long after they've stopped being useful. They're the most common source of eval cost waste, and they're endemic to any pipeline that has been running for more than six months.

Zombies arise through predictable mechanisms. A team adds an eval to investigate a specific quality issue. The issue gets fixed, but nobody removes the eval. A new eval type replaces an old one, but the old one isn't decommissioned because "it doesn't hurt to have extra coverage." A prototype eval was added during development with full-traffic coverage and was never downgraded to sampling after launch. An eval's criteria haven't been updated in a year and no longer reflect the product's quality standards, but nobody has reviewed whether the scores it produces still inform any decision.

The defining characteristic of a zombie eval is that no one acts on its results. If an eval produces scores that no dashboard displays, no alert consumes, and no human reviews, it is a zombie. It exists in the pipeline, consumes compute, appears in cost reports, but influences no decision. It is pure overhead.

Detecting zombies requires a simple practice: every eval type should have a **documented consumer**. The consumer is the team, dashboard, alert, or process that uses the eval's results. When you add a new eval, document who will consume its output and how. During quarterly reviews, verify that the documented consumer still exists and still uses the results. If the consumer has disappeared — the dashboard was deprecated, the alert was silenced, the team was reorganized — the eval is a zombie candidate. Give the team thirty days to identify a new consumer or justify the eval's continued operation. If neither happens, decommission it.

The practice of eval lifecycle management — treating each evaluation type as a resource with a creation date, a documented purpose, a documented consumer, and a periodic review — prevents zombie accumulation. It feels bureaucratic, and it is. But the alternative is a pipeline that grows in cost every quarter while the value it delivers stays flat or declines. A team that reviews and prunes its eval catalog quarterly will spend thirty to fifty percent less on evaluation infrastructure than a team that only adds and never removes.

## The Self-Hosting Break-Even

At what point does running your own judge model on your own GPUs become cheaper than making API calls to a hosted provider? This question comes up whenever eval costs become a significant budget item, and the answer depends on three variables: your eval volume, your accuracy requirements, and your operational capability.

The cost of a self-hosted open-weight judge model is the GPU instance cost amortized over the evaluations it processes. A mid-tier open-weight model — Llama 4 Maverick, Mistral Large 3, or a fine-tuned variant — running on a single A100 or H100 GPU can process roughly five hundred to two thousand evaluations per hour, depending on prompt length and model size. At current cloud GPU pricing of approximately two to four dollars per hour for an A100, that's 0.001 to 0.008 dollars per evaluation — comparable to mid-tier API pricing but without rate limits.

The break-even calculation compares this all-in cost against API pricing. If your API-based evals cost 0.005 dollars each and your self-hosted setup costs 0.003 dollars each including operational overhead, you save 0.002 dollars per evaluation. At ten thousand evaluations per day, that's twenty dollars per day or six hundred dollars per month — probably not enough to justify the operational complexity. At one hundred thousand evaluations per day, that's two hundred dollars per day or six thousand dollars per month — potentially significant. At five hundred thousand evaluations per day, it's one thousand dollars per day or thirty thousand dollars per month — almost certainly worth the investment.

The operational overhead is the variable most teams underestimate. Self-hosting a judge model means managing GPU procurement, model deployment, inference serving (vLLM, TGI, or similar), monitoring, failover, model updates, and on-call responsibilities. For a team that already operates GPU infrastructure for other purposes — model serving, training, embeddings — the marginal cost of adding an eval judge is modest. For a team that would need to build GPU infrastructure from scratch solely for evaluation, the operational burden may outweigh the cost savings until eval volume reaches hundreds of thousands per day.

A hybrid approach often makes the most sense. Use self-hosted open-weight models for the high-volume, routine evaluations — the middle tier of the pyramid — where you need hundreds of thousands of evaluations per day and the model quality requirements are moderate. Use API-based frontier judges for the low-volume, high-stakes evaluations — the top of the pyramid — where you need the best possible accuracy and the volume is small enough that API costs are manageable. This hybrid gives you the cost efficiency of self-hosting where volume drives the budget and the quality of frontier models where accuracy matters most.

## Budget Allocation Across Teams and Products

In organizations with multiple AI products or multiple teams sharing eval infrastructure, budget allocation becomes an organizational problem, not just a technical one. Three allocation models exist, each with trade-offs.

**Centralized allocation** gives a single platform team control of the total eval compute budget and responsibility for distributing it across products and teams. This model produces the best overall cost efficiency because the platform team can identify cross-product redundancies — two teams running the same safety eval independently, for instance — and consolidate them. The weakness is that individual product teams lose control over their eval timing and priority. When the platform team's queue is full, your pre-release eval might wait behind another team's nightly regression.

**Per-team allocation** gives each product team its own eval compute budget — a fixed number of API calls per month, a dedicated GPU allocation, or a dollar-denominated budget. Teams control their own priorities and scheduling within their allocation. The weakness is that utilization is uneven: one team might exhaust its budget by the twentieth of the month while another team's allocation sits half-used. There's no mechanism for the surplus to flow to where it's needed.

**Elastic allocation with guardrails** combines the best of both. Teams draw from a shared pool but have per-team guardrails: a minimum guaranteed allocation (ensuring every team can run critical evals regardless of other teams' usage) and a maximum burst allocation (preventing any single team from monopolizing the shared pool). The platform team sets the guardrails based on each product's criticality and eval volume, and the shared pool absorbs demand spikes without requiring over-provisioning. This is the most complex to implement but the most efficient at scale, and it's the model that most enterprise eval platforms converge on by the time they support five or more product teams.

## The Quarterly Eval Budget Review

Eval compute allocation is not a set-it-and-forget-it decision. Traffic patterns change, new eval types get added, old eval types should be retired, model pricing shifts, and the balance between quality risk and eval cost evolves as the product matures. A quarterly review keeps the allocation aligned with reality.

The review agenda covers four items. First, cost trend analysis: is total eval spend growing, shrinking, or flat? If growing, is the growth justified by increased traffic, new eval types, or higher quality standards — or is it unjustified cost drift from zombie evals and unoptimized configurations? Second, per-type cost review: which eval types cost the most, and are they worth it? Has the cost distribution shifted in a way that suggests a re-tiering opportunity — an API-based eval that could be replaced by a GPU-based classifier, or a GPU-based eval that could be replaced by a CPU heuristic? Third, zombie audit: which eval types have no documented consumer? Which eval results does nobody look at? Which evals haven't had their criteria updated in more than six months? Fourth, forecast: based on projected traffic growth and planned product changes, what will the eval budget need to be next quarter? Should the team request a budget increase, or can it absorb the growth through optimization?

The output of the quarterly review is a cost allocation plan: which eval types to keep, which to decommission, which to re-tier, and what the projected budget for the next quarter looks like. This plan becomes the basis for infrastructure provisioning — how many GPU instances to reserve, what API rate-limit tier to subscribe to, and what the cost alerts should be set to.

Teams that perform quarterly reviews consistently spend twenty to forty percent less on eval infrastructure than teams that don't, not because they evaluate less but because they evaluate smarter. They catch zombies before zombies consume meaningful budget. They re-tier evaluations as cheaper alternatives become available. They right-size their GPU allocations based on actual utilization rather than worst-case projections. And they enter budget conversations with data rather than guesses, which tends to produce better outcomes for everyone involved.

Compute allocation ensures that every dollar in the eval budget produces maximum quality signal. But the evaluations themselves need to run within the development workflow — triggered by code changes, gating releases, and providing feedback to engineers where and when they need it. The next subchapter covers eval-as-code and CI/CD integration: making evaluation a seamless part of the software development lifecycle rather than a standalone process that engineers have to remember to invoke.
