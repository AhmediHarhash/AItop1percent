# Chapter 3 — LLM-as-Judge at Scale: Cost, Calibration, and Reliability

LLM-as-judge is the dominant evaluation paradigm in 2026, but it is also the most expensive, the most fragile, and the most misunderstood. Section 15 covered the mechanics of building LLM judges. This chapter covers the economics of running them at production scale — model selection for cost, prompt optimization for reliability, systematic bias mitigation, and the calibration infrastructure that keeps automated scores aligned with human judgment as your system grows.

---

- **3.1** — The LLM Judge as Your Largest Eval Cost Center
- **3.2** — Judge Model Selection: Frontier vs Open-Weight Economics at Production Volume
- **3.3** — Prompt Optimization for Judges: Why Simple Prompts with Strong Models Beat Complex Prompts
- **3.4** — Multi-Judge Ensembles and Consensus Protocols at Scale
- **3.5** — Judge Calibration Loops: Aligning Automated Scores with Human Judgment Continuously
- **3.6** — Systematic Biases at Scale: Position Bias, Verbosity Bias, and Score Inflation
- **3.7** — MemAlign and Feedback-Driven Judge Improvement: The State of the Art in 2026
- **3.8** — Judge Drift: When Your Evaluator Degrades and Nobody Notices

---

*With cost-effective, calibrated judges in place, the next question is where those judges operate — and the answer, increasingly, is not just offline benchmarks but live production traffic, measured in real time.*
