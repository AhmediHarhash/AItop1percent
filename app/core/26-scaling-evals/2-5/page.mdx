# 2.5 — Change-Based Sampling: Intensifying Evaluation After Deployments and Model Updates

The deployment looked clean. Tests passed, staging evaluation showed no regression, and the release shipped at 2 PM. By 4 PM, customer complaints had tripled. The model update had introduced a subtle tone shift — responses to enterprise customers had become noticeably more casual, dropping the professional register the product was known for. None of the pre-deploy evaluations caught it because the test set didn't include enough enterprise-style queries, and production sampling was running at its normal two percent rate. At two percent, the system evaluated roughly forty enterprise interactions in the first two hours. Of those forty, the automated judge flagged three as potentially off-tone, but three flags out of forty didn't cross the alert threshold. It took five hundred customer interactions — and twelve direct complaints to the account management team — before anyone realized the deployment had damaged the product's voice.

The root cause wasn't a bad model or a careless team. The root cause was that the evaluation system treated the post-deployment period the same as any other period. The same sampling rate. The same judge thresholds. The same alert sensitivity. But the post-deployment period is not like any other period. It is the highest-risk window in your product's lifecycle — the interval when new code, new models, or new configurations encounter the full diversity of real-world traffic for the first time. Treating this window as business-as-usual is one of the most common and most costly evaluation mistakes at scale.

## Why Changes Are the Highest-Risk Moments

Every change to an AI system introduces a distribution shift between what was tested and what will be encountered. Pre-deployment evaluation, no matter how thorough, operates on a subset of production traffic — a test set, a golden set, a staging environment with synthetic load. These subsets approximate production but never replicate it perfectly. The test set might cover ninety-five percent of typical queries, but the five percent it misses could include the exact edge cases where the new model version behaves differently. The staging environment might simulate average load, but it doesn't simulate the seasonal query patterns, the regional language variations, or the adversarial inputs that arrive at unpredictable intervals.

This gap between pre-deploy testing and production reality is not a failure of testing methodology. It is a fundamental property of complex systems interacting with diverse users. No test suite can anticipate every input. No staging environment can replicate every traffic pattern. The only environment that fully represents production is production itself. This means the first hours and days after a change are, by definition, the period when you are most likely to encounter failure modes that pre-deploy evaluation didn't cover.

Industry experience consistently shows the pattern. The overwhelming majority of production quality incidents in AI systems are traceable to a recent change — a model update, a prompt modification, a retrieval index rebuild, a tool integration change. The failure rarely appears immediately. It appears when the changed system encounters a specific combination of input, context, and user expectation that the pre-deploy evaluation didn't include. The longer you run at normal sampling rates after a change, the longer these failures go undetected.

## The Post-Deploy Surge

**The Post-Deploy Surge** is the practice of temporarily increasing your evaluation sampling rate by five to twenty times immediately after any change, maintaining the elevated rate until quality metrics stabilize, and then automatically decaying back to normal rates. It is the single most effective pattern for catching post-deployment regressions before they reach critical mass.

The surge operates on a simple principle: if the highest-risk period is immediately after a change, that period should receive the highest evaluation intensity. A system that normally samples two percent of traffic should sample ten to forty percent during the post-deploy window. The additional sampling cost is temporary — typically twenty-four to seventy-two hours — and the protection it provides is disproportionate to its cost. Catching a regression twelve hours after deployment instead of twelve days after deployment is the difference between rolling back a clean release and managing a customer escalation crisis.

The surge is not a panic response. It is a planned, automated escalation that begins the moment a change is detected and ends when evidence confirms the change is safe. It should be configured before the deployment, triggered automatically by the deployment pipeline, and resolved automatically by the evaluation pipeline. Human judgment is needed to investigate the anomalies the surge discovers. Human judgment should not be needed to initiate or terminate the surge itself.

## What Counts as a Change

The most common implementation mistake with change-based sampling is defining "change" too narrowly. Teams that only surge after model version updates miss the majority of change-induced regressions, because model updates are only one of many changes that affect output quality.

A model version update is the most obvious change. Swapping from one model version to another — whether it's moving from GPT-5 to GPT-5.1, updating a fine-tuned model with new training data, or switching providers entirely — changes the system's core generation behavior. Every response is potentially different. This warrants the highest surge intensity and the longest stabilization window.

A prompt template modification is the second most common change. Modifying the system prompt, adjusting few-shot examples, changing instructions, or altering the formatting guidance changes how the model interprets and responds to every query. Prompt changes are deceptively risky because they seem small — a few words here, a sentence there — but they can shift tone, accuracy, and behavior across the entire output distribution. A team at a B2B SaaS company in early 2025 changed a single instruction in their system prompt from "provide a concise answer" to "provide a thorough answer." Average response length tripled. Customer satisfaction dropped nineteen percent because users wanted quick answers and were getting essays. The change was three words. The impact was product-wide.

A retrieval index rebuild changes what information the model has access to when generating retrieval-augmented responses. If the index includes new documents, removes old ones, changes embedding models, or alters chunking strategies, the context the model receives shifts. This can improve relevance for some queries while degrading it for others. Retrieval changes are particularly insidious because they don't change the model or the prompt — they change the input the model receives, making the regression harder to diagnose.

A tool integration update changes how the model interacts with external systems. Adding a new tool, modifying an existing tool's API, changing tool selection logic, or updating the tool's output format can all change the model's behavior in multi-step agentic workflows. If the model previously called a weather API that returned temperatures in Fahrenheit and the API switches to Celsius, every temperature-related response changes — not because the model changed, but because the tool output changed.

A configuration change includes anything that modifies the system's behavior without changing code: temperature settings, max token limits, safety filter thresholds, rate limits, routing logic. These changes are often made by operations teams through configuration management systems, outside the engineering team's deployment pipeline. They bypass CI/CD and therefore bypass pre-deploy evaluation entirely. Configuration changes should trigger a surge even though they feel minor, because they affect output behavior and they lack the pre-deploy eval safety net that code changes receive.

Even an infrastructure migration — moving from one cloud region to another, changing the inference server, updating the container runtime — can affect behavior if it introduces latency changes that alter timeout behavior, or if different hardware produces slightly different floating-point results that change model outputs at the margins. These effects are rare but real, and a post-migration surge costs little while providing a definitive answer about whether the migration was quality-neutral.

## Implementing the Surge

The implementation of a post-deploy surge has three components: change detection, rate escalation, and automatic decay.

Change detection means your evaluation system knows when something changed. The simplest approach is direct integration with your deployment pipeline. When the CI/CD system deploys a new version, it emits an event that the evaluation system consumes. The evaluation system records the change type, timestamp, and affected scope (which product, which feature, which model). For configuration changes that bypass CI/CD, you need a configuration management system that emits change events — or, at minimum, a manual trigger that operations teams use to notify the eval system that something changed. The worst outcome is a change that nobody tells the eval system about. When that happens, you lose the surge protection precisely when you need it.

Rate escalation means increasing the sampling rate from its normal level to the surge level. The surge rate depends on the change type and the risk profile. Model version updates might warrant a twenty-times multiplier — from two percent to forty percent — because they affect every output. Prompt changes might warrant a ten-times multiplier. Configuration changes might warrant a five-times multiplier. The multiplier should be configurable per change type and stored in your evaluation configuration, not hard-coded. As your team gains experience with which change types are most likely to cause regressions, you adjust the multipliers accordingly.

Automatic decay means reducing the sampling rate back to normal levels once the change has been validated. This is where most teams either over-engineer or under-engineer. The under-engineered approach is a fixed timer: surge for forty-eight hours, then drop back to normal. This works for most changes, but it fails for changes that take longer to stabilize — a retrieval index rebuild whose effects unfold over several days as different users encounter different retrieved content — and it wastes budget on changes that stabilize in two hours. The over-engineered approach is complex statistical process control that requires a PhD to configure. The practical middle ground is a stabilization criterion.

## The Stabilization Criterion

The **stabilization criterion** defines when the surge can safely end. It answers the question: how do you know the change is safe? The answer is not a fixed time window. It is evidence-based: the change is safe when quality metrics have been within normal bounds for a sufficient number of consecutive evaluation windows.

A practical stabilization criterion works as follows. Define "normal bounds" as the mean plus or minus two standard deviations of each quality metric over the thirty days preceding the change. This gives you a baseline that reflects natural variation. After the change, track each quality metric — accuracy, relevance, safety, tone, format compliance — across consecutive evaluation windows. An evaluation window is the time interval at which your system batches and analyzes samples. For a system running at surge rates, this might be every thirty minutes or every hour. When all tracked metrics have been within normal bounds for N consecutive windows, the surge ends and sampling drops to normal.

The value of N depends on your risk tolerance and the change type. A low-risk configuration change might require four consecutive windows (two hours at thirty-minute intervals). A model version update in a regulated domain might require twenty-four consecutive windows (twelve hours). The principle is that one good window isn't enough — you need sustained stability to be confident, because quality degradation sometimes appears in bursts when specific types of queries arrive at specific times of day.

If any metric falls outside normal bounds during the surge, two things happen. First, the surge timer resets. The clock on consecutive stable windows goes back to zero. Second, the violation is routed to the appropriate team for investigation. A safety metric violation goes to trust and safety. An accuracy metric violation goes to the engineering team responsible for the change. A tone metric violation goes to the product team. The surge continues — and the team investigates — until either the metric stabilizes or the change is rolled back.

## Why Pre-Deploy Evaluation Is Necessary but Not Sufficient

Some teams, upon hearing about post-deploy surges, ask: "Why not just make pre-deploy evaluation more thorough so we don't need to surge in production?" This is a reasonable question with a clear answer: you should make pre-deploy evaluation thorough, and you still need to surge in production. The two are complementary, not substitutes.

Pre-deploy evaluation is necessary because it catches the majority of regressions before they reach users. A well-designed pre-deploy suite with a comprehensive golden set, diverse test cases, and multi-dimensional scoring catches eighty to ninety percent of regressions. This is enormous value. Shipping a model update without pre-deploy evaluation is reckless.

Pre-deploy evaluation is not sufficient because it operates on a fixed dataset in a controlled environment. The golden set, no matter how large, is a sample of production traffic. The staging environment, no matter how realistic, is a simulation of production conditions. The test queries, no matter how diverse, were written by people who knew what they were testing for. Production traffic includes queries nobody anticipated, user patterns nobody modeled, and interaction sequences nobody designed test cases for. It includes adversarial inputs, multilingual queries from unexpected locales, edge cases at the boundaries of the product's scope, and the long tail of rare-but-important requests that no test set can comprehensively cover.

The post-deploy surge covers this gap. It evaluates the new system against actual production traffic, at elevated rates, during the period when regressions are most likely to manifest. The golden set tells you "the change passes the tests we designed." The production surge tells you "the change survives contact with real users." Both signals are valuable. Neither alone is sufficient.

## Coordinating Surges Across Simultaneous Changes

At scale, changes don't happen one at a time. An engineering team might update a prompt template on Monday, rebuild the retrieval index on Tuesday, and ship a model version update on Thursday. If each change triggers an independent surge, the evaluation system is running at elevated rates continuously, which defeats the purpose of a targeted temporary increase and inflates eval costs permanently.

The solution is surge coordination. When multiple changes overlap, the evaluation system should merge the surges rather than stack them. The merged surge runs at the maximum multiplier of any active change type. If the prompt change triggered a ten-times surge and the model update triggers a twenty-times surge, the combined rate is twenty times, not thirty times. Each change maintains its own stabilization criterion. The overall surge ends only when all active changes have independently stabilized. If the prompt change stabilizes on Wednesday but the model update is still showing metric fluctuations on Friday, the surge continues at the model-update multiplier until that change also stabilizes.

This coordination requires your evaluation system to maintain a registry of active changes, each with its type, start time, multiplier, and stabilization status. The registry is the single source of truth for the current surge state. When engineering asks "why is sampling at twenty times right now," the registry answers with a list of active changes and their stabilization status. This transparency is important for organizational buy-in, because eval costs during a surge are visible and someone will always ask why the eval bill tripled this week.

## The Organizational Requirement

Change-based sampling requires something that many teams struggle with: discipline around change notification. The evaluation system can only surge when it knows a change happened. If an engineer updates a prompt template through a quick commit that bypasses the standard deployment pipeline, the eval system doesn't know. If an operations team adjusts a configuration value through a direct database edit, the eval system doesn't know. If a third-party model provider silently updates their model — which happened multiple times in 2024 and 2025 with major API providers making unannounced updates to their hosted models — your eval system doesn't know and can't surge.

The first two cases are organizational problems with organizational solutions. Every change to the AI system, no matter how small, must flow through a system that notifies the evaluation pipeline. This is the same principle as requiring all code changes to go through version control, and it requires the same organizational discipline. Teams that tolerate "quick fixes" that bypass the change notification system will eventually suffer a regression that the surge system was designed to catch but couldn't, because nobody told it to surge.

The third case — unannounced provider model updates — requires a different defense. If you rely on a hosted model API, you should run a continuous baseline evaluation against a fixed reference set, independent of any deployment activity. When the baseline scores shift without any internal change, you know the provider updated the model. This external change detection is a form of always-on evaluation that complements change-based sampling by catching the changes you didn't make yourself.

Change-based sampling protects you during the moments you know are dangerous — the deployments and updates you plan and execute. But some of the most damaging quality degradations don't follow a planned change. They emerge from the unexpected: a shift in user behavior, a new category of input the system was never designed to handle, a subtle drift that accumulates over weeks. The next subchapter covers anomaly-triggered sampling — the practice of detecting unusual outputs in real time and routing them to deeper evaluation automatically, whether or not a change preceded them.
