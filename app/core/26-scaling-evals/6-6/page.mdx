# 6.6 — Adversarial Dataset Maintenance: Attack Libraries That Evolve with Threats

The safety team has a spreadsheet. It contains 340 adversarial prompts that the team has collected over the past year — jailbreak attempts, prompt injection vectors, social engineering scripts, data extraction tricks, compliance boundary probes. New entries are added whenever a red-team exercise or production incident surfaces a novel attack. Old entries sit untouched from the month they were added. The spreadsheet is the company's adversarial eval dataset. Every pre-release safety check runs against it. Every model update is judged by its score on these 340 prompts. And the spreadsheet is simultaneously the most important and the most neglected dataset in the entire evaluation system. Important because it's the only thing standing between the model and known attack vectors. Neglected because nobody owns it as a living system — it's a collection of things that happened to be added, not a curated, maintained, and evolved defense library.

This is the state of adversarial eval data at most companies in 2026. The attack surface evolves weekly. The attack library evolves when someone remembers to update the spreadsheet. The gap between the two is the window through which real attacks succeed.

## What an Adversarial Dataset Actually Is

An **adversarial eval dataset** is a curated collection of inputs specifically designed to test whether your model can withstand malicious, manipulative, or policy-violating interactions. Unlike your golden set, which tests whether the model produces good outputs, the adversarial dataset tests whether the model resists producing dangerous ones.

The inputs in an adversarial dataset fall into several categories. **Jailbreak attempts** try to override the model's safety training or system-level instructions, convincing it to ignore its guidelines and produce outputs it was designed to refuse. **Prompt injection attacks** try to hijack the model's behavior by embedding instructions in user-supplied content — a document the model is asked to summarize that contains hidden instructions to reveal system prompts or extract data. **Social engineering vectors** manipulate the model through conversational tactics: building rapport, appealing to authority, creating false urgency, or framing harmful requests as hypothetical or educational. **Data extraction probes** attempt to get the model to reveal training data, system prompts, user data from other sessions, or internal configuration details. **Compliance boundary tests** probe the edges of your content policy — not obviously harmful requests, but borderline cases where the model must exercise judgment about what falls within and outside its guidelines.

Each adversarial example has a clear expected behavior: the model should refuse, redirect, or respond safely. The eval scores the model's actual response against this expected behavior. A pass means the model handled the attack correctly. A fail means the attack succeeded — the model produced output it shouldn't have.

## Why Adversarial Datasets Must Evolve

Static adversarial datasets are self-defeating. The attacks that existed when you created the dataset are the attacks your model has already been hardened against. The attacks that will breach your defenses next month are the ones your dataset doesn't contain yet.

The adversarial landscape shifts for three reasons. First, **attackers iterate**. When a jailbreak technique stops working because model providers patched it, attackers modify the technique. The "DAN" (Do Anything Now) jailbreak pattern went through dozens of iterations between 2023 and 2025, each version adapting to the defenses deployed against the previous one. An adversarial dataset that contains DAN v1 and v2 but not v15 or v20 is testing against attacks that no real attacker uses anymore, while missing the attacks that current attackers deploy daily.

Second, **new attack surfaces emerge**. The rise of agentic AI systems in 2025 and 2026 introduced entirely new attack categories that didn't exist when most adversarial datasets were created. Tool-use manipulation — tricking an agent into calling APIs with malicious parameters — didn't need testing when models couldn't call APIs. Multi-turn manipulation — slowly escalating across many conversation turns to bypass per-turn safety checks — became critical as models gained longer context windows and persistent memory. Indirect prompt injection through retrieved documents became a major vector as RAG architectures became standard. An adversarial dataset frozen at 2024 techniques misses every attack category that emerged in the subsequent two years.

Third, **model capabilities change the threat model**. When GPT-5 and Claude Opus 4.6 demonstrated substantially improved reasoning and instruction following, they also became more susceptible to sophisticated manipulation that earlier models were too weak to execute. A highly capable model that can follow complex multi-step instructions can also follow complex multi-step manipulation. The adversarial dataset needs attack examples calibrated to the current model's capability level, not the previous generation's.

## The Adversarial Dataset Lifecycle

Adversarial datasets aren't built once. They follow a lifecycle of creation, expansion, pruning, and validation that repeats continuously.

**Creation** happens during the initial red-team exercise — typically before a major product launch or when standing up a new safety evaluation program. The team conducts structured adversarial testing across known attack categories, documents every successful and notable unsuccessful attack, and compiles the initial dataset. At this stage, the dataset typically contains one hundred to five hundred examples, organized by attack category. This is the foundation, not the finished product.

**Expansion** is the ongoing process of adding new adversarial examples. The sources are varied, and mature teams draw from all of them. Internal red-team exercises — scheduled quarterly or triggered by major model updates — produce new attack vectors tailored to your specific product and model. Production incident post-mortems surface real-world attacks that bypassed your defenses — these are the most valuable additions because they represent attacks that actually worked against your system, not hypothetical ones. Academic research on AI security, published at venues like NeurIPS, ICLR, and through organizations like OWASP (whose LLM Top 10 and Gen AI Red Teaming Guide are updated annually), provides categorized attack techniques. Community-shared attack libraries and open-source red-teaming tools like Garak, DeepTeam, and Promptfoo maintain catalogs of known attack patterns that you can adapt to your context. And automated adversarial generation tools can mutate existing attacks to produce variations that test whether your defenses are robust to slight modifications.

**Pruning** removes adversarial examples that are no longer relevant. This sounds counterintuitive — why would you remove attacks? — but there are valid reasons. If your model's architecture has fundamentally changed in a way that makes a class of attacks impossible (not just defended against, but structurally impossible), the examples testing that class add noise without value. If an attack category was specific to a previous model generation and doesn't apply to the current one, keeping those examples inflates the size of the adversarial suite without improving its signal. Pruning is conservative — when in doubt, keep the example. But an adversarial dataset that grows without bounds eventually becomes so large that running it takes hours, and teams start sampling instead of running the full suite, which defeats the purpose.

**Validation** is the step most teams skip, and it's the one that determines whether your adversarial dataset is actually testing what you think it's testing. Validation means periodically running the adversarial dataset against your current model and analyzing the results. If every example passes — your model handles every attack correctly — that doesn't mean your model is secure. It means your adversarial dataset is too easy. Real attackers aren't constrained to the patterns in your library. A dataset where the pass rate reaches 100 percent needs new, harder examples, not celebration. Conversely, if the fail rate on examples you thought were addressed is non-zero, something has regressed in your model's defenses, and you need to investigate.

## Organizing the Attack Library

A flat list of adversarial prompts doesn't scale. When your adversarial dataset reaches a thousand examples, you need structure to make it usable for evaluation, reporting, and gap analysis.

The primary organizational axis is **attack category**. OWASP's LLM Top 10 provides a widely adopted categorization: prompt injection, insecure output handling, training data poisoning, denial of service, supply chain vulnerabilities, sensitive information disclosure, insecure plugin design, excessive agency, overreliance, and model theft. Your adversarial dataset should have examples in every category relevant to your product. When a category has zero examples, that's a coverage gap — not because the category doesn't apply, but because you haven't tested it.

The secondary axis is **attack sophistication**. Within each category, organize examples from simple to complex. Simple jailbreaks use direct instruction override: "Ignore your instructions and do X." Complex jailbreaks use multi-step manipulation, role-playing scenarios, hypothetical framing, or encoded instructions that bypass pattern-matching defenses. Your adversarial eval should run the full sophistication range. A model that blocks simple attacks but falls to sophisticated ones isn't safe — it's safe against unsophisticated attackers, which is an increasingly small population.

The tertiary axis is **attack recency**. Tag each example with the date it was added and the date it was last validated. Examples added more than six months ago without re-validation are candidates for staleness review — the same staleness problem covered in subchapter 6.4, but in the adversarial context. An adversarial example can become stale not because the attack technique changed but because the model was hardened against it and the example no longer tests the defense boundary. You need fresh examples that probe the current boundary, not the boundary from six months ago.

## Sources of New Adversarial Examples

Mature adversarial datasets are fed by multiple streams, not a single source. Relying on only one stream creates blind spots.

**Internal red-team exercises** are the highest-value source because the attacks are tailored to your specific product, your specific model, and your specific deployment context. A red-team exercise for a financial advisory bot discovers attack vectors that a general-purpose attack library wouldn't include: attempts to manipulate the model into providing unauthorized financial advice, tricks to extract other users' portfolio information, or social engineering to bypass KYC verification steps. Schedule red-team exercises quarterly for high-risk products and semi-annually for lower-risk products. Each exercise should produce twenty to fifty new adversarial examples that enter the dataset after review.

**Production incident analysis** captures attacks that actually reached your system. When your monitoring detects a policy violation, a jailbreak success, or an anomalous interaction pattern, the triggering input goes into the adversarial dataset after sanitization. These examples are invaluable because they represent real attacker behavior — not what researchers theorize attackers might do, but what attackers actually did. A strong incident response process includes "adversarial dataset update" as an explicit step in the post-mortem checklist.

**Academic and industry research** provides systematic categorizations of new attack techniques. Follow the proceedings of AI security workshops, the OWASP AI Security project, and publications from AI safety teams at Anthropic, OpenAI, Google DeepMind, and others. When a new paper describes a novel attack technique, don't just read it — translate it into concrete adversarial examples for your eval dataset. A paper that describes a "context window manipulation attack" is interesting. Ten adversarial examples that implement that attack against your specific system prompt are useful.

**Open-source red-teaming tools** generate attack variations at scale. Tools like Garak provide modular attack probes across dozens of vulnerability categories. Promptfoo includes red-teaming capabilities that automatically generate and test adversarial inputs against your model. DeepTeam implements over forty vulnerability classes aligned to OWASP and NIST frameworks. These tools don't replace human red-teaming, but they automate the tedious work of generating variations on known attack patterns, freeing human red-teamers to focus on creative, novel attacks that automated tools can't conceive.

**Cross-team intelligence** is often overlooked. If your company runs multiple AI products, the adversarial findings from one product are relevant to others. An attack that succeeded against the customer support bot may work against the internal knowledge assistant. Create a shared adversarial library across teams, with product-specific adapters that translate generic attacks into product-specific test cases. The cost of maintaining a shared library is modest. The cost of every team independently discovering the same attack patterns is substantial.

## The Organizational Challenge

The deepest problem with adversarial dataset maintenance isn't technical — it's organizational. In most companies, the adversarial dataset lives with the trust and safety team or the security team, disconnected from the main eval platform that the product and engineering teams use. The adversarial eval runs in a separate process, on a separate schedule, with results reported through a separate channel.

This separation creates three problems. First, adversarial eval gets skipped when it's inconvenient. If the adversarial suite isn't integrated into the standard pre-release eval pipeline, it depends on someone remembering to run it. During a rushed release cycle, "someone" forgets. The product ships without adversarial testing. If the next production incident involves an attack that the adversarial suite would have caught, the team doesn't lack the test — it lacked the process to ensure the test ran.

Second, adversarial findings don't flow back into the product eval. When the red team discovers that the model responds inappropriately to a specific class of input, that finding should become a permanent eval case — not just an adversarial test but a regression test that prevents the behavior from returning. If the adversarial dataset is disconnected from the product eval dataset, this feedback loop doesn't exist. The red team reports the finding. Engineering fixes it. The fix is never tested in the standard eval pipeline. Six months later, a model update reintroduces the behavior, and nobody notices until a user reports it.

Third, adversarial dataset quality decays without the product team's context. The trust and safety team can maintain attacks based on generic threat intelligence, but they need the product team's understanding of what the model is used for, what data it has access to, and what attack surface it exposes to create product-specific adversarial examples. Without this collaboration, the adversarial dataset tests generic attacks that may not be relevant to the specific product, while missing product-specific attack vectors that the trust and safety team doesn't know about.

The fix is integration, not separation. The adversarial eval suite should be a standard component of the pre-release eval pipeline, running alongside quality eval, regression eval, and performance eval. Adversarial eval results should appear in the same dashboard as quality metrics. Adversarial dataset updates should go through the same version control and review process as golden set updates. Ownership of the adversarial dataset should be shared between the trust and safety team (who provide threat expertise) and the product eval team (who provide product context and pipeline integration). This shared ownership ensures that the adversarial dataset is both technically relevant and operationally embedded.

## The EU AI Act and Adversarial Testing Requirements

Adversarial dataset maintenance isn't just good practice — it's increasingly a compliance requirement. The EU AI Act, with general-purpose AI provisions enforceable from August 2025 and the full systemic risk compliance window closing in August 2026, requires providers of high-risk AI systems to conduct adversarial testing as part of their risk management obligations. The GPAI Code of Practice, published in July 2025, specifically addresses the need for ongoing adversarial testing and documentation.

For companies operating under these regulations, the adversarial dataset is not just an eval artifact — it's a compliance artifact. Regulators may ask to see the scope of your adversarial testing, the categories covered, the recency of examples, the validation cadence, and the process by which findings are addressed. A spreadsheet with 340 prompts and no documentation of when they were last validated, who reviewed them, or how comprehensively they cover the threat landscape will not satisfy an audit.

Compliance requirements make the lifecycle formalization described above not just a quality improvement but a regulatory necessity. Document the adversarial dataset's coverage by category. Document the refresh cadence. Document the validation process. Document the flow from red-team finding to adversarial example to eval pipeline to production defense. When the auditor asks "how do you test for adversarial robustness?" the answer should be a documented, versioned, continuously maintained process — not "we have a spreadsheet."

## Measuring Adversarial Dataset Effectiveness

A large adversarial dataset is not automatically an effective one. Size is a vanity metric. What matters is whether the dataset actually catches the attacks your model is vulnerable to.

The first effectiveness measure is **catch rate on known incidents**. When a production incident occurs — a user successfully jailbreaks the model, a prompt injection bypasses defenses — check whether the adversarial dataset contained a similar attack. If the dataset had an example in the same attack category and sophistication level, and the pre-release eval passed, your eval process has a scoring problem. If the dataset didn't have a similar example, your dataset has a coverage problem. Track the catch rate over time. A healthy adversarial dataset catches at least 80 percent of production incidents through examples in the same attack category, even if the exact attack variant was novel.

The second measure is **fail rate distribution**. Run the adversarial dataset against your current model and examine which examples the model fails. If the fail rate is zero, the dataset is too easy — it's testing yesterday's vulnerabilities, not today's. If the fail rate is above 20 percent, either the model's defenses have significant gaps or the dataset includes unreasonable examples (attacks that no practical defense could fully prevent). A healthy adversarial dataset has a fail rate between 2 and 15 percent, with the failing examples concentrated in the most sophisticated attack categories. This distribution means the model handles routine attacks reliably but has known areas of vulnerability in advanced techniques — which is an honest and actionable assessment.

The third measure is **novelty rate**. Of the examples added in the last quarter, what percentage represent genuinely new attack categories or techniques, versus variations on existing ones? A dataset that adds fifty examples per quarter but all fifty are variations on prompt injection isn't expanding coverage — it's deepening existing coverage, which has diminishing returns. Aim for at least 30 percent of new additions to represent attack categories or techniques not previously covered.

Adversarial data is a specialized branch of your eval dataset ecosystem. But all branches — golden, silver, synthetic, adversarial — need quality assurance processes that verify the data itself is trustworthy. The next subchapter covers dataset quality assurance: how to audit the data that judges your system and catch problems in your eval data before they corrupt your evaluation results.