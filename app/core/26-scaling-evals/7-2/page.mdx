# 7.2 — The Routing Strategy: What Gets Human Eyes and What Does Not

At scale, the most important decision in human review is not how to review. It is what to review. Every output you send to a human reviewer has an opportunity cost — that reviewer is not reviewing something else. A routing strategy that sends the wrong outputs to human review wastes twice: once by spending budget on outputs that automated evaluation could handle, and again by starving the outputs that actually needed human judgment.

## The Review Router

**The Review Router** is the automated system that classifies each output into one of three paths: automated evaluation only, human review required, or human review recommended but deferrable. This classification happens in real time, based on signals available at the moment the output is generated. The router is not a simple threshold — it is a multi-signal decision system that considers risk, confidence, novelty, and strategic value.

Without a router, teams default to one of two bad strategies. Either they route a fixed percentage of outputs to human review — which wastes budget on routine outputs while under-sampling critical ones — or they route nothing, relying entirely on automated evaluation and losing the calibration signal that keeps automated systems accurate. A content moderation team at a social media company routed 10% of all outputs to human review for two years. When they analyzed the results, they found that 87% of the reviewed outputs were routine cases where automated judges agreed with humans, while the hardest 5% of outputs — the ones with genuine ambiguity — were under-sampled by a factor of four. They were spending $22,000 per month on reviews that told them almost nothing they didn't already know.

## What Should Always Get Human Review

Certain output categories justify human review regardless of automated evaluation confidence. The non-negotiable categories fall into four groups, and skipping any of them creates blind spots that compound over time.

Safety-critical outputs — responses involving medical advice, legal guidance, financial recommendations, or content moderation decisions — should always have a human review path because the cost of an undetected failure is asymmetrically high. A false negative on a safety check might cost hundreds of thousands in liability. A false negative on a routine quality check costs almost nothing. The expected value of human review is proportional to the cost of the failure it prevents.

Outputs where automated judges disagree should be routed to human review because disagreement signals ambiguity that only a human can resolve. When Judge A says an output scores 4 and Judge B says it scores 2, the automated system cannot determine who is right without a human tiebreaker. More importantly, these disagreements are the richest source of calibration data for improving your judges. Every resolved disagreement teaches your automated system something specific about its blind spots.

Outputs from newly deployed model versions should be oversampled for human review during the post-deploy surge period. A new model version may behave differently in subtle ways that automated judges, calibrated on the previous version's outputs, fail to detect. Increasing human review to 5-10% of volume for the first seventy-two hours after a deploy provides the early warning system that catches regressions before they affect thousands of users.

Outputs that triggered anomaly detection — unusual length, unexpected language, format violations, or topic drift — should also reach human eyes. Anomalies may be harmless novelty or they may be early indicators of systematic failure. Automated systems are poor at distinguishing between the two, especially for failure modes they haven't encountered before.

## What Should Rarely Get Human Review

High-confidence routine outputs — where automated judges agree, the output matches well-tested patterns, and historical human review of similar outputs showed near-perfect agreement with automated scores — are poor candidates for human review. Reviewing them generates little new information and consumes budget that could be spent on harder cases. A customer support AI that handles password reset instructions produces highly templated outputs where quality variation is minimal. Sending these to human review is like quality-testing identical widgets on an assembly line after you've already verified the machine works.

This does not mean zero human review for routine outputs. A small random sample of "automated only" outputs should still be routed to human review periodically, specifically to validate that the routing strategy itself isn't missing problems. This **validation sample** is typically 0.1-0.5% of automated-only volume — enough to detect systematic routing failures without consuming significant budget. Think of it as the smoke detector for your routing system. It runs in the background, almost never triggers, but when it does, you're glad it was there.

## The Gray Zone and How to Prioritize It

Between "always review" and "never review" lies a large gray zone of outputs where human review would be valuable but the budget is finite. Outputs with moderate judge confidence, outputs in well-established use cases but with unusual input characteristics, outputs where automated scores are passing but marginal — all of these could benefit from human review, and all of them compete for the same limited budget.

Prioritization within the gray zone requires a **gray zone scoring function** that combines multiple signals into a single priority score. Judge confidence contributes the most weight — lower confidence means higher value of human review. A judge that reports 55% confidence on a pass/fail decision is essentially guessing, and human review of that output has high marginal value. A judge at 95% confidence adds little from human verification.

Recency of the model version matters — newer deployments get priority because you have less historical data on the new model's behavior and every human review provides more information per dollar. User tier matters — enterprise customer outputs get priority over free-tier outputs because enterprise customers generate higher revenue impact per quality incident. And strategic value matters — outputs from product areas that are actively being improved get priority because human feedback there directly accelerates development velocity.

The scoring function should produce a numeric score between 0 and 100, and the review router should fill available human capacity from highest score to lowest. When capacity is constrained, the lowest-scoring gray zone items simply don't get reviewed — and that's acceptable because they're the items where human review adds the least incremental value.

## Routing Signal Types in Detail

The router consumes several signal families, each providing a different view of whether an output warrants human attention. Understanding these signals individually matters because each has different reliability characteristics.

Automated judge confidence scores are the primary routing signal. Confidence can be extracted from the judge's probability distribution, from explicit confidence statements when using prompt-based judges, or from calibrated confidence scores when using fine-tuned judge models. The critical detail is that confidence must be calibrated — a judge that reports 90% confidence but is only right 70% of the time is worse than useless as a routing signal because it creates false comfort. Chapter 3 covers confidence calibration in depth.

Anomaly detection flags from the real-time scoring stack (Chapter 4) trigger routing for unusual outputs. These flags detect distributional shift — outputs that fall outside the statistical profile of what the model typically produces. Length anomalies, vocabulary anomalies, structural anomalies, and topic anomalies each capture a different type of deviation.

Risk classification from the sampling strategy (Chapter 2) identifies outputs in high-risk categories based on the input characteristics, the user profile, or the use case context. An output generated in response to a query about medication dosage carries inherently higher risk than one about restaurant recommendations, regardless of what the automated judge thinks about its quality.

User-reported issues create direct escalation paths. When a user flags an output as problematic, that output should bypass the routing algorithm entirely and go straight to human review. User complaints are among the highest-signal indicators of quality failure, and they carry the additional urgency of customer impact.

Novelty detection — comparing output patterns against historical distributions — identifies outputs that don't match any pattern the automated system has seen before. These are the unknown unknowns. Your judges may score them confidently, but that confidence is unreliable because the judge has never been validated on similar outputs. Human review of novel outputs extends the validated coverage of your automated evaluation.

## Measuring Detection Yield

The routing strategy is not static. Its effectiveness must be measured and refined continuously. The primary metric is **detection yield** — what percentage of outputs routed to human review actually reveal quality issues that automated evaluation missed. Detection yield is the closest thing to a single number that tells you whether your routing is working.

If detection yield is below 10%, the router is sending too many routine outputs to human review. Your budget is being spent on confirmations rather than discoveries. Tighten the routing thresholds to filter out more easy cases.

If detection yield is between 15% and 35%, you're in the productive zone. Human reviewers are spending most of their time on outputs that genuinely benefit from human judgment, while still seeing enough clean outputs that they maintain calibration context.

If detection yield is above 50%, the router may be too conservative, and problematic outputs that aren't flagged may be slipping through undetected. The router is only catching the most obvious cases and missing the borderline ones. Loosen the thresholds and validate by routing a sample of non-escalated outputs to human review.

Periodically routing a sample of "automated only" outputs to human review provides the data needed to calibrate the router. If human reviewers find significant issues in the automated-only sample — even 2-3% of items showing quality problems — the routing thresholds need to be tightened. If the automated-only sample shows clean results across multiple review cycles, the current routing is working and may even be loosened further to free up budget.

## Evolving the Router Over Time

The weighting of routing signals should evolve as your evaluation system matures. Early in your human review program, you may weight confidence heavily because your automated judges are not yet well-calibrated and confidence is the most direct indicator of uncertainty. As calibration improves and confidence becomes more reliable, you shift weight toward novelty and risk signals, using human review more strategically to explore the edges of your evaluation coverage rather than backfilling gaps in the middle.

Track the signal contribution quarterly. If anomaly detection flags are generating high detection yield but confidence-based routing is not, shift budget toward anomaly-triggered reviews. If enterprise customer outputs consistently reveal issues that other signals miss, increase the weight of user tier in the scoring function. The router is a living system that improves as you accumulate data about what actually needs human eyes.

The routing strategy connects directly to the workforce design that follows — because what you choose to route determines what kind of reviewers you need.
