# 2.8 — The Sampling Budget: Allocating Eval Capacity Across Products, Features, and Risk Tiers

In early 2025, a SaaS company with four AI-powered products had a total evaluation budget of $18,000 per month. Their most visible product — an AI writing assistant used by marketing teams — consumed 85% of that budget because it generated the most traffic. The writing assistant received thorough evaluation coverage: stratified sampling across user segments, risk-based oversampling of enterprise accounts, weekly golden set refreshes. Their compliance product, which served regulated healthcare clients and generated just 5% of total requests, received 3% of the eval budget. The compliance team had flagged the imbalance twice, but the allocation followed traffic volume, and traffic volume said the writing assistant was more important. When a compliance failure slipped through undetected — the model began generating medication dosage summaries that omitted critical contraindication warnings — the resulting remediation cost the company $280,000 in incident response, customer notification, and regulatory review. That single incident cost more than the entire annual eval budget.

The allocation was not random. It followed a reasonable-sounding principle: evaluate proportionally to traffic. The problem is that proportional allocation ignores the one variable that matters most. Risk.

## The Finite Budget Reality

Evaluation capacity is finite. You have a monthly dollar amount for judge API calls, a fixed number of human review hours, a limited compute budget for running eval pipelines, and a constrained engineering team to maintain the whole system. Every evaluation you run on one product is an evaluation you cannot run on another. This makes eval allocation a zero-sum game within your budget envelope — and zero-sum games require explicit allocation decisions, not defaults.

Most organizations never make that explicit decision. They let eval allocation emerge organically. The product with the most vocal engineering team gets the most eval coverage. The product that launched first has eval infrastructure that newer products lack. The product that caused the last incident gets a temporary budget increase that becomes permanent. Over time, allocation drifts toward path dependency rather than risk alignment. The products that need the most evaluation are often not the products that receive the most evaluation.

The consequences of misallocation are asymmetric. Over-evaluating a stable, low-risk product wastes money but causes no harm. Under-evaluating a high-risk product saves money until the day it doesn't — and that day's cost typically dwarfs the savings. The healthcare compliance product from the opening example saved roughly $540 per month by receiving minimal eval coverage. The single failure cost 520 times that amount. This asymmetry is why eval budget allocation cannot follow traffic volume, revenue contribution, or engineering team size. It must follow risk.

## The Eval Budget Allocation Framework

**The Eval Budget Allocation Framework** determines how evaluation capacity is distributed across products, features, and risk tiers using three factors: risk exposure, traffic volume, and rate of change. Each factor contributes to the allocation, but they are not weighted equally. Risk exposure dominates, traffic volume adjusts, and rate of change modifies.

**Risk exposure** is the primary allocation driver. A product's risk exposure is determined by the consequences of a quality failure in that product. What happens if this product produces a wrong answer? A writing assistant that generates an awkward marketing headline costs the user ten minutes of editing. A compliance product that omits a medication contraindication costs patient safety, regulatory standing, and potentially lives. Risk exposure is not subjective — it maps directly to the risk tiers your organization has already defined. If your risk framework classifies products into critical, high, medium, and low risk tiers, those tiers drive the base allocation.

A starting allocation that works for many organizations dedicates 40% of the eval budget to critical-risk products, 30% to high-risk, 20% to medium-risk, and 10% to low-risk. This is a starting point, not a formula. The exact percentages depend on how many products fall into each tier. If you have one critical-risk product and fifteen low-risk products, the critical product might still warrant 40% of the budget by itself. If you have five critical-risk products, 40% divided five ways might not be enough, and you need to either increase the total budget or accept reduced coverage on lower tiers.

**Traffic volume** adjusts the allocation within risk tiers. Among three high-risk products, the one processing 500,000 requests per day needs more evaluation capacity than the one processing 5,000 requests per day — simply because the larger traffic base requires a larger absolute sample to achieve the same statistical confidence. But volume adjustment happens after risk allocation, not before. The high-risk product with 5,000 daily requests still receives far more eval budget per request than a low-risk product with 500,000 daily requests. Risk sets the tier. Volume adjusts within the tier.

**Rate of change** modifies allocation based on how actively a product is being modified. A product that deployed three model updates in the last month needs more eval coverage than a product running the same model version for six months. Rate of change captures the principle from change-based sampling at the budget allocation level: change creates risk, and risk justifies evaluation. Products in active development, products expanding to new markets or languages, and products undergoing model migrations should receive a temporary budget uplift — typically 1.5x to 2x their baseline allocation — funded by reducing allocation to products in a steady state.

## The Baseline Guarantee

Every product, every feature, every risk tier gets a minimum eval allocation. This is the baseline guarantee, and it is non-negotiable.

The baseline guarantee exists because catastrophic failures do not respect budget allocations. A low-risk, low-traffic product that receives zero evaluation is a product operating blind. It might run perfectly for a year. It might silently degrade in week two. Without any evaluation, you do not know which scenario you are in. The baseline guarantee ensures that even the smallest, lowest-risk product receives enough evaluation to detect catastrophic quality failures within a reasonable time window.

What constitutes "enough" depends on the product's traffic and risk profile, but a useful rule of thumb is this: every product should receive enough evaluated samples per week to detect a 20-percentage-point quality drop with 95% confidence. For most products, this means at minimum 100 to 200 evaluated outputs per week. At a judge cost of $0.02 per evaluation, that is $8 to $16 per week — trivial in the context of any evaluation budget. The baseline guarantee is cheap. Skipping it is expensive.

The baseline guarantee also serves a political function within organizations. Without it, low-priority products become zero-priority products. Their eval allocation gets raided whenever a higher-priority product needs more budget. The baseline guarantee creates a floor that cannot be breached by reallocation decisions. A product owner can say, "I understand we are not the highest priority, but the baseline guarantee ensures my product is not flying blind." This protects the organization from the shortsighted optimization of concentrating all evaluation on the highest-profile product while leaving everything else unmonitored.

## Dynamic Rebalancing

Static allocation — set the budget once, never change it — fails for the same reason static sampling rates fail. The system changes. Products launch. Products sunset. Model updates shift quality profiles. User behavior evolves. An allocation that was perfectly calibrated in January becomes misaligned by March.

Dynamic rebalancing adjusts eval budget allocation on a regular cadence — typically monthly, though organizations with rapid product development might rebalance biweekly. The rebalancing considers three inputs: what changed in the product landscape, what the evaluation data showed, and what incidents occurred.

Product landscape changes drive structural rebalancing. When a new AI product launches, it needs eval budget. Where does that budget come from? Either the total budget increases — which requires organizational approval — or existing products give up allocation. The allocation framework determines who gives up what. Products in a steady state with consistently high quality scores are the first candidates for reduced allocation. Products that recently completed a model migration and stabilized are the second candidates. Products in active development or products with recent quality incidents should not be reduced.

Evaluation data drives efficiency-based rebalancing. If a product's quality scores have been consistently above threshold for three months with no incidents, its allocation might be over-serving. Reducing it by 20% and redirecting that budget to a product with borderline quality creates more detection value per dollar. The adaptive sampling controller handles this at the segment level within a product. Dynamic rebalancing handles it at the product level across the portfolio.

Incident history drives risk-based rebalancing. A product that experienced a quality incident in the last quarter should receive elevated eval budget regardless of what the allocation framework says. The incident revealed that the previous allocation was insufficient to prevent the failure. Increasing allocation after an incident is not punitive — it is corrective. The product demonstrated a risk level that its previous allocation did not adequately cover. Maintaining the elevated allocation for at least one quarter after an incident gives the team time to understand the failure mode, improve the product, and confirm that quality has stabilized at a higher level.

## The Organizational Challenge

Eval budget allocation is a resource allocation problem, and resource allocation problems are organizational problems. Product teams compete for eval resources the way they compete for engineering headcount, infrastructure budget, and management attention. Without clear allocation principles, the outcome is determined by organizational power rather than organizational need. The loudest team wins. The team with the most senior sponsor wins. The team whose product generated the most revenue wins. None of these proxies correlate with evaluation need.

The allocation framework creates an objective basis for decisions. When a product manager asks "why does the compliance product get more eval budget per request than my product?", the answer is concrete: "Because a quality failure in your product costs a user ten minutes. A quality failure in the compliance product costs us regulatory review and patient risk. The allocation reflects that asymmetry." This answer is defensible because it connects allocation to consequences rather than to politics.

But frameworks only work if someone enforces them. Without an owner for the allocation process, the framework becomes a document that nobody references when actual allocation decisions are made. The eval platform team or the central evaluation function — depending on your ownership model — should own the allocation process. They maintain the framework, run the monthly rebalancing, and present allocation recommendations. The actual allocation decision sits with engineering leadership, because it involves trade-offs between products that no single product team should arbitrate.

The hardest organizational conversation happens when the total eval budget is insufficient. If applying the allocation framework reveals that meeting the baseline guarantee for all products while adequately covering critical-risk products exceeds the available budget, you have a strategic decision: increase the budget, reduce coverage below acceptable levels, or reduce the number of products that receive evaluation. All three options have consequences. Increasing the budget requires justification. Reducing coverage accepts risk. Reducing evaluated products creates blind spots. The allocation framework does not make this decision easier, but it makes the trade-offs visible. Visible trade-offs get better decisions than invisible ones.

## The Monthly Eval Budget Review

The monthly eval budget review is the operational ritual that keeps allocation aligned with reality. It is not a status meeting. It is a decision meeting where allocation changes are proposed, debated, and approved.

The review has a standard agenda. First, the evaluation team presents the current allocation — how budget was distributed across products last month, how much of each allocation was actually used, and what the utilization rate was per product. Under-utilized allocations indicate over-serving and are candidates for reduction. Fully utilized allocations with unsampled requests indicate under-serving and are candidates for increase.

Second, the evaluation team presents quality incident data. Any quality failures detected through evaluation, any customer-reported issues that evaluation missed, and any near-misses where evaluation caught a problem before it reached production. Incidents that evaluation missed are the most important signal — they indicate that the current sampling strategy or allocation was insufficient for that product's risk profile.

Third, product teams present planned changes for the next month. Model updates, feature launches, market expansions, and any other changes that will affect their quality risk profile. These planned changes inform temporary allocation adjustments — the rate-of-change modifier in the allocation framework.

Fourth, the evaluation team presents a proposed reallocation. This is where trade-offs become explicit. "We recommend reducing allocation to Product A by 15% based on three months of stable quality, and redirecting that budget to Product D, which is launching a new multilingual feature next week." The engineering leadership approves, modifies, or rejects the proposal.

The monthly review takes 60 to 90 minutes. Organizations that skip it find that their allocation drifts into misalignment within two to three months. Organizations that conduct it consistently find that it becomes one of the most valuable recurring meetings on the engineering calendar — the one meeting where quality investment decisions are made with data rather than intuition.

## Cross-Product Eval Sharing

Eval budget allocation becomes more efficient when products share evaluation infrastructure. Shared infrastructure amortizes fixed costs — the eval platform, the judge model access, the result storage, the monitoring dashboards — across all products. This makes per-product evaluation cheaper, which means the same total budget buys more coverage.

The most significant shared cost is judge model access. If every product team provisions its own judge model API access, each team pays for minimum committed throughput independently. If the organization negotiates a single judge model contract with volume-based pricing, the aggregate throughput reduces the per-evaluation cost. A company running twelve AI products might see per-evaluation costs drop by 25 to 40 percent simply by consolidating judge model access under a single contract with volume discounts.

Shared evaluation pipelines are the second opportunity. The infrastructure to sample outputs, route them to judge models, collect scores, store results, and monitor pipeline health is largely the same regardless of what product the output came from. Building this infrastructure once and running all products through it eliminates the duplication cost of product-specific eval pipelines. The product-specific parts — judge prompts, scoring rubrics, golden sets, quality thresholds — remain product-owned. The generic parts — scheduling, routing, storage, monitoring — are shared.

Shared golden set infrastructure — the tools and processes for maintaining, versioning, and validating golden sets — is the third opportunity. Every product needs golden sets. Every golden set needs versioning, periodic review, and validation against current model behavior. Building the infrastructure to manage golden sets once and letting every product use it is dramatically more efficient than each product team building its own golden set management tooling.

The risk of sharing is coupling. When the shared eval platform goes down, every product loses evaluation coverage simultaneously. When the shared judge model API experiences latency, every product's evaluation pipeline slows down. Shared infrastructure requires shared reliability investment — the eval platform needs the same operational maturity as any production service. Uptime targets, incident response, redundancy, and disaster recovery become non-negotiable when a single platform serves the entire organization's quality infrastructure.

## When the Budget Is Simply Not Enough

Sometimes the allocation framework reveals that you cannot adequately evaluate everything you need to evaluate within the available budget. This is a signal, not a failure. It means your evaluation ambitions exceed your evaluation resources, and you need to make a strategic decision about what to do about it.

The first option is to increase the total eval budget. This requires building a business case that connects eval spend to business outcomes. The healthcare compliance incident from this subchapter's opening is exactly the kind of data that justifies budget increases. A $280,000 remediation cost caused by $540 per month in eval savings is an ROI argument that any CFO can understand. Track every quality incident, estimate its cost, and correlate it to evaluation coverage gaps. Over time, this data builds a compelling case for the eval budget your organization actually needs.

The second option is to increase eval efficiency so that the same budget buys more coverage. Cheaper judge models, faster eval pipelines, smarter sampling strategies, and adaptive sampling all reduce the cost per evaluation. A team that migrates from a premium judge model to a fine-tuned smaller judge model that maintains 95% agreement with the premium model can reduce per-evaluation cost by 60 to 80 percent — effectively tripling or quadrupling their eval budget without spending an additional dollar.

The third option is to explicitly accept risk on lower-priority products. This is the option nobody wants to choose, but it is sometimes the honest answer. If the budget supports thorough evaluation of your three critical-risk products and baseline evaluation of everything else, but not thorough evaluation of everything, document that decision. Make the accepted risk visible to leadership. "We are choosing to evaluate Product E at baseline coverage only. This means we will detect catastrophic failures within one week but may not catch gradual quality drift for a month or more. We accept this risk because Product E is low-risk and the alternative is reducing coverage on our compliance product." Documented risk acceptance is responsible engineering. Undocumented risk acceptance is negligence.

The fourth option, and the one that scales best over time, is to invest in automated evaluation capabilities that reduce the marginal cost of each additional evaluation. When your judge prompts are optimized, your eval pipeline is efficient, your sampling is adaptive, and your infrastructure is shared, the cost of evaluating one more output drops toward the cost of a single judge API call. At that marginal cost, evaluation budget constraints relax significantly. The teams that invest in eval infrastructure early find that their budget problems shrink over time. The teams that treat evaluation as an afterthought find that their budget problems grow.

---

Sampling determines what gets evaluated. But the evaluator itself — the LLM judge that scores each sampled output — is the single largest cost center in any scaled evaluation system. The next chapter tackles judge model selection, judge economics, and the architectural decisions that determine whether your evaluation system is financially sustainable at scale.
