# 5.4 — Parallelization Strategies: Running Thousands of Evals Concurrently Without Bottlenecks

The pre-release eval suite contains 3,200 test cases. Running them sequentially against an LLM judge takes fourteen hours. The release is supposed to ship today. The team needs results in under two hours. The engineering lead opens the pipeline configuration, changes the concurrency setting from one to fifty, restarts the run, and waits. Within ten minutes, every request is failing. The judge API provider has rate-limited the account. The retry logic kicks in, but retries are also rate-limited. The queue backs up. The pipeline starts dropping results because the timeout window expires before retries succeed. What was a fourteen-hour sequential problem is now a broken pipeline that produces no results at all.

This is the parallelization paradox. Sequential evaluation is too slow, but naive parallelization is worse than slow — it's unreliable. The solution is not "run more things at once." The solution is structured concurrency: a set of architectural patterns that maximize throughput while respecting the constraints that actually limit your pipeline. Those constraints are almost never compute. They are rate limits, network bandwidth, result ordering, and write contention. Understanding these constraints is the first step toward parallelization that actually works.

## Why Sequential Evaluation Cannot Scale

The math is simple enough. A single LLM judge call — sending the prompt, waiting for the model to generate a response, parsing the score — takes one to four seconds for typical evaluation prompts. Some complex multi-dimension evaluations take longer, especially when the judge needs to reason through a rubric with multiple criteria. Call it two seconds on average.

At two seconds per evaluation, a sequential pipeline processes thirty evaluations per minute. An eval suite of one thousand test cases takes thirty-three minutes. An eval suite of three thousand test cases takes one hundred minutes. An eval suite of ten thousand test cases takes over five hours. These numbers assume zero overhead for prompt assembly, result parsing, and storage — the real numbers are worse.

For a team running evals on every pull request, five hours is not acceptable. For a team running a pre-release gate that blocks deployment, five hours means half-day release cycles. For a team running nightly regression suites that need to finish before the morning standup, five hours of sequential execution against a growing test suite means the suite eventually outgrows its time window and starts getting trimmed — not because the tests are unnecessary, but because the pipeline can't keep up.

The pressure to parallelize comes from the same source as the pressure to add more tests: quality standards rise, the eval suite grows, and the pipeline must keep pace. Teams that don't invest in parallelization end up in one of two bad states. Either they constrain their eval suite to fit the sequential time budget, accepting worse coverage than they need, or they skip evaluations entirely when deadlines are tight, accepting the risk of shipping untested changes. Neither state is acceptable for a production evaluation system.

## The Three Levels of Parallelism

Not all parallelism is the same. Evaluation pipelines offer three distinct levels where concurrent execution can accelerate throughput, and each level has different constraints and different returns.

**Within-eval parallelism** applies when a single test case requires multiple scoring dimensions. If your eval rubric scores an output on accuracy, tone, completeness, and safety, running those four dimension scores sequentially against the same output means four judge calls in series — eight seconds at two seconds per call. Running them in parallel means all four calls execute simultaneously, completing in the time of the slowest single call. For multi-dimension evaluations, within-eval parallelism can reduce per-case latency by a factor of three to five with no additional API call volume. You're making the same number of calls; they just overlap.

The constraint on within-eval parallelism is that some scoring dimensions depend on others. If your safety score influences whether the accuracy score should be computed at all — for instance, if a safety-failing response should skip quality scoring entirely — you can't parallelize those dimensions. You need the safety result first, then launch the quality dimensions conditionally. Map out your dimension dependencies before assuming full within-eval parallelism is possible.

**Across-eval parallelism** is the most common form: running multiple test cases simultaneously, each with its own judge call or set of judge calls. Instead of processing test case one, then test case two, then test case three, you process all three at once. This is where the biggest throughput gains come from. A pipeline processing one thousand test cases at a concurrency of fifty finishes fifty times faster than sequential execution, minus overhead.

The constraint on across-eval parallelism is external: rate limits. Every LLM API provider enforces limits on how many requests you can send per minute or how many tokens you can consume per minute. If your provider allows six hundred requests per minute and each eval takes one request, a concurrency of fifty will consume six hundred of your rate budget in about twenty seconds, then stall until the rate window resets. The effective throughput is not determined by your concurrency setting — it's determined by your rate limit.

**Across-suite parallelism** operates at the highest level: running different eval suites concurrently. Your regression suite, your safety suite, your performance suite, and your cost suite don't need to run in sequence. They test different dimensions against different criteria. Launching them all in parallel reduces the wall-clock time from the sum of all suite durations to the duration of the longest single suite.

The constraint on across-suite parallelism is that all suites draw from the same rate-limit pool. Running four suites in parallel with each configured for one hundred concurrent workers means four hundred concurrent requests, which may exceed your provider's limits even though each individual suite was configured within bounds. Cross-suite rate limit coordination is essential — and it's the coordination layer that most teams forget until they hit the wall.

## Rate Limits Are the Real Bottleneck

Most teams discover the parallelization ceiling through a rate-limit error, not through a CPU utilization chart. In 2026, the primary constraint on evaluation throughput is not compute power, not memory, not disk — it's the number of requests per minute that your LLM judge provider allows. This is true whether you're using a frontier judge like GPT-5 or Claude Opus 4.6, a mid-tier judge like GPT-5-mini or Gemini 3 Flash, or a self-hosted open-weight judge with its own throughput limits.

Provider rate limits come in two forms that interact with each other. **Request-per-minute limits** cap the total number of API calls regardless of size. **Token-per-minute limits** cap the total input and output tokens processed. A pipeline that sends many small requests may hit the request limit first. A pipeline that sends fewer but larger requests — long evaluation prompts with extensive context — may hit the token limit first. You need to monitor both and design your concurrency to stay within whichever limit you hit first.

The effective throughput of your pipeline is the minimum of three numbers: your configured concurrency, your request-per-minute limit divided by sixty (to convert to requests per second), and your token-per-minute limit divided by your average tokens per request divided by sixty. That minimum is your ceiling. Setting concurrency above the ceiling doesn't make the pipeline faster — it generates a queue of requests waiting for rate-limit windows to reset, adding latency without adding throughput.

Strategies for maximizing throughput within rate limits include several that mature teams combine. **Request pooling** maintains a persistent pool of connections to the API, reusing connections across requests to minimize connection setup overhead. This doesn't increase your rate limit, but it reduces the per-request overhead that eats into your effective throughput. **Prompt compression** reduces the token count per request by stripping unnecessary context from judge prompts — the evaluation equivalent of minification. If you can reduce average prompt size from three thousand tokens to two thousand tokens, you get fifty percent more evaluations per token budget. **Response streaming** allows the pipeline to begin parsing judge output before the full response arrives, reducing end-to-end latency per evaluation and freeing up the connection slot sooner.

## Provider Rotation and Multi-Provider Strategies

When a single provider's rate limits are the bottleneck, the natural question is: can you use multiple providers? The answer is yes, with caveats that are worth understanding before you commit.

**Provider rotation** distributes eval requests across two or more LLM providers. If Provider A allows one thousand requests per minute and Provider B allows one thousand requests per minute, rotating between them gives you an effective budget of two thousand requests per minute. For teams whose eval throughput is genuinely rate-limited rather than budget-limited, this can double or triple effective parallelism.

The primary caveat is judge consistency. Different providers use different models with different scoring tendencies. A response that scores 4.2 on one provider's model might score 3.8 on another. If you're comparing scores across time — tracking whether quality improved or degraded — mixing providers introduces variance that has nothing to do with actual quality changes. One week your metrics might trend up simply because more requests happened to route to the more generous judge.

The mitigation is to use provider rotation within an eval run but not across eval runs. Within a single regression suite, rotate requests across providers to maximize throughput. But ensure that every test case in the comparison baseline was also evaluated with the same rotation strategy and the same provider mix. Alternatively, normalize scores per provider — calibrate each provider's scoring scale against a shared reference set and apply adjustment factors to bring scores into a common range. This adds complexity but eliminates the cross-provider scoring bias.

A second caveat is operational. Multi-provider setups require managing multiple API keys, multiple billing accounts, multiple rate-limit tracking systems, and multiple error-handling codepaths. When Provider A has an outage, your pipeline should automatically reroute to Provider B — but only if Provider B has sufficient headroom to absorb the additional load. The coordination logic is not trivial, and teams often underestimate the operational burden of maintaining two or three provider integrations compared to one.

## Worker Pool Architecture

The worker pool is the execution engine for parallelized evaluation. Each worker takes an eval job from a queue, assembles the judge prompt, sends the API request, parses the response, writes the result, and picks up the next job. The pool's size determines the concurrency level, and getting the size right is the difference between efficient execution and wasted capacity.

**Fixed-size pools** allocate a set number of workers at pipeline startup. If you configure twenty workers, you get twenty concurrent eval requests throughout the run. Fixed pools are simple, predictable, and easy to reason about. They're appropriate when your eval workload is consistent — same suite size, same provider, same rate limits every time. The downside is waste: if the suite has a long tail of slow evaluations, most workers finish early and sit idle while a few workers process the stragglers.

**Autoscaling pools** adjust worker count based on queue depth and rate-limit headroom. When the queue is deep and rate limits have headroom, the pool adds workers. When the queue is shallow or rate limits are approaching saturation, the pool removes workers. Autoscaling pools are more efficient for variable workloads but add complexity — you need monitoring, scaling policies, and cooldown periods to prevent thrashing. Kubernetes-based eval infrastructure naturally supports autoscaling through Horizontal Pod Autoscalers, but the autoscaling metric should be queue depth, not CPU utilization. Eval workers are I/O-bound, not compute-bound. CPU rarely exceeds ten to twenty percent even under full load.

The right pool size is derived from your rate limit, not from your hardware. If your provider allows six hundred requests per minute — ten per second — and each eval request takes two seconds of wall-clock time, then twenty workers will keep ten requests in flight at any given moment, consuming your full rate budget. Adding a twenty-first worker doesn't help because there's no rate headroom for it to use. The formula is: optimal workers equals rate limit per second multiplied by average request latency in seconds, plus a small buffer for variance. For ten requests per second and two-second latency, that's twenty workers plus two to four buffer workers for latency spikes.

## The Coordinator Pattern

At scale, you don't want each worker independently deciding what to evaluate next. You want a **coordinator** — a lightweight process that manages the flow of eval jobs from queue to worker to result store.

The coordinator's responsibilities are straightforward but critical. It pulls eval jobs from the queue and assigns them to available workers. It tracks which jobs are in progress and which have completed. It detects stalled workers — those that accepted a job but haven't returned a result within the expected time window — and reassigns their jobs to other workers. It aggregates results as they arrive, computing running statistics (pass rate, average score, failure count) that the pipeline can display in real time. And it decides when the eval run is complete — either all jobs finished, or the failure rate exceeded a configurable threshold and the run should be aborted early.

The coordinator pattern also handles priority. Not all evals in a run are equally important. Safety evaluations should complete before stylistic evaluations. Evaluations for the most-changed components should complete before evaluations for unchanged components. The coordinator can implement priority queuing, processing high-priority jobs first and filling remaining capacity with lower-priority work. This means that even if the full run takes ninety minutes, the most critical results are available in the first fifteen.

A common mistake is making the coordinator stateful in a way that can't recover from crashes. If the coordinator stores job assignments only in memory and crashes mid-run, the entire run must restart from scratch. The fix is a durable job store — a lightweight database or even a Redis instance — that records job state transitions. If the coordinator restarts, it reads the job store, identifies in-progress jobs that are past their timeout, and reassigns them. The eval run continues from where it left off, losing at most the work of the timed-out jobs.

## Avoiding Data Races in Result Storage

When fifty workers write eval results concurrently, the result storage layer must handle concurrent writes without data corruption, lost results, or phantom entries. This is a classic distributed systems problem, and the solutions depend on your storage backend.

For relational databases, the simplest approach is **idempotent writes with unique constraints**. Each eval result is keyed by a composite identifier: the eval run ID, the test case ID, and the scoring dimension. A unique constraint on this composite key means that duplicate writes — from a retried job, for instance — are rejected rather than creating duplicate entries. The worker uses an upsert operation: insert the result if it doesn't exist, update it if it does. This makes the write operation idempotent — safe to retry without side effects.

For document stores or object storage, the approach is similar but the mechanism differs. Each result is written to a path that encodes the run ID, test case ID, and dimension. If two workers both attempt to write the same result — perhaps because a job was reassigned after a timeout but the original worker eventually completed it — the second write overwrites the first. As long as both results are valid (the evaluation was actually performed), the overwrite is harmless. If you need to detect and resolve conflicting results — for instance, if retries might produce different scores due to model non-determinism — you can version the writes and keep both, flagging the conflict for review.

The subtler data-race problem is in aggregation. If a dashboard queries the result store mid-run to compute a pass rate, it reads whatever subset of results exists at that moment. The pass rate for a half-completed run is misleading — it might look worse than the final result if the easier test cases happen to finish last. The fix is to compute aggregates only over completed eval runs or to clearly mark partial aggregates as provisional. A dashboard that shows "67% pass rate (1,842 of 3,200 complete)" communicates the right information. A dashboard that shows "67% pass rate" without context invites the wrong conclusion.

## The Diminishing Returns of Parallelism

Parallelism has a ceiling, and pushing past it doesn't speed up the pipeline — it makes it worse. Understanding where the ceiling sits prevents wasted engineering effort and avoids the instability that comes from over-parallelization.

The first ceiling is the rate limit, as discussed. No amount of concurrency can exceed the throughput your API provider allows. Once you've sized your worker pool to saturate the rate limit, adding more workers just increases queue contention, memory consumption, and the blast radius of error cascading.

The second ceiling is the network. Each eval request involves sending a prompt to the API and receiving a response. For large prompts — five thousand to ten thousand tokens — each request consumes meaningful bandwidth. At high concurrency, the aggregate bandwidth can saturate a network link, especially in cloud environments where instances share network capacity. Symptoms include increasing request latency that doesn't correlate with API-side latency, suggesting the bottleneck is in the client's outbound or inbound network path.

The third ceiling is the result store. If fifty workers attempt to write results to a single database simultaneously, the database's write throughput becomes the limiting factor. A PostgreSQL instance can handle thousands of writes per second, so this is rarely the bottleneck for API-based evaluation. But if your result store is a shared file system or an under-provisioned cloud database, write latency can spike under concurrent load, slowing down workers that block on writes.

The practical approach is to profile your pipeline's throughput as you increase concurrency. Plot the relationship between worker count and evaluations per minute. The curve rises steeply at first, then flattens, then may decline. The flat point is your effective parallelism ceiling. Operating at or slightly below that point maximizes throughput without wasting resources or introducing instability. For most teams using a single LLM provider, the ceiling is determined by the rate limit, and the optimal worker count is a simple function of rate limit and average latency. Know your ceiling before you invest engineering effort in raising it.

## Parallelism for Self-Hosted Judges

Teams running self-hosted open-weight judge models face a different parallelism landscape. There are no external rate limits, but there are GPU throughput limits that behave differently from API rate limits.

A self-hosted judge running on a single GPU processes requests through a queue that batches inputs for efficient inference. The GPU has a maximum tokens-per-second throughput that depends on the model size, the hardware, and the inference engine. For a seven-billion-parameter judge model on an A100 GPU using vLLM, typical throughput is five hundred to two thousand tokens per second for output generation. An eval that requires two hundred output tokens takes one hundred to four hundred milliseconds of GPU time — much faster than the one to four seconds typical of API calls, because you've eliminated network latency and provider queuing.

This faster per-request time means self-hosted judges achieve higher throughput at lower parallelism. A worker pool of five to ten workers can saturate a single GPU's capacity. Adding more workers creates a queue that adds latency without adding throughput, because the GPU is the bottleneck. The scaling path for self-hosted judges is horizontal: add more GPUs, each with its own inference server, and distribute workers across them. A pool of four GPUs can process four times as many evaluations per second as a single GPU, with a coordinator that load-balances across inference endpoints.

The operational advantage of self-hosted parallelism is predictability. Your throughput ceiling doesn't change based on a provider's usage tiers or time-of-day congestion. The disadvantage is cost: those GPUs are expensive whether the eval pipeline is running or not. Teams that run eval pipelines intermittently — pre-release gates, nightly suites — should use spot instances or preemptible GPUs to avoid paying for idle capacity. Teams that run continuous evaluation may find dedicated GPU instances cost-effective because the sustained utilization justifies reserved pricing.

## Putting It Together: A Throughput Planning Process

Before building a parallelized eval pipeline, work through the throughput plan. Start with the end goal: how many evaluations do you need to complete, and in what time window? A pre-release suite of five thousand test cases that needs to finish in two hours requires forty-two evaluations per minute. A CI/CD gate of two hundred test cases per pull request with a fifteen-minute target requires thirteen evaluations per minute.

Next, identify your rate-limit ceiling. Check your provider's documentation or contact their sales team for your tier's limits. Divide the request-per-minute limit by sixty to get requests per second. Divide the token-per-minute limit by your average tokens per request, then by sixty. Your ceiling is the lower of these two numbers.

Compare the required throughput against the ceiling. If the ceiling exceeds your requirement, size your worker pool to meet the requirement and no more — over-provisioning wastes resources and increases the blast radius if something goes wrong. If the ceiling falls short of your requirement, you have three options: request a rate-limit increase from your provider, implement multi-provider rotation, or reduce your eval suite to fit the throughput window. The third option is a last resort, but sometimes the honest answer is that your eval suite has outgrown your provider budget and the constraint is financial, not architectural.

Finally, add monitoring. Track evaluations per minute in real time during every run. Track the rate-limit utilization — how close you are to the ceiling. Track the p99 latency per evaluation. Set alerts for throughput dropping below expected levels, which indicates a rate-limit hit, a provider degradation, or a worker failure. Parallelized pipelines have more moving parts than sequential ones, and those parts fail in ways that are harder to observe. Monitoring is not optional — it's the mechanism that converts parallelism from a fragile speedup into a reliable architecture.

Parallelization accelerates the evaluations you need to run. But the fastest evaluation is the one you don't run at all. The next subchapter covers eval result caching — strategies for detecting when an evaluation has already been performed and reusing the result instead of repeating the work.
