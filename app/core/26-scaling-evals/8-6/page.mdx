# 8.6 — Agentic Reliability Metrics: pass at k, Trajectory Analysis, and Multi-Step Reliability

When an AI agent is tasked with booking a flight, it might search for options, filter by price and time, select a flight, enter passenger details, and complete the purchase. If it fails at step four — entering the wrong passenger name — the first three steps were wasted. If it retries and succeeds on the second attempt, the overall outcome is achieved but with extra cost and latency. Evaluating agents requires metrics that capture this multi-step, retry-tolerant, trajectory-dependent reality.

## Why Single-Output Metrics Fail for Agents

Traditional evaluation metrics assess a single output: was the response correct? Agents don't produce single outputs. They produce trajectories — sequences of actions, observations, and decisions that unfold over time. A trajectory can succeed overall while containing failed steps. It can fail overall despite many correct intermediate steps. And the quality of the trajectory — how efficiently the agent reached the goal, how many errors it made along the way, how it recovered from failures — matters as much as whether it ultimately succeeded.

Evaluating agents with single-output metrics is like evaluating a pilot by whether the plane lands safely, without considering whether the flight was smooth, how much fuel was used, or how many near-misses occurred en route. The landing matters, but it is not the whole story.

## pass at k: Reliability Through Retries

**pass at k** measures the probability that an agent succeeds at a task within k attempts. pass at 1 is the single-attempt success rate — the hardest standard. pass at 3 means the agent gets three tries to succeed, and any success counts. pass at 5 gives five attempts.

This metric captures a fundamental reality of agentic systems: retries are part of the design. An agent that succeeds 60% of the time on a single attempt but 95% of the time within three attempts may be perfectly acceptable for many use cases, as long as the retry cost (latency, compute, API calls) is within budget. pass at k makes this tradeoff explicit.

The related metric **pass at exactly k** measures how many attempts it takes to succeed. An agent with high pass at 1 is efficient. An agent with low pass at 1 but high pass at 3 is reliable but expensive. An agent with low pass at 5 has a fundamental capability gap that retries cannot solve.

## Trajectory Analysis

Beyond success or failure, trajectory analysis evaluates the quality of the path the agent took. Two trajectories that both achieve the same goal can differ dramatically in quality. One might take three focused steps. Another might take twelve steps including backtracking, error recovery, and redundant actions.

Trajectory quality metrics include step count (fewer is better, normalized by task complexity), tool usage efficiency (did the agent use the right tools in the right order?), error recovery rate (when the agent encountered an error, how quickly did it recover?), and trajectory coherence (did the steps form a logical sequence, or did the agent appear confused?).

These metrics matter because trajectory quality directly affects cost and user experience. An agent that completes a task in three steps costs one-quarter of an agent that takes twelve steps, because each step involves an LLM inference call. An agent that backtracks and retries creates a confusing user experience even if the final outcome is correct.

## Multi-Step Reliability Curves

For complex tasks that require many steps, reliability follows a decay curve. If each step has a 95% success rate and a task requires twenty steps, the probability of completing all twenty steps without any failure is roughly 36%. This is the multi-step reliability problem — individual step reliability that seems high compounds into system reliability that is surprisingly low.

**Multi-step reliability curves** plot the probability of task completion against the number of steps required. This visualization reveals the practical limits of the agent: at what task complexity does reliability drop below acceptable thresholds? For an agent with 95% per-step reliability, tasks requiring more than fifteen steps have below 50% completion probability. For 99% per-step reliability, the threshold extends to about seventy steps.

This analysis informs product decisions. If your agent's reliability curve shows that tasks requiring more than ten steps have less than 60% completion rates, you should either improve per-step reliability, reduce task complexity through decomposition, or add human checkpoints at strategic points in long trajectories.

## Evaluation at Scale

Agentic evaluation is expensive because each evaluation requires running the full agent trajectory, which may involve dozens of LLM calls, tool executions, and wait times. Evaluating a single complex agent task might cost $1-5 in compute and API fees, compared to $0.04 for a single-output judge evaluation. At scale, this cost pressure means agentic evaluation must be heavily sampled, with trajectory-level evaluation reserved for representative task samples and component-level evaluation applied more broadly.

The sampling strategy for agentic evaluation should prioritize task diversity over volume — evaluating the agent on fifty different task types with three attempts each provides more signal than evaluating it on five task types with thirty attempts each.

Agents and compound systems produce outcomes that can be measured. But some outcomes are rare, unpredictable, and disproportionately important. The next subchapter addresses long-tail outcome measurement — the rare events that determine whether a system is trusted.
