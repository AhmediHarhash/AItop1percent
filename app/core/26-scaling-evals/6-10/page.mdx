# 6.10 — Living Benchmarks and Eval Freshness: Staying Ahead of Contamination at Scale

A benchmark that never changes is a benchmark that will eventually be memorized. The only reliable defense against contamination at scale is eval data that moves faster than training data can absorb it. This is not a theoretical concern. By 2026, every major public benchmark older than eighteen months has measurable contamination in at least some frontier models. The benchmarks that remain trustworthy — LiveBench, LiveCodeBench, AntiLeakBench, and their internal equivalents at companies that take evaluation seriously — share one characteristic: they refresh continuously. They treat eval data not as an artifact to be preserved but as a stream to be renewed. This subchapter covers how to apply that principle to your own evaluation infrastructure.

## The Living Benchmark Concept

A **living benchmark** is an evaluation dataset that is continuously refreshed with new examples, so that any contamination from previous versions becomes irrelevant as the benchmark evolves. The key insight is temporal: if your eval examples were created after the model's training data cutoff, those examples cannot be contaminated. A question generated from a news event that happened last month cannot have been in the training data of a model that was trained six months ago. A coding problem derived from a competition held last week cannot have been memorized during pre-training.

Living benchmarks exploit this temporal advantage by maintaining a continuous supply of fresh evaluation examples. Old examples are retired or deprioritized. New examples enter the active eval set on a regular cadence. The benchmark is never fully static, which means it can never be fully memorized.

The concept originated in public AI evaluation. LiveBench, launched in 2024 and continuously maintained since, refreshes its question set monthly using content sourced from recent academic papers, news articles, competition problems, and data releases. Each monthly update adds new questions that postdate the training cutoff of any model being evaluated, ensuring that strong performance reflects genuine capability rather than memorization. LiveCodeBench applies the same principle to code evaluation, sourcing new problems from recent programming competitions on LeetCode, AtCoder, and CodeForces. By the time a problem appears in LiveCodeBench, it was created after the training data collection period for current models.

AntiLeakBench, published at ACL 2025, took the concept further with fully automated benchmark construction. Instead of manually curating new questions, AntiLeakBench identifies new real-world knowledge that postdates a model's training cutoff — facts, events, and information that provably did not exist when the model was trained — and automatically generates evaluation questions testing that knowledge. The framework can update itself continuously as new information becomes available, reducing the human labor required to maintain freshness to near zero.

These public benchmarks prove the concept. The challenge for production teams is implementing the same principle internally, where the stakes are higher and the requirements are more specific to your product.

## Implementing Living Benchmarks Internally

Building an internal living benchmark requires three capabilities: a source of fresh evaluation examples, a validation process that ensures quality, and a lifecycle management system that retires stale examples.

The most reliable source of fresh eval examples is your own production traffic. Every day, your system processes real user queries and generates real outputs. A subset of this traffic, selected through stratified sampling to ensure category coverage, becomes candidate eval material. The candidates are reviewed by human experts who label them — defining the expected correct output or the quality criteria the output should meet. Once labeled and validated, these examples enter the active eval set. Because they are derived from recent production traffic, they reflect current user behavior, current product scope, and current model capabilities. They are maximally representative and minimally contaminated.

The validation process must be rigorous enough to maintain golden-set quality standards. Not every production sample that enters the candidate pool will become an eval example. Some will be too ambiguous — multiple valid answers, unclear ground truth. Some will be too easy — the model already handles this case perfectly, and the example adds no evaluation signal. Some will be too specific to a single user's context to generalize. Human reviewers filter, label, and validate candidates, applying the same quality criteria described in subchapter 6.7. The validation step is the bottleneck in the living benchmark pipeline, and its speed determines how quickly you can refresh your eval set.

Lifecycle management means defining when examples enter the active set, how long they stay, and when they are retired. A reasonable default is that new examples enter the active set after validation, remain active for six to twelve months, and are then retired to an archive. Retired examples can still be used for historical comparisons — how did the model perform on last year's eval set compared to this year's? — but they no longer contribute to the primary quality score that drives deployment decisions.

## The Freshness-Stability Tradeoff

A completely refreshed benchmark makes it impossible to compare scores across time periods. If the eval set you used in January is entirely different from the one you use in July, a score change between the two periods could reflect model improvement, model degradation, or simply the fact that the July eval set is harder or easier than the January one. You can't tell the difference, which means the score change is uninterpretable.

A completely static benchmark, conversely, gives you perfect temporal comparability but grows increasingly contaminated. You can compare January to July with confidence that the eval set didn't change — but you can't be confident that the model didn't memorize the eval data between those two dates.

The solution is a hybrid architecture with two components: a **stable core** and a **fresh periphery**.

The stable core is a subset of your eval set — typically thirty to forty percent of examples — that remains unchanged over long periods. These are time-independent examples: questions that test fundamental capabilities rather than current knowledge, problems whose correct answers don't change with time, and evaluation criteria that are robust to product evolution. The stable core provides temporal comparability. A score on the stable core in January can be meaningfully compared to a score on the stable core in July, because the examples are the same.

The fresh periphery is the remaining sixty to seventy percent of examples, refreshed on a monthly or quarterly cadence. These are time-sensitive examples derived from recent production traffic, current use cases, and evolving product requirements. The fresh periphery provides contamination resistance. Even if the stable core is eventually memorized, the fresh periphery tests capabilities the model could not have seen during training.

The aggregate quality score combines both components, weighted by their proportion. The temporal trend in the aggregate score is interpretable as long as you hold the weighting constant and document which component drove any changes. If the stable core score is flat but the fresh periphery score dropped, the model may have regressed on current-distribution inputs while maintaining capability on historical patterns. If the fresh periphery score is stable but the stable core score improved, the model may have memorized the stable core — a contamination signal that should trigger investigation.

## The Cost of Freshness

Generating, validating, and labeling new eval examples monthly requires sustained investment. This is not a one-time infrastructure build. It is an ongoing operational cost that scales with the size of your eval set and the speed of your refresh cadence.

For a golden set of one thousand examples with a sixty percent fresh periphery refreshed quarterly, you need to generate, validate, and label approximately one hundred fifty new examples every three months — six hundred per year. At a labeling cost of fifteen to thirty dollars per example for expert-validated golden set quality, the annual freshness cost is nine thousand to eighteen thousand dollars in labeling alone, plus the engineering time to manage the pipeline, the reviewer time to validate candidates, and the platform team's time to integrate new examples into the eval infrastructure.

This cost is part of the **Eval Tax** introduced in subchapter 1.3. It is the price of evaluation that means something over time. Teams that refuse to pay this cost get static benchmarks that decay into contaminated artifacts, producing quality scores that look impressive on dashboards and mean nothing in production.

For teams with tighter budgets, synthetic data generation can reduce the per-example cost significantly. Use an LLM to generate candidate eval examples based on production traffic patterns, then have human reviewers validate and label only the generated candidates that pass automated quality filters. This shifts the cost from creation to validation, which is cheaper per example while maintaining the quality floor that golden set status requires.

## Freshness Metrics: Measuring How Current Your Eval Data Is

You can't manage what you don't measure. Freshness metrics quantify how current your eval data is and flag when contamination risk is rising.

The primary freshness metric is **eval set age distribution** — what percentage of your active eval examples are newer than specific age thresholds? Track the percentage newer than three months, newer than six months, and newer than twelve months. A healthy living benchmark in 2026 has at least fifty percent of examples newer than six months. If more than half your eval set is older than six months, contamination risk is elevated and growing. If more than seventy percent is older than twelve months, your eval set is functionally static and should be treated with the same skepticism as a published public benchmark.

The secondary freshness metric is **refresh velocity** — how many new examples are entering the active eval set per month? Track this as both a raw count and as a percentage of the total eval set. If your eval set contains one thousand examples and you're adding fifty new examples per month while retiring fifty old ones, your monthly refresh rate is five percent and your full-rotation time is twenty months. Faster refresh means lower contamination risk but higher operational cost. Slower refresh is cheaper but leaves stale examples in the eval set longer.

The tertiary metric is **stable-core contamination signal** — the score trend on your stable core subset over time, particularly after model retraining events. If the stable core score jumps after a model is retrained on new data, that jump could indicate genuine improvement or contamination. Compare it against the fresh periphery score change. If the stable core improved but the fresh periphery didn't, contamination of the stable core is the most likely explanation. This is the canary in the mine for benchmark integrity.

## Managing the Stable Core

The stable core is not immutable forever. It is stable relative to the fresh periphery — it changes slowly rather than not at all.

Stable core examples should be reviewed annually against three criteria. First, are the correct answers still correct? A question about API best practices might have a different correct answer in 2026 than it did in 2024. Second, is the example still relevant to the product? If the product has deprecated a feature, eval examples testing that feature should be retired from the stable core. Third, is there evidence of contamination? If the stable core score is suspiciously high or has increased over multiple retraining cycles without corresponding improvement on the fresh periphery, individual stable core examples should be investigated for memorization.

When a stable core example is retired, it should be replaced by a new example that tests the same capability but uses different phrasing, different context, and different expected outputs. This maintains the capability coverage of the stable core while resetting the contamination clock for the replaced example. Over time, even the stable core gradually renews — just on a multi-year cadence rather than a monthly one.

## Living Benchmarks and Model Provider Relationships

If you use API-based models from third-party providers, living benchmarks give you a tool for independent quality verification that is resistant to the contamination problems described in the previous subchapter.

When a provider releases a new model version, run your living benchmark against it. Because the fresh periphery of your benchmark is custom, internal, and never published, the provider's model cannot have been trained on it. The scores you get are genuine capability measurements, not contaminated artifacts. If the provider claims their new model is fifteen percent better on public benchmarks, but your living benchmark shows only a three percent improvement, the gap tells you something about the contamination level of those public benchmarks. Your internal measurement is more trustworthy than the provider's headline number because your data is clean.

This independent verification capability is increasingly important as the model market fragments. In 2026, organizations routinely evaluate models from four or more providers — OpenAI, Anthropic, Google, Meta, Mistral, and others — and the provider-published benchmark scores are nearly impossible to compare because each benchmark has a different contamination profile for each provider. Your internal living benchmark, tested identically against all candidates, is the only apples-to-apples comparison tool you have.

## When Living Benchmarks Meet Regulatory Requirements

The EU AI Act's GPAI Code of Practice, enforceable since 2025, requires that providers of general-purpose AI models document their evaluation methodology, including measures taken to ensure benchmark integrity. For organizations deploying high-risk AI systems — which face an August 2026 compliance window — the ability to demonstrate that evaluation data is not contaminated is not a nice-to-have. It is a compliance requirement.

Living benchmarks provide a defensible answer to the regulator's question: "How do you know your evaluation scores are trustworthy?" The answer is: our eval data is continuously refreshed from recent production traffic, independently validated, and verified against contamination using the freshness metrics and canary techniques described in this chapter. Our stable core is reviewed annually and replaced when contamination signals emerge. Our fresh periphery postdates the training data of every model we evaluate. This is a stronger answer than "we used MMLU," because everyone — including the regulator — knows that MMLU scores are unreliable indicators of real-world capability for models trained in 2025 and later.

## Building the Full Dataset Integrity Stack

Looking back across this chapter, the dataset management practices form a stack. At the base, the golden dataset exists as a living system, versioned and tiered across golden, silver, and super-golden quality levels. Above that, refresh cadence keeps the content current. Synthetic data extends coverage into areas that production traffic alone can't reach. Adversarial libraries maintain security coverage that evolves with the threat landscape. Dataset QA ensures the labels are accurate. Red-team integration turns vulnerability discoveries into permanent regression tests. Contamination controls keep eval data isolated from training data. And living benchmarks — the capstone — ensure that even if every other defense fails, the eval data renews itself faster than contamination can erode it.

No single layer is sufficient. A golden set that is perfectly labeled but contaminated is useless. A living benchmark that refreshes monthly but has poor label quality is measuring noise. An adversarial test set that never incorporates new red-team findings becomes a static artifact that misses emerging threats. The stack works because each layer compensates for the weaknesses of the others. Build all of them.

Datasets are the foundation of evaluation. But at scale, some outputs still need human eyes — and the way human review operates at production volume looks nothing like it did at startup scale. The next chapter covers human-in-the-loop evaluation at scale: the collapse of naive reviewer models, the architecture of reviewer workforces, and the infrastructure that turns human judgment into a scalable evaluation signal.
