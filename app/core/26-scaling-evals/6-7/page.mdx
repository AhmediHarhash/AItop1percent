# 6.7 — Dataset Quality Assurance: Auditing the Data That Judges Your System

Your evaluation is only as trustworthy as your eval data. If your golden set contains mislabeled examples, ambiguous ground truth, or duplicates that inflate scores, every evaluation result built on that data is suspect. You can have the most sophisticated eval pipeline in the world — perfect orchestration, flawless aggregation, real-time dashboards — and all of it means nothing if the data you're measuring against is wrong. A golden set with a five percent label error rate doesn't just reduce accuracy by five percent. It creates a noise floor that makes it impossible to detect real regressions smaller than five points. It is the quiet destroyer of evaluation credibility.

Most teams invest heavily in the infrastructure that processes evaluations and almost nothing in the quality of the data those evaluations run against. They treat their golden set as finished the day it was built. It isn't. Eval data degrades over time. Labels that were correct in 2024 may be wrong in 2026 because the product changed, the model changed, or the definition of "correct" shifted. Ground truth that was unambiguous six months ago may now have multiple valid answers because the product expanded its scope. The eval dataset is not a static artifact. It is a living system that requires ongoing quality assurance — and the teams that skip this QA are the ones who discover, months later, that their quality scores have been meaningless.

## Why Eval Data Needs Its Own QA Process

Training data quality has an entire industry built around it. Annotation platforms, quality frameworks, inter-annotator agreement protocols — the tooling for ensuring training data quality is mature and widely adopted. Eval data quality has almost none of this infrastructure, and the reason is a dangerous assumption: that because eval datasets are smaller and more curated, they don't need systematic quality assurance.

The assumption is backwards. Eval data needs more rigorous QA than training data, not less. A single mislabeled example in a training set of a million items has negligible impact on model behavior. A single mislabeled example in a golden set of five hundred items corrupts 0.2 percent of your quality signal — and if that example happens to be the swing case between passing and failing a release gate, it can block a good deployment or approve a bad one. The smaller the dataset, the more each individual example matters, and the more damage a single error causes.

The types of quality issues in eval data are also different from training data. Training data errors affect model learning gradually, and the model itself smooths over many individual errors through the learning process. Eval data errors affect measurement directly and without smoothing. If an eval example has the wrong label, the model gets penalized for being right or rewarded for being wrong. There is no gradient to soften the impact. The error shows up as a hard mismeasurement in your quality score.

## The Five Common Quality Issues in Eval Datasets

**Label errors** are the most straightforward quality issue: the "correct" answer in the golden set is simply wrong. This happens more often than most teams expect. A customer service golden set might mark a response as incorrect because it doesn't include a refund offer, but the policy changed three months ago and refunds are no longer offered for that product category. A medical triage golden set might mark a response as correct that recommends a dosage that has since been updated in clinical guidelines. A coding golden set might mark a solution as wrong because it uses a deprecated library, when in fact the model's answer using the current library is correct and the golden set is outdated.

Label errors compound when the team uses pass-fail thresholds. If your release gate requires ninety percent of golden set examples to pass, and five percent of your golden set has incorrect labels, then the effective ceiling — the best possible score a perfect model could achieve — is ninety-five percent, not one hundred. Your model needs to get every legitimately correct answer right and can only afford to miss five percent of the incorrectly labeled ones. In practice, this means a high-quality model might score ninety-one percent on a golden set with five percent label errors, clearing the ninety percent gate by a single point. That same model on a clean golden set would score ninety-six percent, clearing the gate by six points. The five-percent label error rate turns a comfortable pass into a razor-thin one, creating unnecessary deployment anxiety and wasted investigation time.

**Ambiguous examples** are eval items where multiple valid answers exist but only one is marked as correct. A summarization eval might expect a three-sentence summary that covers revenue, headcount, and product launches — but a summary covering revenue, product launches, and strategic partnerships is equally valid for the source material. A translation eval might mark one phrasing as correct when three other phrasings are equally accurate and natural. A customer response eval might expect a formal tone when a conversational tone is equally acceptable under current guidelines. Ambiguous examples penalize models for producing valid outputs that happen to differ from the single expected answer, creating systematic bias against diverse but correct responses.

**Duplicate and near-duplicate examples** inflate coverage statistics without adding evaluation value. If your golden set contains thirty examples that all test the same narrow capability — say, date formatting in customer emails — your eval appears to cover that capability thoroughly. But those thirty examples are effectively one test repeated with minor variations. They inflate the denominator in your quality calculations, making the overall score disproportionately sensitive to performance on date formatting while under-representing other capabilities. A model that handles dates perfectly but struggles with tone gets a misleadingly high score because thirty of five hundred examples reward its strongest capability.

**Difficulty imbalance** means your eval set is disproportionately easy or disproportionately hard, creating pass rates that don't reflect real-world performance. A golden set where eighty percent of examples are straightforward lookup questions and twenty percent are complex reasoning questions will produce high pass rates that mask poor performance on the hard questions. This is especially dangerous when the hard questions represent the use cases that matter most to your users — the edge cases that drive escalations, the complex queries that determine whether a customer renews.

**Distribution mismatch** occurs when your eval data doesn't reflect your actual production traffic. If forty percent of your production queries are in Spanish but only five percent of your golden set is in Spanish, your quality score is dominated by English performance. If your product shifted from primarily consumer queries to primarily enterprise queries over the last six months, but your golden set still reflects the consumer distribution, your evaluation is measuring a product that no longer exists.

## Building the Audit Process

A dataset quality audit is not a single event. It is a recurring process with defined cadence, methodology, and accountability. The cadence depends on the dataset tier. Golden sets — your highest-confidence evaluation data — should be audited quarterly. Silver sets — broader evaluation data with lower per-example confidence — should be audited semiannually. Adversarial test sets should be audited after every major red-team exercise or threat landscape change.

The audit process has five steps. First, draw a stratified random sample from the eval dataset. The sample should be large enough to provide statistical confidence in your quality estimate — typically two hundred to three hundred examples from a golden set of one thousand to two thousand items, ensuring proportional representation across the dataset's categories. If your golden set covers customer service, medical triage, and technical support, the sample should include examples from all three categories in proportion to their representation in the full dataset.

Second, have independent reviewers relabel the sampled examples. These reviewers should not be the same people who created the original labels. Ideally, they should include at least one domain expert and at least one person who represents the target user persona. Each reviewer labels the example independently, without seeing the existing label. This independence is critical — if reviewers see the existing label, confirmation bias will cause them to agree with it even when it is ambiguous or wrong.

Third, compare the independent labels to the existing labels. Calculate the agreement rate between the new labels and the original labels. This is your **dataset accuracy rate** — the percentage of examples where independent review confirms the existing label. A golden set should target a dataset accuracy rate above ninety-five percent. Below ninety percent, the label errors are materially affecting your quality measurements. Below eighty-five percent, the golden set needs a full relabeling effort before it can be trusted for release decisions.

Fourth, categorize the disagreements. For every example where the independent reviewers disagree with the existing label, determine why. Is the original label simply wrong? Is the example ambiguous with multiple valid answers? Is the original label outdated due to product or policy changes? Is the example too complex for any single correct answer? The distribution of disagreement types tells you what kind of quality problem you have. Predominantly wrong labels means your initial labeling process was flawed. Predominantly ambiguous examples means your rubric needs clarification. Predominantly outdated labels means your refresh cadence is too slow.

Fifth, fix the discrepancies and report the results. Correct the wrong labels. Rewrite ambiguous examples to reduce ambiguity or expand the acceptable answer set. Remove or replace outdated examples. Then compute and report the pre-audit and post-audit accuracy rates to stakeholders. This transparency builds organizational trust in the eval system and creates accountability for maintaining data quality.

## Measuring Inter-Annotator Agreement on Eval Data

The dataset accuracy rate tells you how many labels are correct. **Inter-annotator agreement** tells you how reliable the labeling process itself is. These are related but distinct metrics, and you need both.

To measure inter-annotator agreement on eval data, have at least two independent reviewers label the same sample of examples. Calculate the agreement rate — the percentage of examples where both reviewers assign the same label. Raw agreement rates are misleading because they don't account for chance agreement, so compute Cohen's kappa or a similar chance-corrected agreement metric. A kappa above 0.8 indicates strong agreement and a reliable labeling process. A kappa between 0.6 and 0.8 indicates moderate agreement and suggests the rubric needs clarification. A kappa below 0.6 indicates that the labeling task is either poorly defined or genuinely ambiguous, and the eval data produced under these conditions should be treated with caution.

Low inter-annotator agreement on eval data is not just a quality problem — it is a signal problem. If two expert reviewers can't agree on whether a model output is correct, then the evaluation of that output is measuring reviewer interpretation, not model quality. Scores on ambiguous examples are essentially random with respect to the model's actual performance. The proper response to low agreement is not to pick one reviewer's labels and move on. It is to revise the evaluation rubric until agreement improves, split the ambiguous examples into more specific sub-categories where agreement is higher, or acknowledge that certain eval items carry inherent uncertainty and weight them accordingly in your aggregate metrics.

## Distribution Comparison: Does Your Eval Data Reflect Reality

The most sophisticated label quality is meaningless if your eval data tests the wrong things. Distribution comparison is the practice of quantifying how well your eval dataset's composition matches your actual production traffic.

Start with the dimensions that matter most for your product. Language distribution: what percentage of production traffic is in each language, and what percentage of your golden set covers each language? Query type distribution: how do production queries break down by category, and does your golden set reflect that breakdown? Difficulty distribution: are the hard cases in production proportionally represented in your eval data, or are they undersampled because hard cases are difficult to label?

Compute a distribution similarity metric between your eval data and a representative sample of production traffic across each dimension. The simplest approach is to compare the category-level percentages directly. If forty percent of production traffic is in Spanish but only eight percent of your golden set is in Spanish, the gap is obvious and actionable. More sophisticated approaches use statistical divergence measures to quantify the overall distribution mismatch across all categories simultaneously.

When you find a mismatch — and you will — the fix is not always to restructure your golden set to match production exactly. Sometimes the mismatch is intentional. You might deliberately oversample rare but high-risk categories because their quality matters more than their frequency suggests. The important thing is that the mismatch is documented and deliberate, not accidental and invisible. A team that knows their eval data oversamples safety-critical queries and undersamples routine queries can interpret their quality scores accordingly. A team that doesn't know their eval data's distribution has no basis for interpreting scores at all.

## Tracking Quality Metrics Over Time

Individual audits tell you the state of your eval data at one point in time. Tracking audit results over time tells you whether your data quality is improving, degrading, or stable — and that trend is as important as any single measurement.

Maintain a quality metrics dashboard for your eval datasets. For each dataset and each audit, record the dataset accuracy rate, the inter-annotator agreement kappa, the number and type of discrepancies found, and the distribution comparison scores. Plot these over time. A declining accuracy rate signals that your eval data is growing stale faster than your refresh process updates it. A declining agreement score signals that your evaluation criteria are becoming more ambiguous — possibly because the product has expanded into areas the original rubric doesn't cover. A widening distribution gap signals that production traffic is drifting away from what your eval data represents.

These trends should trigger proactive action. A golden set whose accuracy rate has dropped from ninety-seven percent to ninety-one percent over three quarters doesn't need an emergency relabeling — it needs an accelerated refresh cycle before it drops below the ninety percent threshold where label errors begin materially affecting release decisions. Catching the trend early is far cheaper than discovering, during a critical deployment, that your golden set can no longer be trusted.

## The Organizational Challenge: Who Owns Eval Data Quality

The single biggest reason eval data quality erodes is that nobody owns it. The team that built the golden set six months ago has moved to other projects. The eval platform team owns the infrastructure that processes evaluations but not the data the evaluations run against. The product team uses the quality scores for decisions but doesn't look at the underlying data. Responsibility falls into the gap between teams, and the data slowly rots.

Assign explicit ownership of eval dataset quality to a specific team or role. This doesn't need to be a full-time job — for most organizations, it's a responsibility that lives within the eval platform team or the data quality team. The owner's job is to maintain the audit cadence, escalate quality issues, coordinate with product teams to update labels when policies change, and report quality metrics to the stakeholders who depend on eval data for decisions.

Without this ownership, audits don't happen. Labels don't get updated. Distribution mismatches grow unchecked. And six months later, the organization is making deployment decisions based on evaluation data that doesn't measure what they think it measures — a failure that looks like a quality problem but is actually a governance problem.

## From Audit Results to Action

An audit that discovers problems but doesn't lead to fixes is theater. Every audit should produce a remediation plan with specific actions, owners, and deadlines.

For label errors, the action is straightforward: fix the labels. Assign the corrections to reviewers with the right domain expertise. Set a deadline of two weeks from the audit report. Verify the fixes with a spot check.

For ambiguous examples, the action depends on the type of ambiguity. If the ambiguity is resolvable — the rubric was unclear but can be clarified — update the rubric and relabel the affected examples. If the ambiguity is inherent — the question genuinely has multiple valid answers — restructure the eval to accept multiple correct answers, or replace the example with one that has a clearer ground truth.

For distribution mismatches, the action is to create new examples that fill the gaps. If Spanish coverage is underrepresented, commission new Spanish examples. If complex reasoning queries are undersampled, generate or curate additional complex examples. This is where synthetic data generation, covered in subchapter 6.5, becomes a practical tool — you can use LLM-generated examples to fill distribution gaps quickly, then promote the best ones to the golden set after human review.

For difficulty imbalance, the action is to stratify your eval reporting. Rather than reporting a single pass rate across the entire golden set, break the score down by difficulty tier. Report easy, medium, and hard pass rates separately. This doesn't require changing the golden set — it requires changing how you interpret it. A ninety-five percent overall pass rate that breaks down as ninety-nine percent easy, ninety-four percent medium, and seventy-eight percent hard tells a very different story than a flat ninety-five percent, and it directs improvement efforts where they matter most.

Eval data quality is internal — your team controls the data, the labels, and the audit process. But not all threats to your evaluation data come from inside. The next subchapter covers how adversarial findings from red-team exercises flow into your eval datasets, turning one-time vulnerability discoveries into permanent quality checks.
