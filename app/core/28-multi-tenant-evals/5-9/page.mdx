# 28.9 — Synthetic Ground Truth: Generating Test Data When the Tenant Has None

Not every customer arrives with a filing cabinet of historical examples, a team of domain experts ready to label, and a clear picture of what good output looks like. Some customers are building an entirely new AI capability — they have no production traffic because the product does not exist yet. Others operate in regulated environments where real data cannot leave their network, let alone be used to populate an evaluation system on your platform. Still others are early-stage companies whose total historical output fits in a single spreadsheet. For these customers, organic ground truth does not exist, and waiting for it means waiting months with no eval system, no quality signal, and no way to know whether the AI is working.

**Synthetic ground truth** is the strategy of generating reference data for evaluation using models, templates, or simulation rather than real customer interactions. It is a bootstrapping mechanism — a way to stand up an eval system on day one so the customer has quality scores, drift detection, and regression alerts from the moment they launch. It is also inherently imperfect. Synthetic data reflects what models think the customer's domain looks like, not what the domain actually looks like. Used carefully, it bridges the gap between contract signing and the accumulation of real production data. Used carelessly, it creates an eval system that measures the wrong things confidently, and the customer never discovers the gap because the scores look plausible.

## When Synthetic Ground Truth Is Appropriate

Synthetic ground truth is a last resort, not a default. Three conditions must all be true before you reach for it.

The first condition is **zero organic data**. The customer has no historical examples of the AI task they are deploying. If a customer has even 50 real examples — past customer support transcripts, previous document extractions, historical chat logs — those 50 real examples are more valuable than 500 synthetic ones. Real data carries the distribution of actual inputs, the messiness of genuine edge cases, and the implicit quality standards of the customer's domain. Synthetic data carries the distribution of whatever model or template generated it, which is a different distribution. If organic data exists in any quantity, use it as the foundation and supplement with synthetic data only for coverage gaps. Never replace organic data with synthetic data because synthetic is easier to produce.

The second condition is **regulatory or security constraints on real data**. Some customers can share real data for evaluation. Many cannot. A pharmaceutical company running clinical trial analysis may be prohibited from sending real patient data to any external system, even for evaluation purposes. A defense contractor cannot share classified document summaries. A financial institution subject to strict data residency requirements cannot allow transaction data to leave a specific geographic boundary. In these cases, synthetic data that mimics the structure, complexity, and domain vocabulary of real data without containing actual sensitive content is the only option. The customer's compliance team typically needs to approve the synthetic generation process and confirm that the generated examples do not constitute a derivative of protected data.

The third condition is **a need for day-one eval coverage**. If the customer can wait three months for organic data to accumulate before activating evaluation, synthetic ground truth is unnecessary. The customer launches, production traffic flows, real examples accumulate, and the team curates a golden set from those examples as described in the earlier subchapters of this chapter. But some customers cannot wait. Their contract requires quality reporting from day one. Their internal stakeholders need evidence that the AI is working before they approve broader rollout. Their regulatory framework demands continuous quality monitoring regardless of data availability. For these customers, synthetic ground truth fills the gap between launch and the accumulation of enough organic data to build a proper eval system.

When all three conditions are met, synthetic generation is the right call. When even one is not met, prefer organic data — however small, however messy — as the foundation.

## Generation Methods

Four methods produce synthetic ground truth, each with different fidelity, cost, and risk profiles.

**LLM-generated examples** are the most common method in 2026. You take the customer's domain description — their industry, their use case, their input and output types, their quality criteria — and prompt a strong foundation model (Claude Opus 4.6, GPT-5, Gemini 3 Pro) to generate example inputs and their ideal outputs. A legal tech customer deploying contract clause extraction gets synthetic contracts with clauses to extract. A customer support platform gets synthetic support tickets with ideal agent responses. The strength of this method is speed and domain coverage: a well-crafted generation prompt can produce 200 diverse examples in under an hour. The weakness is that LLM-generated examples carry the model's own biases about what that domain looks like, which may diverge significantly from the customer's actual data. A model asked to generate "typical customer support tickets for a telecom company" will produce examples that reflect the average of its training data about telecom support, not this particular telecom company's specific customer base, product lineup, and communication style.

**Template-based generation** uses structured schemas provided by the customer to produce examples mechanically. The customer defines input templates — field types, value ranges, format specifications, constraint rules — and the generation system fills those templates with randomized but valid values. A healthcare customer provides a schema for patient intake forms with field types for age, diagnosis codes, medication lists, and insurance identifiers. The system generates 300 synthetic intake forms by sampling from valid value ranges for each field. Template generation produces structurally correct examples with high consistency but low diversity. Every generated example conforms to the schema, which means template-generated data excels at testing whether the AI handles valid inputs correctly but fails to test how the AI handles the messy, incomplete, and malformed inputs that characterize real production traffic.

**Cross-tenant augmentation** generates synthetic data by adapting examples from other tenants whose use cases overlap with the new customer's domain. If your platform already serves 40 legal tech customers and the new customer is also a legal tech company, you have a corpus of anonymized examples from existing tenants that reflect the actual distribution of legal documents in production. With explicit consent from the source tenants — and this consent must be explicit, documented, and reviewed by legal — you can adapt those examples by changing domain-specific details, altering entity names, modifying numerical values, and adjusting formatting to match the new customer's specifications. Cross-tenant augmentation produces the highest-fidelity synthetic data because it starts from real production examples, but it carries the highest compliance risk because the line between "adapted example" and "derivative of another tenant's data" is legally ambiguous. Every platform using this method should have their legal team define clear transformation requirements — how many changes constitute sufficient anonymization — and document compliance with those requirements for every generated example.

**Adversarial generation** produces edge case examples specifically designed to test the AI's failure modes. Instead of generating typical inputs, adversarial generation creates inputs that are ambiguous, contradictory, unusually long, unusually short, multilingual, poorly formatted, or otherwise designed to stress-test the system's handling of non-standard conditions. A document extraction customer gets synthetic documents with overlapping tables, handwritten annotations described as text, and pages where the scan quality degrades mid-page. Adversarial examples should constitute 15 to 25 percent of the synthetic golden set. Their purpose is not to test normal operation — the other generation methods cover that — but to ensure the eval system can detect when the AI fails on difficult inputs rather than producing confident but wrong outputs.

## Quality Risks of Synthetic Data

Synthetic ground truth introduces three quality risks that do not exist with organic data. All three are manageable, but only if you acknowledge and monitor them explicitly.

The first risk is **distribution mismatch**. Synthetic data reflects the generating model's or template's distribution, which may not match the customer's actual production distribution. A model generates "typical" customer support tickets based on its training data, but the customer's actual tickets skew heavily toward billing disputes with a specific product line that the model has no reason to emphasize. The eval system built on those synthetic examples measures the AI's performance on a hypothetical distribution rather than the real one. When organic data starts flowing and the real distribution reveals itself, the eval scores may shift dramatically — not because quality changed, but because the measurement basis changed.

Detection of distribution mismatch requires comparing synthetic examples to the earliest organic production data as it accumulates. After the first 100 real production interactions, run a distribution analysis: are the input lengths similar? Do the topic categories match? Do the complexity levels align? If the synthetic distribution differs from the organic distribution on any major dimension, the synthetic examples in that dimension must be flagged and eventually replaced. A B2B content generation platform discovered after three weeks of production traffic that their LLM-generated synthetic examples were 40 percent shorter than real customer inputs on average. The length difference meant the eval system was testing the model on simpler inputs than it actually handled, producing quality scores 7 points higher than the customer's organic experience. The recalibration conversation was difficult.

The second risk is **circular evaluation**. This occurs when the same model family generates both the synthetic ground truth and the production outputs being evaluated. If you use Claude Opus 4.6 to generate synthetic reference outputs and the customer's production pipeline also uses Claude Opus 4.6, the eval system is comparing the model's production outputs against the model's own idea of what good looks like. Agreement will be artificially high because the model has consistent biases, phrasing patterns, and error modes. A genuine quality problem — say, the model consistently misinterprets a domain-specific term — will not appear in eval scores because the synthetic reference outputs contain the same misinterpretation. The eval system gives a perfect score to an output that is consistently wrong.

Preventing circular evaluation requires using a different model family for ground truth generation than the customer's production model. If the customer runs GPT-5 in production, generate synthetic ground truth with Claude Opus 4.6 or Gemini 3 Pro. If the customer runs Claude, generate with GPT-5 or Llama 4 Maverick. Cross-model generation introduces diversity in phrasing and reasoning that breaks the circularity. The generated examples will not perfectly match the production model's style, which is the point — they provide an independent reference rather than a self-referential one. This cross-model requirement should be encoded in your synthetic generation pipeline as a hard constraint, not a guideline.

The third risk is **false coverage confidence**. A golden set of 200 synthetic examples looks comprehensive. It covers 12 use case variants with at least 10 examples each. The eval dashboard shows scores for every variant. The customer sees complete coverage and assumes the eval system is robust. But those 200 examples were generated from a domain description, not from real operational experience. They cover the use case variants the customer could articulate during discovery, which are the common cases. They do not cover the variants the customer has not encountered yet — the edge cases that only emerge from production traffic, the failure modes that only appear with real data, the input patterns that no domain description anticipated. The eval system has coverage that is wide but shallow. It measures the easy cases well and the hard cases not at all.

The mitigation for false coverage confidence is explicit labeling. Every synthetic example in the golden set should be tagged with its generation method and marked as synthetic. The eval dashboard should display a **Synthetic Ratio** — the percentage of the golden set that is synthetic versus organic. When the customer or the platform team views quality scores, the synthetic ratio provides context. A quality score of 89 percent against a golden set that is 100 percent synthetic means something different from a quality score of 89 percent against a golden set that is 80 percent organic. The number is the same. The confidence in that number is not.

## The Synthetic-to-Organic Migration Plan

Synthetic ground truth is a bridge, not a destination. Every customer who starts with synthetic data should have a documented migration plan that replaces synthetic examples with organic ones as real production data accumulates. The migration plan sets explicit targets, timelines, and triggers.

The target is **80 percent organic within six months** of production launch. This means that six months after the customer's AI capability goes live, at least 80 percent of their golden set consists of examples drawn from actual production interactions, curated and validated by either the customer's domain experts or the platform's quality team. The remaining 20 percent can stay synthetic — typically adversarial edge cases and rare scenario coverage that production traffic has not produced naturally.

The migration follows a phased cadence. During the first month of production, the focus is collection. The platform captures production inputs and outputs, applies quality scoring, and identifies candidates for the golden set. No synthetic examples are replaced yet because the organic sample is too small to assess whether the production distribution matches the synthetic distribution. During months two and three, the focus is comparison and selective replacement. The team compares organic examples to their synthetic counterparts and replaces synthetic examples where the organic version covers the same use case variant with greater fidelity. Priority goes to high-frequency use case variants where the organic distribution is most likely to differ from the synthetic distribution. During months four through six, the focus is expansion. The golden set grows beyond its original synthetic size as organic examples cover use case variants that the synthetic generation missed entirely — the edge cases and failure modes that only production traffic reveals.

The migration should be tracked with a metric visible to both the platform team and the customer: the **Organic Coverage Percentage**. This metric appears on the customer's eval dashboard and trends upward over time. A flat line — no increase in organic percentage over 90 days — triggers an alert. Either the team is not curating organic examples, the customer is not providing the domain expert review needed to validate organic candidates, or the production traffic is too low to generate enough examples. Each root cause requires a different intervention, but all require intervention. A customer who is still 100 percent synthetic after six months does not have an eval system. They have a prototype that nobody upgraded, as Section 4 on ground truth makes clear.

## Validation of Synthetic Ground Truth

Synthetic examples must be validated before they enter the golden set, and the validation must involve the customer's domain expertise, not just the platform team's judgment. The platform team can assess whether a synthetic example is structurally valid — correctly formatted, internally consistent, plausible. The customer's domain experts assess whether it is substantively valid — does this look like something we would actually see? Does the reference output represent how we would actually want the AI to respond?

Validation follows a two-pass process. In the first pass, the platform team reviews all generated examples for structural validity. They remove examples with internal contradictions, formatting errors, implausible values, or other artifacts of the generation process. LLM-generated examples in particular produce a consistent 8 to 15 percent defect rate on structural validity — outputs where the model generated a plausible-looking but factually impossible scenario (a medical example with contradictory symptoms, a legal example citing a statute that does not exist, a financial example with arithmetic errors). These defects must be caught before the customer sees them because a single implausible example undermines the customer's confidence in the entire golden set.

In the second pass, the customer's domain expert reviews the structurally valid examples for substantive validity. This review must be framed carefully. Do not send 200 examples and ask "are these good?" The customer's expert will skim the first 20, declare them fine, and move on. Instead, structure the review as a calibration exercise: present 30 examples one at a time, ask the expert to rate each on a 4-point scale (realistic, plausible, unlikely, impossible), and discuss any example rated unlikely or impossible. The discussion reveals domain-specific patterns that the generation process missed and produces guidance for regenerating the problematic examples. After the calibration review, the remaining examples are reviewed in batches with the domain expert applying the standards established during calibration.

The validation step adds one to two weeks to the onboarding timeline. This is time well invested. An unvalidated synthetic golden set is a loaded instrument — it will produce scores, but those scores may not measure what the customer cares about. A validated synthetic golden set is an imperfect but functional instrument — it measures approximately the right things with approximately the right standards while the team waits for organic data to replace the approximation with precision.

## The Hybrid Approach

The most effective synthetic ground truth strategy is not purely synthetic. It is a hybrid that uses synthetic data for coverage and organic data for calibration, even when organic data is scarce.

Even a customer with zero production history usually has something. They have a description of their use case. They have examples of what their competitors produce. They have internal documents that illustrate their domain. They have a few manually created examples from their product design phase. These are not production data, but they are organic in the sense that they come from the customer's actual domain rather than from a generation model's approximation.

The hybrid approach uses these organic fragments as calibration anchors. Generate 200 synthetic examples to provide coverage. Then take the 10 to 15 organic examples the customer can provide and use them to calibrate the synthetic set. Do the synthetic examples match the complexity, length, vocabulary, and domain specificity of the organic ones? If the organic examples average 800 words per input and the synthetic examples average 300, the generation prompt needs adjustment. If the organic examples use specialized terminology that the synthetic examples miss, the generation process needs domain vocabulary injection. The organic fragments do not provide coverage — 10 examples cannot cover 12 use case variants — but they provide the calibration signal that keeps the synthetic distribution honest.

This hybrid model produces an initial golden set where perhaps 5 to 10 percent of examples are organic calibration anchors and 90 to 95 percent are synthetic coverage examples. As production data flows, the organic percentage grows and the synthetic percentage shrinks. The calibration anchors remain throughout because they represent the customer's authentic quality standard, as described in Section 12 on dataset engineering. The synthetic examples are gradually replaced by organic production examples that the anchors helped calibrate against.

The result is an eval system that starts imperfect and improves continuously — which is exactly what it should be. No eval system is perfect on day one, whether the data is organic or synthetic. The difference is that a well-managed synthetic bootstrap makes its imperfection explicit, tracks the improvement trajectory, and gives the customer a clear timeline for when the eval system transitions from approximate to precise.

Chapter 6 moves from the customer lifecycle to the platform lifecycle — how multi-tenant release gates, adapter versioning, and upgrade governance ensure that changes to the shared platform do not silently degrade any individual tenant's quality.
