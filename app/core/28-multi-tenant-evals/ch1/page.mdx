# Chapter 1 — Why Multi-Tenant Evaluation Is a Different Discipline

Most AI teams build evaluation for a single product serving a single definition of quality. That works until you sign your tenth enterprise customer and discover that each one defines "good" differently, carries different risk, operates under different regulations, and expects proof tailored to their context. At that point, single-tenant evaluation does not just fall short. It becomes a liability — hiding customer-specific failures behind platform-wide averages that look healthy while individual contracts quietly erode.

This chapter establishes the foundational argument: multi-tenant evaluation is not single-tenant evaluation with a tenant ID filter. It is a different discipline with different architecture, different failure modes, and different organizational requirements. You will learn why platforms that treat all customers the same eventually lose the ones that matter most, and what it takes to build evaluation systems that serve hundreds of customers without collapsing under the complexity.

---

## What This Chapter Covers

- **1.1** — The Platform That Passed Every Eval and Lost Its Largest Customer
- **1.2** — Single-Tenant vs Multi-Tenant Evaluation: Why the Difference Is Architectural
- **1.3** — The Global Average Trap: How Platform Metrics Hide Customer Pain
- **1.4** — The Five Dimensions of Tenant Divergence: Quality, Risk, Data, Configuration, and Compliance
- **1.5** — When Customer-Specific Quality Becomes a Competitive Advantage
- **1.6** — The Spectrum of Multi-Tenancy: Shared Model to Dedicated Instance and Everything Between
- **1.7** — What Adjacent Sections Do Not Cover: The Unique Territory of Per-Customer Evaluation
- **1.8** — The Cost of Treating All Customers the Same

---

*Every multi-tenant platform starts by treating evaluation as a single-product problem. The ones that survive learn the difference before their largest customer teaches it to them.*
