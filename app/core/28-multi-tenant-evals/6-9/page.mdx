# 28.9 — Adapter Drift: When Tenant Fine-Tunes Age Out

The dashboard is green. Every tenant adapter passes its weekly eval run. The platform team ships a quarterly business review showing 97 percent of adapters above SLA thresholds. Then a healthcare customer calls. Their clinical summarization adapter, trained eight months ago on discharge summaries from their hospital network, has started producing outputs that their physicians flag as "stale." The adapter still scores well against the golden set from training time. But the hospital network changed its discharge template four months ago, adopted three new ICD-11 codes that did not exist when the adapter was trained, and shifted its patient population toward more complex multi-morbidity cases as a regional specialty center expanded. The adapter is performing perfectly against a world that no longer exists.

This is **adapter drift** — the gradual divergence between a per-tenant LoRA adapter's trained behavior and the tenant's current production needs. Unlike base model degradation, which affects every tenant simultaneously and triggers platform-wide alerts, adapter drift is silent, per-tenant, and invisible to any monitoring system that measures against historical ground truth rather than current production reality. The adapter has not gotten worse. The world around it has moved.

## How Adapter Drift Happens

Adapter drift has three root causes, and they frequently compound.

The first cause is **input distribution shift**. The data the tenant sends through the adapter today looks different from the data the adapter was trained on. A legal customer trained their contract review adapter on standard commercial lease agreements. Over eight months, their practice shifted toward complex multi-party development agreements with performance escrow clauses. The adapter was never exposed to these structures during fine-tuning. It does not fail catastrophically — the base model's general capabilities handle the unfamiliar clauses passably — but it loses the domain-specific precision that justified fine-tuning in the first place. The adapter was trained to be an expert on one kind of contract. The tenant now sends it a different kind.

The second cause is **output expectation evolution**. What the tenant considers "good" changes over time, even when the inputs stay the same. An insurance customer trained their claims assessment adapter to produce four-paragraph summaries with a neutral tone. Six months later, their compliance team mandated that all AI-generated claims assessments include explicit uncertainty language ("based on available documentation" and "subject to adjuster review") and a structured risk classification at the end. The adapter still produces clean four-paragraph summaries. They are no longer what the customer wants. The quality bar moved. The adapter did not.

The third cause is **domain vocabulary evolution**. Every specialized domain updates its terminology. Medical coding standards change. Regulatory frameworks issue new guidance with new defined terms. Industry jargon shifts as new technologies or practices emerge. A cybersecurity customer trained their threat analysis adapter in early 2025. By late 2025, the threat landscape vocabulary had shifted — new attack categories, new vulnerability classification frameworks, new compliance requirements from updated NIST guidelines. The adapter still uses the old vocabulary fluently. It sounds like a cybersecurity analyst from a year ago, which is not what a cybersecurity customer paying for current analysis wants.

These three causes are independent but correlated. A tenant whose input distribution shifts usually also experiences output expectation changes, because the new inputs demand new output patterns. A domain whose vocabulary evolves usually also sees distribution shifts, because the new terminology accompanies new concepts and document structures. Monitoring for only one cause misses the other two.

## Measuring Adapter Drift

Measuring drift requires comparing the adapter's performance against two reference points: the original training-time ground truth and the current production reality. The gap between these two measurements is the drift signal.

**Training-time evaluation** is the score the adapter achieves against the golden set that was curated when the adapter was trained. This score should be stable or slowly declining. If it drops sharply, the adapter itself may have been corrupted by a serving infrastructure change — a problem, but not drift. Stable training-time scores with declining production relevance is the signature of drift: the adapter is still doing what it was trained to do, but what it was trained to do is no longer what the tenant needs.

**Current production evaluation** is the score the adapter achieves against a rolling sample of the tenant's recent production traffic, judged by the tenant's current quality criteria. This requires periodically sampling recent production inputs, running them through the adapter, and evaluating the outputs against the tenant's current rubric — not the rubric from training time, but the rubric as it exists today, with any updates the tenant has made to their quality criteria, output format requirements, or compliance language.

The **drift score** is the difference between training-time evaluation performance and current production evaluation performance. An adapter that scores 92 against its training-time golden set but only 78 against current production samples has a drift score of 14 points. The absolute magnitude matters less than the trend. A drift score of 3 is normal — every adapter has minor distribution gaps. A drift score that increases by more than 2 points per quarter is accelerating and requires intervention.

Scheduled drift measurement should run monthly for all active adapters and weekly for adapters in high-churn domains — healthcare, cybersecurity, financial compliance, and any domain where regulatory or industry changes occur frequently. The measurement pipeline collects a stratified sample of the tenant's production traffic from the most recent 30 days, generates adapter outputs for those inputs, runs the tenant's current judge configuration against the outputs, and compares the aggregate score to the training-time baseline. This pipeline is distinct from the continuous quality monitoring that runs on every production request; it is a dedicated diagnostic that specifically isolates adapter-level drift from other quality factors.

## Drift Detection Signals

Beyond the drift score itself, three secondary signals indicate drift before the aggregate score moves enough to trigger an alert.

**Vocabulary divergence** measures whether the adapter's outputs use the same domain terminology as recent high-quality production examples. You compute the term frequency distribution of the adapter's outputs and compare it to the term frequency distribution of the tenant's most recent human-approved or high-scoring outputs. When new terms appear in the reference distribution that the adapter rarely produces, or when the adapter consistently uses terms that have fallen out of the reference distribution, vocabulary drift is present. A financial services adapter that still writes "Basel III" when the tenant's current documents reference "Basel III.1 finalization requirements" is showing vocabulary drift.

**Format compliance decay** tracks whether the adapter's outputs still match the tenant's current structural expectations. Many tenants update their output format requirements incrementally — adding a new section header, requiring a risk classification tag, changing the summary length guidelines. The adapter was trained on the old format. If the tenant updated their format expectations through prompt changes or rubric updates but did not retrain the adapter, the adapter's format compliance score will decline even if the content quality remains strong. Format compliance decay is the most common early signal of output expectation drift because format changes are the easiest for tenants to mandate and the hardest for an untrained adapter to absorb.

**Confidence calibration shift** measures whether the adapter's uncertainty patterns have changed relative to the inputs it processes. Adapters trained on familiar distributions produce outputs with consistent confidence patterns — they are confident on routine inputs and less confident on edge cases. When the input distribution shifts, the adapter may become uniformly uncertain (because many inputs are now partially unfamiliar) or inappropriately confident (because the inputs look superficially similar to training data but contain unfamiliar structure). Tracking the distribution of per-output confidence scores over time and flagging distributional changes surfaces drift before quality scores move.

## When to Recommend Retraining vs Incremental Updates

Not every drifted adapter needs full retraining. The intervention should match the drift cause and severity.

**Prompt-level adjustment** is appropriate when the drift is primarily in output format or style expectations. If the tenant changed their desired output structure but the adapter still understands the domain and produces accurate content, updating the system prompt or the few-shot examples in the adapter's serving configuration may be sufficient. This is the cheapest intervention — zero training cost, immediate deployment — but it only addresses surface-level drift. If the adapter's domain understanding has fallen behind, prompt adjustments mask the problem without fixing it.

**Incremental fine-tuning** is appropriate when the drift is moderate — a drift score between 5 and 12 points — and the root cause is input distribution shift with stable domain fundamentals. The tenant's data has evolved, but the underlying domain has not changed dramatically. Incremental fine-tuning takes the existing adapter weights and continues training on a small dataset of recent production examples that represent the new distribution. This is faster and cheaper than full retraining — typically 10 to 20 percent of the original training cost — and preserves the adapter's existing strengths while teaching it the new patterns. The risk is catastrophic forgetting: if the incremental dataset is too small or too narrowly focused, the adapter may lose capabilities on the original distribution while learning the new one. Mixing 30 percent historical examples with 70 percent recent examples in the incremental training set mitigates this risk.

**Full retraining** is necessary when the drift score exceeds 12 to 15 points, when the domain has fundamentally changed (new regulations, new industry standards, new product categories), or when the tenant's quality criteria have been substantially revised. Full retraining builds a new adapter from scratch using a comprehensive dataset that reflects the tenant's current needs. It is the most expensive intervention — full training cost plus the data curation effort — but it produces the most reliable reset. Full retraining should also be the default when the adapter was originally trained more than twelve months ago, regardless of drift score. Training techniques, base model capabilities, and serving infrastructure all evolve, and an adapter trained on a previous-generation base model may benefit from retraining on the current generation even if its quality has not visibly degraded.

## The Cost-Benefit of Proactive Retraining

Reactive retraining — waiting until a customer complains or an SLA breach triggers an alert — is cheaper in the short term and more expensive in every other way. The customer has already experienced degraded quality. Their trust in the adapter, and by extension in your platform, has eroded. Their internal stakeholders may have already flagged the quality issue to leadership. The support ticket, the root cause investigation, the emergency retraining, and the trust-rebuilding conversation collectively cost far more in engineering time and relationship capital than a proactive retraining would have cost in compute.

Proactive retraining follows a scheduled evaluation cadence. Every adapter is evaluated for drift quarterly. Adapters in high-churn domains are evaluated monthly. When the drift score crosses a configurable threshold — typically 8 to 10 points — the platform automatically generates a retraining recommendation that includes the detected drift cause, the estimated retraining cost, and the projected quality improvement. The recommendation goes to both the platform team and the customer's technical contact. The customer approves the retraining, the platform schedules it, and the adapter is updated before quality degradation becomes visible in production metrics.

The economics favor proactive retraining at scale. For a platform serving 300 tenants with per-tenant adapters, assume 15 percent of adapters drift past the threshold in any given quarter. That is 45 retraining jobs per quarter. At an average retraining cost of $200 to $600 per adapter (depending on dataset size and model complexity), the quarterly proactive retraining budget is $9,000 to $27,000. Compare that to the cost of one churned enterprise customer — typically $100,000 to $500,000 in annual contract value — and the math is not close. Proactive retraining is not a cost center. It is retention insurance, as Section 24 on system cost engineering makes clear.

## Operationalizing Drift Management at Scale

Managing adapter drift for hundreds of tenants requires automation, not heroics. The drift management pipeline has four stages.

The **collection stage** runs continuously, sampling each tenant's production traffic and storing recent examples in a per-tenant drift evaluation buffer. The buffer holds the most recent 30 days of production samples, stratified by input type and complexity.

The **measurement stage** runs on a scheduled cadence — monthly for standard tenants, weekly for high-churn tenants. It pulls from the drift evaluation buffer, generates adapter outputs for the sampled inputs, evaluates those outputs against the tenant's current quality criteria, computes the drift score, and stores the result in the adapter lifecycle record.

The **alerting stage** applies per-tenant drift thresholds. When a drift score crosses the threshold, it generates a retraining recommendation and routes it through the appropriate approval workflow. For self-service tenants, the recommendation appears in their dashboard with a one-click approval. For managed enterprise tenants, the recommendation goes to their customer success manager for discussion during the next business review.

The **execution stage** processes approved retraining jobs through the training pipeline described in Chapter 4 on adapter staleness — data curation, training execution, evaluation against both historical and current ground truth, staged rollout with the adapter canary process, and production deployment.

This four-stage pipeline transforms adapter drift from a per-tenant crisis into a platform-level maintenance operation. Individual tenants do not need to monitor their own adapters. The platform manages the lifecycle, surfaces recommendations, and executes approved retraining with the same operational discipline it applies to base model updates.

The challenge with drift is that it affects each tenant independently. But adapters do not exist in isolation — they share serving infrastructure with hundreds of other adapters, and that sharing introduces coupling effects that no individual adapter measurement captures. The next subchapter examines what happens when shared serving creates hidden interference between adapters that look healthy in isolation but degrade each other in production.
