# 28.10 — Config Hashing and Provenance: Reproducibility Per Tenant

The config hash is the single field in the Tenant Metadata Contract that transforms evaluation from a measurement activity into a reproducible science. Without it, you have scores. With it, you have scores that can be traced to the exact inputs that produced them, compared to scores from different runs with mathematical certainty about whether the inputs matched, and replayed months later to verify that the same configuration produces the same result. Reproducibility is not a luxury for multi-tenant evaluation. It is the mechanism that makes debugging possible, makes auditing tractable, and makes customer trust sustainable.

## What the Config Hash Captures

A config hash is a cryptographic fingerprint of every variable that affects an evaluation outcome. Think of it as the DNA of an eval run. Two runs that share the same config hash used identical inputs in every dimension. Two runs with different config hashes differ in at least one dimension, and the diff between the two configurations tells you exactly which one.

The inputs that a config hash must capture include the rubric definition — every dimension, every weight, every threshold, every scoring instruction. It includes the golden set — the specific version of the ground truth data used as the reference standard, identified by its version hash rather than embedded in its entirety. It includes the model version — which base model was used to generate the outputs being evaluated. It includes the adapter identifier and version — which per-tenant fine-tuned adapter was active, if any. It includes the judge pipeline definition — which judge model was used, which scoring prompt was applied, which calibration parameters were in effect. And it includes the sampling strategy — how outputs were selected for evaluation, including the sample size, the stratification rules, and the random seed if applicable.

Any change to any of these inputs produces a different config hash. A one-word change to the scoring prompt produces a different hash. A single new example added to the golden set produces a different hash. A model version bump from GPT-5.1 to GPT-5.2 produces a different hash. This sensitivity is the point. The hash is not an approximation. It is an exact fingerprint. If the hashes match, the configurations match. If the hashes differ, the configurations differ. There is no ambiguity.

## The Diagnostic Power of Config Hashing

The config hash answers the most common question in multi-tenant evaluation troubleshooting: "why did this score change?" When a tenant's quality score shifts from 91.3 to 87.8 between two evaluation runs, the first diagnostic step is to compare the config hashes of the two runs. If the hashes are identical, the configuration was the same, which means the score change reflects a genuine change in the model's behavior — a real quality shift that warrants investigation. If the hashes are different, the configuration changed between the two runs, which means the score change might be explained entirely by the configuration difference rather than by a model behavior change.

This distinction is not academic. It determines where the engineering team spends its investigation time. A real quality shift sends the team into model debugging — examining the outputs that scored lower, checking for model drift, reviewing recent model updates, analyzing whether the degradation is concentrated in specific output types. A configuration-driven score change sends the team into configuration review — determining which input changed, whether the change was intentional, and what effect it had. These are completely different investigation paths. Without the config hash, the team must pursue both paths in parallel, which doubles the investigation cost and often leads to confusion when the two paths produce contradictory conclusions.

In practice, most score changes in a multi-tenant environment are configuration-driven rather than model-driven. A rubric version update, a golden set refresh, a judge model upgrade — all of these change the config hash and all of them can shift scores. The config hash tells you immediately that the score change is explained by a configuration input change. The investigation collapses from "what is happening to this tenant's quality?" to "this rubric update changed the scoring, as expected." Five seconds of hash comparison replaces five hours of quality debugging.

## Building the Config Hash

The config hash must be deterministic, collision-resistant, and computed from a canonical representation of the configuration. Deterministic means that the same configuration always produces the same hash. Collision-resistant means that different configurations produce different hashes with overwhelming probability. Canonical representation means that meaningless differences in how the configuration is expressed — whitespace, field ordering, encoding differences — do not affect the hash.

The canonical form matters more than teams initially expect. Consider a rubric with five dimensions. If the dimensions are stored as an unordered collection, two identical rubrics with dimensions listed in different orders would produce different hashes. The same problem applies to judge parameters — if calibration parameters are represented as key-value pairs without a canonical ordering, identical parameter sets in different orders produce different hashes. False hash differences create false investigations — the team sees different hashes, spends time tracking down the configuration difference, and discovers that the difference is a serialization artifact rather than a real change.

The solution is to define a canonical serialization for every component of the evaluation configuration. Dimensions are sorted alphabetically by name. Parameters are sorted alphabetically by key. Nested structures are serialized in a fixed traversal order. Floating-point values are serialized with a fixed number of decimal places to avoid platform-dependent rounding differences. The canonical serialization is documented, tested with known inputs and expected outputs, and used exclusively for hash computation.

The hash algorithm should be SHA-256 or equivalent — fast enough to compute on every eval event without measurable overhead, collision-resistant enough to be used as an equality check without further verification. The full configuration is not stored with every event. The hash is stored with every event, and the full configuration is stored once in a configuration registry keyed by hash. This design keeps event storage compact while maintaining full traceability — given a hash, you can always retrieve the complete configuration that produced it.

## The Configuration Registry

The configuration registry is the system that maps config hashes to their full configurations. It is append-only — once a configuration is stored, it is never modified or deleted. This immutability is a requirement, not a preference. If a configuration can be modified after being stored, the hash no longer reliably represents what was active at the time of the eval. Immutability guarantees that any hash retrieved from a historical eval event maps to the exact configuration that was used at the time.

The registry serves three consumers. The first is the diagnostic workflow. When an engineer investigates a score change and finds that the config hashes differ between two runs, they query the registry for both hashes and diff the full configurations. The diff shows exactly what changed — "the rubric added a compliance language dimension weighted at 15 percent" or "the judge model was upgraded from Claude Sonnet 4.5 to Claude Opus 4.5" or "fourteen new examples were added to the golden set." The engineer knows immediately what drove the score change.

The second consumer is the audit workflow. When a customer or regulator asks "what evaluation criteria were applied to produce this score?", the team retrieves the config hash from the eval event, queries the registry for the full configuration, and presents the complete evaluation setup that was active at that time. This is not a reconstruction or an approximation. It is the exact configuration, preserved immutably since the moment it was used. The audit response is instantaneous and irrefutable.

The third consumer is the reproducibility workflow. When a team needs to re-run a historical evaluation — to verify a score, to test a hypothesis, or to resolve a dispute — they retrieve the full configuration from the registry and execute the evaluation with those exact inputs. If the model, adapter, and judge are all still available, the re-run should produce the same score within the bounds of any stochastic variation in the judge's scoring. If the re-run produces a materially different score, something in the pipeline has changed in a way that the config hash did not capture — a finding that itself is valuable and should trigger an investigation.

## Provenance Chains: Tracing Scores to Their Roots

A config hash tells you what configuration was used. A **provenance chain** tells you how that configuration came to exist. Provenance is the sequence of decisions, changes, and dependencies that produced the current evaluation setup for a given tenant.

Consider a tenant whose current evaluation configuration includes rubric version 3.2, golden set version 7, judge model Claude Opus 4.6 with scoring prompt version 4.1, and adapter version 12. The config hash fingerprints this combination. The provenance chain traces how each component arrived at its current version. Rubric version 3.2 was created in the tenant's six-month review, when the quality stakeholders added two compliance dimensions and adjusted the weight on factual accuracy. Golden set version 7 was refreshed in January 2026 when the monthly freshness check identified twelve stale examples. Judge model Claude Opus 4.6 was adopted as the platform default in November 2025. Scoring prompt version 4.1 was updated when the judge model changed, because the new model required adjusted scoring instructions. Adapter version 12 was retrained in December 2025 on a fresh training set.

Each of these is a node in the provenance chain. Each node records who made the change, when, why, and what the previous version was. The chain is stored separately from the configuration registry — the registry stores what was used, and the provenance system stores how it got that way. Together, they provide complete traceability. An auditor who asks "why is this tenant's rubric weighted this way?" can trace the answer from the current rubric version through every version change back to the original onboarding rubric. A customer success manager who asks "when did this tenant's golden set last change?" can query the provenance chain for the most recent change event and see the date, the reason, and the scope of the change.

## Cross-Tenant Provenance Comparison

Provenance chains become especially powerful when compared across tenants. Two healthcare tenants may have similar evaluation configurations — both measure factual accuracy, clinical terminology precision, and HIPAA-relevant disclosure patterns. Their config hashes are different because the specifics differ: different rubric weights, different golden set examples, different adapter versions. But their provenance chains share a common root — both inherited from the healthcare domain template during onboarding. The provenance comparison shows where they diverged: tenant A adjusted clinical terminology precision to weight 25 percent after their six-month review, while tenant B left it at the template default of 15 percent.

This comparison is useful operationally. When the platform team discovers that tenant A's higher weight on clinical terminology precision produces more clinically accurate outputs, they can recommend the same adjustment to tenant B. When a regulatory change requires all healthcare tenants to add a new compliance dimension, the provenance system identifies which tenants are on which rubric versions and which need the update. Without provenance, the team must audit each tenant's configuration individually, which at scale — fifty healthcare tenants, each with a different rubric history — is a multi-day project. With provenance, it is a query.

## Detecting Config Drift

Over time, a tenant's evaluation configuration can drift from its intended state through a series of individually reasonable small changes. A judge calibration parameter is adjusted. A golden set example is added. A sampling strategy is tweaked to increase coverage of a specific output type. Each change produces a new config hash, but the cumulative effect of many small changes can be significant. The rubric that was designed as a coherent whole at the six-month review has been patched twelve times since then, and the current version may no longer reflect a coherent quality philosophy.

Config drift is detected by tracking the rate of config hash changes per tenant over time. A tenant whose config hash changes once per month has a stable configuration. A tenant whose config hash changes three times per week has a rapidly evolving configuration that may be drifting rather than improving. The drift detector flags tenants with high config change rates and routes them to a configuration review — a human examination of whether the accumulated changes still form a coherent evaluation strategy or whether a rubric rearchitection is needed, as covered in the previous subchapter's baseline reset protocol.

The drift detector also flags configuration divergence from the domain template. A healthcare tenant who has diverged significantly from the healthcare template — more than 40 percent of rubric dimensions changed, more than half the golden set examples replaced — has effectively created a custom evaluation strategy. This may be appropriate for their specific use case, but it means they are no longer receiving the benefit of template-level improvements. When the platform team improves the healthcare template — adding a new dimension, refining a scoring instruction — the divergent tenant does not automatically inherit the improvement. The drift detector ensures that this divergence is visible and that the platform team can make an explicit decision about whether to reconcile the tenant's configuration with the updated template.

## The Reproducibility Guarantee

The combination of config hashing, the configuration registry, and provenance chains produces what amounts to a reproducibility guarantee for per-tenant evaluation. For any eval event, at any point in the past, you can answer four questions. What was evaluated — which outputs, from which model, with which adapter? How was it evaluated — which rubric, which judge, which golden set, which sampling strategy? Why was it evaluated that way — what provenance chain led to that configuration? And can you reproduce the result — given the stored configuration, can you re-run the evaluation and get the same score?

The first three questions are answered by querying the metadata, the configuration registry, and the provenance system. The fourth question requires that the platform retain enough infrastructure to re-execute historical configurations. This means not just storing the config hash but retaining access to historical model versions, adapter snapshots, judge model versions, and golden set versions. As Section 18 covers in detail, retention policies for evaluation infrastructure must balance storage costs against the practical need for reproducibility.

For most platforms, a twelve-month reproducibility window is adequate — you can reproduce any eval result from the last twelve months. Beyond that, the config and provenance records are retained permanently, but the ability to re-execute the evaluation depends on whether the historical model and judge versions are still available. This tiered approach acknowledges that full reproducibility has a storage cost while ensuring that the metadata trail is permanent.

The reproducibility guarantee is not just technically satisfying. It is commercially and legally necessary. A customer who disputes a quality score from three months ago needs you to prove what was measured and how. A regulator examining your AI system under the EU AI Act's August 2026 high-risk compliance requirements needs you to demonstrate that your quality assurance process is traceable and reproducible. A partner considering an expansion deal needs confidence that the quality numbers in your quarterly business review are backed by a rigorous measurement process. Config hashing and provenance are the mechanisms that convert eval scores from assertions into evidence. The next subchapter addresses a different dimension of tenant isolation — the physical location where evaluation runs, and why running evals outside a tenant's legal jurisdiction is a compliance violation that no config hash can fix.
