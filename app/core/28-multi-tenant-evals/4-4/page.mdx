# 28.4 — The Tenant Drift Detector: Catching Per-Customer Quality Degradation Before the Customer Does

Three days after deploying a base model upgrade from Llama 4 Scout to Llama 4 Maverick in November 2025, a B2B contract analysis platform received escalations from two enterprise customers within the same hour. One, a mid-size insurance carrier paying $68,000 annually, reported that clause extraction accuracy on their German-language reinsurance contracts had fallen from 93 percent to 74 percent. The other, a logistics company paying $41,000 annually, reported that their custom entity extraction pipeline was returning duplicate entities at a rate that made downstream automation unreliable. Platform-wide accuracy had improved from 88.1 percent to 89.6 percent with the new base model. The two customers' regressions were invisible in the aggregate — the insurance carrier's German reinsurance traffic represented 0.4 percent of total platform volume, and the logistics company's entity extraction represented 0.7 percent. The platform's monitoring dashboard, which tracked six aggregate metrics across all 230 tenants, showed green. Two customers were in active quality crises. Nobody at the platform knew until the customers told them.

The investigation revealed a pattern the team would later see repeatedly. Both customers ran per-tenant LoRA adapters fine-tuned on their domain data. The base model upgrade changed the internal representations that those adapters relied on, degrading the adapters' effectiveness. This is the **Adapter-Base Regression** pattern: when a base model upgrade improves the platform overall but breaks per-tenant adapters that were tuned to the previous base model's representations. The platform's monitoring system could not detect it because the monitoring system tracked platform averages, not per-tenant quality trends. The $109,000 in combined contract value was at risk not because the platform lacked monitoring, but because the monitoring was blind to per-tenant degradation.

## The Global Average Trap Revisited

Chapter 1 of this section introduced **The Global Average Trap** — the failure mode where platform-wide metrics hide customer-specific quality degradation. The Tenant Drift Detector is the engineering answer to that trap. It is the system that answers a deceptively simple question: "Is Customer 247 getting worse this week compared to their own baseline?" — independently of what the platform average says.

The reason this question is deceptively simple is that it requires per-tenant statistical reasoning that most monitoring systems do not provide. Standard monitoring tracks platform-wide metrics over time. When a metric drops, an alert fires. When a metric is stable, no alert fires. This works when all customers experience the same quality. In a multi-tenant platform, it fails silently and repeatedly.

Consider the math. Your platform serves 300 tenants. A base model upgrade improves quality for 285 tenants by an average of 1.8 points. It degrades quality for 15 tenants by an average of 7 points. The platform-wide metric improves by approximately 1.4 points — well within "healthy" territory on your dashboard. The 15 degraded tenants, who together represent $1.1 million in annual recurring revenue, are experiencing a significant regression that your monitoring system cannot detect because the system does not compute per-tenant metrics. The degradation is not a rounding error in the aggregate. It is a real, measurable regression for specific customers. But your monitoring system does not look at specific customers.

The Tenant Drift Detector closes this gap by maintaining independent quality baselines for each tenant and detecting deviations from those baselines regardless of what the platform average does. When Customer 247's accuracy drops by 6 points while the platform average rises by 1.4 points, the drift detector fires an alert for Customer 247. The alert says: this customer is experiencing quality degradation relative to their own history, and the degradation is statistically significant, and here is when it started. The platform dashboard can show green. The drift detector for Customer 247 shows red. Both are correct, and only the second one protects the customer relationship.

## Per-Tenant Baselines: What Normal Looks Like for Each Customer

The foundation of drift detection is the per-tenant baseline — a statistical profile of what "normal" quality looks like for a specific customer over a specific time window. Without a baseline, you cannot distinguish between a quality drop and normal variation. A score of 87 percent is a regression if the customer's baseline is 93 percent and noise if the baseline is 86 percent. The baseline gives meaning to individual measurements.

Building a per-tenant baseline requires three decisions. The first is the metric set. Which quality metrics do you track per tenant? At minimum, track the composite quality score from the tenant's Quality Contract, plus each dimension score individually. A composite score that looks stable can hide a dimension-level regression — overall quality at 90 percent while citation fidelity dropped from 88 to 71 percent, masked by improvement in other dimensions. Track the composite and the components.

The second decision is the baseline window — the historical period used to compute what "normal" looks like. A 30-day rolling window works for most tenants. It is long enough to smooth out daily variation but short enough to adapt to genuine, gradual quality changes. Shorter windows — 7 days — are more sensitive to sudden changes but produce more false alarms from natural variation. Longer windows — 90 days — are more stable but slower to adapt and slower to alert. Some platforms use adaptive windows that shorten after a configuration change and lengthen during stable periods. The right choice depends on your false alarm tolerance and your customer communication cadence. If you report quality monthly, a 30-day baseline aligns with the reporting cycle. If you report weekly, a 14-day baseline may be more appropriate.

The third decision is the baseline statistics. A mean alone is insufficient because it ignores variance. A tenant whose daily accuracy oscillates between 85 and 95 percent has a mean of 90 percent but a very different quality profile than a tenant whose daily accuracy oscillates between 89 and 91 percent. The baseline should include at least the mean, standard deviation, and percentile distribution of each tracked metric over the baseline window. This lets the drift detector distinguish between a score that is low-but-normal (within the tenant's historical variance) and a score that is low-and-anomalous (below the tenant's historical range).

## Detection Methods: Four Patterns of Per-Tenant Drift

Per-tenant quality degradation manifests in four distinct patterns, each requiring a different detection approach. A comprehensive drift detector watches for all four simultaneously.

The first pattern is **score distribution shift**. The tenant's quality scores move to a new, lower center. Yesterday, scores clustered around 92 percent. Today, they cluster around 84 percent. This is the most obvious form of drift and the easiest to detect. A simple threshold — "alert when the tenant's rolling mean drops by more than N points below their baseline mean" — catches it. The threshold should be calibrated per tenant based on their historical variance. A tenant with low variance (standard deviation of 1.5 points) should alert on a smaller shift than a tenant with high variance (standard deviation of 4 points). A useful heuristic is to alert when the rolling mean drops by more than two standard deviations below the baseline mean. For the low-variance tenant, that means an alert at 3 points below baseline. For the high-variance tenant, 8 points. This prevents false alarms from normally variable tenants while still catching real regressions for stable tenants.

The second pattern is **error pattern change**. The tenant's overall score may be stable, but the types of errors change. Previously, errors were formatting issues — correct content, wrong structure. Now, errors are factual — wrong content, correct structure. The composite score may be identical because the total error count is similar, but the nature of the errors has shifted in a way that matters to the customer. Detecting this requires tracking not just scores but error categorization per tenant. When the distribution of error categories shifts — more factual errors, fewer formatting errors — the detector should alert even if the composite score is unchanged.

The third pattern is **latency degradation**. The tenant's response times increase gradually or suddenly. Latency drift is often the first signal of a deeper quality issue — a retrieval pipeline that is returning more results and taking longer to process, a model that is generating longer outputs because it is less certain, a tool call that is timing out and falling back to a slower path. Latency monitoring per tenant follows the same baseline approach: track the tenant's P50, P90, and P99 latency over the baseline window, and alert when current values exceed the baseline by a configurable margin. A P99 latency that moves from 3.2 seconds to 5.8 seconds is worth investigating even if the tenant's quality scores look stable.

The fourth pattern is **tool failure rate increase**. For agentic platforms where tenants have tool access, the rate at which tool calls fail is a quality signal that is distinct from output quality. A tenant whose tool failure rate increases from 2 percent to 12 percent is experiencing a capability regression even if the outputs that do not involve tool calls maintain their quality. Tool failure monitoring per tenant tracks tool call success rates, broken down by tool type, and alerts when any tool's failure rate exceeds the baseline by a configurable margin.

## Alert Triggering and Severity Classification

Not every drift signal warrants the same response. A 2-point score drop for a self-serve customer paying $200 per month does not justify the same escalation as a 2-point drop for an enterprise customer paying $120,000 per year. Drift alerts need severity classification that accounts for both the magnitude of the drift and the business context of the affected tenant.

A four-tier severity model works in practice. Severity one is critical: the tenant's composite score has dropped below their SLA threshold, or a floor-level check is failing. This is an active SLA breach. Response time should be under one hour. The alert routes to the on-call engineer and the customer success manager simultaneously. The customer should be proactively notified before they discover the issue.

Severity two is high: the tenant's composite score has dropped significantly — more than two standard deviations below baseline — but has not yet breached the SLA threshold. This is a regression that will likely become an SLA breach if it continues. Response time should be under four hours. The alert routes to the eval team for investigation.

Severity three is moderate: one or more dimension scores have shifted significantly, but the composite score remains within acceptable range. This could indicate an emerging regression on a specific quality dimension or a shift in error patterns. Response time should be within one business day. The alert routes to the tenant monitoring queue for analysis.

Severity four is informational: the drift detector has flagged a pattern worth noting — a gradual trend, a minor shift, a latency increase — but the change is not yet operationally significant. These are logged and reviewed in weekly quality reviews. They create the early warning signal that lets your team investigate before the drift becomes a customer-visible issue.

The severity thresholds should be configurable per tenant tier. Enterprise tenants may warrant severity-two treatment at a drift magnitude that would be severity-three for standard-tier tenants, because the relationship value and the contractual consequences are proportionally higher.

## The Cold-Start Problem: New Tenants Without Baselines

When a new customer onboards, they have no history. The drift detector has no baseline. It cannot tell whether the customer's first week of quality scores represents "normal" or "degraded" because there is no prior normal to compare against.

The cold-start problem is not just a statistical inconvenience. It is a customer experience risk. New customers are in their highest-churn-risk window — they are evaluating whether the platform meets their expectations, and any quality issue during onboarding disproportionately damages the relationship. This is precisely when you most need drift detection, and precisely when your standard approach does not work.

Three strategies address the cold start. The first is **cohort-based bootstrapping**. When a new tenant onboards, assign them to a cohort of existing tenants with similar configurations — same model, similar prompt complexity, same tier, similar use case domain. Use the cohort's quality distribution as the new tenant's provisional baseline. If existing healthcare tenants on the same model and tier have accuracy distributions centered at 91 percent with a standard deviation of 2.3 points, use that as the new tenant's starting baseline. The provisional baseline is imprecise, but it is far better than nothing. It lets the drift detector fire meaningful alerts from day one, even if those alerts have wider confidence intervals than alerts for established tenants.

The second strategy is **accelerated baseline building**. During the onboarding period, run evaluation more frequently — hourly instead of daily, or on every tenth request instead of a sampled batch — to accumulate baseline data faster. A standard tenant might take 30 days to build a reliable baseline with daily eval runs. An accelerated schedule can build a usable baseline in 7 to 10 days. The cost of extra eval compute during onboarding is trivial compared to the cost of missing a quality regression during the customer's first month.

The third strategy is **explicit baseline negotiation**. During onboarding, work with the customer to define expected quality levels for their use case. The Quality Contract from Chapter 2 provides the framework. If the customer says "we expect at least 90 percent accuracy on clause extraction," that target becomes the initial baseline. The drift detector treats any sustained period below 90 percent as drift, even without historical data. As the tenant accumulates history, the detector transitions from the negotiated baseline to the data-driven baseline, and the alert thresholds adapt accordingly.

In practice, most platforms combine all three: cohort bootstrapping for immediate coverage, accelerated evaluation for faster data accumulation, and negotiated baselines for customer-aligned thresholds. The transition from cold-start mode to established mode happens automatically as the tenant's historical data reaches the minimum window size, typically 14 to 21 days after onboarding.

## Practical Implementation: What to Store, How Often to Compute, How to Scale

Building a drift detector for 500 tenants is not a research project. It is an infrastructure project that requires deliberate decisions about storage, compute cadence, and scaling patterns.

For storage, you need a time-series database that holds per-tenant quality metrics at daily granularity (or finer, depending on your eval cadence). Each entry includes the tenant identifier, the evaluation timestamp, the configuration hash from the configuration registry described in subchapter 4.2, the composite quality score, each dimension score, the error category distribution, and the latency percentiles. At 500 tenants with daily eval runs tracking 8 dimensions each, you produce roughly 4,000 metric entries per day, or 1.5 million per year. This is a modest data volume that any time-series database handles comfortably. The storage cost is negligible — the value of the data is immense.

For compute cadence, the drift detection computation runs after each eval batch completes. It reads the tenant's baseline statistics from the time-series store, compares the latest eval results against the baseline, applies the detection rules for all four drift patterns, and generates alerts if any rule triggers. The computation per tenant is lightweight — statistical comparisons against pre-computed baselines. At 500 tenants, the total drift detection pass takes seconds, not minutes. The bottleneck is never the drift detection math. It is the upstream eval pipeline that produces the scores the detector consumes. Section 15 covers eval pipeline optimization, and Section 26 covers scaling eval compute to high volumes — the drift detector sits downstream of both.

For scaling beyond 500 tenants, the primary challenge is not compute but alert management. At 500 tenants with four drift patterns and four severity levels, you can produce a volume of alerts that overwhelms the team responsible for investigating them. The solutions are familiar from SRE practice: alert deduplication (multiple drift signals for the same tenant within the same eval cycle become a single alert), alert aggregation (if 30 tenants all show drift after a model update, group them into a single investigation), and noise tuning (adjust per-tenant thresholds based on historical false alarm rates). Observability platforms like Langfuse and Portkey provide the per-request tracing and metadata tagging that feed into per-tenant metric aggregation. Portkey's tenant-level analytics and Langfuse's per-user tracking are designed for exactly this use case — routing LLM observability data by customer identifier so that downstream analysis can operate at tenant granularity.

## The Adapter-Base Regression Pattern

The contract analysis platform's November 2025 incident that opened this subchapter was not an isolated event. It is an instance of a recurring pattern that every platform using per-tenant LoRA adapters must monitor for.

**Adapter-Base Regression** occurs when a base model upgrade changes the internal representations that per-tenant adapters depend on. LoRA adapters work by adding low-rank adjustments to the base model's attention weights. Those adjustments are calibrated to the base model's specific weight values. When the base model changes — a new version, a fine-tuning pass, even a quantization change — the adapter's adjustments are now calibrated to weights that no longer exist. The adapter still applies. It still produces outputs. But the outputs degrade because the adapter is adjusting a model it was not trained for.

The drift detector catches Adapter-Base Regression by correlating per-tenant quality changes with base model deployment events. When a base model upgrade deploys and tenant quality scores drop within 48 hours for tenants running adapters trained on the previous base model, the pattern is clear. The alert should include the base model version change, the list of affected tenants, and the adapter versions those tenants are running. The remediation is to retrain or recalibrate the affected adapters on the new base model — a process that takes days to weeks depending on adapter count and the availability of training data.

Prevention is better than detection. Before deploying a base model upgrade, the drift detector's infrastructure can run a pre-deployment shadow evaluation: load each per-tenant adapter against the new base model, run the tenant's eval suite against the adapter-plus-new-base combination, and compare results against the tenant's baseline. If any tenant shows a quality drop exceeding the severity-two threshold, the deployment is blocked until the affected adapters are retrained or the deployment plan includes per-tenant remediation. This shadow evaluation approach is covered in detail in subchapter 4.10, which addresses the broader challenge of evaluating base-plus-adapter interactions.

The Tenant Drift Detector does not eliminate per-tenant quality issues. It converts them from invisible, customer-reported crises into detected, team-investigated incidents. That conversion — from reactive to proactive, from customer-discovered to platform-detected — is the difference between losing customers and keeping them. The next subchapter covers what each customer needs to see in their quality dashboard, and what they must never see, to maintain both transparency and tenant isolation.
