# 28.70 — Report Diffs: Showing What Changed Since Last Month

The quality report lands in the customer's inbox. Accuracy: 89 percent. Tone compliance: 94 percent. Hallucination rate: 2.1 percent. The customer's VP of Engineering glances at the numbers, confirms they look reasonable, and archives the email. Nothing happened. No insight was gained. No action was taken. The report served its contractual purpose — proof that quality is above the floor — and added zero operational value.

Now imagine a different report. Accuracy: 89 percent, up 3 points from last month. Tone compliance: 94 percent, unchanged. Hallucination rate: 2.1 percent, down from 3.4 percent following the November model update. The same numbers, but with context that transforms them from a static snapshot into a story. The VP of Engineering reads this version differently. The accuracy improvement validates the prompt tuning their team invested in last month. The hallucination reduction confirms that the platform's model upgrade delivered on its promise. The stable tone compliance tells them they do not need to revisit their tone rubric. Three metrics, three deltas, three decisions informed. That is the power of a **report diff**.

## Why Absolute Numbers Are Necessary but Not Sufficient

Absolute quality scores answer the question "where do we stand?" Report diffs answer the question "where are we going?" Both questions matter, but the second one drives action.

A customer whose accuracy has been 89 percent for six consecutive months has a stable system. A customer whose accuracy was 92 percent three months ago, dropped to 86 percent two months ago, and recovered to 89 percent last month has a volatile system. The current absolute score is identical. The trajectory is completely different. The first customer can focus on other priorities. The second customer needs to investigate what caused the drop, whether the recovery is durable, and whether the underlying instability has been resolved. Without the diff, both customers receive the same "89 percent" and draw the same (incorrect) conclusion.

Report diffs also create accountability on both sides of the platform relationship. When a delta shows improvement following a platform model update, the platform team gets credit for the work they did. When a delta shows degradation following a change in the customer's data distribution, the customer sees the effect of their own operational decisions. Without diffs, every quality report is a standalone number that neither party can connect to specific actions. With diffs, the report becomes a feedback loop that links actions to outcomes.

## The Diff Structure

A well-designed report diff compares each metric in the current period against the same metric in the previous period, producing three elements per metric: the current value, the previous value, and the delta.

The delta is not just a number. It includes a **direction indicator** (improved, degraded, or stable), the **magnitude** of the change, and a **significance flag** that distinguishes meaningful changes from statistical noise. A 0.3-point improvement in accuracy when your eval sample is 200 examples is within the margin of error. Flagging it as an improvement misleads the customer into believing something changed when the variation is indistinguishable from random fluctuation. The significance threshold depends on your sample size, your scoring method, and the variance of the metric. For most enterprise customers with eval sets of 200 to 500 examples, changes smaller than 1.5 to 2 points are typically not statistically significant and should be reported as "stable" rather than as improvements or degradations.

Beyond per-metric deltas, the diff includes a **composite delta** that summarizes the overall quality trajectory. This is the single number the executive reads. "Overall quality improved by 2 points this month" or "overall quality is stable" or "overall quality degraded by 4 points, driven primarily by a decline in factual accuracy." The composite delta must be accompanied by a brief attribution — which metrics drove the change and, when possible, what caused them to change.

## Automated Change Attribution

The most valuable part of a report diff is not the delta itself but the explanation of why the delta occurred. A 4-point drop in accuracy with no explanation creates anxiety. A 4-point drop in accuracy attributed to a specific model update, with a note that the platform team is investigating and expects resolution within two weeks, creates confidence — not in the current number, but in the platform's operational maturity.

**Automated change attribution** connects score changes to specific events in the platform's operational timeline. The attribution engine correlates quality deltas with four categories of events.

The first category is **platform changes**: model updates, serving infrastructure modifications, shared prompt template changes, and judge model version updates. When a metric shifts and the shift correlates temporally with a platform change, the attribution engine flags the change as "likely related to model update on November 12" or "coincides with judge model migration on December 1."

The second category is **customer configuration changes**: rubric modifications, golden set updates, adapter retraining, and prompt architecture changes made by or for the specific customer. A customer who updated their golden set to include 30 new examples last month may see score changes that reflect the expanded coverage rather than any change in actual model performance. The attribution engine should flag these: "Golden set expanded by 30 examples on November 8. Score changes may reflect broadened evaluation coverage rather than model performance shift."

The third category is **data distribution shifts**: changes in the volume, mix, or characteristics of the customer's production inputs. If a financial services customer's input distribution shifted from 80 percent English to 60 percent English and 20 percent German after they expanded into European markets, quality scores that were calibrated on an English-heavy distribution may shift simply because the input mix changed. Attribution should flag distribution changes detected by the monitoring pipeline.

The fourth category is **golden set or rubric updates themselves**. If you updated the scoring rubric mid-period — perhaps adding a new dimension or adjusting weights — the score change may be an artifact of the measurement change, not a change in what is being measured. This is the most misleading type of delta because it looks like a quality change but is actually a definition change. Your attribution engine must clearly distinguish "accuracy changed because the model improved" from "accuracy changed because we redefined how accuracy is measured."

## The Presentation Challenge

An enterprise customer with 12 quality dimensions, each compared to the previous period with deltas, significance flags, and attribution notes, faces an information density problem. Showing all 47 data points on a single page overwhelms the reader and buries the signal in noise.

The solution is a **layered presentation** with three tiers. The first tier is the executive summary: one to three sentences describing the overall quality trajectory and the most important changes. "Quality improved 2 points overall, driven by a 4-point accuracy gain after the November model update. One metric, response completeness, degraded by 3 points and is under investigation." This is what the VP reads.

The second tier is the metric-level summary: a compact view showing each metric's current value, delta, and significance flag. Metrics are ordered by impact — the largest significant changes appear first, stable metrics appear last. This is what the technical lead reviews. They scan for any metric flagged as a significant degradation and ignore the stable ones.

The third tier is the detailed attribution view: for each metric that showed a significant change, a full explanation of the likely cause, the evidence supporting the attribution, and the recommended action. This is what the platform engineer or the customer's eval specialist reads when they need to investigate a specific change.

Generating all three tiers automatically from the same underlying data ensures consistency and eliminates the manual effort of writing executive summaries for every customer every month. The diff engine computes the deltas and attributions. The report generator formats them into the three tiers. The delivery system routes the right tier to the right audience.

## Handling False Alarms

Not every delta deserves attention. A 1.5-point drop in a metric with high natural variance is noise. A 2-point improvement in a metric that always fluctuates by 3 points is not a trend. Reporting every fluctuation as meaningful trains the customer to ignore your reports entirely — the quality report equivalent of alert fatigue.

Statistical significance testing is the first defense. For each metric, compute the confidence interval based on sample size and historical variance. If the delta falls within the confidence interval, report the metric as "stable" regardless of the direction of the change. Only flag changes that exceed the confidence interval as "improved" or "degraded."

The second defense is **trend confirmation**. A single-period delta is a data point. Three consecutive periods of decline is a trend. Your diff engine should distinguish between single-period fluctuations and confirmed trends. When a metric has degraded for three consecutive periods, the report should flag it differently — not as "degraded 2 points this month" but as "degraded for the third consecutive month, cumulative decline of 6 points." This gives the customer a stronger signal that warrants action, rather than a series of small deltas that individually seem unimportant.

The third defense is **customer-specific sensitivity calibration**. Different customers have different tolerances for score volatility. A customer whose quality scores have historically been stable within a 2-point band should be alerted when a metric moves 3 points. A customer whose scores routinely fluctuate by 5 points should only be alerted for changes beyond that natural range. The diff engine should compute per-customer, per-metric baselines for normal variance and flag only deviations from that customer's specific normal range.

## Time Window Flexibility

Different stakeholders need different comparison windows. The monthly comparison is the default for operational reviews. The quarterly comparison matters for business reviews and SLA assessments. The year-over-year comparison matters for contract renewals and executive presentations.

Your report diff system should support configurable comparison windows without requiring separate report generation logic. The underlying data is the same — per-metric scores by period. The comparison window simply determines which two periods are placed side by side. When a customer's account executive needs to show the customer's quality trajectory over the past year for a renewal conversation, the system produces a 12-month trend with quarterly rollups. When the platform engineering team needs to isolate the effect of last week's model update, the system produces a week-over-week comparison. When the customer's compliance team needs to demonstrate continuous monitoring for an audit, the system produces a monthly series covering the audit period.

Store granular per-period scores and compute comparisons on demand rather than pre-computing every possible comparison. A customer with 12 metrics and 52 weeks of history has 624 metric-period data points. Computing any comparison window from that data takes milliseconds. Pre-computing every pairwise comparison wastes storage and creates a maintenance burden when new metrics are added.

## Diffs as an Accountability Mechanism

The most underappreciated function of report diffs is the accountability they create for both the platform and the customer.

When the platform ships a model update that improves accuracy by 3 points for a customer, the diff proves it. That proof becomes a tangible value narrative at renewal time. "Since you signed in January, your accuracy has improved by 7 points across three model updates." Without diffs, the customer's memory of quality improvement fades. They remember the current number, not the journey. Diffs make the journey visible and give the platform credit for the work behind each improvement.

When the customer changes their data distribution or prompt architecture and quality drops, the diff shows that too. It is not accusatory — it is informational. "Accuracy dropped 4 points this month. The change correlates with the prompt restructuring your team deployed on November 15. We recommend reviewing the updated prompts against the accuracy rubric to identify which changes may have introduced the degradation." The diff gives the customer the information they need to diagnose their own decisions without the platform having to say "this is your fault."

This two-sided accountability transforms the customer relationship from a vendor-client dynamic into a partnership where both sides can see the effects of their actions. It makes the quality report a shared operational tool rather than a contractual compliance artifact. And it gives the account team a conversation starter for every customer meeting that is grounded in data rather than opinion.

The next subchapter addresses a different kind of proof — not what changed, but where data traveled, and why proving that eval data never left a designated region is becoming a non-negotiable procurement requirement.
