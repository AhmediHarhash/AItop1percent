# 28.6 — Audit Trails for Multi-Tenant Evaluation: Proving What Ran, When, and For Whom

When a regulated customer asks you to prove that their last quarterly evaluation ran correctly, what evidence can you produce? Not a dashboard screenshot. Not a Slack message from the engineer who ran it. Evidence. The kind that survives a compliance audit, a legal discovery request, or a customer dispute where millions of dollars in contract value hang on whether your platform can demonstrate that the evaluation actually happened, used the correct configuration, applied the right rubric, and produced results that were not contaminated by another tenant's data.

Most multi-tenant platforms can tell you a score. Very few can tell you the provenance of that score — the complete, tamper-evident chain of decisions, configurations, and computations that produced it. In single-tenant systems, this gap is inconvenient. In multi-tenant systems serving regulated enterprises, it is an existential risk. Your customer's auditor does not care what your dashboard shows. They care what your logs prove.

## What an Audit Trail Must Capture

An evaluation audit trail is the complete record of everything that influenced an eval run's outcome. Not just inputs and outputs — every decision point, every version reference, every environmental condition. The reason this level of detail matters is that audit questions are always retrospective and always specific. "What model version was Tenant 114 evaluated against on March 14?" "Did the rubric used for Tenant 88's Q1 compliance eval include the updated financial accuracy dimension that was added in February?" "Were any of Tenant 203's evaluation samples also present in another tenant's golden set?" You cannot answer these questions from aggregate logs. You need per-run, per-tenant, per-component provenance.

The minimum fields for a multi-tenant eval audit record are extensive but non-negotiable. Each eval run must record the **tenant identifier** — which customer this evaluation belongs to. The **eval configuration version** — the exact version of the tenant's quality contract, including dimension definitions, weights, and thresholds, that was active when the run executed. The **model version** — the specific model checkpoint, adapter version, or API model identifier that produced the outputs being evaluated. The **judge configuration** — which LLM judge was used, which judge prompt version, which scoring rubric version, and what temperature and sampling parameters the judge ran with. The **golden set version** — the exact snapshot of the tenant's evaluation dataset, including a hash of the dataset contents, so you can prove which examples were included and which were not. The **input-output pairs** — the specific inputs that were evaluated and the specific model outputs that were scored, not just the scores themselves. The **judge outputs** — the raw judge responses before aggregation, including any reasoning or chain-of-thought the judge produced. The **aggregate results** — the final scores, pass-fail determinations, and any derived metrics. The **timestamp and execution metadata** — when the run started, when it completed, which compute node processed it, which API endpoints were called, and the geographic region where execution occurred. And the **operator identity** — whether the run was triggered by an automated schedule, a manual request, or a system event, and which human or service account initiated it.

That is a long list. Every item on it has been the subject of an actual audit question at an actual multi-tenant AI platform. Teams that omit any of these fields discover the gap when an auditor asks for something they cannot produce.

## Immutability Is Not Optional

An audit trail that can be modified after the fact is not an audit trail. It is a log. The difference is legally significant. When a financial services customer's compliance officer reviews your evaluation records, the first question is not "what do the records say?" The first question is "how do I know these records have not been altered?"

**Immutable audit logs** solve this through append-only storage with cryptographic integrity verification. Each audit record is written once, given a cryptographic hash, and that hash is chained to the previous record's hash — the same principle that underpins blockchain, applied to your eval logs without the overhead of a distributed consensus protocol. When an auditor asks for Tenant 88's evaluation records from Q1 2026, you can produce the records and prove mathematically that no record has been inserted, deleted, or modified since the run completed. The hash chain provides tamper evidence. If any record was altered, the chain breaks at the point of alteration.

In practice, you do not need to build this from scratch. Cloud providers offer append-only storage services — AWS offers WORM-compliant S3 buckets through Object Lock, Azure provides immutable blob storage, and Google Cloud provides bucket lock policies. For teams needing stronger guarantees, purpose-built audit log services like Amazon QLDB or its successors provide cryptographically verifiable ledger storage. The choice depends on your regulatory requirements and your customers' audit expectations. A healthcare customer under HIPAA may accept immutable cloud storage with access controls. A financial services customer under SEC Rule 17a-4 may require WORM-compliant storage with retention enforcement. A government customer may require on-premises storage with air-gapped backup.

The operational cost of immutable audit logs is real but manageable. Each eval run for a tenant generates between 5 KB and 50 KB of audit data, depending on how many input-output pairs are included. A platform running 10,000 eval runs per day across 500 tenants generates roughly 100 MB to 500 MB of audit data daily. At cloud storage prices in 2026, that is a negligible cost. The engineering cost is in the pipeline changes — every component that touches evaluation must write to the audit log, and no component can skip the write without breaking the chain. This means audit logging is not a feature you add to a finished eval pipeline. It is a constraint you build the pipeline around.

## Retention Policies Per Regulatory Regime

Different tenants operate under different regulations, and different regulations mandate different retention periods. This creates a multi-tenant-specific problem: your audit log storage must enforce per-tenant retention policies, not a single platform-wide policy.

A healthcare customer under HIPAA requires audit records to be retained for a minimum of six years. A financial services customer under SEC regulations may require seven years. A customer under the EU AI Act operating a high-risk system must retain technical documentation and logs for a period proportional to the system's expected lifetime, with the GPAI Code of Practice recommending at least five years from the system's last deployment. A customer in a less regulated industry might only need 18 months of retention for their own internal governance.

If you apply a single retention policy across all tenants, you face a forced choice. Set retention at the highest required period — seven years — and you store seven years of audit data for tenants who only need 18 months, which increases storage costs and data management complexity. Set retention at a lower period and you risk deleting audit records that a regulated tenant needs for compliance. The correct approach is per-tenant retention policies enforced at the storage layer. Each tenant's audit records carry a retention tag derived from their compliance tier, and the storage system enforces retention based on that tag.

Retention is not just about how long you keep data. It is also about how you delete it when the retention period expires. Under GDPR, a European customer may request deletion of their evaluation data — including audit logs — when they leave the platform. Under HIPAA, deletion must be verifiable and documented. Your audit system must support tenant-scoped deletion that removes all of a specific tenant's records without affecting any other tenant's data, and the deletion itself must be audited. You need a log that proves you deleted what you were supposed to delete, when you were supposed to delete it, and that nothing else was affected.

## The Evidence Package

Raw audit logs are useful for engineers investigating incidents. They are useless for the enterprise procurement team evaluating whether to renew a $2 million contract. Regulated customers need **evidence packages** — structured, human-readable summaries of evaluation activity that demonstrate compliance, quality performance, and operational integrity over a defined period.

An evidence package for a quarterly business review typically includes five components. The first is an evaluation summary — total eval runs conducted, total examples evaluated, pass rates per quality dimension, trend lines over the quarter. The second is a configuration changelog — every change to the tenant's eval configuration during the period, including rubric updates, threshold changes, model version updates, and who approved each change. The third is an incident log — any evaluation failures, anomalies, or quality regressions detected during the period, along with root cause analysis and resolution status. The fourth is a compliance attestation — a statement, backed by the immutable audit trail, that all evaluation runs for the tenant executed within their specified compliance boundaries — correct geographic region, correct data isolation, correct retention policies. The fifth is a data integrity verification — cryptographic proof, derived from the hash chain, that the audit records supporting the summary have not been tampered with.

Building evidence packages manually is unsustainable at scale. A platform serving 200 regulated customers that each require quarterly evidence packages needs to produce 800 packages per year. Each package requires pulling data from the audit log, computing summaries, generating visualizations, and formatting the result in a way that a non-technical compliance officer can review. Automate this. Build a pipeline that generates evidence packages on a schedule or on demand, pulling from the immutable audit trail, and delivering the package in a format your customers' compliance teams can consume — typically PDF with embedded data integrity proofs.

The effort to build automated evidence packages pays for itself quickly. A platform architect who previously spent two days per quarter manually assembling evidence for their top 20 customers now clicks a button. More importantly, the automated pipeline produces consistent, verifiable packages that do not depend on any individual engineer's memory of what happened last quarter.

## Cross-Tenant Audit Isolation

A multi-tenant audit trail must be queryable per tenant without exposing any other tenant's data. This requirement sounds obvious but creates specific architectural constraints that teams often discover too late.

The most common failure is an audit query interface that requires a tenant ID filter but does not enforce it at the data layer. An admin running a debugging query forgets the tenant filter and pulls audit records for all tenants. The results scroll across their screen — including Tenant 88's evaluation inputs, which contain patient records subject to HIPAA, and Tenant 203's evaluation outputs, which contain financial data subject to SEC oversight. No data left the platform. No external breach occurred. But an unauthorized internal access to protected data just happened, and under both HIPAA and the EU AI Act, it must be reported.

The defense is tenant-scoped access controls enforced at the storage layer, not the query layer. Each audit record is tagged with a tenant ID, and the storage system enforces that queries can only return records matching the requesting party's authorized tenant scope. Platform administrators who need cross-tenant access for debugging must use a separate, heavily audited administrative interface with a different access control policy and a different audit trail. The principle is the same one you apply to production data — never trust the application layer to enforce access controls when the storage layer can enforce them instead.

This isolation extends to evidence packages. When generating an evidence package for Tenant 88, the pipeline must query only Tenant 88's audit records. If a bug in the evidence generation pipeline accidentally includes a record from Tenant 89, the resulting package is a compliance violation — you have exposed Tenant 89's evaluation data to Tenant 88. The fix is the same as the query isolation fix: tenant-scoped data access enforced at the storage layer, verified through automated tests that check for cross-tenant leakage in generated packages.

## Audit Trails as Competitive Advantage

Most teams treat audit trails as a compliance cost — something they build because regulators require it. This framing misses the commercial reality. In enterprise B2B sales, the ability to produce comprehensive, tamper-evident evaluation audit trails is a differentiator that closes deals.

When your sales team walks into an enterprise procurement meeting and demonstrates that every evaluation run for every customer is logged with full provenance, stored immutably, retained per the customer's regulatory requirements, and available as an automated evidence package, the procurement team's security and compliance reviewers have fewer objections. Their review cycle shortens. The competitors who cannot demonstrate the same capabilities face weeks or months of additional security review, custom audit requirements, and conditional approvals. In regulated industries — healthcare, financial services, government, and increasingly any company subject to the EU AI Act's August 2026 compliance deadline for high-risk systems — audit capability is not a nice-to-have. It is a gate to the sale.

A platform that invested six engineering months in building automated audit trails and evidence packages reported that their average enterprise sales cycle shortened by three weeks because the compliance review phase — previously the longest phase — collapsed from eight weeks to two. The security questionnaire responses that used to require custom answers for each prospect now pointed to the same audit infrastructure, the same immutable log architecture, and the same evidence generation pipeline. The investment paid for itself within two quarters through accelerated revenue.

The audit trail is not just proof of what happened. It is proof that your platform takes evaluation seriously enough to make every run accountable, every score traceable, and every decision reconstructable. That proof is what regulated enterprises are buying. The next subchapter examines a subtler form of accountability failure — cross-tenant contamination in the LLM judge systems that produce the scores your audit trail records.
