# 28.4 — Calibrating Rubrics Against Customer Expectations: The Alignment Workshop

Your rubric dimensions are not your customer's quality definition. They are your platform's approximation of it — assembled from discovery interviews, industry templates, and educated guesses. Until those dimensions are tested against the customer's actual expert judgment, you have a hypothesis about what quality means to them, not a measurement system. The **Alignment Workshop** is where that hypothesis meets reality, and it almost always loses.

Calibration is the iterative process of adjusting rubric definitions, scoring thresholds, and dimension weights until the eval system produces scores that match what the customer's domain experts would assign if they reviewed every output by hand. Without calibration, you produce numbers. With calibration, you produce numbers the customer trusts. The difference determines whether your eval reports drive decisions or get ignored.

## Why Rubric Scores Disagree With Customer Judgment

A rubric built during discovery captures the customer's stated quality requirements. But stated requirements and actual judgment diverge for reasons that have nothing to do with dishonesty or imprecision. The customer's domain experts carry years of implicit knowledge about what a "good" output looks like in their context — knowledge they cannot fully articulate in a discovery interview because they have never needed to formalize it before.

Consider a financial advisory firm whose discovery session produced a rubric with five dimensions: factual accuracy, regulatory compliance, tone, completeness, and citation quality. The platform team built the rubric, ran it against thirty sample outputs, and got scores averaging 0.88 across dimensions. The team sent the scored samples to the customer's compliance director for review. The compliance director rated the same outputs an average of 0.62 — a gap of twenty-six points. The platform said "good." The customer said "not even close."

The root cause was not that the rubric was wrong. It was that the rubric was underspecified at the scoring boundaries. The rubric defined "regulatory compliance" as "the output references applicable regulations accurately." The compliance director interpreted compliance as "the output references applicable regulations, does not reference superseded regulations, does not imply that general guidance applies to a specific client's situation, and uses disclaimer language approved by our legal team." Four sub-requirements that the rubric's one-sentence definition did not capture. The platform's judge scored an output that mentioned the correct regulation without a disclaimer as compliant. The compliance director scored it as a serious deficiency.

This gap between platform rubric interpretation and customer expert interpretation is the **calibration gap**. It exists for every new customer. The only question is whether you discover it before or after the customer loses confidence in your quality reports. The alignment workshop is the mechanism for discovering it before.

## The Workshop Format

The alignment workshop is a structured session — typically two to four hours, conducted over video call or in person — where your eval team and the customer's domain experts jointly score a common set of examples and compare results. The format matters because calibration is not something you can accomplish through email exchanges or asynchronous reviews. The learning happens in the disagreements, and disagreements need real-time discussion to resolve.

**Participants.** From your side: the eval engineer who built the customer's rubric, the solutions architect who ran discovery, and optionally a calibration specialist if your team has one. From the customer's side: two to four domain experts who will ultimately be the standard against which your eval system is measured. These are not executives. They are the people who do the work — the compliance analyst, the clinical reviewer, the senior underwriter. You need people who can look at an output and say "this is wrong because of a specific reason," not people who can say "this feels off."

**Materials.** A set of twenty to thirty outputs generated by the customer's actual production configuration, selected to span the quality range. You want five or six outputs that are clearly good, five or six that are clearly bad, and ten to fifteen that fall in the ambiguous middle where scoring disagreements will be most instructive. If the customer is still in onboarding and does not have production traffic, use outputs generated from their configured prompts against representative test inputs. The ambiguous middle is where calibration lives — the extremes are easy.

**Process.** Each participant independently scores every output against every rubric dimension, using the scoring scale your platform uses — typically a one-to-five ordinal scale or a zero-to-one continuous scale. Nobody sees anyone else's scores until everyone has finished. Then you reveal the scores side by side: your judge's automated scores, your eval team's manual scores, and the customer's expert scores. You discuss every case where the scores diverge by more than one point on a five-point scale or more than 0.15 on a continuous scale.

The discussions are the entire point. When the customer's regulatory expert scores an output as a 2 and your judge scores it as a 4, the conversation that follows reveals what your rubric missed. "You scored this as compliant because it references the right regulation. But it references the 2024 version, which was superseded in January 2026. In our practice, citing a superseded regulation is worse than citing no regulation at all because it creates a false sense of compliance." That single comment teaches you more about what "regulatory compliance" means to this customer than the entire discovery session did.

## Anatomy of the Calibration Gap

The calibration gap is not random noise. It follows predictable patterns that reveal specific rubric deficiencies.

**Dimension underspecification** is the most common pattern. Your rubric defines a dimension in one or two sentences. The customer's experts interpret that dimension through a lens of domain-specific sub-requirements that the rubric does not mention. The financial advisory example above is a case of dimension underspecification. The fix is to expand the rubric definition with the sub-requirements the workshop reveals, turning a one-sentence definition into a three-to-five-sentence definition that captures the customer's actual scoring criteria.

**Threshold mismatch** is the second most common pattern. Your platform's default scoring threshold assumes that a 4 out of 5 means "acceptable with minor issues." The customer's experts consider a 4 to mean "good but not ready for client delivery without edits." Both interpretations are reasonable. But if your eval reports flag outputs scoring 4 or above as "passing" and the customer considers anything below a 5 as "needs review," every report you send will overstate the customer's satisfaction with their output quality. The fix is to set per-customer pass/fail thresholds that match the customer's quality bar, not your platform default.

**Missing dimensions** appear when the workshop reveals a quality aspect the customer cares about deeply that does not appear in your rubric at all. A legal services customer might reveal during calibration that they care intensely about "hedging precision" — whether the output uses exactly the right level of conditional language for each claim, neither too certain nor too tentative. If your rubric does not have a hedging dimension, the customer's scores will diverge from yours on every output where hedging is relevant, and your per-dimension breakdown will not show why.

**Weight mismatch** occurs when all the right dimensions are present but the customer weights them differently than your defaults. A brand-conscious consumer company might weight tone at 40 percent of their overall quality score. Your platform default might weight tone at 15 percent. The per-dimension scores could agree perfectly, but the composite score — the one that appears at the top of the quality report — will disagree because the aggregation formula does not match the customer's priorities.

## Adjusting Rubric Descriptions for Per-Customer Interpretation

After the workshop reveals calibration gaps, the next step is revising the rubric definitions to close them. This does not mean rewriting the rubric from scratch. It means adding per-customer qualifications to each dimension that your judge prompt will use when scoring that customer's outputs.

Your base rubric might define "accuracy" as "all factual claims in the output are correct and verifiable." For a healthcare customer, the per-customer qualification might add: "accuracy includes correct dosage ranges, correct drug interaction warnings, and correct contraindication lists. An output that correctly describes a condition but provides an outdated dosage range is scored as inaccurate." For an e-commerce customer, the qualification might add: "accuracy includes correct product specifications, correct pricing statements, and correct availability status. An output that describes a product correctly but misstates its price is scored as inaccurate."

These per-customer qualifications are stored in the customer's evaluation configuration and injected into the judge prompt at scoring time. They do not change the base rubric — every customer still uses the same dimensions with the same names. They change the per-customer interpretation of what each dimension means at the scoring boundary. This is the same architecture described in the Tenant-Scoped Judge Calibration system covered in Chapter 2, applied specifically to the onboarding context where the calibration is being established for the first time.

The qualification language should be written in the customer's own terminology, drawn directly from the workshop discussions. When the compliance director says "citing a superseded regulation is worse than citing no regulation," that phrasing — not a platform-sanitized version of it — should appear in the qualification. The customer's language carries the nuance that platform-standard language loses.

## Setting Per-Customer Thresholds

What score means "acceptable" is not a platform-level decision. It is a customer-level decision that the platform must honor.

During the workshop, you establish the customer's quality bar by asking a specific question for each dimension: "At what score does an output become unacceptable for your use case?" The answers vary dramatically across customers. A legal compliance team might set their accuracy threshold at 0.95 — anything below that represents unacceptable risk. An internal knowledge base team at a technology company might set their accuracy threshold at 0.80 — below that the content is misleading, but anything above is useful. Both thresholds are correct for their context. Your eval system needs to track both and report against the right one for each customer.

Per-customer thresholds determine three things. First, what appears in the customer's quality report as a pass or fail. An output scoring 0.82 on accuracy is a pass for the technology company and a fail for the legal team. If your report uses a single platform threshold, one of these customers sees a report that does not match their experience. Second, what triggers alerts. If a customer's daily average accuracy drops from 0.91 to 0.87, that is an alert condition for the legal team (below their 0.95 threshold) but not for the technology company (still above their 0.80 threshold). Third, what counts as a regression. A two-point drop in tone quality might be a significant regression for a brand-conscious consumer company and noise for a back-office automation customer. Per-customer thresholds make your regression detection sensitive to what each customer actually cares about rather than applying a one-size-fits-none platform standard.

## Weighting Dimensions Per Customer

Composite quality scores — the single number that summarizes overall quality in a customer's report — are weighted averages of per-dimension scores. The weights determine which dimensions dominate the composite, and those weights must reflect the customer's priorities, not your platform's defaults.

During the workshop, you establish dimension weights through a forced-ranking exercise. Give the customer's domain experts a hundred points and ask them to distribute those points across rubric dimensions according to importance. A healthcare customer might allocate 40 points to accuracy, 25 to completeness, 20 to compliance, 10 to tone, and 5 to formatting. A luxury brand customer might allocate 35 points to tone, 25 to accuracy, 20 to brand alignment, 15 to completeness, and 5 to formatting. These weight distributions become the customer's weighting profile, stored in their evaluation configuration and applied every time a composite score is computed.

The forced-ranking exercise works better than simply asking "how important is each dimension on a scale of one to five" because it forces trade-offs. When every dimension is rated 5 out of 5 importance, you learn nothing about relative priority. When the customer has a hundred points and must choose between giving accuracy 40 or tone 40, the trade-off reveals their true priorities. Some customers will struggle with this exercise — "everything is important" — and the facilitation skill is helping them find the edge case that breaks the tie. "If you had to choose between an output that is perfectly accurate but uses the wrong tone, and an output that uses perfect tone but has a minor factual error, which would you deploy?" The answer reveals the weight hierarchy.

## Convergence Criteria: When Is Calibration Done

Calibration is iterative. You run the workshop, discover gaps, adjust rubric definitions and thresholds, re-score the same set of examples, and check whether the adjusted rubric produces scores that align with the customer's judgment. The question is when to stop iterating.

The convergence criterion is **inter-rater agreement** between your calibrated judge and the customer's domain experts, measured by Cohen's kappa or percentage agreement within one scoring point. The target is a kappa of 0.80 or above, which indicates substantial agreement. For continuous scoring scales, the equivalent target is percentage agreement within 0.10 on a zero-to-one scale at or above 85 percent.

Most calibration workshops reach convergence in two to three rounds. The first round reveals the major gaps — missing sub-requirements, threshold mismatches, missing dimensions. After adjustments, the second round typically closes the gap to kappa 0.70 to 0.75. A third round, focused on the remaining disagreements, usually reaches 0.80 or above. If you have not reached convergence after three rounds, one of two things is happening: either the customer's quality definition is internally inconsistent (their own experts disagree with each other, which means there is no stable target for your judge to match), or your rubric structure cannot capture a quality dimension the customer cares about, and you need to add a new dimension.

When the customer's own experts disagree — their inter-rater agreement is below 0.75 — you cannot calibrate your judge to a standard that does not exist. In this situation, the honest conversation is: "Your experts disagree on what a 3 versus a 4 looks like for this dimension. Before we can calibrate our system, we need to help your team align." This is a service you can provide — running a calibration session among the customer's own experts — and it often strengthens the relationship because you are helping them formalize a quality standard they have never articulated.

Document the convergence score and the date it was achieved. This becomes the baseline for re-calibration checks. If a future verification run shows the judge-customer agreement dropping below 0.75, you know calibration has drifted and it is time for another workshop.

## The Re-Calibration Trigger

Calibration is not permanent. It degrades over time, and the triggers for re-calibration are both time-based and event-based.

**Time-based re-calibration** runs on a six-month cadence for most customers. Customer expectations shift as their business evolves, as their internal quality standards mature, and as they compare your outputs to competitors. The customer who set their accuracy threshold at 0.85 during onboarding in January may have raised it to 0.90 by July because their end users have become more demanding. A six-month re-calibration catches this drift before it creates a persistent gap between your scores and the customer's satisfaction.

**Event-based re-calibration** triggers immediately when specific conditions occur. A major product change on the customer's side — launching a new use case, entering a new market, facing new regulatory requirements — changes what quality means in their context. A judge model upgrade on your side — swapping from Claude Opus 4.5 to Claude Opus 4.6 as your primary judge — changes how the judge interprets rubric definitions, as covered in the calibration verification system in Chapter 2. A pattern of customer complaints about score accuracy — three or more instances in a quarter where the customer disputes a score and the dispute is substantiated — signals that calibration has drifted enough to erode trust.

The re-calibration workshop is shorter than the initial workshop because you are not building from scratch. You are verifying and adjusting an existing calibration. Typically ninety minutes suffices: re-score fifteen of the original calibration examples with the current judge, compare to the customer's current expert scores, identify and fix the specific drift points, and verify convergence. The investment is modest — a few hours of team time every six months — and the cost of skipping it is a customer whose trust in your eval system slowly erodes until they stop reading your quality reports entirely.

## Calibration as a Relationship Signal

The alignment workshop is not just a technical calibration exercise. It is the first moment in the customer relationship where both sides discover how well they understand each other. The platform team that enters the workshop overconfident — "we nailed the rubric in discovery, calibration is just a formality" — learns humility fast. The customer who enters the workshop skeptical — "I don't think an automated system can evaluate our content" — often leaves impressed, not because the system scores perfectly on the first try, but because the team demonstrated the ability to listen, adjust, and improve.

How you handle the calibration gap matters more than the gap itself. If you present the initial scores, see the twenty-six-point divergence, and immediately say "let us fix this — tell us exactly what we missed," the customer learns that your platform is adaptive and that their feedback drives real changes. If you present the scores and defend them — "well, our rubric says this is compliant" — the customer learns that your platform's definition of quality does not bend to theirs, and they start planning their exit.

The workshop also establishes the vocabulary the customer relationship will use going forward. When you and the customer agree on what a "3" means for accuracy in their domain, that shared understanding becomes the foundation for every quality conversation, every SLA review, every escalation. Without that shared vocabulary, quality discussions devolve into the customer saying "this is bad" and your team saying "our scores say it is fine" — a conversation with no resolution.

The calibrated rubric and the agreed-upon thresholds from the alignment workshop feed directly into the next phase of onboarding: the baseline sprint, where you run the full eval suite against the customer's actual production configuration and establish the quality scores that every future improvement will be measured against.
