# 28.7 — Customer-Specific Safety and Compliance Evaluation

A content moderation platform lost its second-largest customer in November 2025 — a children's educational publisher worth $2.1 million in annual recurring revenue — because of a single unfiltered output. The platform served 230 enterprise customers from the same infrastructure, and its safety filters were calibrated to the median use case: corporate knowledge bases, internal documentation assistants, and customer support bots. Those filters blocked explicit sexual content, graphic violence, and hate speech. They did not block mildly suggestive language, dark humor, or references to alcohol and drug use. For 220 of the platform's 230 customers, that calibration was appropriate. The corporate knowledge base customer does not need profanity filters tuned to protect eight-year-olds. But the children's educational publisher did. When a student asked the AI assistant about historical figures and received a response that included a casually phrased reference to a historical figure's drinking habits — accurate, not flagged by the standard safety filter, completely inappropriate for a product used by elementary school students — the publisher's trust and safety team escalated within hours. The platform's response was "the output was technically within our safety guidelines." The publisher's response was a 90-day termination notice.

The platform had conflated platform safety with customer safety. Platform safety defines the baseline: the minimum set of safety rules that every output on the platform must satisfy. Customer safety defines the specific requirements that each tenant's outputs must meet based on their audience, their industry, their regulatory environment, and their risk tolerance. These are not the same thing. A platform that applies identical safety filters to a children's education company and an adult entertainment company is either too restrictive for one or too permissive for the other. Usually both.

## The Safety Configuration Matrix

Customer-specific safety evaluation requires a **safety configuration matrix** — a per-tenant specification of content policies, output constraints, and risk thresholds that the evaluation system enforces independently for each customer. The matrix is not a single dial turned from "strict" to "lenient." It is a multi-dimensional configuration that addresses different categories of safety risk at different sensitivities for each tenant.

The primary dimensions of the safety matrix are content policy, PII handling, toxicity thresholds, hallucination tolerance, and domain-specific constraints. Each dimension operates independently because the same tenant may require extreme strictness on one dimension and standard treatment on another.

**Content policy** defines what topics, language patterns, and output types are permitted for a tenant. A children's education customer prohibits any reference to violence, substance use, sexual content, profanity, and potentially frightening scenarios — a policy far stricter than the platform default. A B2B data analytics customer permits all of those topics in outputs because their users are adults processing business data and the AI's responses about data trends may legitimately reference market crashes, drug trials, or workplace violence statistics. A creative writing platform actively requires the AI to produce content involving conflict, tension, and mature themes — their safety policy is permissive by design, with restrictions only on content that could facilitate real-world harm.

**PII handling rules** define how the tenant's evaluation system treats personally identifiable information in model outputs. A healthcare customer requires that any PII in outputs be flagged and blocked before delivery to the end user, with the evaluation system scoring PII leakage as a critical failure. A customer support platform may permit limited PII in outputs — the agent responding to a customer may need to reference their name and account number — but requires that PII never appears in evaluation logs or scoring samples. A government customer may require that PII be redacted from all stored data, including evaluation data, within 24 hours of generation. Each of these policies requires different evaluation logic, different data handling in the eval pipeline, and different retention rules for evaluation artifacts.

**Toxicity thresholds** define the sensitivity of the tenant's toxicity detection. A platform-wide toxicity threshold might flag outputs that score above 0.7 on a toxicity classifier. A children's education customer might require a threshold of 0.15 — flagging anything even mildly negative or confrontational. An adult content platform might use a threshold of 0.85, filtering only extreme toxicity. These thresholds are not arbitrary preferences. They reflect the tenant's audience, their legal exposure, and their brand risk tolerance. An output that is perfectly safe for one tenant is a critical safety failure for another, and the evaluation system must score them accordingly.

**Hallucination tolerance** varies by the consequences of incorrect information. A medical information platform tolerates near-zero hallucination because incorrect medical information can harm patients. Their evaluation scores hallucination as a blocking failure — any output flagged as hallucinated is scored zero regardless of other quality dimensions. A creative writing platform tolerates a high degree of fabrication because inventing plausible-sounding details is a feature, not a bug. A B2B analytics customer tolerates no hallucination on numerical data but accepts paraphrased explanations that may not be literally precise. The evaluation system must apply the correct hallucination tolerance for each tenant, which means different scoring weights, different failure modes, and different escalation paths.

## Industry-Specific Compliance Requirements

Beyond content safety, tenants in regulated industries carry compliance obligations that the platform's evaluation system must verify. These obligations are not optional add-ons. They are legal requirements that, if violated, expose both the tenant and the platform to regulatory action.

**Healthcare tenants** operate under HIPAA in the United States, which requires that protected health information — PHI — be safeguarded in all processing, storage, and transmission. For the evaluation system, this means PHI that appears in model outputs must be detected and handled according to the tenant's data processing agreement. Evaluation samples containing PHI must be stored in HIPAA-compliant infrastructure with access controls, encryption, and audit logging. The evaluation pipeline itself becomes a business associate under HIPAA, which means the platform must sign a Business Associate Agreement for every healthcare tenant and demonstrate that its evaluation infrastructure meets the BAA requirements. Evaluation-specific compliance checks include verifying that PHI is not persisted in evaluation caches, that evaluation results do not contain patient-identifiable information, and that evaluation data is retained only for the period specified in the BAA.

**Financial services tenants** operate under a cluster of regulations depending on their jurisdiction and activities. SOX compliance requires that financial reporting processes — including AI systems that generate or process financial data — maintain documented audit trails and internal controls. PCI DSS applies when the tenant processes payment card data, requiring that card numbers never appear in evaluation samples or scoring logs. SEC and FINRA regulations in the United States impose record-keeping requirements on communications generated by AI systems in broker-dealer contexts — the evaluation system must preserve scored samples as part of the communication record. For European financial customers, MiFID II and DORA add requirements for operational resilience and technology risk management that encompass evaluation infrastructure.

**European tenants** face the EU AI Act, which entered its high-risk system enforcement phase on August 2, 2026. For tenants deploying high-risk AI systems — which includes AI used in employment, credit scoring, education, law enforcement, and critical infrastructure — the Act requires comprehensive technical documentation, conformity assessment, and post-market monitoring. The evaluation system is central to demonstrating compliance because it provides the quality metrics, the accuracy measurements, and the bias assessments that the technical documentation demands. Each European tenant's evaluation configuration must produce the specific evidence artifacts that the EU AI Act requires: performance metrics broken down by relevant demographic groups, accuracy measurements against defined benchmarks, and documentation of the evaluation methodology itself. The platform cannot produce a single set of EU AI Act documentation for the platform as a whole. Each tenant's deployment constitutes a separate AI system under the Act and requires its own technical documentation.

**Government tenants** in the United States require FedRAMP authorization for cloud services that process federal data. FedRAMP 20x, the streamlined authorization framework rolling out through 2026, emphasizes continuous monitoring and automated security validation. For the evaluation system, this means evaluation infrastructure serving government tenants must reside in FedRAMP-authorized environments, evaluation data must be processed within authorized boundary, and continuous monitoring data from the evaluation system feeds into the government customer's security posture reporting.

## Evaluating Compliance Per Tenant

Platform-wide compliance checks — running the same compliance evaluation against every tenant — produce meaningless results because not every compliance requirement applies to every tenant. A HIPAA check against a marketing analytics customer generates false positives on terminology that resembles PHI but is actually marketing data. A PCI DSS check against a healthcare customer wastes compute on a regulation that does not apply to them. The evaluation system must run **compliance eval suites** that are specific to each tenant's regulatory requirements.

The implementation pattern is a compliance profile attached to each tenant's evaluation fingerprint. The profile specifies which regulations apply to the tenant, which specific requirements within each regulation are relevant, and what evidence artifacts the compliance evaluation must produce. When the evaluation pipeline processes a tenant's outputs, it loads the tenant's compliance profile and executes only the compliance checks specified in that profile. A healthcare tenant gets HIPAA checks. A financial tenant gets SOX and PCI checks. A European education tenant gets HIPAA-equivalent data protection checks plus EU AI Act conformity checks. A marketing analytics tenant gets none of these — their compliance profile contains only the platform baseline safety checks.

Each compliance check produces a structured result: pass, fail, or flagged-for-review. A pass means the output satisfies the compliance requirement. A fail means the output violates the requirement and must be blocked, remediated, or escalated. Flagged-for-review means the automated check detected a potential violation that requires human judgment — a pattern that might be PHI but could also be a coincidental match, or a financial figure that might be a real account number or might be a sample dataset. The flagged-for-review category is essential because compliance violations carry such severe consequences that false negatives are unacceptable, but false positives at high volume create review backlogs that slow the entire system.

## The False Negative Cost Difference

Here is the reframe that most multi-tenant platforms miss: a safety or compliance failure is not a fixed-severity event. Its severity depends entirely on the tenant.

A toxic output delivered to a user of a B2B data analytics platform generates a support ticket, an apology, and possibly a blog post about the incident. The financial cost is minimal. The reputational cost is limited to the affected user and perhaps their immediate team.

The same toxic output — identical content, identical technical failure — delivered to an eight-year-old using a children's education platform generates a parental complaint, a media inquiry, a potential regulatory investigation under COPPA (the Children's Online Privacy Protection Act), and a brand crisis for the educational publisher that licensed the AI. The financial cost can reach seven figures when legal defense, regulatory fines, and lost contracts are totaled. The reputational cost is existential for both the publisher and the platform.

This asymmetry means that the platform's safety evaluation must weight false negatives differently per tenant. A false negative rate of 0.5 percent on toxicity detection might be acceptable for the B2B analytics customer — one toxic output in 200 is bad but not catastrophic. The same 0.5 percent false negative rate for the children's education customer is unacceptable because the consequences of a single miss are orders of magnitude worse. The children's education tenant needs a false negative rate closer to 0.01 percent, which requires stricter filters, more conservative thresholds, and potentially a secondary safety evaluation layer that re-checks outputs the first layer passed.

The evaluation system must reflect this asymmetry in how it scores safety failures per tenant. The scoring configuration for the children's education tenant should weight safety dimensions at 40 to 50 percent of the total quality score, with safety failure acting as a blocking condition that zeroes the composite score regardless of other dimensions. The scoring configuration for the B2B analytics tenant might weight safety at 10 percent, with safety failure flagged as a quality issue but not a blocking condition. Same evaluation pipeline, same scoring engine, same safety detectors — but radically different consequences based on the tenant's safety profile.

## Maintaining Compliance Evidence Trails Per Tenant

Regulatory compliance is not satisfied by passing checks. It is satisfied by proving you passed checks, when you passed them, using what methodology, over what time period, and with what results. The evidence trail is the proof, and for multi-tenant platforms, the evidence trail must be per-tenant.

A **per-tenant compliance evidence trail** records every compliance evaluation execution for a tenant: the date and time, the compliance checks that were run, the version of each check's evaluation logic, the inputs that were evaluated, the results produced, and any human review decisions made on flagged items. This trail is not an aggregated summary. It is a per-execution, per-output record that an auditor can trace from a specific output through the compliance evaluation to the final determination.

The evidence trail must be stored in a manner consistent with the tenant's regulatory requirements. HIPAA evidence trails must be in HIPAA-compliant storage with appropriate access controls and retention periods. EU AI Act evidence must be available for presentation to national supervisory authorities for the lifetime of the AI system plus a defined post-decommission period. Financial services evidence trails must comply with the record-keeping requirements of the applicable financial regulator, which in some jurisdictions require seven-year retention.

The platform's internal evidence storage is not sufficient. Many enterprise tenants require the ability to **export their compliance evidence** into their own compliance management systems. The export must include the full trace — from the original output through each compliance check to the final determination — in a format that the tenant's compliance tools can ingest. Common formats include structured JSON-equivalent data files with schema documentation, PDF reports with digital signatures for immutability verification, and API access for real-time evidence retrieval by the tenant's automated compliance monitoring.

Building and maintaining per-tenant evidence trails at scale is expensive. A platform serving 500 tenants, evaluating 1,000 outputs per tenant per day, running an average of 3 compliance checks per output, generates 1.5 million compliance evidence records per day. At 365 days per year with multi-year retention requirements, the evidence store grows by hundreds of millions of records annually. The storage and retrieval infrastructure must be designed for this scale from the beginning, not bolted on when the first auditor asks for evidence and the team realizes they do not have it.

## The EU AI Act August 2026 Deadline and Per-Tenant Technical Documentation

The EU AI Act's enforcement of high-risk AI system requirements on August 2, 2026 creates a specific, deadline-driven documentation obligation for multi-tenant platforms. If your platform serves European customers who deploy high-risk AI systems, each of those deployments requires its own technical documentation package that demonstrates compliance with the Act's requirements.

The technical documentation must include a description of the AI system and its intended purpose, the data used for training and evaluation, the evaluation methodology and results, performance metrics disaggregated by relevant groups to assess potential bias, the risk management measures implemented, and the post-market monitoring plan. For a multi-tenant platform, "the AI system" is not the platform — it is each tenant's specific deployment, with their specific configuration, their specific data, their specific evaluation results, and their specific risk profile.

This means the platform must be capable of generating per-tenant technical documentation packages that reflect each tenant's actual configuration, evaluation history, and compliance posture. A tenant running a customer support bot with standard prompts and a default model generates a different technical documentation package than a tenant running a credit scoring system with a fine-tuned adapter and custom evaluation rubrics, even though both run on the same platform infrastructure.

The documentation generation should be automated. Manual assembly of technical documentation for even 50 European tenants deploying high-risk systems is a full-time job for a compliance team. Automated generation pulls from the tenant's evaluation fingerprint (configuration), the evaluation results store (performance metrics), the compliance evidence trail (compliance check history), and the platform's shared documentation (infrastructure description, shared components). The system composes these sources into a per-tenant documentation package that can be presented to a national supervisory authority on request.

Platforms that wait until July 2026 to build this capability will not make the deadline. The infrastructure for per-tenant compliance evidence, per-tenant evaluation results storage, and per-tenant documentation generation takes three to six months to build and validate. The compliance team needs another two to three months to review the automated documentation against the Act's requirements and identify gaps. Starting this work in early 2026 — now — is the only path that provides margin for the inevitable complications.

## The Shared Infrastructure Paradox

The deepest tension in customer-specific safety evaluation is the shared infrastructure paradox. The entire value proposition of a multi-tenant platform is shared infrastructure — one platform, one engineering team, one deployment, many customers. Customer-specific safety evaluation pulls against that value proposition by requiring per-tenant logic, per-tenant configuration, per-tenant evidence, and per-tenant documentation. Taken to its extreme, per-tenant compliance turns a multi-tenant platform into a managed collection of single-tenant deployments, eliminating the efficiency gains that justified the architecture in the first place.

The resolution is layered safety architecture. The platform maintains a **shared safety layer** that handles the safety requirements common to all tenants — blocking universally harmful content, preventing prompt injection, detecting basic toxicity, enforcing output length limits. This shared layer is maintained by the platform's safety team and updated centrally. Above the shared layer, each tenant has a **tenant-specific safety layer** that applies the additional filters, thresholds, and compliance checks defined in their safety matrix. The tenant-specific layer consumes services from the shared layer — the same toxicity classifier, the same PII detector, the same hallucination checker — but applies tenant-specific thresholds and interpretation rules.

This layered approach preserves the efficiency of shared infrastructure while delivering the specificity that per-tenant compliance demands. The toxicity classifier is trained and maintained once. Its output is interpreted differently for each tenant based on their threshold configuration. The PII detector runs once per output. Its findings trigger different actions for different tenants — blocking for healthcare, logging for analytics, redacting for government. The compliance evidence trail captures both the shared evaluation results and the tenant-specific interpretation, providing the complete audit trail that regulators require.

The cost of the tenant-specific layer is marginal compared to the cost of the shared infrastructure. The toxicity classifier — the expensive component — runs once. The threshold comparison — the cheap component — runs per tenant. This economic structure means that adding a new tenant's safety configuration costs configuration effort and minimal compute, not a proportional increase in safety infrastructure. One hundred tenants with different safety configurations cost roughly the same in safety infrastructure as one tenant with the strictest configuration, plus the configuration management overhead.

## Cross-Tenant Safety Learning Without Cross-Tenant Data Sharing

When the children's education tenant's safety evaluation catches a novel attack pattern — a prompt injection technique that bypasses the standard toxicity filter by encoding harmful content in what appears to be a historical narrative — that detection has value for every other tenant on the platform. But sharing the specific examples, the specific outputs, or the specific tenant context across tenant boundaries violates the data isolation requirements discussed in Chapter 1 of this section.

The solution is **abstracted safety intelligence** — extracting the pattern from the tenant-specific instance and adding it to the shared safety layer without revealing the source tenant, the specific content, or the specific context. The pattern becomes a new rule or classifier input in the shared toxicity filter: "detect harmful content encoded as historical narrative." The rule is tested against a platform-maintained safety test set that contains no tenant-specific data. Once validated, the rule is deployed to the shared safety layer and benefits all tenants.

This abstraction is not automatic. It requires a safety engineer to review the tenant-specific detection, extract the generalizable pattern, construct a tenant-independent test case, and validate the updated rule. The process takes one to three days per pattern, which means it does not happen in real time. But over weeks and months, the shared safety layer accumulates intelligence from across the tenant base, becoming more robust than any single tenant's safety evaluation could be on its own. This is the true network effect of multi-tenant safety — every tenant's safety improves because of patterns detected across the population, without any tenant's data being exposed to another.

Detecting quality regressions across such heterogeneous safety and compliance configurations — where what counts as a regression differs for every customer — is the challenge addressed in the next subchapter on regression detection across heterogeneous tenant configurations.