# 28.2 — Discovery: Extracting Quality Requirements From Customers Who Think in Business Terms

The call starts the way they always do. The customer's VP of Product is talking about how their AI agents need to "sound like us." Their Head of Compliance wants outputs that "meet regulatory standards." Their Engineering Director asks for "high accuracy." Your evaluation engineer has a rubric template open on a second screen, waiting for something specific enough to encode. Nothing said so far is encodable. "Sound like us" is not a quality dimension. "Meet regulatory standards" is not a threshold. "High accuracy" is not a measurable claim until someone defines what accuracy means for their domain, which inputs, and against what reference standard. Thirty minutes into the call, your team has pages of notes and zero evaluatable requirements.

This is the discovery problem. Enterprise customers define quality in business terms because that is the language they operate in. They cannot tell you that they need a tone consistency score above 0.82 on a five-point Likert scale, because they have never thought about quality in those terms. They can tell you that when their customer service bot sounds corporate and cold, their customers complain, and when it sounds warm but imprecise, compliance flags it. Your job during discovery is not to teach them evaluation vocabulary. It is to extract, from their business language and their concrete reactions to real outputs, the quality dimensions, weights, and threshold ranges that will become their Tenant Eval Fingerprint.

## The Translation Problem

The gap between business language and evaluation language is not a vocabulary problem. It is a decomposition problem. When the VP says "sound like us," she is actually encoding three or four distinct quality dimensions into a single phrase. "Sound like us" might mean: use our specific terminology for our products, not generic industry terms. It might mean: maintain a conversational but professional register, never formal or academic. It might mean: mirror the sentence length and paragraph structure of our existing brand copy. It might mean: reference our company values when appropriate, but never sound like a press release. Each of those is a separate, measurable quality dimension — terminology accuracy, register consistency, structural alignment, and brand voice fidelity. But the customer experiences them as one undifferentiated feeling: "this sounds like us" or "this doesn't."

Your evaluation team's most valuable skill during discovery is performing this decomposition in real time. When the customer says "high accuracy," the eval engineer asks: accuracy of what? Factual claims? Named entities? Numerical figures? Citations? Temporal references? Each answer points to a different dimension. When the customer says "meet regulatory standards," the compliance specialist asks: which regulations? What specific language requirements do those regulations impose? Are there terms that must appear verbatim, or is semantic equivalence sufficient? Each answer narrows the requirement from an unscoreable abstraction to a scoreable criterion.

The translation fails when the evaluation team takes business language at face value and converts it directly into rubric dimensions without decomposition. A rubric dimension called "brand voice" that tries to capture everything the customer means by "sound like us" will produce scores that are too blunt to be actionable. The customer gets a brand voice score of 76 percent but cannot tell whether the problem is terminology, register, structure, or values alignment. The score is specific enough to be a number but too vague to drive improvement. Effective discovery decomposes broad business requirements into narrow evaluation dimensions that each measure one thing clearly enough that a low score immediately suggests a specific fix.

## The Discovery Workshop Format

Discovery is not a single meeting. It is a structured series of workshops, typically two to three sessions of 90 minutes each, conducted over one to two weeks. Each session has a specific purpose, specific attendees, and specific outputs. Running discovery as an unstructured conversation — "tell us about your quality needs" — produces the same vague requirements every time.

The first workshop is **stakeholder mapping and priority surfacing**. The attendees are the customer's decision makers — VP of Product, Head of Compliance, Engineering Director, and anyone else who has authority over what "good" means for their use case. The goal is not to define quality dimensions yet. The goal is to understand what the customer is trying to accomplish with the platform's outputs, what downstream business processes depend on those outputs, and what happens when outputs are wrong. You are mapping the blast radius of quality failures. A customer whose AI-generated contract summaries feed directly into an automated approval workflow has different quality requirements than a customer whose summaries are reviewed by a paralegal before any action is taken. The automated workflow demands near-perfect accuracy because errors propagate without human review. The paralegal-reviewed workflow can tolerate lower accuracy because the human catches mistakes. This context shapes every dimension and threshold in the fingerprint.

The second workshop is **quality dimension extraction and the "show me bad" exercise**. The attendees now include the customer's domain practitioners — the people who actually work with the platform's outputs daily. The format is hands-on. Your team presents 15 to 20 outputs of deliberately varying quality — some excellent, some mediocre, some clearly flawed — and asks the customer's practitioners to sort them into three categories: outputs they would accept without changes, outputs they would edit before using, and outputs they would reject entirely. The practitioners then explain their reasoning for each sorting decision. This exercise is where implicit quality dimensions surface. The customer may not have mentioned formatting in the first workshop, but when three practitioners independently reject outputs with inconsistent heading structures, formatting becomes a dimension.

The third workshop is **dimension prioritization and threshold negotiation**. Your team presents the proposed quality dimensions extracted from the first two workshops and asks the customer to rank them by importance. This ranking becomes the basis for dimension weights in the fingerprint. You also present calibration examples — outputs scored by your proposed rubric — and ask the customer whether the scores match their perception. If your rubric scores an output at 88 percent and the customer considers it mediocre, either the dimensions are wrong, the weights are wrong, or the thresholds are wrong. You iterate until the scores and the customer's judgment converge. Two to three calibration rounds within this workshop are typical. More than five rounds suggests the customer's internal stakeholders disagree about quality, which is a problem you need to surface explicitly rather than paper over.

## The "Show Me Bad" Technique

This technique deserves its own explanation because it is the single most effective tool for extracting quality requirements, and most teams do not use it.

Customers struggle to define what good looks like because "good" feels obvious and therefore difficult to articulate. But they can instantly and articulately describe what bad looks like. Show a healthcare customer an output that uses a deprecated drug name and they will explain, in precise detail, why that specific substitution is dangerous and which regulatory body's guidance it violates. Show a financial services customer an output that rounds a percentage to one decimal place instead of two and they will explain exactly which clients would flag that error and what downstream calculation it would break. Show a media customer an output that uses passive voice in a headline and they will explain their style guide's active-voice requirement with examples.

The technique works because criticism is more specific than praise. When you ask "what makes a good output?" the customer says "accuracy, clarity, and our brand voice." When you show them a bad output and ask "what is wrong with this?" they say "this drug name was deprecated in 2024, this dosage unit should be milligrams not grams, and the conclusion contradicts the data in paragraph three." The second response gives you three concrete quality dimensions — terminology currency, unit accuracy, and internal consistency — that the first response missed entirely.

To execute the technique, prepare a set of 15 to 20 outputs that span the quality spectrum for the customer's domain. Include outputs with different types of flaws: factual errors, tone mismatches, formatting inconsistencies, missing information, overly verbose responses, and domain terminology mistakes. Do not tell the customer which outputs are intentionally flawed. Let them discover the flaws themselves. The flaws they notice first reveal their highest-priority quality dimensions. The flaws they miss or dismiss reveal dimensions they care about less. This prioritization data is more reliable than asking customers to rank dimensions abstractly, because it is grounded in concrete reactions rather than theoretical preferences.

## Extracting Implicit Quality Dimensions

Some of the most important quality dimensions never get mentioned in workshops. They surface only through patterns in the customer's feedback.

Implicit dimensions are quality requirements the customer holds but cannot name. They are the criteria that live in the customer's gut rather than in their specifications. A legal customer might consistently reject outputs that use the word "must" where they prefer "shall" — but they never mentioned word choice as a requirement because in their mind, the difference between "must" and "shall" is not a preference but an obvious fact of legal drafting. A retail customer might flag outputs that list features before benefits — a structural preference so deeply embedded in their marketing philosophy that they do not think of it as a quality dimension, just as "the right way to write."

You detect implicit dimensions by analyzing patterns across the customer's feedback during the "show me bad" exercise and the calibration rounds. If the customer consistently rates outputs lower when they exceed a certain length, length sensitivity is an implicit dimension — even if the customer never mentioned length as a concern. If the customer consistently prefers outputs that cite specific numbers over outputs that use qualitative descriptors, specificity is an implicit dimension. These patterns often matter more than the explicit requirements, because they represent the quality standards the customer will enforce instinctively, report after report, even if they never consciously articulate them.

The discovery document should include both explicit and implicit dimensions, clearly labeled. Explicit dimensions are those the customer named. Implicit dimensions are those your team extracted from behavioral patterns. Presenting implicit dimensions back to the customer is a powerful trust-building moment: "We noticed that you consistently preferred outputs with concrete numbers over qualitative statements. Should we include 'quantitative specificity' as a dimension in your evaluation?" When the customer says yes — and they almost always do — they feel understood at a level they did not expect, which increases their confidence in the evaluation system before the first score is ever computed.

## The Discovery Document

The output of the discovery phase is not a set of meeting notes. It is a structured document that serves as the specification from which the Tenant Eval Fingerprint will be constructed. The Quality Contract described in Chapter 2 is the formal agreement. The discovery document is the evidence base from which that contract is derived.

A complete discovery document contains five sections. The first is **business context** — what the customer uses the platform for, what downstream processes depend on the outputs, and what the business impact of quality failures looks like in their specific context. This section ensures that the evaluation team understands the stakes, not just the criteria. Evaluating outputs for a customer whose AI summaries inform investment decisions is a different level of rigor than evaluating outputs for a customer whose summaries populate a knowledge base.

The second section is **quality dimensions with definitions**. Each dimension has a name, a plain-language description of what it measures, and one or more concrete examples showing what a high score and a low score look like on that dimension. "Terminology accuracy" is defined as "the degree to which the output uses the customer's specific domain terminology rather than generic synonyms," with examples showing correct terminology usage and common substitution errors.

The third section is **priority ranking and proposed weights**. Dimensions are ordered by importance, with proposed percentage weights based on the customer's ranking and the behavioral patterns observed during workshops. The weights are proposed, not final — they will be validated during rubric calibration. But starting with a well-informed proposal saves calibration rounds.

The fourth section is **threshold indicators**. These are not precise numbers yet — those come from calibration. They are directional ranges: "the customer considers outputs below approximately 70 percent on citation fidelity to be failures" or "the customer's tolerance for formatting errors appears to be higher, accepting outputs with minor structural inconsistencies." These indicators give the eval engineer a starting point for threshold setting during fingerprint construction.

The fifth section is **open questions and risks**. This is the honest section. It documents the things the discovery team could not resolve — dimensions where the customer's internal stakeholders disagreed, requirements that were mentioned but not explored in depth, areas where the customer's stated priorities seemed to conflict with their revealed preferences. This section prevents the false confidence that comes from a clean discovery document that hides unresolved ambiguity.

## Common Discovery Failure Modes

Discovery fails in predictable ways, and each failure mode has a specific downstream cost.

The first failure mode is **wrong attendees**. The customer sends their most junior person — an analyst who does not have authority to define quality standards and will not push back on your proposals. The discovery process runs smoothly because there is no friction, and that is the problem. The analyst agrees with everything, signs off on the discovery document, and three months later the VP of Product sees the first quality report and says, "This is not what we asked for." The VP was never in the room. The analyst did not represent the VP's priorities. The fix is to require senior stakeholder attendance at least for the first and third workshops. If the customer pushes back, explain that discovery decisions directly affect every quality report they will receive, and getting it right requires the people who define quality, not the people who consume it.

The second failure mode is **discovery skipped because we already know**. The sales team conducted deep pre-sales conversations. The customer provided a requirements document. The eval team feels like they understand the customer's needs without formal discovery. This fails because pre-sales conversations optimize for deal closure, not evaluation accuracy. The requirements document describes what the customer wants the platform to do, not how they define quality on the outputs. Discovery uncovers the gap between what the customer asked for and what they actually need measured. Skipping it means building a fingerprint from incomplete information and hoping the gaps are not in the dimensions the customer cares about most.

The third failure mode is **discovery done by the wrong team**. Sales runs discovery because they own the customer relationship, or customer success runs it because they will own the ongoing account. Neither team has the evaluation expertise to decompose business requirements into scoreable dimensions, detect implicit quality criteria, or assess whether proposed dimensions are actually measurable with the platform's current evaluation capabilities. Discovery must include an evaluation engineer or someone with equivalent evaluation design expertise. The relationship owner can facilitate. The eval expert must do the extraction.

The fourth failure mode is **discovery without the "show me bad" exercise**. The team conducts the workshops entirely through conversation — asking questions, taking notes, discussing priorities. This produces discovery documents that accurately reflect what the customer says but miss what the customer feels. Conversation captures explicit requirements. The "show me bad" exercise captures implicit ones. Discovery without both is discovery with half the signal.

## From Discovery to Fingerprint

The discovery document is not the end of the process. It is the input to the next step — translating discovered quality requirements into the machine-readable Tenant Eval Fingerprint described in Chapter 2. But the quality of that translation depends entirely on the quality of the discovery. A rich discovery document — with well-decomposed dimensions, behavioral evidence for weights, threshold indicators from calibration exercises, and honest documentation of open questions — gives the eval engineer a clear specification to work from. A thin discovery document — "customer wants accuracy, tone, and compliance" — forces the eval engineer to make assumptions, and every assumption is a place where the fingerprint might diverge from the customer's actual expectations.

The discovery phase also produces something less tangible but equally important: a relationship foundation. The customer's stakeholders have spent three sessions working directly with your evaluation team. They have seen how your team thinks about quality. They have contributed their expertise. They feel ownership over the quality definitions because they helped create them. That sense of ownership means they are more likely to engage with quality reports, provide feedback when scores feel wrong, and participate in the quarterly reviews that keep the fingerprint calibrated over time. A customer who feels like quality definitions were imposed on them resists the evaluation system. A customer who feels like they co-created the definitions defends it.

The discovery document maps business requirements to evaluation dimensions. But those dimensions need ground truth — concrete examples of what correct looks like — before they can be scored. The next subchapter covers building the first golden set: how to bootstrap ground truth for a customer who arrives with no labeled data and no evaluation baseline.
