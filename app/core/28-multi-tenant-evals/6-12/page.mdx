# 28.12 — Per-Tenant Experimentation: A/B Testing When Traffic Is Unequal

How do you know a model change actually helps a specific customer? Not the aggregate across your platform. Not the average across your tenant base. This specific customer, with their specific data distribution, their specific quality criteria, and their specific use case. The answer, in any rigorous engineering organization, is experimentation — run the old version and the new version simultaneously, split traffic, measure outcomes, and let the data decide. But in a multi-tenant system, this straightforward answer collides with a brutal reality: your tenants do not produce equal traffic. Your largest customer sends 400,000 requests per month. Your smallest sends 200. Applying the same A/B testing framework to both is like applying the same clinical trial design to a hospital with 10,000 patients and a rural clinic with 12. The statistics do not transfer. The conclusions do not hold. And if you ignore this inequality, your experiment results will confidently tell you things that are not true for the customers who matter most.

## Why Standard A/B Testing Fails for Multi-Tenant

Standard A/B testing assumes a single, homogeneous population. You split traffic randomly, measure a metric in each group, compute the difference, and test whether the difference is statistically significant. The framework relies on three assumptions that multi-tenant systems violate.

The first violated assumption is **population homogeneity**. Standard A/B tests assume that any user in the treatment group is exchangeable with any user in the control group. In a multi-tenant system, this is false. A request from your healthcare customer is not exchangeable with a request from your e-commerce customer. They have different input distributions, different output requirements, different quality rubrics, and different definitions of success. Pooling all requests into a single experiment treats a clinical discharge summary and a product description as interchangeable units of observation. The aggregate experiment result is a weighted average that may not represent any individual tenant's experience.

The second violated assumption is **equal representation**. Standard A/B tests assume roughly balanced groups with sufficient sample sizes. In a multi-tenant system, traffic follows a power law. Your top 10 tenants might generate 65 percent of total platform traffic. Your bottom 100 tenants might generate 3 percent combined. An experiment that splits traffic 50-50 at the platform level gives your largest tenant a sample size of 200,000 in each arm and your smallest tenant a sample size of 100 in each arm. The largest tenant can detect a 0.5 percent quality difference with high confidence. The smallest tenant cannot detect anything less than a 15 percent difference. You have one experiment running at two radically different levels of statistical power, and the aggregate result is dominated by the tenants with the most traffic, not the tenants most affected by the change.

The third violated assumption is **absence of tenant-level confounders**. A model change might improve quality for tenants whose inputs are predominantly short-form text and degrade quality for tenants whose inputs are long-form documents. In a standard A/B test, this interaction effect is invisible — the short-form improvement swamps the long-form degradation in the aggregate. You declare the experiment a success. You ship the change. Your long-form tenants experience a regression you never detected because the experiment was not designed to detect per-tenant effects.

## Per-Tenant Experiment Design

The solution is to abandon platform-level experimentation as the primary decision framework and replace it with **per-tenant experimentation** — separate experiments, separate analyses, and separate decisions for each tenant or tenant cluster.

Per-tenant experimentation splits traffic within each tenant rather than across the platform. Tenant A's traffic is split 50-50 between control and treatment. Tenant B's traffic is split 50-50 independently. The analysis runs separately for each tenant, producing a per-tenant effect estimate and a per-tenant confidence interval. The decision to ship the change is made per tenant: if the treatment improves quality for tenant A with sufficient confidence, tenant A gets the change. If it degrades quality for tenant B, tenant B stays on the control.

This design eliminates the homogeneity and confounder problems. Each tenant's experiment is internally homogeneous — all requests within the tenant share the same domain, quality criteria, and input distribution. Tenant-level effects are measured directly rather than inferred from aggregates. The trade-off is statistical power. Splitting one large experiment into 300 small experiments means each small experiment has less data and less power than the aggregate would have had.

Implementing per-tenant experimentation requires three infrastructure capabilities. First, your serving layer must support per-tenant traffic splitting — the ability to route a configurable percentage of each tenant's traffic to an alternate model version, adapter version, or configuration. This is distinct from platform-level canary routing, which splits traffic across all tenants simultaneously. Per-tenant splitting requires tenant-aware routing logic that maintains a separate split ratio for each tenant. Second, your eval pipeline must support per-tenant experiment analysis — computing quality metrics separately for control and treatment groups within each tenant, applying the tenant's own quality rubric, and producing per-tenant statistical summaries. Third, your experiment management system must track hundreds of concurrent per-tenant experiments without conflating their results.

## Minimum Detectable Effect Per Tenant

The concept that governs per-tenant experimentation is **minimum detectable effect** — the smallest quality difference the experiment can reliably detect for a given tenant at a given confidence level and statistical power. The minimum detectable effect is a function of the tenant's traffic volume, the variance in their quality scores, the confidence level you require (typically 95 percent), and the statistical power you target (typically 80 percent).

For a tenant sending 100,000 requests per month with a quality score standard deviation of 8 points, a two-week experiment with a 50-50 split provides roughly 25,000 observations per arm. At 95 percent confidence and 80 percent power, the minimum detectable effect is approximately 0.2 points — you can detect very small quality changes with high reliability. This tenant is experiment-rich.

For a tenant sending 500 requests per month with the same variance, a two-week experiment provides roughly 125 observations per arm. The minimum detectable effect jumps to approximately 2.8 points. You can only detect large quality changes. Anything below a 2.8-point shift is invisible to the experiment. This tenant is experiment-poor.

Most multi-tenant platforms have a small number of experiment-rich tenants and a large number of experiment-poor tenants. The minimum detectable effect creates a per-tenant experimentation budget: each tenant can only answer questions above a certain effect size threshold, and that threshold varies by two orders of magnitude across your tenant base.

The practical consequence is that per-tenant experimentation is not universally applicable. For your top 50 tenants by traffic volume, per-tenant A/B testing works well. You can detect meaningful quality differences within reasonable experiment durations. For your bottom 200 tenants, per-tenant A/B testing is underpowered. A two-week experiment cannot detect anything meaningful because the sample size is too small. Running the experiment anyway — and drawing conclusions from it — is worse than not running it, because statistically insignificant results are often misinterpreted as evidence of no effect.

## Stratified Experimentation

For tenants where individual experimentation is underpowered, **stratified experimentation** provides an alternative. Instead of running separate experiments per tenant, you group tenants with similar characteristics into strata and run experiments at the stratum level.

Strata are defined by the tenant attributes most likely to moderate the treatment effect: industry vertical, input type (short-form vs long-form), primary use case category, adapter rank, and quality score variance. A stratum of "healthcare tenants using clinical document summarization with rank-16 adapters" might include 8 tenants who collectively generate 12,000 requests per month. Individually, none of these tenants has enough traffic for a powered experiment. Collectively, the stratum provides sufficient sample size to detect a 1.5-point quality difference.

Stratified analysis requires a critical assumption: that tenants within a stratum respond similarly to the treatment. If the model change improves quality for 7 of the 8 healthcare tenants in the stratum but degrades quality for one, the stratum-level result will show improvement, and the one degraded tenant will be invisible. To mitigate this, stratified experimentation should include a per-tenant breakdown within each stratum. The stratum provides the statistical power for significance testing. The per-tenant breakdown provides the directional signal for individual tenants, even if those individual signals are not statistically significant on their own. A tenant that shows a negative quality delta in an otherwise positive stratum deserves investigation, even if its individual result is not significant.

Stratification is not a perfect substitute for per-tenant experimentation. It trades precision for power. But for the long tail of low-traffic tenants, it is the only way to get any experimental signal at all.

## Handling Experiments That Help Most Tenants but Harm a Few

This is the hardest problem in multi-tenant experimentation, and there is no clean answer.

You run a platform-level experiment on a base model update. The stratified results show quality improvement for 280 tenants, neutral effect for 40 tenants, and quality degradation for 12 tenants. The aggregate effect is positive. The weighted average effect is positive. Every statistical test says ship it. But those 12 tenants — three of whom are in your top 20 by revenue — will experience worse quality if you deploy the change.

The temptation is to ship the change and address the 12 affected tenants individually. This is the right call for small degradations — a 1 to 2 point quality drop that can be mitigated with prompt adjustments or adapter retraining. It is the wrong call for large degradations. A 10-point quality drop for a tenant paying $350,000 per year is not an acceptable cost of platform-wide improvement. That tenant signed a contract with quality guarantees, and "most customers got better" is not a defense for violating those guarantees.

Three strategies handle this tension.

**Conditional rollout** deploys the change to tenants who benefit and withholds it from tenants who do not. This is operationally complex — you are now running two versions simultaneously — but it respects per-tenant quality commitments. The withheld tenants remain on the previous version while the platform team investigates the degradation root cause and develops a tenant-specific mitigation. This approach is feasible when the number of withheld tenants is small (under 20) and the degradation root causes are distinct enough to investigate individually.

**Adaptive rollout** deploys the change gradually, monitoring per-tenant quality in real time, and automatically halts deployment for any tenant whose quality drops below a configurable threshold. The rollout proceeds for tenants where quality holds steady or improves and pauses for tenants where it degrades. This requires tight integration between your deployment system, your real-time eval pipeline, and your per-tenant threshold configuration. The eval pipeline described in Section 15 provides the foundation, and the release gate architecture from Section 18 provides the enforcement mechanism.

**Tenant-specific adaptation** accepts the base change for all tenants but applies per-tenant compensations for the affected tenants. The compensation might be an adapter adjustment, a prompt modification, a quality threshold recalibration, or a retrieval parameter tuning. This is the most sustainable approach because it does not require long-term version pinning, but it requires engineering investment in each affected tenant. For 12 affected tenants, that might mean 12 investigation cycles and 12 custom mitigations — substantial effort but far less than the cost of losing those customers.

## Experiment Governance

Per-tenant experimentation at scale requires governance — rules about who can run experiments on which tenants, how experiments are approved, and how conflicts between concurrent experiments are resolved.

**Experiment approval authority** should be tiered by tenant value and experiment risk. Low-risk experiments on low-value tenants — testing a minor prompt variation on a self-service customer — can be approved by the platform team autonomously. High-risk experiments on high-value tenants — testing a base model change on a top-10 customer — require approval from the customer's technical contact and your customer success manager. Any experiment that could affect a tenant's SLA compliance requires explicit customer approval before traffic splitting begins.

**Experiment exclusion zones** prevent experimentation during periods when the customer cannot tolerate quality variance. A tenant in the middle of a regulatory audit is not a good experiment candidate. A tenant whose contract specifies a quality floor with financial penalties for violations is a higher-risk experiment subject than a tenant with softer quality expectations. Your experiment governance policy should maintain a per-tenant experiment eligibility status that considers contractual obligations, regulatory constraints, and active holdback requests from the preceding subchapter.

**Concurrent experiment limits** prevent multiple experiments from running on the same tenant simultaneously. If you are already testing a prompt change on tenant A, running a simultaneous model change experiment on the same tenant makes both experiments uninterpretable — you cannot attribute quality changes to either experiment individually. The standard discipline is one experiment per tenant at a time, with a cooldown period (typically one to two weeks) between experiments to let the baseline re-stabilize. For platforms with aggressive experiment velocity, a two-experiment limit with orthogonal treatment dimensions (one prompt experiment and one model experiment, never two prompt experiments) can work, but the analysis becomes substantially more complex.

**Cross-tenant experiment coordination** prevents conflicts when a platform-level change is being tested simultaneously with tenant-level experiments. If the platform team is running a base model experiment across all tenants and the customer success team is running a per-tenant prompt experiment on a specific tenant, the two experiments interact. The per-tenant prompt experiment is measuring a quality effect that includes the base model experiment's impact. An experiment coordination system — even something as simple as a shared experiment calendar and a collision detection check before each experiment launch — prevents these conflicts from producing misleading results.

## Building the Experimentation Culture

Per-tenant experimentation is not just an infrastructure capability. It is a cultural shift from "we believe this change is better" to "we have per-tenant evidence that this change is better for each customer we ship it to." That shift is uncomfortable. It slows deployment velocity. It forces the team to confront cases where a good-looking aggregate hides per-tenant harm. It requires investment in infrastructure, statistical expertise, and governance that many platform teams would rather spend on features.

The payoff is in the customer conversations. When your enterprise customer asks "how do you know this update is good for us?" and you can answer with their specific experiment results — their data, their quality rubric, their traffic, their confidence interval — you have given them something no competitor offers. Not benchmarks from a generic test suite. Not aggregate improvements across your platform. Evidence, specific to their business, that you tested the change on their workload and it made their outcomes better. That specificity is what turns a platform vendor into a trusted partner.

Chapter 7 moves from the engineering of multi-tenant releases to the business layer — cost allocation, customer reporting, evidence packages, and the feedback loops that close the gap between what the platform measures and what the customer experiences.
