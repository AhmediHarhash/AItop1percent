# 28.7 — Cross-Tenant Contamination in LLM Judge Systems

The **shared-context judge** is the most common and most dangerous contamination pattern in multi-tenant evaluation. It looks efficient. It feels harmless. And it silently leaks information between tenants in ways that violate data isolation, corrupt evaluation results, and create liability your platform may not discover for months.

The pattern works like this. Your eval pipeline uses an LLM as a judge — Claude Opus 4.6, GPT-5, or a similar frontier model — to score tenant outputs against quality rubrics. To improve judge consistency and accuracy, you include few-shot examples in the judge prompt: three or four examples of scored outputs that demonstrate what a high-quality evaluation looks like. Those few-shot examples are drawn from a shared pool. They work well. Judge accuracy improves. Nobody notices that the few-shot examples for Tenant A's evaluation contain data from Tenant B. Nobody notices because the system never checks.

This is not a theoretical concern. It is the default behavior of most multi-tenant eval systems built quickly. Engineers building the eval pipeline focus on judge accuracy, not tenant isolation. They create a pool of high-quality scored examples, use them across all tenants, and move on to the next feature. The contamination is invisible in metrics, invisible in logs, and invisible until someone audits the judge prompts and discovers that Tenant 44's legal document analysis is being scored using few-shot examples that contain Tenant 71's financial records.

## Data-Level Contamination: Shared Examples and Shared Context

Cross-tenant contamination in LLM judge systems happens through five specific mechanisms, each requiring its own defense. The first three operate at the data level — they involve actual tenant data appearing where it should not.

**Vector one: shared few-shot examples.** This is the pattern described above. Your judge prompt includes few-shot examples — input-output pairs with rubric scores — to calibrate the judge's scoring behavior. If those examples are drawn from a pool that contains data from multiple tenants, every tenant's judge call contains fragments of other tenants' data. The contamination is direct. Tenant A's proprietary customer interaction appears in the context window of the judge scoring Tenant B's outputs. Under HIPAA, GDPR, or any data protection regime, this is a breach — even if the judge model never stores the data, the fact that one tenant's data appeared in another tenant's processing context violates data isolation commitments.

The defense is **per-tenant few-shot pools**. Each tenant's few-shot examples are drawn exclusively from that tenant's own evaluated data or from a tenant-specific set of synthetic calibration examples. When you onboard a new tenant and have no tenant-specific examples yet, use synthetic examples that are domain-relevant but contain no real tenant data. Never borrow examples from another tenant to bootstrap a new tenant's judge calibration, no matter how similar their use cases appear.

**Vector two: judge context window accumulation.** Some eval pipeline implementations reuse the same API session or conversation context across multiple evaluations to reduce cost. The judge scores Tenant A's output, then — in the same conversation thread — scores Tenant B's output. The judge model now has Tenant A's data in its context window when scoring Tenant B. Even if the model does not explicitly reference Tenant A's data in its scoring of Tenant B, the data is present in the attention mechanism and can subtly influence the judge's scoring behavior. Research published in early 2025 on preference leakage in LLM-as-judge systems demonstrated that prior context influences judge scores in measurable ways, even when the prior context is ostensibly unrelated to the current evaluation.

The defense is **tenant-isolated judge sessions**. Every tenant's evaluation runs in a fresh API session with no prior context from any other tenant. This means no conversation threading across tenants, no session reuse, and no batch API calls that mix tenants in the same request. Each tenant's judge calls start from a clean context window containing only that tenant's rubric, that tenant's few-shot examples, and the specific output being evaluated. This increases API costs — you lose the efficiency of batching — but the cost of a cross-tenant contamination incident dwarfs the cost of separate API calls.

**Vector three: judge response caching.** Caching is a natural optimization for eval systems. If you evaluate the same input-output pair against the same rubric twice, why call the judge again? Cache the first result and return it immediately on the second call. The problem arises when the cache key is not tenant-scoped. If the cache key is computed from the input, output, and rubric — but not the tenant ID — then two tenants with similar inputs can receive each other's cached scores. Tenant A's output is scored. The result is cached. Tenant B submits a similar output. The cache returns Tenant A's score. Tenant B now has a score that was computed using Tenant A's few-shot examples, Tenant A's rubric nuances, and Tenant A's judge configuration.

The defense is **tenant-partitioned caching**. The tenant ID must be part of every cache key. This means separate cache namespaces per tenant, separate eviction policies per tenant, and no cross-tenant cache lookups under any circumstances. If your caching layer does not support tenant-scoped keys, build a key prefix scheme that includes the tenant ID as the first component. A cache miss is always safer than a cross-tenant cache hit.

## Configuration and Model-Level Contamination

The remaining two contamination vectors operate at a higher level than data. They involve evaluation logic and model behavior leaking between tenants, which is harder to detect because no raw data changes hands — yet the evaluation results are still corrupted.

**Vector four: rubric leakage through shared judge prompts.** Even when few-shot examples are tenant-specific, the judge prompt itself might be shared across tenants with only parameter substitutions. If the prompt template includes references to scoring criteria that are specific to one tenant — "evaluate compliance with the pharmaceutical labeling standard" — and that template is accidentally applied to another tenant, the wrong tenant receives scores calibrated for someone else's quality definition. This is not a data contamination — it is a configuration contamination — but the effect is the same: one tenant's evaluation criteria leaking into another tenant's scoring.

The defense is **versioned, tenant-scoped judge prompt templates**. Each tenant's judge prompt is stored as a versioned artifact associated with that tenant's eval configuration. When the eval pipeline assembles a judge call, it retrieves the prompt template for the specific tenant being evaluated, not a shared template with variable substitution. The prompt is part of the tenant's eval fingerprint, as discussed in earlier subchapters. If the prompt does not match the tenant, the eval run should fail loudly rather than proceed silently with the wrong configuration.

**Vector five: training data contamination in fine-tuned judges.** Some platforms fine-tune their own judge models for better scoring accuracy in their specific domain. If the fine-tuning dataset for the judge model contains scored examples from multiple tenants, the judge has literally learned from one tenant's data and applies that learning when scoring another tenant's outputs. This is the deepest form of contamination because it is baked into the model weights, not just the context window. You cannot fix it by clearing the context or partitioning the cache. The contamination is permanent until you retrain the model.

The defense is to never fine-tune judge models on multi-tenant data without explicit tenant consent and strict data handling protocols. If you fine-tune a shared judge, use only synthetic data or data from tenants who have explicitly opted in and whose data protection agreements permit use in model training. Better yet, if a tenant requires a fine-tuned judge, fine-tune a separate judge instance for that tenant using only their data. The cost of maintaining per-tenant judge models is high, but it is the only approach that provides genuine isolation at the model weight level.

## Detecting Contamination After the Fact

Prevention is essential. Detection is your safety net. You need both because contamination vectors evolve as your pipeline changes, and a vector you sealed six months ago can reopen when an engineer adds a new caching layer or refactors the prompt assembly code.

The first detection method is **prompt auditing**. Periodically sample judge calls from your eval pipeline and inspect the full prompt that was sent to the judge. Check every few-shot example against the tenant's approved example set. Check every rubric element against the tenant's approved rubric version. Check for any content that belongs to a different tenant. This audit can be automated by comparing prompt contents against a per-tenant allowlist of approved content hashes. Any content in the prompt that does not match the allowlist for the current tenant is a contamination flag.

The second detection method is **cache cross-reference analysis**. Sample cache hits and verify that the cached result's tenant ID matches the requesting tenant's ID. If your cache does not store the originating tenant ID with each cached result, you cannot perform this check — which is itself a finding, because it means your cache cannot prove isolation. Add tenant ID to every cache record specifically to enable this verification.

The third detection method is **scoring anomaly detection**. If a tenant's scores suddenly shift in a way that does not correlate with any change in their model, rubric, or data, investigate whether the scoring shift correlates with changes to another tenant. A contaminated judge can produce scores that reflect another tenant's quality characteristics. If Tenant A's scores start drifting toward Tenant B's typical score range around the same time that Tenant B's rubric was updated, the correlation is worth investigating. It does not prove contamination — many things cause score shifts — but it narrows the search.

## Why Contamination Is Hard to See in Metrics

One reason cross-tenant judge contamination persists for so long is that it rarely produces obviously wrong results. The contamination is subtle. A judge using Tenant A's few-shot examples to score Tenant B might produce scores that are directionally reasonable — the judge is still a capable evaluator, after all — but systematically biased in ways that only appear when you look at fine-grained score distributions over time.

Consider a healthcare customer whose judge prompts are contaminated with few-shot examples from a marketing customer. The healthcare rubric emphasizes citation accuracy and clinical precision. The marketing examples emphasize brand voice and engagement. The judge, processing both signals in its context, produces healthcare scores that are slightly inflated on readability dimensions and slightly deflated on clinical accuracy dimensions. The composite score might look normal. But the dimensional breakdown reveals a subtle distortion — the judge is unconsciously importing the marketing customer's quality priorities into the healthcare customer's evaluation. Over weeks, this distortion accumulates in trend data, corrupts baseline comparisons, and eventually leads the healthcare customer to believe their clinical accuracy is declining when it is actually stable. The noise is coming from the contaminated judge, not from the model.

This subtlety is what makes contamination so dangerous. A judge that returns completely wrong scores would be caught immediately. A judge that returns slightly biased scores — biased by three to five points on a specific dimension, in a direction consistent with another tenant's evaluation criteria — can operate undetected for months. The only reliable detection is to look at the judge's inputs, not just its outputs. If the inputs are contaminated, the outputs cannot be trusted, even if they look reasonable.

## The Organizational Cost of Contamination

The technical fixes for cross-tenant judge contamination are straightforward. Per-tenant few-shot pools. Isolated judge sessions. Tenant-partitioned caching. Scoped prompt templates. None of these require exotic engineering. What makes contamination expensive is discovering it after the fact and dealing with the consequences.

When a regulated customer discovers that another tenant's data appeared in their evaluation context, the incident response follows a predictable and painful trajectory. The customer demands a full investigation. Your team must audit every judge call for that tenant to determine the scope of contamination — how many eval runs were affected, what data was exposed, and whether the contamination influenced scores that were used to make production decisions. The customer's legal team reviews the findings against their data protection agreement with your platform. If the contamination involved data subject to HIPAA, GDPR, or the EU AI Act, regulatory notification requirements may be triggered. The customer loses trust in your evaluation system — and by extension, in every quality score your platform has ever reported to them. Rebuilding that trust takes months, costs engineering resources, and often requires contractual concessions.

A B2B AI platform serving financial services customers in 2025 discovered that their judge caching layer had been returning cross-tenant cached scores for seven weeks. The cache key included the input hash, the output hash, and the rubric version — but not the tenant ID. Two tenants using the same model for similar document analysis tasks had enough input overlap that roughly 4 percent of their judge calls returned cached results from the other tenant. The scores were similar enough that no anomaly detection caught the issue. It was discovered during a manual audit prompted by an unrelated customer complaint. The remediation — re-running seven weeks of evaluations for both tenants, auditing the results for score differences, and producing corrected evidence packages — consumed three engineers for two weeks. The commercial cost included a contract renegotiation with one of the affected tenants and a security review that delayed the renewal of the other.

Seven weeks of contamination. A missing field in a cache key. That is the margin between isolation and incident.

## Building Contamination Resistance Into the Pipeline

The systemic fix is to treat tenant isolation as a design constraint in the eval pipeline, not a feature layered on top. Every component that touches evaluation data — prompt assembly, judge invocation, response parsing, score aggregation, caching, logging — must be tenant-aware from the ground up.

This means the eval pipeline has a tenant context that is set once at the beginning of a run and propagated to every downstream component. The prompt assembler reads from the tenant's few-shot pool. The judge invoker opens a fresh session. The cache reads from and writes to the tenant's namespace. The logger tags every record with the tenant ID. The aggregator computes scores using only that tenant's results. No component shares state across tenants. No component has a code path that could accidentally cross tenant boundaries.

Code review checklists for eval pipeline changes should include an explicit isolation check: "Does this change introduce any shared state that could cross tenant boundaries?" New caching layers, new prompt optimizations, new batching strategies — all of these are contamination opportunities. The review question forces the engineer to think about isolation before the change ships, not after an auditor finds the gap.

Testing tenant isolation — verifying through active measures that your boundaries actually hold — is a discipline of its own. The next subchapter covers how to build and run those tests, because contamination resistance that you have not verified is not resistance you can trust.
