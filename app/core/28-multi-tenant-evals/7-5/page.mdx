# 28.65 — The Customer Feedback Loop: Incorporating Per-Customer Corrections Into the Eval System

In March 2025, a document intelligence platform serving 280 enterprise customers launched a new eval pipeline that scored every output on accuracy, completeness, and format compliance. The scores were precise, reproducible, and wrong — at least according to 23 customers who filed complaints in the first six weeks. A legal services firm reported that outputs the eval scored at 92 percent accuracy were being rejected by their attorneys because the model omitted jurisdictional qualifiers that no rubric had been taught to measure. An insurance company flagged that perfectly formatted claims summaries used terminology their adjusters did not recognize, making the outputs useless despite high eval scores. A logistics provider pointed out that "complete" summaries were missing carrier exception codes that existed nowhere in the platform's golden sets. The eval system was measuring what the platform defined as quality. The customers were experiencing what their businesses defined as quality. Those two definitions did not match, and no amount of rubric tuning by the platform team alone could close the gap.

The mechanism that closes that gap is the **customer feedback loop** — the structured pipeline through which per-customer corrections, disagreements, and complaints flow back into the eval system and change how quality is measured for that specific tenant. Without this loop, your eval system calcifies around the assumptions you made during onboarding. With it, the eval system learns from the only source that actually knows what "correct" means for each customer: the customer themselves.

## Types of Customer Feedback

Customer feedback about quality arrives through at least five channels, and treating them all the same is a mistake because they carry different levels of signal and different levels of noise.

**Explicit corrections** are the highest-signal feedback. The customer identifies a specific output, explains what was wrong, and often provides what the correct output should have been. A healthcare customer who flags a clinical summary and writes "this omitted the differential diagnosis, which must always be included per our internal protocol" is handing you a labeled training example for their per-customer eval. Explicit corrections are rare — they require effort from the customer — and they are gold.

**Re-processing requests** are implicit signals. When a customer sends the same input through the system a second or third time, often with slight prompt modifications, they are telling you the first output was unsatisfactory. They may not articulate why. But the pattern — same input, multiple attempts — is a quality failure signal that your ingestion pipeline should capture automatically. A tenant whose re-processing rate climbs from 4 percent to 11 percent over a quarter is experiencing quality degradation that their eval scores may not reflect.

**Support tickets** contain quality feedback buried in operational language. A customer who writes "the output formatting changed and our downstream parser is breaking" is reporting a format compliance failure. A customer who writes "the model seems less accurate this week" is reporting a quality regression. Support tickets are noisy — they mix quality issues with billing questions, access problems, and feature requests — but they are often the first signal of a quality gap because customers contact support before they contact their eval dashboard.

**Escalation patterns** are the most expensive feedback channel and the strongest signal. When a customer escalates from their technical contact to their account executive to their VP of Engineering, the severity of the quality issue increases at each stage. Escalations that reach the executive level almost always represent fundamental misalignment between the eval system's measurement and the customer's actual quality requirements. They are too costly to be the primary feedback mechanism, but they must be captured and analyzed when they occur.

**Usage pattern changes** are the subtlest signal. A customer who reduces their API call volume by 30 percent over two months may be experiencing quality issues they never reported. A customer who stops using a specific feature — say, the summarization endpoint — may have found the output quality insufficient for their use case. These signals require correlation with other data sources to be actionable, but they should feed into the feedback pipeline as weak signals that trigger proactive outreach.

## The Feedback Ingestion Pipeline

Capturing feedback from five different channels and routing it to the eval engine requires a structured ingestion pipeline, not an ad hoc process where engineers manually update rubrics when customers complain loudly enough.

The pipeline starts with a **unified feedback interface** — a single system where all customer quality signals converge regardless of their origin channel. Explicit corrections submitted through the customer dashboard, re-processing events detected by the serving layer, support tickets tagged with quality-related labels, escalation records from the customer success team, and usage anomalies flagged by the analytics system all flow into this interface. Each feedback item is tagged with the tenant identifier, the timestamp, the originating channel, the specific output or outputs referenced, and any customer-provided correction or explanation.

The interface feeds into a **classification stage** that sorts feedback by type and urgency. Feedback that includes an explicit correction with a reference output is classified as a potential golden set update. Feedback that identifies a pattern — "all outputs this week are missing section headers" — is classified as a potential rubric adjustment. Feedback that is vague — "quality seems worse" — is classified as requiring investigation. Feedback from a tier-1 customer or involving a contractual quality dimension is flagged for priority processing. This classification determines the downstream workflow: golden set updates follow the version-controlled curation pipeline, rubric adjustments go through the calibration process described in Chapter 5, and vague feedback triggers a diagnostic investigation.

The classified feedback then enters a **validation queue** where the platform's eval team reviews each item before it modifies the eval system. Not all customer feedback is correct. Some corrections reflect the customer's preferences rather than objective quality failures. Some contradict earlier feedback from the same customer. Some are based on misunderstanding what the system was configured to do. Validation ensures that only feedback that genuinely represents a gap between the eval system and the customer's quality requirements makes it into the eval configuration.

## Validation and Deduplication

Validation is where the feedback loop either builds credibility or erodes it. Accept every customer correction without validation and you build an eval system that chases noise. Reject corrections without explanation and you teach customers that feedback is pointless.

The validation process examines each feedback item against three criteria. First, **reproducibility**: can the platform team reproduce the quality issue the customer identified? If the customer flagged an output as incorrect but the eval system scores it at 94 percent against the customer's own rubric, either the rubric is missing a dimension or the customer is applying criteria that were never part of the quality contract. Both are valuable discoveries, but they lead to different actions. Second, **consistency**: does this correction align with or contradict previous feedback from the same customer? A customer who previously approved outputs with this exact structure but now flags them as incorrect may have changed their requirements without updating their quality contract. The feedback is valid — their needs changed — but the response is a quality contract update, not a silent eval modification. Third, **scope**: does the correction apply to this specific output, to a category of outputs, or to all outputs? A customer who corrects a single factual error is providing a data point. A customer who says "all outputs should include a disclaimer paragraph" is requesting a rubric change. Misclassifying scope leads to either under-correction (treating a systemic issue as a one-off) or over-correction (treating a one-off as a systemic pattern).

Deduplication handles the reality that customers often report the same issue multiple times through different channels. The support ticket about formatting and the explicit correction about missing section headers and the re-processing request on the same document may all be reporting the same underlying quality gap. Without deduplication, the eval team wastes effort investigating the same issue three times and may apply redundant corrections that compound into over-adjustment.

## Updating Golden Sets With Customer Corrections

When validation confirms that a customer correction represents a genuine quality gap, the correction must flow into the customer's evaluation configuration — typically their golden set — through a version-controlled process with full audit trail.

The correction is first formatted as a **candidate golden set entry**: the original input, the system's output that was flagged as incorrect, the customer's correction or the corrected reference output, and the quality dimensions that the correction addresses. This candidate enters a staging area where it is evaluated for compatibility with the existing golden set. Does it duplicate an existing entry? Does it contradict an existing entry — testing the same scenario but with a different expected outcome? Does it introduce a new dimension or scenario that the golden set did not previously cover?

If the candidate introduces a contradiction — the customer now expects a different output for a scenario that was previously marked as correct — the conflicting entry must be resolved, not simply overridden. The resolution may involve updating the existing entry to match the customer's current expectations, splitting the scenario into two entries that cover different time periods or contexts, or discussing the contradiction with the customer to clarify which expectation is current. Every resolution is logged with the rationale, creating an audit trail that explains why the golden set changed and who approved the change.

The updated golden set is tagged with a new version identifier. The previous version is archived, not deleted. This version control is essential for three reasons. It allows the platform to track how eval criteria evolved over time for each customer. It enables rollback if a golden set update inadvertently degrades eval accuracy. And it satisfies audit requirements under the EU AI Act's documentation obligations for AI systems, which require traceability of evaluation criteria changes — a requirement detailed in Section 29 on enterprise governance.

## The Over-Fitting Danger

Here is the tension that every platform team must navigate: customer feedback improves per-customer eval accuracy, but unchecked incorporation of feedback can over-fit the eval system to the customer's most recent preferences rather than their stable quality requirements.

A customer who submits twelve corrections in a month is giving you valuable signal. But if ten of those corrections reflect a temporary shift in one reviewer's preferences — say, their new legal counsel prefers shorter summaries — and two reflect a genuine, lasting quality requirement, treating all twelve equally distorts the eval system. The golden set now optimizes for one reviewer's current mood rather than the customer's durable quality bar.

The mitigation is a **cooling period** and a **confirmation threshold**. New corrections enter the golden set in a provisional state. They affect eval scores but are flagged as provisional in quality reports. After 30 to 60 days, the platform reviews provisional entries: do subsequent corrections confirm the same direction, or did the customer's feedback revert? Entries that are confirmed by subsequent evidence become permanent. Entries that were contradicted or never reinforced are reviewed with the customer before either being kept or removed. This cooling mechanism prevents eval whiplash — the pattern where the eval system oscillates between conflicting customer preferences, never stabilizing on a consistent quality measurement.

## Feedback Latency and Customer Trust

How quickly corrections affect future eval scores is both a technical parameter and a relationship signal. A customer who submits a correction and sees no change in their quality scores for three months concludes that feedback is ignored. A customer who submits a correction and sees their scores shift within a week knows the system is responsive.

The target feedback latency depends on the correction type. Rubric-level changes — adding a new quality dimension or adjusting dimension weights — should be reflected in the next scheduled eval run, typically within 24 to 48 hours. Golden set additions should be incorporated within one to two weeks, allowing time for validation and deduplication. Systemic corrections — feedback that requires retraining a judge model or restructuring the eval pipeline — may take four to six weeks and should be communicated to the customer with a timeline and status updates.

Regardless of the latency, the customer should receive acknowledgment within 24 hours that their feedback was received and is being processed. After validation, they should receive a summary of what action was taken: "Your correction was added to your golden set as a new entry covering jurisdictional qualifier requirements," or "Your feedback identified a gap in your rubric's format compliance dimension, which has been updated to include section header requirements." This acknowledgment is not bureaucratic overhead. It is the mechanism through which customers learn that the eval system is theirs — shaped by their input, responsive to their reality, improving because they invested the effort to provide corrections.

## Closing the Loop Visibly

The most powerful version of the feedback loop is visible. When a customer submits a correction and, two weeks later, their quality report includes a note — "Based on your March 15 correction, we added jurisdictional qualifier checks to your accuracy dimension; your accuracy score now reflects this requirement" — the customer sees cause and effect. Their feedback changed the system. Their next quality report is more aligned with their reality because they invested in making it so.

This visibility transforms the customer relationship from passive consumer to active participant. Customers who see their feedback reflected in the eval system submit more corrections, invest more in their golden sets, and tolerate temporary quality issues more patiently because they know the system is learning. Customers who submit feedback into a black hole stop submitting feedback, stop trusting the eval scores, and start treating the quality reports as marketing material rather than operational truth.

The feedback loop is the mechanism through which per-customer eval accuracy improves over time. But feedback is only one input that competes for the platform's finite eval resources — compute, judge API quota, and human reviewer hours. When four hundred customers all need eval runs and all expect timely results, the question of who gets resources first becomes unavoidable. The next subchapter addresses fair resource allocation across hundreds of customers with different tiers, SLAs, and expectations.
