# 28.9 — Tenant-Scoped Judge Calibration: Aligning Scores Per Customer

An LLM judge that has never seen your customer's domain will still produce a score. It will produce that score confidently. It will produce it in the format you asked for, against the rubric you provided, with a reasoning chain that sounds plausible. And that score will be wrong — not randomly wrong, but systematically wrong in a direction that depends on the gap between what the judge was calibrated on and what the customer's content actually looks like. This systematic error is **Judge Calibration Drift**, and it is the single largest hidden source of per-tenant evaluation inaccuracy in multi-tenant platforms.

Judge Calibration Drift is what happens when an LLM judge tuned, prompted, or few-shot-calibrated on one domain produces scores that are consistently too high or too low when applied to another domain. A judge whose anchor examples come from e-commerce product descriptions learns that a "4 out of 5" response is fluent, accurate, and has the right tone. That same judge, applied to a clinical radiology report, will rate a concise, technically precise report as a 3 because it lacks the conversational tone the judge learned to associate with high quality. The judge is not malfunctioning. It is applying its calibration faithfully — the calibration just does not match the customer's domain.

## Why Domain Shift Is Not a Prompt Problem

The first instinct most teams have when they discover tenant-specific scoring inconsistencies is to fix the judge prompt. Add more instructions. Make the rubric more explicit. Tell the judge to "evaluate based on the clinical context, not general quality." This helps, but it does not solve the problem, because the drift is not primarily in the prompt interpretation. It is in the judge's internal representation of quality.

LLM judges, even frontier models like Claude Opus 4.6 or GPT-5.1, carry implicit quality priors learned during pre-training and reinforcement from human feedback. Those priors associate certain surface features with quality: longer responses are often rated higher, responses with hedging language are rated lower for factual tasks, responses that use domain jargon are rated higher for technical tasks. These associations are baked into the model. They are not things you can override with a rubric paragraph. When you ask a judge to score a German insurance policy extraction against a rubric that says "accuracy is the primary dimension," the judge will still subtly penalize terse outputs and reward elaborate ones, because that association runs deeper than prompt-level instruction.

Research from 2025 into LLM-as-a-judge systems consistently showed that domain gaps cause human-judge agreement to drop by ten to fifteen percentage points compared to in-domain scoring. That means if your judge agrees with human experts 85 percent of the time on e-commerce content, it might agree only 70 to 75 percent on clinical content — even with the same rubric structure, the same scoring scale, and the same prompt template. The gap is not in your rubric. It is in the judge's calibration.

## What Per-Tenant Calibration Actually Means

Per-tenant judge calibration is the process of adjusting your judge's scoring behavior so that it aligns with each customer's definition of quality at each score level. When a healthcare tenant and a fintech tenant both use a five-point scale, a score of 4 should mean "meets expectations with minor issues" for both — but what counts as a "minor issue" is radically different. For the healthcare tenant, a minor issue might be a formatting inconsistency in a discharge summary. For the fintech tenant, a minor issue might be a slightly imprecise description of a fee structure. The judge needs to understand those differences at the scoring boundary level, not just at the rubric definition level.

There are three techniques that make this work at scale: per-tenant calibration sets, anchor examples, and calibration verification runs. Each serves a different purpose, and mature platforms use all three together.

## Per-Tenant Calibration Sets

A **per-tenant calibration set** is a small collection of outputs — typically fifteen to forty examples — that have been scored by human experts who understand the tenant's quality definition. These examples span the full range of the scoring scale: three to five examples at each score level, chosen to represent the scoring boundaries where reasonable people might disagree. The calibration set is not a golden set. Golden sets measure model output quality. Calibration sets measure judge scoring accuracy.

The distinction matters. Your golden set asks: "did the model produce a good output?" Your calibration set asks: "when the judge sees this output, does it assign the same score a human expert would?" These are different questions with different data requirements. A golden set needs representative coverage of the tenant's use case. A calibration set needs careful selection of boundary cases where judge error is most likely and most consequential.

Building calibration sets requires human effort from people who understand the tenant's domain. For a healthcare tenant, that means clinicians reviewing sample outputs and assigning scores with written rationale. For a legal tenant, that means attorneys doing the same. You cannot shortcut this with generic annotators, because the entire point of the calibration set is to capture the tenant's specific quality standard — and generic annotators do not know what a radiologist considers a minor issue versus a critical one.

The investment pays off in scoring accuracy. Teams that implement per-tenant calibration sets typically see judge-human agreement increase from the seventy to seventy-five percent range to the eighty-two to eighty-eight percent range for domain-specific content. That twelve-point improvement is the difference between an eval system that catches real quality issues and one that generates so many false positives and false negatives that nobody trusts it.

## Anchor Examples: Teaching the Judge What Each Score Means

**Anchor examples** are the few-shot demonstrations you include in the judge prompt to ground each score level in concrete, tenant-specific instances. Where calibration sets are used to measure judge accuracy, anchor examples are used to improve it. You select the clearest, most unambiguous examples from the calibration set — typically one or two per score level — and embed them directly in the judge prompt as demonstrations.

The anchor example for a score of 5 on a pharmaceutical tenant's rubric might be a regulatory summary that is factually complete, uses no speculative language, traces every claim to a source document, and formats citations in the style the tenant requires. The anchor for a score of 2 might be a summary that gets the basic facts right but includes two hedging phrases ("this likely indicates" and "the data suggests") that are disqualifying in FDA submission contexts. A general-purpose judge would rate that second example as a 3 or even a 4 — it is mostly correct and reads well. The anchor teaches the judge that in this tenant's domain, speculative language is a severe defect, not a minor style issue.

Research from 2025 shows that including one well-chosen anchor per score level improves judge scoring accuracy by fifteen to twenty percent compared to rubric-only prompts. Adding two to three anchors per level can push improvement to twenty-five to thirty percent, though some studies found diminishing returns beyond two examples per level — additional anchors can confuse the judge if they introduce conflicting signals. The optimal number depends on the complexity of the scoring boundaries. For tenants with clear, binary quality criteria, one anchor per level is sufficient. For tenants with nuanced, multi-dimensional quality definitions, two per level with rationale explanation produces the best results.

## Calibration Verification Runs

Building calibration sets and anchor examples is not a one-time activity. Judge calibration drifts for three reasons, and all three are active in a multi-tenant platform.

The first reason is judge model updates. When your judge model provider ships a new version — GPT-5 to GPT-5.1, Claude Opus 4.5 to Claude Opus 4.6 — the new model's scoring behavior changes. The anchors that perfectly calibrated the old judge may be interpreted differently by the new one. A phrase that the old model treated as clearly negative might be rated more leniently by the new one. This is not a bug in the new model. It is a consequence of the training process producing subtly different quality associations.

The second reason is tenant domain evolution. A tenant's content changes over time. A fintech tenant launches a new product line, and their outputs now include terminology the judge has never seen in the anchor examples. A healthcare tenant expands from radiology to cardiology, and the quality expectations shift in ways the original calibration did not anticipate. The calibration set that was representative six months ago may no longer cover the tenant's current quality surface.

The third reason is rubric evolution, covered in the previous subchapter. When a tenant's quality definition changes, the calibration must change with it. But teams sometimes update the rubric without updating the calibration set, creating a gap where the judge is being calibrated against outdated examples while scoring against updated criteria.

**Calibration verification runs** are the mechanism that catches all three forms of drift. A calibration verification run takes the tenant's calibration set, sends each example through the current judge pipeline, compares the judge's scores to the human-assigned scores, and computes an agreement metric — typically Cohen's kappa for ordinal scoring or simple percentage agreement within one point. When the agreement drops below a threshold — most teams set this between 0.75 and 0.80 kappa — the system flags that tenant's judge configuration for recalibration.

The frequency of verification runs depends on how often the drift sources change. For tenants using a judge model that updates quarterly, running verification after each model update is the minimum. For tenants whose content changes rapidly, monthly verification catches domain evolution before it creates significant scoring error. For tenants with stable content and stable rubrics, quarterly verification is usually sufficient. The cost is modest — scoring fifteen to forty calibration examples through your judge pipeline costs a few dollars per tenant per run — and the alternative is discovering calibration drift when a customer complains that your quality reports do not match their internal assessment.

## The Calibration Registry

At scale, managing per-tenant calibration requires infrastructure. A **calibration registry** is a data store that tracks, for each tenant: the current calibration set with version identifier, the anchor examples embedded in the judge prompt, the date and result of the last calibration verification run, the judge model version the calibration was validated against, and the agreement metric from the most recent verification. When any of these components changes — new judge model, updated rubric, refreshed calibration set — the registry marks the tenant's calibration as "unverified" and queues a verification run.

Without a registry, calibration management becomes a manual process that degrades at scale. A platform with a hundred tenants and no calibration registry will inevitably have tenants running on stale calibration — anchor examples that reference a previous rubric, calibration sets that were built for a previous judge model, verification runs that have not happened in six months. The registry converts calibration from a manual discipline into a system property, with explicit state tracking and automated verification triggers.

## The Organizational Cost

Per-tenant judge calibration is not free. Building a calibration set requires two to four hours of domain expert time per tenant. Selecting anchor examples requires someone who understands both the scoring system and the tenant's domain well enough to choose the most instructive examples. Calibration verification runs require compute and analyst review when agreement drops below threshold. For a platform with three hundred tenants, the initial calibration effort might take six to eight weeks of distributed work, and the ongoing maintenance adds one to two hours per tenant per quarter.

This cost is real, and some teams will argue it is not worth it. The counterargument is that without per-tenant calibration, your judge scores are wrong in ways that vary by tenant, and nobody knows which tenants are affected or by how much. You are running an evaluation system that produces numbers without knowing whether those numbers mean what they claim to mean. For platform-level reporting, the inaccuracies might average out. For per-customer quality SLAs, for per-customer quality dashboards, for per-customer regression detection — the use cases that justify multi-tenant evaluation in the first place — uncalibrated judges produce unreliable signals that erode the trust you are trying to build.

## When Full Calibration Is Not Feasible

Not every tenant justifies the full calibration investment. A tenant on a free tier or a self-service plan may not have access to domain experts who can score calibration examples. A tenant in a domain so niche that your platform team cannot find appropriate reviewers may be impractical to calibrate with human experts. For these tenants, there are two fallback approaches.

The first is **calibration inheritance**, where the tenant inherits calibration from the closest matching tenant cluster. If your platform groups tenants by domain — all healthcare tenants, all legal tenants, all e-commerce tenants — you can build a domain-level calibration set and apply it as the default for tenants in that domain who do not have tenant-specific calibration. This is less accurate than per-tenant calibration, but it is significantly better than no domain calibration at all. The domain-level calibration captures the most important scoring adjustments — clinical text should not be penalized for terseness, legal text should not be penalized for repetitive citation formatting — even if it misses tenant-specific nuances.

The second is **synthetic calibration validation**, where you use a stronger model to generate scoring rationales and compare them to your judge's rationales for the same examples. If Claude Opus 4.6 is your production judge, you might use a panel of three frontier models — including the production judge — to score a set of tenant outputs and flag cases where the production judge's score diverges from the panel consensus. This does not replace human calibration, but it identifies the most likely calibration failures and focuses human review effort where it will have the most impact.

Per-tenant judge calibration transforms your evaluation system from one that produces scores into one that produces meaningful scores. The difference is whether a "4 out of 5" for your pharmaceutical tenant actually means what a pharmaceutical quality expert would call a 4, or whether it means what an e-commerce-calibrated judge thinks a pharmaceutical output deserves. For every tenant that matters to your revenue and reputation, you want the former. The next subchapter addresses what happens when you try to compare those calibrated scores across tenants — and why the comparison is more dangerous than most teams realize.
