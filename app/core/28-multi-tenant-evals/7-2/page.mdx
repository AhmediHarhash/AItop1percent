# 28.62 — When Eval Costs Make a Customer Unprofitable: The Unit Economics Problem

In late 2025, a document intelligence platform serving 180 enterprise customers discovered something their finance team had never modeled. Their largest healthcare customer — a hospital network paying $420,000 annually — required eval pipelines that consumed $22,000 per month in judge API calls alone. The customer's clinical documents averaged 3,800 tokens, required dual-judge evaluation with both GPT-5 and a domain-specialized medical judge, triggered human physician review on 14 percent of flagged outputs at $95 per review hour, and demanded monthly full-set evaluation against a 6,000-example golden set for regulatory compliance. When the platform team tallied all eval costs — judge calls, compute, storage, human review, and the two engineers who spent 30 percent of their time maintaining the customer's custom eval configuration — the annual eval cost for this single customer reached $310,000. On a $420,000 contract, that left $110,000 to cover inference compute, storage, support, and margin. The customer was technically profitable. Barely. And any increase in eval complexity — a new document type, a regulatory change requiring additional judge criteria, an expansion to a second hospital system — would push them into the red.

This is the **eval unit economics problem**: the scenario where the cost of proving quality exceeds the revenue that quality generates. It affects every multi-tenant platform eventually, and it affects the customers you can least afford to lose — the ones with the most complex requirements, the highest quality bars, and the deepest regulatory obligations.

## Calculating Per-Customer Eval Cost

The formula is straightforward. The difficulty is in the inputs.

Start with **judge call costs**. Count the average number of eval judgments per document for this customer. Multiply by the average number of documents processed per month. Multiply by the average cost per judgment — which itself is a function of the judge model used, the input token count per judgment, and the output token count per judgment. A customer processing 15,000 clinical documents per month with dual-judge evaluation at an average cost of $0.08 per judgment generates $2,400 per month in judge costs alone. If the same customer requires weekly drift evaluation against a 6,000-example golden set, add another 24,000 judgments per month at the same unit cost — an additional $1,920. The monthly judge bill for this customer is $4,320, or $51,840 annually.

Add **compute costs**. The eval orchestration pipeline consumes CPU and memory proportional to eval volume and complexity. A customer running 15,000 production evals plus 24,000 drift evals per month, with result aggregation, anomaly detection, and report generation, might consume $800 to $1,200 per month in attributed compute.

Add **storage costs**. Eval results, historical scores, golden set data, audit logs, and evidence packages accumulate. A customer with regulatory retention requirements — three to seven years depending on jurisdiction — generates more storage than a customer with a standard 12-month retention window. Typical per-tenant storage costs range from $50 per month for lightweight tenants to $600 per month for tenants with large golden sets and long retention periods.

Add **human review costs**. This is where the numbers become alarming. If 14 percent of a customer's 15,000 monthly documents are flagged for human review, that is 2,100 reviews per month. At an average review time of 12 minutes and a reviewer cost of $55 per hour (or roughly $11 per review for a general analyst), monthly human review costs are $23,100. For specialized domains requiring expert reviewers — physicians at $95 per hour, licensed attorneys at $120 per hour — the cost doubles or triples. Human review is the single largest eval cost driver for complex enterprise customers, and it is the category most platforms undercount because it sits in a different budget line from infrastructure costs.

Add **engineering allocation**. If your platform team spends measurable time maintaining this customer's custom eval configuration — updating golden sets, tuning judge prompts, debugging per-tenant eval failures, building custom report templates — that time has a cost. Two engineers spending 30 percent of their time on a single customer's eval at a fully loaded cost of $180,000 per engineer represents $108,000 annually in engineering allocation.

Sum these categories and compare to the customer's annual contract value. The resulting ratio — total eval cost divided by contract value — is the **eval cost ratio**. An eval cost ratio above 0.50 means more than half the customer's contract value is consumed by evaluation alone, before you account for inference, storage, support, and margin. An eval cost ratio above 0.70 is a structural problem that pricing adjustments alone cannot solve.

## The Dangerous Feedback Loop

The eval unit economics problem contains a self-reinforcing cycle that makes it worse over time. Customers with the highest quality bars demand the most rigorous evaluation. Rigorous evaluation surfaces more issues. More surfaced issues require more human review and more frequent golden set updates. More review and updates increase eval costs. Higher eval costs squeeze margins. Squeezed margins create pressure to reduce eval investment. Reduced eval investment degrades quality. Degraded quality triggers customer escalation. Customer escalation demands more rigorous evaluation. The cycle repeats, each iteration more expensive than the last.

The feedback loop accelerates when the customer is in a regulated industry. A healthcare customer whose eval pipeline surfaces a potential compliance issue does not have the option to de-escalate. The issue must be investigated, documented, and resolved — activities that consume human review hours regardless of their impact on your margin. A financial services customer whose eval pipeline detects a drift in regulatory language compliance cannot accept reduced eval frequency as a cost-saving measure. The regulatory obligation exists independent of your pricing model.

This feedback loop explains why the most complex customers are often the most unprofitable from an eval perspective, even when their contract values are among the highest. They are not unprofitable because they are bad customers. They are unprofitable because the eval investment required to serve them well was not accounted for when the contract was priced.

## Strategies for Managing Unprofitable Eval Relationships

No single strategy fixes the eval unit economics problem. The right approach depends on the root cause.

**Tiered eval depth** adjusts the rigor of evaluation to match the customer's tier. Not every document needs dual-judge evaluation. Not every output needs human review eligibility. Define three eval tiers — standard, enhanced, and comprehensive — and map customers to tiers based on their contractual quality commitments and regulatory requirements. Standard evaluation uses single-judge assessment on a sample of outputs. Enhanced evaluation uses dual-judge assessment on all outputs with human review for disagreements. Comprehensive evaluation adds full-set golden set runs, regulatory compliance checks, and expert human review. The cost difference between tiers can be three to five times. A customer on standard evaluation might cost $3,000 per month in eval spend. The same customer on comprehensive evaluation might cost $15,000. Matching the tier to the contract value is the first and most impactful lever.

**Sampling strategies** reduce eval volume without eliminating coverage. Instead of evaluating every output for a high-volume customer, evaluate a statistically representative sample. A 20 percent stratified sample of 15,000 documents is 3,000 documents — enough for reliable quality metrics while reducing judge costs by 80 percent. The key is that sampling must be stratified by document type, complexity, and risk level. Random sampling misses rare but critical document categories. Stratified sampling ensures every category is represented even at reduced volume. Section 26 on scaling evaluation systems covers sampling design in detail.

**Amortizing golden set costs** spreads the cost of golden set creation and maintenance across the customer's contract term rather than absorbing it as a startup cost. A 6,000-example golden set with expert annotation might cost $45,000 to build. Absorbed in month one, it destroys that month's economics. Amortized across a 36-month contract, it adds $1,250 per month — material but manageable. The financial model must treat golden set investment as a capitalized asset, not an operating expense, to reflect its true economic life.

**Eval-aware pricing** adjusts the customer's contract value to reflect actual eval costs. This requires the cost attribution data from the previous subchapter. When you can demonstrate that a customer's eval requirements cost $310,000 annually, you have the evidence to negotiate a contract that supports that investment. The conversation changes from "we need to raise your price" to "your quality requirements generate these specific costs, and your pricing needs to cover them." Many enterprise customers, particularly in regulated industries, understand and accept eval-aware pricing because they recognize that the alternative — cheaper pricing with less rigorous evaluation — creates regulatory risk they cannot afford.

## Structural Unprofitability vs Acquisition Cost

Not all negative eval margins are problems. Some are investments.

A customer in their first year on the platform typically has higher eval costs than a mature customer. The golden set must be built from scratch. The judge configuration requires iterative tuning. The eval pipeline triggers more human reviews because the system is still learning the customer's quality standards. First-year eval costs might be 40 to 60 percent higher than steady-state costs. If the customer's contract extends to three years, the first-year eval premium is a customer acquisition cost — painful but temporary.

The distinction between temporary and structural unprofitability is the trend line. If eval costs decrease over the first six to twelve months as the configuration matures, the golden set stabilizes, and the human review rate declines, the relationship is converging toward profitability. If eval costs remain flat or increase because the customer's requirements keep expanding — new document types, new regulatory requirements, new quality dimensions — the unprofitability is structural.

Structural unprofitability has only three resolutions: reprice the contract, reduce the eval scope by mutual agreement, or accept the loss as a strategic investment in a reference customer whose name on your client list is worth more than the margin they generate. The worst outcome is silent resentment — continuing to serve the customer while quietly degrading eval quality to cut costs. The customer eventually notices, and the relationship deteriorates in ways that no pricing adjustment can repair.

Understanding which customers are unprofitable and why is the starting point. But customers do not care about your eval costs — they care about whether you are meeting your quality promises. The next subchapter examines the artifact that bridges that gap: the customer quality report that proves SLA compliance with evidence, not assurances.
