# 28.8 — Handoff From Onboarding to Steady-State Operations

Most teams treat the onboarding-to-steady-state handoff as a date. The customer was onboarding on Tuesday. They are in steady-state on Wednesday. The onboarding team moves to the next customer. The steady-state team adds a line to their roster. Everyone assumes continuity. What actually happens is closer to a controlled crash: the person who understands the customer's quality dimensions, scoring edge cases, and calibration history walks away, and the person who inherits the account starts from a spreadsheet summary that captures none of the judgment that made the eval system work.

The assumption that onboarding "ends" is itself the problem. Onboarding does not end. It transitions. The customer's eval system does not become self-sustaining on a date — it becomes self-sustaining when the steady-state team understands the customer as deeply as the onboarding team does. That understanding transfers through structured knowledge exchange, not through a handoff email with a link to a Confluence page. Every platform that treats the handoff as a calendar event instead of a knowledge transfer process will see the same pattern: a customer who was happy during onboarding becomes frustrated within 60 days of the transition, not because the product changed, but because the people who understood their quality context disappeared.

## What Must Transfer

The handoff is not a transfer of responsibility. It is a transfer of context. And context, unlike tasks, does not live in a ticketing system.

The onboarding team accumulates four categories of context that the steady-state team needs to operate effectively. The first is **rubric context** — not the rubric document itself, which is in the system, but the interpretation layer around it. Why does this customer's "accuracy" criterion require exact numerical matching rather than tolerance-based matching? Because their compliance team audits extracted figures against source documents and flags any deviation, even rounding differences. Why does the "tone" dimension have a higher weight than the default? Because this customer is a consumer-facing brand whose legal team reviews every AI-generated customer communication for brand voice compliance. The rubric is a set of rules. The context is the set of reasons behind those rules. Without the reasons, the steady-state team will face their first rubric dispute and have no foundation for resolving it.

The second category is **golden set rationale**. The golden set contains reference examples that define what good looks like for this customer. Each example was selected for a reason: this one covers the most common input pattern, that one covers an edge case the customer flagged during calibration, this other one was added after the baseline sprint revealed a scoring anomaly. Without that rationale, the steady-state team treats the golden set as an immutable artifact. When new edge cases emerge in production — and they always do — the team does not know whether to add examples, which existing examples to replace, or how the current examples were chosen in the first place. The golden set ossifies, and its coverage degrades as the customer's production data evolves past what the original examples represent.

The third category is **calibration history**. During onboarding, the calibration workshop produced a record of disagreements — moments where the customer's scorers and the eval system diverged, the discussion that resolved the disagreement, and the rubric adjustment that followed. This history is the institutional memory of how the customer interprets quality. When a similar disagreement arises in steady-state — and it will, because calibration is never perfectly complete — the steady-state team needs to reference the original resolution. Did we resolve this by adjusting the rubric or by educating the customer? Was the customer's interpretation adopted or was theirs overridden? Without this history, the steady-state team risks reopening settled debates, giving the customer inconsistent answers, or making rubric changes that contradict decisions already made.

The fourth category is **communication preferences and relationship dynamics**. Some customers want a weekly quality report delivered by email. Others want a monthly call with their VP of Product. Some customers have a single technical point of contact who handles everything. Others distribute eval responsibility across three teams who do not always coordinate. One customer's champion is the Head of Data who cares about statistical rigor. Another customer's champion is the VP of Operations who cares about speed and simplicity. These dynamics are not documented in any system. They live in the onboarding lead's head. When the steady-state team takes over without understanding them, they default to the platform's standard communication cadence, which may be exactly wrong for this customer.

## The Handoff Document

The handoff document is the structured artifact that captures all four categories of context and serves as the steady-state team's operating reference for this customer. It is not a summary. It is not a one-pager. It is a comprehensive reference that a new team member can read and understand why this customer's eval system is configured the way it is.

The document has five sections. The first section is the **customer profile** — the business context that drives their quality requirements. What does the customer do? Who are their end users? What regulatory environment do they operate in? What are the business consequences when AI quality degrades? A legal tech customer whose end users are paralegals reviewing contracts has different stakes than a marketing platform whose end users are content writers drafting social media posts. The customer profile ensures the steady-state team understands why quality matters to this customer, not just what quality metrics they track.

The second section is the **eval architecture summary** — the specific configuration of rubrics, golden sets, scoring pipelines, and reporting schedules for this customer. This section is technical and precise. Which rubric version is active? How many golden set examples exist, broken down by use case variant? What is the scoring cadence — real-time, daily batch, weekly sample? What model and adapter configuration does the customer run? What is their quality floor for each metric, and what happens contractually if they drop below it? This section should be detailed enough that an engineer who has never seen this customer's account can reconstruct their eval pipeline from the document.

The third section is the **calibration record** — every calibration session that occurred during onboarding, including the disagreements, resolutions, and rubric adjustments. This is typically the longest section and the one most often omitted from handoff documents because writing it properly requires the onboarding lead to reconstruct decisions they made weeks earlier. The cost of omitting it is paid months later when the steady-state team makes a rubric change that contradicts a calibration decision and the customer says, "We already resolved this during onboarding."

The fourth section is the **known issues and edge cases** — the problems the onboarding team identified but did not resolve, the scoring anomalies that were deemed acceptable for launch, and the edge cases where the rubric produces ambiguous results. Every onboarding produces these. Some are technical — "the model occasionally generates outputs in a mixed format that the rubric does not handle cleanly, and we score those manually for now." Some are organizational — "the customer's compliance team disagrees with their product team about how strictly to enforce the tone criterion, and we are using the product team's interpretation pending an internal decision." These are time bombs. If the steady-state team does not know they exist, they will discover them reactively when they explode.

The fifth section is the **relationship guide** — communication cadence, key contacts, decision-making dynamics, and the customer's sensitivity profile. Which topics require careful framing? Has the customer had a negative experience with a previous vendor that shapes their expectations? Is their executive sponsor deeply engaged or largely absent? This section reads more like a relationship brief than a technical document, and that is the point. The steady-state team is inheriting a relationship, not just an account.

## The Handoff Meeting

The handoff document is necessary but not sufficient. Documents convey information. Meetings convey judgment. The handoff meeting is where the onboarding team transfers the unwritten understanding that no document captures completely.

The meeting has three participants: the onboarding lead, the steady-state owner, and the customer's primary contact. The customer's presence is not optional. Including the customer in the handoff accomplishes two things that no internal meeting can. First, it creates an explicit moment of ownership transfer. The customer knows that their contact is changing, knows who their new contact is, and has met them. This prevents the disorienting experience of emailing their onboarding lead three weeks after handoff and receiving an auto-reply that directs them to someone they have never heard of. Second, it gives the customer the opportunity to surface concerns that they may not have raised during onboarding — issues they were saving for "later" that are now at risk of being lost in the transition.

The meeting follows a structured agenda. The onboarding lead presents the handoff document section by section, with the customer confirming each section's accuracy. The steady-state owner asks questions — and the quality of those questions is the best predictor of handoff success. If the steady-state owner asks clarifying questions about edge cases and calibration decisions, they are engaging with the material deeply enough to own it. If they nod silently through the presentation, they will discover their knowledge gaps later, at the customer's expense.

The customer's role in the meeting is to validate and augment. "Your document says our accuracy floor is 88 percent. That was correct during onboarding, but our compliance team has since raised the target to 91 percent." "You mention that our legal team reviews the tone criterion quarterly. That review happened last week and we are sending updated guidelines next Monday." These real-time corrections prevent the handoff document from becoming stale at the moment of transfer.

One pattern that consistently improves handoff quality is the **reverse walkthrough**. After the onboarding lead presents the document, the steady-state owner summarizes the customer's eval system back to the onboarding lead and the customer in their own words. This forces active recall rather than passive acknowledgment. If the steady-state owner can explain why the golden set includes 7 edge case examples for the customer's contract termination clause extraction — not just that it does, but why those specific examples were chosen — the knowledge has transferred. If they cannot, the gap is visible and can be addressed before the onboarding lead moves on.

## The Shadow Period

The handoff meeting transfers knowledge. The shadow period transfers competence. These are different things. You can understand a customer's eval system from a document and a meeting. You can only operate it confidently after handling the first round of real issues with a safety net.

The **30-day shadow period** is the window after the handoff meeting where the steady-state team handles all customer interactions and eval operations, but the onboarding lead remains available for escalation. The steady-state owner is the primary contact. They respond to the customer's questions, manage the weekly quality report, handle score disputes, and make rubric adjustment decisions. But they can escalate to the onboarding lead when they encounter a situation they do not have context for — "the customer is asking about a scoring decision that was made during calibration week 3 and I cannot find it in the handoff document" or "the customer's VP just asked why the tone score dropped and I am not sure whether this is a model change or a data distribution shift."

The shadow period serves as a controlled degradation test for the handoff document. Every escalation during the shadow period represents a gap in the document or the meeting. Track escalations by category: rubric interpretation questions, golden set decisions, communication style questions, technical configuration questions. At the end of 30 days, review the escalation log. If more than 5 escalations occurred, the handoff process has systematic gaps that need to be fixed before the next customer transitions. If more than 10 occurred, the handoff was substantially incomplete, and the steady-state team is effectively re-onboarding the customer through trial and error.

The shadow period also protects the customer from the jarring experience of a competence drop at transition. During onboarding, the customer interacted with someone who knew their business intimately — someone who could answer questions in context, who remembered conversations from three weeks ago, who could explain scoring decisions without looking them up. If the handoff is a hard cutover with no shadow, the customer's next interaction is with someone who pauses, looks things up, and gives less confident answers. The customer reads that as a quality drop in the vendor, even if the platform itself is unchanged. The shadow period smooths this transition by giving the steady-state owner time to build confidence before the safety net disappears.

## Common Handoff Failures

Five handoff failures account for the majority of post-transition customer dissatisfaction, and all five are preventable.

The first is **no documentation**. The onboarding lead carries all context in their head, intending to "write it up later." Later does not come because the next customer's onboarding has already started. The steady-state team inherits the account with nothing — no rubric rationale, no calibration history, no edge case catalog. They reconstruct context by reading the rubric document (which contains rules without reasons), reviewing eval logs (which contain scores without explanations), and asking the customer questions that were already answered during onboarding. This last behavior is the one that damages the relationship. A customer who spent four hours in calibration workshops explaining their quality expectations does not want to explain them again to a new contact three weeks later.

The second is **no overlap period**. The onboarding lead finishes on Friday and the steady-state team starts Monday. There is no joint period, no escalation path, and no mechanism for the steady-state team to access the onboarding lead's knowledge after the transition. In an enterprise SaaS platform serving 90 clients, this hard-cutover model resulted in a 40 percent increase in customer-reported issues during the first 30 days post-handoff. When the platform introduced a 30-day shadow period, the post-handoff issue rate dropped to 12 percent above baseline — still elevated, but manageable.

The third is **customer notification failure**. The customer does not know their contact changed. They email the onboarding lead, who has moved on. The email goes unanswered for 48 hours until someone notices. The customer's first experience of steady-state is being ignored. This seems trivially avoidable, and it is — which is why its frequency is so damaging to team credibility. A simple handoff email from the onboarding lead introducing the steady-state owner, with both copied, prevents the problem entirely. Its absence signals organizational dysfunction that the customer files away and remembers when they evaluate renewal.

The fourth is **scope creep at transition**. The customer uses the handoff meeting to introduce new requirements — "while we are talking about the eval system, we also want to add three new use case types." The steady-state team, eager to build a positive relationship, accepts these requirements without realizing they are out of scope for the handoff and should be handled as a separate engagement. The new requirements absorb the shadow period's capacity, preventing the steady-state team from building competence on the existing eval system. They end up partially understanding the old system and partially implementing the new requirements, doing neither well.

The fifth is **the re-ask problem**. The steady-state team asks the customer questions that were already answered during onboarding. "What does your team mean by 'accuracy' in this context?" The customer answered that question in the discovery workshop. They answered it again during calibration. They expected the answer to be documented and transferred. When the steady-state team asks a third time, the customer does not just lose patience. They lose confidence that the platform is professionally managed. Every re-asked question is a small withdrawal from the trust account that onboarding built.

## Metrics for Handoff Quality

Two metrics tell you whether your handoff process is working.

The first is **customer satisfaction at day 30 post-handoff**. Send a brief survey — three questions maximum — to the customer's primary contact 30 days after the steady-state team takes over. Ask whether the transition was smooth, whether the new team demonstrated adequate knowledge of their account, and whether they have had to repeat information that was covered during onboarding. Track the score over time and across customers. If satisfaction drops below 7 out of 10 for more than 20 percent of handoffs, the process has a structural problem. Compare satisfaction at day 30 post-handoff to satisfaction at the end of onboarding. If there is a consistent drop of more than 2 points, the handoff is destroying value that onboarding created.

The second is **escalation rate in the first 60 days post-handoff**. Track the number and nature of escalations from the steady-state team to the onboarding lead (during the shadow period) and to senior support or customer success leadership (after the shadow period ends). Healthy handoffs produce two to four escalations during the shadow period and near-zero after. Unhealthy handoffs produce ten or more escalations during shadow and a continuing stream after. The escalation rate is a proxy for knowledge transfer completeness — each escalation is a question the steady-state team cannot answer from the handoff document and meeting, which means the handoff missed it.

Combine these two metrics into a **Handoff Health Score** that you review monthly. Segment by onboarding lead, by steady-state team, by customer tier, and by industry vertical. Patterns will emerge. You may discover that one onboarding lead consistently produces excellent handoff documents and their customers transition smoothly, while another consistently skips the calibration record section and their customers experience re-ask problems within weeks. You may discover that healthcare customers require longer shadow periods than fintech customers because regulatory nuance is harder to transfer. These patterns feed back into process improvements that raise handoff quality across the platform.

The organizational patterns described in Chapter 8 provide the broader framework for how onboarding and steady-state teams should be structured, governed, and measured. But no organizational design compensates for a bad handoff. Get the transition right, and the organizational structure supports a customer who trusts the platform. Get the transition wrong, and the organizational structure manages a customer who is already looking for the exit.

The next subchapter addresses a different onboarding challenge entirely — what happens when the customer arrives with no data, no reference examples, and no domain experts available to label, and you must bootstrap ground truth synthetically.
