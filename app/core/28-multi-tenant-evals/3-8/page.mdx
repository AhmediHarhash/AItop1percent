# 28.8 — Testing Isolation: How to Verify That Tenant Boundaries Actually Hold

In early 2026, a multi-tenant document intelligence platform deployed what they believed was a fully isolated eval pipeline. Data was stored in per-tenant buckets. Judge calls used per-tenant API keys. Cache keys included tenant identifiers. The engineering team had built isolation correctly — or so they thought. During a routine security exercise, the red team planted a **canary token** — a distinctive, unmistakable string that could not appear naturally — in Tenant A's golden set. The canary was a synthetic evaluation example containing the phrase "ZEBRA-TANGO-4491-ISOLATION-CHECK" embedded in a fictional customer support transcript. Three days later, that exact string appeared in the evaluation results for Tenant B. Not in Tenant B's scores, not in Tenant B's reports, but in the few-shot examples assembled for Tenant B's judge prompt. A shared few-shot pool that was supposed to have been deprecated six months earlier was still being used as a fallback when the per-tenant pool had fewer than three examples. The fallback path had never been tested. The canary found it.

That discovery — made through deliberate testing, not through a customer complaint or a regulatory audit — saved the platform from a contamination incident that could have affected dozens of tenants for months. The lesson is unambiguous: you cannot trust isolation you have not tested. Isolation that exists only in architecture diagrams and code review checklists is isolation you hope works. Isolation that has been verified through active testing is isolation you know works. The difference between hope and knowledge is the difference between surviving an audit and failing one.

## Why Passive Assurance Is Not Enough

Most teams rely on passive assurance for tenant isolation. They review the code. They verify that tenant IDs are included in data paths and cache keys. They check that access controls are configured correctly. They deploy and move on, trusting that the isolation they designed is the isolation they have.

Passive assurance fails for three reasons. First, code changes over time. The engineer who builds the isolation layer understands the constraints. The engineer who adds a caching optimization six months later may not. A new code path that bypasses the tenant-scoped cache — perhaps for performance, perhaps by accident — creates a cross-tenant vector that no one detects because no one is actively looking. Code review catches some of these regressions. It does not catch all of them, especially in large codebases where the eval pipeline spans multiple services.

Second, infrastructure changes independently of code. A cloud provider updates their caching service's default behavior. A Kubernetes namespace reconfiguration changes how pods share storage volumes. A load balancer update routes requests to a different pool. These infrastructure changes can break tenant isolation without any code change occurring. Passive assurance based on code review cannot detect infrastructure-level isolation failures.

Third, the composition of individually correct components can produce incorrect behavior. The prompt assembler correctly scopes its few-shot selection to the current tenant. The judge invoker correctly opens a fresh session per tenant. The cache correctly partitions by tenant ID. But the orchestration layer that sequences these components passes the tenant context through an environment variable that gets overwritten when two eval runs execute concurrently on the same worker. Each component is correct in isolation. The composed system leaks. This class of bug is nearly impossible to catch through code review and trivially easy to catch through active testing.

## Canary Tokens: Your Primary Isolation Verification Tool

A **canary token** in the evaluation context is a synthetic, uniquely identifiable data element that you inject into one tenant's evaluation environment and then monitor for its appearance in any other tenant's environment. If the canary shows up where it should not, you have a confirmed isolation breach. If it does not, you have evidence — not proof, but evidence — that isolation held for the path the canary traveled.

Effective canary tokens share several properties. They are **unique** — each canary contains a string or pattern that cannot be generated by any natural process or appear in any real tenant data. Random UUIDs work. Structured identifiers like "CANARY-TENANT-A-20260301-GOLDEN-SET-7" work better because they encode context that helps you trace the leak path when a canary surfaces. They are **inert** — they do not affect model behavior, judge scoring, or aggregate metrics in any meaningful way. A canary that skews a tenant's quality scores defeats the purpose. They are **detectable** — you can search for them efficiently across all tenant environments, eval logs, cache entries, judge prompts, and evaluation results. If you cannot search for the canary, you cannot detect the leak.

The deployment pattern for canary tokens in multi-tenant eval is straightforward. Create a synthetic evaluation example — an input, a model output, and an expected score — for each tenant. Embed a unique canary identifier in the input and the output. Add this example to the tenant's golden set or few-shot pool. Then run a detection sweep across all other tenants' environments: their golden sets, their few-shot pools, their cache entries, their judge prompt logs, their evaluation result stores. Any canary that appears outside its originating tenant is a confirmed breach.

Run canary detection on a regular schedule — weekly at minimum, daily if your eval pipeline changes frequently. Run it after every significant pipeline change: new caching layers, new prompt assembly logic, new orchestration patterns, new infrastructure deployments. The canary test adds negligible overhead — a handful of synthetic records per tenant and a search query across tenant environments — but it provides a concrete, verifiable signal that isolation held or broke during the testing window.

## Synthetic Test Tenants

Canary tokens verify that real tenant data does not leak between real tenants. **Synthetic test tenants** verify that the isolation mechanisms themselves work correctly, independent of any real customer data.

A synthetic test tenant is a fully configured, non-production tenant that exists solely for isolation testing. It has its own eval configuration, its own golden set, its own few-shot pool, its own quality rubric, its own cache namespace, and its own audit trail. It looks like a real tenant to every component in the eval pipeline. The difference is that its data is synthetic, its configuration is designed for testability rather than real-world evaluation, and its results are consumed by your testing infrastructure, not by any customer or dashboard.

You need at least two synthetic test tenants — call them Tenant Alpha and Tenant Beta. Tenant Alpha and Tenant Beta are configured with maximally different characteristics to make cross-contamination easy to detect. Alpha's golden set contains examples in a distinctive domain — say, veterinary medicine — with a unique vocabulary. Beta's golden set contains examples in a completely different domain — say, aerospace engineering. If a veterinary term appears in Beta's judge prompt, or an aerospace term appears in Alpha's evaluation results, the contamination is unmistakable.

The testing protocol runs every isolation verification path. First, trigger a full eval run for Alpha, then immediately trigger a full eval run for Beta on the same compute infrastructure. Check that Beta's judge prompts contain no Alpha content and vice versa. Second, run Alpha and Beta evaluations concurrently on shared workers. This tests the concurrent execution path that is most likely to produce context leakage through shared environment variables, thread-local storage, or worker state. Third, populate Alpha's cache with results, then run Beta's evaluation. Check that Beta never receives Alpha's cached scores. Fourth, run Alpha's evaluation, then query Beta's audit trail. Check that no Alpha records appear in Beta's audit scope.

Synthetic test tenants should be part of your continuous integration pipeline. Every deployment to the eval system runs the cross-tenant isolation test suite using Alpha and Beta. If any test fails, the deployment is blocked. This converts isolation from a property you believe in to a property you verify on every release.

## Isolation Fuzzing

Canary tokens and synthetic tenants test the paths you expect. **Isolation fuzzing** tests the paths you did not think of. It is the evaluation equivalent of fuzz testing in software security — throwing unexpected, boundary-pushing inputs at the system to find failures that structured tests miss.

Isolation fuzzing for multi-tenant eval works by deliberately stressing the assumptions your isolation mechanisms rely on. It asks questions like: What happens when two tenants have identical input data but different rubrics? Does the system correctly evaluate each under their own rubric, or does a shared-data optimization collapse them into a single evaluation? What happens when a tenant ID contains special characters — dashes, underscores, unicode — that might break namespace partitioning in the cache or the file system? What happens when a tenant's eval run is interrupted mid-execution and restarted — does the restart correctly restore the tenant context, or does it pick up the context of whatever tenant ran last? What happens when a tenant is deleted and their tenant ID is reassigned to a new customer — do any residual records from the old tenant surface in the new tenant's environment?

Each of these scenarios targets a specific class of isolation failure. The identical-input scenario tests whether the system shortcuts evaluation when it detects duplicate data across tenants. The special-character scenario tests whether namespace partitioning is robust against injection attacks. The interrupt-restart scenario tests whether tenant context survives process failures. The tenant-reuse scenario tests whether deletion is complete.

You do not need to run isolation fuzzing continuously. A monthly fuzzing campaign, or a campaign triggered by significant pipeline changes, provides sufficient coverage. The goal is not to test every possible input but to exercise the boundary conditions that deterministic tests miss. Each fuzzing campaign should produce a report of tested scenarios, findings, and remediations. Over time, the scenarios that revealed failures are converted into deterministic tests and added to the CI suite.

## Cross-Tenant Access Testing

Access controls enforce who can see what. **Cross-tenant access testing** verifies that those controls actually work by deliberately attempting unauthorized access and confirming that the attempt fails.

The pattern is simple: authenticate as Tenant A and attempt to access Tenant B's evaluation data. Try to read Tenant B's golden set. Try to query Tenant B's audit trail. Try to retrieve Tenant B's evaluation scores. Try to access Tenant B's evidence packages. Every attempt should fail with an access denied error. Any attempt that succeeds is a confirmed vulnerability.

Cross-tenant access testing should be automated and run regularly. Build a test suite that iterates through every API endpoint, every data access path, and every reporting interface in your eval system. For each endpoint, authenticate as a test tenant and attempt to access resources belonging to a different test tenant. Record the result. Alert on any success.

The critical detail is testing both explicit and implicit access paths. Explicit paths are the obvious ones — API endpoints that accept a tenant ID parameter. Can you pass a different tenant's ID and get their data? Implicit paths are subtler. An endpoint that returns evaluation results might not accept a tenant ID parameter because it infers the tenant from the authentication context. But does it always infer correctly? If a service-to-service call passes a tenant ID in a header, and the receiving service trusts that header without validating it against the authenticated identity, an internal caller could spoof the tenant context. Cross-tenant access testing must cover these service-to-service paths, not just the external API surface.

## Penetration Testing for Eval Systems

Cross-tenant access testing verifies that known access paths enforce isolation. **Penetration testing** goes further — it asks whether an adversary could discover and exploit access paths that your testing did not cover.

Eval systems are not typically targets for external attackers. But in a multi-tenant platform, tenants themselves are potential adversaries. A tenant might attempt to access competitor data through the eval system. A tenant's administrator might probe the platform's API to find data belonging to other tenants. A disgruntled employee at one tenant might use their platform access to extract information about another tenant's quality performance or proprietary rubric criteria.

Penetration testing for eval systems focuses on three attack surfaces. The first is the eval API — can a tenant craft API requests that bypass tenant scoping and return data from other tenants? This includes parameter manipulation, header injection, and authentication token reuse attacks. The second is the data layer — can a tenant access the underlying storage where eval data resides, bypassing the API entirely? This tests storage-level access controls, encryption at rest, and network segmentation. The third is the reporting layer — can a tenant manipulate report generation to include data from other tenants? This tests whether the evidence package generator correctly scopes its data queries.

Annual penetration testing by an external security firm is the standard for platforms serving regulated enterprises. Many enterprise customers — particularly in financial services and healthcare — require evidence of regular penetration testing as a condition of their contract. The pen test report becomes part of your sales collateral, demonstrating to prospects that your isolation has been verified by an independent party, not just by your own engineering team.

## Building an Isolation Verification Schedule

Isolation testing is not a one-time project. It is an ongoing operational practice that must be scheduled, tracked, and reported on, just like uptime monitoring or security patching.

A mature isolation verification schedule includes five layers. First, **continuous canary monitoring** — canary tokens active in every tenant environment, with automated detection sweeps running daily. Second, **CI-integrated isolation tests** — synthetic test tenant Alpha-Beta tests running on every eval pipeline deployment. Third, **weekly cross-tenant access testing** — automated access tests covering every API endpoint and data path. Fourth, **monthly isolation fuzzing** — boundary-condition campaigns targeting pipeline assumptions. Fifth, **annual penetration testing** — external security assessment of the eval system's isolation boundaries.

Each layer catches different classes of isolation failure. Continuous canaries catch data leakage through operational paths. CI tests catch isolation regressions introduced by code changes. Weekly access tests catch access control misconfigurations. Monthly fuzzing catches unexpected interaction patterns. Annual pen tests catch systemic vulnerabilities that internal testing cannot see.

The results of isolation testing must be tracked and reported. For regulated customers, your ability to demonstrate a continuous isolation verification program — not just "we tested it once" but "we test it every day and here are the results" — is a significant compliance differentiator. Include isolation test results in the evidence packages described in the previous subchapter. When a customer asks "how do you know our data is isolated?", your answer is not "we designed it that way." Your answer is "we verify it continuously, and here is the evidence."

## The Cultural Shift

The hardest part of isolation testing is not technical. It is cultural. Engineers who built the isolation system resist testing it because testing implies distrust of their work. Product managers resist the overhead of synthetic test tenants and canary infrastructure because it is invisible to customers. Leadership questions the cost of annual penetration testing because "we have not had an incident."

The response to all three objections is the same. You have not had an incident that you know about. The purpose of isolation testing is to discover incidents before customers do, before regulators do, and before a contract dispute does. The cost of a canary token infrastructure is a few engineering days. The cost of a cross-tenant data breach at a platform serving healthcare and financial services customers is measured in regulatory fines, contract terminations, and litigation. These numbers are not close.

Build the testing. Run it continuously. Report the results. Make isolation verification as routine and non-negotiable as unit testing. The teams that do this sleep better — and their customers renew.

The tenant metadata contract — the set of fields that every component in your eval system must carry, propagate, and never drop — is what makes all of this isolation infrastructure function correctly. The next subchapter defines that contract and explains why losing a single field in transit can collapse every isolation guarantee you have built.
