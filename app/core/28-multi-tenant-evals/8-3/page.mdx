# 28.75 — Customer Lifecycle Events: Onboarding, Expansion, Downtier, Migration, and Churn

Every customer lifecycle event requires an eval system response. This is not a metaphor. It is an operational requirement as concrete as provisioning compute or updating a billing record. When a customer signs a contract, the eval system must bootstrap their quality configuration. When a customer adds a new use case, the eval system must expand coverage. When a customer reduces spend, the eval system must adjust its investment without abandoning monitoring. When a customer migrates to a new model or configuration, the eval system must re-establish baselines. When a customer leaves, the eval system must clean up, archive, and tear down. Most platforms handle the first event — onboarding — with a defined process (Chapter 5 covers this in depth). They handle the remaining four events with improvisation, silence, or nothing at all.

The cost of that gap is not theoretical. A customer who expands into three new languages without eval coverage for those languages discovers quality problems their own users report — not the platform. A customer who downtiers from enterprise to growth continues consuming tier-1 eval resources for months because nobody adjusted the configuration. A customer who migrates from one model to another loses their quality baseline and spends a quarter wondering whether quality improved or degraded. A customer who churns leaves behind orphaned golden sets, zombie eval jobs, and configuration data that clutters the system until someone notices it during a capacity audit eight months later.

## Onboarding: The One Event Platforms Get Right

Onboarding is the most mature lifecycle response in multi-tenant evaluation, largely because it is the most visible. Sales signed the deal. The customer is watching. The clock is ticking. Chapter 5 details the full Onboarding Ramp — discovery, golden set bootstrapping, rubric calibration, baseline sprint, handoff to steady-state — so this subchapter will not repeat that material. What matters here is the contrast: onboarding gets a defined process, a dedicated team, tracked metrics, and executive visibility. Every other lifecycle event gets informal handoffs and hope.

The reason is structural. Onboarding has a clear trigger (contract signature), a clear owner (the onboarding team), a clear deliverable (the first quality report), and a clear deadline (typically 30 to 90 days depending on customer tier). Other lifecycle events have ambiguous triggers, no clear owner, no defined deliverable, and no deadline. Building eval responses for expansion, downtier, migration, and churn means creating the same organizational infrastructure that onboarding already has — triggers, owners, deliverables, timelines — for events that nobody has formalized.

## Expansion: When the Customer Outgrows the Original Eval Scope

**Expansion** means the customer adds new use cases, new languages, new document types, new user segments, or new geographies to their platform usage. Each expansion widens the quality surface that the eval system must cover. The eval configuration that was comprehensive when the customer processed English-language contract summaries becomes dangerously incomplete when the customer adds French and German regulatory filings.

The eval system response to expansion has four requirements. First, identify the new quality surface. If the customer adds three new languages, the eval team needs to understand what quality means in each language, whether the existing rubric dimensions apply (they usually do not map cleanly — Section 23 on multilingual evaluation explains why), and what the customer's quality expectations are for the new surface versus the original one. Second, extend the golden set. The customer's existing golden set does not cover the new use cases. New golden set examples must be curated, validated by domain experts, and integrated into the existing set without diluting the coverage of the original use cases. Third, recalibrate or extend the rubric. New use cases may require new quality dimensions, different dimension weights, or different threshold settings. A customer whose original use case was summarization and whose expansion use case is question-answering needs rubric adjustments because the quality dimensions have different relative importance. Fourth, re-establish the baseline. The customer's quality baseline was set for their original scope. Expansion changes the denominator. The composite quality score after expansion may drop simply because the new use cases start at lower quality — not because the original use cases degraded. The customer needs to understand this baseline reset or they will misinterpret the score change as regression.

The typical timeline for an expansion eval response is two to four weeks for a single new use case or language, four to eight weeks for a major expansion involving multiple new surfaces. The owner is the federated customer team (the CSM or embedded eval specialist), with the central eval team providing tooling and methodology guidance.

What goes wrong when expansion has no eval response: the customer's quality score becomes an average of well-covered and poorly-covered use cases. The well-covered original use case masks quality problems in the new use case. The customer's end users in the new segment report issues that the eval system does not detect. The customer loses trust in the platform's quality measurement because the numbers do not match their experience — the same CS-eval alignment failure described in the previous subchapter, except this time the root cause is clear: the eval system was never updated to cover the expansion.

## Downtier: Reducing Eval Investment Without Abandoning Quality

**Downtier** means the customer reduces their contract scope, moves to a lower pricing tier, or cuts spend. The business reality is straightforward: the customer is paying less, and the platform's eval investment must adjust accordingly. But the eval response to downtier is the most organizationally awkward lifecycle event because nobody wants to tell a customer that their quality monitoring is being reduced.

The eval system response has three requirements. First, reclassify the customer's eval tier. A customer moving from a $300,000 enterprise contract to a $100,000 mid-market contract should move from tier-1 eval (continuous monitoring, dedicated golden sets, human-in-the-loop review) to tier-2 eval (daily sampling, shared judge infrastructure, automated reporting). Chapter 7's coverage of tiered eval economics defines what each tier includes. Second, adjust the eval configuration. Reduce the sampling rate, switch from dedicated to shared judge models, remove human review triggers that the new tier does not include, and adjust alerting thresholds to the tier-2 standard. Third, preserve the quality floor. Even a downtier customer deserves evaluation that catches catastrophic failures. The minimum viable eval — a weekly automated sample with platform-default rubrics — should remain active for every customer regardless of tier.

The timeline for a downtier eval response should be two to three weeks from the contract change effective date. The owner is the federated customer team, with sign-off from the central eval team to ensure the new configuration meets tier-appropriate standards.

What goes wrong when downtier has no eval response: the customer continues consuming tier-1 eval resources at tier-2 pricing. Over twelve months, a platform with thirty customers who downtier without eval adjustment accumulates $120,000 to $200,000 in misallocated eval spend — running expensive evaluation for customers whose contract value no longer justifies it. The reverse is equally damaging: if the platform reduces eval investment too aggressively during downtier, the customer experiences a sudden quality visibility gap that accelerates the churn trajectory the downtier may already signal.

## Migration: Parallel Evaluation and Baseline Re-Establishment

**Migration** means the customer moves to a different model, a different prompt configuration, a different retrieval pipeline, or a different version of the platform. Migration is the lifecycle event where eval matters most and is most often neglected, because the urgency of the migration itself consumes all available attention.

The eval system response has four requirements. First, run parallel evaluation. Before the migration cutover, run the customer's eval pipeline against both the old and new configurations simultaneously. Score the same inputs through both paths and compare results dimension by dimension. Parallel evaluation reveals regressions the migration introduces before the customer experiences them. A customer migrating from GPT-5 to Claude Opus 4.6 may see improvements in reasoning quality but regressions in formatting consistency — and parallel eval catches this before cutover rather than after.

Second, re-establish the baseline. The customer's historical quality scores were earned under the old configuration. Those scores are not directly comparable to scores under the new configuration. The migration requires a new baseline sprint — a period of evaluation against the new configuration that establishes the starting point for future trend analysis. Without this baseline reset, the customer's quality dashboard shows a discontinuity at the migration date that confuses every subsequent analysis.

Third, validate the golden set. The customer's golden set was built against the old configuration's characteristics. If the migration changes the model, the expected output characteristics may shift — longer outputs, different formatting conventions, different vocabulary preferences. The golden set may need examples refreshed to reflect the new configuration's output profile while maintaining the same quality standards.

Fourth, communicate the transition clearly. The customer's quality report should explicitly mark the migration boundary. Reports before the boundary use the old baseline. Reports after the boundary use the new baseline. The two periods should not be naively compared. The migration report — a one-time document showing the parallel eval results, the dimension-by-dimension comparison, and the new baseline — gives the customer confidence that the migration was evaluated rigorously.

The typical timeline is two to four weeks of parallel evaluation before cutover, one to two weeks of baseline establishment after cutover. The owner depends on who initiated the migration: platform-initiated migrations (model upgrades, infrastructure changes) are owned by the central eval team; customer-initiated migrations (changing configurations, switching use cases) are owned by the federated customer team.

What goes wrong when migration has no eval response: the customer's quality trends show a mysterious shift at the migration date that nobody can explain six months later. The customer's golden set silently becomes less representative of current outputs, and eval accuracy degrades gradually. The customer asks, "Are we better or worse after the migration?" and nobody can answer with confidence.

## Churn: Data Cleanup, Archival, and Eval Teardown

**Churn** means the customer is leaving. It is the lifecycle event that receives the least eval attention because the relationship is ending, and every team's instinct is to minimize effort on a departing customer. That instinct is wrong for three reasons.

First, regulatory and contractual obligations. The customer's eval data — golden sets, quality scores, evidence packages, audit logs — may be subject to data retention or deletion requirements under their contract, under GDPR, or under industry-specific regulations like HIPAA. A healthcare customer who churns does not simply stop receiving quality reports. Their protected health information embedded in golden set examples must be identified, handled according to the data processing agreement, and either returned, deleted, or archived in a compliant manner. Failure to handle churn data correctly creates legal exposure that persists long after the revenue ends.

Second, platform hygiene. Every churned customer who is not properly torn down leaves artifacts in the eval system: orphaned golden sets consuming storage, zombie eval jobs consuming compute (eval cron jobs that nobody disabled), configuration records that clutter the tenant registry, and LLM judge prompt versions that reference a customer who no longer exists. A platform that churns fifty customers per year without proper teardown accumulates enough orphaned resources to materially affect eval pipeline performance and cost. One platform discovered during a cost audit that 12 percent of their monthly eval compute was consumed by jobs running for customers who had churned six to eighteen months earlier — a waste of roughly $4,500 per month that nobody noticed because the jobs ran silently.

Third, institutional learning. Every churned customer represents a quality relationship that ended. The eval data from that relationship — what worked, what did not, which rubric configurations produced the highest alignment scores, which golden set strategies failed — is valuable for improving the onboarding and steady-state processes for future customers. Archiving eval data before deletion preserves this learning. Deleting everything immediately destroys it.

The eval system response to churn has four steps. First, disable active eval jobs within 48 hours of churn confirmation to stop consuming compute and judge API credits. Second, execute the data handling protocol defined in the customer's contract and data processing agreement — typically a choice between data deletion with certification or data export with transfer. Third, archive eval metadata (configuration records, rubric versions, aggregate quality scores, lessons learned) in an anonymized format that preserves institutional learning without retaining customer-identifiable data. Fourth, remove the tenant from all active monitoring, dashboards, alerting, and reporting systems to prevent phantom alerts and stale data in platform-wide quality metrics.

The timeline for churn eval response is 48 hours for job disablement, 30 days for data handling and archival, and 60 days for full system cleanup. The owner is the central eval team for infrastructure teardown and the federated customer team for data handling coordination with the customer.

## Building the Lifecycle Response Matrix

The pattern across all five events is the same: define the trigger, assign the owner, specify the eval actions, set the timeline, and track completion. The **Lifecycle Response Matrix** is a single document that maps each lifecycle event to its eval requirements. It lives in your eval operations runbook and is reviewed quarterly.

The matrix forces organizational clarity. When you write down that the expansion eval response takes two to four weeks and requires golden set extension, rubric recalibration, and baseline re-establishment, it becomes obvious that someone needs to own that work. When you write down that churn requires data handling within 30 days, it becomes obvious that the process must be triggered automatically when the churn is confirmed in your billing system rather than depending on someone remembering to file a ticket.

The most mature platforms in 2026 automate the lifecycle trigger detection. When the billing system records an expansion (new SKUs, new usage tiers), the eval system automatically creates an expansion response task with the appropriate owner, timeline, and checklist. When a migration is scheduled in the deployment system, the eval system automatically spins up parallel evaluation infrastructure. When a churn is confirmed, the eval system automatically disables active jobs and initiates the data handling workflow. The human still does the judgment work — curating golden sets, recalibrating rubrics, making quality decisions — but the system ensures no lifecycle event passes without an eval response.

## Why Onboarding Alone Is Not Enough

The platforms that invest heavily in onboarding but neglect other lifecycle events create a predictable failure pattern. The customer's first six months are excellent: the eval system is calibrated, the golden set is fresh, the baseline is recent, and the quality reports match the customer's experience. Then the customer expands into new use cases, and eval coverage does not follow. Quality scores begin drifting from customer perception. The CS team does not understand why the previously reliable quality reports suddenly feel disconnected. By month twelve, the customer's trust in the eval system — trust that was carefully built during onboarding — has eroded because the eval system stopped adapting to the customer's evolving reality.

The Lifecycle Response Matrix prevents this erosion by treating every lifecycle event with the same operational rigor that onboarding receives. Onboarding builds trust. The lifecycle response sustains it.

Platform evolution creates its own category of lifecycle disruption — when the platform itself changes and customers cannot all move at the same speed. The next subchapter examines versioning and migration strategies for the platform side of the equation, where the eval system must simultaneously support customers on different versions of the same product.
