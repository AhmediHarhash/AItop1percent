# 28.4 — Tenant-Scoped Golden Sets: Building, Versioning, and Protecting Per-Customer Ground Truth

A golden set is the most honest artifact in your evaluation system. It is a curated collection of inputs and their known-correct outputs that serves as the reference standard against which all model performance is measured. In a single-tenant product, you maintain one golden set. In a multi-tenant platform, you maintain one for every customer — and each one must be treated as a first-class versioned artifact with the same rigor you apply to production code. Anything less, and your per-customer evaluation is built on sand.

The golden set is where abstract quality definitions become concrete. The Quality Contract says the customer cares about clinical accuracy. The Tenant Eval Fingerprint encodes clinical accuracy as a weighted dimension with a threshold of 82 percent. But neither of those artifacts answers the question: what does clinical accuracy actually look like for this customer's specific data? The golden set answers that question with examples. It contains the customer's actual inputs paired with outputs that the customer — not the platform, not the model, not the eval team — has confirmed as correct. Those confirmed examples are ground truth, and without them, every score your evaluation pipeline produces is a number without an anchor.

## Why Per-Customer Golden Sets Are Table Stakes in 2026

Two years ago, many multi-tenant platforms operated with a single platform-wide golden set and assumed it was representative enough to evaluate all customers. That approach was always fragile, but the market tolerated it because multi-tenant AI evaluation was a young discipline and enterprise buyers did not know to ask for better. That tolerance is gone. By 2026, enterprise procurement teams routinely ask three questions during vendor evaluation: "Do you maintain a golden set specific to our use case? How is it versioned? How do you prevent our ground truth data from being accessible to other tenants?" Platforms that cannot answer all three lose deals.

The reason is straightforward. Enterprise customers learned — often painfully — that platform-wide golden sets do not catch customer-specific regressions. A golden set built from a cross-section of all customers' data will be dominated by the most common use cases and underrepresent any individual customer's specific patterns. A healthcare customer whose medical terminology represents 0.4 percent of the platform-wide golden set will not see meaningful regression detection for their use case from that set. Their quality could collapse, and the platform-wide golden set evaluation would show green.

Per-customer golden sets solve this by ensuring that each customer's evaluation is grounded in their own data. When you evaluate model performance for tenant 247, you evaluate against tenant 247's golden set — examples drawn from tenant 247's actual inputs, approved by tenant 247's domain experts. A regression in how the model handles German insurance terminology shows up immediately because the golden set contains German insurance examples that the customer has confirmed as correct. The signal is not diluted by 339 other customers' examples. It is pure, specific, and actionable.

## Building the Initial Golden Set During Onboarding

The golden set construction process begins during customer onboarding, runs in parallel with Quality Contract negotiation and fingerprint creation, and must be completed before the customer's evaluation enters production. A golden set built after the customer is already live creates a dangerous gap — the customer is receiving production outputs with no ground truth against which to measure them. Any regression during that gap is invisible.

The raw material for the initial golden set comes from two sources. The first is **customer-provided examples** — inputs and outputs that the customer brings to onboarding, drawn from their existing workflows. A legal customer might provide 200 contract clauses with their expected extracted fields. A logistics customer might provide 150 shipment records with correctly parsed tracking data. These customer-provided examples are gold because they encode the customer's actual quality expectations in the most concrete form possible. They are not your team's interpretation of what the customer wants. They are the customer's own demonstration of it.

The second source is **platform-generated examples validated by the customer**. During the pilot period — typically two to four weeks — the platform generates outputs for the customer's actual inputs, and the customer's domain experts review a curated sample, marking each output as correct, partially correct, or incorrect. For correct outputs, the input-output pair enters the golden set directly. For partially correct outputs, the customer provides the corrected version, and the input paired with the corrected output enters the golden set. Incorrect outputs are valuable for a different reason — they establish the boundary of what the model gets wrong, which informs the golden set's coverage of failure modes.

The target size for an initial golden set depends on the customer's use case complexity. A customer with a narrow, well-defined use case — extracting five specific fields from a standardized document format — might need 100 to 150 examples for adequate coverage. A customer with a broad use case — generating diverse content across multiple domains and formats — might need 400 to 600 examples to cover the quality surface area. The guiding principle is coverage over volume. A golden set of 80 examples that covers all of the customer's input types, edge cases, and failure modes is more valuable than a golden set of 500 examples drawn entirely from the customer's most common input pattern.

## Coverage as a Design Discipline

Designing a golden set is not random sampling. It is deliberate coverage planning. The golden set must contain examples across every dimension of variation that matters for the customer's evaluation: different input types, different languages if applicable, different complexity levels, different edge cases, and — critically — examples that probe known failure modes.

The coverage plan starts with the customer's input taxonomy. If the customer sends four types of documents — contracts, invoices, correspondence, and regulatory filings — the golden set must contain examples of all four types, with representation proportional to their importance, not their frequency. Contracts might represent 60 percent of the customer's volume but 90 percent of their business-critical outputs. The golden set should over-represent contracts relative to their volume because the cost of a regression on contracts is far higher than a regression on routine correspondence.

Edge cases deserve explicit inclusion. Every domain has inputs that sit at the boundary of what the model handles well — unusually long documents, documents with mixed languages, inputs with ambiguous structure, documents that reference other documents not included in the context. These edge cases are precisely where regressions tend to appear first, because model updates often optimize for common patterns at the expense of unusual ones. A golden set that contains only typical examples will miss the early signs of degradation on the inputs that matter most.

Adversarial examples also belong in the golden set, particularly for customers in regulated industries. These are inputs designed to probe specific failure modes — a medical document that uses a drug name that closely resembles another drug's name, a financial document with a percentage that could be misinterpreted depending on whether it refers to a rate or a change, a legal document where the relevant clause is buried in an appendix rather than the main text. Adversarial examples test whether the model handles the kinds of ambiguity that cause real-world failures.

## Versioning as a First-Class Requirement

A golden set that is not versioned is a golden set you cannot trust. Without versioning, you cannot answer the most basic diagnostic question when scores change: did quality actually shift, or did the golden set change?

Version control for golden sets follows the same principles as version control for code, but with additional constraints specific to evaluation data. Each version is an immutable snapshot. Version 1.0 is the onboarding golden set. When examples are added, removed, or corrected, a new version is created — 1.1, 1.2, or 2.0 depending on the magnitude of the change. Adding five examples to cover a new document type might be version 1.1. Correcting 30 examples because the customer's quality standards changed is version 2.0. The distinction matters because evaluation trend analysis must account for version changes. A quality score that drops from 88 to 82 at the same time the golden set was upgraded from version 1.3 to 2.0 is a different signal than a quality score that drops from 88 to 82 while the golden set remains at version 1.3.

Every evaluation result must record the golden set version it was computed against. This is not optional metadata — it is a required field in the evaluation result schema. Without it, historical comparisons are meaningless. The question "has quality improved over the last six months" can only be answered if you know which golden set version was used for each evaluation in that period. A quality improvement that coincides with a golden set simplification — removing the hardest examples — is not an improvement. It is an illusion.

The versioning system must also support rollback. If a golden set update introduces examples that the evaluation team later determines were incorrectly labeled — the customer's domain expert made an error, or the corrected output was based on a misunderstanding — the platform must be able to revert to the previous version and re-run evaluations to determine the true quality trajectory. Rollback without data loss is the safety net that makes golden set evolution possible without the fear that a bad update will corrupt historical trends.

## Protecting Golden Sets From Contamination

Contamination is the silent killer of golden set integrity. A contaminated golden set produces evaluation scores that look good but mean nothing — the model appears to perform well not because it has genuine capability but because evaluation data has leaked into training data, inference context, or the model's cached responses.

The most common contamination vector in multi-tenant platforms is accidental inclusion of golden set examples in fine-tuning or prompt-optimization datasets. If the platform uses customer outputs to improve the model — even through indirect methods like prompt engineering based on failure patterns — and golden set examples are included in those outputs, the model may learn to produce correct answers on the golden set while failing on novel inputs. The eval scores go up. The customer's actual experience does not change. The gap between eval scores and reality widens until a customer complaint forces an investigation.

Preventing this contamination requires structural separation, not just policy. Golden set examples must be tagged in the data layer with a flag that excludes them from any training, fine-tuning, or optimization pipeline. This flag must be enforced at the data infrastructure level — a query that pulls training data must structurally exclude tagged examples, not rely on the querying engineer to remember the exclusion. In platforms that use Section 12's dataset engineering pipelines, the golden set exclusion should be a hard constraint in the pipeline configuration, not a soft guideline in a runbook.

The second contamination vector is temporal leakage. If the golden set contains examples drawn from production outputs that were generated after a model update, evaluating that model update against those examples is circular — you are testing the model's ability to reproduce its own outputs, which tells you nothing about quality. Golden set examples must have clear provenance: when was the input generated, when was the output reviewed, and which model version produced it. Examples generated by the model being evaluated must never be included in the golden set used to evaluate it.

The third contamination vector is cross-tenant leakage. In a multi-tenant platform, tenant A's golden set must never be accessible to processes operating on tenant B's data. This is not just a privacy concern — it is an evaluation integrity concern. If golden set examples from one tenant leak into another tenant's evaluation context, scores for both tenants become unreliable. The isolation requirements described in Section 28.2 apply with full force to golden sets. Per-tenant encryption, access controls, and audit logging are not optional for golden set storage. They are baseline requirements.

## Drift Management: When the Golden Set Ages Out

Golden sets decay. The customer's data patterns change over time. New document types appear. Regulatory language evolves. Product catalogs expand. The golden set that provided comprehensive coverage twelve months ago may now be missing entire categories of inputs that represent 15 percent of the customer's current volume. Evaluating against a stale golden set is only slightly better than not evaluating at all — it catches regressions on historical patterns but is blind to regressions on current patterns.

Drift detection requires monitoring the gap between the golden set's input distribution and the customer's actual input distribution. If the golden set contains zero examples of a document type that now represents 8 percent of the customer's traffic, that is a coverage gap that must be filled. Automated drift detection compares the metadata distribution of golden set examples — document types, languages, complexity levels, input lengths — against the metadata distribution of recent production inputs. When the distributions diverge beyond a configured threshold, the system flags the golden set for review and provides a report of what is missing.

The refresh cadence depends on the customer's rate of change. A customer with a stable, narrow use case might need golden set updates once or twice per year. A customer in a fast-moving domain — a fintech customer whose product catalog changes quarterly, a media customer launching new content formats regularly — might need monthly updates. The fingerprint should include a recommended refresh cadence, and the platform should automate reminders to customer success teams when a golden set has not been reviewed within its recommended window.

Refreshing a golden set does not mean replacing it. It means augmenting it with new examples that cover emerging patterns while retaining examples that cover persistent patterns. A golden set that is entirely replaced on refresh loses its value as a regression detection tool — if every example is new, you have no baseline against which to measure whether the model's performance on established patterns has changed. The recommended approach is additive: new examples are added to cover gaps, outdated examples are retired to an archive, and the core examples that represent persistent patterns remain across versions. Over time, a well-maintained golden set becomes a living archive of the customer's quality history — every version capturing what "correct" meant at a specific point in the relationship.

## The Organizational Cost of Per-Customer Golden Sets

Maintaining per-customer golden sets for 500 tenants is expensive. It requires dedicated processes for creation, validation, versioning, drift monitoring, and refresh. It requires customer success teams to facilitate periodic reviews with customers. It requires evaluation engineers to build and maintain the infrastructure for versioned storage, contamination prevention, and drift detection. It requires data operations support for format validation, deduplication, and quality auditing.

The alternative — not maintaining per-customer golden sets — is more expensive. A platform without customer-specific ground truth cannot detect customer-specific regressions until the customer reports them. As Section 28.1 demonstrated, by the time the customer reports a quality problem, the damage to the relationship is already significant. Per-customer golden sets are the early warning system that converts silent regressions into detected incidents. The cost of maintaining 500 golden sets is the cost of not losing customers to quality failures you could have caught.

The practical approach to managing this cost is tiered investment. Tier-one customers — those paying the highest contract values, operating in the most regulated industries, or generating the most complex use cases — receive comprehensive golden sets with quarterly refresh cycles, dedicated coverage planning, and monthly drift monitoring. Tier-two customers receive solid golden sets with semi-annual refresh cycles and automated drift detection. Tier-three customers receive starter golden sets with annual refresh and drift detection only when evaluation scores fluctuate. This tiering is not about treating customers unequally. It is about allocating finite resources where the risk and value are highest.

Not every customer can articulate what belongs in their golden set, or even what "correct" looks like for their use case. The next subchapter addresses this challenge directly — what to do when customers cannot define quality but still expect you to measure it.