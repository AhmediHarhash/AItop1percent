# 28.4 — The Five Dimensions of Tenant Divergence: Quality, Risk, Data, Configuration, and Compliance

Every multi-tenant platform eventually discovers that its customers are not just different accounts using the same product. They are different worlds that happen to share the same infrastructure. The divergence between tenants is not random. It follows five predictable dimensions, and each dimension creates a distinct evaluation requirement that you must design for or suffer the consequences of ignoring. These five dimensions are quality, risk, data, configuration, and compliance. Understanding them is the difference between a platform that scales evaluation gracefully and one that collapses into per-customer chaos.

The mistake most teams make is treating tenant divergence as a single variable — "some customers are pickier than others." That framing is dangerously incomplete. A customer can have identical quality standards to another customer but wildly different risk profiles. Two customers can share the same regulatory regime but operate on data distributions so different that the same model produces excellent results for one and garbage for the other. Divergence is multidimensional. If you only track one axis, you will miss the others until they break something you cannot easily fix.

## Dimension One: Quality Divergence

**Quality divergence** is the most visible dimension and the one that most teams encounter first. It means that different tenants define "good output" differently, even when they use the same model for the same task. A contract analysis platform serves both a white-shoe law firm and a high-volume insurance company. The law firm defines quality as comprehensive, nuanced analysis that surfaces every possible risk, even if the output is long and conservative. The insurance company defines quality as fast triage that categorizes contracts into three risk buckets with ninety-five percent accuracy, and considers a lengthy analysis a defect because it slows their processing pipeline.

Same model. Same task category. Opposite quality definitions. If you evaluate both tenants against a single quality rubric, one of them always looks like it is failing. The law firm's definition penalizes the insurance company for brevity. The insurance company's definition penalizes the law firm for thoroughness. Neither customer is wrong. They are operating in different quality universes.

Quality divergence shows up along several sub-axes. Tone requirements vary — a customer service platform serving a luxury brand expects warmth, specificity, and elevated language, while the same platform serving a discount retailer expects friendly efficiency and directness. Completeness standards vary — a medical summarization tool serving a research hospital needs every relevant detail captured, while the same tool serving a primary care clinic needs only the critical findings in two sentences. Accuracy thresholds vary — a financial compliance customer might require ninety-nine percent precision on regulatory citations because a false positive triggers a costly manual review cycle, while a marketing analytics customer tolerates eighty-five percent precision because their workflow includes a human review step that catches the rest.

The evaluation implication is that you need per-tenant quality rubrics, not just per-tenant labels on a shared rubric. Your LLM-as-judge prompts, your scoring criteria, your pass-fail thresholds — all of these must be parameterized by tenant. We will explore the mechanics of building these parameterized rubrics in Chapter 2, but the architectural decision starts here: quality divergence is not an edge case to handle later. It is the default state of any multi-tenant platform with more than a handful of enterprise customers.

## Dimension Two: Risk Divergence

**Risk divergence** determines the stakes of failure for each tenant, and stakes dictate how thorough, frequent, and conservative your evaluation must be. A healthcare customer and a content marketing customer can use the same summarization model, but the consequences of a bad summary are not remotely comparable. A hallucinated claim in a patient summary could lead to a misdiagnosis. A hallucinated claim in a blog post draft leads to an edit cycle. The same error rate — say, three percent hallucination — is acceptable for one customer and potentially catastrophic for the other.

Risk divergence means your evaluation intensity must vary by tenant. High-risk tenants need more frequent evaluation runs, larger sample sizes for statistical significance, stricter pass-fail thresholds, and faster alerting when metrics drift. A financial services customer operating under SEC regulations might require that every model update passes a full regression suite against two thousand labeled examples before the new model serves a single production request to their users. A content recommendation customer might be comfortable with a rolling canary evaluation that monitors the first thousand requests and auto-promotes if quality holds.

The danger of ignoring risk divergence is that you end up applying the same evaluation rigor to all tenants. If you set the bar at the level your highest-risk customer requires, you create unsustainable evaluation costs and slow your release cycle for every customer. If you set the bar at the level your lowest-risk customer accepts, you expose your highest-risk customers to failures that could cost them regulatory fines, lawsuits, or patient harm — and cost you the contract, the reputation, and potentially legal liability. The solution is a tiered evaluation framework where risk level determines evaluation depth, sample size, and release gate strictness. Section 18 covers release gates in general; what makes multi-tenant release gates different is that the same model update must pass different gates for different tenants, simultaneously.

## Dimension Three: Data Divergence

**Data divergence** is the dimension that most directly affects model performance, and it is the one most teams underestimate. Different tenants operate on different data distributions. A customer support platform serving a consumer electronics company processes queries about warranty returns, battery life, and screen replacements. The same platform serving an enterprise software company processes queries about API rate limits, deployment configurations, and SSO integration. The vocabulary is different. The query length distribution is different. The expected answer structure is different. The frequency of ambiguous queries is different. Even the language mix is different — the electronics company might see forty percent of queries in Spanish and Portuguese, while the software company sees ninety-five percent in English.

When data distributions diverge this significantly, a model that performs well on the aggregate distribution may perform poorly on any individual tenant's distribution. This is the statistical version of the global average trap discussed in subchapter 28.3, but at the data level. Your overall accuracy might be ninety-one percent, but if the electronics company's Spanish warranty queries have seventy-two percent accuracy and the software company's API troubleshooting queries have ninety-six percent accuracy, the aggregate number is lying to everyone.

Data divergence creates a specific evaluation requirement: per-tenant evaluation datasets that reflect each tenant's actual distribution. You cannot evaluate Tenant A's quality using Tenant B's data, or using a blended sample that underweights the query types Tenant A actually sends. This means your evaluation pipeline must either maintain per-tenant golden sets — curated, labeled datasets that represent each tenant's distribution — or dynamically sample from each tenant's production traffic for ongoing evaluation. Both approaches have costs. Golden sets require maintenance as tenant distributions shift. Dynamic sampling requires careful isolation to ensure that one tenant's evaluation data never contaminates another's. Chapter 5 covers the onboarding process where you bootstrap these per-tenant evaluation datasets for new customers.

## Dimension Four: Configuration Divergence

**Configuration divergence** is the dimension that makes multi-tenant evaluation architecturally complex rather than just logistically challenging. In a mature multi-tenant AI platform, different tenants do not just run the same model with different data. They run different configurations entirely. Tenant A uses Claude Opus 4.6 with a custom system prompt tuned for legal analysis and a retrieval pipeline pulling from their proprietary document corpus. Tenant B uses GPT-5-mini with a shorter system prompt optimized for speed, pulling from a shared knowledge base. Tenant C uses a fine-tuned Llama 4 Maverick adapter trained on their historical customer interactions, served through LoRAX on a shared base model. Tenant D uses the same base model as Tenant C but with a completely different LoRA adapter and a different set of tool integrations.

Each configuration is a different system. Different models, different prompts, different retrieval sources, different tools, different adapter weights. Evaluating Tenant A's configuration with Tenant B's setup tells you nothing useful. Your evaluation pipeline must be able to reconstruct each tenant's exact configuration and run evaluations against that configuration, not against a platform default that no tenant actually uses.

Configuration divergence has a particularly insidious failure mode: configuration drift that is invisible in aggregate metrics. Suppose you update a shared retrieval index that three hundred tenants rely on. The update improves retrieval quality for most tenants but degrades it for the twelve tenants whose documents have unusual formatting. Your platform-wide retrieval precision metric goes up. Those twelve tenants' precision goes down. If your evaluation only tracks the aggregate, you celebrate the improvement while twelve customers quietly lose trust. If your evaluation tracks per-tenant configuration-aware metrics, you catch the regression before those customers file support tickets.

The practical requirement is that your evaluation system must treat tenant configuration as a first-class input. Every eval run must be tagged with the full configuration state — model version, prompt version, adapter version, retrieval index version, tool versions — so that you can attribute quality changes to specific configuration changes. This becomes critical for the release gate architecture covered in Chapter 6, where you need to answer the question: "If I update component X, which tenants are affected, and does their quality hold?"

## Dimension Five: Compliance Divergence

**Compliance divergence** is the dimension that carries the most severe consequences for getting wrong, because the failures are legal rather than technical. Different tenants operate under different regulatory regimes, and those regimes impose specific requirements on how you evaluate, log, audit, and report on AI system behavior. A healthcare customer in the United States operates under HIPAA, which restricts how patient data can be used in evaluation and requires specific audit trail capabilities. A financial services customer in the European Union operates under the EU AI Act, which as of 2026 requires documented risk assessments, human oversight mechanisms, and quality management systems for high-risk AI applications. A government customer might require FedRAMP-certified infrastructure, meaning your evaluation pipeline itself must run within approved security boundaries.

Compliance divergence means your evaluation system must enforce different rules for different tenants — not just different quality thresholds, but different operational constraints. Some tenants require that evaluation data never leaves a specific geographic region, which means your eval pipeline must support regional execution, not just regional data storage. Some tenants require that every model output used in evaluation is logged with full provenance and retained for a specific period — seven years for some financial regulations. Some tenants require that human reviewers involved in their evaluation hold specific certifications or security clearances. Section 29 covers governance and compliance in general; what multi-tenant compliance adds is the requirement to enforce different compliance policies simultaneously within the same system, without cross-contamination between tenants that operate under different regimes.

The evaluation failure mode unique to compliance divergence is not a quality failure — it is a legality failure. If your evaluation pipeline accidentally routes a HIPAA-covered tenant's patient data through a non-compliant evaluation environment, you have not just violated your quality standards. You have violated federal law. If your evaluation logging for an EU AI Act-covered tenant does not meet the documentation requirements for high-risk systems, your customer faces regulatory fines that they will pass to you through your contract. Compliance divergence must be enforced at the infrastructure level, not the policy level. It is not enough to write a document saying "Tenant X's data stays in eu-west-1." Your evaluation pipeline must make it architecturally impossible for that data to go anywhere else.

## How the Five Dimensions Compound

The reason multi-tenant evaluation is genuinely hard — and not just regular evaluation with tenant IDs — is that these five dimensions compound. A single tenant occupies a specific point in a five-dimensional space. They have their own quality definition, their own risk tier, their own data distribution, their own configuration, and their own compliance requirements. No two tenants occupy exactly the same point. A platform with three hundred tenants has three hundred unique combinations of evaluation requirements.

This compounding is what makes naive approaches fail. You cannot solve multi-tenant eval by adding a tenant filter to your existing pipeline. You need an evaluation architecture that treats the tenant's position in this five-dimensional space as the fundamental unit of evaluation design. The tenant's quality rubric determines what you measure. The tenant's risk tier determines how thoroughly you measure it. The tenant's data distribution determines what samples you evaluate against. The tenant's configuration determines what system you are actually evaluating. The tenant's compliance regime determines how you execute, log, and report the evaluation.

When you see these five dimensions clearly, the architectural requirements become obvious. You need parameterized rubrics, tiered evaluation intensity, per-tenant evaluation datasets, configuration-aware eval pipelines, and compliance-enforced execution boundaries. Each of these is a system to build and maintain. Together, they constitute a discipline.

## From Divergence to Differentiation

Recognizing the five dimensions of tenant divergence is the diagnostic step. Building systems that handle them is the engineering step. But there is a strategic step that most teams miss entirely. Tenant divergence is not just a cost to manage. It is a source of competitive advantage. Every dimension of divergence that your platform handles well is a dimension that makes your platform harder to replace. A competitor might match your model quality. They cannot easily match three hundred tenant-specific quality rubrics, two hundred risk-tiered evaluation pipelines, and forty compliance-enforced execution boundaries that took you two years to build.

The next subchapter explores this strategic reframe: how per-customer quality evaluation transforms from a cost center into the most defensible competitive moat in B2B AI.