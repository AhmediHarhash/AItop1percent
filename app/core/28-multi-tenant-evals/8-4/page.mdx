# 28.76 — Versioning and Migration: When the Platform Evolves and Customers Cannot Move at the Same Speed

In September 2025, an enterprise document intelligence platform serving 190 customers across legal, healthcare, and financial services upgraded its LLM judge from a Claude Sonnet 4 configuration to Claude Opus 4.5. The platform team tested exhaustively. Aggregate quality improved by four points. The new judge caught subtle errors the old one missed, and the scoring rubric was updated to take advantage of the model's stronger reasoning. The team pushed the update to production on a Friday. By Monday morning, 23 customers had opened support tickets. Not because their quality got worse — most of it got better — but because their quality scores changed. A legal customer whose contract compliance pipeline had been calibrated against the old judge's scoring distribution discovered that outputs previously scored at 91 now scored at 87 under the new rubric. Their internal compliance threshold was 88. They had not changed anything. Their model had not changed. But their quality gate now failed every run, blocking document processing for a team of 40 paralegals. Eleven other customers had automated alerts configured against specific score thresholds that the new judge's recalibrated distribution triggered or missed. The platform improved, and the improvement broke things for customers who had built processes around the old version's specific behavior.

This is the versioning problem. Your platform evolves continuously — new models, new judge configurations, new rubric versions, new golden set schemas, new pipeline optimizations. Your customers operate on change control cycles measured in weeks or months. The gap between your velocity and their tolerance for change compounds over time, creating a sprawl of active versions that becomes one of the most expensive hidden costs in multi-tenant operations.

## The Version Sprawl Problem

Every platform change that affects customer-visible behavior — a judge model update, a rubric revision, a golden set schema change, a scoring algorithm adjustment — creates a new version. If you cannot migrate every customer simultaneously, you must support the old version alongside the new one. Over time, this creates **version sprawl**: a growing inventory of active versions, each requiring maintenance, monitoring, and support.

After 18 months of operations, a typical multi-tenant eval platform accumulates a staggering version surface. Four rubric versions because major rubric updates happened quarterly but 15 to 20 percent of customers could not migrate in time for each cycle. Three judge model versions because the platform upgraded its judge twice but some customers have regulatory freezes or internal change control that prevents them from accepting new scoring models mid-quarter. Two golden set schemas because a schema improvement six months ago changed how reference examples are structured, and 30 customers still have golden sets in the old format. Seven distinct eval pipeline configurations because combinations of rubric version, judge version, and schema version create configuration permutations that multiply faster than anyone anticipated. The platform team expected to be running one version. They are running a combinatorial explosion.

## The Real Cost of Supporting Multiple Versions

Version sprawl is expensive, and the cost is both direct and hidden. The direct cost is infrastructure: each active judge model version requires its own serving endpoint or model deployment, each rubric version must be maintained in the configuration system, and each pipeline configuration must be tested after every platform change to confirm it still works. Industry experience shows that maintaining each active version costs roughly $8,000 to $15,000 per month when you factor in compute for judge model hosting, engineering time for testing and patching, support team training on version-specific behavior, and the monitoring infrastructure to track quality across all active versions.

The hidden cost is velocity tax. Every new feature, every bug fix, every optimization must be validated against all active versions before deployment. A platform running four rubric versions, three judge versions, and two golden set schemas faces a compatibility matrix that turns a one-day deployment into a one-week validation exercise. The engineering team spends increasing time maintaining old versions and decreasing time building new capabilities. This is **the maintenance tax of version sprawl**: the more versions you support, the slower you move, and the slower you move, the more pressure builds for the next version change, which creates yet another version when some customers cannot keep up.

## Migration Planning: Moving Customers Without Breaking Their Systems

The antidote to version sprawl is proactive migration — systematically moving customers from old versions to new ones on a defined timeline. But migration in a multi-tenant eval platform is not a database schema change you can run at midnight. Each customer has built processes, alerts, thresholds, and workflows around the specific behavior of their current eval version. Migration means changing that behavior, which means the customer must adjust their downstream processes.

Effective migration planning starts with impact analysis. Before announcing a migration, run the new version against every customer's production data in shadow mode. Compare the old version's scores to the new version's scores for every customer. Group customers into three categories. Low-impact customers see score changes of less than two points on their composite metrics — they can migrate with minimal disruption. Medium-impact customers see changes of two to five points — they need advance notice and may need to adjust thresholds. High-impact customers see changes greater than five points — they require individualized migration support, possibly including rubric recalibration or threshold renegotiation.

The migration timeline follows from the impact analysis. Low-impact customers get 30 days notice and migrate in the first wave. Medium-impact customers get 60 days notice and migrate in the second wave after the platform team has resolved any issues discovered in wave one. High-impact customers get 90 days notice and receive a dedicated migration plan that includes a parallel-run period before cutover.

## The Parallel-Run Approach

The safest migration technique for enterprise customers is the **parallel run**. During a parallel-run period — typically two to four weeks — the platform runs both the old and new eval versions simultaneously on the customer's production data. Both versions produce scores. The customer sees both sets of scores side by side: "your current version scored this output at 91, the new version scored it at 88." This transparency gives the customer time to understand how the new version behaves differently, adjust their thresholds and alerts if needed, and validate that their downstream processes still work with the new scoring distribution.

The parallel run is expensive. You are running two complete eval passes on every customer in the migration window, which doubles compute cost during that period. For a platform with 50 customers in a parallel-run window, the incremental cost is typically $15,000 to $40,000 per month depending on eval volume and judge model pricing. This cost is real, but it is far cheaper than the alternative: force-migrating customers without validation and then spending weeks in support escalations when their quality gates break.

The customer approves the cutover. This is non-negotiable. The platform can recommend the migration date, present the parallel-run data, and provide analysis showing that the new version maintains or improves quality. But the customer signs off on the switch. Enterprise customers operating under change control policies cannot accept unilateral changes to systems they depend on, and the eval system is a system they depend on because it determines whether their AI outputs meet their quality standard.

## Sunset Policies: Defining When Old Versions Die

Without a sunset policy, old versions never die. The platform keeps running them because some customer still uses them, and the cost of supporting them grows invisibly until someone adds up the infrastructure bills and realizes the platform is spending $120,000 per year maintaining a rubric version that four customers use.

A **sunset policy** defines the lifecycle of every eval version with three milestones. First, the **end-of-active-development date**, typically six months after a new version launches. After this date, the old version receives security patches and critical bug fixes only — no new features, no optimization. Second, the **migration deadline**, typically twelve months after the new version launches. By this date, all customers must have migrated to the new version. Third, the **end-of-life date**, typically one month after the migration deadline. On this date, the old version is decommissioned. Eval jobs that reference the old version fail with a clear error directing the customer to the current version.

The timeline communicates seriousness. Six months of notice followed by three months of active migration support followed by a hard cutoff gives even the most change-averse customer enough time to plan. Customers who still have not migrated at the end-of-life date are either customers who do not intend to migrate or customers whose account team failed to drive the process. Either way, keeping the old version alive past end-of-life sets a precedent that sunset dates are suggestions, not commitments, and that precedent makes every future migration harder.

## The Immovable Customer

Every platform has at least one: the customer who refuses to migrate because their regulatory audit depends on version stability. A pharmaceutical company whose FDA submission references specific eval configurations cannot change those configurations until after the submission clears review, which may take 6 to 18 months. A government agency whose Authority to Operate specifies the exact versions of every software component in their stack cannot accept version changes without reauthorization, a process that takes 90 to 180 days. A financial institution whose internal model risk management framework requires 60-day validation of any change to a scoring system treats your eval version upgrade as a model change requiring full validation.

These customers are not being difficult. They are operating under real constraints that your platform's release velocity does not accommodate. The organizational response is not to abandon the sunset policy for them. It is to create a **version exception process** with four guardrails. The customer formally requests a version hold, documenting the reason and the expected duration. The platform grants the hold with a defined expiration date — not indefinite. The customer pays a version maintenance surcharge that covers the incremental cost of supporting their version past end-of-life — typically $3,000 to $8,000 per month depending on complexity. And the platform assigns a specific engineer to monitor the held version, ensuring it continues to function even as the rest of the infrastructure evolves around it.

The surcharge is not punitive. It is cost recovery. Supporting an old version costs real money, and customers who require that support should bear the cost rather than subsidizing it across the entire customer base. As discussed in the tenant holdback patterns covered in Chapter 6, the economics of version exceptions must be explicit and visible so the platform team can make rational decisions about which exceptions to grant and when to insist on migration.

## Version Governance: Who Decides, Who Communicates, Who Manages Exceptions

Version governance requires clear ownership across three functions. The **platform engineering team** decides when a new version is ready for release and when an old version has reached its technical end-of-life. They own the migration tooling — parallel-run infrastructure, impact analysis pipelines, and version compatibility testing. The **customer success team** communicates version changes to customers, manages migration timelines per account, and escalates when customers cannot meet deadlines. The **product leadership** approves exceptions to the sunset policy, sets the version maintenance surcharge, and makes the final call when a customer's exception request conflicts with the platform's operational efficiency.

Without this three-way ownership, version governance defaults to whoever has the loudest voice. Engineering wants to sunset old versions immediately because they slow down development. Customer success wants to extend deadlines indefinitely because customers are complaining. Product leadership is unaware that the platform is running seven configurations because nobody reports version sprawl as a business metric. The result is ad hoc decisions, inconsistent treatment across customers, and a version inventory that grows without bound.

The discipline that prevents this is treating version count as a first-class operational metric, reported monthly alongside customer count, quality scores, and infrastructure cost. When the leadership team sees that the platform is running 14 active version configurations at a combined maintenance cost of $140,000 per month, they have the information to make rational decisions about sunset timelines, exception policies, and migration investment.

The version sprawl problem compounds as the customer base grows. Supporting four versions across 30 customers is manageable. Supporting four versions across 300 customers, each on a different migration timeline, is an organizational challenge that pushes the eval team past its capacity. The next subchapter examines the specific breaking points that appear as a multi-tenant platform scales from tens of customers to hundreds.
