# 28.1 — The Quality Contract: Defining What Good Means for Each Customer

Every customer relationship in a multi-tenant platform begins with an implicit quality expectation. The **Quality Contract** makes it explicit. Without one, your platform and your customer are measuring success against two different standards — and neither side knows it until the gap becomes a crisis.

A commercial SLA tells the customer what uptime they can expect, how fast you will respond to incidents, and what financial remedies apply when you miss targets. A Quality Contract goes deeper. It defines what "correct" means for that customer's specific use case, which dimensions of quality matter most, what thresholds separate acceptable from unacceptable, and how those thresholds connect to the business outcomes the customer actually cares about. The SLA is the legal document. The Quality Contract is the engineering document that makes the SLA enforceable at the evaluation layer. One lives in the contract. The other lives in the eval pipeline. And if you only have the first, you have a promise without a measurement system.

## Why Implicit Quality Expectations Destroy Relationships

In late 2025, a B2B content generation platform serving 180 enterprise customers discovered that their three largest customers each had fundamentally different expectations for what "high quality" meant. The first, a pharmaceutical company, expected every claim in generated content to be verifiable against published clinical literature, and any unsupported assertion was a quality failure regardless of how well-written the text was. The second, a consumer electronics retailer, considered quality primarily through the lens of brand voice consistency — they would tolerate minor factual imprecision in product descriptions but rejected any output that sounded "generic" or off-brand. The third, a financial advisory firm, cared almost exclusively about regulatory compliance and numerical accuracy, and would flag an entire document as failed if a single percentage was rounded incorrectly.

All three customers were evaluated against the same rubric. That rubric measured fluency, factual accuracy, tone appropriateness, and formatting compliance, with equal weights across all four dimensions. The pharmaceutical company's outputs scored well because the rubric's factual accuracy check was shallow — it verified that claims were plausible, not that they were citation-backed. The retailer's outputs scored poorly on factual accuracy for minor imprecisions that the retailer did not care about, while the brand voice dimension was too generic to capture their specific requirements. The financial firm received scores that fluctuated wildly because the rubric treated a rounding error the same as a tone mismatch. None of the three customers trusted the platform's quality reports, because the reports measured something other than what each customer considered quality.

The platform had no Quality Contracts. It had SLAs that guaranteed 99.5 percent uptime and 24-hour response time on severity-one tickets. Those SLAs said nothing about what "correct output" meant for each customer. When the pharmaceutical company escalated a compliance concern — generated content had included a claim about drug efficacy that was directionally correct but not supported by any specific study — the platform's response was that the output "passed quality evaluation." That response made the situation worse, because it demonstrated that the platform's definition of quality and the customer's definition of quality were not the same thing. The escalation eventually involved legal teams on both sides and cost the platform four months of trust rebuilding.

## What Goes Into a Quality Contract

A Quality Contract is not a checklist and not a legal document. It is a structured, versioned agreement between your platform's evaluation team and the customer's stakeholders that specifies exactly how quality will be measured for that customer's outputs. It contains five elements, each of which must be defined explicitly rather than assumed.

The first element is **quality dimensions** — the specific axes along which the customer's outputs will be evaluated. For the pharmaceutical company, those dimensions would include citation fidelity, clinical accuracy, regulatory language compliance, and readability. For the retailer, the dimensions would include brand voice alignment, product specification accuracy, promotional tone calibration, and format adherence. These are not generic categories pulled from a universal rubric. They are dimensions negotiated with the customer based on their use case, their domain, and the downstream business processes that depend on the platform's outputs.

The second element is **dimension weights** — the relative importance of each dimension. The pharmaceutical company weights citation fidelity at 40 percent of the total quality score, clinical accuracy at 30 percent, regulatory compliance at 20 percent, and readability at 10 percent. A perfectly readable output that cites no sources scores low. A citation-dense output with awkward phrasing scores high. These weights encode the customer's actual priorities, not what the platform thinks should matter. Getting the weights wrong is almost as damaging as using the wrong dimensions, because it produces scores that misrepresent the customer's experience.

The third element is **thresholds** — the minimum acceptable score for each dimension and for the composite quality score. A composite threshold of 85 percent means nothing if the citation fidelity dimension can score 60 percent and be hidden by high scores elsewhere. Effective Quality Contracts define both composite thresholds and per-dimension floors. The pharmaceutical company might require a composite score of 85 percent with a per-dimension floor of 75 percent on citation fidelity and clinical accuracy. No amount of readability can compensate for missing citations.

The fourth element is **acceptable failure modes** — the specific ways in which an output can fail without triggering an SLA breach. Every system produces some proportion of low-quality outputs. The question is which failures the customer considers tolerable and which ones constitute a breach. The pharmaceutical company might accept occasional readability issues — an awkward sentence, a paragraph that runs long — but consider any uncited clinical claim a severity-one failure. The retailer might accept minor factual imprecisions in non-safety-critical product descriptions but consider a brand voice violation in customer-facing marketing copy a severity-one failure. These distinctions are impossible to encode without explicitly negotiating them.

The fifth element is **measurement methodology** — how, when, and on what sample the quality score will be computed. A Quality Contract that specifies dimensions, weights, and thresholds but does not specify whether quality is measured on a random sample, a stratified sample, or an exhaustive evaluation leaves room for disagreements that are surprisingly bitter. The pharmaceutical company might require that every output involving drug efficacy claims be evaluated, not just a statistical sample. The retailer might accept random sampling at a 95 percent confidence level. The financial firm might require evaluation of every output that contains a numerical figure. These methodological choices determine whether the quality score actually reflects the customer's experience or merely approximates it.

## How a Quality Contract Differs From an SLA

The distinction between a Quality Contract and an SLA is not academic — it determines who in your organization is responsible for what, and where escalations land when things go wrong.

An SLA is a commercial commitment with financial penalties. It lives in the legal agreement between your company and the customer. It uses language like "99.5 percent availability," "4-hour response time for severity-one issues," and "service credits for missed targets." SLAs are enforceable in contract disputes. They are reviewed by legal teams, negotiated by sales teams, and tracked by finance teams. Their purpose is accountability between businesses.

A Quality Contract is a technical specification with operational consequences. It lives in the evaluation pipeline as configuration. It uses language like "citation fidelity at 40 percent weight," "per-dimension floor of 75 percent," and "exhaustive evaluation on drug efficacy outputs." Quality Contracts are maintained by evaluation engineers, negotiated by customer success teams in collaboration with the customer's technical stakeholders, and tracked by the evaluation infrastructure. Their purpose is alignment between the platform's measurement system and the customer's actual quality expectations.

The SLA might say "platform will maintain 90 percent accuracy on customer outputs." The Quality Contract specifies what "accuracy" means for that customer, how it is measured, which dimensions are included, and what sample methodology is used. Without the Quality Contract, the SLA's accuracy commitment is unenforceable in practice because neither side agrees on what accuracy means.

In mature multi-tenant platforms, the Quality Contract feeds the SLA. The evaluation pipeline computes quality according to the Quality Contract's specifications, and those computed scores determine whether the SLA's quality commitments are being met. When a customer disputes quality, the resolution process starts with the Quality Contract — did the platform measure what the customer expected? — before escalating to the SLA. This layered approach separates "are we measuring the right thing" from "are we meeting our commitment on the thing we measured."

## Building the First Quality Contract With a New Customer

The onboarding process for a new enterprise customer should include Quality Contract negotiation as a defined step, not an afterthought. In practice, this means a structured conversation between the customer's domain experts and your platform's evaluation team, conducted before the customer's first production workload.

That conversation follows a specific arc. You begin by asking the customer to describe, in their own words, what a perfect output looks like and what a failed output looks like. You are not asking for rubric dimensions yet. You are asking for concrete examples — actual inputs and the outputs the customer considers excellent, acceptable, and unacceptable. These examples become the seed of the customer's golden set and the raw material from which you extract quality dimensions.

From those examples, you identify the dimensions that distinguish excellent from unacceptable in the customer's domain. Sometimes the customer names them explicitly: "regulatory compliance is our top priority." Sometimes the customer reveals them implicitly: they keep rejecting outputs that are factually correct but use the wrong terminology for their industry. Extracting implicit dimensions is one of the evaluation team's most valuable skills, because customers often cannot articulate their own quality standards. They know quality when they see it but cannot decompose it into named criteria. Your job is to perform that decomposition, propose it back to the customer, and iterate until both sides agree.

Once dimensions are identified, you negotiate weights and thresholds through calibration exercises. You present the customer with 15 to 20 scored examples and ask them to rate each one as excellent, acceptable, or failed. You then compare their ratings against your proposed rubric's scores. Where the scores diverge from the customer's judgment, you adjust weights and thresholds. Two or three calibration rounds are usually sufficient to reach alignment, provided the customer's stakeholders are consistent in their own judgments. If their stakeholders disagree with each other — the compliance officer rates an output as failed while the marketing director rates it as acceptable — you have a deeper problem that the Quality Contract process surfaces early, before it becomes a production dispute.

The resulting Quality Contract is versioned and stored as evaluation configuration. Version 1.0 represents the initial agreement. When the customer's needs evolve — new product lines, new regulatory requirements, new quality priorities — the contract is updated with a new version, and historical evaluation results are always tied to the contract version that was active when they were computed. This versioning prevents a common dispute pattern where a customer claims quality has degraded when what actually changed was their expectations.

## When the Quality Contract Saves the Relationship

The platform that negotiated Quality Contracts with all new customers starting in early 2026 had an experience six months later that validated the entire approach. A logistics customer's quality scores dropped from 88 percent to 79 percent over three weeks. Under the old system, that drop would have been invisible in aggregate metrics and surfaced only when the customer complained. Under the new system, the per-customer eval pipeline detected the drop within 48 hours, correlated it with a model update that had changed how the platform handled shipment tracking terminology, and triggered an alert to the customer success team.

The customer success manager contacted the customer proactively — before the customer had noticed the problem. The conversation was straightforward because both sides had a shared vocabulary. The Quality Contract specified that shipment terminology accuracy was a dimension weighted at 25 percent, with a floor of 80 percent. The current score on that dimension was 71 percent. The resolution was clear: roll back the terminology handling to the previous model's behavior for this customer while the engineering team worked on a fix. The customer experienced two days of degraded quality instead of three weeks. The customer success manager framed the conversation not as "we broke something" but as "our per-customer monitoring caught this and we are already fixing it." The customer responded by expanding their contract.

That interaction illustrates the Quality Contract's deepest value. It is not just a measurement tool. It is a communication framework. When both sides share a vocabulary — named dimensions, agreed weights, defined thresholds — quality conversations become technical and specific instead of emotional and vague. "Your quality dropped" is an accusation. "Your shipment terminology dimension fell below the 80 percent floor we agreed on in your Quality Contract, and here is what we are doing about it" is a professional resolution. The Quality Contract converts quality from a subjective perception into an objective, measurable, actionable signal.

## Maintaining Quality Contracts at Scale

A platform with 500 customers cannot afford to treat each Quality Contract as a bespoke artifact maintained by hand. The Quality Contract must be structured as configuration that follows a standard schema — a set of dimension definitions, weights, thresholds, failure mode classifications, and measurement methodology parameters that can be stored, versioned, validated, and applied programmatically. As discussed in Section 15 on automated eval pipelines, the principle is the same: your evaluation criteria must be machine-readable, not just human-readable.

The schema approach enables tooling that would be impossible with freeform documents. You can validate that a Quality Contract is internally consistent — weights sum to 100 percent, thresholds fall within valid ranges, specified dimensions exist in the platform's dimension library. You can diff two versions of a Quality Contract to see what changed. You can audit which contracts have not been reviewed in the last six months. You can generate per-customer eval configurations directly from the contract schema, eliminating manual translation errors.

The overhead of maintaining 500 Quality Contracts is real, but it is manageable when contracts follow a standard structure. In practice, most customers cluster into a small number of archetypes — healthcare customers with compliance-heavy contracts, financial services customers with precision-heavy contracts, media customers with tone-heavy contracts, logistics customers with entity-accuracy-heavy contracts. A platform might have 500 contracts but only 12 to 15 distinct contract archetypes, each with customer-specific parameter variations. Building tooling around archetypes — templates, default weights, suggested dimensions — reduces the effort of creating a new Quality Contract from a multi-day exercise to a two-hour calibration session.

The Quality Contract establishes what "good" means for each customer. But translating that definition into a machine-readable evaluation configuration — one that the eval pipeline can execute automatically against every output — requires a more precise representation. The next subchapter introduces the Tenant Eval Fingerprint, the data structure that converts a Quality Contract into the dimensions, thresholds, and weights that drive per-customer evaluation at runtime.