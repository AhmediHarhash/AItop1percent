# Section 28 — Multi-Tenant and Customer-Specific Evals

When you serve one customer, evaluation is a conversation between your team and your product. When you serve five hundred customers, evaluation becomes a different discipline entirely. Every customer defines quality differently. Every customer carries different risk. Every customer expects proof that your platform meets their standard, not just some aggregate average that hides their specific pain.

This section is for the platform architect who wakes up to a Slack message that reads "Customer 247 says quality dropped last week" and needs to know whether that is a real regression in that customer's configuration, a change in the customer's expectations, a side effect of a model update that helped three hundred other customers, or noise. Single-tenant evaluation cannot answer that question. Multi-tenant evaluation must.

The techniques in Sections 3, 15, and 26 give you the foundations: how to design eval strategy, how to build eval pipelines, and how to scale them to high volume. This section covers the dimension those sections do not — customer-complexity scaling. Not "how do we evaluate a million outputs a day" but "how do we evaluate outputs for five hundred customers who each define good differently, run different configurations, operate under different regulations, and expect personalized proof that they are getting what they pay for."

---

## What You Will Learn

- **Chapter 1 — Why Multi-Tenant Evaluation Is a Different Discipline:** The architectural and organizational differences between evaluating a single product and evaluating a platform that serves hundreds of enterprise customers with divergent expectations.
- **Chapter 2 — Customer-Specific Quality Definitions and Rubrics:** How to define, parameterize, and maintain per-customer quality standards without building per-customer code, including tenant-scoped judge calibration and golden sets.
- **Chapter 3 — Tenant Isolation in Evaluation Systems:** The data, compute, and reporting isolation boundaries that prevent cross-tenant contamination, including legal jurisdiction enforcement and audit trail requirements.
- **Chapter 4 — Configuration-Aware Evaluation and Per-Tenant Monitoring:** How to evaluate each customer's actual configuration rather than platform defaults, and how to detect per-tenant quality drift before the customer does.
- **Chapter 5 — Customer Onboarding for Evaluation Systems:** The structured process of translating new customer expectations into measurable quality definitions, bootstrapping ground truth, and establishing baseline metrics.
- **Chapter 6 — Multi-Tenant Release Gates, Adapter Lifecycle, and Cross-Customer Impact:** How model updates, adapter drift, and platform changes create per-customer blast radius, and how to manage releases when one change affects hundreds of customers differently.
- **Chapter 7 — Cost Allocation, Customer Reporting, Evidence, and the Feedback Loop:** How to attribute eval costs per customer, produce audit-grade evidence packages, handle quality disputes, and turn per-customer feedback into platform improvement.
- **Chapter 8 — Organizational Patterns and the Multi-Tenant Eval Maturity Model:** Team structures, escalation policies, self-service eval capabilities, and the five-level maturity progression from one-eval-for-all to autonomous per-customer quality operations.

---

*The platform that evaluates all customers the same way will eventually lose the customers who matter most. The one that evaluates each customer on their own terms will become irreplaceable.*
