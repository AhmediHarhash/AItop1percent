# 28.10 — Cross-Adapter Interference: Shared Serving, Hidden Coupling

In the spring of 2025, a B2B document processing platform serving 280 enterprise customers deployed a new batch of per-tenant LoRA adapters to their shared serving cluster. Every adapter passed its pre-deployment eval suite individually. The first 48 hours of production looked normal. Then, during peak traffic on a Tuesday morning, three customers simultaneously reported quality degradation — outputs truncated mid-sentence, extraction accuracy dropping by 12 to 15 points, and response latencies spiking past their SLA thresholds. The platform team investigated each customer's adapter in isolation. All three passed their eval suites cleanly. The problem was not any individual adapter. The problem was what happened when 47 adapters competed for GPU memory, adapter scheduling slots, and batch processing bandwidth on the same serving infrastructure at the same time. The adapters were healthy. The serving environment they shared was not.

This failure pattern — where adapters that pass every individual quality check degrade each other through shared infrastructure contention — is one of the most difficult problems in multi-tenant adapter serving. Your eval pipeline tests adapters in isolation. Your production environment runs them in combination. The gap between those two conditions is where cross-adapter interference hides.

## How Shared Serving Creates Hidden Coupling

Multi-tenant adapter serving platforms like LoRAX, vLLM with multi-LoRA support, and S-LoRA achieve their cost efficiency by sharing a single base model across hundreds of tenant adapters. The base model weights occupy GPU memory once. Each tenant's LoRA adapter adds a small set of low-rank matrices — typically 10 to 50 megabytes per adapter, depending on rank and layer coverage — that are loaded alongside the base model weights at inference time. The architectural promise is clean isolation: each request routes to its specific adapter, and the adapter's low-rank matrices modify the base model's behavior only for that request.

The isolation promise holds at the mathematical level. Adapter A's weight matrices do not contaminate adapter B's outputs. But the infrastructure that loads, schedules, and executes those adapters introduces coupling at every other level. GPU memory is finite. PCIe bandwidth between CPU and GPU is shared. Batch scheduling decisions affect every request in the batch. Cache eviction policies determine which adapters stay hot and which must be reloaded. These infrastructure-level couplings create behavioral dependencies between adapters that no individual adapter eval can detect.

The critical insight is that adapter quality is not just a function of the adapter's weights. It is a function of the adapter's weights, the serving context in which those weights execute, and the other adapters competing for resources in that same serving context. Test the adapter alone and you measure the weights. Test the adapter in production and you measure the whole system — weights, infrastructure, and interference.

## Adapter Scheduling and Batching Effects

Modern multi-adapter serving systems use continuous batching to maximize GPU utilization. Incoming requests are grouped into batches, and the serving engine processes each batch's base model computation once, then applies the relevant adapter matrices per request. This is efficient when all requests in a batch use the same adapter or adapters of similar computational profiles. It becomes problematic when a batch mixes adapters with different rank sizes, different layer coverage, and different sequence lengths.

**Rank heterogeneity within batches** is the most common source of scheduling-induced quality degradation. A batch that combines requests for a rank-4 adapter (lightweight, fast) with requests for a rank-64 adapter (heavyweight, compute-intensive) must allocate compute resources for the worst case. The rank-4 requests wait while the rank-64 computation completes. In isolation, the rank-4 adapter processes a request in 180 milliseconds. When batched with rank-64 requests, the same request takes 340 milliseconds. For latency-sensitive tenants whose SLAs specify response times under 300 milliseconds, this batching penalty alone causes an SLA breach that has nothing to do with the adapter's quality and everything to do with its scheduling neighbors.

Worse, some serving engines implement timeout-based truncation to prevent slow requests from blocking the batch indefinitely. If a request exceeds the per-request timeout — typically set at the batch level, not the adapter level — the output is truncated at whatever point generation has reached. The adapter did not produce a bad output. The serving infrastructure cut off a good output mid-generation because another adapter in the same batch consumed the time budget. The resulting truncated response fails the tenant's quality eval, and the root cause is invisible in any adapter-level diagnostic.

**Adapter affinity scheduling** mitigates this by grouping requests for adapters with similar computational profiles into the same batches. Adapters are classified into profile tiers — lightweight (rank 8 or below), medium (rank 9 to 32), heavyweight (rank 33 and above) — and the batch scheduler preferentially groups requests within the same tier. This does not eliminate batching interference, but it reduces the variance in per-request latency within a batch, which prevents the worst cross-adapter timing penalties. LoRAX and recent versions of vLLM support adapter-aware batch scheduling policies, though the default configurations often prioritize throughput over per-adapter fairness.

## Memory Contention With Many Concurrent Adapters

GPU memory is the binding constraint in multi-adapter serving. The base model occupies the majority of GPU memory — a 7 billion parameter model in FP16 requires roughly 14 gigabytes, and a 70 billion parameter model requires roughly 140 gigabytes. Each LoRA adapter adds incrementally to this footprint. On a GPU with 80 gigabytes of memory serving a 7B base model, you have approximately 60 gigabytes available for adapter weights, KV cache, and runtime overhead. If each adapter averages 30 megabytes, you can theoretically hold 2,000 adapters in memory simultaneously.

The theory breaks down for two reasons. First, the KV cache — the intermediate computation state that enables efficient autoregressive generation — grows with sequence length and batch size, and it competes with adapter storage for the same memory pool. During peak traffic with long sequences, the KV cache can consume 20 to 40 gigabytes, reducing available adapter memory by half or more. Second, not all adapters are the same size. High-rank adapters for complex domains can reach 200 megabytes or more, and a handful of these heavy adapters can displace dozens of lighter ones.

When the number of active adapters exceeds available GPU memory, the serving system falls back to **adapter swapping** — loading adapters from CPU memory or disk into GPU memory on demand. This swapping introduces two failure modes relevant to quality.

The first failure mode is **cold start latency spikes**. When a request arrives for an adapter that has been evicted from GPU memory, the system must load the adapter weights from CPU memory before processing can begin. This loading takes 5 to 50 milliseconds depending on adapter size and PCIe bandwidth, but under memory pressure with many concurrent swaps, contention on the PCIe bus can inflate this to 200 milliseconds or more. For tenants who rarely send traffic — perhaps a few requests per hour — their adapter is almost always evicted between requests. Every request incurs a cold start penalty. Their experienced latency is consistently worse than tenants who send high-volume traffic and keep their adapter resident in GPU memory. This is a structural unfairness: low-traffic tenants pay the same platform fees but receive worse performance because the serving infrastructure's caching policy penalizes infrequent use.

The second failure mode is **silent quality degradation under memory pressure**. Some serving configurations respond to extreme memory pressure by reducing the KV cache allocation per request, which limits the effective context window the model can use. A request that would normally process 4,000 tokens of context may only receive a 2,000-token context allocation when memory is exhausted. The adapter produces an output — it does not error — but the output is based on half the context the tenant's prompt expected. The quality degrades because the adapter is working with incomplete information, not because the adapter itself is flawed. This degradation is invisible in logs unless you specifically monitor the actual context window allocation per request versus the requested context window.

## Detection Methods

Detecting cross-adapter interference requires monitoring that no individual adapter eval provides. Three detection approaches work at scale.

**Latency percentile monitoring per adapter** tracks not just average latency but the P95 and P99 latency per tenant adapter over time. Cross-adapter interference manifests as latency variance — the mean may be acceptable, but the tail latencies spike during peak traffic when contention is highest. A tenant whose P99 latency doubles during business hours compared to overnight hours is likely experiencing contention effects. The overnight performance represents the adapter's true capability. The daytime spike represents the interference penalty.

**Quality-latency correlation analysis** looks for cases where quality scores decline precisely when latency increases. If a tenant's eval scores are consistently lower on outputs that also had elevated latency, the quality degradation is likely infrastructure-induced rather than adapter-induced. The adapter needed more time to produce a good output but was cut short by batching pressure or memory constraints. This correlation is only visible if your eval pipeline captures per-request latency alongside per-request quality scores and enables joint analysis.

**Controlled isolation testing** periodically runs each adapter's eval suite under two conditions: in the shared production serving environment and in an isolated test environment with no adapter contention. The quality delta between these two conditions is the **interference penalty** — the cost this adapter pays for sharing infrastructure with others. An interference penalty above 3 to 5 quality points warrants investigation. Above 8 points, the adapter is being materially harmed by its serving context.

## Isolation Strategies

When detection reveals meaningful cross-adapter interference, five isolation strategies reduce the coupling without abandoning the cost benefits of shared serving.

**Adapter affinity groups** assign adapters that serve similar workloads or have similar computational profiles to the same serving replicas. Healthcare adapters share replicas with other healthcare adapters. High-rank adapters share replicas with other high-rank adapters. This reduces rank heterogeneity within batches and ensures that adapters with similar memory footprints compete with predictable peers rather than unpredictable strangers. Affinity grouping does not eliminate interference, but it makes interference consistent and predictable, which is far easier to manage than variable interference.

**Memory budgets per adapter pool** reserve a fixed portion of GPU memory for specific adapter tiers or tenant categories. Your top-tier enterprise customers with SLA-bound latency requirements get a reserved memory pool that guarantees their adapters are never evicted, regardless of traffic patterns from other tenants. This is the multi-tenant equivalent of reserved instances in cloud computing — you trade some utilization efficiency for guaranteed performance. Typically, reserving 20 to 30 percent of adapter memory for your top 10 to 15 percent of customers by revenue provides sufficient protection without materially degrading the shared pool.

**Dedicated serving replicas** for the highest-value or most interference-sensitive tenants remove those tenants from shared infrastructure entirely. A customer paying $800,000 per year whose adapter serves latency-critical production workflows should not compete for GPU memory with 200 other adapters. Dedicating a serving replica to that customer and two or three similar high-value customers eliminates interference for the customers where interference is most costly. The additional infrastructure cost is typically 5 to 10 percent of the tenant's annual contract value — a sound investment in retention.

**Request-level priority queuing** assigns priority levels to requests based on the tenant's SLA tier. High-priority requests are scheduled into batches first and receive guaranteed KV cache allocations. Low-priority requests fill remaining capacity. This does not eliminate interference, but it ensures that when contention forces a trade-off, the trade-off favors the tenants with the strictest quality and latency requirements. Priority queuing is supported natively in LoRAX and can be implemented through custom scheduling middleware in vLLM deployments, as Section 19 on deployment and runtime control describes.

**Traffic-aware autoscaling** monitors the aggregate adapter memory pressure across the serving fleet and spins up additional replicas before pressure reaches the threshold where swapping and contention begin. The scaling trigger should not be average GPU utilization — by the time average utilization is high, some individual requests are already experiencing interference. The trigger should be the P95 adapter load latency. When the time to load an evicted adapter exceeds a configurable threshold — typically 50 milliseconds — the autoscaler adds capacity. Scaling down occurs when adapter load latency drops below the threshold and stays there for 15 minutes, preventing flapping during traffic bursts.

## The Monitoring Gap

The fundamental challenge with cross-adapter interference is that most monitoring architectures are designed per-tenant. They answer the question "how is this tenant's adapter performing?" but not "how is this tenant's adapter performing because of what other adapters are doing?" Bridging this gap requires a monitoring layer that correlates per-tenant quality metrics with infrastructure-level metrics — GPU memory utilization, batch composition, adapter swap frequency, KV cache allocation per request — and surfaces the relationships between them.

This monitoring layer is not optional for platforms serving more than 50 concurrent adapters. Below that threshold, memory pressure is rarely severe enough to cause quality-visible interference. Above it, interference becomes a regular occurrence during peak traffic, and without infrastructure-aware monitoring, the platform team chases per-adapter quality issues that are actually infrastructure issues in disguise.

The investment in interference monitoring pays off not just in quality stability but in infrastructure cost optimization. Understanding which adapters interfere with each other enables smarter placement, smarter scaling, and ultimately lower infrastructure cost per adapter than a naive shared-everything deployment. The platform that understands its interference patterns spends less and delivers more than the platform that treats shared serving as a solved problem.

Adapter drift and cross-adapter interference are technical problems the platform team can solve with engineering and monitoring. But some multi-tenant challenges are not technical. The next subchapter addresses the human side: tenants who refuse to accept platform updates, and the contractual, operational, and economic implications of supporting customers who will not move.
