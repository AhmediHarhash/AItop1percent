# 28.8 — Regression Detection Across Heterogeneous Tenant Configurations

Most platform teams believe they have regression testing. They run a suite of eval cases against the latest model version, compare scores to the previous release, and flag anything that drops below a threshold. For a single-product company shipping one configuration to every user, this works. For a multi-tenant platform where five hundred enterprise customers each run a different combination of prompts, retrieval settings, safety filters, model versions, and adapter weights, this approach tests a product that no customer actually uses. Your regression suite passes. Your release ships. And fourteen customers file support tickets within 72 hours because the update degraded their specific configurations in ways your tests never checked.

The uncomfortable truth is that in a heterogeneous multi-tenant system, the same model update is simultaneously a regression and an improvement. It depends entirely on whose configuration you test against. A retrieval prompt change that boosts quality for tenants using short-context customer service configurations may degrade quality for tenants using long-context legal document configurations. A safety filter update that satisfies the compliance requirements of your healthcare customers may make your creative writing customers' outputs feel over-constrained and robotic. There is no single regression result. There are hundreds of regression results, one per configuration, and they frequently contradict each other.

## Why Standard Regression Suites Fail for Multi-Tenant

Standard regression testing assumes a canonical product configuration. You maintain a test suite, run it against the new build, and compare results to the previous build. The implicit assumption is that if the test suite passes, every user benefits or at least no user is harmed. This assumption collapses the moment your customers run different configurations.

Consider a platform serving 340 enterprise customers. Each customer has configured at least three dimensions that affect model behavior: their system prompt template, their retrieval pipeline settings (chunk size, retrieval depth, reranking model), and their safety filter sensitivity level. The system prompt alone has 47 distinct variants across the customer base. The retrieval pipeline has 12 meaningfully different configurations. The safety filter runs at five sensitivity levels. Multiply these together and you have over 2,800 unique configuration combinations in production, each producing subtly different model behavior for the same input query.

Your regression suite tests one of these combinations — the platform default. When you ship a model update and test it against the default configuration, you have tested 0.03 percent of the configuration space your customers actually occupy. The other 99.97 percent is untested. You are not running regression testing. You are running a confidence ritual that provides no actual coverage guarantee for any specific customer.

The failure mode is not rare. In late 2025, a B2B AI platform serving document processing customers shipped a retrieval model upgrade that improved average quality by 6 percent across their default configuration. Three enterprise customers — a law firm, a patent analysis company, and a pharmaceutical research team — experienced quality drops of 11 to 18 percent. All three used retrieval configurations with maximum depth settings and specialized reranking, the exact configurations the regression suite did not cover. The platform spent four weeks debugging, rolling back selectively, and rebuilding trust with customers who had started evaluating competitors.

## The Configuration-Aware Regression Matrix

The solution is to stop testing "the product" and start testing each meaningful configuration cluster. A **configuration-aware regression matrix** maps each model update against a representative set of tenant configurations and produces a per-configuration regression verdict rather than a single platform-wide verdict.

Building this matrix requires three inputs. First, you need a complete inventory of active configurations — every tenant's current prompt template, retrieval settings, safety filter level, adapter version, and any other parameter that affects model behavior. As the previous subchapter on customer-specific safety evaluation covered, this inventory should already exist in your configuration management system. Second, you need a mapping from configurations to evaluation datasets — each configuration cluster needs its own eval set that exercises the behaviors unique to that cluster. A healthcare customer's eval set should include medical terminology, HIPAA-sensitive scenarios, and the specific output formats that healthcare prompts are designed to produce. A creative writing customer's eval set should include tone variation, stylistic flexibility, and the absence of unnecessary safety hedging. Third, you need a scoring mechanism that compares results per configuration rather than in aggregate.

The matrix itself is a grid. Rows are configuration clusters. Columns are the metrics you track — accuracy, relevance, safety compliance, latency, format adherence. Each cell contains the score delta between the current release and the proposed release for that specific configuration cluster. A positive delta means improvement. A negative delta means regression. The matrix gives you a visual map of where the release helps, where it hurts, and where it has no effect. A release that shows green across 95 percent of configuration clusters but red for three clusters is not a pass. It is a conditional pass that requires investigation of those three clusters before you ship.

## Prioritized Regression Testing

You cannot test every configuration for every change. If you have 340 tenants with 2,800 unique configuration combinations and each eval run takes eight minutes of compute time, exhaustive configuration regression testing for a single release takes 373 hours. That is over fifteen days of continuous compute for one release cycle. No platform ships that slowly.

Prioritized regression testing accepts that exhaustive coverage is impossible and allocates testing budget where it matters most. The prioritization framework uses three dimensions: revenue exposure, risk profile, and configuration uniqueness.

**Revenue exposure** ranks configurations by the total annual contract value of tenants running them. If your top 20 customers represent 68 percent of platform revenue and they collectively run 35 distinct configuration clusters, those 35 clusters get tested on every release. The remaining 2,765 configurations are tested on a rotating basis.

**Risk profile** ranks configurations by the likelihood and severity of regression. Configurations that use per-tenant LoRA adapters are higher risk than configurations using only the base model because adapter interactions with model updates are less predictable. Configurations with strict compliance requirements — healthcare, financial services, legal — are higher risk because a regression may violate regulatory obligations, not just degrade quality. Configurations that were recently modified by the tenant are higher risk because new configurations have less historical data to predict their behavior under model changes.

**Configuration uniqueness** identifies outlier configurations that differ substantially from the cluster centers. A tenant who runs a configuration that no other tenant resembles is the most likely to experience a regression that your cluster-representative testing misses. These outliers deserve disproportionate testing relative to their revenue because they occupy configuration space that no other test covers.

The practical outcome is a tiered testing plan. Tier one includes 30 to 50 high-priority configuration clusters that are tested on every release, representing your most important revenue, your highest-risk compliance cases, and your most unusual configurations. Tier two includes 100 to 200 configuration clusters that are tested on a weekly rotating basis — a different subset each week, cycling through all of them over a four-to-six-week period. Tier three includes all remaining configurations that are tested monthly or when specific components they depend on change. This tiered approach reduces the compute cost from 373 hours to roughly 12 hours for a single release, which fits within a standard CI pipeline.

## Statistical Approaches to Heterogeneous Regression Detection

Per-configuration regression detection cannot rely on simple pass-fail thresholds applied uniformly. The reason is variance. Some configuration clusters produce highly consistent eval scores — a healthcare document extraction pipeline that always scores between 91 and 94 on accuracy. Others produce inherently variable scores — a creative content generation pipeline that scores between 72 and 88 depending on the complexity of the prompt. A two-point drop in the healthcare configuration is a meaningful regression. A two-point drop in the creative configuration is within normal variance. Applying the same threshold to both misses real regressions in stable configurations and raises false alarms in variable ones.

The approach that works at scale is **per-cluster effect size measurement**. Instead of asking "did the score drop below a threshold," you ask "is the score change statistically meaningful relative to this cluster's historical variance?" Effect size metrics — Cohen's d is the most common — compare the magnitude of the score change to the standard deviation of historical scores for that specific configuration cluster. A Cohen's d of 0.2 represents a small effect. A Cohen's d of 0.8 represents a large effect. You set your regression threshold in effect size units rather than raw score units, which means each configuration cluster has its own effective sensitivity calibrated to its own variance profile.

In practice, this means maintaining a rolling window of historical eval scores for each configuration cluster — typically the last 90 days of results — and computing the mean and standard deviation for each metric. When a new release is evaluated against a cluster, the score delta is divided by the historical standard deviation to produce the effect size. Clusters where the effect size exceeds your threshold — typically between negative 0.3 and negative 0.5, depending on your tolerance — are flagged as potential regressions requiring human review.

This statistical approach also handles the multiple comparison problem. When you test 50 configuration clusters simultaneously, random variance alone will produce a few apparent regressions even when nothing has actually changed. Applying a Bonferroni correction or controlling the false discovery rate ensures that your regression alerts reflect real quality changes rather than statistical noise. Without this correction, every release triggers a handful of false regression alerts, the team learns to ignore them, and the real regressions slip through the noise.

## The Interaction Problem

Changes to shared components create non-linear effects across different configurations. A shared component is any piece of infrastructure that multiple tenant configurations depend on — the base model, the retrieval index, the safety classification layer, the prompt assembly engine, the output formatting pipeline. When you update a shared component, the impact is not uniform. It interacts differently with each tenant's specific configuration, and the interaction effects are often larger than the direct effects.

Consider a safety classifier update that adjusts the sensitivity threshold for medical content. Tenants running healthcare configurations with strict safety filters see their outputs become more conservative — which is what the update intended. But tenants running general-purpose configurations with moderate safety filters also see their outputs become more conservative on any prompt that tangentially involves health topics. A customer support bot that previously answered "Can I take this supplement with my medication?" by referencing the product FAQ now refuses to answer because the updated safety classifier flags the query as medical advice. The safety classifier change was not designed for this configuration. But the interaction between the updated classifier and the general-purpose prompt template and the moderate safety filter setting produces an unexpected behavioral change.

Interaction effects are difficult to predict because they emerge from combinations. The safety classifier change alone is benign. The general-purpose prompt alone is benign. The moderate safety filter alone is benign. But the combination of all three, after the update, produces a regression that none of them would produce individually. This is why component-level unit testing — testing the safety classifier in isolation — does not catch multi-tenant regressions. You must test the full configuration stack, and you must test it for configurations that combine the updated component with tenant-specific settings you did not have in mind when you designed the update.

The practical mitigation is **interaction-aware test selection**. When a specific shared component changes, you identify which configuration dimensions interact with that component and prioritize testing configurations that exercise those dimensions at their extremes. A safety classifier update should trigger regression testing for configurations at both ends of the safety sensitivity spectrum — the most restrictive and the most permissive — because those are the configurations most likely to exhibit non-linear interaction effects. A retrieval model update should trigger testing for configurations at both ends of the retrieval depth spectrum. This targeted interaction testing supplements the general prioritized regression matrix by focusing additional compute on the configurations most likely to be affected by the specific change being shipped.

## Regression Ownership in Multi-Tenant Systems

In a single-product company, regression ownership is straightforward. The team that shipped the change owns the regression. They fix it or roll back. In a multi-tenant platform, regression ownership is ambiguous because the same change produces different outcomes for different customers, and "fixing" the regression for affected customers might undo the improvement for unaffected ones.

A model update improves quality for 200 tenants, has no measurable effect on 295 tenants, and degrades quality for 5 tenants. Who owns the regression? The model team designed the update to improve the majority case. The five affected tenants run unusual configurations that the model team did not specifically target. The platform team operates the infrastructure but did not author the model change. The customer success team manages the relationship with the affected tenants but has no authority to roll back or modify the update.

The ownership framework that works at scale assigns regression ownership based on scope and root cause. **Component regressions** — where the change itself is flawed and the fix is to improve the component — are owned by the component team. The model team fixes the model. **Configuration regressions** — where the change is correct for the majority but interacts poorly with specific configurations — are owned by the platform evaluation team, who must either adjust the affected configurations, provide those tenants with a compatibility workaround, or pin those tenants on the previous version while a longer-term fix is developed. **Contractual regressions** — where the quality drop for affected tenants violates SLA commitments — escalate to the customer success and engineering leadership teams because they involve contractual and business decisions, not just technical ones.

The critical organizational discipline is that regression ownership must be assigned within 24 hours of detection. Unowned regressions languish. No team investigates because no team is accountable. The five affected customers wait for resolution while internal teams debate responsibility. After a week, the customers have already started evaluating competitors. After a month, at least one has started a migration. The cost of ambiguous regression ownership is measured in churned enterprise contracts, not in engineering hours.

## Building the Regression Feedback Loop

Regression detection is not a one-time gate. It is a feedback loop that improves your release process over time. Every regression that reaches production — every quality drop detected by monitoring rather than prevented by testing — is evidence that your regression matrix has a gap. The feedback loop closes that gap.

After every production regression, the platform team conducts a regression review that answers three questions. First, was this configuration cluster in the regression matrix? If not, it should be added. Second, if it was in the matrix, why did the regression test not catch the drop? The eval dataset may need updating, the effect size threshold may be too permissive, or the interaction between the changed component and this configuration may not have been tested. Third, what is the minimal test case that would have caught this regression, and can it be added to the automated suite?

Over six to twelve months, this feedback loop transforms a sparse regression matrix — covering 30 priority configurations — into a comprehensive matrix that covers the configuration clusters responsible for 95 percent of production regressions. The matrix is never complete. New configurations appear as tenants onboard and modify their setups. But it converges toward the configurations that actually produce regressions, which is a much smaller set than the total configuration space. Most configurations are robust to most changes. The feedback loop identifies the fragile configurations — the ones where small shared component changes produce large behavioral shifts — and concentrates testing effort where it prevents the most damage.

As Section 18 covers in its treatment of regression testing and release gates, the goal is not zero regressions. The goal is zero undetected regressions. Every regression that your matrix catches before release is a success. Every regression that reaches production and is detected by monitoring within minutes is acceptable. Every regression that reaches production and is first detected by a customer support ticket is a failure of the regression system. The configuration-aware regression matrix, combined with statistical effect size detection and interaction-aware test selection, gives you the tools to catch the vast majority of multi-tenant regressions before they reach the customers who would be harmed by them.

The next subchapter addresses a practical prerequisite for everything described here: when you cannot test every configuration, how do you sample the configuration space to maximize coverage with bounded compute?
