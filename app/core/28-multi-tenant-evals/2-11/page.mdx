# 28.11 — Fairness in Multi-Tenant Evals: Avoiding Unequal Model and Judge Treatment

Every tenant deserves equal evaluation quality regardless of their contract size. This is not a platitude. It is a design requirement with direct consequences for your platform's reliability, your customers' trust, and your legal exposure. Violating it does not just feel wrong — it produces a system where small customers receive worse quality detection, slower regression alerts, and less accurate scoring, which means they experience more quality problems, which means they churn at higher rates, which means your platform's growth depends on a shrinking pool of large accounts that eventually gain enough leverage to extract unsustainable concessions. Eval fairness is not charity toward small customers. It is the structural foundation that keeps your platform viable.

## The Attention Gradient

In every multi-tenant platform, attention follows revenue. The customer paying $1.4 million per year gets a dedicated customer success manager, quarterly business reviews, and direct access to the engineering team. The customer paying $18,000 per year shares a success manager with forty other accounts, gets a monthly automated health report, and files support tickets into a shared queue. This gradient is natural and economically rational. It is how B2B businesses allocate scarce human resources to maximize return.

The problem is that this attention gradient silently extends into the evaluation system. Large customers get their golden sets reviewed and refreshed quarterly because their success manager advocates for it. Small customers' golden sets go stale because nobody is asking when the last refresh happened. Large customers get calibration sets built by domain experts their account team recruited. Small customers inherit domain-cluster defaults that nobody has verified for their specific use case. Large customers' quality anomalies trigger a Slack alert in a dedicated channel monitored by a senior engineer. Small customers' anomalies go into a shared queue where they wait behind higher-priority work.

None of this is a policy decision. No team meeting concluded with "let's give small customers worse evaluation." The gradient emerges organically from how organizations allocate attention. The customer success manager with forty accounts does not have time to review golden sets for all of them, so they review them for the three that are up for renewal this quarter. The engineer assigned to calibration work starts with the high-revenue tenants because the priority list is sorted by contract value. The alerting system fires the same alerts for all tenants, but the response time depends on who is watching and how loudly the account team escalates.

## What Unequal Eval Quality Actually Produces

When the attention gradient reaches the evaluation system, the consequences are not just theoretical. They are measurable, specific, and damaging.

**The first consequence is detection lag.** A large customer with a well-maintained golden set, calibrated judges, and actively monitored quality dashboards will have a model regression detected within hours. A small customer with a stale golden set, uncalibrated judges, and a dashboard that nobody checks regularly might not have the same regression detected for weeks — if it is detected at all before the customer notices it themselves. The detection lag is not because the monitoring system treats them differently. It is because the inputs to the monitoring system — the golden set quality, the judge calibration accuracy, the dashboard review cadence — are all lower quality for the smaller customer. The monitoring system works the same for both. The data feeding it does not.

**The second consequence is false signal rate.** An uncalibrated judge produces more false positives and more false negatives. For a well-calibrated tenant, the judge scores 84 and that 84 is meaningful — it is within two points of what a human expert would assign. For an uncalibrated tenant, the judge scores 84 but the true quality might be 78 or 91. The eval system says the tenant is performing at 84, and nobody knows whether that number is trustworthy. When the score drops from 84 to 79, is that a real regression or judge noise? For the calibrated tenant, you know. For the uncalibrated tenant, you are guessing. The small customer is not just getting worse detection — they are getting unreliable detection, which is arguably worse because it undermines trust in the entire eval signal.

**The third consequence is invisible quality decay.** When a large customer's quality dips, the account team notices, the engineering team investigates, and the problem gets fixed. When a small customer's quality dips by the same amount, the eval signal might not catch it (stale golden set), or the alert might not be actioned promptly (shared queue), or the customer might not be informed (no dedicated success manager reviewing the report). The quality decay continues unchecked until either the customer complains or they leave. By the time a small customer files a support ticket about quality, they have often been experiencing the problem for four to eight weeks — long enough to have already started evaluating alternatives.

## The Eval Quality Gap Is a Churn Multiplier

The relationship between eval quality and customer retention is not linear. It is compounding. Small customers who receive worse eval quality experience more undetected regressions. More undetected regressions mean more quality problems that persist until the customer notices. More customer-noticed problems mean lower trust. Lower trust means higher churn sensitivity — the next problem, even a minor one, tips them over the threshold into actively shopping for alternatives.

Industry data from B2B SaaS platforms consistently shows that small and mid-tier customers churn at two to three times the rate of enterprise customers. Many platform teams attribute this to the inherent volatility of smaller businesses or to the lower switching costs of smaller contracts. But a significant fraction of that churn is caused by the platform itself providing unequal service quality — not in the product, which treats all tenants identically, but in the evaluation and monitoring layer that detects and responds to quality problems. The model serves the same quality to a $20,000 customer and a $1.4 million customer. The eval system detects, alerts, and triggers remediation faster for the $1.4 million customer. The outcome is that the $20,000 customer lives with problems longer, trusts the platform less, and leaves sooner.

Reducing that churn differential by even thirty percent has meaningful revenue impact. If a platform has two hundred small-tier customers at an average of $24,000 per year and the annual churn rate is eighteen percent, that is thirty-six churned customers and $864,000 in lost revenue. Reducing churn to twelve percent saves $144,000 per year — more than enough to fund the eval fairness improvements that drive the reduction.

## The Baseline Eval Quality Floor

The solution is not to give every tenant the same eval investment. That would be economically irrational — a tenant paying $18,000 per year cannot justify the same dedicated attention as a tenant paying $1.4 million. The solution is to define a **baseline eval quality floor** that every tenant receives regardless of contract size, and to enforce that floor as a system property rather than an organizational commitment.

The floor has four components.

The first component is **minimum golden set freshness**. Every tenant's golden set must have been reviewed and validated within a defined window — ninety days is a common choice for most platforms, sixty days for tenants in rapidly changing domains. The system tracks last-review timestamps per tenant and automatically flags golden sets that are approaching their freshness deadline. When a golden set goes stale, it triggers a review task that goes to the team responsible for that tenant's domain cluster — not to the account team, who may not prioritize it if the account is small.

The second component is **minimum calibration verification frequency**. Every tenant's judge calibration must be verified at a defined cadence. As covered in the previous subchapter, this means running the calibration set through the judge pipeline and confirming that agreement remains above threshold. The cadence is tied to judge model update cycles and tenant domain stability, not to contract size. A small healthcare tenant and a large healthcare tenant both deserve calibration that accurately reflects clinical quality standards.

The third component is **equal alerting response requirements**. When the eval system detects a quality anomaly for any tenant, the response timeline must be the same regardless of contract value. If your SLA for investigating a quality alert is four hours for enterprise tenants, it must be four hours for all tenants. The investigation depth may differ — a large tenant gets a senior engineer and a root cause analysis document, while a small tenant gets a standard investigation and a support ticket update — but the initial response time must be equal. Unequal response times create a system where small customers' problems compound while large customers' problems are contained.

The fourth component is **minimum eval coverage**. Every tenant must have evaluation coverage on the dimensions that matter for their use case. If a tenant's use case involves factual accuracy and tone compliance, both dimensions must be evaluated even if the tenant has not explicitly asked for tone evaluation. The coverage minimum ensures that no tenant has blind spots in their evaluation simply because nobody allocated the effort to configure a complete eval for their account.

## Tiered Coverage Versus Unequal Quality

This is the nuance that trips teams up. Chapter 7 of this section covers tiered eval coverage — the practice of offering different evaluation packages at different pricing tiers. Enterprise tier gets ten evaluated dimensions, weekly eval runs, custom golden sets, and dedicated calibration. Standard tier gets five evaluated dimensions, monthly eval runs, and domain-cluster golden sets. Self-service tier gets three evaluated dimensions, quarterly eval runs, and platform-default golden sets.

Tiered coverage is acceptable and necessary. Different customers pay for different levels of eval investment, just as they pay for different levels of support, different SLA commitments, and different feature access. The boundary is between coverage breadth and evaluation quality. A standard-tier tenant evaluates fewer dimensions — that is a coverage decision, and the customer understands they are paying for less. But the dimensions that are evaluated must be evaluated well. If a standard-tier tenant evaluates factual accuracy, that factual accuracy evaluation must use a calibrated judge, a reasonably fresh golden set, and monitoring with the same alerting response time as an enterprise tenant's factual accuracy evaluation. The smaller customer gets fewer dimensions evaluated. They do not get the same dimensions evaluated badly.

The distinction is between "we evaluate fewer things for you" and "we evaluate the same things worse for you." The first is a business decision. The second is a quality failure. Customers who discover they are in the second category — that their quality scores are less reliable, their regressions are detected later, their alerts are investigated more slowly — will leave. And they will leave angry, because the platform sold them quality evaluation and delivered quality theater.

## Detecting the Fairness Gap

The Eval Quality Gap is not something any team creates intentionally. It emerges gradually and invisibly. Detecting it requires explicit measurement.

The metrics that reveal the gap are not difficult to compute, but they are rarely tracked. Start with golden set freshness distribution — sort all tenants by the age of their most recent golden set review and check whether the distribution correlates with contract size. If it does, you have a freshness gap. Next, check calibration verification coverage — what percentage of tenants have had a calibration verification run in the last ninety days, and does that percentage differ between revenue tiers? Then check alert response times — when the eval system fires a quality alert, how long does it take for the alert to be acknowledged and investigated, broken down by tenant tier?

These metrics will almost certainly reveal a gap the first time you measure them. The gap is the starting point for improvement, not evidence of negligence. Every platform has it. The platforms that measure it can fix it. The platforms that do not measure it let it grow.

## The Automation Imperative

The reason the Eval Quality Gap exists is that eval quality depends on human attention, and human attention follows revenue. The solution is to reduce the dependency on human attention for the components that should be equal across tenants. Automate golden set freshness monitoring. Automate calibration verification scheduling. Automate alert routing with enforced SLA timers that do not discriminate by tier. Automate the detection of tenants whose eval configurations have drifted below the quality floor.

When the baseline eval quality floor is enforced by the system rather than by organizational discipline, the gap closes not because people start caring more about small customers but because the system removes the opportunity for unequal treatment. The golden set freshness deadline fires the same automated review task for every tenant. The calibration verification runs on the same schedule regardless of contract value. The alert response timer starts the same countdown for every tenant's quality anomaly. Human attention still follows revenue for the discretionary investments — the extra dimensions, the custom calibration sets, the quarterly business reviews. But the floor is the floor, and the system enforces it.

## The Legal Dimension

In 2026, the fairness of evaluation systems is not purely an ethical or business consideration. Under the EU AI Act, providers of high-risk AI systems must demonstrate that their quality management systems ensure consistent performance across deployment contexts. For a multi-tenant platform serving healthcare tenants, this means you must be able to show that your evaluation system provides consistent quality assurance for all healthcare tenants — not just the ones who pay the most. A regulator who discovers that your $20,000 healthcare tenant had a stale golden set and uncalibrated judges while your $1.4 million healthcare tenant had quarterly refreshes and domain-expert calibration will not accept "we prioritize by revenue" as an explanation. The regulatory expectation is that the quality assurance system provides adequate coverage for all deployments in high-risk categories, regardless of commercial considerations.

This does not mean every tenant must have identical evaluation. It means every tenant in a given risk category must have evaluation that is adequate for that risk category. A healthcare tenant on the self-service tier evaluates fewer dimensions than a healthcare tenant on the enterprise tier. But the dimensions that are evaluated — particularly those relevant to patient safety and regulatory compliance — must meet the same adequacy standard. The eval quality floor is not just good business practice. For regulated domains, it is a compliance requirement.

## Building Fairness Into the Architecture

Eval fairness is not a policy you enforce through guidelines. It is a property you build into the evaluation architecture. The architecture must include automated freshness tracking per tenant, automated calibration scheduling per tenant, tier-independent alert response timers, a minimum eval coverage configuration that is applied during tenant onboarding and verified during periodic audits, and a fairness dashboard that surfaces the metrics described above to the platform team on a weekly basis.

The dashboard is the accountability mechanism. When the team can see that seventeen tenants have golden sets older than ninety days and all seventeen are on the standard tier, the problem becomes visible and actionable. Without the dashboard, the problem is invisible and persistent. The automation enforces the floor. The dashboard reveals the gaps. Together, they convert eval fairness from aspiration into operation.

The quality contract you signed with each customer promises that your platform evaluates their quality accurately and responds to problems promptly. That promise does not have a footnote that says "unless you are a small customer." The evaluation system must honor the promise for everyone. The next chapter shifts from defining quality per customer to a different challenge entirely: ensuring that evaluation data, configuration, and results from one tenant never leak into another tenant's environment — the discipline of tenant isolation in evaluation systems.
