# 28.10 — Score Comparability: When Two Tenants Cannot Share a Scale

The customer success manager for the East region pulls up her dashboard and filters to her two largest accounts. Tenant A, a logistics company using the platform for shipping document extraction, shows an overall quality score of 86 percent. Tenant B, a legal services firm using the platform for contract analysis, shows 84 percent. She writes in her weekly report that both accounts are performing well and within SLA, with Tenant A slightly ahead. She flags Tenant B for a quality improvement initiative and schedules a call with their account lead to discuss raising the score to match Tenant A's performance.

Her counterpart in the West region sees the same dashboard. He knows both accounts. He knows that Tenant A's 86 percent is measured against a rubric that evaluates whether extracted shipping fields match the source document — binary correctness, with partial credit for fields that are close but not exact. He knows that Tenant B's 84 percent is measured against a rubric that evaluates whether contract summaries capture all material terms, use appropriate legal language, flag risk clauses accurately, and maintain the logical structure of the source agreement. Tenant B's 84 percent against a four-dimension legal rubric represents significantly higher absolute quality than Tenant A's 86 percent against a single-dimension extraction rubric. The East region manager's quality improvement initiative for Tenant B is not just unnecessary — it risks triggering a rubric tightening that could make Tenant B's score drop further while actual quality stays the same or improves.

This is **The Comparability Problem**: when per-tenant rubrics differ in structure, dimension, or difficulty, the resulting scores are not on the same scale, and treating them as comparable produces wrong decisions at every level of the organization.

## Why Scores Look Comparable When They Are Not

The Comparability Problem is insidious because numbers feel objective. When you see 86 percent and 84 percent on the same dashboard, your brain processes them as two measurements on the same scale. It takes active effort to remember that those percentages were produced by different rubrics evaluating different dimensions of quality in different domains. The number itself carries no information about the rubric that produced it. An 84 looks like an 84 regardless of whether it measures binary field extraction accuracy or multi-dimensional legal analysis quality.

This illusion is amplified by platform tooling. Dashboards aggregate scores into unified views. Trend lines plot all tenants on the same axis. Alerts fire when any tenant drops below a threshold. Leadership reports rank tenants by score. Every layer of the platform's reporting infrastructure treats scores as comparable because the infrastructure does not know — and was not designed to track — the differences between the rubrics that produced those scores. The tooling creates the appearance of a single scale across all tenants, and that appearance becomes the basis for decisions about where to invest quality improvement effort, which accounts need intervention, and whether the platform is meeting its obligations.

## The Three Sources of Incomparability

Scores become incomparable for three distinct reasons, and understanding which source of incomparability you are dealing with determines whether you can fix it or must accept it.

**The first source is dimensional incomparability.** Tenant A's rubric evaluates one dimension — extraction accuracy. Tenant B's rubric evaluates four dimensions — completeness, legal language, risk flagging, and structural fidelity. Even if you weight all four of Tenant B's dimensions equally and average them into a single score, that composite is measuring a fundamentally different thing than Tenant A's single-dimension score. Achieving 84 percent across four dimensions requires being good at four things simultaneously. Achieving 86 percent on one dimension requires being good at one thing. These are not comparable achievements, and no normalization can make them comparable because they are not measuring the same construct.

**The second source is difficulty incomparability.** Even when two tenants evaluate the same dimension — say, factual accuracy — the difficulty of achieving a given accuracy level varies by domain. Extracting shipping weights from standardized bill-of-lading forms is a structurally easier task than extracting material terms from unstructured legal prose. A model that scores 86 percent on shipping extraction is performing at a lower capability level than a model that scores 84 percent on contract term extraction, even though the number is higher. The difficulty gap between domains is not captured in the score. A percent is a percent, and the dashboard does not know that one percent was harder to earn than the other.

**The third source is threshold incomparability.** Different tenants set different thresholds for what counts as a pass at each score level. Tenant A's rubric says an extracted field is "correct" if it matches the source value exactly or is within an acceptable formatting variation. Tenant B's rubric says a contract summary is "correct" only if it captures the clause in question with no material omission and uses language that a practicing attorney would consider appropriate. Tenant A's "correct" is a binary comparison. Tenant B's "correct" is a human judgment about adequacy. The word "correct" means different things at different thresholds of rigor, and the scores produced against those different thresholds are not on the same scale.

## The Decisions That Go Wrong

The Comparability Problem does not just produce wrong numbers. It produces wrong decisions, and those decisions have real consequences for customers and for the platform.

The most common wrong decision is misallocated quality improvement effort. When leadership sees that Tenant B has a lower score than Tenant A, the natural response is to prioritize improving Tenant B's quality. Engineering time gets allocated to investigate Tenant B's performance. The team discovers that Tenant B's model is actually performing well — it is the rubric that is demanding. They spend two weeks trying to improve a score that does not need improving, while Tenant C, whose score looks healthy but whose rubric is so lenient that it masks real problems, gets no attention at all.

The second wrong decision is incorrect SLA enforcement. If your platform promises all enterprise tenants quality above 80 percent, and that threshold is applied uniformly across rubrics of varying difficulty, some tenants will appear to be comfortably above the line while actually receiving mediocre quality against an easy rubric, and other tenants will appear to be at risk while actually receiving excellent quality against a hard rubric. The SLA becomes meaningless because the threshold has no consistent interpretation across tenants.

The third wrong decision is flawed platform-level quality reporting. Aggregating incomparable scores — averaging Tenant A's 86 with Tenant B's 84 to report a "platform average of 85" — produces a number that describes nothing. It is not the average quality of the platform. It is the average of numbers produced by different instruments measuring different things. Reporting it to leadership, to the board, or to investors as a meaningful metric is misleading even if unintentionally so.

## Platform-Floor Dimensions: What You Can Compare

Incomparability does not mean you can never compare anything across tenants. It means you must be deliberate about what you compare. The way to create legitimate cross-tenant comparison is to identify **platform-floor dimensions** — quality dimensions that are shared across all tenants, evaluated with the same rubric, at the same difficulty level.

Not every quality dimension qualifies as a platform floor. But some dimensions are universal enough that a single rubric applies to all tenants. Response latency is one — a response that takes 4.2 seconds is 4.2 seconds regardless of the tenant's domain. Factual grounding rate is another, if defined carefully: the percentage of claims in the output that can be traced to the provided context, evaluated by the same grounding judge with the same grounding rubric, is comparable across tenants because the evaluation methodology is identical. Safety compliance — whether the output contains prohibited content categories — is comparable when the prohibited categories are defined at the platform level rather than the tenant level.

These platform-floor dimensions give you a cross-tenant quality baseline that is legitimately comparable. You can rank tenants by latency and the ranking means something. You can compare grounding rates across tenants and the comparison is valid. You can report a platform-wide safety compliance rate and that number reflects reality. The key constraint is that the dimension must use the same rubric, the same evaluation methodology, and the same judge configuration for all tenants. The moment you allow per-tenant customization of the evaluation for a given dimension, that dimension is no longer comparable.

Most mature platforms in 2026 operate with three to five platform-floor dimensions that are comparable and five to fifteen tenant-specific dimensions that are not. The platform-floor dimensions power cross-tenant dashboards, platform-level reporting, and aggregate trend analysis. The tenant-specific dimensions power per-customer quality management, SLA enforcement, and customer-facing quality reports. Mixing the two — using tenant-specific dimensions for cross-tenant comparison or using platform-floor dimensions as a proxy for tenant-specific quality — produces the wrong decisions described above.

## Normalization: Tempting but Dangerous

When teams discover the Comparability Problem, the first engineering instinct is to normalize scores. Convert all tenant scores to z-scores. Rank-normalize across tenants. Compute percentiles within each rubric difficulty tier. These approaches are statistically sound in the abstract and practically dangerous in a B2B context.

The danger is that normalization obscures the information the customer cares about. A tenant does not want to know that they are at the 72nd percentile of quality across all tenants. They want to know whether their model meets their quality bar. Normalization answers the platform's question — "how does this tenant compare to others?" — but not the customer's question — "is my system performing well?" Worse, normalized scores can create perverse incentives. If a tenant improves their absolute quality but other tenants improve more, the normalized score drops. The customer's quality got better, the dashboard says it got worse, and the customer loses trust in your reporting.

There is one context where normalization is useful: internal platform operations. When the engineering team needs to decide which tenants to prioritize for quality improvement, normalizing scores within comparable rubric categories helps identify which tenants are furthest below their potential. A tenant scoring 78 percent against a rubric where similar tenants score 90 percent is a better improvement candidate than a tenant scoring 82 percent against a rubric where similar tenants score 84 percent. But this normalization is for internal prioritization, never for customer-facing reporting or SLA enforcement.

## How to Communicate Incomparability

The hardest part of the Comparability Problem is not technical. It is organizational. Customer success teams, sales teams, and executives all want a single number that summarizes platform quality. They want to rank customers by score, identify the bottom decile for intervention, and report a single platform quality number to the board. Telling them "scores are not comparable across tenants" feels like an excuse for not doing the work, especially when the dashboard makes the scores look perfectly comparable.

The communication strategy that works is to separate the reporting layers explicitly. Build two dashboards, not one. The first dashboard shows platform-floor dimensions — the metrics that are genuinely comparable across tenants. This satisfies the organizational need for cross-tenant comparison with numbers that actually support it. The second dashboard shows per-tenant quality against per-tenant rubrics. This satisfies the customer-specific need for quality tracking against their own standards. Label the first "Platform Quality" and the second "Customer Quality." Make the names different enough that nobody accidentally treats one as the other.

When someone asks "what is our overall quality?" the answer is the platform-floor metrics: "our grounding rate is 94 percent, our latency P95 is 1.8 seconds, our safety compliance is 99.7 percent." When someone asks "how is Tenant B doing?" the answer is their per-tenant metrics: "Tenant B's contract analysis quality is 84 percent against their four-dimension legal rubric, which their account lead has confirmed exceeds their requirements." These are different questions with different answers, and the reporting infrastructure must enforce that separation rather than inviting conflation.

## The Score Metadata Requirement

The technical implementation of comparability management requires that every score in your system carries metadata about the rubric that produced it. A bare score of 84 is information-free. A score of 84 with metadata identifying the rubric version, the dimensions evaluated, the judge model used, the calibration set version, and the evaluation date is information-rich. When someone queries the system for "all tenants scoring below 80," the system can filter by rubric comparability group — showing only tenants whose scores were produced by comparable rubrics — rather than returning a misleading list that mixes incomparable measurements.

This metadata is not optional overhead. It is the mechanism that prevents the Comparability Problem from creating wrong decisions. Every score stored without rubric metadata is a score that will eventually be compared to an incomparable score by someone who does not know the difference. The metadata makes the difference visible and queryable rather than requiring organizational memory to keep track of which scores can be compared to which.

## When Comparison Is Simply Impossible

Some teams spend months trying to make incomparable scores comparable. They build elaborate normalization schemes, difficulty-adjusted weighting systems, and cross-rubric mapping functions. And after all that effort, the scores are still not truly comparable because the underlying constructs being measured are different. Comparing a binary extraction accuracy score to a multi-dimensional legal analysis score is like comparing a sprint time to a gymnastics score. Both are numbers. Both measure performance. They are not on the same scale and cannot be put on the same scale because they measure fundamentally different things.

The mature response is to accept this limitation and build your systems accordingly. Not every metric needs to be comparable across tenants. Per-tenant quality tracking, per-tenant regression detection, per-tenant SLA enforcement — these are the primary use cases for tenant-specific scores, and none of them require cross-tenant comparison. The platform-floor dimensions handle the cases where comparison matters. The tenant-specific dimensions handle the cases where per-customer accuracy matters. Trying to force comparability where it does not exist wastes engineering effort and produces misleading outputs.

The Comparability Problem is ultimately a lesson in intellectual honesty. It is easier to put all scores on one dashboard and let people draw their own conclusions. It is harder — and more responsible — to build systems that make the limits of comparison explicit. The next subchapter examines a related challenge: ensuring that the evaluation system itself treats all tenants fairly, regardless of their contract size or platform importance.
