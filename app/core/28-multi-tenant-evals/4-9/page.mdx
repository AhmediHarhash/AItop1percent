# 28.9 — Configuration Space Sampling: Testing What They Run Without Testing Everything

How do you test five hundred products when you only have budget to test fifty? That is the central question of configuration space sampling, and every multi-tenant platform must answer it or accept that the vast majority of their customers are running untested configurations in production. The answer is not "test more." The answer is "test smarter" — by choosing the fifty configurations that provide maximum coverage of the five hundred, catching the regressions that matter while spending compute only where it changes outcomes.

## The Combinatorial Explosion

Every tenant configuration is a point in a high-dimensional space. Each dimension represents a configurable parameter — the system prompt template, the retrieval depth, the chunk size, the reranking model, the safety filter sensitivity, the output format, the model version, the adapter weights, the tool access list, the temperature setting. A platform with ten configurable dimensions, each with five to twenty possible values, creates a configuration space with millions of theoretical combinations. Your five hundred tenants occupy five hundred points in that space — a vanishingly small fraction of the total, but still far more than you can test exhaustively on every release.

The math is unforgiving. If a single configuration eval run takes eight minutes of compute on your judge infrastructure and you have five hundred active tenant configurations, exhaustive per-tenant regression testing for one release takes 4,000 minutes — roughly 67 hours. If you release weekly, that is 67 hours of dedicated eval compute per week just for regression, not counting the compute for ongoing monitoring, new tenant onboarding, compliance audits, and ad-hoc quality investigations. Most platforms cannot justify that budget. Even those that can in theory cannot tolerate the wall-clock time — a 67-hour regression cycle means your release is stale before the results come back.

Configuration space sampling reduces this to a tractable number — typically 30 to 80 representative configurations — that provides high coverage with bounded compute. The trade-off is that some configurations go untested on any given release. The craft is in choosing which ones to test so that the untested configurations are the ones least likely to regress.

## Cluster-Based Representative Sampling

The most effective sampling strategy starts with clustering. Tenant configurations that are similar along the dimensions that matter most — model version, retrieval settings, safety filter level — tend to behave similarly under model updates. If two tenants both run the same system prompt template, the same retrieval depth of fifteen documents, and the same moderate safety filter, a regression that affects one will almost certainly affect the other. You do not need to test both. You need to test one and apply the result to both.

**Cluster-based representative sampling** groups tenant configurations into clusters based on similarity, then selects one representative from each cluster to include in the regression test suite. The representative should be the configuration closest to the cluster center — the median configuration along each dimension — because it is the most likely to reflect the cluster's typical behavior. If a regression affects the cluster center, it likely affects the cluster. If it does not affect the cluster center, it likely does not affect the cluster either.

The clustering algorithm matters less than the distance metric. K-means, DBSCAN, and hierarchical agglomerative clustering all produce reasonable results. What matters is how you define "distance" between two configurations. Not all configuration dimensions contribute equally to behavioral divergence. Changing the system prompt template from a customer service prompt to a legal analysis prompt has a massive effect on model behavior. Changing the temperature from 0.7 to 0.8 has a much smaller effect. Your distance metric should weight dimensions by their impact on eval outcomes. In practice, you determine these weights empirically — run eval sets across configurations that differ on one dimension at a time, measure the score variance attributable to each dimension, and use those variances as weights.

A platform with five hundred tenants typically produces 25 to 60 meaningful clusters, depending on how much configuration diversity exists. Each cluster gets one representative in the regression suite. The result is a test set of 25 to 60 configurations that covers the behavioral range of all five hundred tenants in a fraction of the compute time.

## Boundary Sampling

Cluster centers capture typical behavior. They miss atypical behavior — the configurations at the extremes of each dimension where model updates are most likely to produce unexpected results. **Boundary sampling** complements cluster-representative sampling by explicitly targeting configurations at the edges of the configuration space.

For each configurable dimension, boundary sampling selects configurations at the minimum and maximum values. If retrieval depth ranges from 3 to 50 documents across your tenant base, boundary sampling includes a configuration at depth 3 and a configuration at depth 50. If safety filter sensitivity ranges from level 1 (minimal filtering) to level 5 (maximum filtering), boundary sampling includes level 1 and level 5. These extreme configurations are disproportionately likely to exhibit regressions because model updates are often designed and tested for the middle of each dimension's range. The edges are where assumptions break.

The boundary configurations need not correspond to real tenants. You can construct synthetic boundary configurations by combining extreme values across multiple dimensions — maximum retrieval depth with minimal safety filtering, for example. These synthetic boundaries stress-test the configuration space in ways that no real tenant's configuration does, which is precisely why they are valuable. They find regressions that would affect any tenant who drifts toward the boundary in the future, even if no tenant currently occupies that exact point.

In practice, boundary sampling adds 10 to 20 configurations to the regression suite — two per configurable dimension for the most impactful dimensions. Combined with the 25 to 60 cluster representatives, the total regression suite is 35 to 80 configurations. At eight minutes per eval run, the total regression time is under 11 hours — within the budget of a weekly release cycle.

## Revenue-Weighted Sampling

Not all tenants are equally important to the business. A configuration cluster representing three free-tier tenants and a configuration cluster representing your largest enterprise customer paying $2.4 million per year should not receive equal testing priority. **Revenue-weighted sampling** adjusts the test budget allocation so that configurations associated with higher-revenue tenants receive disproportionate coverage.

The mechanism is straightforward. After clustering and selecting representatives, assign each cluster a weight equal to the total annual contract value of tenants in that cluster. Allocate test budget — measured in eval compute minutes — in proportion to these weights. A cluster representing $8 million in revenue might get three representatives instead of one, tested against a larger eval dataset. A cluster representing $200,000 in revenue gets its single representative tested against the standard eval dataset.

Revenue weighting also determines the order in which configurations are tested when compute budget is tight. If a release needs to ship today and only 40 of the planned 60 configurations can be evaluated before the deployment window, revenue weighting ensures that the 40 that run are the ones protecting the most revenue. The 20 that are skipped are the lowest-revenue clusters. This is not ideal — those 20 clusters still have real customers — but it is the rational allocation when time is constrained.

The risk of pure revenue weighting is that it systematically undertests small customers. A regression that affects fifty small tenants collectively worth $500,000 may never be caught because no individual small tenant's cluster ranks high enough for priority testing. The mitigation is to set a floor — every cluster gets at least one representative in the rotation, regardless of revenue. Revenue weighting determines how much additional coverage a cluster receives, not whether it receives any coverage at all.

## Change-Driven Sampling

Cluster-based and revenue-weighted sampling determine what to test in general. **Change-driven sampling** determines what to test for a specific change. When a particular component is updated — the retrieval model, the safety classifier, the output formatter — you can narrow the configuration space to only the configurations that depend on the changed component.

If the retrieval model is updated, only configurations that use retrieval are affected. Configurations that run direct generation without retrieval are unchanged and do not need regression testing for this specific update. Within the retrieval-using configurations, those with the deepest retrieval depths and most aggressive reranking are most likely to exhibit behavior changes. Change-driven sampling identifies the impacted configuration subspace and allocates all available testing budget within that subspace, ignoring the unaffected configurations entirely.

The implementation requires a dependency map — a record of which configuration dimensions depend on which shared components. The retrieval model component is depended on by the retrieval depth, chunk size, and reranking model dimensions. The safety classifier component is depended on by the safety filter sensitivity dimension. The base model component is depended on by every dimension, which means base model updates trigger the broadest regression testing. Adapter updates are the narrowest — they affect only the single tenant whose adapter was modified.

Change-driven sampling is particularly valuable for hotfixes and emergency patches where speed matters. If a critical bug in the safety classifier requires an immediate patch, you do not need to test all 60 representative configurations. You need to test the 15 configurations that use safety filtering at levels 3 through 5, confirm the fix resolves the bug without regression, and ship. The other 45 configurations are unaffected by the change and do not block the release.

## The Coverage Metric

Sampling strategies are only useful if you can measure how well they cover your actual configuration space. The **configuration coverage metric** answers a simple question: what percentage of your active tenant configurations have been evaluated in the last N days?

Coverage is measured at the cluster level. If a cluster representative was tested in the last seven days, the entire cluster is considered covered. If no representative was tested, the cluster is uncovered. Total coverage is the number of covered clusters divided by the total number of clusters, weighted by either tenant count or revenue, depending on what you are optimizing for.

A healthy multi-tenant platform maintains at least 90 percent configuration coverage on a rolling seven-day window for tier-one clusters and at least 80 percent on a rolling 30-day window for all clusters. Coverage below these thresholds means a significant portion of your customer base is running configurations that have not been validated against recent changes. That is not testing. That is hoping.

Coverage dashboards should show not just the current percentage but the trend. If coverage is declining over time — because the number of tenants is growing faster than the testing budget, or because configuration diversity is increasing — the platform team needs to invest in either more compute, better clustering, or both. Coverage decline is a leading indicator of undetected regressions. If you wait until a customer reports a regression to realize your coverage was insufficient, you waited too long.

## Continuous Versus Release-Triggered Sampling

The final design decision is when to sample. **Release-triggered sampling** runs the regression suite before each release. It is the minimum requirement — you should never ship a release without regression testing against representative configurations, as Section 18 covers in its treatment of release gates. But release-triggered sampling only catches regressions caused by your changes. It does not catch regressions caused by upstream changes — API provider model updates, third-party retrieval index refreshes, external safety classifier updates — that happen between your releases.

**Continuous sampling** runs representative configurations on a rolling schedule, independent of your release cycle. Each day, a subset of representative configurations is evaluated against the current production system. Over a week, all representatives are tested. This continuous background evaluation catches regressions that your release gates would miss — a quality drop caused by an upstream model provider silently updating their model weights, for example, or a gradual quality degradation caused by data drift in a tenant's retrieval corpus.

The cost of continuous sampling is additional compute — roughly the same as one release-triggered regression suite per week, spread across daily runs. For platforms with weekly or bi-weekly release cycles, continuous sampling effectively doubles the eval compute budget for regression detection. For platforms with daily releases, continuous sampling adds relatively little incremental cost because the release-triggered runs already provide near-daily coverage.

The combination of both approaches — release-triggered sampling that catches your own regressions and continuous sampling that catches everything else — provides the most comprehensive coverage. The release gate prevents you from shipping known regressions. The continuous monitoring catches unknown regressions from external changes. Together, they ensure that your configuration coverage is not just a snapshot at release time but an ongoing property of your production system.

As Section 26 covers in its treatment of scaling evaluation systems, the compute cost of sampling-based regression testing grows linearly with the number of configuration clusters, not with the number of tenants. Adding your five hundred and first tenant to an existing cluster does not increase your testing cost. Adding your five hundred and first tenant with a configuration that creates a new cluster adds one representative to the suite. This scaling property is what makes configuration space sampling tractable at enterprise scale — the testing budget tracks configuration diversity, not customer count.

The next subchapter addresses the most challenging evaluation surface in multi-tenant platforms: the interaction between shared base models and per-tenant LoRA adapters, where a single base model upgrade can silently degrade hundreds of tenant-specific fine-tunes.
