# 28.63 — The Customer Quality Report: Proving SLA Compliance With Evidence, Not Promises

The customer quality report is the single most important artifact your platform produces for its customers. Not the API. Not the dashboard. Not the model outputs themselves. The report. It is the document that transforms "we believe our system is performing well for you" into "here is the evidence that our system met every commitment we made to you this month." Without it, your quality claims are promises. With it, they are proof.

Every multi-tenant platform eventually learns this lesson. The ones that learn it before a customer escalation build trust. The ones that learn it after — during a contract renewal negotiation where the customer's procurement team asks "how do we know you're meeting our SLA?" and your team cannot produce evidence — learn it at the cost of revenue.

## What Belongs in the Quality Report

A customer quality report must answer five questions, and it must answer each one with data, not narrative.

**Are you meeting our accuracy commitments?** This section shows the customer's quality scores broken down by the dimensions that matter to them — overall accuracy, per-document-type accuracy, per-use-case accuracy. If the SLA specifies 92 percent accuracy on clinical document summarization and 88 percent accuracy on insurance claims processing, the report shows both numbers separately. The scores come directly from the eval pipeline — the same judges, the same rubrics, the same golden sets that the customer agreed to during onboarding. Aggregate accuracy without per-category breakdown is useless. A customer whose SLA covers five document types needs to see five numbers, not one average that hides a failure in one category behind success in four others.

**Are you meeting our latency commitments?** Latency percentiles — p50, p90, p95, p99 — tell the customer whether their end users are experiencing acceptable response times. The report shows latency by request type and by time period, so the customer can see whether the Tuesday morning spike that their users complained about was a real platform issue or a transient network event. Latency SLAs are typically specified at the p95 or p99 level, and the report must show the actual percentile value against the contractual threshold.

**What is the error rate, and what kinds of errors occurred?** Errors decompose into categories: timeouts, malformed outputs, hallucinations caught by the eval pipeline, safety filter triggers, and retrieval failures for RAG-based systems. The total error rate matters less than the error composition. A customer can tolerate a 0.3 percent timeout rate. They cannot tolerate a 0.3 percent hallucination rate. The report separates these categories so the customer can assess risk by failure type.

**Is quality stable, improving, or degrading?** Trend lines over the last three to six months show the customer whether the platform is getting better or worse for their specific workload. A single month's quality score is a snapshot. The trend is the story. A customer whose accuracy has drifted from 94 percent to 91 percent over four months needs to see that trajectory even if 91 percent still meets the SLA. The drift signals a developing problem that warrants attention before it becomes an SLA breach.

**What is the SLA compliance status?** A clear, unambiguous statement: "Your platform met all contractual quality commitments during this reporting period" or "Your platform did not meet the latency commitment for insurance claims processing during the week of October 14, with p95 latency reaching 4.2 seconds against a 3-second threshold." No hedging, no qualifications, no "depending on how you measure it." The customer's legal team will read this section. Make it precise.

## What Does NOT Belong in the Quality Report

Equally important is what you exclude.

**Internal model details** — which base model version is serving the customer, which adapter rank is deployed, which judge model evaluates their outputs — do not belong in a customer-facing quality report unless the customer specifically requested them. Exposing internal architecture creates two risks. First, it invites questions the customer is not equipped to evaluate ("why are you using rank-16 instead of rank-32?"), diverting the quality conversation into a technical debate. Second, it creates contractual entanglement — if you mention a specific model version in the quality report, the customer may argue that switching to a different version violates the implicit commitment.

**Other customers' data** must never appear in any form, even aggregated. A report that says "your accuracy is in the top quartile of our healthcare customers" reveals information about your other healthcare customers' quality. Per-tenant isolation is absolute. The report covers this customer's data and nothing else.

**Raw scores without context** confuse more than they inform. A quality score of 0.87 means nothing to a VP of operations who does not know the scale, the methodology, or the baseline. Every number in the report needs a reference frame: the contractual threshold, the historical average, or the trend direction. "Accuracy: 93.2 percent (SLA threshold: 90 percent, previous month: 92.8 percent)" gives the reader everything they need in one line.

## Report Frequency: Monthly, Weekly, and Real-Time

The right reporting cadence depends on the customer's tier and their operational needs.

**Monthly reports** are the standard for most customers. They provide enough data points for meaningful trend analysis while keeping the reporting overhead manageable. A monthly report covers 30 days of production data, evaluates quality against all contractual dimensions, and lands in the customer's inbox within five business days of month-end. For a platform with 300 tenants, generating 300 monthly reports is a batch operation that the eval pipeline handles automatically.

**Weekly reports** serve tier-1 enterprise customers — those paying more than $200,000 annually or operating in regulated industries where quality degradation has legal consequences. Weekly reports provide faster feedback on emerging issues. If a model update on Tuesday causes a subtle quality shift, a weekly report surfaces it by the following Monday. A monthly report might not surface it for three weeks. The trade-off is that weekly reports contain more noise. A bad week might be a statistical fluctuation rather than a real trend. The report must distinguish between statistically significant shifts and normal variation, which requires larger sample sizes or wider confidence intervals.

**Real-time dashboards** are the premium tier offering. Instead of periodic reports delivered by email, the customer gets a live dashboard showing quality metrics updated hourly or daily. Real-time dashboards are expensive to build and maintain — they require streaming eval results into a customer-facing analytics layer with per-tenant access controls, as described in Section 27 on global infrastructure. But for customers who operate AI-assisted workflows with immediate business impact — clinical decision support, financial trading analysis, real-time fraud detection — the ability to see quality in real time is a feature they will pay for.

## Quality Reports vs Eval Reports

These are different documents serving different audiences, and conflating them is a common mistake.

An **eval report** is an internal document for the platform team. It contains every metric the eval pipeline produces: per-judge agreement rates, confidence distributions, edge case analysis, model comparison results, adapter performance breakdowns, and pipeline health indicators. It uses technical language. It includes diagnostic information. It is designed to help engineers identify and fix quality issues.

A **customer quality report** is an external document for the customer's organization. It contains the subset of eval data that answers the customer's questions — am I getting what I paid for? It uses business language. It includes context and trend analysis. It is designed to build trust and demonstrate accountability.

The relationship between the two is that the quality report is derived from the eval report by filtering, aggregating, and contextualizing. The eval report might show that judge agreement for a customer's clinical summaries dropped from 89 percent to 82 percent. The quality report translates that into: "Clinical summary accuracy remained at 93.1 percent, within the 90 percent SLA threshold. Internal quality monitoring detected increased variability in complex multi-morbidity cases, which the platform team is investigating. No customer action is required." The customer gets the outcome. The platform team keeps the diagnostic details.

## Presenting Negative Results Without Triggering Unnecessary Escalation

Every platform has bad months. A model update introduces a subtle regression. A judge configuration change produces slightly different scores. A customer's input distribution shifts in a way that temporarily reduces quality. The quality report must present these honestly — hiding negative results destroys trust far more than the negative result itself — but it must present them with appropriate context.

The framework is **fact, context, action**. State the fact: "Latency p95 for insurance claims processing exceeded the 3-second SLA threshold during the week of October 14, reaching 4.2 seconds." Provide context: "The increase was caused by a temporary infrastructure scaling event during a platform-wide model migration. The issue was detected within 4 hours and resolved within 12 hours." Describe the action: "The platform team has implemented pre-scaling procedures for future model migrations to prevent recurrence."

What you must never do is bury the negative result in positive data, hoping the customer will not notice. They will notice. Their technical team reads the report line by line. And when they discover a buried negative — or worse, when they discover an omission — the trust damage is exponential. The customer does not just lose confidence in this report. They lose confidence in every previous report they accepted at face value.

## The Legal Weight of Quality Reports

Quality reports are not marketing materials. In the context of enterprise contracts with SLA penalties, quality reports become **contractual evidence**. If your contract specifies a 92 percent accuracy threshold with financial penalties for non-compliance, and your quality report shows 92.3 percent accuracy for the month, that report is the documentary evidence that no penalty is owed. If a dispute arises, both parties will point to the quality report as the source of truth.

This legal weight means quality reports must meet evidentiary standards. The methodology must be documented and agreed upon before the first report is generated. The data must be auditable — the customer or their auditor must be able to trace any number in the report back to the underlying eval results. The calculation must be reproducible — running the same eval pipeline against the same data must produce the same scores. The report must be tamper-evident — the customer must trust that the platform did not selectively exclude low-scoring results or retroactively adjust the methodology. Section 29 on enterprise governance covers the organizational controls that support these requirements.

## Automating Report Generation From the Eval Pipeline

Manual report generation does not scale past 20 customers. At 300 customers with monthly reports, you need 300 reports generated within five business days of month-end. Each report pulls from tenant-specific eval data, applies tenant-specific SLA thresholds, computes tenant-specific trend analysis, and formats the results according to the customer's tier and preferences.

The automation pipeline has four stages. The **data extraction stage** queries the eval results store for all results within the reporting period, filtered by tenant_id. The **computation stage** calculates quality metrics, latency percentiles, error breakdowns, trend comparisons, and SLA compliance status using the tenant's contractual definitions. The **formatting stage** renders the computed data into the report template — different templates for different customer tiers, as the next subchapter explores. The **delivery stage** sends the report through the customer's preferred channel — email, secure portal upload, or API push to the customer's internal systems.

The automation must handle edge cases: customers who onboarded mid-month and have partial data, customers whose SLA thresholds changed during the reporting period, customers with multiple contracts covering different use cases. These edge cases are where manual intervention creeps back in. Designing the automation to handle them from day one — even if the first implementation is simple — prevents the operational burden from scaling linearly with the customer count.

Generating the report is the technical challenge. But the same quality data needs to reach three very different audiences within a single customer organization — each with different expertise, different concerns, and different definitions of a useful report. The next subchapter examines how to design quality reports that work for the customer's technical team, their executive leadership, and their legal counsel simultaneously.
