# 28.7 — The Quality Inheritance Problem: Base Rubrics, Overrides, and Composition

What happens when a customer needs everything your platform rubric measures plus three dimensions it doesn't? You add the three dimensions. Simple enough. Now what happens when that customer also needs to change the weight on two existing dimensions, remove one dimension entirely, and use different scoring thresholds on four others? You are no longer configuring a rubric. You are forking it. And when 200 customers each fork the base rubric in different directions, you no longer have a rubric system. You have 200 rubric systems that share a common ancestor but have diverged beyond recognition.

This is the **Quality Inheritance Problem** — the multi-tenant rubric challenge that emerges the moment you move past flat, per-customer rubric definitions and attempt to create a layered system where customers inherit from shared foundations and override specific elements. The inheritance model is attractive because it promises efficiency: define common quality dimensions once, let customers customize what they need, and avoid the maintenance burden of 200 fully independent rubrics. In practice, inheritance creates its own category of failures — silent conflicts, unintended overrides, and composition chains so deep that nobody can predict what a given customer's effective rubric actually evaluates.

## Why Inheritance Is Necessary

Before diagnosing the problems, it helps to understand why every multi-tenant rubric system converges on inheritance. The alternative — fully independent per-customer rubrics — works at ten customers and collapses at a hundred.

With fully independent rubrics, every quality improvement requires updating each customer's rubric individually. When your platform learns that citation accuracy should be evaluated differently — perhaps because a new evaluation technique produces more reliable scores — you must propagate that change to every customer who evaluates citation accuracy. At twenty customers, this is a morning's work. At two hundred customers, it is a multi-week project that requires verifying that the change doesn't break per-customer calibrations. At five hundred, it is effectively impossible to keep all rubrics current. The customers onboarded first have rubrics built on evaluation methodology from two years ago. The customers onboarded last month have rubrics built on current methodology. The same dimension is evaluated differently across different customers, not because they have different quality requirements but because they were onboarded at different times.

Inheritance solves this propagation problem. You define a base rubric that contains the platform's current best evaluation methodology for each quality dimension. Customers inherit from the base. When you improve a dimension's evaluation methodology, you update the base rubric, and every customer who inherits that dimension gets the improvement automatically. Only customers who have explicitly overridden the dimension retain their custom version. This is efficient, maintainable, and theoretically clean. The theory breaks down in three specific ways.

## Failure Mode One: The Silent Override Conflict

A financial services platform serving 160 enterprise clients introduced a three-layer rubric hierarchy in early 2025: a platform base rubric, industry-specific rubric layers for healthcare, finance, legal, and retail, and per-customer overrides on top. The system was elegant. A healthcare customer inherited the platform base, then the healthcare industry layer overrode three dimensions to add medical terminology accuracy and conservative confidence language, then the customer's own overrides adjusted two thresholds.

Nine months later, the platform team updated the base rubric's completeness dimension. The old methodology measured whether the output addressed all topics in the input. The new methodology measured whether the output addressed all topics with appropriate depth — a meaningfully different and better evaluation. The update propagated to every customer who had not overridden the completeness dimension. Sixty-two customers got the improvement. But fourteen customers in the healthcare industry layer had an industry-level override on completeness that predated the base update. That override did not merely adjust the threshold — it replaced the completeness methodology entirely, because the healthcare layer needed to evaluate completeness against clinical documentation standards rather than general topic coverage.

The conflict was invisible. The system resolved it by giving the healthcare layer's override priority over the base update — which is the correct behavior under inheritance rules. But the healthcare team that had written the industry override eight months earlier had not anticipated that the base methodology would change. Their override was designed to modify the old completeness methodology. When the base changed, the override continued to function, but it was now overriding a methodology it had never been designed to interact with. The effective completeness evaluation for healthcare customers was neither the new base methodology nor the intended healthcare adaptation. It was a hybrid that produced internally inconsistent scores.

The platform discovered the conflict three months later when a healthcare customer questioned why their completeness scores were fluctuating by 12 points week over week on stable outputs. The investigation revealed that the override was interacting unpredictably with the new base methodology, producing score instability that neither the base nor the override would produce independently. Resolving the conflict required the healthcare team to rewrite their override against the new base methodology — work that should have been part of the original base update but wasn't, because the system had no mechanism to flag that an industry override might be affected by a base change.

## Failure Mode Two: The Deep Inheritance Chain

Inheritance chains grow deeper than anyone intends. The platform base rubric defines the foundation. The industry layer modifies it. The customer-tier layer modifies the industry layer for premium versus standard customers. The customer-specific layer modifies the tier layer. A special configuration for one customer's regulated division modifies the customer layer. Five levels deep, the effective rubric is the product of five composition steps, each applied in sequence, each potentially conflicting with the others.

At this depth, a fundamental problem emerges: nobody can predict what the effective rubric actually evaluates without tracing the full inheritance chain. An engineer looking at a customer's quality score of 87 cannot tell whether that score reflects the base rubric's methodology, the industry layer's overrides, the tier adjustments, or the customer-specific modifications. Debugging a quality dispute requires walking the chain step by step, resolving conflicts at each level, and understanding the interaction effects between overrides. This is the rubric equivalent of debugging a deeply nested class hierarchy in object-oriented programming — every layer adds behavior that is individually reasonable and collectively incomprehensible.

A document intelligence platform discovered this problem in mid-2025 when an engineer attempted to explain a quality score to a premium healthcare customer. The customer's effective rubric was the product of four inheritance levels. The engineer spent two days tracing the chain and discovered that two overrides at different levels had contradictory effects on the same dimension — one increased the weight of citation accuracy, the other decreased the weight of the broader accuracy category that contained citation accuracy. The net effect was unpredictable and depended on the order in which the overrides were applied. The engineer could not explain the score because the score was not explainable — it was the output of a composition that nobody had designed as a whole.

## Failure Mode Three: The Orphaned Override

Overrides are created for specific reasons. A customer needs conservative confidence language, so they override the tone dimension. An industry layer needs specialized terminology scoring, so it overrides the vocabulary dimension. Over time, the reason for the override becomes detached from the override itself. The person who created the override leaves the company or moves to a different team. The documentation, if it existed, becomes stale. The override persists in the system, actively modifying the customer's evaluation, but nobody knows why it exists or whether it is still necessary.

Orphaned overrides accumulate. A platform that has been operating for two years with 200 customers and three industry layers might have 400 to 600 active overrides. Of those, industry experience suggests that 20 to 30 percent are orphaned — they exist because someone created them, but the reason is lost. Removing them is risky because the override might be the only thing preventing a quality regression for the affected customer. Keeping them is wasteful because they add complexity to the inheritance chain without known purpose. They are the rubric equivalent of dead code — except that dead code is inert and orphaned overrides actively affect customer quality scores.

## Composition Rules That Work

The inheritance problems described above are not inherent to the model. They are consequences of under-specified composition rules. A rubric inheritance system that defines clear composition rules avoids the worst failure modes.

**Rule one: maximum depth of three.** The inheritance chain should never exceed three levels: platform base, one intermediate layer, and customer-specific overrides. Three levels are enough to capture platform-wide methodology, vertical or tier-specific adjustments, and customer customization. Deeper chains add composition complexity without proportional value. If a customer's requirements cannot be expressed in three levels, the customer needs a fully independent rubric, not a deeper chain.

**Rule two: override-by-replacement, not override-by-modification.** When a customer overrides a dimension, they replace the entire dimension definition — methodology, scoring criteria, and threshold. They do not modify the inherited dimension. This prevents the silent override conflict described earlier, where an override designed to modify one version of a dimension interacts unpredictably with a different version after a base update. Override-by-replacement is less flexible but dramatically more predictable. The customer's effective rubric for any overridden dimension is exactly what they defined, with no inheritance interaction effects.

**Rule three: explicit conflict detection on base updates.** When the platform base rubric changes, the system must identify every override that touches the same dimension and flag it for review. Not automatic resolution — human review. An engineer or quality specialist examines each flagged override, determines whether it is still compatible with the updated base, and either confirms the override or marks it for customer-level revision. This review is the work that the financial services platform skipped and paid for with three months of score instability.

**Rule four: override provenance.** Every override must record who created it, when, why, and what business requirement it addresses. This is the anti-orphaning discipline. The provenance record is not documentation for its own sake — it is the information that allows a future engineer to determine whether the override is still needed. Overrides without provenance should be flagged for review on a quarterly cycle. If no one can explain why an override exists, it is a candidate for deprecation through a controlled experiment: temporarily disable it, observe the effect on the customer's quality scores, and remove it permanently if scores are unaffected.

**Rule five: effective rubric materialization.** For every customer, the system should be able to produce the complete, resolved, effective rubric — the result of applying all inheritance and overrides — as a single flat document. This materialized rubric is what actually drives evaluation. It is what the engineer consults when debugging a quality score. It is what the customer reviews during quality calibration sessions. The inheritance chain is the mechanism. The materialized rubric is the artifact that humans interact with. If the system cannot produce a clean, readable, fully resolved rubric for any customer at any time, the inheritance system is too complex.

## The Composition Testing Discipline

Rubric inheritance systems need tests, just as software inheritance systems do. Without tests, changes to the base rubric or intermediate layers propagate unverified and produce the failure modes described above.

The most effective testing approach is **golden output regression testing per composition level**. You maintain a set of reference outputs — ten to twenty per industry layer, five to ten per customer tier — that have known expected scores under the current rubric composition. When any layer in the inheritance chain changes, you re-score these reference outputs and compare the results to the expected scores. Deviations beyond a tolerance threshold trigger a review before the change propagates to production. This test catches silent override conflicts, unintended inheritance interactions, and methodology changes that produce unexpected effects at downstream layers.

The golden output set must be maintained as the rubric evolves. When a base update legitimately changes how completeness is scored, the expected scores on the golden outputs must be updated to reflect the new methodology. This is work. It is unavoidable work. The alternative is deploying rubric changes without knowing their per-customer effects — which is how the financial services platform ended up with three months of score instability and a customer questioning whether the platform's evaluation system could be trusted.

## When to Abandon Inheritance

Inheritance is not always the right model. Some customers have quality requirements so different from the platform base that the inheritance chain does more harm than good. A government defense contractor whose rubric shares two dimensions with the platform base but adds twelve custom dimensions is not inheriting a rubric. They are maintaining a fully independent rubric that happens to share two dimensions with the base. The inheritance overhead — tracking overrides, resolving conflicts, tracing composition chains — exceeds the benefit of sharing those two dimensions.

The signal that a customer should exit the inheritance model is the **override ratio**. If a customer has overridden more than 60 percent of the base rubric's dimensions, the inheritance chain is not providing meaningful efficiency. The customer should be migrated to a fully independent rubric that is maintained separately. This increases maintenance cost for that customer but eliminates the composition complexity that makes their rubric unpredictable. For most platforms, 5 to 15 percent of customers will need fully independent rubrics. The other 85 to 95 percent benefit from inheritance — as long as the composition rules are disciplined enough to prevent the failure modes described above.

## The Organizational Ownership Question

Rubric inheritance creates an ownership question that pure per-customer rubrics do not have. Who owns the base rubric? Who owns the industry layers? Who reviews overrides? Who approves base updates that affect downstream customers?

The most effective model assigns ownership explicitly. A platform evaluation team owns the base rubric and is responsible for its methodology, its updates, and its testing. Industry-specific quality specialists own their respective layers and are responsible for compatibility with the base. Customer success managers own per-customer overrides and are responsible for their provenance and relevance. Base rubric updates require sign-off from industry layer owners before deployment. Industry layer updates require notification to customer success managers for affected customers.

This ownership model creates governance overhead. It also prevents the alternative: a system where anyone can modify any layer without review, composition conflicts accumulate silently, and the first sign of trouble is a customer escalation about inexplicable quality scores. The overhead is the cost of predictability. In a multi-tenant system where quality scores affect customer retention, pricing, and regulatory compliance, predictability is worth the cost.

The inheritance model, properly governed, gives you the best of both worlds — shared methodology that propagates improvements efficiently and per-customer customization that respects each customer's unique quality requirements. Improperly governed, it gives you the worst of both worlds — shared fragility that propagates failures efficiently and per-customer complexity that makes every quality score suspect. The difference is composition discipline. The next subchapter addresses a dimension that composition rules alone cannot solve: the fact that quality definitions change over time, and a rubric that was perfect at onboarding may be obsolete within a year.