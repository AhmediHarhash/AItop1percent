# 28.5 — When Customers Cannot Articulate What Good Looks Like

The **blank-slate customer problem** is the most common and most quietly destructive pattern in multi-tenant quality management. It works like this: a new enterprise customer signs a contract. The contract says "high-quality AI outputs" or "accuracy consistent with industry standards." The onboarding team asks the customer to define their quality requirements so the platform can configure per-customer evaluation. The customer responds with some variation of "we want it to be good" or "we'll know it when we see it." The onboarding team logs the request as incomplete, configures the customer on the platform's default rubric, and moves on. Six months later, the customer escalates because the outputs don't match what they expected. When pressed for specifics, they can now articulate exactly what "good" means — but only because they have spent six months accumulating examples of what "bad" looks like. The quality definition arrives through frustration rather than through design.

This pattern played out at a legal technology platform in mid-2025 that served 180 law firm customers. During onboarding, the platform asked each new customer to define quality criteria for contract analysis outputs. Fewer than 15 percent of customers provided anything beyond "accurate and comprehensive." The platform defaulted the remaining 85 percent to its standard rubric. Over the following year, 23 of those default-rubric customers escalated quality complaints. In every case, the customer's actual quality requirements diverged from the platform default in specific, predictable ways — one firm cared intensely about citation format, another about the treatment of ambiguous clauses, another about the level of confidence language used in summaries. These requirements were knowable at onboarding. The customers just could not articulate them unprompted.

## Why Customers Cannot Define Quality

The inability to define quality is not laziness, and treating it as such is the first mistake platform teams make. Enterprise customers cannot articulate quality requirements for three distinct reasons, and each requires a different intervention.

The first reason is **domain-expert blindness**. Domain experts who have spent decades in their field have internalized quality standards so deeply that they cannot consciously enumerate them. A senior radiologist who reviews AI-generated findings knows instantly whether the output is acceptable, but asking her to list every criterion she evaluates is like asking a native speaker to list every grammar rule they follow. The knowledge is procedural, not declarative. It lives in pattern recognition, not in checklists. The customer doesn't know what good looks like because they know it so well they cannot decompose it into components.

The second reason is **outcome confusion**. Many enterprise customers define quality in terms of outcomes rather than attributes. They say "we want outputs that don't create compliance risk" or "we need summaries our analysts can trust." These are valid business requirements, but they are not evaluable criteria. You cannot build a rubric dimension for "doesn't create compliance risk" because the definition of compliance risk depends on dozens of specific attributes — citation accuracy, hedging language, data provenance, regulatory terminology — that the customer has not unpacked. The customer is describing what quality achieves, not what quality contains.

The third reason is **no prior reference frame**. For customers deploying AI for the first time in a given workflow, there is no existing standard to articulate against. They have quality standards for human-produced work, but they have never translated those standards into criteria applicable to AI outputs. The dimensions that matter for evaluating a human analyst's report — such as reasoning depth, context sensitivity, and judgment calls — map imperfectly onto AI output evaluation. The customer genuinely does not know what good AI output looks like because they have never seen enough AI output to form a calibrated opinion.

## The Danger of the Default Rubric

When a customer cannot define quality, the path of least resistance is to assign the platform's default rubric and move on. This is administratively efficient and evaluatively dangerous.

The default rubric reflects the platform team's best guess at universal quality. It measures the dimensions that matter across all customers — accuracy, coherence, completeness, format compliance. These dimensions are necessary but almost never sufficient. Every enterprise customer has at least two or three quality dimensions that the default rubric does not capture, and those uncaptured dimensions are typically the ones the customer cares about most. The legal firm that needs citation-format precision. The healthcare company that needs conservative confidence language. The financial services firm that needs outputs to match their internal terminology. These are the dimensions that determine whether the customer renews, and they are invisible to the default rubric.

The deeper problem is that the default rubric creates a false sense of measurement. Your per-customer quality dashboard shows a score. The customer sees the score. The score looks reasonable. But the score is measuring the wrong things. A customer can have a 94 percent quality score on the default rubric and a 62 percent score on the criteria they actually care about — criteria that nobody has defined yet. The dashboard says green. The customer's experience says red. And neither party knows this until the gap produces enough frustration to trigger an escalation.

## Structured Discovery: The Extraction Protocol

The solution to the blank-slate problem is not to wait for the customer to define quality. It is to extract quality definitions through a structured discovery process that does not require the customer to articulate criteria from scratch. The most effective extraction protocol has four stages, each designed for a different type of customer blindness.

**Stage one is comparative judgment.** Instead of asking "what does good look like?" you show the customer ten pairs of outputs and ask "which is better?" For each pair, the outputs differ on one or two specific dimensions — one has more formal language, one is more concise, one includes citations, one uses hedging language. The customer chooses. After twenty to thirty comparisons, you have a preference map that reveals which dimensions the customer prioritizes and which they tolerate variation on. Comparative judgment works because it converts a generative task — "describe quality" — into a discriminative task — "choose the better one." Discriminative tasks are cognitively easier and activate the same tacit knowledge that the customer cannot consciously articulate.

**Stage two is failure annotation.** You give the customer twenty real outputs from their domain and ask them to mark anything they would not accept. Not what they like — what they reject. Failures are easier to identify than successes because they trigger stronger reactions. A legal professional who cannot list every quality criterion will immediately circle speculative language in a contract summary. A compliance officer who cannot define "good" will instantly flag an output that uses industry jargon their regulators would question. Failure annotation reveals the boundaries of acceptable quality — the lines the customer draws even when they cannot draw a map.

**Stage three is dimension elicitation through scenarios.** You present the customer with specific scenarios and ask targeted questions. "If the model encounters an ambiguous clause, should it flag the ambiguity or interpret it? If there are multiple valid interpretations, should the output present all of them or choose the most likely? If the model lacks confidence in a finding, should it say so explicitly or omit the finding?" Each scenario forces the customer to make a choice about a specific quality dimension. After fifteen to twenty scenarios, you have explicit positions on the dimensions that will become their rubric.

**Stage four is calibration review.** You take the preference map from comparative judgment, the rejection patterns from failure annotation, and the dimension positions from scenario elicitation, and you synthesize them into a draft rubric. You present this rubric to the customer with five sample outputs scored against it. The customer reviews the scores. Invariably, they disagree with at least one or two scores — "that one should be higher because the citation format is exactly right" or "that one should be lower because the summary missed the key risk factor." Each disagreement reveals a rubric refinement. Two rounds of calibration review typically produce a rubric that the customer recognizes as their own, even though they never could have written it from scratch.

## The Time Investment and Its Returns

The structured discovery protocol takes time. A full extraction, from comparative judgment through calibration review, requires three to five hours of customer engagement spread across two to three sessions. For a platform onboarding twenty new enterprise customers per month, that is sixty to one hundred hours of discovery work monthly. This is not trivial. Teams resist it because it slows onboarding and requires skilled facilitators — not account executives, not junior customer success managers, but people who understand both the customer's domain and evaluation methodology.

The return, however, is disproportionate. The legal technology platform that introduced structured discovery in early 2026 tracked the outcomes for customers who went through the protocol versus customers who had been onboarded with default rubrics the previous year. Customers with extracted quality definitions had 73 percent fewer quality escalations in their first six months. Their six-month renewal rate was 94 percent, versus 81 percent for default-rubric customers. The average time to resolve a quality complaint dropped from 11 days to 3 days because the per-customer rubric gave the support team a shared reference frame — they could evaluate the complaint against the customer's own criteria instead of investigating from scratch. The discovery investment paid for itself within the first customer escalation it prevented.

## Proxy Bootstrapping When Discovery Is Not Possible

Not every customer will participate in structured discovery. Some enterprises send a procurement team to onboard, and the domain experts who actually use the platform are unavailable until after go-live. Some customers are too small to justify five hours of discovery. Some customers genuinely do not care about customization — they want a working product with standard quality. For these customers, you need a bootstrapping strategy that creates a reasonable quality definition without full discovery.

The most effective bootstrapping approach is **industry-vertical proxy rubrics**. If you serve fifty healthcare customers and a new healthcare customer cannot define quality, you start them on the rubric that most healthcare customers converge toward after discovery. This proxy rubric is not a guess. It is derived from the aggregated preferences of previous customers in the same vertical who did complete discovery. Over time, each vertical develops a characteristic quality profile — healthcare customers tend to weight citation accuracy and conservative language heavily, financial services customers weight numerical precision and regulatory terminology, legal customers weight citation format and treatment of ambiguity. The proxy rubric captures these vertical-specific tendencies and provides a starting point that is dramatically better than the platform default.

The proxy rubric is always marked as provisional. The customer is told explicitly: "We've configured your evaluation based on quality patterns typical of healthcare organizations. Within your first 90 days, we'll review your quality scores and refine the rubric based on your specific needs." This sets the expectation that the rubric will evolve. It also creates a natural checkpoint — the 90-day review — where you can run a lightweight version of the discovery protocol informed by three months of actual outputs.

## When "We'll Know It When We See It" Is the Answer

Some customers resist structured discovery not because they lack knowledge but because they genuinely believe quality is subjective and cannot be parameterized. "Our partners review every output anyway — they'll catch anything wrong." This response is more dangerous than the blank slate because it signals that the customer intends to use human review as their quality definition, which means your eval system has nothing to measure against.

The intervention for this customer is not to argue that quality can be parameterized. It is to show them that it already is — they just haven't noticed. You take twenty outputs that their reviewers have already accepted or rejected and analyze the rejection patterns. You demonstrate that the rejections cluster around three or four specific dimensions. You show them that "we'll know it when we see it" is actually "we reject outputs that use speculative language, miss key risk factors, or deviate from our formatting standard." The quality definition exists. It is embedded in the reviewers' behavior. Your job is to surface it, name it, and configure your eval system to measure it.

The worst outcome is accepting "we'll know it when we see it" at face value and leaving the customer on the default rubric. This guarantees that the first quality dispute will be unresolvable — you have no customer-specific criteria to evaluate against, the customer has no explicit standard to point to, and both sides argue from intuition rather than evidence. The cost of that first unresolvable dispute, measured in engineering hours, customer relationship damage, and potential commercial concessions, far exceeds the cost of structured discovery.

## The Organizational Requirements

Structured discovery does not happen naturally. It requires three organizational commitments that most platform teams have not made.

The first commitment is a **discovery-trained team**. The people running discovery sessions need dual expertise — enough domain knowledge to understand the customer's context and enough eval expertise to translate preferences into rubric dimensions. Most platforms do not have this role. Customer success teams understand the customer but not evaluation methodology. Eval teams understand methodology but not the customer's domain. The discovery facilitator sits at the intersection. Some platforms create this as a dedicated role. Others train senior customer success managers in evaluation methodology. Either approach works. What does not work is assigning discovery to whoever happens to be available.

The second commitment is **discovery tooling**. Comparative judgment requires a tool that presents output pairs and captures preferences. Failure annotation requires a tool that lets customers mark and categorize rejections. Scenario elicitation requires a structured questionnaire that adapts to the customer's domain. Calibration review requires a tool that shows outputs alongside rubric scores. These tools do not need to be sophisticated — some platforms run discovery using shared documents and spreadsheets. But they need to exist, and the process needs to be standardized enough that every discovery session produces comparable outputs regardless of which facilitator runs it.

The third commitment is **executive buy-in for onboarding time**. Discovery adds three to five hours to the onboarding process. In organizations that measure onboarding by time-to-go-live, this addition is unwelcome. Leadership must understand that the alternative to three hours of discovery is not zero hours — it is twenty hours of quality escalation investigation six months later, plus the risk of losing the customer entirely. Framing discovery as onboarding cost reduction rather than onboarding time increase makes the argument concrete. The legal technology platform calculated that every dollar spent on discovery saved four dollars in future escalation costs. That ratio converts executive skepticism into executive advocacy.

The blank-slate customer problem is not a customer problem. It is a platform problem. You are the expert in AI quality evaluation. The customer is the expert in their domain. Structured discovery is the protocol that connects those two expertises into a quality definition that neither party could have produced alone. The next subchapter tackles the challenge that follows naturally from quality definition — translating those definitions into SLA metrics that both your engineering team and the customer's legal team can hold each other to.