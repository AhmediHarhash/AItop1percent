# 28.8 — Model Migration Across a Diverse Customer Base

In March 2025, a document intelligence platform serving 280 enterprise customers across legal, financial services, and insurance verticals began planning its migration from Llama 3 70B to Llama 4 Maverick as its primary base model. The migration was not optional. Llama 3 was approaching end-of-community-support, the platform's customers were requesting the improved multilingual capabilities and longer context windows that Llama 4 offered, and the platform's competitors had already announced Llama 4 support. The engineering leadership estimated the migration would take eight weeks. It took seven months.

The technical migration — swapping the model weights, updating the serving infrastructure, adjusting the tokenizer — took eleven days. The other six and a half months were consumed by per-tenant validation, adapter retraining, customer communication, quality regression investigation, holdout management, and the infrastructure cost of running both models simultaneously while customers migrated at different speeds. The platform's head of engineering later described the experience as "the most expensive eleven-day project in the company's history," because the eleven days of technical work generated seven months of operational complexity.

This is the reality of major model migrations in multi-tenant systems. The model swap is the easy part. The hard part is that every tenant has built their quality expectations, their adapter weights, their prompt architectures, and their evaluation baselines around the current model's specific behaviors. Moving to a new model does not just change the outputs. It invalidates every tenant-specific artifact that was optimized for the old model's characteristics. Each of your 280 customers is, in effect, receiving a new AI system that happens to share a name with the old one.

## Why Model Migrations Are Inevitable

No base model lasts forever. The model that is your platform's foundation today will be superseded within 12 to 18 months by a model that is meaningfully better — better at reasoning, better at following instructions, better at handling long contexts, cheaper to serve, or required by new regulatory frameworks. The pace of model generations in 2024 through 2026 has been relentless. GPT-4o gave way to GPT-5 and then GPT-5.1 and GPT-5.2. Claude 3.5 Sonnet gave way to Claude Opus 4.5 and Claude Opus 4.6. Llama 3 gave way to Llama 4 Scout and Llama 4 Maverick. Gemini 2 Pro gave way to Gemini 2.5 and then Gemini 3 Pro. Each generation brought capabilities that customers demanded and competitors shipped.

A platform that refuses to migrate models stagnates. A platform that migrates without a structured process causes chaos. The only viable path is structured, phased migration that treats the process as a multi-month operational project rather than a multi-week technical one.

The triggers for migration are typically one of four: the current model reaches end-of-support or end-of-availability from the provider, a new model offers capabilities that customers are requesting, competitive pressure from platforms that have already adopted the new model, or cost advantages that the new model offers — Llama 4 Maverick's mixture-of-experts architecture, for example, reduced per-token inference cost by roughly 35 percent compared to Llama 3 70B for equivalent quality, a savings that compounds across hundreds of tenants.

## Parallel Running: The Non-Negotiable First Step

Every model migration must begin with **parallel running** — operating both the old and new models simultaneously and routing identical traffic to both. Parallel running is expensive, typically doubling your inference infrastructure costs for the duration. It is also non-negotiable, because it is the only way to measure the actual per-tenant impact of the migration before committing to it.

Parallel running serves three purposes. First, it generates per-tenant quality comparison data. For each tenant, you can compare the old model's outputs against the new model's outputs on the same inputs, scored against the same rubric, evaluated against the same golden set. This comparison produces a **migration readiness score** for each tenant — a quantitative answer to the question "how does this tenant's quality change if we switch models?"

Second, parallel running reveals adapter compatibility. Per-tenant LoRA adapters trained against the old model will not transfer directly to the new model if the base architecture changed, as it did between Llama 3 and Llama 4. Parallel running with adapter-less configurations establishes the new model's baseline performance for each tenant, which becomes the starting point for adapter retraining. Parallel running with adapters applied to both models (old adapters on the old model, newly retrained adapters on the new model) validates that the retrained adapters recover the quality that the original adapters provided.

Third, parallel running generates the evidence that customers need to approve the migration. For gated-change customers — those who require explicit approval before any model change, as covered in the previous subchapter — the parallel running data is the migration proposal. It shows the customer exactly what will change, by how much, and in which dimensions. A customer presented with "the new model improves your factual accuracy by 3 points, maintains your tone scores, and shows a 1.5-point regression in format compliance that we have a plan to address" is far more likely to approve than a customer told "we are switching models and we think it will be better."

The parallel running phase typically lasts four to six weeks. Shorter than that and you do not have enough data to generate statistically meaningful per-tenant comparisons. Longer than that and the infrastructure costs become difficult to justify. The sweet spot depends on tenant traffic volume — high-traffic tenants generate meaningful comparison data in two weeks, while low-traffic tenants may need the full six weeks.

## Per-Tenant Migration Readiness Assessment

Not every tenant is equally ready to migrate. The **migration readiness assessment** evaluates each tenant across four dimensions and produces a readiness classification that determines when the tenant migrates.

The first dimension is **quality parity**. Does the new model produce quality scores within an acceptable tolerance of the old model for this tenant's specific use case and configuration? The tolerance is typically defined in the customer's SLA — often plus or minus two points on the composite score and plus or minus five points on any individual dimension. Tenants where the new model meets or exceeds the old model's quality are classified as migration-ready. Tenants where the new model shows regressions outside tolerance require intervention before migration.

The second dimension is **adapter readiness**. Has the tenant's per-tenant adapter been retrained against the new base model and validated against the tenant's golden set? Adapter retraining is a prerequisite for any tenant that uses custom adapters. The retraining process must be completed, the new adapter must be evaluated, and the results must meet the tenant's quality thresholds before the tenant can migrate. For a platform with 280 tenants, 180 of whom use custom adapters, adapter retraining is a logistics operation that requires scheduling compute resources, managing training queues, and prioritizing tenants based on their migration wave assignment.

The third dimension is **configuration compatibility**. Does the tenant's prompt architecture, retrieval configuration, tool access setup, and post-processing pipeline work correctly with the new model? Model migrations frequently break prompts that were tuned for the old model's specific instruction-following behavior. A prompt that worked perfectly with Llama 3's instruction format may produce degraded outputs with Llama 4's different tokenization or response structure. Configuration compatibility testing must cover the tenant's full configuration surface, not just a sample.

The fourth dimension is **customer approval**. Has the customer acknowledged the migration plan and, for gated-change customers, provided explicit approval? Some customers approve based on the parallel running data alone. Others require their own testing period in a staging environment. A few require legal review of the new model's license terms — Meta's enterprise licensing for Llama 4, for example, introduced new terms around usage reporting that some customers' legal teams needed to review.

## The Migration Schedule

Model migration across hundreds of tenants is not a single event. It is a phased rollout that typically spans eight to twelve weeks after parallel running completes, with tenants migrating in waves based on their readiness classification.

**Wave one** includes tenants classified as fully ready — quality parity confirmed, adapters retrained and validated, configurations compatible, customer approval received. These are your easiest migrations and your best source of early confidence data. Wave one typically includes 15 to 20 percent of the tenant base. The wave-one migration should be monitored intensively, with per-tenant quality reports generated within 24 hours of migration. Any unexpected quality issues in wave one inform adjustments to the process for subsequent waves.

**Wave two** includes tenants that are ready with minor interventions — perhaps a prompt adjustment, a threshold recalibration, or a configuration tweak that addresses a minor incompatibility. Wave two typically migrates two to three weeks after wave one, once the wave-one results have been analyzed and any process issues have been resolved. Wave two is usually the largest wave, covering 40 to 50 percent of the tenant base.

**Wave three** includes tenants with more significant challenges — quality regressions that required targeted adapter retraining, configuration incompatibilities that required custom fixes, or customers who needed additional time for their own testing and approval processes. Wave three migrates three to five weeks after wave two and covers 20 to 30 percent of the tenant base.

**Holdouts** are tenants who cannot or will not migrate within the standard timeline. Some have unresolved quality regressions that the platform team has not yet fixed. Some have contractual terms that specify the model version they run. Some have customers whose legal or compliance teams have not completed their review. Holdouts remain on the old model indefinitely, at least until their specific blockers are resolved. Managing holdouts is covered in detail later in this chapter.

## The Cost of Dual Infrastructure

During the migration period, your platform runs two base models simultaneously. For a platform serving 280 tenants on GPU infrastructure, the incremental cost of the dual deployment is significant. If the old model requires 16 A100 GPUs for the platform's peak traffic and the new model requires similar capacity, the dual deployment doubles GPU costs during the migration period. At cloud GPU pricing in early 2026, that is roughly $25,000 to $40,000 per week in incremental infrastructure costs, depending on your cloud provider and reservation strategy.

The total infrastructure cost of a 12-week migration with a 6-week parallel running phase is therefore $450,000 to $720,000 in incremental compute alone, before accounting for the engineering time to manage the migration, the adapter retraining compute, and the customer communication overhead. A 16-week migration — common when holdouts extend the timeline — pushes the total above $800,000.

This cost must be planned, budgeted, and justified before the migration begins. Engineering leaders who propose model migrations without infrastructure cost projections get caught between finance teams who demand the old model be decommissioned immediately and customer teams who demand the old model be maintained until every tenant is ready. The conflict between these two pressures — cost reduction versus customer readiness — defines the tension of every multi-tenant model migration.

The resolution is a **decommission schedule** that balances both pressures. The old model is maintained at full capacity until wave two completes — at that point, 60 to 70 percent of tenants have migrated, and the traffic to the old model has dropped enough to scale down the infrastructure. After wave three, the old model infrastructure is reduced to the minimum needed to serve holdouts, typically 20 to 30 percent of the original capacity. This stepped decommission reduces the incremental cost while maintaining service for tenants who have not yet migrated.

## Handling Holdouts

Holdouts are the tenants who remain on the old model after the standard migration timeline. Every migration has them. A platform migrating 280 tenants should expect 15 to 30 holdouts — tenants who cannot migrate for technical, contractual, or organizational reasons. Managing holdouts is an operational discipline that most platforms underestimate.

The first holdout category is **technical holdouts** — tenants whose quality on the new model does not meet their SLA thresholds despite adapter retraining and configuration adjustments. These tenants need targeted engineering work. Perhaps their use case exploits a specific capability of the old model that the new model handles differently. Perhaps their adapter requires a different training approach for the new architecture. Technical holdouts typically resolve within four to eight weeks after the standard migration completes, as the platform team works through the specific issues.

The second category is **contractual holdouts** — tenants whose contracts specify the model version or model family. These contracts were written when the platform ran a specific model, and the customer's legal team has not yet approved the change to a different model. Contractual holdouts require legal engagement, contract amendment, and sometimes renegotiation of terms. They can take three to six months to resolve.

The third category is **organizational holdouts** — tenants whose internal approval processes have not completed. The technical team approves, but the compliance team needs three more weeks. Or the CTO approves, but the board requires a briefing. Organizational holdouts are resolved by patience and by providing the customer with the documentation their internal process requires — migration impact assessments, model comparison reports, data processing addendums, and security reviews of the new model's characteristics.

The cost of holdouts is concrete. Each holdout requires the platform to maintain the old model infrastructure for longer than planned. If 25 holdouts keep the old model running for an additional eight weeks beyond the planned decommission date, that is $100,000 to $160,000 in incremental infrastructure costs. This cost should be tracked per-holdout and reported to engineering leadership so that holdout resolution receives appropriate prioritization.

## Success Criteria for Migration

The migration is not complete when the last tenant switches models. It is complete when every migrated tenant's quality is stable, their evaluation baselines have been re-established on the new model, and their quality trends are tracking as expected.

**Per-tenant quality stability** means that the tenant's composite quality score on the new model is within the SLA-defined tolerance of their pre-migration score for at least three consecutive evaluation cycles. A single evaluation cycle can be noisy. Two consecutive cycles can be coincidence. Three consecutive cycles within tolerance is evidence of stability.

**Baseline re-establishment** means that the tenant's quality trend line has been reset to account for the model change. The pre-migration and post-migration periods should be clearly delineated in the tenant's quality reporting, with the model transition annotated as a known change event. Trend analysis that spans the migration boundary without this annotation will show a discontinuity that looks like a quality event rather than a planned migration.

**Adapter performance validation** means that every retrained adapter has been evaluated against the tenant's golden set and produces quality scores that meet or exceed the pre-migration adapter's performance. Adapters that were retrained during the migration may need one or two additional training iterations after migration completes, as the production traffic distribution often differs subtly from the training data used during the parallel running phase.

The migration completion report, shared with engineering leadership and customer success leadership, should document the total migration duration, the per-wave results, the holdout count and resolution timeline, the total infrastructure cost, the per-tenant quality impact, and the lessons learned for the next migration. Because there will be a next migration. Model generations are accelerating. The platform that builds a repeatable migration process during its first major migration completes its second migration in half the time.

As discussed in Chapter 4, the interaction between adapter weights and base model weights is the source of the most complex migration failures. What this chapter has not yet addressed is what happens when adapters age out entirely — when the passage of time, not a deliberate migration, causes tenant-specific fine-tunes to degrade against a base model that has continued to evolve. The next subchapter covers adapter drift and the lifecycle management practices that prevent silent quality erosion across your tenant base.
