# 28.2 — The Three Isolation Layers: Data, Compute, and Reporting

Isolation in multi-tenant evaluation operates at three distinct layers, and a failure at any one of them is a failure at all of them. You can encrypt every byte of tenant data at rest, enforce row-level access controls in every database query, and still leak tenant information through a shared compute node that carries state between evaluation jobs. You can isolate data and compute perfectly and still expose cross-tenant information through a reporting dashboard that computes percentile rankings across the full tenant population. The three layers are not independent safety nets where one catches what another misses. They are sequential gates where a breach at any single gate compromises the entire isolation guarantee.

This is **The Isolation Boundary** — the principle that tenant separation must be enforced simultaneously at the data layer, the compute layer, and the reporting layer, with each layer independently verified and each layer assuming the others might fail. The moment you design any layer with the assumption that "the other layers will catch it," you have created the gap through which contamination enters.

## Layer One: Data Isolation

Data isolation is the foundation. If tenant data is not separated at the storage and access level, nothing built on top of it can be trusted. In evaluation systems, data isolation means that every piece of data used in a tenant's evaluation — production samples, golden set examples, annotation results, judge outputs, historical scores — is accessible only to processes authorized for that tenant. No query, no batch job, no debugging session should ever return data belonging to a tenant other than the one being evaluated.

The implementation approaches range from fully shared to fully separated, and the right choice depends on your scale, your regulatory exposure, and your customers' contractual requirements. At one end of the spectrum is **database-level separation**, where each tenant's evaluation data lives in its own database instance. This provides the strongest isolation guarantee because the tenant boundary is enforced by the database engine itself. A query against Tenant A's database cannot return Tenant B's data because Tenant B's data does not exist in that database. The tradeoff is operational overhead: 500 tenants means 500 database instances, each requiring provisioning, backup, monitoring, and schema migration coordination. Cloud-managed databases reduce but do not eliminate this overhead. Healthcare and financial services platforms, where regulatory requirements demand demonstrable data separation, often adopt this model despite the cost because the regulatory benefit outweighs the operational burden.

At the other end is **shared-table isolation**, where all tenants' evaluation data lives in the same tables, with a tenant identifier column that scopes every query. This is operationally simpler — one database, one schema, one migration process — but the isolation guarantee rests entirely on the correctness of query filters. Every query in the evaluation pipeline must include a tenant filter. Every ad-hoc query run by an engineer during debugging must include a tenant filter. Every batch job that processes evaluation data must scope its reads and writes by tenant. A single query missing a tenant filter — whether in production code, a migration script, or a one-off analytics query — produces cross-tenant data exposure. The clinical documentation platform described in the previous subchapter used shared-table isolation and lost it to a query filter bug introduced during a database migration.

Between these extremes is **schema-level separation**, where each tenant has its own schema within a shared database instance. This provides stronger isolation than shared tables because schema-level access controls prevent cross-schema queries without explicit permissions, but it is operationally lighter than separate database instances because the infrastructure is shared. Schema-level separation works well for platforms with 50 to 200 tenants. Beyond 200, the number of schemas in a single database instance can create performance and management challenges that push teams toward partitioned database clusters.

Whichever model you choose, data isolation requires three enforcement mechanisms that most teams implement but few verify rigorously. The first is **access control enforcement** — every data access path in the evaluation pipeline must be validated against the tenant context of the current evaluation job. This includes not just primary data reads but also reference data lookups, cache reads, and metadata queries. The second is **write isolation** — evaluation results, intermediate scores, and audit logs must be written to tenant-scoped storage, never to shared locations where another tenant's evaluation process could read them. The third is **data lifecycle isolation** — when tenant data is retained, archived, or deleted, those operations must be scoped to the tenant. A data retention policy that deletes evaluation samples older than 90 days must delete only the specified tenant's samples, and a legal hold that preserves all evaluation data for a specific tenant must not preserve other tenants' data in the process.

## Layer Two: Compute Isolation

Compute isolation prevents one tenant's evaluation workload from affecting another tenant's evaluation in terms of performance, correctness, or information leakage. This is the layer most teams underinvest in because compute isolation failures are less visible than data isolation failures. When data leaks across tenants, someone eventually notices the wrong data in their report. When compute resources leak across tenants, the symptom is degraded performance or subtly distorted scores — problems that look like system issues, not isolation failures.

**The Noisy Neighbor Effect** is the most common compute isolation failure in shared evaluation infrastructure. When Tenant A's evaluation job runs a computationally intensive scoring pass across 50,000 samples while Tenant B's evaluation job runs simultaneously on the same compute cluster, Tenant B's job may experience higher latency, timeout errors, or incomplete scoring because Tenant A's job consumed the available GPU memory or CPU capacity. In production serving systems, noisy neighbor protection is a well-understood discipline — request queuing, rate limiting, resource quotas. In evaluation systems, these protections are often absent because evaluation is perceived as a batch workload that does not have the same latency requirements as production serving. But evaluation SLAs exist. If your contract guarantees that per-tenant evaluation reports are delivered within 24 hours of the evaluation window closing, a noisy neighbor that causes Tenant B's evaluation to take 36 hours is an SLA violation.

Resource isolation in evaluation compute follows the same spectrum as data isolation. **Dedicated compute** gives each tenant their own evaluation workers, whether physical machines, virtual machines, or dedicated Kubernetes pods as discussed in Section 27. This provides strong isolation but is cost-prohibitive for most platforms — 500 tenants each needing dedicated GPU-equipped evaluation workers would require a compute budget that exceeds the revenue those tenants generate. **Shared compute with resource quotas** is the pragmatic middle ground. All tenants share an evaluation compute cluster, but each tenant's evaluation jobs are subject to resource limits — maximum CPU cores, maximum GPU memory, maximum execution time — that prevent any single tenant from consuming more than their allocated share. Kubernetes namespaces with resource quotas, as covered in Section 27's discussion of multi-tenant cluster management, provide native enforcement for this model.

The more insidious compute isolation concern is **state leakage** — computation that carries information from one tenant's evaluation into another's. This happens most commonly in LLM-as-judge systems, where a large language model scores evaluation samples. If the judge processes Tenant A's samples and then Tenant B's samples within the same inference session, the judge's context window may contain residual information from Tenant A's samples when scoring Tenant B's. Even if the explicit context is cleared between tenants, some serving frameworks maintain key-value caches that can influence subsequent generations. The impact is subtle — slightly different scores than the tenant would have received if evaluated in isolation — but in high-stakes domains where score differences of two or three points trigger contractual consequences, this subtle influence matters.

The mitigation for state leakage is **session isolation** — ensuring that every tenant's evaluation uses a fresh inference session with no carried state from previous tenants. In practice, this means clearing the key-value cache between tenant evaluation runs when using frameworks like vLLM, or routing each tenant's judge evaluations to a dedicated model instance that processes only that tenant's samples before being recycled. For platforms using external LLM APIs, session isolation means using separate API sessions per tenant, with no connection pooling that might carry headers or state between tenants. The performance cost of session isolation — cold-start latency for each tenant's evaluation, inability to amortize model loading across tenants — is the price of correctness. A fast evaluation that produces contaminated scores is worse than a slow evaluation that produces accurate ones.

## Layer Three: Reporting Isolation

Reporting isolation is the layer closest to the customer and therefore the layer where failures are most visible and most damaging to trust. A customer who opens their evaluation report and sees another customer's data has an immediate, visceral reaction that no technical explanation can fully address. But reporting isolation failures are not limited to the dramatic scenario of misrouted data. They include any situation where a customer's report contains information derived from, influenced by, or revealing of other tenants.

The most common reporting isolation failure is **aggregate leakage** — including platform-wide statistics in per-tenant reports. A report that tells Tenant A "your accuracy is 92 percent, compared to the platform average of 88 percent" reveals the platform average, which is a function of all tenants' performance. If Tenant A knows there are 500 tenants on the platform and their accuracy is 92 percent compared to an average of 88 percent, they can infer something about the distribution of other tenants' quality. For most B2B customers, this is a minor concern. For customers in competitive industries — two rival banks on the same platform, two competing pharmaceutical companies — any information about aggregate platform performance is competitive intelligence that your contracts likely prohibit sharing.

The fix is straightforward in principle: per-tenant reports contain only per-tenant data. No platform averages, no percentile rankings, no peer comparisons unless the customer explicitly opts into a benchmarking program with informed consent from all participating tenants. In practice, this requires discipline from the reporting team, who may want to include comparative context to make reports more useful. The context is useful. It is also a data isolation violation if it reveals information about other tenants.

**Temporal leakage** is a subtler reporting isolation concern. If Tenant A's report includes a chart showing their quality trend over time alongside a "platform health" trend line, and Tenant A correlates dips in the platform health line with events they know about — a competitor's product launch, a regulatory change that affected their industry — they can infer when the platform experienced quality issues with other tenants. The inference may be imprecise, but it is information the customer is extracting about other tenants from a report that should contain only their own data.

**Metadata leakage** occurs when report infrastructure reveals tenant information through technical artifacts rather than content. A report URL that includes a sequential tenant ID tells the customer how many tenants exist on the platform and roughly when they onboarded. A report file stored in a shared object store with predictable naming conventions allows a sophisticated customer to enumerate other tenants' report locations, even if access controls prevent them from reading those reports. An evaluation dashboard that loads tenant names into a dropdown for the current user's session but also includes other tenant names in the page's underlying data model — visible in the browser's developer tools — leaks the entire tenant list. These are engineering oversights, not malicious design, but the impact on customer trust is the same.

## How the Three Layers Interact

The most dangerous isolation failures occur at the boundaries between layers, where one layer's output becomes another layer's input. Data that is properly isolated in storage can become contaminated during compute if a shared processing step mixes tenant data. Compute that is properly isolated can produce contaminated reports if the reporting layer aggregates results across tenants before filtering. The interaction between layers creates contamination vectors that are invisible when each layer is reviewed in isolation.

Consider a concrete example. Your evaluation pipeline stores each tenant's data in a dedicated schema — solid data isolation. The compute layer pulls data from each schema independently and scores it — solid compute isolation. The reporting layer reads computed scores from a shared results table, where all tenants' scores are stored with tenant ID tags, and generates per-tenant reports by filtering on tenant ID. If the reporting query joins the results table with a dimension weights table that is not tenant-scoped — because the engineer assumed dimension weights were platform-wide, not per-tenant — the report may compute weighted scores using the wrong tenant's weights. The data was isolated. The compute was isolated. The report is contaminated.

This example illustrates why isolation must be verified end-to-end, not layer by layer. A security review that checks data access controls, then checks compute resource quotas, then checks report filtering, will certify each layer as isolated while missing the cross-layer contamination vector that actually matters. End-to-end verification means tracing a single tenant's evaluation from data pull through compute through reporting and confirming that at no point does any data, state, configuration, or derived information from another tenant enter the pipeline. As Section 15 discusses in the context of automated eval pipelines, the principle applies here: every stage of the pipeline must be auditable and every handoff between stages must preserve tenant context.

## Building the Isolation Verification Framework

Isolation is not a property you build once and trust forever. It is a property you verify continuously, because every code change, every infrastructure migration, every new feature is an opportunity for an isolation gap to appear. The verification framework has three components.

The first is **synthetic tenant testing**. You create two or more synthetic tenants with known, distinguishable data. Tenant Test-Alpha has evaluation samples containing a unique marker — a specific term, a specific format, a specific content pattern — that appears only in Test-Alpha's data. Tenant Test-Beta has a different marker. You run the full evaluation pipeline for both synthetic tenants and then scan the outputs — scores, reports, logs, caches, intermediate files — for markers belonging to the wrong tenant. If Test-Alpha's marker appears anywhere in Test-Beta's pipeline artifacts, you have an isolation breach. This test runs on every deployment, not just on initial setup, because the most dangerous isolation gaps are the ones introduced by routine code changes that nobody thought would affect tenant boundaries.

The second is **audit logging with cross-tenant detection**. Every data access in the evaluation pipeline is logged with the tenant context of the requesting process and the tenant ownership of the accessed data. An automated monitor scans these logs for any access where the requesting tenant does not match the data tenant. This catches isolation breaches in real time, even those that the synthetic tenant test does not cover — because real data access patterns are more complex than any test scenario can replicate.

The third is **periodic penetration testing** focused specifically on the evaluation pipeline. An engineer acting as a malicious or careless insider attempts to access cross-tenant evaluation data through the available interfaces — query tools, debugging consoles, report dashboards, API endpoints, log aggregation systems. Each attempt is documented, and each successful access is treated as an isolation gap requiring remediation. This testing should happen quarterly at minimum, and after every significant infrastructure change.

## The Organizational Dimension of Isolation

Technical isolation mechanisms are necessary but not sufficient. The strongest database access controls are meaningless if an engineer with administrative access runs an unfiltered query during a debugging session and sees evaluation data for a tenant they should not access. The most rigorous compute isolation is undermined if an operations team member copies evaluation results between tenants' storage locations while troubleshooting a data pipeline issue.

Organizational isolation means that your internal access controls mirror your tenant isolation architecture. Engineers should not have access to all tenants' evaluation data by default. Access should be scoped to the tenants an engineer is actively working on, granted through a formal access request process, and automatically revoked after a defined period. Debugging tools should enforce tenant scoping — an engineer investigating a scoring anomaly for Tenant 247 should see only Tenant 247's data, even in debugging and staging environments. As discussed in Section 29 on enterprise governance, the principle of least privilege applies to internal access to evaluation data with the same rigor it applies to production customer data.

The organizational challenge compounds at scale. A platform with 500 tenants and 40 engineers cannot practically restrict each engineer to a small number of tenants because on-call rotations, incident response, and feature development require flexible access. The solution is time-bounded, audited access — engineers can access any tenant's evaluation data but only through a process that creates an audit record, specifies the business justification, and automatically expires the access within 24 to 48 hours. This does not prevent cross-tenant access; it makes cross-tenant access visible, accountable, and time-limited.

The three isolation layers — data, compute, and reporting — define the architecture. The verification framework ensures the architecture holds. The organizational controls ensure that human access does not bypass the technical controls. Together, they constitute the Isolation Boundary. But isolation applies not just to production evaluation data. It applies to the ground truth that evaluation measures against — the golden sets that define what "correct" means for each tenant. The next subchapter examines why shared golden set infrastructure is a liability and how per-tenant ground truth separation protects both accuracy and trust.
