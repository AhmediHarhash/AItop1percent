# 28.4 — Customer-Specific Release Gates: When One Customer's Pass Is Another Customer's Block

The release manager stares at a screen with three hundred and twelve green results and eight red ones. The platform-wide release candidate has been evaluated, the canary group has validated without incident, and the opt-in wave produced clean results for everyone who participated. But eight customers — each running a distinct configuration, each with a different contractual quality floor, each operating in a different domain — have failed their per-tenant evaluation against the release candidate. The release is ready for 97.5 percent of the customer base. It is not ready for the other 2.5 percent. There is no single version of this release that works for everyone.

This is not an edge case. This is every release.

## Why Per-Customer Gates Exist

A platform-wide release gate asks one question: "Does this release meet the platform's quality standard?" That question has a single answer. A per-customer release gate asks a different question for every tenant: "Does this release meet this customer's quality standard?" That question has as many answers as you have customers.

The need for per-customer gates emerges directly from the Quality Contract architecture described earlier in this section. Each customer's Quality Contract defines their specific quality dimensions, their per-dimension thresholds, their composite score floor, and their tolerance for quality variation. A pharmaceutical customer's contract may specify 92 percent accuracy on medical terminology, 88 percent on regulatory format compliance, and zero tolerance for hallucinated drug interactions. A marketing agency's contract may specify 80 percent on brand voice consistency and 75 percent on factual accuracy, with no constraint on format compliance at all. The same release that improves factual accuracy by 3 points and degrades format compliance by 2 points is a clear win for the marketing agency and a potential breach for the pharmaceutical customer.

Per-customer release gates evaluate the release candidate against each customer's Quality Contract individually, producing a per-customer verdict that reflects that customer's specific definition of acceptable quality. Without per-customer gates, the release decision is either overly conservative — holding the release because any customer fails, which means no release ever ships — or overly aggressive — shipping the release because most customers pass, which means some customers are harmed. Neither approach works. Per-customer gates enable the third option: ship the release to customers who pass, hold it for customers who fail, and handle the holdback customers individually.

## The Three Verdicts

Every per-customer release gate evaluation produces one of three verdicts: **pass**, **conditional**, or **block**.

**Pass** means the release candidate meets or exceeds all of the customer's Quality Contract thresholds. The customer can receive the release without restriction. No special monitoring, no enhanced alerting, no customer notification beyond the standard release communication. Pass is the desired outcome for every customer, and in a well-managed platform, 85 to 95 percent of customers should receive a pass verdict for any given release.

**Conditional** means the release candidate meets the customer's contractual floors but degrades one or more quality dimensions by a measurable amount that, while not breaching the contract, represents a real quality decrease the customer may notice. A conditional verdict triggers three requirements. First, enhanced monitoring for the customer during the first two weeks post-deployment — automated alerting if any dimension drops further, daily quality reports rather than weekly. Second, proactive customer success outreach — the CSM contacts the customer to inform them of the change, acknowledge the quality shift, and outline the mitigation plan. Third, a remediation timeline — the platform team commits to a specific plan to restore the degraded dimensions, whether through adapter retraining, prompt adjustment, or a future release that addresses the regression. Conditional is not a soft pass. It is a pass with an obligation to make the customer whole.

**Block** means the release candidate would push the customer below one or more contractual quality floors, or would degrade quality by more than the catastrophic threshold defined in their Quality Contract. A blocked customer must not receive the release. Period. The block persists until one of three things happens: the release is modified to address the customer's specific issue, the customer's configuration is adjusted to make the release compatible, or the customer explicitly waives the block after reviewing the impact data and accepting the quality change. The first option is rare — modifying a release for one customer is expensive and creates divergent release branches. The second option is common — retraining an adapter, adjusting a prompt template, or reconfiguring retrieval settings often resolves the incompatibility. The third option is a last resort — customers who waive blocks are accepting known quality degradation, which should be documented in writing with sign-off from someone with contractual authority.

## How Per-Customer Gate Evaluation Works

The per-customer gate evaluation is a direct application of the Blast Radius Map data described in the previous subchapter, combined with the customer's Quality Contract.

For each active customer, the gate evaluation pipeline performs the following sequence. First, it retrieves the customer's Quality Contract from the contract registry — the specific dimensions, thresholds, composite floor, and degradation tolerances defined for that customer. Second, it retrieves the customer's release candidate evaluation scores from the Blast Radius Map — the per-dimension and composite scores produced by running the customer's eval suite against the release candidate in staging. Third, it retrieves the customer's current production baseline scores — the per-dimension and composite scores from their most recent eval run against the current production version.

The pipeline then applies three checks. The **floor check** compares each release candidate dimension score to the customer's contractual floor for that dimension. If any dimension score falls below its floor, the verdict is block. The **degradation check** compares the release candidate scores to the current baseline scores. If any dimension degrades by more than the customer's degradation tolerance — typically 3 to 5 points for standard contracts, 1 to 2 points for stringent contracts — but remains above the contractual floor, the verdict is conditional. The **improvement check** verifies that no dimension has degraded at all, or that all degradations are within noise margin — less than 1 point. If all dimensions pass the improvement check, the verdict is pass.

The checks are applied in priority order: floor check first, degradation check second, improvement check third. A customer who fails the floor check is blocked regardless of what the degradation or improvement checks would show. A customer who passes the floor check but fails the degradation check is conditional regardless of what the improvement check shows. Only a customer who passes all three checks receives a pass verdict.

## Handling Blocks

When a customer receives a block verdict, the platform team faces a per-customer remediation challenge. The customer cannot receive the release in its current form. The team must determine why the customer was blocked and what can be done about it.

The most common cause of blocks is **adapter-base incompatibility** — the release includes a base model update that degrades the customer's per-tenant LoRA adapter. The remediation is adapter retraining against the new base model weights, as described in the adapter evaluation subchapter. Retraining typically takes 2 to 8 hours of compute plus a validation cycle of 4 to 24 hours depending on the customer's eval suite size. During the retraining period, the customer remains on the current production version.

The second most common cause is **prompt sensitivity shift** — the new model responds differently to the customer's custom prompt templates, producing outputs that score lower on dimensions the customer's rubric emphasizes. The remediation is prompt adjustment — modifying the customer's prompt templates to produce the desired behavior from the new model. This is often faster than adapter retraining — hours rather than days — but requires prompt engineering expertise and access to the customer's prompt configuration.

The third most common cause is **retrieval interaction change** — the new model processes retrieved context differently, causing the customer's RAG pipeline to produce lower-quality outputs even when the retrieval itself has not changed. This is the subtlest and most difficult block to remediate because the root cause spans two systems — the model and the retrieval pipeline — and the fix may require changes to either or both. Retrieval-based blocks account for roughly 10 to 15 percent of all blocks but consume a disproportionate share of remediation effort.

For each blocked customer, the platform team should produce a **block resolution plan** that specifies the root cause, the proposed remediation, the estimated timeline, the person responsible, and the customer communication plan. The block resolution plan is a mini-project — it has a scope, a schedule, a responsible party, and a deliverable (a passing re-evaluation). The customer success manager communicates the plan to the customer, sets expectations for the timeline, and provides regular updates until the block is resolved.

## The Operational Complexity of Multiple Versions

When some customers are blocked and others have passed, the platform is running multiple versions simultaneously. The 95 percent who passed are on the new version. The 5 percent who are blocked remain on the old version. This version fragmentation has operational consequences that compound over time.

The serving infrastructure must route each tenant's requests to the correct model version. In 2026, serving frameworks like vLLM, LoRAX, and Predibase support multi-version routing through configuration-based request routing — each tenant's configuration specifies which model version they are pinned to, and the routing layer directs requests accordingly. The infrastructure cost is the cost of running both versions simultaneously, which adds 30 to 50 percent to serving compute for the duration of the fragmentation.

The monitoring system must track quality metrics for tenants on different versions separately. A quality regression on the old version should not trigger alerts for the new-version cohort, and vice versa. The monitoring must also track the blocked customers' remediation progress — is the adapter retraining complete? Has the re-evaluation passed? Is the customer ready to migrate? These remediation-tracking signals are distinct from quality monitoring signals and require their own operational workflow.

The support system must handle tickets from customers on different versions. A support agent responding to a quality complaint must know which version the customer is running, because troubleshooting steps differ between versions. If the support team does not have version-aware context, they waste time investigating the wrong version's behavior.

The maximum sustainable version count for most platforms is three — the current production version, the previous version for recently blocked customers, and occasionally a version before that for customers with long remediation cycles or regulatory holdbacks. Beyond three simultaneous versions, the operational burden becomes prohibitive. Each additional version requires its own infrastructure allocation, its own monitoring configuration, its own support documentation, and its own security patching schedule. The version count should be tracked as a platform health metric, with alerts when the count exceeds the sustainable maximum.

## Customer Notification Requirements

Every per-customer release gate verdict — pass, conditional, or block — triggers a notification obligation, though the content and urgency differ.

Pass customers receive standard release communication: a summary of what changed, a note that their quality is projected to be stable or improved, and a link to their updated quality dashboard. This communication can be automated and batched. It is informational, not actionable — the customer does not need to do anything.

Conditional customers receive proactive outreach from their customer success manager: a detailed explanation of which dimensions are projected to degrade and by how much, the remediation plan and timeline, the enhanced monitoring that will be active, and a direct contact for questions or concerns. This communication must be personalized — a templated email that says "some quality dimensions may be affected" without specifying which dimensions, for which customer, by how much, is worse than no communication at all. The customer needs to understand exactly what is changing in their specific context.

Blocked customers receive the most detailed communication: a clear statement that the release will not be applied to their account, the reason for the block stated in terms of their Quality Contract, the block resolution plan with timeline and responsible party, and the confirmation that they will remain on the current version with no quality disruption until the block is resolved. Blocked customers are often the most valuable and most demanding customers on the platform. They have the most complex configurations, the most specific quality requirements, and the highest expectations. The notification they receive should reflect their importance — direct, specific, and delivered by someone with the authority to commit to the resolution timeline.

The notification workflow must be wired into the release pipeline, not treated as a manual follow-up. When the per-customer gate evaluation produces verdicts, the notification system should automatically generate the appropriate communication for each verdict category, pre-populate it with customer-specific details from the Blast Radius Map and Quality Contract, and route it to the appropriate delivery channel — automated for pass, CSM-delivered for conditional, CSM-plus-account-executive for block.

## When Blocks Accumulate

A single release with five blocked customers is manageable. Each block gets a resolution plan, the team works through them over one to three weeks, and all customers are on the current version before the next release cycle begins. But what happens when blocks accumulate across multiple releases?

Consider a platform that releases monthly. Release one blocks five customers. Three are resolved before release two. Release two blocks four new customers and one of the remaining blocks from release one still has not been resolved. The platform now has six blocked customers across two versions. Release three blocks three new customers and one block from release two remains unresolved. The platform now has ten blocked customers across three versions. Each month, the block backlog grows because new blocks accumulate faster than old blocks are resolved.

This accumulation pattern — **Block Drift** — is a leading indicator that the platform's per-customer remediation capacity is insufficient for its release cadence. The remediation team cannot clear blocks fast enough to keep up with the rate at which new releases produce new blocks. The result is a growing population of customers stuck on progressively older versions, with progressively more version fragmentation, progressively higher infrastructure costs, and progressively more operational complexity.

The antidote to Block Drift is to treat the block backlog as a first-class constraint on the release cadence. If the remediation team can resolve an average of eight blocks per release cycle and each release produces an average of five new blocks, the backlog shrinks over time and the system is sustainable. If each release produces an average of ten new blocks, the backlog grows and the system is heading toward collapse. The sustainable release cadence is the cadence at which new blocks do not exceed the remediation team's resolution capacity. If you want to release more frequently, you need more remediation capacity — more engineers who can retrain adapters, adjust prompts, and debug retrieval interactions for blocked customers.

## The Economics of Per-Customer Gates

Per-customer release gates add cost. The eval compute for per-tenant assessment, the operational complexity of version fragmentation, the remediation effort for blocked customers, the communication overhead for conditional and blocked notifications — none of this is free.

A platform with 400 customers running per-customer gates for monthly releases can expect to spend roughly $1,500 to $3,000 in eval compute per release for the per-tenant assessment, 20 to 40 hours of engineering time per release for block remediation, 10 to 20 hours of CSM time per release for conditional and block communication, and 30 to 50 percent incremental infrastructure cost during version fragmentation periods. Annualized, this adds $60,000 to $120,000 in direct costs plus 400 to 700 hours of team time.

Compare that to the cost of not having per-customer gates. The writing platform described in the opening subchapter of this chapter shipped a release without per-customer evaluation and put $2.7 million in contract value at risk. A single incident of that magnitude pays for six to ten years of per-customer gate infrastructure. The economics are not close. Per-customer gates are expensive in absolute terms and trivially cheap relative to the contract risk they prevent.

The real cost is not the infrastructure or the compute. It is the organizational discipline required to maintain per-tenant eval suites, compute Blast Radius Maps for every release, evaluate per-customer gates faithfully, and remediate blocks systematically. That discipline is harder to budget for than compute, and it is the discipline that separates platforms that scale safely from platforms that accumulate risk with every release.

The per-customer gate tells you which customers can receive a release and which cannot. But across all those customers, hidden signals in one customer's quality data might help you improve quality for another. The next subchapter covers Cross-Tenant Intelligence — how to learn from the entire customer base without violating the isolation boundaries that each customer's data demands.
