# 28.79 — Anomaly Escalation Policy: When a Tenant Gets Premium Evaluation

The on-call eval engineer is staring at two alerts that fired within four minutes of each other. The first is a tier-one pharmaceutical customer whose clinical summarization accuracy dropped 6 points overnight. The second is a tier-three marketing agency whose content generation quality dipped 4 points over the past 48 hours. Both are real anomalies. Both need investigation. The engineer has time to start one before the morning standup. She picks the pharmaceutical customer without hesitation — not because the marketing agency does not matter, but because the escalation policy she follows makes the priority unambiguous. That policy is the difference between a platform that triages by gut feel and one that triages by design.

Every multi-tenant platform eventually faces concurrent quality anomalies across customers of different value and risk. Without a written escalation policy that maps anomaly severity to customer tier and defines exactly who gets notified, how fast, and with what resources, the platform defaults to whoever shouts loudest or whoever the on-call engineer happens to notice first. That is not a quality system. It is a lottery.

## The Four Escalation Levels

An **anomaly escalation policy** defines four levels of response, each with its own notification targets, response time commitments, and resource allocation. The levels are not arbitrary — they reflect the organizational cost of being wrong at each level of severity.

**Level 1 — Informational** is for minor metric movements that exceed normal variance but do not breach any quality floor. A customer's accuracy dips 1.5 points in a 24-hour window, which is outside one standard deviation but inside two. The eval system logs it, adds it to the next daily digest, and takes no immediate action. If the metric recovers within the next eval cycle, the entry stays in the log but never becomes a notification. If the movement persists for 48 hours or widens, it escalates automatically to Level 2. Most informational anomalies — roughly 70 percent on mature platforms — resolve without escalation. But the log creates an audit trail that becomes invaluable when a customer later asks "has our quality changed in the last quarter?"

**Level 2 — Investigation** is triggered when a metric movement persists, widens, or breaches a soft threshold. A customer's score drops by more than 3 points over a 48-hour window, or a quality dimension that was previously stable shows a new failure pattern. Investigation-level anomalies generate a notification to the customer's eval owner and the platform's quality lead. The response commitment depends on tier: for tier-one customers, the investigation starts within 2 hours of detection. For tier-two, within 8 hours. For tier-three, next business day. The investigation involves pulling recent production samples for the affected customer, running targeted eval against their specific configuration, and determining whether the anomaly is real, transient, or an artifact of scoring noise.

**Level 3 — Incident** is declared when an investigation confirms a genuine quality regression — the customer's quality has degraded in a way that affects their end users, their compliance posture, or their contracted quality floor. Incident-level escalation generates notifications to the customer's eval owner, the engineering lead, the customer success manager, and the customer themselves if the contract includes proactive quality notifications. Response resources ramp up: for tier-one customers, a dedicated investigation pod of two to three engineers is assembled within 4 hours. For tier-two, a single engineer is assigned within 12 hours. For tier-three, the issue enters the standard engineering queue with a 48-hour triage commitment.

**Level 4 — Critical** is reserved for anomalies that indicate either a severe quality failure for a tier-one customer or a pattern that suggests a platform-wide problem. A tier-one customer's score drops by more than 10 points. Multiple customers across different configurations show correlated quality drops. A safety eval dimension fails for any customer in a regulated industry. Critical escalation triggers an immediate response: page the on-call engineering lead, assemble a war room within 1 hour, notify the VP of Engineering and the customer success VP, and begin root cause analysis with dedicated compute resources. Critical incidents also trigger the premium evaluation mode described below. On a well-run platform, Level 4 escalations happen fewer than three times per quarter. When they happen, they take priority over everything.

## How Customer Tier Maps to Escalation Severity

The same metric movement produces different escalation levels depending on which customer it affects. This is not favoritism. It is risk-weighted triage.

A 3-point accuracy drop for a tier-one pharmaceutical customer paying $1.8 million per year on a HIPAA-regulated clinical summarization use case starts at Level 2, with a 2-hour investigation commitment. That same 3-point drop for a tier-three marketing agency on a $4,000 monthly self-serve plan starts at Level 1, with a log entry and no immediate notification. The metric movement is identical. The business risk is not. The pharmaceutical customer has contractual quality floors, regulatory exposure, and a revenue impact that justifies immediate investigation. The marketing agency's quality expectation is met by a broader range, and a 3-point movement may fall within their normal operating variance.

The tier-to-severity mapping is not static. Two factors can override it. First, the nature of the anomaly: a safety dimension failure escalates to Level 3 or Level 4 regardless of customer tier, because safety failures in any customer's production traffic create platform-level liability. A tier-three customer whose hate speech filter starts passing toxic content gets the same urgent response as a tier-one customer, because the reputational and regulatory risk does not scale with contract value. Second, the anomaly pattern: if three or more tier-three customers show correlated quality drops within the same 24-hour window, the escalation jumps to Level 3 even though each individual customer would normally be Level 1. Correlated anomalies across unrelated customers suggest a platform-wide cause — a model update, a data pipeline issue, an infrastructure change — and platform-wide causes demand platform-level response.

## Response Time Commitments by Tier

Response time commitments must be published internally and enforced through the alerting system, not left to individual judgment. The commitments define the maximum time between anomaly detection and the start of human investigation.

Tier-one customers: Level 2 anomalies get a 2-hour investigation start. Level 3 incidents get a 4-hour root cause analysis launch. Level 4 critical incidents get a 1-hour war room. These commitments mean that at least one member of the eval team is on-call 24/7 for tier-one escalations. On a platform with 15 to 20 tier-one customers, the on-call rotation requires at least 4 engineers to maintain sustainable coverage without burnout.

Tier-two customers: Level 2 anomalies get an 8-hour investigation start. Level 3 incidents get a 12-hour root cause analysis launch. These commitments can be met during business hours in most time zones, which reduces the on-call burden significantly.

Tier-three customers: Level 2 anomalies get a next-business-day investigation start. Level 3 incidents get a 48-hour triage commitment. Tier-three customers do not receive Level 4 escalations unless the anomaly pattern suggests a platform-wide problem, in which case the escalation is driven by the platform concern rather than the individual customer's tier.

These response times are not suggestions. They are operational SLAs that the eval team tracks, measures, and reports on. A mature platform reviews escalation response times monthly and treats consistent misses as an infrastructure or staffing problem, not an individual performance problem.

## Concurrent Anomaly Triage

The hardest operational question is not how to respond to one anomaly. It is how to respond when two anomalies arrive at the same time and you do not have enough people for both.

The rule is straightforward: the higher-tier customer gets the senior resource first. When a tier-one customer and a tier-three customer both have Level 2 anomalies, the senior eval engineer starts the tier-one investigation. A junior engineer or an automated triage workflow handles the tier-three anomaly's initial assessment. If the team has capacity for both, both proceed in parallel. If it does not, the tier-one customer gets first priority.

One exception overrides this rule: when the lower-tier customer's anomaly shows signs of a platform-wide problem. If the tier-three customer's quality drop correlates with an infrastructure change, a model update, or a pattern visible in other customers' metrics, their anomaly may be the early warning of something that affects the tier-one customer too — it just has not shown up yet. In this case, the platform-wide signal takes priority over the individual customer signal, because fixing the root cause protects all customers, including the tier-one customer who has not yet been affected.

The concurrent triage decision requires judgment, and it cannot be fully automated. But the policy reduces the judgment to a small set of questions: Is the higher-tier customer affected? Does the lower-tier customer's anomaly suggest a platform-wide cause? Are there correlated signals across other customers? A written decision tree with these three questions eliminates 90 percent of the ambiguity that makes concurrent triage stressful for on-call engineers.

## Premium Evaluation Mode

When an anomaly escalates to Level 3 or Level 4, the affected customer enters **premium evaluation mode** — a temporary state where the platform dramatically increases its eval investment for that customer.

In steady state, a tier-two customer might receive daily evaluation on a 15-percent sample of their production traffic. In premium evaluation mode, that same customer receives continuous evaluation on 100 percent of their traffic, with additional human review on every output that scores below the customer's quality floor. The eval frequency increases by 5 to 10 times. The scoring depth increases — instead of the standard three-dimension rubric, every dimension in the customer's rubric is scored. Human reviewers are assigned to review flagged outputs within 2 hours instead of the normal 24-hour review window. The purpose is not permanent — it is to achieve rapid, high-confidence diagnosis of the anomaly's root cause and to verify that any fix actually resolves the problem as the customer experiences it, not just as the platform-wide metric reports it.

Premium evaluation mode is expensive. The incremental eval cost for a single customer in premium mode can reach $800 to $2,000 per day, depending on traffic volume and the depth of human review. This cost is justified because the alternative — extended customer pain, churn risk, and the operational cost of a prolonged investigation without high-frequency data — is almost always more expensive. A platform that spends $3,000 on three days of premium evaluation to diagnose and fix a regression for a $600,000-per-year customer has made the right investment.

## De-Escalation Criteria

Entering premium evaluation mode is straightforward: an escalation triggers it. Exiting premium evaluation mode requires discipline, because the temptation is to leave it running indefinitely "just in case." That temptation must be resisted — premium evaluation at scale is not sustainable, and every customer in premium mode consumes resources that reduce your capacity to respond to the next anomaly.

De-escalation follows three criteria, all of which must be met. First, the root cause has been identified and a fix has been deployed. Not proposed, not in testing — deployed to production. Second, the customer's quality metrics have returned to within one standard deviation of their pre-anomaly baseline for at least 72 consecutive hours. This waiting period catches regressions that appear stable initially but degrade again within days. Third, the customer's eval owner and the engineering lead have both signed off on the de-escalation. This dual sign-off prevents premature de-escalation driven by resource pressure rather than genuine recovery.

When all three criteria are met, the customer's eval configuration returns to its standard tier settings over a 48-hour transition period. The transition is gradual — moving from 100 percent eval coverage to steady-state coverage in a single step would create a monitoring gap during the exact period when the fix is most likely to show unexpected behavior. Stepping down in stages — 100 percent to 50 percent to steady state — maintains elevated visibility while reducing cost.

## The Post-Incident Review

Every Level 3 and Level 4 escalation generates a post-incident review within five business days of de-escalation. The review is not a blame exercise. It is an eval system improvement exercise that asks four questions.

First, how was the anomaly detected? Did the eval system catch it, or did the customer report it? If the customer reported it, why did the eval system miss it? This question drives improvements to drift detection thresholds, as covered in the alert design principles from Chapter 4. Second, was the response time appropriate? Did the team meet the tier-based response time commitment? If not, what blocked them — staffing, tooling, unclear ownership, or competing priorities? Third, did premium evaluation mode provide the data needed to diagnose the root cause? Were there gaps in the premium eval coverage that slowed diagnosis? Fourth, what changes to the eval system would have caught this anomaly earlier or at a lower severity level?

The answers to these questions feed back into the eval system's configuration: threshold adjustments, new golden set entries derived from the incident's failure cases, updated rubric dimensions, and refined tier-to-severity mappings. Over time, the post-incident review process converts every escalation into a permanent improvement to the eval infrastructure. A platform that has run 40 post-incident reviews has an eval system that is 40 incidents smarter than one that treated each escalation as a one-time crisis.

## Connecting Escalation Policy to Organizational Design

The escalation policy does not exist in isolation. It depends on the team structures from subchapter 8-1, the customer success interface from subchapter 8-2, and the self-service capabilities from subchapter 8-6. An escalation policy that assigns a named eval owner per tier-one customer requires the federated team model. A premium evaluation mode that triggers human review requires the reviewer infrastructure described in Section 14. Response time commitments that span time zones require the global infrastructure patterns from Section 27.

The escalation policy is ultimately a contract between the eval team and the rest of the organization: here is what we will detect, how fast we will respond, and what resources we will allocate. When that contract is written, published, and enforced, escalation is orderly. When it is unwritten and improvised, escalation is politics — and politics always favors whoever shouts loudest, not whoever needs help most.

The eval system that powers this escalation policy is not a side project or a monitoring dashboard. It is an internal product with its own users, its own roadmap, and its own operating requirements — which is the subject of the next subchapter.
