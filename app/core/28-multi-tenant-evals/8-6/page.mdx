# 28.78 — Self-Service Eval: Letting Customers Define Their Own Quality Criteria

Why does every change to a customer's eval configuration require a support ticket? A customer wants to adjust a rubric weight — they file a ticket and wait two days. A customer wants to upload fresh golden set examples because their product launched a new feature — they email the eval team and wait a week. A customer wants to check whether last Tuesday's quality dip was an anomaly or a trend — they ask their customer success manager, who asks the eval team, who runs a query and sends the results three days later. Every one of these interactions is a delay that frustrates the customer, consumes platform team bandwidth, and creates a bottleneck that scales linearly with customer count. At 30 customers, the support ticket model is annoying. At 200 customers, it is unsustainable.

The answer is not to hire more people to process tickets faster. The answer is to eliminate the tickets entirely by giving customers the tools to manage their own eval configurations. **Self-service eval** is the capability that transforms per-customer evaluation from a support burden into a product feature — and it is the single most important organizational scaling lever for multi-tenant eval operations because it shifts routine work from your team to the people who actually understand their own quality requirements better than you ever will.

## What Self-Service Eval Looks Like

A self-service eval system gives customers direct access to the configuration, monitoring, and reporting layers of their evaluation pipeline through a purpose-built interface. The customer logs into a portal and sees their quality landscape: current scores across every eval dimension, trend lines over the past 30, 60, and 90 days, and the alert history showing when thresholds were triggered and what the platform did in response.

From this dashboard, the customer can take five categories of action. First, **rubric configuration**: the customer can view their current rubric dimensions, adjust the weights assigned to each dimension, modify the thresholds that define pass and fail for their quality gates, and preview how those changes would affect their historical scores before committing. Second, **golden set management**: the customer can upload new reference examples, retire outdated ones, tag examples by category or difficulty, and see freshness metrics showing when each example was last updated and how well the current golden set represents their production traffic. Third, **alert configuration**: the customer can define alert rules — which quality dimensions to monitor, what threshold triggers an alert, who receives the notification, and whether the alert triggers an automated response like pausing a deployment. Fourth, **on-demand eval runs**: the customer can submit a batch of outputs for evaluation outside the regular automated cycle — useful when they deploy a prompt change and want immediate quality feedback rather than waiting for the next scheduled run. Fifth, **reporting and evidence**: the customer can generate quality reports for any time period, compare scores across versions, export evidence packages for compliance audits, and share reports with internal stakeholders who do not have platform access.

The key design principle is that customers configure within parameters the platform defines. A customer can adjust rubric weights but cannot create rubric dimensions that the platform's judge model is not trained to evaluate. A customer can upload golden set examples but cannot exceed the storage quota assigned to their tier. A customer can configure alerts but cannot set polling frequencies so aggressive that they overwhelm the notification infrastructure. The platform provides the guardrails. The customer operates freely within them.

## The Guard Rails That Prevent Chaos

Self-service without guard rails is self-service sabotage. A customer who accidentally sets all rubric weights to zero gets scores that mean nothing. A customer who uploads 10,000 golden set examples with inconsistent labeling degrades eval quality rather than improving it. A customer who configures alerts on every dimension at the tightest possible threshold drowns in notifications and stops trusting the system. Guard rails protect customers from themselves without limiting the autonomy that makes self-service valuable.

**Validation rules** catch configuration errors before they take effect. A rubric weight change must produce weights that sum to 100 percent. A golden set upload must pass format validation — correct schema, required fields present, reference outputs within length limits — before examples are accepted. An alert threshold cannot be set below the statistical noise floor for the relevant dimension, which prevents alerts from firing on random variation rather than genuine quality shifts. These validations run in real time as the customer makes changes, with clear explanations of what failed and why.

**Change previews** show the customer what their change will do before it goes live. When a customer adjusts a rubric weight, the system re-scores a sample of recent outputs under the new weights and shows the customer: "Under your current weights, 94 percent of outputs pass your quality gate. Under the proposed weights, 88 percent would pass. The biggest impact is on the completeness dimension, which you increased from 15 to 25 percent." The customer can then decide whether the change reflects their intent or whether they need to adjust further. This preview eliminates the most common self-service failure: making a change without understanding its consequences.

**Rollback capability** lets the customer undo changes that produce unexpected results. Every configuration change is versioned, and the customer can revert to any previous configuration with one action. The system also supports automatic rollback triggers — if a configuration change causes quality scores to drop by more than a customer-defined threshold within 24 hours, the system reverts to the previous configuration and notifies the customer. This safety net makes customers more willing to experiment because the cost of a mistake is low.

**Tier-based access controls** limit which self-service capabilities are available at each pricing tier. Tier-3 customers might get dashboard access and basic alert configuration. Tier-2 customers add rubric weight adjustment and golden set management. Tier-1 customers get the full suite including on-demand eval runs and evidence package generation. This tiering aligns self-service capabilities with the sophistication of the customer's quality management needs and, as Section 30 explores, creates a natural upsell path where customers who need more control move to higher tiers.

## The Adoption Challenge

Building self-service eval tools is the easy part. Getting customers to use them is the hard part. Enterprise customers — particularly those in regulated industries — are cautious about changing systems they depend on. They may prefer the familiarity of filing a support ticket over the uncertainty of a new interface. They may worry that changes they make will break something. They may simply not know the tools exist because nobody told them.

Industry experience shows that self-service adoption follows a predictable curve. In the first three months after launch, 10 to 15 percent of customers use the tools — these are the technically sophisticated early adopters who would have built their own eval dashboards if you had not provided one. From three to nine months, adoption climbs to 30 to 40 percent as customer success teams actively demonstrate the tools during business reviews and training sessions. After twelve months, adoption plateaus at 50 to 65 percent. The remaining 35 to 50 percent of customers either lack the internal expertise to manage eval configurations (common in smaller organizations without dedicated AI quality staff) or operate under governance constraints that require all changes to go through the platform team regardless of available tooling (common in regulated enterprises).

The 50 to 65 percent adoption plateau is not a failure. It is the natural ceiling. Not every customer will self-serve, and that is fine. The goal is not 100 percent adoption — it is reducing the per-customer management burden enough to change the economics of scaling.

Three practices accelerate adoption past the early-adopter phase. First, onboarding includes a self-service walkthrough. When a new customer is set up, the onboarding process includes a guided session where the customer makes their first rubric adjustment and uploads their first golden set batch using the self-service tools. Customers who use the tools during onboarding are three times more likely to continue using them. Second, business reviews feature self-service demonstrations. During quarterly reviews, the customer success manager shows the customer something specific they can do through self-service — "here is where you can adjust that completeness weight you mentioned wanting to change" — tied to a real need the customer expressed. Third, the platform sends contextual prompts when self-service actions are relevant. When a customer files a ticket to adjust a rubric weight, the response includes: "We have made this change for you. In the future, you can make this adjustment directly from your quality dashboard — here is a quick guide showing how." This shifts behavior one interaction at a time.

## The Hybrid Approach: Self-Service for Routine, Platform-Managed for Strategy

The most effective operating model is not pure self-service and not pure platform-managed. It is a hybrid where the boundary is drawn by the complexity and impact of the change.

Routine adjustments are self-service: rubric weight tuning, golden set additions, alert threshold changes, on-demand eval runs, report generation. These are frequent, low-risk, and well-suited to customer autonomy because the customer understands their own requirements better than the platform team does.

Strategic changes are platform-managed: initial eval configuration during onboarding, major rubric redesigns when the customer's use case evolves, migration to new judge versions, evidence package certification for regulatory audits. These are infrequent, high-impact, and require the platform team's expertise to execute correctly. A customer reconfiguring their rubric from scratch without guidance may create a configuration that technically validates but produces misleading quality signals — dimensions that overlap, weights that over-index on easily scored criteria, thresholds that are too lenient for the customer's actual risk profile.

The hybrid approach also serves as a training ground. Customers start with platform-managed configuration and gradually take on more self-service responsibility as they build confidence and expertise. A customer who was fully platform-managed at onboarding may, twelve months later, manage 80 percent of their eval configuration independently, only engaging the platform team for version migrations and annual rubric reviews. This progression is the operational maturity journey for each customer, paralleling the platform-level maturity model explored later in this chapter.

## How Self-Service Changes the Economics

The economic impact of self-service eval is measurable and significant. Without self-service, each customer requires an average of four to six hours per month of platform team time for eval management — rubric adjustments, golden set updates, quality investigations, report generation, and alert troubleshooting. At a fully loaded team cost of $120 per hour, that is $480 to $720 per customer per month in human operational cost alone.

With self-service adoption at 50 percent and a hybrid model where self-service customers require one to two hours per month of platform team time (mostly for strategic changes and exception handling), the blended average drops to two to three hours per customer per month. For a platform with 200 customers, this reduction — from an average of five hours to 2.5 hours per customer — saves 500 hours per month. That is roughly three full-time equivalents, or $450,000 to $600,000 per year in operational cost.

More importantly, self-service changes the scaling math. Without self-service, scaling from 200 to 400 customers requires roughly doubling the eval operations team — adding three to four people at a cost of $500,000 to $700,000 per year. With self-service, the same growth requires one to two additional people because the incremental per-customer workload is halved. Self-service does not eliminate the need for team growth. It makes team growth sub-linear with customer growth, which is the difference between a multi-tenant eval operation that scales profitably and one that scales into a cost crisis.

## The Trust Dividend

Beyond economics, self-service eval produces a trust dividend that is harder to quantify but equally valuable. Customers who can see their eval configuration, understand how their quality scores are calculated, and make adjustments on their own terms trust the platform more than customers who receive scores from an opaque system they cannot inspect or influence. The transparency effect is powerful: a customer who can open their quality dashboard and see exactly which rubric dimensions are weighted at what levels, which golden set examples define their quality standard, and which alert rules govern their notifications feels ownership over their quality outcomes. They are not receiving quality verdicts from a black box. They are participating in quality management as a partner.

This trust dividend shows up in measurable business outcomes. Platforms that offer self-service eval report 15 to 25 percent fewer escalations about quality score disagreements because customers who can inspect the eval configuration understand why a score is what it is. They report higher customer satisfaction scores in annual surveys, particularly on questions about platform transparency and control. And they report lower churn rates among customers who actively use self-service tools — not because self-service prevents quality issues, but because customers who feel in control are more likely to work with the platform to resolve issues rather than concluding the platform is unresponsive and looking for alternatives.

Self-service eval is also a pricing lever. As explored in Section 30, the self-service portal and its capabilities can be packaged as a premium feature within higher pricing tiers. Customers who want direct control over their quality criteria pay for the privilege, and the platform recovers the development cost of the self-service tooling through differentiated pricing. The platform builds the tool once and monetizes it across every customer who upgrades to access it.

Self-service handles routine configuration. But what happens when a customer's quality drops sharply and the routine tools are not enough? The next subchapter addresses anomaly escalation policy — the process that determines when a tenant receives premium evaluation attention, who triggers it, and how the platform prevents escalation from consuming the resources meant for normal operations.
