# 28.5 — The Baseline Sprint: Establishing Day-One Quality Metrics Before Optimization

Three weeks after signing a $480,000 annual contract, an enterprise document intelligence platform told their newest customer — a mid-size insurance company processing 12,000 claims per month — that quality had improved by 15 percent since launch. The customer's VP of Operations responded with a single question: "Fifteen percent better than what?" Nobody on the platform team had an answer. They had started optimizing the customer's prompts and retrieval pipeline on day three of the engagement, before running a single evaluation pass against the customer's actual production configuration. Every improvement claim was measured against the most recent checkpoint, not against a fixed starting point. The "15 percent improvement" was an accumulation of deltas between weekly snapshots, each measured against a moving target. When the VP asked for the absolute quality level — not the trend, but the number — the team could not produce one. The customer's confidence in the platform's quality reporting never fully recovered, and the renewal conversation eight months later was dominated by a request to "start over with real numbers this time."

This is the **Missing Baseline Problem**, and it is remarkably common. Without a fixed day-one quality measurement taken before any optimization begins, every subsequent quality conversation lacks an anchor. Improvement claims become unverifiable. SLA commitments become unmeasurable. The customer cannot tell whether they are getting better or whether the platform is just getting better at presenting the same performance in a more favorable light.

## What the Baseline Measures

The baseline is a comprehensive evaluation snapshot of the customer's output quality across every tracked dimension, taken against their actual production configuration, before anyone touches a setting to improve anything. It measures what the customer gets on day one — the unoptimized, uncurated, untuned starting point.

The baseline includes per-dimension scores for every rubric dimension calibrated during the alignment workshop. If the rubric has six dimensions — accuracy, completeness, tone, compliance, citation quality, and formatting — the baseline reports a score for each. It includes the composite quality score computed using the customer's calibrated dimension weights. It includes the score distribution shape for each dimension — not just the mean, but the percentiles. A mean accuracy of 0.85 where 90 percent of outputs score between 0.82 and 0.88 is a very different system than one with the same mean where 70 percent score above 0.90 and 30 percent score below 0.60. The distribution tells the customer whether quality is consistent or bimodal. It includes a failure mode categorization — the specific types of errors that appear in outputs scoring below the customer's threshold, grouped into categories that the customer's domain experts recognize. "Outdated regulatory references" is a failure mode. "Low score on dimension 3" is not.

The baseline also includes a comparison to what we call the **platform floor** — the minimum quality level your platform guarantees regardless of customer configuration. If the customer's baseline scores are above the platform floor on every dimension, the system is functioning as designed and optimization will improve an already-acceptable product. If the baseline scores fall below the platform floor on any dimension, you have a configuration problem or a coverage gap that needs to be fixed before optimization begins. Launching an optimization sprint against a system that is below the platform floor is like repainting a house with a cracked foundation. The paint looks nice. The foundation still needs fixing.

## Why the Baseline Must Reflect the Customer's Real Configuration

The baseline must be measured against the customer's actual production configuration — the specific prompt templates, model selection, retrieval pipeline, adapter weights, safety filters, and output formatting they will run in production. Not your staging defaults. Not a "clean" configuration your team assembled for testing. Not a curated subset of inputs that showcases the best outputs. The customer's real configuration running against representative samples of their real traffic.

This sounds obvious, but the temptation to measure baseline on a controlled setup is strong. Your staging environment is clean and predictable. The customer's production configuration might have a prompt template that was hastily written during discovery, a retrieval pipeline pointing at a knowledge base that was populated two days ago, and an adapter that was trained on a preliminary dataset. The staging baseline will look better. That is exactly why it is the wrong baseline.

The principle here is that the baseline must represent the experience the customer will actually have, not the experience you wish they would have. If the customer's production configuration produces mediocre outputs because their prompt template needs work, the baseline should reflect that mediocrity. When you later improve the prompt template and the scores rise from 0.72 to 0.86, the improvement is real, measurable, and attributable to a specific change. If you had baselined against a staging configuration that already scored 0.84, the same improvement would show as a two-point gain instead of a fourteen-point gain — obscuring the actual value your optimization delivered.

This aligns with the Configuration-Aware Evaluation principles covered in Chapter 4. Every evaluation that does not match the customer's actual production setup is evaluating a product the customer does not use. The baseline sprint is the first place this principle applies, and it is the place where violating it causes the most lasting damage because every future measurement is anchored to the baseline. A wrong baseline distorts every improvement claim that follows.

## The Baseline Report Format

The baseline report is the first evaluation artifact the customer sees, and it sets the standard for every subsequent quality report. The format should be informative without being overwhelming, and honest without being demoralizing.

The report opens with a summary section: the composite quality score, the overall pass rate against the customer's calibrated thresholds, and the total number of outputs evaluated. For a customer with a composite threshold of 0.80, the summary might read: "Composite quality score of 0.78 across 850 evaluated outputs. 62 percent of outputs meet or exceed your quality threshold." This is the number the customer's executive sponsor will read. Everything else is context.

The second section breaks down performance by rubric dimension. Each dimension shows its mean score, its score at the 25th and 75th percentiles, and the percentage of outputs meeting the customer's per-dimension threshold. A dimension where the mean is 0.87 but the 25th percentile is 0.61 tells the customer that a quarter of their outputs on that dimension are significantly below average — a consistency problem that the mean alone hides.

The third section categorizes failure modes. For every output that scored below the customer's threshold on any dimension, the baseline report groups the failure into a category. Typical categories include factual errors, missing information, tone misalignment, regulatory non-compliance, formatting violations, and hallucinated content. Each category shows its frequency and its impact on the composite score. A failure mode that occurs in 8 percent of outputs but drags the composite down by 0.06 points is a higher-priority fix than one that occurs in 15 percent of outputs but only affects the composite by 0.02 points. The failure mode categorization tells the customer not just "where are we" but "what should we fix first."

The fourth section compares the customer's baseline to the platform floor. For each dimension, the report shows whether the customer's score is above or below the floor. Dimensions above the floor are candidates for optimization — making good performance better. Dimensions below the floor are candidates for remediation — fixing a system that is not meeting minimum standards. This distinction matters because it changes the urgency and the resource allocation. Remediation needs to happen before the customer goes live. Optimization can happen iteratively over weeks or months.

## Setting Expectations With the Customer

Baseline scores are almost always lower than the customer expects. This is normal, and framing it correctly during the baseline readout is one of the highest-leverage moments in the onboarding relationship.

The customer signed a contract expecting a certain quality level. They heard numbers during the sales process — platform averages, case study results, benchmark scores. The baseline report shows them their specific quality level with their specific configuration, which is typically 10 to 20 points below what they expected. A customer who expected 0.90 accuracy sees 0.76 in the baseline report. Their instinct is alarm: "We were told this platform delivers 90 percent accuracy. We are at 76. This is unacceptable."

The framing that works is honest and forward-looking. "This baseline reflects your current configuration before any optimization. The platform average of 0.90 is achieved after a typical optimization cycle of four to six weeks. Your baseline of 0.76 is in the normal range for day-one measurements — most customers in your industry start between 0.72 and 0.82. The gap between 0.76 and your target of 0.90 tells us exactly where to focus optimization, and the failure mode analysis shows that 60 percent of the gap is driven by two specific issues we can address in the first two weeks."

This framing accomplishes three things. It normalizes the baseline score by placing it in the context of other customers' starting points. It converts the gap from a problem into a roadmap by connecting it to specific, addressable issues. And it sets a measurable improvement target that both sides can track. The customer leaves the readout not alarmed but informed, with a clear picture of where they start, where they are going, and what the path looks like.

What does not work is defensiveness. "The baseline is just a starting point, these numbers don't really mean anything yet" teaches the customer that your eval system produces numbers that do not mean anything. "Your configuration needs some adjustments, that is why the scores are low" implies the customer did something wrong. "Other customers see similar numbers" without specifics sounds like a dodge. Be precise. Be honest. Be specific about what drives the gap and what will close it.

## The Baseline as a Contractual Anchor

For enterprise customers with quality SLAs, the baseline is not just an operational tool. It is a contractual instrument. Quality improvement commitments in the contract are measured from the baseline. If the contract guarantees "a minimum 15 percent improvement in composite quality within 90 days," the baseline defines the starting point from which that 15 percent is calculated.

This makes the baseline measurement methodology a contract-relevant decision. How many outputs are sampled? What time period does the sample cover? Are the outputs selected randomly or stratified by use case? Is the baseline measured once or averaged over several days to account for variance? These are not just technical questions — they are terms that affect whether the contract's quality commitment is achievable and verifiable.

Best practice is to define the baseline methodology in the contract or in a jointly signed onboarding protocol. The methodology should specify: minimum sample size (typically 500 to 1,000 outputs for statistical reliability), sampling method (random sampling from the customer's actual traffic, or if pre-launch, from a representative test set), evaluation configuration (the customer's production config, explicitly stated), scoring rubric version (the calibrated rubric from the alignment workshop), and the evaluation date range. Both parties sign off on the baseline report, confirming that the methodology was followed and the scores are accepted as the starting point.

Without this formalization, baseline disputes can poison the relationship months later. "We don't agree with the baseline — your sample wasn't representative" or "You measured the baseline before our knowledge base was fully loaded" are arguments that are impossible to resolve retroactively. Formalizing the methodology upfront eliminates these disputes before they start.

## Common Mistakes in Baseline Measurement

Three baseline measurement mistakes are common enough to warrant explicit warning because each one produces a baseline that is either too optimistic, too pessimistic, or too unreliable to serve as an anchor.

**Measuring baseline on too small a sample.** A baseline computed on 50 outputs has a confidence interval so wide that a subsequent measurement on 50 outputs might show a 10-point improvement purely from sampling variance. The customer sees improvement. You report improvement. No actual improvement occurred. The minimum for a statistically meaningful baseline depends on the score variance, but for most multi-dimensional rubric evaluations, 500 outputs is the practical floor and 1,000 outputs is the comfortable target. Below 500, you are measuring noise.

**Measuring baseline on curated data instead of real traffic.** The temptation to cherry-pick baseline inputs is strong, especially when the customer is watching. "Let us use these 200 inputs that we know cover the main use cases" produces a baseline that reflects your test set, not the customer's actual traffic distribution. Real traffic includes the edge cases, the ambiguous inputs, the inputs in unexpected formats, and the inputs that your system handles worst. A curated baseline excludes exactly the cases where quality matters most, and the optimization work guided by that baseline will optimize for the wrong distribution.

**Setting the baseline before calibration is complete.** This mistake creates a circular problem. The baseline uses uncalibrated rubric definitions, which means the scores do not reflect the customer's quality definition. When you later calibrate the rubric and re-score the same outputs, the scores change — sometimes significantly. Now you have two baselines: the pre-calibration baseline that was reported to the customer and the post-calibration baseline that reflects their actual quality standard. Which one anchors the improvement commitment? If you use the pre-calibration baseline, your improvement numbers include the calibration adjustment, which is not a real quality improvement — it is a measurement correction. If you use the post-calibration baseline, you are retroactively changing a number the customer already saw. Neither option is clean. The solution is to always complete calibration before running the baseline sprint.

## The Sprint Itself

The baseline sprint is a focused one-to-two-week effort with a specific sequence of steps. Week one focuses on data collection and evaluation execution. Week two — if needed — handles analysis, report generation, and the baseline readout meeting with the customer.

Day one through day three: confirm the customer's production configuration is finalized and stable. Verify that the rubric calibration from the alignment workshop is applied to the judge pipeline. Confirm the sampling methodology with the customer — random sampling from live traffic if the customer is already live, or representative test set if pre-launch. Set up the evaluation pipeline to run against the customer's specific configuration, loading their prompt templates, model selection, retrieval settings, and any adapter weights.

Day three through day seven: execute the evaluation. Run the full rubric across the target sample. For a 1,000-output sample with six rubric dimensions scored by an LLM judge, the compute time is typically four to eight hours depending on the judge model. Run the evaluation twice on a 10 percent subsample to verify scoring consistency — if the same output receives materially different scores on two runs, the judge configuration needs debugging before the baseline is trustworthy.

Day seven through day ten: analyze the results. Compute per-dimension means, percentile distributions, and threshold pass rates. Categorize failure modes. Compare to the platform floor. Generate the baseline report. Conduct internal review — does the report tell a coherent story? Are there anomalies that need investigation before sharing with the customer?

Day ten through day fourteen: conduct the baseline readout meeting with the customer. Present the report, frame the scores in context, discuss the failure mode priorities, and jointly agree on the optimization targets. Get the customer's sign-off on the baseline as the contractual anchor.

Two weeks is sufficient for most customers. Rushing the sprint into one week increases the risk of measurement errors and leaves no buffer for anomaly investigation. Extending it beyond two weeks delays the start of optimization without improving baseline quality, and it signals to the customer that the platform is not operationally efficient.

The completed baseline establishes the foundation for every quality conversation that follows. When the customer asks "are we getting better?" the answer is a specific number measured against a specific starting point, not an anecdote or a feeling. When the contract review comes up and the SLA improvement target is evaluated, both sides are looking at the same anchor. The baseline sprint turns quality from a subjective conversation into an objective one — and in multi-tenant platforms serving hundreds of customers, objective quality conversations are the only ones that scale.

The baseline, however, was built through a manual process — discovery, calibration workshops, hands-on evaluation configuration. That process took weeks and required senior engineers from your team at every step. The next subchapter confronts the question every growing platform faces: how do you onboard twenty customers per month when the process that works for one customer at a time does not scale to ten?
