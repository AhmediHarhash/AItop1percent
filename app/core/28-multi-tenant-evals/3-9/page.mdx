# 28.9 — The Tenant Metadata Contract: The Fields You Must Never Lose

Every eval event must carry a canonical set of metadata fields. Lose any one of them and the event becomes unattributable, unauditable, and useless. This is not a best practice. It is a survival requirement for any platform that serves multiple customers from shared infrastructure and needs to prove, under audit pressure or regulatory scrutiny, that every evaluation result belongs to exactly one tenant, ran against exactly one configuration, used exactly one judge pipeline, and was computed at exactly one point in time. The **Tenant Metadata Contract** is the set of fields that makes this proof possible. Drop a single field and you create an eval event that floats in limbo — a score without a home, a result without an owner, a data point that cannot participate in any trend, any comparison, or any audit trail.

## Why Metadata Loss Is Worse Than Data Loss

When an eval event is lost entirely — deleted, dropped, never recorded — you know it is missing. A gap in the timeline, a missing score, a null entry in the report. Gaps are visible. Metadata loss is invisible. The eval event exists. The score is recorded. The result appears in dashboards and reports. But the event is missing the fields that tell you what it means. A score of 87 with no tenant identifier could belong to any of your five hundred customers. A score of 87 with no config hash could have been computed against any of seventeen different evaluation configurations that tenant has used over the past year. A score of 87 with no judge version could have come from a judge pipeline that was deprecated three months ago for producing inflated scores.

The insidious part is that metadata-incomplete events do not cause errors. They pass through your pipeline. They get stored. They get aggregated. They get shown in reports. Nobody notices they are incomplete until someone asks a question the metadata would have answered. A customer disputes a quality score from six weeks ago. Your team pulls the event and discovers it has no rubric version field. Was the score computed under rubric version 2.1, which the customer approved, or version 2.3, which you were testing internally? You do not know. The event cannot tell you. The dispute becomes unresolvable, and the customer's trust in your evaluation system takes a hit it may never recover from.

## The Canonical Field Set

The Tenant Metadata Contract defines the minimum set of fields that every eval event must carry. These are not optional. They are not "nice to have." Dropping any one of them creates a category of question that your platform can no longer answer.

**Tenant identifier** is the field that assigns ownership. Every eval event must be tagged with the unique identifier for the customer whose data was evaluated. Without it, results cannot be isolated, cannot be routed to the correct dashboard, and cannot be excluded from other tenants' views. In a platform with five hundred tenants, a single untagged event is a potential data exposure — if it appears in the wrong tenant's report, you have a breach.

**Workspace identifier** distinguishes between multiple environments within a single tenant. Enterprise customers often run separate workspaces for production, staging, and experimentation. An eval result from a staging workspace that appears in the production quality dashboard misleads the customer into thinking their production quality changed when it did not. The workspace identifier keeps these environments distinct.

**Config hash** is the fingerprint of the entire evaluation configuration — the combination of rubric, golden set, model version, adapter weights, judge pipeline, and sampling strategy that produced this specific result. The config hash is how you answer the question "did these two eval runs use the same setup?" If two runs produce different scores and share the same config hash, the model's behavior changed. If they have different config hashes, the configuration changed. Without this field, you cannot distinguish between model drift and configuration drift, which means you cannot diagnose the root cause of any score change.

**Model version** records which version of the production model generated the outputs being evaluated. When a platform runs three model versions in parallel — the current default, the canary for the next release, and a pinned version for a tenant that declined the upgrade — the model version field is what prevents scores from being aggregated across incompatible model generations. A quality trend that mixes scores from GPT-5 and GPT-5.2 outputs is meaningless.

**Adapter identifier** records which fine-tuned adapter, if any, was applied to the base model for this tenant. Many multi-tenant platforms in 2026 run shared base models with per-tenant LoRA adapters. The adapter identifier connects the eval result to the specific adapter version, which matters because adapter updates are per-tenant events that must be tracked independently from base model updates. An eval event with no adapter identifier cannot participate in adapter lifecycle analysis — you cannot tell whether a quality change was caused by an adapter update or a base model change.

**Judge version** records which version of the LLM judge pipeline scored the output. Judge pipelines evolve — you update the judge model, refine the scoring prompt, adjust calibration parameters. An eval event with no judge version cannot be compared to events scored by a different judge version. As Section 15 covers in detail, judge version changes can shift scores by three to eight points even when the underlying output quality is unchanged. Without this field, you mistake judge drift for model drift.

**Rubric version** records which version of the tenant's quality rubric was applied. As the previous subchapter described, rubrics evolve over the customer lifecycle. An eval event with no rubric version cannot be placed on the correct trend line. A score of 89 under rubric version 1.0 and a score of 84 under rubric version 2.0 represent different measurements against different standards. Without the rubric version field, they appear to show a decline that may not exist.

**Golden set version** records which version of the ground truth data was used as the reference standard. Golden sets are refreshed periodically — new examples added, outdated ones retired, edge cases updated. The golden set version tells you whether two eval runs are being compared against the same standard. Without it, a score improvement could mean the model got better or the golden set got easier. You cannot tell.

**Timestamp** is the most basic field but also the most commonly degraded. Not "the date of the eval run" but the precise time the evaluation was computed, in UTC, with enough resolution to disambiguate events within the same run. Timestamps with only date-level granularity cannot support intra-day trend analysis. Timestamps in local time zones without UTC offsets become ambiguous when tenants operate across regions.

**Region** records the physical location where the evaluation was executed. As the subchapter on evaluation locality will cover in detail, some tenants have strict requirements about where their data is processed. The region field is the proof that a given eval event was computed in the tenant's required legal jurisdiction. Without it, you cannot demonstrate data residency compliance.

## The OpenTelemetry Approach to Metadata Propagation

In 2026, the standard mechanism for propagating metadata through distributed systems is OpenTelemetry. The OpenTelemetry semantic conventions for generative AI — ratified as stable conventions in late 2025 — provide a standardized framework for attaching metadata to AI system events, including evaluation results. Rather than inventing your own metadata schema and propagation mechanism, you attach the Tenant Metadata Contract fields as OpenTelemetry span attributes and resource attributes, which are then propagated automatically through your eval pipeline and exported to your observability backend.

The advantage of using OpenTelemetry for metadata propagation is not just standardization. It is enforcement. OpenTelemetry exporters can be configured to reject spans that are missing required attributes. This means that an eval event without a tenant identifier is not just poorly tagged — it fails to export, which triggers an alert, which forces the pipeline to either attach the missing field or surface the gap before the event reaches storage. The metadata contract is not documented in a wiki that engineers read once and forget. It is encoded in the telemetry pipeline configuration as a set of required attributes that the system enforces on every event.

The specific implementation uses OpenTelemetry resource attributes for fields that are constant across an eval run — tenant identifier, workspace identifier, region — and span attributes for fields that may vary across individual evaluations within a run — config hash, model version, adapter identifier. The judge version, rubric version, and golden set version attach as span attributes on the scoring span, because different items within a single eval run could theoretically be scored by different judge configurations during a rolling migration. The timestamp is handled automatically by the OpenTelemetry SDK as the span's start time.

## What Breaks When Each Field Is Missing

Understanding the Tenant Metadata Contract requires understanding the specific failure mode that each missing field creates.

Without the tenant identifier, the eval event cannot be attributed. It appears in platform-wide aggregates but not in any tenant-specific view. If the eval involved tenant-specific data — which it almost always does — the unattributed event is also a potential data leak, because the data exists in storage without a tenant scope to restrict access.

Without the workspace identifier, staging results contaminate production dashboards. A tenant running experiments in their staging workspace sees those experimental scores mixed into their production quality trend. The apparent quality volatility confuses the customer and undermines their confidence in the eval signal.

Without the config hash, score changes become undiagnosable. Two eval runs show different results. Did the configuration change? Did the model's behavior change? Without the config hash, the engineering team must manually reconstruct what configuration was active at the time of each run — a process that takes hours and is error-prone. With the config hash, they compare hashes in seconds and know immediately whether the configuration was the same.

Without the model version, model upgrade impact cannot be measured. The platform ships a new model version, and quality scores shift. But which shifts are caused by the model change and which are caused by other factors? Without the model version on each event, you cannot isolate model-caused score changes from adapter-caused or rubric-caused changes. The upgrade analysis becomes guesswork.

Without the adapter identifier, adapter lifecycle management breaks. A per-tenant adapter is updated, retrained, or retired. The quality scores before and after must be attributed to the correct adapter version. Without this field, the adapter team cannot measure the impact of their changes and cannot determine whether a quality improvement was caused by their adapter update or by a coincidental model version change.

Without the judge version, score calibration becomes unreliable. The judge pipeline was updated two weeks ago, and a tenant's scores dropped by four points. Is this a real quality regression or a judge scoring change? Without the judge version field, you cannot correlate score shifts with judge updates, which means every score change is ambiguous.

Without the rubric version, historical trends are uninterpretable. The customer's quality report shows a twelve-month trend. But the rubric changed three times during that period. Which scores were computed under which rubric? Without this field, the trend is a composite of incompatible measurements presented as a continuous line.

Without the golden set version, ground truth integrity is unverifiable. An auditor asks "what reference standard was this score measured against?" You point to the current golden set. But the eval was computed six months ago, and the golden set has been refreshed twice since then. The current golden set is not the one that was used. Without the golden set version, you cannot retrieve the exact reference standard that was active at the time of the evaluation.

Without the region, data residency compliance is unprovable. A German banking customer requires that all data processing, including evaluation, occurs within the EU. Your eval pipeline processed their data. Where? Without the region field, you have no evidence that the processing occurred in the legally required jurisdiction.

## Schema Enforcement: Making the Contract Unbreakable

Documenting the Tenant Metadata Contract in a specification is necessary but insufficient. Engineers read specifications once, forget them, and write code that drops fields because the code works without them. The contract must be enforced in the pipeline itself, not in a document that sits alongside it.

Schema enforcement operates at three layers. The first layer is at event creation — the code that generates an eval event must include all required fields or fail to compile, fail to serialize, or fail to emit the event. In typed languages, this means defining the eval event as a type with non-optional fields. In dynamic languages, this means runtime validation at the event creation boundary. Either way, the goal is the same: an eval event missing a required field should never enter the pipeline.

The second layer is at the telemetry export boundary. OpenTelemetry exporters can be configured with attribute requirements. An exporter that rejects spans missing the tenant identifier, config hash, or judge version prevents incomplete events from reaching storage. Events rejected at the export boundary trigger an alert that surfaces the gap to the platform team. The alert says "eval pipeline in region eu-west-1 produced 47 events without adapter identifiers in the last hour," which is actionable. The platform team investigates, discovers a code path that was not propagating adapter context correctly, and fixes it before the gap compounds.

The third layer is at the storage and query boundary. Even if an incomplete event slips through the first two layers — a scenario that should be rare but will eventually happen — the storage layer should tag it as incomplete. Queries that aggregate eval results should filter out incomplete events by default, or at minimum flag them. A tenant quality dashboard that includes unannotated events in its trend line is silently degrading the trend's accuracy. A dashboard that excludes them and shows "12 of 4,847 events excluded due to incomplete metadata" is transparent about the gap without letting it corrupt the analysis.

## The Cost of Retroactive Metadata Recovery

When a metadata field is lost, teams inevitably attempt to reconstruct it. This effort is almost always more expensive than getting the metadata right in the first place. Reconstructing a model version requires correlating event timestamps with deployment logs, identifying which model version was active in which region at the time the eval ran, and hoping that the deployment logs are complete and the timestamps are precise enough to resolve ambiguities during rolling deployments. Reconstructing a rubric version requires finding the rubric configuration that was active for that tenant at that timestamp, which means querying a configuration version history that may or may not retain the granularity you need.

A mid-stage platform that processes fifty thousand eval events per day and loses metadata on one percent of them generates five hundred events per day that require manual reconstruction. At an average of fifteen minutes per event, that is 125 person-hours per day of retroactive metadata recovery. No team can sustain that. The events stay incomplete. The gaps accumulate. The quality of your data layer degrades silently until the next audit or customer dispute reveals that months of evaluation data are partially unattributable.

The investment in schema enforcement — the typed event definitions, the exporter validation, the storage-layer tagging — costs a few engineering-weeks to implement and prevents thousands of hours of retroactive recovery. It is one of the highest-leverage infrastructure investments a multi-tenant eval platform can make.

## The Metadata Contract as an Organizational Boundary

The Tenant Metadata Contract is not just a technical specification. It is an organizational contract between the teams that produce eval events and the teams that consume them. The eval pipeline team promises that every event will carry the full set of fields. The reporting team builds dashboards that depend on those fields being present. The compliance team designs audit processes that rely on those fields for traceability. The customer success team creates quality reports that assume every score can be attributed to a specific configuration and time period.

When the contract is violated — when fields are missing — every downstream team suffers. The reporting team produces dashboards with gaps. The compliance team cannot complete audit responses. The customer success team cannot resolve quality disputes. The pain is not felt by the team that dropped the field. It is felt everywhere else, weeks or months later, by people who had no visibility into the metadata propagation code.

This is why the contract must be owned by a specific team — typically the platform evaluation infrastructure team — and enforced as a system property rather than a team norm. The owning team defines the schema, configures the enforcement layers, monitors the completeness metrics, and treats any metadata gap as a priority incident. The contract is reviewed when new fields are proposed, which happens as the platform's evaluation capabilities expand. Adding a field to the contract is a deliberate decision with implications for every producer and consumer. Removing a field — which should be exceedingly rare — requires migration of all consumers that depend on it.

The Tenant Metadata Contract is the foundation that every other isolation and auditability mechanism in this chapter depends on. Without it, audit trails are incomplete, config provenance is broken, data residency is unprovable, and tenant isolation is unverifiable. With it, every eval event is a self-contained record that answers who, what, when, where, and how — the questions that auditors, customers, and regulators will inevitably ask. The next subchapter addresses a specific field in that contract — the config hash — and explains how to generate, store, and use it to achieve reproducibility across tenant evaluation runs.
