# 28.73 — Centralized vs Federated Eval Teams: Who Owns Per-Customer Quality

Everyone assumes that a centralized evaluation team is the right way to own quality across a multi-tenant platform. Centralize the expertise, centralize the tooling, centralize the methodology, and let one team handle all five hundred customers. It sounds efficient. It is also wrong — or more precisely, it is right for the first thirty customers and a liability by the time you reach a hundred. The centralized model scales the infrastructure. It does not scale the judgment. And per-customer quality is, at its core, a judgment problem that requires intimacy with each customer's domain, priorities, and definition of "good."

But the opposite model — fully federated, where each customer success manager or account team owns their customer's evaluation — is equally wrong. Federated ownership creates intimacy at the cost of consistency. When fifty different people configure rubrics, maintain golden sets, and interpret quality scores with no shared methodology, you end up with fifty different evaluation systems wearing the same brand. The answer is neither pure centralization nor pure federation. It is a hybrid that centralizes what must be consistent and federates what must be contextual.

## The Centralized Model and Its Breaking Point

The centralized model works beautifully when your platform serves ten to thirty enterprise customers. A team of three to five evaluation engineers owns the entire eval pipeline: they build the rubrics, curate the golden sets, run calibration exercises, configure per-tenant scoring, and produce quality reports. Every customer's evaluation passes through the same hands, which means methodology is consistent, tooling improvements benefit everyone simultaneously, and quality debates are resolved by people who understand evaluation deeply. At this scale, the centralized team knows every customer by name. They remember that Customer A cares about citation accuracy more than tone, that Customer B's domain experts disagree with each other about what "concise" means, and that Customer C's golden set has a known gap in their regulatory document coverage.

The breaking point arrives between fifty and a hundred customers, and it is not gradual. It is a cliff. The centralized team that handled thirty customers with personal attention is now triaging. Quality reviews that used to take two hours per customer per week are compressed to thirty minutes. Golden set updates that used to happen monthly slip to quarterly. Calibration exercises that used to involve the customer's domain experts become internal-only because scheduling workshops across seventy customers is logistically impossible. The team knows the methodology perfectly. They no longer know the customers. A quality regression for Customer 47 sits in a queue behind regressions for Customers 12 through 46. The eval engineers become bottlenecks, and the most common complaint from customer success managers becomes, "I filed a quality issue three weeks ago and haven't heard back."

The centralized team responds predictably: they standardize. Instead of custom rubrics per customer, they create templates. Instead of per-customer golden sets, they build shared golden sets by vertical. Instead of individual calibration exercises, they run batch calibrations by customer tier. Each of these shortcuts is individually rational and collectively devastating. The platform's evaluation system gradually loses the per-customer specificity that made it valuable. Customers start receiving quality reports that feel generic — technically correct but missing the nuances that matter to their specific use case.

## The Federated Model and Its Fragmentation

The federated model swings to the opposite extreme. Each customer success manager or account team owns their customer's evaluation configuration, golden set maintenance, and quality interpretation. The CSM who talks to the customer three times a week also configures their eval rubric, reviews their quality reports, and decides when a regression is serious enough to escalate.

This model excels at customer intimacy. The person who owns the eval knows the customer's priorities, their domain quirks, their internal politics, and which quality dimensions generate the most complaints. When Customer A's VP of Product mentions in a call that citation accuracy has become more important because of a new compliance mandate, the CSM updates the rubric weights that afternoon. The feedback loop between customer need and eval configuration is tight.

The fragmentation problem emerges within months. CSM 1 defines "accuracy" as factual correctness of claims. CSM 2 defines "accuracy" as faithfulness to retrieved documents. CSM 3 uses the word "accuracy" in rubric descriptions but means something closer to completeness. All three show an "accuracy" score in their customer's quality report, and all three scores measure different things. When the platform team tries to benchmark quality across customers, the numbers are incomparable. When a new CSM takes over an account, they inherit a rubric configuration they do not understand because the previous CSM built it from personal intuition rather than documented methodology.

The deeper problem is capability mismatch. Customer success managers are relationship experts, not evaluation engineers. Asking a CSM to maintain a golden set, calibrate LLM judge prompts, and interpret statistical significance in quality score changes is asking them to do a job they were not hired for and have not been trained in. Some CSMs rise to the challenge. Most produce evaluation configurations that are well-intentioned but technically unsound — rubrics with overlapping dimensions, golden sets with undetected distribution skew, threshold settings that generate either no alerts or constant false positives.

## The Hybrid Model

The hybrid model draws a clear line between what must be centralized and what must be federated. That line separates infrastructure and methodology from configuration and interpretation.

The **central eval platform team** owns the evaluation infrastructure (the pipeline, the judge orchestration, the scoring engine, the reporting system), the evaluation methodology (rubric design standards, golden set curation guidelines, calibration protocols, statistical methods for drift detection), and the judge management (which LLM judges are available, how they are prompted, how they are versioned, how their performance is monitored). This team also owns the tooling that federated teams use — the rubric configuration interface, the golden set management tools, the quality dashboard, and the alert configuration system. They are building a product for internal users, and the internal users are the federated customer teams.

The **federated customer teams** own the per-customer evaluation configuration (which dimensions matter, how they are weighted, what thresholds trigger alerts), the per-customer golden sets (curated from the customer's actual domain data, validated by the customer's domain experts), the quality interpretation (what a score change means for this specific customer, whether a regression is business-critical or cosmetic), and the customer communication (explaining quality reports, facilitating calibration sessions, translating customer feedback into eval configuration changes). The federated team does not build eval tooling. They use the tooling the central team provides.

## Staffing the Hybrid

The central eval platform team for a platform serving 200 to 500 enterprise customers typically comprises four to six engineers and one to two evaluation methodology specialists. The engineers build and maintain the eval pipeline, the judge orchestration, the dashboards, and the configuration tooling. The methodology specialists define rubric standards, train federated teams on calibration techniques, audit per-customer configurations for quality, and resolve methodology disputes when federated teams disagree about how to measure something.

The federated side operates at a ratio of roughly one eval-capable person per fifteen to twenty enterprise customers. "Eval-capable" does not mean a dedicated eval engineer per pod — it means someone on the customer team (often a technically oriented CSM or a solutions engineer) who has been trained in evaluation methodology and uses the central team's tooling to configure and maintain their customers' evaluation setups. At 300 enterprise customers, that is fifteen to twenty people on the federated side, plus the four to six on the central team. A platform with 500 customers and a mix of enterprise and mid-market might have a central team of six to eight and a federated group of twenty-five to thirty.

## The Interface Contract

The line between central and federated teams only works if both sides agree on what they provide to each other. This **interface contract** defines mutual obligations.

The central team commits to providing stable, versioned eval tooling with documented APIs and configuration interfaces. They commit to publishing rubric design guidelines that federated teams follow. They commit to running quarterly methodology audits that review a sample of per-customer configurations for correctness. They commit to a response time SLA for infrastructure issues — when the eval pipeline breaks, the central team owns the fix, and the SLA is typically four hours for tier-1 impact and twenty-four hours for tier-3 impact.

The federated teams commit to following the methodology guidelines the central team publishes. They commit to using the tooling as designed rather than building shadow systems. They commit to escalating methodology questions — "should we create a new quality dimension for this customer, or can we achieve the same coverage by adjusting weights on existing dimensions?" — rather than improvising solutions. They commit to maintaining their customers' golden sets at the cadence the central team prescribes.

When the two sides disagree — and they will — the resolution mechanism matters. A federated team member who believes a customer needs a rubric structure that the central team's guidelines do not support should escalate to the methodology specialist, not invent their own approach. The methodology specialist either approves an exception, updates the guidelines to accommodate the new pattern, or explains why the existing guidelines already handle the case. This escalation path prevents both rigidity (the central team ignoring legitimate customer needs) and fragmentation (federated teams making up their own rules).

## Transitioning from Centralized to Hybrid

If your platform currently runs a centralized eval team and you are hitting the scaling wall, the transition to hybrid follows a predictable sequence.

First, document the methodology. Everything the centralized team does by instinct — how they choose rubric dimensions, how they calibrate thresholds, how they decide when a golden set needs updating — must be written down as repeatable guidelines. This documentation becomes the training material for federated teams and the standard that methodology audits enforce.

Second, build the tooling. The centralized team's eval configuration process, which currently lives in their heads and their scripts, becomes a product — a configuration interface that federated team members can use without understanding the underlying pipeline. This tooling investment is the largest cost of the transition, typically three to four months of engineering time.

Third, train the first cohort. Select five to eight customer team members who are technically capable and interested. Train them on evaluation methodology using the documented guidelines. Have them shadow the centralized team on real customer configurations. Certify them when they can independently configure a customer's evaluation setup to the central team's standards.

Fourth, transfer accounts gradually. Start with tier-2 and tier-3 customers whose evaluation configurations are straightforward. The first cohort of federated team members takes ownership of fifteen to twenty customers each. The centralized team remains available for consultation and reviews the first few configurations from each federated member before granting autonomous operation.

Fifth, retain the hard cases. Tier-1 and tier-0 customers, those with complex regulatory requirements or highly customized evaluation setups, often stay with the central team longer. Some may never fully transfer — and that is acceptable. The goal of the hybrid model is not to eliminate centralized evaluation, but to scale the federated layer so the central team can focus on the customers and challenges that genuinely require deep evaluation expertise.

The transition typically takes six to nine months from first documentation effort to fully operational hybrid model. Rushing it produces federated team members who are certified but not competent, and customer configurations that pass audits but miss the subtleties that the centralized team used to catch instinctively. Section 29 covers the governance structures — decision rights, escalation hierarchies, and accountability frameworks — that keep the hybrid model from drifting back toward either extreme.

The hybrid model defines who owns what. But the hardest organizational seam in multi-tenant evaluation is not within the eval team — it is between the eval team and the customer success team. The next subchapter examines that interface and the failures that emerge when quality measurement and relationship management operate in isolation.
