# 28.3 — Customer-Specific Rubrics That Scale: Parameterized Scoring Without Per-Customer Code

In mid-2025, a document processing platform serving 140 enterprise customers made a decision that seemed pragmatic at the time: when their fifth-largest customer required a custom evaluation rubric — one that weighted medical terminology accuracy higher than the platform default — an engineer wrote a dedicated scoring function for that customer. When the eighth-largest customer needed a rubric that penalized passive voice in legal summaries, another engineer wrote another dedicated function. Within nine months, the platform had 23 customer-specific scoring functions, each maintained by whichever engineer happened to be available when the customer's needs changed. When a platform-wide improvement to the scoring engine shipped in early 2026, it required manual updates to all 23 custom functions. Fourteen were updated. Nine were missed. Those nine customers ran on stale scoring logic for six weeks before anyone noticed — and three of them experienced score inflation that masked a genuine quality regression because their custom rubric had not received a fix for a known scoring bias.

The platform's CTO later called this "the custom code trap." The trap works like this: writing one custom scoring function is trivial. Writing five is manageable. Writing 23 is already a maintenance burden. Writing 200 is organizational suicide. Every custom function must be tested independently, updated when the scoring engine changes, monitored for drift, and documented well enough that someone other than the original author can modify it. At 200 customers, you need a team whose full-time job is maintaining per-customer eval code — code that does not add features, does not improve the product, and exists purely because the evaluation system was not designed for configuration-driven customization.

The alternative is **parameterized rubrics** — a rubric architecture where the scoring logic is shared across all customers and the per-customer variation is expressed entirely through configuration. The moment you write custom evaluation code for one customer, you create a maintenance burden that scales linearly with your customer count. Parameterized rubrics eliminate that linear scaling by making customer-specific evaluation a data problem rather than a code problem.

## The Anatomy of a Parameterized Rubric

A parameterized rubric separates the scoring mechanism from the scoring criteria. The mechanism — how to evaluate an output against a dimension, how to combine dimension scores into a composite, how to apply thresholds — is shared code maintained by the evaluation engineering team. The criteria — which dimensions to evaluate, what weight each carries, what examples illustrate good and bad performance, what thresholds trigger alerts — are configuration loaded from the tenant's eval fingerprint at runtime.

To make this concrete, consider a single dimension: terminology accuracy. The scoring mechanism takes an output, a reference terminology list, and a scoring scale, then produces a score based on how many domain-specific terms were used correctly. That mechanism is the same for every customer. What changes per customer is the reference terminology list, the scoring scale anchors, and the weight this dimension carries in the composite score. A legal customer's fingerprint provides a terminology list of 400 legal terms with their preferred forms. A medical device customer's fingerprint provides a terminology list of 1,200 product names and regulatory codes. The scoring mechanism processes both lists identically — the difference is in the data it receives, not the logic it executes.

This separation means that onboarding a new customer never requires writing new evaluation code. It requires populating the customer's fingerprint with the appropriate configuration — selecting dimensions from the platform's dimension library, providing domain-specific reference data, setting weights and thresholds, and validating the configuration against sample outputs. An evaluation engineer who onboards a new customer is configuring, not coding. That distinction is what makes the system scale from 5 customers to 500 without a proportional increase in engineering headcount.

## The Dimension Library: Your Platform's Quality Vocabulary

Parameterized rubrics depend on a well-maintained **dimension library** — the catalog of all quality dimensions the platform knows how to evaluate. Each dimension in the library is a discrete, testable evaluation capability with defined inputs, outputs, and scoring behavior. The library is the platform's quality vocabulary, and every tenant's fingerprint is a sentence constructed from that vocabulary.

Building the dimension library is an ongoing investment. You start with the dimensions that your earliest customers need — factual accuracy, formatting compliance, tone appropriateness, perhaps five or six core dimensions. As you onboard more customers, the library grows. A healthcare customer needs clinical accuracy, which requires a new dimension with specific scoring logic for medical claims. A financial services customer needs numerical precision, which requires a dimension that evaluates mathematical correctness at a granularity that the generic accuracy dimension cannot provide. Each new dimension is built once, added to the library, and immediately available for inclusion in any tenant's fingerprint.

The library should contain between 20 and 50 dimensions for a mature platform. Fewer than 20 and you probably cannot capture the quality variation across diverse enterprise customers. More than 50 and you likely have redundant dimensions that overlap in what they measure or dimensions so narrow that they serve only a single customer. The sweet spot is enough dimensions to cover the quality vocabulary of your market without so many that the library itself becomes difficult to maintain and validate.

Each dimension in the library has a standard interface. It accepts an output, optional reference data specific to the tenant, and a scoring configuration that specifies the scale, the anchors, and any dimension-specific parameters. It returns a score on a normalized scale. This standard interface is what makes dimensions composable — any combination of dimensions can be assembled into a tenant-specific rubric without integration work, because every dimension speaks the same scoring language.

The library also needs a deprecation process. Dimensions that are no longer used by any tenant, or dimensions that have been superseded by a more capable replacement, should be marked as deprecated rather than deleted. Deprecated dimensions remain available for historical evaluation replay but are not offered for new fingerprint configurations. This prevents the library from growing indefinitely while preserving the ability to understand historical evaluation results.

## Configuration Schema Design

The configuration that drives parameterized rubrics must follow a strict schema. A loose schema — one that allows free-form fields or unvalidated parameters — creates the same maintenance problem as custom code, just in a different location. Instead of debugging custom scoring functions, you end up debugging malformed configuration files that produce unexpected scores.

A well-designed schema enforces several constraints. First, every dimension referenced in a tenant's configuration must exist in the dimension library. If a configuration specifies "brand_voice_alignment" as a dimension, the schema validation confirms that the dimension library contains an entry by that name with valid scoring logic. Referencing a non-existent dimension is a configuration error caught at validation time, not a runtime failure discovered when the evaluation pipeline crashes.

Second, weights must sum to 100 percent. This sounds trivial, but a surprising number of quality issues in multi-tenant eval stem from weight configurations that sum to 97 or 103 percent. The schema validator catches this before the configuration enters production. Some platforms allow weights that do not sum to 100, relying on normalization at scoring time. This is a design choice, but it creates confusion in reporting — a customer who sees that their five dimensions are weighted at 15, 20, 25, 20, and 15 expects them to sum to 95 and wonders where the other 5 percent went. Requiring the sum to equal 100 eliminates that confusion.

Third, thresholds must fall within valid ranges. A critical floor of 120 percent or a target of negative 5 is clearly an error, but subtler misconfigurations are also possible. A critical floor set higher than the target creates a logical impossibility — the output must exceed the floor to avoid a critical alert, but the floor is above the target, meaning every output that meets the target also exceeds the floor, making the floor meaningless. The schema validator checks for these logical inconsistencies.

Fourth, reference data provided for each dimension must conform to the dimension's expected format. If the terminology accuracy dimension expects a list of term-preferred form pairs, the schema validates that the reference data contains pairs, not unpaired terms or terms with multiple preferred forms. Format validation at configuration time prevents scoring errors that are difficult to diagnose at evaluation time.

## Scoring at Runtime: How the Pipeline Consumes Configuration

When the evaluation pipeline processes an output for tenant 247, the sequence is precise. The pipeline looks up tenant 247's active fingerprint version. From the fingerprint, it extracts the rubric configuration — the list of active dimensions, their weights, their reference data, and their threshold definitions. For each dimension in the list, it invokes the corresponding scoring function from the dimension library, passing the output and the tenant-specific reference data. Each scoring function returns a normalized score. The pipeline computes the weighted composite score, applies the tenant's threshold definitions, and records the result.

This sequence executes identically for every tenant. The only variation is in the data — which dimensions, which weights, which reference data, which thresholds. The code path is the same whether you have 5 tenants or 5,000. Adding a new tenant adds a new configuration entry and zero new code. Modifying a tenant's quality requirements modifies their configuration and zero existing code. Improving a scoring function improves it for every tenant that uses the dimension simultaneously.

The runtime overhead of configuration-driven scoring is minimal. Loading a tenant's fingerprint configuration is a single read from a configuration store, cacheable at the evaluation worker level. Invoking scoring functions from the dimension library is the same operation whether the function was selected from a configuration or hardcoded in a custom function. The marginal cost of parameterization — a configuration lookup per evaluation — is negligible compared to the cost of the scoring itself, which involves model inference or embedding comparison.

## Handling Edge Cases: When Configuration Is Not Enough

Parameterized rubrics handle the vast majority of per-customer evaluation needs through configuration. But honesty requires acknowledging that some customer requirements do not fit cleanly into a configuration-driven model. A customer might require scoring logic that is fundamentally different from anything in the dimension library — not a different parameterization of an existing dimension, but a qualitatively different evaluation approach.

When this happens, the right response is not to abandon the parameterized model and write custom code. The right response is to ask whether the customer's requirement represents a dimension that other customers might also need. If a legal customer requires evaluation of logical argument structure — something no existing dimension measures — the answer is usually to build a "logical coherence" dimension, add it to the library, make it available to all tenants, and activate it in the legal customer's fingerprint. The customer's unique need becomes a platform capability that enriches the dimension library for everyone.

Truly one-off requirements — evaluation logic that serves exactly one customer and will never serve another — are rare. In four years of operating a multi-tenant eval platform at scale, the pattern holds that at least one other customer eventually needs any dimension that seemed unique when first requested. But when a genuine one-off arises, the pragmatic solution is a "custom dimension" slot in the fingerprint that accepts a customer-specific scoring function while keeping it within the fingerprint framework rather than as a standalone custom pipeline. This preserves the benefits of the parameterized model — unified scoring, consistent reporting, central monitoring — while accommodating the exception. The custom dimension is tracked, monitored, and flagged for review each quarter to determine whether it should be generalized into the library or deprecated.

## The Maintenance Dividend

The long-term advantage of parameterized rubrics becomes visible when you need to make platform-wide scoring improvements. Consider a scenario where your evaluation team discovers that the factual accuracy dimension produces inflated scores on outputs containing statistical claims — the dimension checks whether claims are plausible but does not verify that cited statistics are correct. The fix requires updating the scoring logic to include statistical verification.

In a custom-code model, this fix must be individually applied to every customer-specific scoring function that includes factual accuracy evaluation. If 160 out of 200 customers use factual accuracy, that is 160 code changes, each requiring testing in the context of the customer's specific scoring function. In practice, this means weeks of engineering work and the near certainty that some customers will be missed.

In a parameterized model, the fix is a single change to the factual accuracy dimension in the dimension library. Every tenant whose fingerprint includes factual accuracy immediately benefits from the improvement on the next evaluation run. One code change, one test suite, one deployment, 160 customers upgraded simultaneously. The scoring improvement is available to new customers onboarded the next day without any additional work.

This maintenance dividend compounds over time. Every scoring improvement, every bug fix, every calibration adjustment propagates to all tenants that use the affected dimension. Over two years of operation, a platform with parameterized rubrics might deploy 40 or 50 dimension improvements, each one automatically reaching every affected tenant. A platform with custom code would need to manually propagate each improvement to each customer, accumulating a maintenance debt that eventually becomes so large that the team stops making improvements altogether — not because the improvements are not valuable, but because the propagation cost exceeds the improvement value.

## Migration From Custom Code to Parameterized Rubrics

If your platform already has customer-specific scoring code, migration to parameterized rubrics is possible but requires discipline. The process works in three phases.

The first phase is dimension extraction. You analyze every custom scoring function to identify the underlying dimensions it evaluates. Most custom functions, when examined closely, are variations on a small number of dimensions with different parameters. Two customers might have completely different scoring functions that both fundamentally evaluate terminology accuracy — they just do it with different term lists and different scoring scales. Extracting these latent dimensions and formalizing them in a dimension library typically reduces 50 custom functions to 8 to 12 library dimensions.

The second phase is parity validation. For each customer migrating from a custom function to a parameterized configuration, you run both the old custom function and the new parameterized rubric against the same outputs and compare scores. The scores should be equivalent within a tight tolerance — if the parameterized rubric produces significantly different scores, either the dimension configuration is wrong or the library dimension's logic does not capture something the custom function was doing. Parity validation catches these discrepancies before they affect the customer.

The third phase is cutover, done one customer at a time. You switch the customer's evaluation from the custom function to the parameterized configuration, monitor for score deviations over a two-week burn-in period, and decommission the custom function once parity is confirmed. This customer-by-customer approach is slower than a bulk migration but dramatically safer — a scoring discrepancy affects one customer, not all of them.

The parameterized rubric architecture ensures that your scoring system scales with your customer count without proportional engineering investment. But scoring is only meaningful when measured against ground truth — the curated examples that define what "correct" looks like for each customer. The next subchapter addresses the challenge of building, versioning, and protecting per-customer golden sets at scale.