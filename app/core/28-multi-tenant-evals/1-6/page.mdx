# 28.6 — The Spectrum of Multi-Tenancy: Shared Model to Dedicated Instance and Everything Between

The engineering lead stares at a whiteboard covered in arrows, boxes, and crossed-out alternatives. On the left side, the words "Shared Model" sit above a single box with three hundred tenant labels pointing into it. On the right side, "Dedicated Instance" sits above three hundred separate boxes, each with one tenant label. Between them, a dozen half-drawn architectures stretch across the board — shared base with per-tenant adapters, shared inference with per-tenant routing, shared model with per-tenant retrieval, dedicated fine-tunes on shared infrastructure. The team has been debating for two weeks. The conversation keeps circling back to the same question: where on this spectrum should we build, and what does each choice mean for how we evaluate quality?

That question has no single right answer. But it has a clear framework, and the framework starts with recognizing that multi-tenancy is not a binary choice between "everyone shares everything" and "everyone gets their own." It is a spectrum with at least five distinct architectures, each with its own cost profile, customization ceiling, operational complexity, and — critically for this section — evaluation surface. Where you sit on the spectrum determines what your evaluation system needs to do, how much per-tenant isolation it must enforce, and how complex your release management becomes.

## Architecture One: Fully Shared Model

The simplest multi-tenant architecture is a single model serving all tenants through a shared inference endpoint. Every tenant sends requests to the same model with the same weights. Customization comes entirely from the system prompt and any per-tenant instructions injected at request time. This is how most multi-tenant AI platforms start, and for good reason — it is the cheapest to operate, the simplest to deploy, and the easiest to maintain. One model version, one deployment, one set of infrastructure.

The evaluation surface of a fully shared model is deceptively simple. You have one model to evaluate, which sounds easy until you realize that one model must meet three hundred different quality definitions. The model itself is identical for all tenants, but the behavior varies because the prompts vary. Tenant A's system prompt asks for formal legal analysis. Tenant B's system prompt asks for casual customer chat. Tenant C's system prompt asks for structured data extraction. Same weights, radically different expected behavior. Your evaluation must test the model under each tenant's prompt configuration, against each tenant's quality rubric, using each tenant's representative data.

The failure mode specific to fully shared models is what you might call **prompt interference** — the phenomenon where a model update that improves behavior under one prompt style degrades it under another. You upgrade from GPT-5 to GPT-5.1 because benchmarks show improvement across the board. And the benchmarks are right — for most prompt styles. But Tenant C's structured extraction prompt relied on a specific pattern-following behavior that GPT-5 exhibited and GPT-5.1 does not. Their extraction accuracy drops from ninety-one percent to seventy-eight percent. If your evaluation only tests the model against a generic benchmark, you will miss this entirely. If your evaluation tests against each tenant's actual prompt configuration, you will catch it before the upgrade reaches production.

Fully shared models are appropriate when tenant customization requirements are modest — differences in tone, length, formatting — and when the quality variance between tenants is acceptable. They become untenable when tenants need fundamentally different model behaviors, when quality requirements diverge beyond what prompt engineering can address, or when one tenant's performance requirements conflict with another's. At that point, you need to move along the spectrum.

## Architecture Two: Shared Model with Per-Tenant Retrieval

The next step along the spectrum keeps a shared model but gives each tenant its own retrieval pipeline. Every tenant's requests go to the same language model, but the context injected into each request comes from tenant-specific document stores, knowledge bases, or vector indexes. This is the standard architecture for multi-tenant RAG systems, and it adds a significant new evaluation dimension: retrieval quality is now per-tenant, even though generation quality comes from a shared model.

The evaluation surface expands substantially. You must now evaluate retrieval and generation separately for each tenant. Tenant A's vector index might contain fifty thousand legal documents with dense, technical language. Tenant B's index might contain two thousand product FAQs with short, colloquial entries. The same embedding model and the same retrieval parameters will perform differently on these two corpora. Chunk size, overlap, embedding model, re-ranking strategy — any of these can be optimal for one tenant and suboptimal for another. Your evaluation pipeline must track per-tenant retrieval metrics — precision at k, recall at k, mean reciprocal rank — independently from per-tenant generation metrics.

The failure mode unique to this architecture is **retrieval-generation attribution ambiguity**. When a tenant's output quality drops, you need to determine whether the problem is in retrieval (the model received bad context) or generation (the model received good context but produced a bad response). This attribution requires evaluation at two points in the pipeline rather than one, and the attribution must be per-tenant because the retrieval quality baseline is different for each tenant. Section 7 covers RAG evaluation in depth; what this section adds is the requirement to run those evaluations per-tenant, with per-tenant baselines, and to attribute regressions correctly across the retrieval-generation boundary for each tenant independently.

## Architecture Three: Per-Tenant LoRA Adapters on a Shared Base

This is the architecture that has become the 2026 standard for B2B AI platforms that need meaningful per-tenant customization without the cost of dedicated model instances. A single base model — Llama 4 Maverick, Mistral Large 3, or a similarly capable open-weight foundation — runs on shared infrastructure. Each tenant gets a **LoRA adapter**: a small set of trained weights that modify the base model's behavior for that tenant's specific domain, tone, task, or data distribution. At inference time, the correct adapter is loaded for each request, producing tenant-specific output from shared hardware.

The serving infrastructure that makes this practical has matured rapidly. **LoRAX**, the multi-LoRA inference server originally developed by Predibase, can serve thousands of concurrent adapters on a single GPU by dynamically loading and offloading adapter weights between GPU and CPU memory. It batches requests for different adapters together using heterogeneous continuous batching, which means requests for Tenant A's legal adapter and Tenant B's customer service adapter can share the same batch without interference. vLLM, the dominant open-source inference engine in 2026, now supports native multi-LoRA serving with similar batching capabilities. Both systems expose OpenAI-compatible APIs with per-request adapter selection, making them drop-in replacements for single-model serving with the added capability of per-tenant customization.

The evaluation surface of the adapter architecture is fundamentally different from shared-model evaluation, and this is where many teams underestimate the complexity. Each adapter is a different model, even though they share a base. Tenant A's adapter, trained on their legal corpus, produces different outputs than Tenant B's adapter, trained on their support transcripts. You are no longer evaluating one model under different prompts. You are evaluating hundreds of models that happen to share a base. Every adapter must be evaluated independently against its tenant's quality rubric, using its tenant's representative data, with its tenant's risk-appropriate sample sizes.

The failure modes multiply accordingly. **Adapter degradation** occurs when a base model update breaks adapters that were trained against a previous base version. You upgrade the base from Llama 4 Maverick v1.2 to v1.3, and forty percent of your adapters degrade because the weight space they were trained in has shifted. Your evaluation must test every adapter against its tenant's quality definition after any base model change. **Adapter interference** occurs when the adapter serving system's memory management — loading and offloading adapters between GPU and CPU — introduces latency spikes or context corruption during high-concurrency periods. Your evaluation must include load-testing scenarios where many tenants' adapters are active simultaneously, not just isolated per-adapter quality tests. Chapter 6 covers the release gate architecture for adapter lifecycle management in detail.

## Architecture Four: Dedicated Fine-Tuned Models on Shared Infrastructure

Some tenants require more customization than a LoRA adapter can provide. When a tenant's domain is sufficiently specialized — medical coding, patent analysis, regulatory compliance in a specific jurisdiction — a full fine-tune of a base model produces meaningfully better results than an adapter. In this architecture, each tenant (or each tenant tier) gets a fully fine-tuned model, but those models run on shared infrastructure managed by your platform team.

The evaluation surface resembles dedicated instances more than shared models. Each tenant's model is a completely independent artifact with its own weights, its own capabilities, and its own failure modes. But because the infrastructure is shared, you still face multi-tenant operational challenges: scheduling fine-tuned models onto available GPUs, managing model loading times, preventing one tenant's inference traffic from starving another's, and coordinating model updates across tenants that share hardware.

The evaluation complexity specific to this architecture is **model version sprawl**. If you have fifty tenants with dedicated fine-tunes, you have fifty models to evaluate. Each model has its own training data, its own training configuration, its own evaluation dataset, and its own quality baseline. When you update the base model that underpins all fifty fine-tunes, you must re-train and re-evaluate all fifty. When a tenant's data changes and triggers a re-fine-tune, you must evaluate the new model against the previous version to detect regressions. The evaluation volume scales linearly with tenant count, and the coordination complexity scales worse than linearly because interactions between model updates and shared infrastructure create second-order effects.

This is the architecture where evaluation cost becomes a meaningful line item. Evaluating fifty models with per-tenant golden sets of three hundred examples each, using LLM-as-judge pipelines that themselves consume model inference, generates evaluation costs that can exceed ten percent of your serving costs. Section 24 covers system cost engineering; in the dedicated fine-tune architecture, evaluation cost engineering becomes a per-tenant pricing decision.

## Architecture Five: Fully Dedicated Instances

At the far end of the spectrum, each tenant gets their own infrastructure — their own model, their own GPU allocation, their own serving endpoint, their own evaluation pipeline. This is the architecture for your highest-tier enterprise customers: the ones who pay enough to justify dedicated resources, who require strict isolation for regulatory reasons, or whose workload characteristics are so different from other tenants that sharing infrastructure creates unacceptable interference.

The evaluation surface for dedicated instances is conceptually simplest — you are evaluating a single-tenant system, which is the problem that Sections 3, 15, and 26 already cover. But the operational challenge is that you are running this conceptually simple evaluation multiplied by however many dedicated tenants you have. If you have fifteen tenants on dedicated instances, you have fifteen independent evaluation pipelines to maintain, fifteen sets of golden data to curate, fifteen alerting configurations to tune, and fifteen release processes to coordinate. The evaluation architecture is not more complex per-tenant. It is more complex in aggregate because you lose the economies of scale that shared infrastructure provides.

The failure mode unique to dedicated instances is **evaluation drift between tenants**. When evaluation runs independently for each tenant, the evaluation methodology itself can diverge over time. Tenant A's evaluation pipeline gets updated with a new judge model version. Tenant B's does not, because the update required a change that conflicted with Tenant B's compliance requirements. Tenant C's evaluation uses a sampling strategy that was updated six months ago but nobody propagated to Tenants A and B. Over time, you are no longer comparing apples to apples across tenants, which makes cross-tenant quality analysis unreliable and platform-wide improvement difficult to measure.

## The Bridge Architecture and Why It Dominates

In practice, most mature B2B AI platforms in 2026 do not sit at a single point on the spectrum. They operate a **bridge architecture** that spans multiple points simultaneously. The majority of tenants — often eighty to ninety percent — run on the shared base model with per-tenant LoRA adapters. A smaller set of high-value or highly specialized tenants get dedicated fine-tunes on shared infrastructure. The highest-tier or most regulated tenants get dedicated instances. The platform team manages all three tiers through unified tooling, but the evaluation requirements differ at each tier.

This bridge architecture is why AI gateways like Portkey and LiteLLM have become essential infrastructure for multi-tenant platforms. Portkey's per-request routing can direct each tenant's traffic to the correct model, adapter, or dedicated instance based on tenant configuration, while tracking per-tenant cost, latency, and quality metrics through a single observability layer. LiteLLM provides the same routing capability as an open-source, self-hosted option that teams deploy as an internal gateway. Both systems give the evaluation pipeline a single point of instrumentation — one place where you can sample requests, tag them with tenant context, and route evaluation data to the appropriate per-tenant pipeline.

The evaluation architecture for a bridge setup must handle all five spectrum positions simultaneously. It needs shared-model evaluation for the prompt-only tenants. It needs per-adapter evaluation for the LoRA tenants. It needs per-model evaluation for the dedicated fine-tune tenants. It needs per-instance evaluation for the isolated tenants. And it needs a unified reporting layer that lets you compare quality across all these architectures, because a tenant who outgrows their LoRA adapter might graduate to a dedicated fine-tune, and you need to prove that the migration improved their quality rather than regressing it.

## Choosing Where to Place a Tenant

The decision about where each tenant sits on the spectrum is not purely technical. It is a product, financial, and contractual decision that shapes your evaluation burden. Three factors drive the placement. First, the tenant's customization needs — do they require behavior changes that prompt engineering can handle, that an adapter can handle, or that only a full fine-tune can deliver? Second, the tenant's isolation requirements — do their compliance obligations require dedicated infrastructure, or is logical isolation within shared infrastructure sufficient? Third, the tenant's revenue — does the contract justify the marginal cost of more dedicated resources and more intensive evaluation?

Getting this placement wrong in either direction is expensive. Place a tenant too low on the spectrum — shared model when they need an adapter — and you cannot meet their quality requirements, leading to churn. Place a tenant too high — dedicated instance when an adapter would suffice — and you burn margins on infrastructure and evaluation overhead that the contract does not support. Chapter 5 covers the customer onboarding process where this placement decision is made, using a structured assessment of the five divergence dimensions covered in the previous subchapter to determine where each new tenant belongs.

## The Evaluation Tax at Each Level

Every step up the spectrum increases your per-tenant evaluation cost. Shared model evaluation costs nearly nothing per additional tenant — you run the same model against different test sets. Adapter evaluation costs the inference required to run each adapter against its test set plus the cost of maintaining per-adapter golden sets. Dedicated fine-tune evaluation costs the full model evaluation for each tenant plus the ongoing cost of re-evaluation after each training run. Dedicated instance evaluation costs the full pipeline maintenance for each tenant's independent system.

These costs are real and must be factored into your per-tenant pricing. A tenant on a LoRA adapter might cost you $200 per month in evaluation overhead. A tenant on a dedicated fine-tune might cost $1,200 per month. A tenant on a dedicated instance might cost $3,500 per month. If your pricing does not account for these differences, you will lose money on your most demanding customers — the very customers whose retention matters most for your competitive position. Section 30 covers how to build these costs into your pricing model.

The next subchapter maps this section's territory against adjacent sections — distinguishing what Sections 3, 15, 26, and 27 cover from what this section uniquely addresses: the per-customer dimension that turns standard evaluation into multi-tenant evaluation.