# 28.10 — Evaluating the Bridge Model: Base Plus Adapter Interactions

In March 2025, a legal technology platform serving 140 law firms upgraded their shared base model from Llama 3.1 70B to Llama 4 Maverick. The upgrade was thoroughly tested against the platform's standard eval suite and showed a 9 percent improvement in document summarization quality, a 12 percent improvement in citation accuracy, and a 4 percent reduction in latency. Every metric looked better. The team deployed on a Thursday. By Monday, 23 law firms had filed support tickets. Their custom fine-tunes — per-tenant LoRA adapters trained over the previous eight months to handle each firm's specific legal terminology, formatting conventions, and jurisdictional preferences — were producing outputs that ranged from subtly degraded to completely incoherent. One firm's contract review adapter began inserting clauses from the wrong legal jurisdiction. Another firm's litigation summary adapter started generating outputs in a formal style that was clearly from the base model rather than the firm's trained voice. The platform had tested the new base model. They had not tested the new base model combined with any of the 140 adapters trained against the old one. The remediation took six weeks, cost $420,000 in emergency retraining, and resulted in the loss of two enterprise customers worth a combined $1.1 million in annual revenue.

This is **Adapter-Base Regression** — the pattern where upgrading the shared base model silently degrades per-tenant LoRA adapters that were trained against the previous base weights. It is the defining evaluation challenge of the multi-tenant bridge model architecture, and it is a failure mode that single-tenant systems never encounter because they do not separate the base model from per-tenant adaptations.

## How the Bridge Model Architecture Works

The **bridge model architecture** is the dominant pattern for multi-tenant AI platforms in 2026. Instead of running a separate model instance for each customer — prohibitively expensive at scale — the platform runs one shared base model and loads per-tenant LoRA adapters at inference time. When a request arrives for Tenant 47, the serving infrastructure loads Tenant 47's adapter weights into the base model, processes the request, and returns the result. When the next request arrives for Tenant 203, the infrastructure swaps Tenant 203's adapter weights in. Frameworks like LoRAX, vLLM with S-LoRA, and Predibase's serving stack handle this swap efficiently using techniques like heterogeneous continuous batching and adapter exchange scheduling, which allow hundreds of adapters to share a single GPU cluster with minimal latency overhead.

The economics are compelling. Running 140 separate full model instances for 140 law firms would require roughly 70 A100 GPUs at current 2026 pricing — approximately $840,000 per year in compute alone. The bridge model architecture serves all 140 from a cluster of 8 GPUs with adapter swapping, reducing compute to approximately $96,000 per year. The 89 percent cost reduction is why nearly every multi-tenant AI platform that offers fine-tuning has adopted this architecture. The per-tenant LoRA adapters are small — typically 0.1 to 2 percent of the base model's parameters — and can be stored in CPU memory and swapped to GPU on demand with single-digit millisecond overhead.

The architecture works beautifully until you change the base model.

## Why Adapter-Base Interactions Create Unique Evaluation Requirements

A LoRA adapter does not encode absolute behavior. It encodes a correction relative to a specific base model's weights. During training, the adapter learns low-rank matrices that modify specific attention layers in the base model to shift its behavior toward the tenant's desired output patterns. These corrections are precise adjustments calibrated to the specific representations the base model produces at each layer. The adapter does not know what a "good legal summary" looks like in the abstract. It knows how to nudge this particular base model's representations toward producing legal summaries in this particular firm's style.

When you swap in a new base model — even an improved version from the same model family — the internal representations change. The attention patterns shift. The residual stream carries different information at different layers. The adapter's corrections, calibrated to the old representations, are now being applied to new representations they were never trained against. The result is unpredictable. Sometimes the adapter's corrections are compatible with the new base weights and the output is fine or even improved. Sometimes they are incompatible and the output degrades. Sometimes they are catastrophically incompatible and the output is incoherent.

This is not a bug in LoRA or in the adapter training process. It is a fundamental property of how parameter-efficient fine-tuning works. The adapter and the base model form a coupled system. Evaluate either one in isolation and you get misleading results. The base model without the adapter scores well on general benchmarks — which is exactly what the platform team tested before deploying. The adapter applied to the old base model scores well on tenant-specific benchmarks — which is what the adapter was validated against when it was trained. But the adapter applied to the new base model was never tested by anyone, because the combination did not exist until the upgrade moment. That untested combination is what every tenant actually runs.

## Detecting Adapter-Base Regression Before Deployment

The only reliable way to detect Adapter-Base Regression is to evaluate every active adapter against the new base model before deploying the base model upgrade to production. This means running each tenant's eval suite with the new base model plus their existing adapter and comparing the results to their historical baseline scores on the old base model plus the same adapter.

The evaluation pipeline for this works in three stages. The first stage is **compatibility scoring**. For each active adapter, load it onto the new base model in a staging environment, run the tenant's eval dataset, and compute all tracked quality metrics. Compare these scores to the last evaluation run on the old base model with the same adapter. Compute the per-metric delta. Flag any adapter where any metric drops by more than a threshold you define — typically a Cohen's d of 0.3 or a raw score drop of 3 points, whichever is more conservative for that metric.

The second stage is **impact classification**. Flagged adapters are classified into three categories. Green means the adapter's quality is unchanged or improved on the new base — the upgrade is safe for this tenant. Yellow means the adapter's quality dropped measurably but remains above the tenant's contractual quality floor — the upgrade is risky but not immediately harmful, and the adapter should be scheduled for retraining. Red means the adapter's quality dropped below the tenant's quality floor or shows catastrophic degradation — the tenant must not be upgraded until the adapter is retrained and revalidated.

The third stage is **upgrade decision**. Based on the impact classification, the platform team decides the rollout strategy. Tenants with green adapters are upgraded immediately. Tenants with yellow adapters are upgraded with enhanced monitoring and a fast rollback trigger. Tenants with red adapters are held on the old base model version until their adapter is retrained against the new base weights and revalidated.

For the legal technology platform that lost $1.1 million in revenue, this three-stage process would have taken approximately four days of compute — running 140 adapter-base compatibility evaluations, each taking roughly 40 minutes — and would have identified the 23 incompatible adapters before the upgrade shipped. Four days of eval compute versus six weeks of remediation and two lost customers.

## Adapter Staleness Detection

Not every adapter degradation is caused by a base model upgrade. Adapters also go stale over time as the tenant's use patterns evolve away from the training data, as the domain vocabulary shifts, or as the tenant's expectations change without a corresponding adapter update. **Adapter staleness** is the gradual divergence between what the adapter was trained to do and what the tenant currently needs it to do.

Detecting adapter staleness requires continuous evaluation, not just pre-upgrade evaluation. Each adapter should be evaluated against its tenant's eval suite on a regular cadence — weekly for high-value tenants, monthly for standard-tier tenants. The metrics to track are not just absolute quality scores but the trend. An adapter producing a steady quality score of 88 is healthy. An adapter whose quality score has drifted from 91 to 85 over three months is going stale, even though 85 might still be above the quality floor.

The staleness signal becomes actionable when combined with a retraining recommendation engine. The engine compares the adapter's current eval performance to its performance at training time. If the gap exceeds a threshold — typically a 5 to 8 percent relative decline — the engine generates a retraining recommendation that includes the estimated retraining cost, the expected quality improvement based on historical retraining outcomes for similar adapters, and the priority based on the tenant's tier and contract value.

Staleness detection also serves as an early warning system for Adapter-Base Regression. An adapter that is already stale — operating near its quality floor — is far more vulnerable to a base model upgrade than a fresh adapter operating well above its floor. The staleness metric feeds directly into the impact classification during pre-upgrade evaluation. A stale adapter flagged as yellow on compatibility might be reclassified as red because it has no quality margin to absorb even a small degradation from the base model change.

## The Retraining Cascade

When a base model upgrade produces red and yellow classifications for a significant fraction of adapters, the platform faces a **retraining cascade** — the need to retrain dozens or hundreds of adapters against the new base model in a compressed timeline.

Consider a platform with 300 active adapters upgrading from one base model generation to the next. Pre-upgrade evaluation flags 40 adapters as yellow and 15 as red. The 15 red adapters belong to tenants who cannot be upgraded until retraining is complete. The 40 yellow adapters can be upgraded with monitoring but should be retrained soon. The total retraining workload is 55 adapters.

Each adapter retraining requires the tenant's training dataset (typically 5,000 to 50,000 examples), compute for fine-tuning (30 minutes to 4 hours on an A100 depending on dataset size and adapter rank), post-training evaluation against the tenant's eval suite, and a human review checkpoint for high-value tenants. At an average of 2 hours per adapter including compute, validation, and review, 55 adapters represent 110 hours of work — roughly two weeks for a team of four engineers working in parallel.

During those two weeks, the 15 red tenants remain on the old base model. This means your serving infrastructure must run both the old and new base models simultaneously. The old model serves red-adapter tenants and any yellow-adapter tenants who opt out of the early upgrade. The new model serves everyone else. This dual-model serving increases infrastructure cost by 30 to 50 percent for the duration of the retraining cascade. The cost is temporary but not negligible — for a platform spending $96,000 per year on serving, a two-week dual-model period adds approximately $2,800 to $3,700 in extra compute.

The prioritization of retraining within the cascade follows the same revenue-weighted logic as regression testing. Red adapters for the highest-revenue tenants are retrained first. Yellow adapters for high-revenue tenants follow. Red adapters for lower-revenue tenants come next, then yellow adapters for lower-revenue tenants. This ordering minimizes the revenue at risk during the retraining period and gets the most important customers onto the new base model as quickly as possible.

The retraining cascade also reveals a strategic tension. Frequent base model upgrades — say, quarterly — mean smaller per-upgrade adapter compatibility gaps but more frequent retraining cascades. Infrequent upgrades — annually — mean larger gaps when they happen but fewer disruptions overall. Most platforms in 2026 settle on a semi-annual major base model upgrade cycle with quarterly minor updates that preserve adapter compatibility. The minor updates adjust inference parameters, update safety classifiers, or swap retrieval components but do not change the base model weights that adapters are trained against.

## Adapter Version Pinning

Not every tenant can or should upgrade simultaneously. **Adapter version pinning** allows specific tenants to remain on a specific combination of base model version and adapter version while the rest of the platform moves forward.

Pinning is the right choice for several scenarios. A tenant in the middle of a regulatory audit cannot accept any model behavior change until the audit is complete. A tenant whose adapter is classified as red and whose retraining will take more than a week needs to stay on the old base model rather than experience degraded quality. A tenant who has explicitly negotiated a "no changes without approval" clause in their contract — common in financial services and healthcare — must be pinned until they approve the upgrade.

The implementation requires that your serving infrastructure support multiple base model versions simultaneously. In vLLM and LoRAX, this means running separate model worker pools — one per base model version — and routing tenant requests to the correct pool based on their pinning configuration. The routing layer checks the tenant's version pin against the available model pools and directs the request accordingly. If a tenant's pin references a model version that is no longer deployed, the request fails with an explicit error rather than silently falling back to the current version.

The cost of pinning is infrastructure fragmentation. Each pinned base model version requires its own GPU allocation, its own health monitoring, and its own operational support. Three active base model versions means three times the serving infrastructure overhead. This cost creates a natural incentive to minimize the duration and number of pins — you want every tenant migrated to the current base model as quickly as their constraints allow. The platform should track pinning metrics: number of pinned tenants, total revenue on pinned versions, average pin duration, and longest active pin. A platform where 30 percent of tenants are pinned on a version that is two generations behind has a maintenance burden that compounds with every subsequent upgrade.

The governance policy for pinning should define maximum pin duration — typically 90 days for standard tiers and 180 days for regulated industries — and require explicit renewal if the pin needs to extend. Without expiration policies, pins accumulate indefinitely. A platform that supported three base model versions last year supports six this year and twelve next year. Each version requires security patches, compatibility testing, and operational knowledge. The combinatorial burden of maintaining old versions is the hidden cost of unlimited pinning, and it becomes unsustainable faster than most teams expect.

## Building the Adapter-Base Evaluation Pipeline

The complete evaluation pipeline for the bridge model architecture combines pre-upgrade compatibility testing, continuous staleness monitoring, retraining cascade management, and version pinning governance into a single system. The pipeline operates on two timescales.

On the slow timescale — triggered by base model upgrades — the pipeline runs compatibility scoring across all active adapters, classifies adapters into green, yellow, and red categories, generates the retraining queue, configures version pins for red tenants, and produces the upgrade rollout plan that specifies which tenants move to the new base model on day one, which move after monitoring confirms stability, and which wait for retraining.

On the fast timescale — running continuously in production — the pipeline monitors per-tenant quality metrics for adapter staleness, detects quality degradation that emerges after upgrade even in adapters classified as green, triggers automatic retraining recommendations when staleness thresholds are breached, and alerts the platform team when a pinned tenant's pin is approaching expiration.

The pipeline integrates with the configuration-aware regression matrix described in the previous subchapters. Each adapter-base combination is a configuration in the matrix. When the base model changes, every adapter-base combination is a configuration that needs regression testing. The configuration space sampling strategies — cluster representatives, boundary configs, revenue weighting — all apply to the adapter space. Adapters trained on similar data with similar ranks will cluster together and can be represented by a single cluster representative. Adapters at the extreme of rank, dataset size, or training duration are boundary configurations. Adapters for the highest-revenue tenants get priority testing.

The bridge model evaluation pipeline is the most complex evaluation system most multi-tenant platforms will build. It operates across model versions, adapter versions, tenant configurations, and time — a four-dimensional evaluation surface that no single-tenant system encounters. But it is the price of the bridge model architecture's economics. You saved 89 percent on serving costs by sharing a base model. You invest a fraction of those savings into the evaluation infrastructure that makes sharing safe. The alternative — discovering Adapter-Base Regression from customer support tickets — costs orders of magnitude more in lost contracts, emergency retraining, and eroded trust.

Chapter 5 shifts from evaluating existing customers to a different challenge entirely: onboarding new customers into the evaluation system, bootstrapping ground truth from nothing, and establishing the quality baselines that everything in this chapter depends on.
