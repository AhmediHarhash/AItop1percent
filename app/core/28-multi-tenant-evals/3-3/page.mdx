# 28.3 — Ground Truth Separation: Why Shared Golden Sets Are a Liability

The instinct to share golden set infrastructure across tenants is strong, and it feels rational. Building per-tenant golden sets is expensive. Maintaining them is labor-intensive. A shared golden set infrastructure — shared annotation tooling, shared storage, shared versioning, shared quality assurance processes — reduces that cost by amortizing the infrastructure investment across every tenant on the platform. The math looks compelling: one golden set pipeline serving 200 tenants costs a fraction of 200 independent pipelines. Engineering leadership sees the consolidation as efficiency. Finance sees it as margin improvement. Both are wrong about what they are actually consolidating.

When you share golden set infrastructure, you are not sharing tooling. You are sharing the definition of truth. And truth in multi-tenant evaluation is not a shared resource. It is a per-tenant contract. The moment one tenant's definition of "correct" bleeds into another tenant's ground truth, every evaluation score produced against that contaminated ground truth is unreliable. The efficiency gain from sharing infrastructure disappears the first time a contaminated golden set produces a false-positive quality certification that a customer relies on for a regulatory filing.

## What Contamination Looks Like in Practice

In early 2026, a B2B customer support automation platform maintained a shared golden set of 4,200 annotated interactions used to evaluate response quality across all tenants. The platform served 95 enterprise customers across financial services, telecommunications, and retail. The shared golden set contained examples from all three industries, annotated with a common rubric that assessed accuracy, helpfulness, tone, and resolution completeness. The platform's evaluation team considered this approach sound because the rubric dimensions were universal — every customer cares about accuracy and helpfulness.

The contamination started with annotation drift. The annotation team that maintained the golden set developed calibration norms influenced by the dominant industry on the platform: financial services, which represented 55 percent of the tenant base. Over months, "helpful" came to mean "provides precise regulatory-compliant information with appropriate disclaimers," because that is what helpfulness looks like in financial services. The annotators were not deliberately biased. They were human, and the majority of examples they annotated came from financial services contexts. Their internal standard for what constituted a helpful response shifted toward the financial services definition without anyone noticing.

The consequence showed up in the retail customers' evaluation scores. A telecommunications company's customer support outputs that said "I'd be happy to help you upgrade your plan" without providing a detailed technical comparison of all available plans scored lower on helpfulness than the same response would have scored a year earlier. A retail customer's outputs that used casual, brand-appropriate language scored lower on tone because the annotation team's calibration for "appropriate tone" had shifted toward the formal register that financial services customers required. The retail customers' quality scores declined by 4 to 7 points over six months, not because their quality changed but because the shared ground truth's definition of quality migrated toward another industry's standards.

None of the retail customers connected their declining scores to annotation drift in a shared golden set. They assumed the platform's model quality was degrading for their use case and began evaluating alternatives. Two of the platform's twelve retail customers churned within that six-month period, citing "declining quality" as the primary reason. The platform's evaluation dashboard showed the retail segment's average quality declining, which the evaluation team interpreted as a model performance issue. They spent three months optimizing the model for retail use cases before an intern performing a routine annotation audit discovered that the golden set's helpfulness annotations had drifted significantly from the original calibration guidelines.

## The Five Contamination Vectors in Shared Golden Sets

Annotation drift is one contamination vector, but it is not the only one. Shared golden set infrastructure creates five distinct pathways for cross-tenant influence, each requiring a different prevention mechanism.

The first vector is **annotation calibration bleed**. When annotators work across multiple tenants' examples in a shared pipeline, their calibration for each quality dimension is influenced by the distribution of examples they see. If 60 percent of examples come from healthcare tenants, the annotators' implicit standard for accuracy shifts toward clinical precision. This does not require error or bias — it is a natural consequence of human calibration being influenced by recent experience. The prevention mechanism is annotator assignment isolation: annotators working on Tenant A's golden set examples do not work on Tenant B's during the same calibration window. As Section 13 discusses in the context of labeling operations, annotator calibration is fragile and context-dependent. Cross-tenant annotation assignment destroys that calibration.

The second vector is **example leakage**. In a shared storage system, golden set examples from one tenant can appear in another tenant's evaluation sample. This is the data isolation problem discussed in the previous subchapter, applied specifically to ground truth. The consequences are worse than production data leakage because golden set examples define what "correct" looks like. If a healthcare tenant's golden set includes examples from a retail tenant, the evaluation scores computed against that golden set measure the model's performance against the wrong definition of correctness. The scores are not just inaccurate — they are structurally misleading, because they measure the wrong thing with high confidence.

The third vector is **schema influence**. Even when examples are separated per tenant, a shared annotation schema — the set of dimensions, scales, and guidelines that annotators use — can create cross-tenant influence. A schema updated to accommodate one tenant's requirements changes the annotation instructions for all tenants. Adding a "regulatory compliance" dimension to the shared schema because one financial services tenant requested it means every tenant's annotators now see that dimension in their workflow, even if it does not apply to their industry. Annotators exposed to irrelevant dimensions make slower, less accurate annotations because their cognitive load increases. And if annotators begin scoring the irrelevant dimension even for tenants where it does not apply — because the schema tells them to — the resulting annotations distort the golden set.

The fourth vector is **version coupling**. A shared golden set has a single version history. When the evaluation team updates the golden set — adding new examples, correcting annotations, adjusting calibration standards — the update applies to all tenants simultaneously. If the update introduces a regression in one industry's examples while improving another's, the tenants whose golden set quality degraded have no way to roll back without affecting the tenants whose golden set improved. Version coupling means that golden set quality is a single dial for the entire platform, and any adjustment that helps some tenants harms others.

The fifth vector is **selection bias in shared pools**. When the evaluation pipeline samples examples from a shared golden set, the sampling distribution may not match any individual tenant's data distribution. A shared pool of 4,200 examples with 55 percent from financial services, 30 percent from telecom, and 15 percent from retail means that a random sample drawn for a retail tenant's evaluation will predominantly contain examples from other industries. Even if the sampling is stratified by tenant, the shared pool's overall composition influences the stratification boundaries, the available examples per stratum, and the statistical power of per-tenant evaluations. A retail tenant with 630 examples in a shared pool of 4,200 has much less golden set coverage than a financial services tenant with 2,310 examples, leading to less reliable evaluation scores for the smaller tenants.

## The Real Cost of Separation

The argument against per-tenant golden sets is cost, and the cost is real. Building a per-tenant golden set requires curating examples specific to each tenant's domain, annotating those examples with tenant-specific rubrics, maintaining calibration across tenant-specific annotation teams, versioning each golden set independently, and storing each set in tenant-isolated infrastructure. For a platform with 200 tenants, this is a substantial investment.

The cost breaks down into three categories. First, **curation cost**: identifying and selecting golden set examples from each tenant's production data. This typically requires 4 to 8 hours per tenant for the initial set and 1 to 2 hours per month for ongoing maintenance. At 200 tenants, that is 800 to 1,600 hours for initial curation and 200 to 400 hours per month for maintenance. Second, **annotation cost**: labeling each tenant's examples against their specific quality rubric. With per-example annotation costs ranging from $0.50 to $3.00 depending on domain complexity, a golden set of 100 to 200 examples per tenant costs $10,000 to $120,000 for initial annotation across 200 tenants. Third, **infrastructure cost**: per-tenant storage, versioning, and access control for golden sets. This is typically the smallest cost — a few hundred dollars per month in cloud storage and access management — but it is the cost that grows most directly with tenant count.

These numbers sound large. They are large. But the comparison is not between per-tenant golden sets and zero golden sets. It is between per-tenant golden sets and shared golden sets that produce contaminated evaluation scores. The retail customers who churned from the customer support platform represented $840,000 in annual contract value. The three months of engineering time spent optimizing the model for a problem that turned out to be annotation drift cost approximately $210,000 in engineering salary and opportunity cost. The total cost of the shared golden set's contamination — $1.05 million in direct impact — exceeded the cost of building per-tenant golden sets for every retail customer on the platform by a factor of four.

## The Archetype Approach to Manageable Separation

Full per-tenant golden set independence does not require building everything from scratch for every customer. The practical approach is tenant archetypes — groups of tenants whose quality definitions are similar enough that they share a golden set template, with per-tenant customization applied on top.

A healthcare archetype might define golden set structure for medical documentation quality: clinical accuracy, terminology compliance, regulatory language, patient safety flags. Every healthcare tenant starts with this archetype's template — a base set of 50 annotated examples, a calibration guide, and a dimension weighting pattern. Each tenant then customizes the template with their specific terminology, their specific regulatory requirements, and 30 to 50 tenant-specific examples that capture their unique quality characteristics. The result is a golden set that is 40 to 60 percent shared across the archetype and 40 to 60 percent unique to the tenant.

This archetype approach reduces curation and annotation costs by 50 to 70 percent compared to fully independent golden sets while preserving the per-tenant isolation that prevents contamination. The archetype template is maintained by the evaluation team and updated based on industry trends and regulatory changes. The per-tenant customizations are maintained through the Quality Contract process described in the previous chapter. When the archetype template is updated, the update is proposed to each tenant in the archetype as a golden set revision — not applied automatically, because automatic updates would reintroduce the version coupling problem that per-tenant separation is designed to prevent.

## Storage and Access Architecture for Per-Tenant Golden Sets

Per-tenant golden sets require storage architecture that enforces isolation by default rather than relying on query-level filtering. The recommended approach, consistent with the data isolation principles from the previous subchapter, is per-tenant object store prefixes with independent access policies. Each tenant's golden set lives under a dedicated prefix — a separate logical namespace within your object store — with access policies that restrict read and write access to evaluation processes authorized for that tenant.

The versioning model for per-tenant golden sets should follow the same principles discussed in Section 3's treatment of ground truth management. Each golden set version is immutable once published. Updates create new versions, never modify existing ones. Historical evaluation results reference the golden set version that was active when they were computed, enabling exact reproduction of any historical evaluation. When a customer disputes a quality score from three months ago, you can reproduce the evaluation using the exact golden set version, exact rubric configuration, and exact model version that produced the original score. This reproducibility is a contractual requirement for many enterprise customers and a regulatory requirement in healthcare and financial services.

The access architecture must also account for the annotation workflow. Annotators working on Tenant A's golden set need read-write access to Tenant A's golden set storage and no access to any other tenant's storage. This is straightforward with per-tenant access policies but becomes complicated when annotators work across multiple tenants — which they often do, because maintaining a dedicated annotation team per tenant is cost-prohibitive for most platforms. The solution is time-scoped, tenant-scoped access grants: an annotator is granted access to Tenant A's golden set for the duration of their annotation assignment, and that access is automatically revoked when the assignment ends. The annotator's next assignment for Tenant B grants them access to Tenant B's golden set and removes access to Tenant A's. At no point does the annotator have concurrent access to multiple tenants' golden sets.

## Cross-Referencing Golden Set Integrity With Evaluation Scores

Per-tenant golden sets create a per-tenant integrity requirement: each golden set must be validated independently for annotation quality, coverage, and calibration. As Section 26 discusses in the context of scaling evaluation systems, golden set quality degrades over time as the domain evolves and the golden set becomes stale. In multi-tenant systems, this degradation happens at different rates for different tenants. A healthcare tenant whose regulatory landscape shifts due to new guidelines may need golden set updates every quarter. A logistics tenant in a stable domain may need updates annually. A shared golden set would force both tenants onto the same update cycle, either updating the logistics tenant's ground truth unnecessarily or leaving the healthcare tenant's ground truth stale.

Per-tenant golden set monitoring detects degradation by tracking the correlation between golden set scores and real-world quality outcomes. If a tenant's golden set scores remain stable while their customer satisfaction metrics decline, the golden set may no longer reflect the tenant's actual quality requirements — a signal that the golden set needs revision. If golden set scores drop while customer satisfaction remains stable, the golden set's calibration may have drifted from the tenant's current definition of quality. Both signals require investigation, and both can only be detected at the per-tenant level. A platform-wide correlation between golden set scores and outcomes would mask these per-tenant divergences in the same way that platform-wide accuracy masks per-tenant accuracy problems.

The investment in per-tenant ground truth separation is significant but bounded. The contamination risk of shared golden sets is unbounded — it compounds over time as annotation drift accumulates, as the tenant base diversifies, and as regulatory scrutiny increases. The archetype approach provides a practical middle path that captures most of the cost savings of sharing while preserving the isolation guarantees that enterprise customers require. But ground truth separation is only one dimension of the regulatory challenge facing multi-tenant evaluation systems. The next subchapter examines the specific privacy and data protection requirements that GDPR, HIPAA, and the EU AI Act impose on evaluation data — requirements that many platforms discover only when a regulator asks questions they cannot answer.
