# 28.74 — The Customer Success and Eval Team Interface: Where Quality Meets Relationship Management

The customer success manager is on a call with a $400,000-per-year healthcare customer. The customer's VP of Product is frustrated. "Your quality scores say 91 percent. Our clinicians say the summaries are getting worse. Someone is wrong, and I trust my clinicians." The CSM mutes the call, opens the eval dashboard, and stares at a chart showing stable composite scores for the past six weeks. She has no idea how to reconcile what the dashboard says with what the customer is experiencing. She unmutes and says the only thing she can: "Let me bring in our eval team to investigate." The VP sighs. "That is what you said last month."

This scene plays out across every multi-tenant platform that has separated quality measurement from relationship management without building a bridge between them. The eval team knows how to read the numbers. The customer success team knows what the customer is feeling. Neither team has what the other needs, and the customer falls through the gap.

## The Incentive Misalignment

Customer success teams are measured on retention, expansion revenue, net promoter scores, and customer health scores. Eval teams are measured on pipeline uptime, eval coverage, judge accuracy, and platform-wide quality metrics. Neither team is directly measured on per-customer eval accuracy — the degree to which a specific customer's quality scores reflect that customer's actual quality experience.

This misalignment creates predictable behavior. The CS team is motivated to make the customer feel good about quality, even when the numbers are concerning. The eval team is motivated to produce accurate numbers, even when those numbers create uncomfortable conversations. When a customer's quality score drops from 89 percent to 82 percent, the eval team sees a signal that warrants investigation. The CS team sees a data point that could endanger a renewal conversation. Neither instinct is wrong. But without a shared framework for deciding what to do with quality signals, the two teams default to their incentives: the eval team files a technical investigation ticket, and the CS team files a customer communication ticket, and neither team reads the other's ticket.

The fix is not restructuring incentives entirely — CS teams should remain focused on relationships and eval teams on measurement. The fix is creating a shared metric that both teams own. The **Customer Quality Alignment Score** measures the gap between what the eval system reports and what the customer perceives. When the system says 91 percent and the customer says quality is degrading, the alignment score is low regardless of which side is correct. Both teams are accountable for closing that gap — the eval team by investigating whether the rubric is measuring the wrong things, the CS team by gathering specifics from the customer about what "getting worse" actually means in their experience.

## The Information Asymmetry

The CS team has customer context that the eval team needs but does not receive. They know that the customer just hired a new Head of AI who has different quality standards than the previous one. They know that the customer's end users have been complaining about a specific document type that the eval system does not currently sample. They know that the customer is evaluating a competitor and every quality conversation is now a retention moment.

The eval team has measurement context that the CS team needs but does not receive. They know that the customer's quality score is stable at the composite level but that one dimension — faithfulness to source documents — has been declining for three weeks. They know that the customer's golden set has not been updated in five months and may no longer represent the customer's current use cases. They know that a platform-wide model update improved most customers' scores but degraded this customer's scores on medical terminology because of a change in tokenization behavior.

When these two information streams do not merge, both teams make bad decisions. The CS team reassures the customer that quality is fine based on a composite score that hides a per-dimension regression. The eval team investigates a score anomaly without knowing that the customer changed their content strategy three weeks ago, which explains the shift entirely. The customer experiences both failures as incompetence — "they told me quality was fine" and "they investigated a problem that I caused."

## The Handoff Problem

The most visible symptom of the broken interface is the quality report handoff. The eval team produces a per-customer quality report — composite score, per-dimension breakdown, trend analysis, flagged regressions — and delivers it to the CS team for customer communication. The CS team receives a document they partially understand and must present it to a customer who will ask questions about methodology, statistical significance, and dimension-level trends.

Three failure modes emerge from this handoff. First, the **translation failure**: the CSM cannot explain what a score change means in business terms. "Your accuracy score dropped from 88 to 83 percent" is a technical statement. The customer wants to know what that means for their end users, how many outputs were affected, and what is being done about it. A CSM who cannot bridge from the number to the business impact delivers the report as a data dump rather than a quality conversation.

Second, the **filtering failure**: the CSM decides which parts of the quality report to share with the customer and which to suppress, based not on materiality but on comfort. A CSM protecting a renewal conversation may downplay a regression that the customer should know about, which delays the customer's response and makes the eventual discovery worse. A CSM overreacting to a minor fluctuation may alarm the customer about a statistical noise event that would have self-corrected.

Third, the **escalation failure**: the customer asks a question during the quality review that the CSM cannot answer — "why did faithfulness drop this month?" — and the CSM promises to follow up. The follow-up goes to the eval team as a generic question without the context of the conversation. The eval team investigates the question technically without understanding the urgency or the relationship stakes. The answer comes back three days later, which in customer time feels like three weeks.

## The Joint Operating Model

Fixing the interface requires a structured operating model, not just better communication habits.

The **weekly quality sync** is a thirty-minute standing meeting between the eval team and the CS team's account pods. Each pod covers fifteen to twenty customers. The sync reviews three things: customers with quality score changes exceeding the threshold since last week, customers with upcoming business events that affect quality expectations (renewals, expansions, QBR preparation), and open quality investigations and their status. This meeting is not a data review — the data is available in dashboards. It is a context exchange where the eval team shares what the numbers mean and the CS team shares what the customer is experiencing.

The **shared dashboard** presents eval data in two views. The eval-facing view shows per-dimension scores, statistical significance flags, judge agreement rates, golden set coverage metrics, and pipeline health indicators. The CS-facing view shows the same underlying data translated into customer language: quality trend (improving, stable, or declining), risk areas (dimensions where scores are approaching thresholds), action items (what the platform is doing about flagged issues), and talking points (how to discuss quality with the customer this week). Both views draw from the same data. The difference is framing — the eval view optimizes for diagnostic precision, the CS view optimizes for customer communication clarity.

The **escalation protocol** defines three levels. Level 1 is a quality question that the CSM can answer using the dashboard and the most recent quality sync context — no eval team involvement needed. Level 2 is a quality question that requires eval team investigation but is not time-sensitive — the CSM files a structured request with customer context, business stakes, and the specific question, and the eval team responds within 48 hours. Level 3 is a quality incident that affects a tier-1 customer or involves a score drop exceeding 10 percentage points — the eval team is paged immediately and joins a joint call with the CSM and the customer within four hours. The escalation levels are not about severity. They are about how quickly the eval team's expertise needs to reach the customer conversation.

## The Embedded Eval Specialist Model

The most effective organizational pattern at scale is the **embedded eval specialist** — a person with evaluation expertise who works alongside three to four customer success managers, serving as the bridge between measurement and relationship.

The embedded eval specialist is not a full eval engineer. They do not build pipeline infrastructure or develop new judge prompts. They are trained in evaluation methodology, can interpret quality reports at the dimension level, can identify when a score change is meaningful versus statistical noise, and can explain evaluation concepts in customer-accessible language. They attend key customer calls when quality is on the agenda. They prepare the CS-facing view of quality reports. They triage quality questions into the three escalation levels before they reach the eval team. They translate customer complaints into structured investigation requests that the eval team can act on efficiently.

A platform serving 200 enterprise customers might have eight to twelve embedded eval specialists, each supporting three to four CSMs covering fifteen to twenty customers. The embedded specialist participates in the weekly quality sync, attends quarterly business reviews for their customers, and serves as the first line of quality interpretation. This model costs roughly $100,000 to $130,000 per specialist per year, but it eliminates the translation failures, filtering failures, and escalation delays that drive customer quality dissatisfaction.

The embedded model works because it changes the information flow. Instead of eval data flowing from the eval team to a document to the CS team to the customer — with context lost at each handoff — the embedded specialist carries context in both directions. They hear the customer's concern on the call and carry it back to the eval team with full business context. They hear the eval team's analysis and carry it to the customer call with full technical context. The gap between measurement and relationship closes because one person stands in both worlds.

## Metrics That Bridge Both Teams

Three metrics should be jointly owned by the CS team and the eval team, reported monthly, and reviewed by leadership from both organizations.

The **Customer Quality Alignment Score** measures the correlation between eval system scores and customer-reported quality satisfaction. You compute it by comparing each customer's composite eval score trend with their quality satisfaction rating (collected quarterly during business reviews or monthly via a structured survey). A platform with high alignment scores has an eval system that customers trust. A platform with low alignment scores has an eval system that is technically correct but experientially wrong — measuring things the customer does not care about, or missing things they care about deeply.

**Time-to-resolution for quality issues** measures the elapsed time from when a customer raises a quality concern to when the concern is resolved — not just investigated, but resolved in the customer's perception. The clock starts when the CS team logs the concern, passes through the eval team's investigation, and stops when the customer confirms that the issue is addressed. This metric is jointly owned because delays at any point in the chain — the CS team failing to provide context, the eval team taking too long to investigate, the CS team failing to communicate the resolution — extend the clock.

**False positive rate in quality alerts** measures how often the eval system generates alerts that, upon investigation, turn out to be noise rather than genuine quality issues. Every false positive alert consumes CS team time, creates unnecessary customer anxiety if communicated, and erodes trust in the eval system. The eval team owns reducing false positives through better threshold calibration. The CS team owns reporting false positives back to the eval team so the calibration can improve. Neither team can reduce the rate alone.

## What Happens When Both Teams Disagree

The most difficult moment in the CS-eval interface is when the two teams have genuinely different assessments of a customer's quality. The eval team says the numbers show quality is stable. The CS team says the customer is experiencing degradation. Both sides have evidence.

The resolution is not to declare one side right. The resolution is to identify which part of the quality surface each side is observing. The eval system measures the dimensions it was configured to measure, at the sample rate it was configured to sample, against the golden set it was last calibrated against. If the customer's quality concern falls outside those parameters — a new use case the eval does not cover, a dimension the rubric does not include, a shift in the customer's definition of quality since the last calibration — then the eval system is accurate within its scope and the customer is accurate within their experience. The correct response is to expand the eval scope, not to argue about whose numbers are right.

This disagreement is actually the most valuable signal in the system. It reveals gaps in eval coverage. Platforms that treat CS-eval disagreements as evidence of a broken relationship are missing the point. The disagreement is evidence of an eval system that needs updating, and the CS team just told you exactly where.

The interface between CS and eval teams determines whether quality measurement serves the customer or merely serves the dashboard. But quality relationships are not static. Customers expand, downgrade, migrate, and eventually leave. Each of those lifecycle events demands a specific eval system response, and the next subchapter maps every one of them.
