# 28.2 — Mapping the Tenant Configuration Space: Prompts, Models, Tools, and Retrieval

Every tenant on your platform runs a unique combination of settings. The prompt template, the model or adapter, the retrieval pipeline, the tool permissions, the safety filters, the output format — each of these is a dimension, and each customer occupies a specific point in the multi-dimensional space they form. Until you map that space formally, you cannot evaluate it systematically. You are working blind, testing configurations by intuition or accident rather than by design. The **Tenant Configuration Space** is the formal representation of every axis along which a customer's setup can differ from the default, and mapping it is the prerequisite for every configuration-aware evaluation technique that follows in this chapter.

This is not a metadata exercise. It is an architectural decision that determines what you can test, what you can monitor, what you can reproduce, and what you can explain to a customer when something goes wrong. A platform that cannot reconstruct the exact configuration a customer was running at 3:17 PM last Tuesday cannot debug the quality complaint that customer filed the next morning.

## The Seven Configuration Dimensions

Multi-tenant AI platforms vary in what they expose for customization, but seven dimensions appear in nearly every platform that serves enterprise customers. These dimensions are not independent — changes in one frequently affect the behavior of others — but they are distinct enough to require separate tracking.

The first dimension is **prompt templates and system instructions**. This is the most commonly customized dimension and the one with the largest impact on output quality. A customer's system prompt controls the persona, the constraints, the output style, and often the domain-specific instructions the model follows. When a healthcare customer's system prompt says "always include ICD-10 codes in your assessment" and a legal customer's system prompt says "never reference specific statutes unless the user provides them," those two prompts create fundamentally different products from the same underlying model. A platform serving 300 customers might have 400 prompt template variants — some customers run multiple templates for different use cases within their account. Each variant must be tracked as a distinct configuration element because each produces different model behavior that requires different evaluation criteria.

The second dimension is **model and adapter selection**. In 2026, multi-tenant platforms increasingly offer model choice as a configurable parameter. A customer on the self-serve tier might run GPT-5-mini for cost efficiency, while an enterprise customer runs Claude Opus 4.6 for complex reasoning tasks. Beyond base model selection, platforms using LoRA adapters through serving infrastructure like LoRAX or vLLM's multi-LoRA support assign per-tenant adapter weights that specialize the base model for each customer's domain. A tenant running the base Llama 4 Maverick model and a tenant running the same base with a legal-domain LoRA adapter are functionally running different models. Each combination of base model plus adapter version must be tracked as a distinct configuration element.

The third dimension is **retrieval pipeline configuration**. For RAG-based platforms, retrieval settings vary dramatically across tenants. The knowledge base is different — each customer has their own document corpus. The chunking strategy may be different — a customer with dense technical manuals needs different chunk sizes than a customer with conversational FAQ documents. The embedding model may differ. The top-k parameter, the similarity threshold, the reranking model, the metadata filters — each of these is a configuration knob that affects what context the model sees and therefore what output it produces. Two customers running the same model with the same prompt but different retrieval configurations will get different results, and those results require different evaluation baselines.

The fourth dimension is **tool access and permissions**. Agentic platforms expose tools — database queries, API calls, calculations, web searches, file operations — and each customer has a different tool permission set. One customer might enable web search and database lookup but disable file operations. Another might enable all tools but restrict database queries to a specific schema. The tool permission set determines what actions the model can take, which determines both the capability and the risk profile of the customer's experience. Evaluating a customer with full tool access against the same rubric as a customer with restricted tool access produces meaningless scores.

The fifth dimension is **safety filter levels**. Platforms typically offer tiered safety filtering — a strict tier that blocks any potentially sensitive content, a moderate tier that allows factual discussion of sensitive topics, and a permissive tier for domains like healthcare or legal where content that would be inappropriate in a consumer context is professionally necessary. A medical information platform might need to discuss drug interactions and adverse effects that a consumer chatbot would appropriately filter. The safety filter configuration determines both what the model can say and what it refuses to say, which means it directly affects the quality dimensions the customer cares about. A strict filter that blocks a medically necessary response is a quality failure for the healthcare customer, even though the filter is working as designed.

The sixth dimension is **output formatting and structure**. Enterprise customers frequently require specific output formats — JSON structures with particular fields, markdown with specific heading hierarchies, plain text with maximum character counts, structured data conforming to industry schemas like HL7 FHIR for healthcare or XBRL for financial reporting. The output format configuration is not cosmetic. It determines how downstream systems consume the model's output, and a formatting failure can break an integration pipeline even when the content is correct. Two customers with identical content quality but different formatting requirements need different format validation in their eval rubrics.

The seventh dimension is **custom business logic and post-processing rules**. Many platforms allow customers to configure post-processing steps — content filters that redact specific entity types, transformation rules that convert units, validation checks that enforce domain constraints. These rules sit between the model's raw output and what the customer's end user sees. Evaluating the raw model output without applying the customer's post-processing rules evaluates a product the customer never sees. Evaluating the post-processed output requires knowing and applying those rules during evaluation.

## Building the Configuration Registry

The configuration space exists whether you track it or not. Your customers are running their configurations right now. The question is whether you have a structured, queryable representation of those configurations that your eval system can consume, or whether the configurations exist only as scattered settings across databases, feature flags, and config files that no single system can reconstruct.

A **Configuration Registry** is the centralized, versioned store of every tenant's resolved configuration across all seven dimensions. "Resolved" is the key word — it is not enough to store the overrides a customer has applied. You need to store the full resolved configuration, which includes all defaults the customer inherited plus all overrides they applied. When a customer changes nothing and runs on all defaults, their entry in the registry should still contain the complete default configuration, explicitly. This way, when the default changes — and defaults change regularly during model upgrades, prompt improvements, and retrieval tuning — you can tell exactly which customers were affected because you can diff their resolved configuration before and after the change.

The registry stores each configuration as a versioned document. Every time a customer's configuration changes — whether the customer changed a setting, an admin changed a default, or a model upgrade forced a reconfiguration — the registry creates a new version. Old versions are never deleted. They are retained indefinitely, because the ability to reconstruct the exact configuration a customer was running at any point in the past is essential for debugging, audit, and compliance. When a customer says "quality was fine two weeks ago but broke on Thursday," the registry lets you pull the configuration that was active two weeks ago and the configuration that became active on Thursday, diff them, and identify exactly what changed.

The practical implementation depends on your platform's architecture. Some teams build the registry as a dedicated service with its own database, exposing a REST API that the eval system calls to fetch a tenant's current or historical configuration. Others embed the registry in their existing configuration management infrastructure, adding versioning and resolution logic to whatever system already stores tenant settings. The implementation matters less than the contract: the registry must be able to answer three questions at any time. What is tenant X's current resolved configuration? What was tenant X's resolved configuration at time T? What changed in tenant X's configuration between time T1 and time T2?

## Configuration Hashing for Reproducibility

When your eval system runs an evaluation against a specific tenant's configuration, the results are only meaningful if you can prove that the evaluation used exactly the configuration the tenant was running at the time. This is not just a debugging convenience. For regulated industries, it is a compliance requirement — the EU AI Act's transparency obligations and healthcare regulations like HIPAA require demonstrable traceability between an AI system's configuration and its evaluated performance.

**Configuration hashing** provides this traceability. The process is straightforward: take the tenant's full resolved configuration document, sort it deterministically to eliminate ordering artifacts, compute a cryptographic hash of the result, and store that hash alongside every eval result produced under that configuration. The hash becomes the eval result's configuration fingerprint. Two eval results with the same configuration hash were produced under identical configurations. Two eval results with different hashes were produced under different configurations, and you can retrieve both configuration versions from the registry to identify what changed.

The hash also serves as a cache key for eval results. If a tenant's configuration has not changed since the last eval run — same hash — and the eval suite itself has not changed, you can reuse the previous results without re-running the evaluation. This is not just a performance optimization. It is a correctness guarantee: by tying eval validity to configuration state, you ensure that eval results are never stale with respect to the configuration they describe.

In practice, the hash must cover not just the tenant's configuration but also the eval suite version, the judge model version, and any shared platform components that affect evaluation. If the tenant's configuration is unchanged but the judge model was updated, the hash should change because the evaluation environment changed. This composite hash — covering tenant configuration, eval suite version, and evaluation infrastructure state — provides end-to-end reproducibility. Given a composite hash, you can reconstruct the exact conditions under which an eval result was produced.

## Tracking Configuration Changes Over Time

A static configuration registry tells you what exists now. A versioned registry tells you how you got here. The versioning dimension is critical because configuration changes are the single most common cause of per-tenant quality shifts that are invisible to platform-wide monitoring.

When Customer 84's accuracy drops from 91 percent to 83 percent, the first question is always "what changed?" The answer is almost always one of three things: the customer changed their configuration, the platform changed a default that affected the customer's resolved configuration, or an upstream dependency changed in a way that interacted with the customer's configuration. All three answers require the ability to trace configuration changes over time.

The registry should produce a change log for each tenant — a chronological record of every configuration change, who or what initiated it, what dimension changed, and what the old and new values were. Customer-initiated changes are logged with the customer's identity and the interface they used. Platform-initiated changes — default updates, model upgrades, retrieval tuning — are logged with the engineering change request or deployment identifier that triggered them. Automated changes — adaptive systems that adjust configurations based on usage patterns — are logged with the triggering condition and the algorithm version that made the decision.

This change log is the first thing an engineer opens when debugging a per-tenant quality issue. It converts "something changed" into "this specific setting changed at this specific time, initiated by this specific actor." A document intelligence platform that implemented configuration change logging found that 62 percent of per-tenant quality investigations were resolved within 30 minutes by correlating the quality change with a configuration change in the log. Before the log existed, the same investigations averaged four hours because the team had to manually reconstruct what might have changed across multiple systems.

## Configuration Interactions and Combinatorial Complexity

The seven dimensions do not operate independently. A prompt template that works well with one model may produce poor results with another. A retrieval configuration that provides appropriate context for a moderate safety filter may surface content that a strict safety filter blocks, leaving the model with insufficient context. A tool permission set designed for one output format may generate results that break a different format's validation rules. These interactions mean that the configuration space is not just the sum of its dimensions — it is the product.

If you have 400 prompt variants, 7 model and adapter combinations, 12 retrieval configurations, 8 tool permission sets, 4 safety filter levels, 6 output formats, and 10 post-processing rule sets, the theoretical configuration space contains 400 times 7 times 12 times 8 times 4 times 6 times 10 — approximately 64 million unique configurations. You cannot test 64 million configurations. You do not need to, because most of those combinations do not exist in production. But you do need to know which combinations actually exist, which combinations are theoretically possible, and which combinations are known to interact badly.

The configuration registry enables this analysis by making the actual configuration distribution visible. Instead of reasoning about the theoretical space, you can query the registry for the actual configurations in production. A typical 500-customer platform will have between 800 and 2,000 unique resolved configurations — far more than one, far fewer than 64 million. That real distribution is your evaluation target. You are not testing the theoretical space. You are testing the actual space, guided by the registry.

Within that actual space, interaction testing focuses on dimensions that are known to interact. Prompt-model interactions are the most common source of configuration-specific regressions — a prompt optimized for one model may contain instructions or formatting that another model interprets differently. Retrieval-safety interactions are the second most common — the interplay between what context is retrieved and what content is filtered can create blind spots where the model lacks the information it needs. These known interaction patterns become the priority targets for configuration-aware evaluation, as we will explore in detail when we discuss configuration space sampling in subchapter 4.9.

## Connecting Configuration State to Eval Results

The configuration registry is not useful in isolation. Its value is realized when every eval result is linked to the exact configuration under which it was produced. This linkage enables three capabilities that are impossible without it.

The first capability is causal analysis. When a tenant's quality changes, you can query: did their configuration change? If yes, which dimension changed, and does the quality change correlate with the configuration change? If the configuration did not change, the quality change must be caused by something external — a model update, a data distribution shift, an infrastructure change — and the investigation shifts accordingly. Without the linkage, you cannot distinguish "the customer changed something" from "something changed under the customer."

The second capability is configuration-aware trending. Instead of tracking quality over time as a single time series per tenant, you track quality per configuration version per tenant. This lets you answer questions like: "Customer 84's quality was 91 percent on configuration version 7, then they switched to configuration version 8 and quality dropped to 83 percent. Version 7 used the standard retrieval pipeline; version 8 uses a custom pipeline with a different chunk size. The chunk size change is the likely cause." This granularity turns vague quality complaints into specific, actionable engineering investigations.

The third capability is cross-tenant configuration analysis. If 15 customers all use a specific prompt template variant and 3 of them show quality degradation after a model update while the other 12 do not, the configuration registry lets you identify what distinguishes the 3 from the 12. Perhaps the 3 also use a specific retrieval configuration that interacts badly with the new model. This cross-tenant pattern detection is impossible without structured configuration data linked to eval results.

The Tenant Metadata Contract introduced in Chapter 3 — which defines the schema and access controls for per-tenant evaluation metadata — is the governance layer that sits on top of the configuration registry. The registry stores the data. The Metadata Contract defines who can read it, who can modify it, what isolation boundaries apply, and how long each version is retained. Together, they form the foundation for everything else in this chapter: configuration-aware eval execution, layered quality standards, drift detection, and per-tenant dashboards.

The next subchapter introduces the architecture of layered quality standards — the Platform Floor and Tenant Ceiling model that lets you guarantee minimum quality for every customer while allowing individual customers to define stricter requirements above that floor.
