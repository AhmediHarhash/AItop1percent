# 28.2 — Building the Tenant Eval Fingerprint: Dimensions, Thresholds, and Weights

The platform team stares at a spreadsheet with fourteen columns and 320 rows. Each row represents an enterprise customer. Each column represents a quality attribute — accuracy, tone, compliance, formatting, latency tolerance, safety sensitivity, domain vocabulary adherence, and seven more. Some cells have precise numbers. Some have notes like "very important" or "same as default." Some are blank. Two customer success managers contributed rows based on memory from onboarding calls. The evaluation engineer who built the spreadsheet three months ago left the company last week. Nobody is confident that any row fully represents what its customer actually needs.

This spreadsheet is an attempt at something essential — a per-customer quality profile that captures how evaluation should work for each tenant. But a spreadsheet is the wrong artifact. It cannot be versioned with the same rigor as code. It cannot be validated programmatically. It cannot be loaded directly into an evaluation pipeline. It cannot enforce consistency across hundreds of entries maintained by dozens of people. What the platform needs is not a spreadsheet but a formal data structure — a **Tenant Eval Fingerprint** — that encodes every quality-relevant parameter for each customer in a machine-readable, versionable, auditable format that the eval pipeline consumes directly.

## What the Tenant Eval Fingerprint Contains

The Tenant Eval Fingerprint is the machine-readable representation of the Quality Contract described in the previous subchapter. Where the Quality Contract is a negotiated agreement between your team and the customer, the fingerprint is the technical artifact that your eval pipeline reads at runtime to determine how to score, threshold, alert, and report for that specific tenant. Think of the Quality Contract as the architectural blueprint and the fingerprint as the compiled configuration that the system actually executes.

A complete fingerprint contains six categories of information. Omitting any one of them creates blind spots that eventually surface as scoring errors, missed regressions, or customer disputes.

The first category is **quality dimensions and weights**. This is the core of the fingerprint — the list of dimensions against which the tenant's outputs will be scored, and the relative weight each dimension carries in the composite score. A healthcare tenant's fingerprint might specify clinical accuracy at 35 percent, regulatory compliance at 25 percent, citation fidelity at 20 percent, readability at 10 percent, and formatting at 10 percent. A media tenant's fingerprint for the same platform might specify brand voice at 30 percent, factual accuracy at 25 percent, engagement quality at 20 percent, SEO compliance at 15 percent, and formatting at 10 percent. The dimensions are drawn from the platform's dimension library — a shared catalog of all quality dimensions the platform knows how to evaluate — but each tenant's fingerprint selects which dimensions apply and how much each one matters.

The second category is **threshold definitions**. For each dimension and for the composite score, the fingerprint specifies the minimum acceptable score, the target score, and optionally, a critical floor below which a single output is flagged regardless of composite performance. A financial services tenant might set a composite target of 90 percent, a composite floor of 82 percent, and a per-dimension critical floor of 70 percent on numerical precision. An output that scores 95 percent composite but 65 percent on numerical precision still triggers an alert because it fell below the critical floor on a dimension the customer considers non-negotiable.

The third category is **safety and compliance requirements**. Not every tenant has the same sensitivity to safety dimensions. A children's education platform requires zero tolerance for age-inappropriate content and has an extremely narrow band for acceptable tone. A B2B industrial supply chain platform might have minimal content safety requirements but strict requirements around data confidentiality — the model must never reference one supplier's pricing when generating content for a competing supplier. The fingerprint encodes which safety classifiers apply to the tenant's outputs, what thresholds trigger safety failures, and whether safety failures override composite quality scores. For most enterprise tenants, a safety failure at any severity constitutes a quality failure regardless of how well the output scores on other dimensions.

The fourth category is **data distribution parameters**. Each tenant's evaluation sample must reflect their actual usage patterns. A tenant that sends 70 percent of their queries in Japanese and 30 percent in English needs an eval sample that mirrors that ratio, not a sample drawn from the platform's overall distribution where Japanese queries might represent 4 percent. The fingerprint specifies the expected language distribution, document type distribution, query complexity distribution, and any other input characteristics that should be reflected in the evaluation sample. When the actual input distribution drifts from the fingerprint's specified distribution, the eval system flags the drift — because a distribution shift often precedes a quality shift.

The fifth category is **domain vocabulary and entity specifications**. Tenants in specialized domains use terminology that the platform's default evaluation may not handle correctly. A legal tenant's fingerprint specifies that terms like "force majeure," "indemnification," and "liquidated damages" must be preserved exactly as provided, never paraphrased or simplified. A medical device tenant specifies that product names, model numbers, and regulatory classification codes must match their reference catalog precisely. These vocabulary constraints become part of the scoring criteria — a dimension like "terminology accuracy" that checks whether domain-specific terms were handled according to the tenant's specifications.

The sixth category is **reporting and alerting configuration**. Different tenants have different tolerances for score fluctuation and different expectations for how quickly they are notified of quality changes. A tier-one enterprise customer paying over $500,000 annually might require alerting within one hour of a quality threshold breach, with automatic escalation to the customer's technical contact if the breach is not acknowledged within four hours. A smaller tenant might accept daily quality digests. The fingerprint encodes the alerting thresholds, escalation paths, notification channels, and reporting frequency for each tenant.

## The Fingerprint Is Not the Rubric

A common confusion: teams sometimes treat the fingerprint and the rubric as the same thing. They are not. The rubric defines how to score a single output — what criteria to apply, what scale to use, what constitutes a 4 versus a 3 on a given dimension. The fingerprint defines the evaluation context for an entire tenant — which rubric configuration to use, what thresholds to apply to the scores, how to sample the outputs, what alerts to fire, and what reports to generate.

The rubric is a component within the fingerprint, not a synonym for it. A platform might have one parameterized rubric template that serves all tenants, instantiated with different parameters for each tenant based on their fingerprint's dimension and weight specifications. Or it might have three rubric families — one for extraction tasks, one for generation tasks, one for classification tasks — with each tenant's fingerprint specifying which family applies and what parameters to use. The fingerprint selects and configures the rubric. It also configures everything else about how evaluation works for that tenant.

This distinction matters operationally because changes to the fingerprint and changes to the rubric have different blast radii. Updating a rubric's scoring logic affects every tenant that uses that rubric family. Updating a tenant's fingerprint affects only that tenant. When a customer success manager adjusts a tenant's dimension weights after a quarterly review, they are modifying the fingerprint, not the rubric. When an evaluation engineer improves how the platform measures citation fidelity, they are modifying the rubric, and that modification needs per-tenant impact assessment because it affects every tenant whose fingerprint includes the citation fidelity dimension.

## Building Fingerprints During Customer Onboarding

The fingerprint construction process begins during onboarding, immediately after the Quality Contract negotiation described in the previous subchapter. In practice, the evaluation team translates the Quality Contract's human-readable agreements into the fingerprint's machine-readable format. This translation is not mechanical — it requires judgment calls that have downstream consequences.

The most consequential judgment call is dimension granularity. The Quality Contract might specify that the customer cares about "accuracy." But accuracy can be decomposed into factual accuracy, numerical accuracy, entity accuracy, temporal accuracy, and logical consistency. The evaluation team must decide whether to represent the customer's accuracy requirement as a single dimension or as multiple sub-dimensions with individual weights. Too coarse and you lose the ability to pinpoint which kind of accuracy failure is driving quality drops. Too granular and you create a fingerprint with 25 dimensions that is difficult to calibrate and produces composite scores that are hard to interpret.

The guideline that works for most platforms is between five and ten weighted dimensions per tenant. Fewer than five means you are almost certainly compressing distinct quality requirements into single dimensions that cannot be independently measured. More than twelve means you are likely splitting dimensions that the customer mentally groups together, creating calibration complexity without corresponding diagnostic value. A healthcare tenant with eight dimensions — clinical accuracy, regulatory compliance, citation fidelity, terminology precision, patient safety, readability, formatting, and completeness — is well within the range. A tenant with twenty dimensions probably has three or four that could be consolidated without losing evaluative power.

After dimension selection, the team calibrates thresholds. This is where the Quality Contract's example-based calibration outputs get converted into numerical boundaries. If the calibration exercise showed that the customer considers outputs scoring below 72 percent on clinical accuracy to be "failed," the fingerprint's critical floor for that dimension is set at 72. If outputs scoring between 72 and 84 were considered "acceptable but not ideal," and outputs above 84 were "good," the fingerprint captures a three-tier threshold: critical floor at 72, target at 84, and a stretch target if applicable.

The final step is validation. The team runs the newly constructed fingerprint against a sample of the tenant's historical outputs — either provided by the customer during onboarding or generated during a pilot period. The goal is to verify that the fingerprint's scores align with the customer's quality perception. If the fingerprint scores an output at 91 percent that the customer considers mediocre, the weights or thresholds need adjustment. Two to three validation rounds are typical before the fingerprint enters production.

## Storing and Versioning Fingerprints

The fingerprint must be treated as a first-class versioned artifact. This means storage in a system that supports immutable versioning, audit trails, and rollback capabilities. Git-based configuration repositories work well for teams that treat eval configuration as code. Dedicated configuration management systems — the same kind used for feature flags or infrastructure configuration — also work, provided they support per-tenant versioning.

Every fingerprint has a version number, a creation timestamp, an author, and a change description. Version 1.0 is the onboarding fingerprint. Version 1.1 might adjust the clinical accuracy weight from 35 to 40 percent after the first quarterly review. Version 2.0 might add a new dimension — patient data redaction — in response to a regulatory requirement that did not exist when the customer onboarded. Historical evaluation results always reference the fingerprint version that was active when the evaluation ran. This makes trend analysis reliable — if a customer's quality score dropped from 88 to 82 over three months, you need to know whether that drop reflects actual quality degradation or a fingerprint change that made the evaluation stricter.

The versioning system must also enforce approval workflows. A fingerprint change that adjusts threshold floors affects whether SLA commitments are met. Lowering a critical floor from 75 to 70 is not a minor tweak — it means outputs that previously triggered SLA breaches will now be considered acceptable. Raising a floor from 75 to 80 is also consequential — it means the platform will report more failures for that customer, potentially triggering financial penalties that did not exist under the previous version. Both directions require review and approval from customer success, evaluation engineering, and ideally the customer's own stakeholders.

## When Fingerprints Drift From Reality

A fingerprint is accurate on the day it is created and becomes less accurate every day after that. Customer needs evolve. Business priorities shift. New product lines launch. Regulatory environments change. The fingerprint that perfectly captured a tenant's quality requirements twelve months ago may be meaningfully out of date today.

Fingerprint drift manifests in two ways. The first is dimension drift — the customer starts caring about a quality dimension that is not in their fingerprint. A retail customer that previously cared only about product description accuracy might launch a customer service chatbot that requires empathy and de-escalation quality, dimensions that their fingerprint does not include. The eval pipeline continues scoring the new use case against the old dimensions, producing scores that look healthy while the customer's experience deteriorates in a dimension that is not being measured.

The second is threshold drift — the customer's standards change but the fingerprint's thresholds do not. A fintech customer that initially accepted 80 percent accuracy on transaction categorization might raise their internal standard to 90 percent after building downstream automation that depends on categorization accuracy. The fingerprint still scores against the 80 percent floor, so the eval system reports the customer as meeting quality targets while the customer considers the same outputs inadequate.

Detection requires both automated signals and human check-ins. On the automated side, the platform should monitor the gap between the customer's implicit quality signals — support tickets, output rejection rates, feedback scores — and the fingerprint's reported quality. If the fingerprint says quality is 89 percent but the customer's output rejection rate is climbing, the fingerprint may be measuring the wrong things. On the human side, quarterly fingerprint reviews with the customer's stakeholders catch drift that automated signals miss. The review asks three questions. Are the dimensions still the right dimensions? Are the weights still the right weights? Are the thresholds still the right thresholds? These reviews take one to two hours per customer, which is significant at 500 customers but manageable when spread across customer success teams with dedicated evaluation review slots.

## The Fingerprint as Competitive Intelligence

Here is an insight that most platform teams miss. The collection of fingerprints across your entire customer base is one of the most valuable strategic assets your company owns. It encodes, in machine-readable form, what "quality" means across hundreds of enterprise use cases in your domain.

That aggregate knowledge enables capabilities that no individual fingerprint can provide. You can identify which quality dimensions matter most across your customer base, guiding where to invest in evaluation improvements. You can spot emerging trends — if seven customers added a data privacy dimension to their fingerprints in the last quarter, that is a market signal. You can benchmark new customers against similar existing customers, offering onboarding guidance like "customers in your industry typically weight regulatory compliance at 20 to 30 percent — here is why." You can identify quality dimensions where your platform consistently underperforms, directing engineering investment to the areas that affect the most customers.

This strategic use of fingerprint data requires treating fingerprints as proprietary. The individual fingerprint configurations are customer-specific and confidential. But the aggregate patterns — anonymized, of course — are strategic intelligence about what the market considers quality. Platforms that extract and act on this intelligence compound their advantage over time, because each new customer's fingerprint enriches the platform's understanding of quality in their domain.

The Tenant Eval Fingerprint defines what to measure and how to threshold it. But the question of how to actually score outputs against those specifications — how to build rubrics that can serve 500 different fingerprint configurations from a single codebase — is an entirely different engineering challenge. The next subchapter tackles the architecture of parameterized rubrics that scale without requiring per-customer code.