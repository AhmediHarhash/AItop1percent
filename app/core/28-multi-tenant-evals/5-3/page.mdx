# 28.3 — Building the First Golden Set: Bootstrapping Ground Truth for a New Customer

A legal technology company spent six weeks onboarding onto a multi-tenant AI platform in early 2025. Discovery went well — they identified nine quality dimensions, prioritized them clearly, and the customer's General Counsel attended every workshop. The fingerprint was calibrated in two rounds. The team was ahead of schedule. Then the evaluation engineer asked for the customer's labeled data to build the golden set, and the customer said they had none. No labeled examples. No reference outputs. No quality-validated dataset of any kind. They had used their previous system for three years without ever formally evaluating a single output. The eval team, under timeline pressure, built a 40-example golden set by running the customer's most common queries through the platform, having one internal annotator review the outputs in an afternoon, and declaring the golden set ready. The first quality report showed 93 percent composite quality. The customer's General Counsel looked at the report, looked at the outputs, and said: "If you think these are 93 percent quality, your system does not understand our work." The golden set was too small, too easy, and built by someone who did not understand legal drafting. Every score it produced was inflated. The platform spent the next four months rebuilding the golden set from scratch while the customer questioned every quality number they received.

This is the bootstrapping problem. New customers arrive with no evaluation infrastructure. They have no labeled data, no ground truth, and no reference standard against which to measure their outputs. But your evaluation pipeline needs ground truth to function — without it, scores have no anchor. The first golden set must be assembled from whatever raw material the customer can provide, supplemented by whatever your team can generate, and validated by the customer's own domain experts. The quality of this initial golden set determines the accuracy of every eval score for the next six to twelve months, because the golden set is the reference standard that all scoring is measured against.

## Where the Raw Material Comes From

The raw material for a first golden set comes from four sources, and most customers can provide at least two of them.

The most valuable source is **customer-provided examples**. These are inputs and outputs that the customer brings from their existing workflows, whether from a previous AI system, manual processes, or historical records. A healthcare company might provide 300 physician note summaries that their medical directors have reviewed and approved. A financial services firm might provide 200 client communication drafts that their compliance team has cleared. These examples are gold because the customer has already validated them — not formally as golden set entries, but practically as outputs that met their quality bar in production. The evaluation team's job is to formalize that implicit validation: confirm with the customer's domain experts that each example still represents their current quality standard, document the quality dimensions each example demonstrates, and organize the set by use case coverage.

Not every customer has this luxury. Startups onboarding their first AI product have no historical outputs. Companies switching from a manual workflow to an AI-assisted one have examples of human-generated outputs but no AI outputs to evaluate against. Companies who used a previous AI vendor may have outputs but no confidence in their quality. For these customers, the second source is essential.

The second source is **production traffic sampling during a pilot period**. During the first two to four weeks of the customer's pilot, the platform generates outputs for the customer's actual inputs. A curated sample of these outputs — stratified across the customer's input types, complexity levels, and use cases — is presented to the customer's domain experts for review. Each output gets one of three labels: correct as-is, correct with modifications (and the domain expert provides the corrected version), or incorrect. Correct outputs enter the golden set directly. Modified outputs enter as the input paired with the corrected version. Incorrect outputs do not enter the golden set but are documented separately as negative examples that inform rubric calibration.

The third source is **synthetic generation**. When the customer has very little historical data and the pilot period has not yet produced enough volume, the evaluation team can generate synthetic inputs that represent the customer's use cases and run them through the platform to produce candidate outputs. The key constraint is that synthetic inputs must be realistic — they must reflect the actual distribution, complexity, and edge cases of the customer's real traffic. A synthetic golden set built from artificially simple inputs will produce artificially high scores. The domain specialist on the onboarding team plays a critical role here, designing synthetic inputs that exercise the quality dimensions the customer cares about, including the hard cases that real traffic will eventually include.

The fourth source is **transfer from similar tenants**. If the platform already serves customers in the same industry with similar use cases, and those customers consent, anonymized golden set examples from the existing tenant can bootstrap the new tenant's golden set. A new insurance customer benefits from seeing how the platform evaluated similar insurance documents for an existing customer — not the proprietary data, but the input structures, the quality dimensions, and the types of examples that proved most useful for regression detection. Transfer bootstrapping is faster than building from scratch but carries a risk: the new customer's quality standards may differ from the existing customer's in ways that the transferred examples do not capture. Transferred examples should always be supplemented with customer-specific examples and should never constitute more than 30 to 40 percent of the final golden set.

## Minimum Viable Golden Set Size

How many examples does the first golden set need? The answer depends on the customer's use case complexity, but there are practical minimums below which the golden set cannot reliably distinguish between genuine quality differences and noise.

For a customer with a narrow, well-defined use case — extracting specific fields from a standardized document type, classifying inputs into a fixed set of categories — 50 to 100 examples provide a serviceable initial golden set. These examples must cover the full range of input types and include at least ten to fifteen edge cases. A golden set of 80 examples that spans all five document subtypes and includes ten boundary cases catches most regressions that affect the customer's core workflow.

For a customer with a moderate use case — generating multi-paragraph content across three to five domains, answering questions that require synthesis from multiple sources — 150 to 250 examples are needed. The breadth of the use case means more variation in what "correct" looks like, and each variation needs enough representation to produce stable per-dimension scores. A golden set of 200 examples that covers four content domains with 40 to 60 examples each provides enough signal to detect domain-specific regressions.

For a customer with a broad, complex use case — multi-turn conversational agents, open-ended advisory outputs, creative content across many formats — 300 to 500 examples are needed for initial calibration. The open-endedness of the use case means that the space of possible correct outputs is large, and the golden set must sample that space broadly enough to distinguish between outputs that are correct-but-different and outputs that are actually wrong. Below 300 examples for a complex use case, you will see high score variance between evaluation runs — the same model producing composite scores that fluctuate by five to eight percentage points depending on which subset of the golden set happens to align with the model's current behavior.

These are starting sizes. The golden set should grow steadily during the first six months, targeting roughly double the initial size by the six-month mark. Growth comes from adding examples that cover new input patterns observed in production, edge cases discovered through customer feedback, and deliberately constructed adversarial examples that probe the failure modes most relevant to the customer's domain.

## The Annotation Process for the First Golden Set

The first golden set is only as trustworthy as the people who validated it. This is where most bootstrapping efforts fail — not in the collection of examples, but in who reviews them and how.

The customer's domain experts must participate in the annotation process. Not your annotators. Not your evaluation team. The customer's people — the practitioners who understand the domain well enough to judge whether an output is correct in context, not just fluent or plausible. A contract extraction output that looks correct to a general annotator might contain a clause interpretation error that only a practicing attorney would catch. A medical summary that reads well to a non-clinician might use a drug classification that was updated in a recent guideline revision. Domain expertise is not optional for golden set annotation. It is the difference between a golden set that reflects actual quality and a golden set that reflects surface-level plausibility.

The practical challenge is getting enough of the customer's domain experts' time. These are expensive people with demanding schedules — physicians, attorneys, compliance officers, senior engineers. They cannot spend 40 hours annotating 300 examples. The solution is structured annotation that maximizes their impact per hour. Present examples in batches of 20 to 30, pre-sorted by your evaluation team into likely-correct, likely-needs-modification, and likely-incorrect categories. The domain expert spends most of their time on the ambiguous cases, not re-confirming obviously correct outputs. Provide clear annotation guidelines — what to evaluate, how to mark modifications, what constitutes rejection — so the domain expert's time is spent on judgment, not on figuring out the annotation interface.

For each example, the domain expert provides three things: a correctness label (correct, needs modification, or incorrect), modifications if applicable (the corrected output), and a brief rationale for the label. The rationale matters more than it seems. A golden set entry that says "correct" tells the evaluation pipeline what the right answer is. A golden set entry that says "correct because the dosage conversion uses the 2025 FDA guideline rather than the deprecated 2019 table" tells the evaluation team what the domain expert actually checked, which informs rubric dimension calibration and helps future annotators understand the standard.

## Quality Requirements for the Initial Set

A golden set is not a random collection of labeled examples. It is a designed artifact, and the design must satisfy specific quality requirements if the set is to serve its purpose as a reliable evaluation anchor.

The first requirement is **use case coverage**. The golden set must include examples from every major category of inputs the customer sends to the platform. If the customer uses the platform for three distinct tasks — document summarization, entity extraction, and question answering — the golden set must include examples of all three, with representation proportional to their business importance rather than their volume. A customer who sends 80 percent summarization queries and 20 percent entity extraction queries but considers extraction errors three times more costly than summarization errors should have a golden set that overweights extraction relative to volume.

The second requirement is **difficulty distribution**. The golden set must include easy cases, typical cases, and hard cases. Easy cases — straightforward inputs with obvious correct outputs — establish the score floor. If the model cannot get easy cases right, something fundamental is broken. Typical cases — representative of the customer's daily traffic — determine the steady-state quality score. Hard cases — edge cases, ambiguous inputs, adversarial examples — test the model's robustness and are where regressions typically appear first. A golden set composed entirely of easy and typical cases will produce high, stable scores that mask degradation on the hard cases the customer cares about most. The recommended distribution is roughly 20 percent easy, 50 percent typical, and 30 percent hard, though this varies by domain.

The third requirement is **dimensional balance**. If the customer's fingerprint includes seven quality dimensions, the golden set must include examples that exercise each dimension. Some examples will test multiple dimensions simultaneously — a medical summary might test both clinical accuracy and regulatory language compliance. But each dimension should have examples where it is the primary axis of variation — examples where the quality difference between correct and incorrect is driven specifically by that dimension. Without dimensional balance, some dimensions will have rich ground truth data and produce reliable scores, while others will have sparse ground truth and produce scores that fluctuate based on which few examples happen to be included.

The fourth requirement is **annotator agreement documentation**. If multiple domain experts participate in annotation — which is ideal — the golden set should document inter-annotator agreement rates. Where two experts disagree on whether an output is correct, that disagreement is itself valuable data. It marks the boundary where the customer's own quality standard is ambiguous. These boundary cases should remain in the golden set, labeled with the disagreement, so that future calibration can account for the inherent ambiguity rather than pretending it does not exist.

## Version Control From Day One

The golden set will evolve. This is not a problem to prevent — it is a feature to manage. But managing golden set evolution requires version control from the very first version, not bolted on later after the first crisis.

Version 1.0 is the onboarding golden set. It is imperfect. It has coverage gaps. Some examples may turn out to be mislabeled. Some dimension representations may be too thin. That is expected. The point of version 1.0 is not perfection but adequacy — a good-enough foundation from which the golden set improves over time.

Every change to the golden set after version 1.0 creates a new version. Adding 15 examples to cover a document type that was missing is version 1.1. Correcting five mislabeled examples identified during the first quarterly review is version 1.2. A major overhaul that adds 80 new examples and retires 20 outdated ones after the customer's domain expanded is version 2.0. The version number matters because every evaluation result is linked to the golden set version it was computed against. Without that linkage, score trends are uninterpretable. A quality improvement from 82 to 87 percent means something real only if both scores were computed against the same golden set version. If the golden set changed between measurements — especially if it got easier — the improvement might be an artifact of the evaluation infrastructure, not a genuine gain in model performance.

The versioning system must also track provenance metadata for every example: when it was added, who annotated it, which version of the annotation guidelines was used, and what the source input was. This metadata is not bureaucratic overhead — it is the audit trail that makes the golden set defensible. When a customer challenges a quality score, the ability to trace that score back to specific golden set examples, each with documented provenance, transforms the conversation from "your numbers feel wrong" to "let us look at the specific examples where the score differs from your expectation."

## The Bootstrap Trap

There is a specific failure mode that afflicts first golden sets so frequently it deserves a name: **the Bootstrap Trap**. It occurs when the initial golden set is too small, too easy, or both, producing artificially high scores that set unrealistic expectations the platform can never maintain.

The mechanism is straightforward. A golden set of 40 examples that covers only the customer's most common, most straightforward input type will produce high scores because the model handles common, straightforward inputs well. The first quality report shows 92 percent composite quality. The customer is delighted. Three months later, as the golden set expands to include harder cases, edge cases, and new input types, the composite score drops to 79 percent. The model has not gotten worse. The measurement has gotten more accurate. But the customer perceives a 13-point regression, and no amount of explanation about golden set maturation will fully undo that perception. The first number the customer saw became their reference point, and every subsequent number is judged against it.

The Bootstrap Trap is particularly dangerous because it rewards the exact behavior that causes it. A team under time pressure builds a small, easy golden set. The first report looks great. The customer is happy. The sales team celebrates. Everyone moves on. Six months later, the quality conversation has deteriorated into a recurring dispute about whether quality has declined or measurement has improved, and neither side fully trusts the other's interpretation.

The prevention is disciplined minimum standards for the initial golden set: enough examples to cover the use case surface area, deliberate inclusion of hard cases and edge cases, and customer domain expert validation that the examples represent the full difficulty spectrum of their actual work. It also requires honest framing of the first quality report. The report should state explicitly that the golden set is version 1.0, that it will grow over the next six months, and that scores may adjust as the golden set's coverage becomes more representative. Setting this expectation upfront does not feel as good as delivering a 92 percent score with confidence. But it protects the relationship from the far more damaging experience of perceived regression.

## When the Customer Has Almost Nothing

Some customers arrive with genuinely nothing — no historical outputs, no labeled data, no reference documents, and a domain so specialized that your platform has no similar tenants to transfer from. These are the hardest onboardings, and they require a different bootstrapping strategy.

The approach is to compress the pilot period and the golden set construction into a single phase. The customer provides a set of representative inputs — at minimum 50, ideally 100 or more. The platform generates outputs for each input. The customer's domain expert reviews the outputs in a structured annotation session, typically taking four to eight hours spread over two or three days. Every output gets a label and, for modified or incorrect outputs, a corrected version. The golden set is constructed entirely from this pilot annotation.

The risk with this approach is that the golden set is built entirely from a single model version's outputs. If the model changes — a version update, a fine-tuning adjustment, a routing change — the golden set may not probe the new model's specific weaknesses. To mitigate this, the evaluation team should include at least 20 percent synthetic adversarial examples: inputs designed to probe known failure modes of the model family the platform uses. If the platform runs on Claude Opus 4.6, the evaluation team should include examples that test known challenges for that model family — multi-step reasoning chains, domain-specific terminology in low-resource languages, outputs requiring precise numerical computation. These adversarial examples ensure the golden set has diagnostic power even if the model changes.

The compressed bootstrapping approach produces a functional golden set in two to three weeks rather than the standard three to four. But it comes with lower initial coverage and should be explicitly scheduled for expansion during the first quarterly review. The customer should understand that the initial golden set is a starter set, not a comprehensive one, and that the first six months include planned golden set growth.

## Connecting the Golden Set to the Fingerprint

The golden set and the Tenant Eval Fingerprint are tightly coupled artifacts that must be developed together, not sequentially. The golden set provides the concrete examples against which the fingerprint's dimensions, weights, and thresholds are calibrated. The fingerprint specifies which quality dimensions the golden set must cover.

When the golden set is missing coverage for a fingerprint dimension, that dimension's scores are unreliable. If the fingerprint includes "regulatory language compliance" as a dimension weighted at 20 percent, but the golden set contains only three examples where regulatory language is the primary quality differentiator, the regulatory language scores will have high variance and low diagnostic value. The evaluation team should validate, before the golden set enters production, that every fingerprint dimension has at least ten to fifteen dedicated examples — examples where that dimension is the primary axis of quality variation.

This coupling also means that golden set gaps are fingerprint gaps. When the quarterly review reveals that the golden set underrepresents a particular input type, the question is not just "should we add more examples of this type?" It is also "does the fingerprint need a new dimension to evaluate this type, or do the existing dimensions already cover it?" Golden set expansion and fingerprint evolution should be reviewed together, because they are two representations of the same underlying question: what does quality mean for this customer right now?

The golden set provides the ground truth. But ground truth alone does not produce scores — it needs rubrics that define how to evaluate outputs against that ground truth, calibrated to produce scores the customer trusts. The next subchapter covers rubric calibration: the alignment workshop where your scoring system and the customer's judgment learn to agree.
