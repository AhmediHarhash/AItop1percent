# 28.5 — Per-Tenant Dashboards: What Each Customer Needs to See and What They Must Never See

The customer success manager shares her screen. She is walking a healthcare customer's VP of Engineering through their quality dashboard for the first time. The dashboard shows accuracy scores, trend lines over the past 90 days, SLA compliance indicators, and a breakdown by use case. The VP nods along, asks a few clarifying questions, and then points to a chart in the lower right corner. "What's this metric — platform-wide p50 latency? Does that include all your customers?" The CSM pauses. It does. The chart was added by an engineer who thought it provided helpful context. But the VP is already doing arithmetic in his head, comparing his tenant's throughput against the platform aggregate, estimating how many other customers share the infrastructure, and wondering whether the platform is oversubscribed. In thirty seconds, a well-intentioned dashboard widget turned a trust-building exercise into competitive intelligence extraction.

This is the **dashboard exposure problem**, and every multi-tenant platform encounters it. The customer-facing quality dashboard serves two competing objectives. It must show enough detail to build trust, demonstrate quality, and prove SLA compliance. But it must never expose internal platform metrics, other tenants' data, proprietary evaluation methodology, or aggregate statistics that let a sophisticated customer reverse-engineer competitive information. The dashboard is your customer's window into your quality system. Make the window too opaque and they stop trusting the glass. Make it too transparent and you hand them information that damages your business, your other customers, or both.

## What Customers Need to See

Start from the customer's perspective. When they log into a quality dashboard, they have three questions. Is the system working for me? Is it meeting my contractual commitments? Is quality improving, degrading, or stable?

Answering these three questions requires a specific set of metrics. First, the customer needs their **quality scores by dimension** — the individual scores for each quality dimension in their evaluation rubric. If their rubric includes factual accuracy, tone appropriateness, and format compliance, they need to see each score independently, not just a blended composite. The composite tells them the overall number. The per-dimension breakdown tells them where to focus if they want improvement. A customer whose composite score is 87 percent needs to know whether that comes from 95 percent accuracy dragged down by 72 percent tone compliance, or from 85 percent across the board. The diagnosis is completely different, and the customer cannot make informed decisions without the dimensional view.

Second, the customer needs **trend lines over time** — not just the latest score, but the trajectory. A score of 88 percent that was 92 percent last month tells a different story than a score of 88 percent that was 83 percent last month. Trend lines should span at least 90 days and ideally show weekly or biweekly data points. Daily data points create visual noise that obscures meaningful trends. Monthly data points update too infrequently for customers to react. Weekly cadence gives customers enough resolution to spot degradation early without drowning them in variance.

Third, the customer needs **SLA compliance status** — a clear, unambiguous indicator of whether their current quality meets the thresholds defined in their contract. This is not a nice-to-have. For regulated customers, SLA compliance status feeds directly into their own compliance reporting. A healthcare customer who committed to their regulator that their AI system maintains 90 percent accuracy needs a dashboard that tells them, in plain language, whether they are above or below that line. Green, yellow, red. No ambiguity. The yellow zone should activate when they are within 5 percentage points of the SLA threshold, giving them early warning before a breach occurs.

Fourth, the customer needs **incident history** — a record of quality events that affected their tenant. If their accuracy dropped below the SLA threshold for two hours on March 14th because of a model update, they need to see that event, its duration, its root cause summary, and the resolution. Incident history builds trust not by hiding problems but by demonstrating that you detect, resolve, and document them. The customer who sees a clean incident log does not think "nothing ever goes wrong." They think "they are not telling me when things go wrong." A log with three resolved incidents tells them the system works because it shows the detection and recovery machinery in action.

Fifth, the customer needs **comparison to their own historical baseline** — how current quality relates to their own past performance. This is the comparison that is safe and valuable. Showing a customer that their accuracy this quarter is 4 points higher than last quarter validates the investment they are making in the platform. The comparison must always be against the customer's own history, never against other tenants, never against a platform average, never against an anonymized cohort.

## What Customers Must Never See

The list of prohibited data is longer than most teams expect, because the exposure risks are subtle.

**Other tenants' data in any form.** This is the obvious prohibition, but it extends beyond raw data. Even anonymized references to other tenants — "Customer A achieves 94 percent accuracy on similar workloads" — reveal competitive information. The customer knows their industry. They know who else is likely on the platform. Anonymization is not protection when the customer can narrow the possibilities to two or three competitors.

**Platform-wide aggregates.** A chart showing "average platform accuracy: 91 percent" tells the customer how they rank relative to the population. A customer at 88 percent now knows they are below average. A customer at 95 percent now knows they are in the top tier. Both inferences leak information about the distribution of quality across your customer base. Even seemingly harmless aggregates like "total evaluations processed this month: 14.2 million" reveal platform scale and utilization patterns that your customers — who may also be your competitors' customers — can use for competitive intelligence.

**Raw LLM judge scores and rationales.** Your evaluation pipeline may use Claude Opus 4.6 or GPT-5 as judge models that produce detailed scoring rationales. Those raw rationales often contain information about the scoring methodology, the rubric prompts, and the judge's reasoning process. This is proprietary intellectual property. The customer should see the resulting score and a human-readable summary of what drove the score. They should not see the raw judge output that reveals how you prompt the judge, what criteria the judge applies, or what model you use for judging. A sophisticated customer who sees your judge prompt can replicate your evaluation methodology and potentially build their own eval system, reducing their dependency on your platform.

**Internal model identifiers and version strings.** Your internal system might route a customer's traffic through "gpt-5-0126-ft-medical-v3" or "llama4-maverick-lora-tenant247-20260115." These identifiers reveal your model selection strategy, your fine-tuning approach, your version cadence, and potentially which base models you use. The customer should see a human-readable model label — "Medical Documentation Model v3" — not the internal identifier that exposes your infrastructure decisions.

**Cost attribution details.** The per-evaluation cost of scoring a customer's outputs is internal financial data. Revealing that each evaluation costs $0.12 in LLM judge tokens gives the customer leverage in pricing negotiations and reveals your margin structure. If they know your eval costs, they can estimate your total cost to serve, which informs their assessment of how much room you have to discount. Cost data belongs in your internal dashboards, not your customer-facing ones.

**Evaluation pipeline performance metrics.** Queue depths, processing latency, worker utilization, API rate limit consumption — these are operational metrics that reveal infrastructure capacity and contention patterns. A customer who sees that the evaluation queue depth spiked to 12,000 during their eval run infers that the platform is resource-constrained. That inference, accurate or not, erodes confidence.

## Dashboard Architecture: The Tenant-Scoped Data Access Layer

The technical architecture that prevents data leakage is not a filter applied at the presentation layer. It is a data access pattern enforced at the query layer, before any data reaches the dashboard rendering engine.

The **tenant-scoped data access layer** sits between the dashboard application and the metrics store. Every query from the dashboard passes through this layer, which injects the tenant identifier into every data access operation. The dashboard code never constructs queries directly against the metrics store. Instead, it calls a scoped API that accepts metric names, time ranges, and aggregation parameters, and the access layer adds the tenant filter before executing the query. This design means that a bug in the dashboard code — a missing filter, an incorrect join, a new widget that forgot to include the tenant scope — cannot produce cross-tenant data leakage because the access layer enforces scoping regardless of what the dashboard requests.

The access layer should operate on the principle of deny-by-default. If a query does not include a valid tenant identifier, it returns an empty result set, not an unscoped result. This is the opposite of how most systems are initially built. Most teams start with unscoped queries that work during development and add tenant filtering as they scale. The problem is that any query added before the tenant filter was implemented — or any query where the developer forgot to apply the filter — returns unscoped data. Deny-by-default ensures that the failure mode is missing data, not leaked data. A blank chart is embarrassing. A chart showing another tenant's data is a contract breach.

The metrics store itself should use **pre-aggregated, tenant-scoped tables** rather than raw event data with tenant filtering applied at query time. Pre-aggregation serves two purposes. First, it improves performance — computing a 90-day trend line from pre-aggregated daily scores is a 90-row scan, while computing it from raw evaluation events might require scanning millions of rows. Second, pre-aggregation eliminates the possibility that a complex aggregation query accidentally crosses tenant boundaries. When the daily accuracy score for tenant 247 is stored as a single row in a tenant-scoped table, there is no query that can inadvertently join it with tenant 248's data.

## Role-Based Views: Technical, Executive, and Legal

Not every person at a customer organization needs the same dashboard. The platform engineer responsible for integration needs different metrics than the VP who approves budget renewal, and both need different views than the compliance officer preparing for an audit.

The **technical view** shows the most detail: per-dimension scores, trend lines at weekly granularity, evaluation sample counts, model version change annotations on the timeline, and the ability to drill into specific evaluation periods. Technical users want to understand why a score changed, and they need enough granularity to correlate quality shifts with their own deployment or configuration changes. If a customer updated their prompt template on March 7th and accuracy dropped on March 8th, the technical view should make that correlation visible through its timeline annotations.

The **executive view** shows the outcome: composite quality score, SLA compliance status, quarter-over-quarter trend, and incident count with resolution summary. Executives do not want per-dimension breakdowns unless those dimensions are contractually significant. They want to know whether the platform is delivering value. The executive view should fit on a single screen with no scrolling. If it requires explanation beyond what labels and color coding provide, it is too complex.

The **legal and compliance view** shows evidence: SLA compliance history with contractual threshold lines, incident reports with timestamps and resolution details, evaluation coverage statistics showing what percentage of outputs were evaluated, and data handling compliance indicators. Compliance officers use this view to populate their own regulatory filings. The data must be exportable in a format that can be attached to compliance documentation — typically CSV or PDF with date stamps and verification hashes.

Implementing role-based views requires coordination with the customer's identity provider. The dashboard should consume the customer's SSO assertions to determine which role a user holds and render the appropriate view. If the customer has not configured role mappings, default to the executive view — the least detailed option. Showing too little is correctable. Showing too much is a breach.

## Real-Time Versus Batch Dashboards

Quality dashboards operate on two timescales, and mixing them creates confusion.

**Batch dashboards** update on a fixed schedule — daily, weekly, or after each evaluation cycle. They show aggregated scores, trend lines, and SLA compliance based on completed evaluation runs. Batch dashboards are the primary view for most customers because they reflect the stable, validated quality picture. The scores have been computed, checked for anomalies, and approved for display. Batch dashboards are the source of truth for SLA compliance calculations and contractual reporting.

**Real-time dashboards** show evaluation results as they are computed, with scores updating continuously as new evaluations complete. Real-time views are valuable for customers who are actively monitoring a deployment change, running a prompt experiment, or responding to a quality incident. But real-time scores are inherently noisier than batch scores because they reflect a smaller sample of recent evaluations and have not been smoothed by aggregation.

The mistake most platforms make is building a single dashboard that mixes real-time and batch data without clearly distinguishing them. A customer sees their accuracy at 91 percent on the batch view but 86 percent on the real-time widget and concludes that quality is dropping, when in fact the real-time number reflects a small sample that will average back to 91 percent once the full evaluation cycle completes. Separate the two views clearly. Label the real-time view as "live, partial data" and the batch view as "validated, complete cycle." Most customers should default to the batch view and access the real-time view only when they have a specific reason to monitor live quality.

## The "Explain This Score" Requirement

Customers will click on a low score. This is not a possibility. It is a certainty. When a customer's accuracy drops from 93 percent to 87 percent, they want to understand why. If the dashboard shows the number but cannot explain it, the customer's next action is a support ticket, an escalation to their CSM, or a loss of confidence that no number of future green indicators will restore.

The **score explanation layer** provides context for every metric the customer sees. For a quality score, the explanation includes which evaluation samples contributed most to the score change, what patterns appeared in low-scoring outputs, and whether the change correlates with a known event — a model update, a configuration change, a data distribution shift. The explanation does not need to be a detailed root cause analysis. It needs to give the customer enough context to form a mental model of what happened.

Building score explanations requires linking evaluation results back to sample-level data. When the customer clicks on their 87 percent accuracy score, the system retrieves the evaluation samples from that period, identifies the lowest-scoring outputs, and presents a summary: "accuracy decreased primarily due to lower performance on outputs involving multi-step calculations, which accounted for 34 percent of evaluated samples this cycle compared to 18 percent in the previous cycle." The customer now understands that the score dropped because the mix of queries changed, not because the model got worse. That understanding prevents a support escalation and reinforces trust.

The explanation layer must respect the same data access controls as the rest of the dashboard. A customer clicking "explain this score" should never be shown raw evaluation samples from another tenant, aggregate patterns computed across tenants, or internal debugging information. The explanation draws exclusively from the customer's own evaluation data, within the customer's own tenant scope.

## Customization: Letting Tenants Configure What Matters

Different customers care about different metrics. A customer whose primary use case is document summarization cares most about factual accuracy and completeness. A customer whose use case is customer support cares most about tone and response time. Showing both customers the same default dashboard forces one of them to wade through irrelevant metrics to find the ones that matter.

**Tenant-configurable dashboards** let each customer choose which metrics appear in their primary view, how those metrics are ordered, and what thresholds trigger visual alerts. This customization is expressed as a dashboard configuration stored alongside the tenant's evaluation fingerprint — the same configuration system described in Subchapter 3 of this chapter. The customer specifies their primary metrics, their preferred aggregation window, and their alert thresholds. The dashboard renders according to their configuration.

Customization has boundaries. Customers can reorder, hide, or emphasize metrics that the platform makes available to them. They cannot add metrics that the platform does not expose, request data outside their tenant scope, or modify how scores are computed. The customization is presentational, not analytical. The underlying evaluation remains governed by the tenant's rubric configuration as described in Chapter 3. The dashboard reflects those results through the customer's preferred lens.

The implementation pattern that works at scale is a **widget library** — a catalog of dashboard components that the customer can arrange in their preferred layout. Each widget is a self-contained component that queries a specific metric through the tenant-scoped access layer and renders it according to the customer's configuration. Adding a new widget to the library makes it available to all customers simultaneously. Removing a deprecated widget requires checking which customers have it in their layout and providing a migration path, the same way dimension deprecation works in the rubric library.

## The Quarterly Business Review Dashboard

The dashboard's most visible moment is the **quarterly business review**, where the customer's leadership team evaluates whether to renew, expand, or downgrade their platform commitment. The QBR dashboard is not the same as the day-to-day operational dashboard. It is a curated, narrative-driven view designed to show the trajectory of value delivery over the quarter.

The QBR view should include quality trend over the quarter with annotations for significant events, SLA compliance percentage for the period with comparison to the previous quarter, incident count and mean time to resolution, evaluation coverage statistics showing the breadth and depth of quality monitoring, and a "what improved" section highlighting specific quality dimensions that gained ground. This last element is critical. Showing what improved gives the customer a reason to believe that the platform is actively working on quality, not just measuring it. A QBR dashboard that shows static scores tells the customer "nothing changed." A QBR dashboard that shows improvement tells them "your investment is generating returns."

The QBR view is often the only dashboard that the customer's executive leadership ever sees. It must be self-explanatory without a presenter walking through it. Every chart needs a plain-language title. Every metric needs a one-sentence interpretation. Every trend line needs enough context that a VP who has never seen the dashboard before can understand the story in under two minutes.

## The Trust Calibration Principle

The right amount of dashboard transparency is not a fixed setting. It is a calibration that depends on the customer's sophistication, their industry's regulatory requirements, and the maturity of their relationship with your platform.

Early-stage customers who are still building confidence in the platform need more detail — per-dimension breakdowns, sample-level explanations, frequent update cadences. They are actively validating that your quality system works. Giving them visibility accelerates trust-building and reduces the support burden of answering questions that a transparent dashboard would have answered.

Mature customers who have been on the platform for years and have stable quality often prefer less detail — composite scores, SLA status, quarterly trends. They trust the system and do not need to validate it at granular level. Overwhelming them with detail they no longer need wastes their time and creates noise.

Regulated customers in healthcare, financial services, or government need the most detail, regardless of maturity, because they must demonstrate to their regulators that they monitor the quality of their AI systems. Their dashboard requirements are driven by external compliance mandates, not by internal preference. The dashboard must produce exportable evidence that satisfies auditors, which means timestamps, completeness metrics, and data handling compliance indicators that a general-purpose dashboard might not include.

The platform that serves all three customer types from a single dashboard view satisfies none of them. Role-based views, tenant-configurable layouts, and the widget library architecture described above make it possible to serve each customer the transparency they need — no more, no less.

The dashboard tells each customer how their system is performing right now. But what happens when that performance degrades? The next subchapter addresses alert design for multi-tenant systems, where the challenge is not detecting problems but routing them correctly — distinguishing between a single customer's quality issue and a platform-wide emergency.