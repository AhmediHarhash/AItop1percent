# 28.6 — Negotiating Quality: SLA Metrics That Bridge Engineering and Contract Language

In September 2025, a customer data platform serving 220 enterprise clients signed a $3.6 million annual contract with a multinational retail conglomerate. The contract included a quality SLA that read: "Provider shall maintain AI output quality at or above 95 percent accuracy as measured by industry-standard evaluation methods." Both sides signed. Legal was satisfied. Sales celebrated. Engineering read the clause six weeks later during onboarding configuration and realized they had no idea what it meant.

What counted as accuracy? The platform's eval system measured four distinct dimensions — factual correctness, completeness, format compliance, and relevance. None of them was called "accuracy." Did 95 percent mean all four dimensions had to be at or above 95, or that a composite score across all four had to be at or above 95? "Industry-standard evaluation methods" was worse — there was no industry standard for evaluating AI-generated customer insights in retail. The platform used LLM-as-judge scoring. The customer assumed human evaluation. The contract's quality clause was simultaneously binding and unmeasurable. When the customer's first quarterly review arrived and they asked for quality evidence, engineering produced a composite LLM-judge score of 93.2 percent. The customer's quality team produced a human-reviewed sample scored at 88.1 percent. Both numbers were defensible. Neither matched the contract language. The dispute consumed 340 hours of engineering and legal time over the following quarter and nearly ended the relationship.

## The Translation Gap

This is the **translation gap** — the space between what contracts promise and what evaluation systems can measure. It exists in virtually every enterprise AI contract, and it grows wider as AI capabilities become more nuanced and quality definitions become more multidimensional.

The translation gap has two sides. On the legal side, contracts are written in language optimized for enforceability. Legal teams want terms that are clear enough to adjudicate in a dispute. They gravitate toward single numbers — "95 percent accuracy" — because single numbers create unambiguous pass-fail thresholds. They reference "industry standards" because courts recognize industry standards as reasonable benchmarks. They avoid technical specificity because overly specific language limits the contract's flexibility and creates loopholes.

On the engineering side, evaluation systems are built for diagnostic precision. Engineering teams want metrics that are specific enough to debug — not "accuracy" but "factual correctness on numerical claims in the summary's first three paragraphs." They know that a single number hides the multi-dimensional reality of AI quality, where a model can be excellent on one dimension and mediocre on another. They know that "industry standard" means nothing concrete because no industry has standardized AI evaluation methodology. They know that the measurement method — human eval versus LLM-judge, sample size, sampling strategy, golden set construction — affects the number at least as much as the actual model quality.

The gap between these two perspectives is where quality disputes are born. Not because either side is wrong, but because each side is optimizing for a different function. Legal optimizes for enforceability. Engineering optimizes for diagnosability. A good SLA metric must satisfy both.

## Bad SLA Metrics and Why They Persist

Certain SLA metrics appear in AI contracts repeatedly despite being functionally unmeasurable. They persist because they sound reasonable to non-technical stakeholders and because nobody on the technical team reviews the contract language before it is signed.

**"User satisfaction above X percent"** is the most common bad metric. It sounds intuitive — of course we want users to be satisfied. But user satisfaction in AI systems is a trailing indicator influenced by dozens of factors beyond output quality: interface design, response latency, the user's mood, the complexity of the task, and whether the user has a comparison point. A customer whose users have never used an AI tool before will report high satisfaction on mediocre outputs because they have no baseline. A customer whose users previously used a superior competitor will report low satisfaction on good outputs because of anchoring. User satisfaction correlates loosely with output quality and tightly with expectation management. It is a product metric, not a quality metric. When you commit to 90 percent user satisfaction in an SLA, you are committing to a number you cannot control through quality improvements alone.

**"Accuracy as measured by industry-standard methods"** is the second most common bad metric. As of 2026, no standardized evaluation methodology exists for most AI output types. The EU AI Act's GPAI Code of Practice, finalized in mid-2025, establishes risk-based evaluation requirements but does not prescribe specific measurement methods for output quality. The NIST AI Risk Management Framework provides principles but not operational metrics. "Industry standard" in a contract means whatever the disputing parties' lawyers argue it means, which makes it useless as an engineering target and dangerous as a contractual commitment.

**"Quality comparable to human performance"** is the third recurring bad metric. Human performance on most AI tasks varies enormously. A senior domain expert produces different quality than a junior analyst. Quality comparable to which human? Measured how? On what sample? With what inter-rater reliability? In practice, this metric creates a moving target that the customer's quality team calibrates retroactively to whatever standard supports their position in a dispute. If the AI output matches a senior expert's quality, the customer can argue the standard should be the team's best performer. If it matches the team average, the customer can argue the standard should be "qualified professional." The metric is subjective enough to be interpreted in any direction.

These metrics persist because the people writing contracts and the people building evaluation systems rarely sit in the same room during contract negotiation. Sales teams agree to quality language because it closes the deal. Legal teams draft it because it follows contractual conventions. Engineering discovers it after signing and must figure out how to measure something that was never designed to be measured. The fix is not better lawyers. It is engineering involvement in SLA definition before the contract is signed.

## Good SLA Metrics: The Five Properties

A good SLA metric satisfies five properties simultaneously. Missing any one of them creates a metric that either cannot be measured, cannot be enforced, or will produce disputes.

**Property one: operational definition.** The metric must specify exactly what is being measured, on what data, using what method. Not "accuracy" but "factual correctness of extracted numerical values, measured against a golden set of 200 customer-provided reference documents, scored by the platform's automated evaluation pipeline with quarterly human validation of a 50-document subset." This level of specificity makes contracts longer. It also makes them enforceable without dispute.

**Property two: reproducibility.** Two competent evaluators applying the metric to the same outputs must produce the same result within a stated tolerance. If the metric depends on human judgment — as many AI quality metrics do — the SLA must specify the evaluation protocol, the evaluator qualifications, and the acceptable inter-rater agreement threshold. A metric that produces different results depending on who measures it is a metric that will produce a dispute every quarter.

**Property three: controllability.** The platform must be able to influence the metric through engineering actions. Metrics that depend heavily on factors outside the platform's control — user behavior, data quality from the customer's upstream systems, changes in the customer's business processes — create SLAs where the platform can fail the quality commitment without any quality degradation in its own systems. Good SLA metrics measure what the platform produces, not what happens after the customer receives it.

**Property four: decomposability.** When the metric drops below threshold, engineering must be able to diagnose why. A composite score of 91.3 percent that falls below a 93 percent threshold tells you nothing about what to fix. A metric structure that breaks into factual correctness at 95, completeness at 88, format compliance at 97, and relevance at 86 tells you that completeness and relevance need attention. SLA metrics should be composite scores backed by visible dimension-level scores, with the composite methodology specified in the contract.

**Property five: stability.** The metric must produce consistent scores on consistent outputs across time. If the evaluation methodology itself is subject to drift — as LLM-as-judge systems are when the judge model is updated — the SLA must specify how judge drift is handled. Does the contract pin a specific judge model version? Does it specify recalibration intervals? Does it define a tolerance band for measurement drift? Without stability provisions, a platform can fail an SLA because the judge model changed, not because the output quality changed.

## The Negotiation Process

Translating quality definitions into SLA metrics is a negotiation between four stakeholders, and leaving any of them out produces a metric that fails at least one of the five properties.

**Engineering** owns operational definition and decomposability. They know what the eval system can measure, at what granularity, with what reliability. They prevent the contract from promising metrics the system cannot produce. Engineering's input prevents the "95 percent accuracy" problem — vague metrics that sound precise but map to nothing in the evaluation pipeline.

**Customer success** owns the customer relationship and understands what the customer actually cares about. They translate the customer's domain-specific quality concerns into language that engineering can operationalize. They prevent the contract from measuring the wrong things — technically precise metrics that miss the dimensions the customer will judge their experience by.

**Legal** owns enforceability and dispute resolution. They ensure the metric language is clear enough to adjudicate if disagreement arises. They add measurement dispute resolution procedures, escalation paths, and the consequences of threshold violations. Legal's input prevents contracts that are technically elegant but legally unenforceable.

**The customer's technical team** owns validation. They confirm that the proposed metrics align with how they will evaluate quality internally. This is critical — if the SLA metric produces a score of 94 and the customer's internal evaluation produces a score of 82, the gap will become a dispute regardless of which number is "correct." The customer's technical team helps align the SLA measurement methodology with the customer's own quality assessment process.

The negotiation itself follows a three-round structure. In the first round, the customer defines quality in their own language — "we need accurate summaries that our analysts can trust." In the second round, engineering and customer success translate this into candidate metrics — factual correctness measured against a golden set, completeness measured against source document coverage, analyst acceptance rate measured through a feedback mechanism. In the third round, legal and the customer's procurement team review the candidate metrics for enforceability, agree on thresholds, define measurement frequency, and establish dispute resolution procedures. Three rounds, typically completed across two to three meetings, each lasting sixty to ninety minutes. This process takes longer than having sales agree to "95 percent accuracy." It also prevents the 340-hour disputes that follow from undefined metrics.

## The Metric Catalog Approach

Platforms that serve hundreds of enterprise customers cannot negotiate bespoke SLA metrics for every contract. The negotiation process described above produces better metrics, but it does not scale to twenty new customers per month. The solution is a **metric catalog** — a pre-defined set of SLA-ready metrics that satisfy all five properties, organized by quality dimension and evaluation methodology.

The metric catalog serves as a menu. Each metric in the catalog has a name, an operational definition, a measurement methodology, a recommended threshold range, and documented stability characteristics. During contract negotiation, the customer selects from the catalog rather than defining metrics from scratch. The catalog might include fifteen to twenty metrics: factual correctness by golden set comparison, completeness by source coverage analysis, format compliance by template matching, response relevance by query-alignment scoring, citation accuracy by reference verification, confidence calibration by prediction-outcome correlation, and so on. Each metric is already implemented in the evaluation pipeline. Each has documented reliability. Each satisfies the five properties.

The catalog does not eliminate customization. Customers still choose which metrics apply to their use case, set thresholds based on their requirements, and weight dimensions according to their priorities. But the customization happens within a structured framework rather than through unconstrained negotiation. The customer selects from proven metrics rather than inventing new ones. Engineering knows that every metric in the catalog is measurable, reproducible, and decomposable. Legal knows that every metric has been used in prior contracts without dispute. The catalog converts SLA negotiation from a creative exercise into a configuration exercise, which is exactly the shift you need to scale quality contracts across hundreds of customers.

## Measurement Dispute Resolution

Even well-defined SLA metrics produce disputes. The customer's internal evaluation produces a different number than your platform's evaluation. A model update changes scores in a way the customer questions. The golden set becomes stale and the customer argues that it no longer represents their current requirements. Good SLA contracts anticipate these disputes and include resolution mechanisms.

The most effective dispute resolution mechanism is **third-party calibration**. The SLA specifies that when the customer and platform disagree on a quality score by more than a defined margin — typically 5 to 8 points — a calibration process is triggered. Both parties submit a sample of 50 to 100 outputs to an agreed-upon evaluation protocol. Both parties score the sample independently. The scores are compared, disagreements are discussed, and the rubric is recalibrated to align the two evaluation perspectives. This process costs four to eight hours per instance. It happens rarely — once or twice per year for most customers. And it prevents the alternative: months of escalating disagreement that damages the relationship and consumes far more than eight hours.

The SLA should also specify what happens when the platform fails the quality threshold. Not every threshold violation is a breach. Seasonal changes in the customer's data, temporary upstream data quality issues, and evaluation measurement noise all create threshold violations that do not reflect genuine quality degradation. Good SLAs include a **cure period** — typically 15 to 30 days — during which the platform can investigate and remediate before the violation becomes a formal breach. The cure period gives engineering time to diagnose and fix without the pressure of immediate contractual consequences. It converts threshold violations from adversarial events into collaborative debugging opportunities.

## The Contract-Pipeline Alignment Audit

Once the SLA is signed, the final step is verifying that your evaluation pipeline actually measures what the contract promises. This sounds obvious. It is skipped more often than any platform team will admit.

The **contract-pipeline alignment audit** walks through every SLA metric and confirms three things. First, the evaluation pipeline computes exactly the metric described in the contract — not a similar metric, not a metric with the same name but different methodology, but the exact metric. Second, the pipeline's measurement frequency matches the contract's reporting requirements. If the SLA specifies monthly quality reports, the pipeline must produce monthly per-customer scores, not just on-demand scores that someone remembers to run. Third, the pipeline's data sources match the contract's scope — if the SLA measures quality on the customer's production traffic, the pipeline cannot substitute a synthetic test set.

The alignment audit takes two to four hours per customer. It prevents the scenario where the customer's first quarterly review reveals that the platform's eval system measures something slightly different from what the contract describes. That scenario, which happened to the customer data platform in the opening example, does not just create a dispute. It creates the impression that the platform either does not understand its own SLA or does not take it seriously. Neither impression is recoverable.

The translation gap between contract language and engineering measurement is not a problem you solve once. Every new customer brings new quality definitions that must be translated into measurable metrics. Every contract renewal creates an opportunity to refine SLA metrics based on a year of measurement experience. Every new capability your platform adds creates new quality dimensions that may need SLA coverage. The gap is permanent. The discipline of bridging it is what separates platforms that retain enterprise customers from platforms that lose them to the first competitor who can prove quality per-customer. The next subchapter addresses what happens when customer quality requirements do not fit neatly into a flat rubric — the inheritance, composition, and override patterns that make multi-tenant rubric systems both powerful and perilous.