# 28.1 — The Onboarding Ramp: From Signed Contract to First Quality Report

The first quality report you deliver to a new enterprise customer is the most consequential document your platform will ever produce for that account. It sets the baseline. It establishes whether your numbers feel credible. It determines whether the customer trusts your measurement system or spends the next twelve months second-guessing every score you send them. Get the onboarding right and you earn the kind of trust that survives a bad quarter. Rush it, and you spend more time defending your evaluation methodology than improving the customer's actual quality.

Most platform teams treat onboarding as a logistics exercise. Provision the tenant, configure the API keys, connect the data pipeline, run the first eval, send the report. That approach works for developer tools where the product is self-explanatory. It does not work for evaluation systems, where the product is judgment. An evaluation system that scores a customer's outputs without first understanding what that customer considers good, without building ground truth specific to their domain, without calibrating rubrics against their actual expectations, produces numbers that are precise but meaningless. The customer looks at a score of 87 percent and asks, "87 percent of what?" If you cannot answer that question in their language, with their examples, against their definition of quality, the number is noise.

**The Onboarding Ramp** is the structured, repeatable process that prevents this. It is not a checklist. It is a phased methodology that takes a new enterprise customer from signed contract to first quality report through a sequence of steps that each build on the last. Skip a step and the steps after it produce unreliable outputs. Rush a step and you create technical debt that compounds for months. The Onboarding Ramp is the difference between a platform that evaluates customers and a platform that evaluates customers correctly.

## The Five Phases

The Onboarding Ramp has five phases, each with a specific deliverable and a specific exit criterion. No phase begins until the previous phase's exit criterion is met. This sequencing feels slow to sales teams who want the customer live in two weeks, and it is the single most important process discipline in multi-tenant evaluation.

The first phase is **Discovery**. This is where your team extracts quality requirements from the customer, translates business language into evaluation language, and produces a structured discovery document that maps business objectives to measurable quality dimensions. The deliverable is the discovery document. The exit criterion is customer sign-off on the quality dimensions, their relative importance, and the initial threshold ranges. Discovery typically takes one to two weeks for enterprise customers, with two to three structured workshops of 90 minutes each.

The second phase is **Golden Set Bootstrapping**. Using the quality dimensions identified in discovery, your team builds the customer's initial golden set — the curated collection of inputs and verified-correct outputs that will serve as ground truth for all subsequent evaluation. The deliverable is a versioned golden set with documented coverage across the customer's use cases. The exit criterion is customer domain expert validation that the golden set's examples accurately represent what "correct" looks like for their specific context. Bootstrapping typically takes two to three weeks, depending on how much historical data the customer can provide.

The third phase is **Rubric Calibration**. Your evaluation team constructs the customer's Tenant Eval Fingerprint — the machine-readable configuration that defines dimensions, weights, thresholds, and scoring parameters — and calibrates it against the golden set and the customer's judgment. The deliverable is a validated, versioned fingerprint. The exit criterion is demonstrated alignment between the fingerprint's scores and the customer's quality assessments on a held-out validation set. Calibration typically takes one to two weeks, including two to three calibration rounds.

The fourth phase is **Baseline Sprint**. Your team runs the calibrated evaluation pipeline against the customer's actual production traffic — or a representative pilot sample if the customer is not yet live — and produces the first quality report. This report establishes the Day One baseline: the quality level from which all future improvement will be measured. The deliverable is the first quality report with per-dimension scores, composite score, identified failure patterns, and recommended priorities. The exit criterion is customer acknowledgment that the baseline accurately reflects their current quality experience. The baseline sprint typically takes one to two weeks.

The fifth phase is **Handoff to Steady-State**. Your team transitions the customer from onboarding mode to ongoing operations. This includes setting up automated evaluation cadence, configuring alerting thresholds, establishing the quarterly review schedule, and briefing the customer success manager who will own the relationship going forward. The deliverable is a documented operational plan. The exit criterion is the first automated evaluation run completing successfully without manual intervention. Handoff typically takes one week.

## Timeline Expectations by Customer Tier

Not every customer needs the full 90-day enterprise ramp. The timeline scales with the complexity of the customer's quality requirements, the specificity of their domain, and the volume of their production traffic.

Enterprise customers — those with complex, regulated, or highly customized use cases, typically paying more than $200,000 annually — get the full ramp. Ninety days from contract signature to first quality report. These customers usually have domain-specific terminology that requires custom vocabulary handling, regulatory constraints that mandate specific evaluation dimensions, and internal stakeholders with strong opinions about what quality means. Cutting the timeline below 60 days for these customers almost always results in a golden set that is too small, a rubric that is poorly calibrated, or a baseline that does not reflect the customer's actual quality experience. The time saved during onboarding becomes time spent on recalibration, re-bootstrapping, and relationship repair during the first year.

Mid-market customers — those with well-defined but less complex use cases, typically paying $50,000 to $200,000 annually — can complete the ramp in 30 to 45 days. These customers often fit into one of the platform's existing customer archetypes, which means the discovery phase can start from a template rather than a blank page. Their golden sets tend to be smaller and their rubric calibration faster because their quality requirements cluster around standard dimensions with standard weights. The risk at this tier is over-templating — assuming the customer fits an archetype when they actually have domain-specific requirements that the template misses.

Self-serve customers — those using the platform at lower volume with standard configurations, typically paying less than $50,000 annually — can be onboarded in seven to fourteen days through a largely automated process. Discovery is replaced by a self-service quality configuration wizard. The golden set is bootstrapped from the customer's first production outputs rather than from historical data. Rubric calibration uses the platform's default rubric with customer-selected dimension weights. The baseline is computed automatically after the first week of production traffic. This tier sacrifices depth for speed, and that is the right trade-off as long as the customer understands what they are getting. The danger is when a self-serve customer has enterprise-grade quality requirements but onboards through the self-serve path because nobody identified the mismatch.

## What Success Looks Like at Each Phase

Success at each phase is not "we completed the activities." Success is "we produced an artifact that the customer trusts and that the evaluation pipeline can use."

Discovery succeeds when the customer's stakeholders look at the discovery document and say, "Yes, this captures what we care about." Not "this looks reasonable" — that is polite acceptance, not genuine alignment. The customer should recognize their own priorities in the document, not feel like they are reading a generic template with their company name inserted. Discovery fails when the customer signs off quickly without engaging deeply, because that means they either do not understand the document or have delegated review to someone without authority, both of which will surface as alignment problems during calibration.

Golden set bootstrapping succeeds when the customer's domain experts have personally reviewed and approved every example in the set, and when the set covers the breadth of the customer's actual use cases rather than just the easy, common ones. Bootstrapping fails when the golden set is built entirely by your team without customer domain expert involvement, because the customer's implicit quality standards — the ones they cannot articulate but recognize on sight — are not captured.

Rubric calibration succeeds when the fingerprint's scores on a held-out validation set match the customer's quality judgments within an acceptable tolerance — typically within five percentage points on the composite score and within eight points on any individual dimension. Calibration fails when the team declares alignment after one round because the numbers looked close enough, without testing on examples the customer considers borderline.

Baseline sprint succeeds when the customer looks at the first quality report and their reaction is, "That matches what we are seeing," not "Those numbers seem high" or "That does not reflect our experience." A baseline that overstates quality is worse than one that understates it, because an inflated baseline creates expectations the platform cannot maintain. The customer expects improvement from 91 percent. If reality is closer to 78 percent, every future report will feel like regression even if quality is stable.

Handoff succeeds when the customer success manager can answer the customer's quality questions without escalating to the evaluation engineering team. The CSM should understand the customer's Quality Contract, know which dimensions matter most, and be able to interpret quality reports in the context of the customer's specific priorities. Handoff fails when the CSM inherits the account with a folder of documents they have not read and a fingerprint they do not understand.

## The Cost of Skipping Phases

Teams skip phases for predictable reasons. Sales promised the customer would be live in three weeks. The customer's launch date is fixed. The customer says they "already know what they want" and discovery feels redundant. The evaluation team is onboarding four customers simultaneously and does not have bandwidth for full golden set bootstrapping on all of them.

Every skipped phase has a specific cost, and that cost is always higher than the time the skip saved.

Skipping discovery means the evaluation team builds a fingerprint based on assumptions rather than evidence. Those assumptions will be wrong in at least two or three dimensions. The customer will receive quality reports that measure the wrong things, and the first quarterly review will be spent re-doing the discovery that should have happened during onboarding. The typical cost is four to six weeks of recalibration work spread across the first two quarters, plus the relationship damage of delivering reports the customer does not trust for three to four months.

Skipping golden set bootstrapping means the evaluation pipeline has no customer-specific ground truth. It evaluates against the platform's default golden set or against a hastily assembled set that does not cover the customer's actual use cases. Regressions specific to the customer's domain go undetected. The typical cost is one or two missed regressions in the first six months — each of which the customer discovers before the platform does, which destroys the credibility of the evaluation system.

Skipping rubric calibration means the fingerprint's scores do not align with the customer's quality perception. Every quality report becomes a source of friction rather than trust. The customer's internal teams start maintaining their own quality tracking because they do not trust the platform's numbers, which means the platform has lost its role as the source of truth for quality. Recovering from this takes six to twelve months and requires essentially re-onboarding the customer.

Skipping the baseline sprint means the customer's first quality report arrives without context. A score of 82 percent has no meaning without a baseline — is that good? Bad? Improving? Degrading? The customer cannot tell, and neither can your team. Every subsequent report exists in a vacuum until enough data accumulates to establish trends retroactively, which takes three to four months of steady-state operation.

## The Onboarding Team

The onboarding ramp requires a specific team composition, and the people on that team matter as much as the process they follow.

The **eval engineer** owns the technical execution of onboarding. They build the golden set, construct the fingerprint, calibrate the rubric, run the baseline sprint, and validate the pipeline before handoff. This person must understand both the platform's evaluation architecture and the customer's domain well enough to make judgment calls about dimension granularity, threshold sensitivity, and golden set coverage. A junior engineer who can follow a checklist is not sufficient. The eval engineer needs the judgment to recognize when a customer's stated requirements do not match their revealed preferences — when they say accuracy matters most but their feedback consistently focuses on tone.

The **customer success manager** owns the relationship and serves as the translator between the customer's business language and the evaluation team's technical language. During discovery, the CSM facilitates the workshops and ensures the right stakeholders attend. During calibration, the CSM helps the customer interpret scores and articulate disagreements. After handoff, the CSM becomes the customer's primary contact for quality conversations. A CSM who does not understand evaluation fundamentals — who cannot explain why a composite score of 85 percent can coexist with a per-dimension score of 72 percent — will create more confusion than they resolve.

The **domain specialist** provides expertise in the customer's specific industry or use case. For a healthcare customer, this might be someone with clinical terminology knowledge. For a financial services customer, this might be someone who understands regulatory language requirements. The domain specialist reviews the golden set for domain accuracy, validates that the rubric's quality dimensions capture the domain's actual quality surface, and flags domain-specific edge cases that the eval engineer might miss. Not every customer requires a dedicated domain specialist — a platform that serves primarily e-commerce customers may have sufficient domain knowledge on the eval team itself. But customers in healthcare, legal, financial services, or other regulated domains almost always require domain-specific expertise during onboarding.

This three-person team — eval engineer, CSM, domain specialist — is the minimum viable onboarding team for enterprise customers. Mid-market customers can often be onboarded by an eval engineer and a CSM without a dedicated domain specialist. Self-serve customers are onboarded by automated tooling with CSM oversight.

## Why the First Quality Report Is a Trust Instrument

The first quality report does not just measure quality. It establishes whether the customer believes that the platform can measure quality. That distinction is everything.

Consider what the customer experiences during onboarding. They sat through discovery workshops. They provided examples and reviewed golden set candidates. They participated in calibration exercises. They invested time and attention from their domain experts, their product team, and possibly their compliance team. The first quality report is the payoff for that investment. It is the moment where the customer sees whether all that work translated into something real — a measurement system that reflects their actual quality experience.

If the first report is credible — if it identifies the strengths and weaknesses the customer already senses, if the scores align with their intuition, if the failure patterns match what they see in practice — the customer trusts the platform's evaluation system. That trust is worth more than any SLA guarantee, because a customer who trusts your measurement system will engage with your improvement recommendations, accept your regression explanations, and defend your platform internally when stakeholders question quality. Trust turns the evaluation system from a reporting tool into a shared language for quality.

If the first report is not credible — if it assigns high scores to outputs the customer considers mediocre, or flags dimensions the customer does not care about, or misses the failure patterns the customer's own team has been tracking — the customer concludes that the platform does not understand their quality needs. From that point forward, every quality conversation begins with skepticism. The customer starts building their own evaluation processes as a backup. The platform's quality reports become something the customer receives and ignores rather than something they act on. Recovering from a bad first report is possible but expensive — it typically requires a second discovery phase, a golden set rebuild, and a full recalibration cycle, all while the customer is questioning whether they chose the right platform.

The Onboarding Ramp exists to make the first quality report trustworthy. Every phase contributes to that outcome. Discovery ensures the report measures what the customer cares about. Golden set bootstrapping ensures the measurement is grounded in the customer's own ground truth. Rubric calibration ensures the scores align with the customer's judgment. The baseline sprint ensures the report reflects the customer's actual current quality. Skip any phase and the first report's credibility is at risk.

## Tracking Onboarding Health Across the Portfolio

At scale — when your platform is onboarding fifteen to twenty new enterprise customers per quarter — individual onboarding quality is necessary but not sufficient. You need portfolio-level visibility into onboarding health.

The metrics that matter are straightforward. **Ramp completion rate** measures what percentage of customers complete all five phases within their tier's expected timeline. A healthy platform completes 80 percent or more within timeline. Below 70 percent means the process is broken or the team is under-resourced. **Phase skip rate** measures how often phases are compressed or skipped, and which phases are most commonly cut. If golden set bootstrapping is skipped for 30 percent of new customers, the platform is accumulating evaluation debt that will surface as customer quality disputes in six to twelve months. **First report credibility score** measures customer satisfaction with the first quality report, typically via a structured feedback form asking whether the report's scores matched their experience. A credibility score below 75 percent means the onboarding process is not producing trustworthy baselines. **Time to steady-state** measures how long after handoff before the customer's evaluation runs autonomously without manual intervention. The target is zero manual intervention within two weeks of handoff.

These metrics should be reviewed monthly by the evaluation platform leadership team — not just the onboarding team, but also the engineering leads responsible for the eval pipeline, because onboarding failures often trace back to platform limitations rather than process failures. If golden set bootstrapping consistently takes longer than expected, the tooling for golden set construction may need investment. If calibration rounds consistently fail to converge, the rubric architecture may not be flexible enough to accommodate the diversity of customer requirements. Onboarding metrics are a signal about platform maturity, not just team performance.

## The Ramp as a Revenue Protection Mechanism

Here is the reframe that should change how your organization budgets for onboarding. The Onboarding Ramp is not a cost center. It is a revenue protection mechanism.

Enterprise customers who complete the full ramp renew at significantly higher rates than those who were rushed through onboarding. The reason is not mysterious. Customers who trust the evaluation system engage with quality reports, act on recommendations, and see measurable improvement over time. That improvement becomes the justification for contract renewal. Customers who do not trust the evaluation system ignore quality reports, build parallel tracking systems, and arrive at renewal conversations uncertain whether the platform is delivering value. The first group is easy to renew. The second group is easy to lose.

The math is not complicated. A 90-day onboarding ramp for an enterprise customer costs roughly $15,000 to $25,000 in team time — the eval engineer's time, the CSM's time, the domain specialist's time, and the customer stakeholders' time. An enterprise customer paying $300,000 annually who churns after one year because they never trusted the evaluation system represents $300,000 in lost revenue, plus the acquisition cost of finding their replacement. The onboarding investment is 5 to 8 percent of the first year's contract value. The cost of skipping it is the entire contract.

The Onboarding Ramp defines the structure. But the structure begins with a single, critical step: understanding what the customer actually needs. The next subchapter covers the discovery phase — how to extract quality requirements from customers who think in business terms, not evaluation terms.
