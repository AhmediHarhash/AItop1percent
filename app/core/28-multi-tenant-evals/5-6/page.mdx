# 28.6 — Onboarding at Scale: Templated Workflows vs Custom Engagements

How do you onboard twenty enterprise customers per month into a multi-tenant evaluation system when the process that produced great results for your first ten customers required six weeks of senior engineer time per customer? You do not scale by working harder. You scale by separating what must be custom from what can be templated, and then building the infrastructure that makes the templated parts fast without making the custom parts disappear.

Every multi-tenant AI platform reaches this inflection point. The bespoke onboarding process — hand-crafted discovery sessions, custom rubric design from scratch, manually assembled golden sets, one-on-one calibration workshops — delivered excellent outcomes for early customers. But it consumed 120 to 160 hours of engineering and solutions architecture time per customer. At that rate, onboarding twenty customers per month requires 2,400 to 3,200 hours of senior staff time — roughly fifteen to twenty full-time senior engineers doing nothing but onboarding. The math does not work. Either you hire an army of onboarding specialists who dilute your institutional knowledge, or you build systems that encode that knowledge into repeatable workflows. The second path is the one that scales.

## The Three Onboarding Tiers

The first structural decision is acknowledging that not every customer needs the same onboarding depth. A Fortune 500 healthcare system paying $1.2 million per year and deploying your platform across 40 clinical workflows has different onboarding needs than a mid-market e-commerce company paying $48,000 per year for product description generation. Trying to give both customers the same onboarding experience wastes resources on one and underserves the other.

**White-glove enterprise** onboarding is the full bespoke experience. Dedicated solutions architect. Multi-session discovery workshops. Custom rubric design with per-dimension expert calibration. Hand-curated golden sets built with the customer's domain specialists. A full alignment workshop with three calibration rounds. A baseline sprint with on-site readout. Typical duration: six to ten weeks. Typical platform team investment: 120 to 200 hours. This tier is reserved for customers whose annual contract value exceeds a threshold you define based on your unit economics — typically $200,000 per year or above. At most platforms, this tier serves 10 to 15 percent of customers but represents 50 to 60 percent of revenue.

**Guided mid-market** onboarding uses templated workflows with human checkpoints. The customer fills out a structured discovery questionnaire instead of sitting through multi-session workshops. The rubric is assembled from industry-specific templates with guided customization. The golden set is bootstrapped from template examples and refined through a single calibration session. The baseline sprint is partially automated. Typical duration: two to three weeks. Typical platform team investment: 30 to 50 hours. This tier serves the middle of your customer base — typically 30 to 40 percent of customers — and balances onboarding quality with scalability.

**Self-service** onboarding is the fully templated path. The customer selects their industry vertical and primary use case from a menu. The system generates a starting rubric, a starter golden set, and a default calibration profile based on the template. The customer refines the configuration through a self-service interface. The baseline runs automatically once the configuration is finalized. Typical duration: three to five days. Typical platform team investment: two to five hours for review and approval. This tier serves your long tail — 50 to 60 percent of customers — and scales without linear headcount growth.

The tier assignment is not just about contract value. It also considers domain complexity and regulatory requirements. A $60,000 customer in healthcare might need guided or even white-glove onboarding because the regulatory requirements demand expert calibration, while a $150,000 customer in internal knowledge management might be well served by the guided tier because the quality definition is straightforward. The tier matrix maps contract value on one axis and domain complexity on the other, and the assignment comes from the intersection.

## What Can Be Templated

Not every step of onboarding benefits equally from templatization. Some steps are highly repeatable across customers in the same industry. Others are irreducibly specific to each customer. Knowing which is which determines where to invest in templates and where to preserve human engagement.

**Discovery questionnaires** are highly templatable. The questions you ask during discovery — what use cases, what quality dimensions matter, what regulations apply, what output formats are required, what failure modes are unacceptable — are largely the same across customers in a given industry. An insurance customer's discovery questionnaire covers claims processing, policy summarization, underwriting analysis, and regulatory documentation. A legal customer's questionnaire covers contract review, case research, brief drafting, and compliance analysis. Building industry-specific questionnaires that guide the customer through a structured self-assessment takes months upfront but eliminates hours of live discovery per customer.

The templated questionnaire should not be a simple form. It should be a branching workflow where each answer opens the next relevant set of questions and closes irrelevant ones. A customer who answers "no" to "does your use case involve patient data" should not see the next fifteen questions about HIPAA requirements. A customer who answers "yes" to "do your outputs include financial projections" should see questions about numerical accuracy, disclaimer requirements, and regulatory disclosure that a non-financial customer never encounters. The branching logic encodes the discovery expertise your solutions architects built over hundreds of customer engagements.

**Rubric starting points by industry** are the second major templating opportunity. Your platform has onboarded dozens of healthcare customers. You know that healthcare rubrics almost always include accuracy, clinical completeness, regulatory compliance, patient safety language, and appropriate hedging. The starting rubric for a new healthcare customer should include these dimensions with pre-written definitions, pre-set scoring criteria, and pre-calibrated anchor examples drawn from your library of previous healthcare calibrations. The customer customizes from this starting point rather than building from scratch.

Industry-specific rubric templates save two kinds of time. They save the platform team time by eliminating the rubric design phase for common dimensions. And they save the customer time by giving them a rubric that already "mostly works" rather than asking them to articulate quality dimensions they have never formalized before. A healthcare compliance officer who sees a template rubric with "clinical accuracy" defined as "all diagnoses, treatments, and medication references are factually correct and current as of the output date" can nod and add their customization, rather than staring at a blank page trying to articulate what accuracy means.

**Golden set bootstrapping** can be partially templated through industry-specific example libraries. Your platform maintains a library of anonymized, representative examples for each industry vertical — not real customer data, but synthetic or licensed examples that represent the types of inputs and outputs common in that industry. When a new insurance customer starts onboarding, the system generates a starter golden set of 40 to 60 examples drawn from the insurance library, covering the standard use cases: claims assessment, policy interpretation, coverage determination, and customer communication. The customer reviews and replaces examples that do not match their specific context, but the starting point is 70 to 80 percent usable rather than empty.

**Baseline sprint automation** is the most straightforwardly templatable step. The sequence — confirm configuration, execute evaluation, compute statistics, generate report — is identical for every customer. The variables are the customer's configuration, their rubric, their golden set, and their thresholds. All of these are captured in the customer's evaluation configuration by the time the baseline sprint begins. An automated baseline pipeline takes the configuration as input, runs the evaluation, generates the report, and delivers it for human review. The automation reduces the baseline sprint from a two-week effort to a two-day execution plus a one-day review.

## What Cannot Be Templated

Certain onboarding steps resist templatization because they depend on knowledge that is specific to each customer and cannot be captured in industry-level templates.

**Customer-specific quality dimension discovery** happens when a customer cares about a quality aspect that no template anticipates. A pharmaceutical company might require that every output distinguish between FDA-approved indications and off-label uses — a quality dimension that is not in the standard healthcare template because it is specific to pharmaceutical regulatory contexts. A government contractor might require outputs to comply with a specific style guide that no other customer uses. These dimensions only emerge through conversation with the customer's domain experts, and they cannot be predicted from industry category alone.

**Domain vocabulary calibration** is the process of teaching your judge what specific terms mean in the customer's context. "Adequate" means something specific in clinical trial documentation — it has a regulatory definition. "Material" means something specific in financial reporting — it has a legal threshold. If your judge interprets these terms using their common English meaning rather than their domain-specific meaning, scores will systematically miss. Vocabulary calibration requires examples from the customer's actual domain, reviewed by the customer's actual experts.

**Regulatory requirement mapping** can be partially templated — HIPAA requirements are common to all US healthcare customers — but the specific regulatory mix each customer faces is unique. A healthcare customer operating in the EU faces GDPR and the EU AI Act in addition to local clinical data regulations. A financial services customer in Singapore faces MAS guidelines that differ from US SEC requirements. The regulatory map for each customer must be verified by someone who understands both the regulations and the customer's specific exposure, which usually means a compliance-aware solutions architect reviewing the template output.

The boundary between templatable and custom shifts over time. As you onboard more customers in a given industry, patterns emerge that were previously unique. The pharmaceutical quality dimension that seemed specific to one customer turns out to be relevant to five others. The government style guide requirement appears in three more contracts. When a "custom" dimension appears three or more times, it graduates into the industry template. Your template library is a living system that learns from every onboarding engagement.

## The Template Library

A mature onboarding system maintains a template library organized by industry vertical, use case, and regulatory environment. The library is not just a collection of documents. It is a structured knowledge base with version control, usage analytics, and feedback loops.

**Industry templates** form the top level. Each industry — healthcare, financial services, legal, e-commerce, insurance, government, technology, manufacturing — has a template package that includes: a discovery questionnaire with industry-specific branching logic, a starting rubric with pre-defined dimensions and scoring criteria, a golden set bootstrap library with representative examples, a regulatory requirement checklist, and a set of calibration anchor examples organized by dimension and score level.

**Use case templates** nest within industry templates. Within healthcare, the clinical documentation use case has different quality dimensions than the patient communication use case. Within financial services, regulatory reporting has different requirements than customer advisory. Use case templates refine the industry template with use-case-specific scoring criteria, failure mode categories, and example sets.

**Regulatory overlays** are modular additions that layer regulatory requirements on top of industry and use case templates. A HIPAA overlay adds patient data handling requirements to any healthcare template. An EU AI Act overlay adds transparency and documentation requirements to any customer operating in the European Union. A SOC 2 overlay adds data handling and audit trail requirements. Overlays are composable — a healthcare customer operating in the EU with a SOC 2 requirement applies all three overlays to their base template.

Maintaining the template library requires dedicated ownership. Someone — a templates lead or a templates committee — reviews completed onboardings, identifies patterns that should be templated, updates existing templates based on calibration feedback, and retires templates that no longer reflect current best practices. Without ownership, the library stagnates. Templates written eighteen months ago reference outdated model names, use rubric definitions that predate your latest scoring improvements, and miss regulatory changes that occurred since they were created. A stale template library is worse than no templates because it creates false efficiency — the onboarding goes fast, but the result is a miscalibrated eval setup that requires rework later.

## Automating the Baseline Sprint

The baseline sprint is the most mechanical phase of onboarding and the best candidate for end-to-end automation. The automated baseline pipeline takes five inputs: the customer's production configuration, their calibrated rubric, their golden set, their threshold definitions, and their sampling parameters. It produces one output: a baseline report ready for human review.

The automation pipeline executes a fixed sequence. First, it verifies the customer's configuration is complete — all required fields populated, model selection valid, retrieval pipeline connected, prompt templates finalized. If any configuration element is missing or invalid, the pipeline halts and generates a specific error message identifying the gap. Second, it collects the evaluation sample — either pulling from the customer's live traffic if they are already active, or generating outputs from a representative test set if they are pre-launch. Third, it runs the full rubric evaluation across the sample using the customer's calibrated judge configuration, including their per-customer rubric qualifications and anchor examples. Fourth, it computes all statistics — per-dimension means, percentiles, threshold pass rates, composite scores, failure mode categorization. Fifth, it generates the baseline report in the standard format, comparing scores to the platform floor and highlighting dimensions that need attention.

The entire pipeline runs without human intervention. What requires human involvement is the review: a solutions architect or senior eval engineer reviews the generated report for anomalies, verifies that the failure mode categorization is coherent, and confirms that the report is ready for customer delivery. This review typically takes 30 to 60 minutes. Compare that to the manual baseline sprint, which takes 60 to 80 hours of engineering time over two weeks.

The automated pipeline also enables faster iteration. If the customer reviews the baseline and says "this sampling is not representative — we process twice as many small claims as large claims, and your sample was evenly split," you can adjust the sampling parameters and re-run the pipeline in hours rather than re-running a two-week manual process. Speed of iteration matters during onboarding because the customer's patience is finite and every delay extends the time to their first quality report.

## Metrics for Onboarding Quality

Scaling onboarding without measuring onboarding quality produces fast onboarding that delivers poor evaluation setups. Three metrics track whether your templated workflows are producing outcomes as good as your bespoke process.

**Time-to-first-report** measures how many days elapse between contract signature and the customer receiving their first baseline quality report. For white-glove customers, the target is typically 30 to 45 days. For guided mid-market, 10 to 15 days. For self-service, 3 to 5 days. This metric tracks operational efficiency, but it does not track quality. A fast onboarding that produces a miscalibrated eval setup is worse than a slow onboarding that produces a good one.

**Customer satisfaction at day 30** captures the customer's subjective assessment of the onboarding experience. This is typically a brief survey — four to six questions covering clarity of the quality report, confidence in the eval system's accuracy, responsiveness of the onboarding team, and overall satisfaction. A score below 7 out of 10 on any dimension triggers a follow-up from the customer success team. The day-30 survey also asks: "Do you trust the quality scores in your evaluation reports?" This single question is the most predictive indicator of long-term retention. Customers who do not trust their eval scores at day 30 churn at three times the rate of customers who do.

**Eval accuracy at day 90 versus day 30** is the quality metric that matters most. At day 30, the customer's eval setup has been running for a month. At day 90, it has been running for three months and has likely undergone at least one optimization cycle. Compare the judge-human agreement rate at day 90 to day 30. If agreement improved or held steady, the onboarding produced a sound evaluation foundation that subsequent work built on. If agreement declined, the onboarding produced a fragile setup that broke under real-world conditions — a signal that the templates or the process skipped something important.

Track these metrics by onboarding tier. If your self-service tier shows day-30 trust scores of 5.2 out of 10 while your white-glove tier shows 8.4, the self-service templates need improvement. If your guided tier shows declining day-90 eval accuracy, the guided calibration process is not producing durable rubric configurations. The tier-by-tier comparison reveals where the trade-offs between scale and quality have tilted too far toward scale.

## The Scaling Curve

The transition from bespoke to templated onboarding does not happen all at once. It follows a curve that most platforms traverse over twelve to eighteen months.

At zero to twenty customers, everything is bespoke. You are learning what questions to ask, what rubric dimensions matter, what calibration patterns emerge. This is the period where institutional knowledge is built. Do not try to templatize yet. Every customer teaches you something new, and premature templatization locks in knowledge that is still forming.

At twenty to fifty customers, you begin extracting patterns. Your third insurance customer asks the same discovery questions as the first two. Your fifth healthcare customer needs the same rubric dimensions as the previous four. You build your first industry templates — not polished products, but rough drafts that capture the 70 percent of onboarding that repeats. The guided tier emerges as a middle ground between bespoke and nonexistent.

At fifty to one hundred customers, the templates mature. You have enough onboarding data to know which template dimensions are universal and which are customer-specific. Your self-service tier launches for simpler use cases. The automated baseline pipeline comes online. You shift senior engineering time from executing onboardings to improving the onboarding system.

At one hundred to three hundred customers, the template library becomes the primary onboarding engine. White-glove onboarding is reserved for the top tier. Guided and self-service handle the volume. The feedback loop — onboarding outcome data flowing back into template improvement — is automated. New templates are generated from patterns detected in the onboarding data rather than designed from scratch.

Beyond three hundred customers, the scaling challenge shifts from onboarding to maintenance. You have hundreds of active evaluation configurations, hundreds of calibrated rubrics, hundreds of golden sets — all requiring periodic re-calibration, re-validation, and updating. The systems that scale onboarding must also scale the ongoing maintenance that every onboarded customer generates. Section 26 covers how evaluation systems scale beyond the onboarding phase, and the infrastructure patterns described there apply directly to maintaining the evaluation configurations that onboarding creates.

## The Trade-Off That Never Goes Away

The tension between template efficiency and customer specificity is permanent. Push too far toward templates and you produce cookie-cutter evaluation setups that miss what makes each customer's quality definition unique. Push too far toward customization and you burn through onboarding capacity at a rate that caps your customer growth.

The resolution is not a fixed ratio. It is a continuous adjustment driven by data. Track which template elements are most frequently overridden by customers during guided onboarding — those elements need better defaults or finer-grained template options. Track which custom elements are most frequently added by white-glove customers — those elements are candidates for template inclusion. Track which onboarding configurations produce the highest day-90 eval accuracy — those configurations represent your best practices, and they should inform the next template update.

The platform that gets this balance right does not feel templated to the customer. The healthcare customer receives a rubric that already speaks their clinical language, a golden set that already covers their primary use cases, and a calibration profile that already knows that clinical terseness is a feature, not a flaw. They customize the last 20 to 30 percent, not the first 80 percent. The onboarding feels efficient and expert simultaneously — which is exactly the experience that builds trust in the evaluation system before it has produced its first real quality report.

But templates only solve the efficiency side of onboarding. The next subchapter tackles the opposite problem: the onboarding trap, where moving too fast through the onboarding process — whether bespoke or templated — creates technical debt in the evaluation configuration that compounds for months and never gets repaid.
