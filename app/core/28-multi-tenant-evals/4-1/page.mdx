# 28.1 — The Configuration Blindspot: Evaluating a Product Nobody Uses

**The Default Configuration Anti-Pattern** is the most widespread and least visible failure mode in multi-tenant evaluation. It works like this: your eval team builds a suite against the platform's staging defaults — the base prompt template, the standard model, the default retrieval pipeline, the out-of-the-box safety filters. Every test passes. Every score looks healthy. And not a single customer on your platform actually runs that configuration.

The moment a customer changes their system prompt, swaps to a different model, enables a LoRA adapter, adjusts their retrieval top-k, turns on a domain-specific safety filter, or configures custom output formatting, they are running a version of your product that your eval suite has never tested. Multiply that by 200 customers, each with a unique combination of settings, and you have 200 untested products in production. Your eval dashboard shows green. Your customers experience something else entirely. You are evaluating a product nobody uses, and the gap between what you test and what customers run widens every time a customer edits a setting.

This subchapter covers how the blindspot forms, why it is so difficult to detect from the platform side, what it costs when it surfaces, and the first steps toward closing the gap. The configuration space problem at scale follows in the next subchapter — but first you need to understand why the blindspot exists in the first place.

## How the Blindspot Forms

The blindspot does not appear overnight. It forms gradually through a sequence of decisions that each make sense in isolation.

When your platform launches, you have five customers. They all run roughly the same configuration — maybe one has a custom system prompt, another uses a slightly different model, but the differences are minor. Your eval suite tests the default setup, catches issues early, and accurately predicts what customers experience. The alignment between eval configuration and production configuration is high because there is not much configuration to diverge from.

By the time you reach 50 customers, the divergence has started. Your self-serve tier lets customers edit prompt templates through a UI. Your enterprise tier offers custom retrieval pipelines and per-tenant adapter weights. A few customers have requested specific model versions pinned to their accounts because a newer model changed behavior they depend on. Your eval suite still tests the default configuration. It catches platform-wide regressions, but it misses the configuration-specific failures that affect subsets of customers. Nobody notices because the affected customers have not complained yet — or they complained to support, and the tickets were closed individually without connecting them to the eval gap.

At 200 customers, the divergence is structural. Your configuration management system shows 340 unique prompt template variants, 7 model and adapter combinations in active use, 12 distinct retrieval pipeline configurations, and 4 tiers of safety filter strictness. Your eval suite still tests one configuration. The probability that the eval configuration matches any specific customer's production configuration is effectively zero. You are not testing what customers run. You are testing what nobody runs.

The insidious part is that this divergence is invisible from the eval dashboard. Every metric on that dashboard reports results for the default configuration. If the default configuration performs well — and it usually does, because that is the configuration the engineering team optimizes for — the dashboard shows high scores, stable trends, and passing gates. The dashboard cannot tell you that Customer 84's custom prompt template interacts badly with last week's model update, because the eval suite has never run Customer 84's prompt template. The dashboard is accurate and useless at the same time.

## The Reproduction Gap

The most common symptom of the configuration blindspot is the reproduction gap: customers report quality issues that your team cannot reproduce. A customer says their extraction accuracy has dropped. Your engineer pulls up the eval suite, runs the relevant test cases, and sees no drop. They check the model, the pipeline, the data — everything looks normal. They tell the customer the issue is not reproducible on their end. The customer is frustrated because the problem is clearly happening in their production traffic.

The root cause of the reproduction gap is almost always configuration mismatch. The customer runs a different prompt template than the one in your eval suite. Or they use a fine-tuned adapter that the eval suite does not load. Or their retrieval pipeline pulls from a customer-specific knowledge base that the eval suite does not have access to. The eval suite is testing a different system than the one the customer is using. Of course the issue is not reproducible — you are testing the wrong product.

A healthcare SaaS platform serving 140 clinic networks experienced this pattern 23 times in a single quarter during late 2025. Each time, a customer reported degraded quality on clinical note summarization. Each time, the on-call engineer checked the eval suite and found no regression. Each time, the ticket was closed with a note that the issue could not be reproduced. It was not until a customer success manager noticed the pattern — 23 unreproducible quality complaints in three months, up from 4 the previous quarter — that the team investigated the configuration layer. They discovered that 67 of their 140 customers had modified their system prompts through the self-serve customization interface, and none of those modified prompts were represented in the eval suite. The eval suite was testing a product that 48 percent of customers had already moved past.

## The Cost of False Confidence

The configuration blindspot does not just create a detection gap. It creates false confidence — a state more dangerous than ignorance because it leads to affirmative decisions based on incorrect evidence.

When your eval suite shows a 93 percent accuracy score and you are confident that score reflects your customers' experience, you make decisions accordingly. You approve a model update because the eval shows improvement. You defer an investment in better retrieval because the eval shows retrieval quality is stable. You tell a prospect during a sales call that your platform maintains 93 percent accuracy. Every one of these decisions is grounded in a number that does not describe any customer's actual experience.

The cost materializes in three forms. First, deferred regressions. Issues that would have been caught by configuration-aware eval are caught instead by customers, weeks or months after they begin. By the time you learn about them, the damage — churn risk, support burden, trust erosion — has already accumulated. Second, misallocated engineering investment. If your eval says retrieval quality is high but 40 percent of your customers run a custom retrieval configuration that performs poorly, you are optimizing the wrong thing. You invest in model quality while the real problem is retrieval quality for specific configurations. Third, eroded trust in the eval system itself. When engineers learn that the eval suite does not test what customers run, they stop trusting the eval suite entirely. They start making decisions based on gut feel and customer complaints instead of metrics. The eval infrastructure becomes theater — expensive, maintained, and ignored.

A legal document review platform learned this lesson after a model upgrade that its eval suite approved unanimously. The upgrade improved accuracy on the default English-language configuration by 2.1 points. It also degraded accuracy by 8 points for 11 customers who used a German-language adapter and by 5 points for 7 customers who ran a specialized contract clause extraction prompt. None of those configurations were in the eval suite. The platform learned about the regressions from customer complaints over the following four weeks, during which two enterprise customers initiated formal quality dispute processes under their SLAs. The total cost — engineering time to diagnose and remediate, customer success time to manage escalations, a service credit totaling $47,000, and an estimated $220,000 in pipeline revenue that stalled because the sales team lost confidence in quoting accuracy numbers — exceeded $300,000. The configuration-aware eval system that would have caught both regressions before deployment was estimated at $85,000 in engineering investment.

## Why Default-Configuration Testing Persists

If the configuration blindspot is so expensive, why does nearly every multi-tenant platform start by testing only the default configuration? Three forces conspire to make it the path of least resistance.

The first force is organizational. The eval team and the customer configuration team are usually different groups. The eval team builds suites based on the platform's reference architecture. The customer success team manages per-tenant configurations. Neither team fully understands the other's domain. The eval team does not know what configurations exist in production. The customer success team does not know what the eval suite tests. This organizational gap means nobody is responsible for ensuring alignment between the two, and nobody notices when they diverge.

The second force is technical complexity. Testing one configuration requires one set of test cases, one model setup, one retrieval pipeline, and one scoring rubric. Testing 200 configurations requires either 200 separate eval runs or a configuration-aware eval framework that can dynamically load and execute against arbitrary configurations. The first option is prohibitively expensive in compute and wall-clock time. The second option requires eval infrastructure that most platforms have not built. The path of least resistance is to test the default and hope it is representative. It never is.

The third force is metric design. Platform-wide metrics are easy to track, easy to report, and easy to set targets against. Per-customer metrics are messy, hard to aggregate, and difficult to present to leadership. The reporting infrastructure typically supports one dashboard with one set of numbers. Adding per-customer dimensions requires a different reporting architecture — one that can show both the platform view and the customer view, and surface cases where those views diverge. Most platforms defer this investment until after a painful customer-reported incident forces the issue.

## The Three Signals That You Have This Problem

You do not need a sophisticated detection system to identify the configuration blindspot. Three signals are visible from existing data if you know where to look.

The first signal is the support ticket correlation test. Pull your quality-related support tickets from the last 90 days. For each ticket, check whether the reporting customer's production configuration matches the configuration tested in your eval suite. If more than 30 percent of quality complaints come from customers whose configurations are not represented in the eval suite, you have the blindspot. In the healthcare platform example, the number was 91 percent — nearly every quality complaint came from a configuration the eval suite had never tested.

The second signal is the configuration drift report. Query your configuration management system for the number of customers running the exact default configuration with zero modifications. If that number is below 50 percent of your customer base, your eval suite represents less than half your customers. In practice, platforms with self-serve configuration options see this number drop below 20 percent within 12 months of enabling customization. A B2B content generation platform discovered that only 14 out of 210 customers — 6.7 percent — were still running the unmodified default configuration. Their eval suite tested the experience of 6.7 percent of customers and told them nothing about the other 93.3 percent.

The third signal is the eval-to-production config diff. Pick ten customers at random. For each customer, compare their production configuration — the prompt template, model, retrieval settings, adapter weights, safety filters, and output formatting they actually use — against the configuration used in your eval suite. Count the differences. If the average customer differs from the eval configuration on more than two dimensions, you are testing a meaningfully different product than what they use. If the average customer differs on more than four dimensions, the eval results are essentially decorrelted from their production experience.

## Closing the Gap Starts With Visibility

The first step toward configuration-aware evaluation is not building a new eval framework. It is building visibility into what configurations exist in production and how they differ from what you test.

This means instrumenting your configuration management system to produce a live inventory of every tenant's active configuration. Not just the settings they have changed, but their full resolved configuration — the combination of defaults they inherited and overrides they applied. This inventory becomes the input to your eval planning process: instead of deciding what to test based on what the eval team built, you decide what to test based on what customers actually run.

The inventory also enables a coverage metric: what percentage of your customer-weighted traffic is represented by your eval suite's configuration set? If your eval suite tests configurations that cover 95 percent of customer traffic, you have reasonable coverage. If it covers 40 percent, you have a blindspot that is likely producing false confidence. This coverage metric — which we will call the **Configuration Coverage Score** — should sit alongside accuracy and latency on your eval dashboard. It tells you not how well your platform performs, but how much you actually know about how well it performs.

The coverage metric also drives prioritization. You do not need to test every configuration from day one. You need to test the configurations that matter most — the ones used by your highest-revenue customers, your most regulation-sensitive customers, and the configurations that differ most dramatically from the default. Start there. Expand coverage as your configuration-aware eval infrastructure matures. But start with visibility, because until you know what your customers run, you cannot know whether your eval suite measures anything that matters.

The next subchapter covers how to formally map the tenant configuration space — every dimension where a customer can diverge from the default, how those dimensions interact, and how to build a registry that makes the configuration space tractable for evaluation.
