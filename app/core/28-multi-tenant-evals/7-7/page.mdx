# 28.67 — When Customers Disagree With Your Quality Scores: Dispute Resolution and Recalibration

Most platforms treat a customer who says "your quality score is wrong" the same way they treat a customer who says "my invoice is wrong" — as a support ticket to be resolved, filed, and forgotten. This is a mistake that costs more than the dispute itself. A billing error is transactional. A quality score dispute is existential. It challenges the foundation of the entire customer relationship: the shared belief that the eval system measures what matters. When that belief breaks, the customer stops trusting the scores, stops trusting the reports, and starts looking for a platform they can trust. The team that treats quality disputes as support tickets is the team that discovers, three months later, that the customer already signed with a competitor.

A quality score dispute is not a complaint. It is the single most valuable signal in multi-tenant evaluation. The customer is telling you, in specific terms, where your measurement diverges from their reality. No amount of internal rubric review, no calibration exercise, no red-team audit produces information this precise. The customer is doing the work of identifying the exact gap between what you measure and what they need measured. The only question is whether your platform has a process to capture that signal and act on it, or whether it lets the signal decay into frustration and churn.

## Why Disputes Happen

Quality score disputes are not random. They cluster around five predictable causes, and diagnosing the right cause determines the resolution path.

**Rubric misalignment** is the most common cause. The customer's definition of quality has drifted from the rubric that was established during onboarding. A legal technology customer onboarded twelve months ago with a rubric that weighted accuracy at 40 percent, completeness at 30 percent, and tone at 30 percent. Over the past year, their compliance team gained influence over the AI integration, and their practical definition of quality shifted toward regulatory citation accuracy — a dimension the rubric does not measure at all. Every output the eval system scores as high-quality may be missing the regulatory citations that the compliance team now considers non-negotiable. The rubric measures what the customer wanted a year ago. The dispute reflects what the customer needs now.

**Domain-specific edge cases** trigger disputes when the eval system handles a scenario incorrectly because it lacks domain knowledge. A pharmaceutical customer whose clinical trial summarizer encounters a rare adverse event category may see the eval score the output highly because it is syntactically correct, well-structured, and factually consistent with the input — while the customer's medical affairs team flags it as dangerously inadequate because the output failed to include mandatory safety language for that specific adverse event class. The eval system is not wrong in general. It is wrong for this specific scenario that requires domain expertise the rubric was never designed to capture.

**Changing customer requirements** cause disputes that feel unfair to both sides. The customer changed their requirements but did not update the quality contract. The platform is scoring against the agreed rubric. The customer is experiencing quality against their new standard. Both are right. Neither is satisfied. This is the most common source of recurring disputes with the same customer.

**Genuine eval errors** account for a smaller but critical fraction of disputes. The judge model hallucinated a score. The scoring pipeline dropped a dimension. A golden set entry was corrupted during the last update. These are bugs, and they erode trust faster than any other cause because they call the entire system's reliability into question. A customer who discovers one genuine eval error will question every score they have ever received.

**Different definitions of correctness** generate the most philosophically interesting disputes and the hardest to resolve. A customer in the creative writing vertical disputes a factual accuracy score because they define "accurate" as "consistent with the established world-building" rather than "consistent with real-world facts." A marketing customer disputes a tone score because their brand voice guide defines "professional" differently from the eval system's default interpretation. These disputes are not about errors — they are about the fundamental question of what "correct" means, and the answer is different for every customer.

## The Dispute Resolution Process

Effective dispute resolution follows a five-step process that prioritizes understanding over defensiveness. The goal is not to prove the eval system right. The goal is to align the eval system with the customer's reality, because the customer's reality is the only reality that matters for their quality measurement.

**Step one: acknowledge within four hours.** The customer must know their dispute is being taken seriously before they have time to escalate. The acknowledgment names the specific scores or outputs in question, confirms that the platform team will investigate, and provides a timeline for resolution — typically five to ten business days for standard disputes, 48 hours for tier-1 customers or disputes involving contractual quality thresholds.

**Step two: reproduce the disputed score.** Before investigating whether the score is wrong, confirm that the score is what the system actually produced. Run the disputed output through the eval pipeline again with the customer's current configuration. If the re-run produces a different score, you have a reproducibility issue that is a separate and more urgent problem. If the re-run confirms the original score, the investigation moves to whether the scoring criteria are correct, not whether the system executed correctly.

**Step three: investigate the root cause.** This step requires the eval team to look at the disputed output through the customer's eyes, not the rubric's eyes. What did the customer expect? What did the output deliver? Where is the gap? The investigation may involve reviewing the customer's quality contract, examining recent feedback from the customer, checking whether the rubric covers the dimension the customer is concerned about, and consulting with the customer's technical contact to understand their specific quality criteria. The output of this step is a root cause classification — rubric misalignment, domain edge case, requirement change, genuine error, or definition mismatch.

**Step four: explain the finding to the customer.** Transparency here is non-negotiable. If the score was wrong, say so. If the rubric was incomplete, say so. If the customer's expectations have evolved past the quality contract, say so — respectfully, but clearly. The explanation should include what the eval system measured, why it produced the score it did, and what the gap is between the measurement and the customer's expectation. Never hide behind "the system is working as designed" when "the system's design does not capture what you need" is the honest answer.

**Step five: resolve and implement the fix.** The resolution depends on the root cause, and there are three possible outcomes.

## The Three Possible Outcomes

Every quality score dispute resolves into one of three outcomes, and naming them explicitly prevents the resolution process from becoming an open-ended negotiation.

**Outcome one: the eval was wrong.** The scoring pipeline had a bug, the judge model made an error, or the rubric contained a flaw that produced an incorrect score. The fix is technical — patch the pipeline, correct the golden set entry, adjust the judge configuration. The customer receives a corrected score, an explanation of what went wrong, and a description of what was changed to prevent recurrence. This outcome is the easiest to resolve and the most damaging to trust if handled slowly. When the eval is wrong, fix it fast and own the error completely.

**Outcome two: the customer's expectation was misaligned.** The eval system scored correctly against the agreed quality contract, but the customer expected something different — either because their requirements changed without updating the contract or because they misunderstood what the quality dimensions measure. The resolution is a recalibration conversation, not a score change. The platform team walks through the quality contract with the customer, identifies where their current expectations diverge from the documented criteria, and proposes updates to the rubric, golden set, or dimension weights that bring the measurement in line with current needs. This outcome requires diplomatic skill — the customer must feel heard and respected even when the conclusion is "the score was correct given what we agreed to measure."

**Outcome three: both were partially right.** The eval system measured correctly against the rubric, but the rubric was insufficiently detailed to capture the nuance the customer cares about. The output was genuinely good by one standard and genuinely inadequate by another. This is the most common outcome and the most productive. It surfaces the specific gap in the evaluation framework and creates a clear action item: refine the rubric, expand the golden set, or add a quality dimension to capture what the current measurement misses. The customer sees that their feedback produced a concrete improvement. The eval system becomes more accurate for their specific use case. Both sides win.

## Investigating Without Undermining Authority

The investigation phase walks a razor edge. Investigate too aggressively and the eval team starts doubting every score, treating customer complaints as evidence that the system is broken. Investigate too defensively and the customer feels dismissed, concluding that the platform values its metrics more than their experience.

The balance is a clear separation between questioning the criteria and questioning the execution. When investigating a dispute, the eval team should never question whether the scoring pipeline executed correctly — that is a reliability issue that should be caught by automated validation, not customer complaints. Instead, the investigation should focus on whether the criteria themselves capture what the customer needs. This framing treats the eval system's execution as trustworthy while acknowledging that its configuration may be incomplete. The system works. The question is whether it measures the right things.

This distinction protects the eval system's authority while remaining genuinely open to customer feedback. A customer who hears "the system scored your output correctly, but we may be measuring the wrong dimensions for your use case" feels heard. A customer who hears "the system scored your output correctly, so we disagree with your assessment" feels dismissed. Same finding. Different framing. Radically different customer outcome.

## Escalation Paths

Some disputes cannot be resolved at the technical level. When a dispute involves a contractual quality threshold — the customer's SLA specifies 90 percent accuracy and their eval scores show 87 percent — the dispute becomes a contractual issue regardless of the root cause. The customer does not care whether the rubric was misaligned or the judge model had a bad day. They care that the number in their quality report is below the number in their contract.

The escalation path for contractual disputes routes through three stages. The first stage is technical review — the eval team investigates whether the score is accurate. The second stage is commercial review — the account management team assesses whether the SLA breach triggers financial remedies and what the business impact of the breach is. The third stage is executive resolution — when the technical and commercial teams cannot resolve the dispute, senior leadership from both the platform and the customer align on a path forward, which may include temporary score adjustments, accelerated rubric recalibration, or contract amendments.

The escalation path must have defined timelines at each stage. A dispute that sits in technical review for three weeks while the customer's quality report shows an SLA breach is a dispute that has already been lost, regardless of the investigation's outcome. For tier-1 customers, the total resolution timeline from initial dispute to final resolution should not exceed fifteen business days. For contractual disputes involving financial remedies, the timeline should not exceed ten.

## Recalibration Workshops

Recurring disputes with the same customer signal that the quality contract itself needs revision, not just individual scores. The **recalibration workshop** is a structured session — typically two to four hours — where the platform's eval team and the customer's quality stakeholders sit together and walk through the entire evaluation framework for that customer.

The workshop covers four areas. First, reviewing every quality dimension in the current rubric: does the customer still agree that these are the right dimensions, weighted correctly? Second, examining the golden set: does it still represent the customer's current input distribution and quality expectations? Third, testing the judge configuration: running ten to fifteen disputed or borderline examples through the eval pipeline and discussing each score with the customer to identify where the judge's reasoning diverges from the customer's judgment. Fourth, documenting changes: every rubric update, golden set revision, and dimension weight adjustment agreed upon in the workshop is documented, version-controlled, and deployed within one week of the session.

Recalibration workshops are expensive — four hours of senior time from both sides — but they are the most effective mechanism for preventing future disputes. A platform that conducts recalibration workshops with its top fifty customers once per year typically sees dispute rates drop by 40 to 60 percent in the quarter following the workshop. The reduction is not because the customers stopped having quality concerns. It is because the eval system now measures what they actually care about.

## Disputes as Training Data

Resolved disputes are the highest-quality input for improving per-customer evaluation, and discarding them after resolution wastes their full value.

Every resolved dispute contains a specific example where the eval system's measurement diverged from the customer's reality, the root cause of the divergence, and the correction that was applied. These resolved disputes should be systematically fed into three improvement pipelines. First, the customer's golden set — adding the disputed example with the customer-approved expected outcome as a new evaluation entry. Second, the judge calibration pipeline — using the disputed example as a calibration case to test whether the judge model would score it correctly after the rubric update. Third, the platform-wide pattern detection system described in Chapter 6 — anonymized dispute patterns across multiple customers reveal systemic rubric gaps that affect the entire platform.

A platform that treats disputes as defects to be eliminated misses the point. Disputes are information. They are the mechanism through which the eval system learns what it does not yet know. The platform that resolves disputes fast, learns from them systematically, and shows customers the impact of their feedback is the platform that earns the kind of trust that no quality report alone can create. But turning customer-specific eval insights into platform-wide improvements introduces a different challenge entirely — one of trust, legal boundaries, and the discipline to improve without violating isolation. That challenge is the subject of the next subchapter.
