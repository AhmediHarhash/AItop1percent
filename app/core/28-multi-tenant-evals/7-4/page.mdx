# 28.64 — Designing Quality Reports for Different Customer Audiences: Technical, Executive, and Legal

The head of AI at a logistics company opens the platform quality report. Twelve pages of metrics, percentile breakdowns, per-document-type accuracy curves, judge agreement rates, and drift trend analysis. She understands every number. She forwards a summary to her CTO with a note: "We're in good shape." The CTO opens the same twelve-page report, skims the first two pages, sees numbers he does not have context for, and asks the head of AI to join a call to "walk me through what this means." The call takes 45 minutes. The following week, the company's legal counsel requests the quality report for an upcoming EU AI Act compliance review. The counsel opens the report, finds no mention of transparency obligations, no audit trail references, no compliance mapping, and sends an email to procurement: "This report does not meet our regulatory evidence requirements. Please request a compliant version from the vendor."

One report. Three audiences. Three failures. The technical team wanted the detail and got it. The executive wanted the story and got data. Legal wanted compliance evidence and got neither. This is the **audience mismatch problem**, and it afflicts every platform that treats the quality report as a single document rather than a system of views over the same underlying data.

## Why One Report Cannot Serve Three Audiences

The fundamental issue is that technical teams, executives, and legal counsel have incompatible definitions of a useful report.

**Technical teams** want granularity. They want to see accuracy broken down by document type, by time window, by model version. They want to know which judge evaluated their outputs and what the inter-judge agreement rate was. They want drift metrics, not just current-state snapshots. They want to be able to investigate anomalies — if a metric dipped in week three, they want to drill into that week's data. The technical team uses the quality report as a diagnostic tool. A report that summarizes away the detail is useless to them because they cannot act on summaries.

**Executive teams** want narrative. They want to know: are we getting value from this platform? Is quality stable or declining? Are there risks we need to worry about? Are our SLAs being met? Executives process quality information through business impact. A 2.1 percent decline in accuracy on insurance claims means nothing to a CTO. "Your claims processing accuracy declined this month, which our analysis projects would have affected approximately 340 claims if left unaddressed — the platform team identified the root cause and deployed a fix within the reporting period" means everything. The executive team uses the quality report as a trust signal. Too much detail erodes trust by making them feel like they are being shown problems rather than solutions.

**Legal teams** want defensibility. They want to know: does this report constitute sufficient evidence of compliance with our contractual obligations and regulatory requirements? Can we submit this report to a regulator as part of an EU AI Act transparency filing? Does it document the methodology, the data provenance, and the chain of custody for the quality measurements? Legal teams do not evaluate whether the numbers are good. They evaluate whether the documentation would survive scrutiny. A report that shows excellent quality scores but cannot demonstrate how those scores were computed is worthless for legal purposes.

## The Three-Tier Report Architecture

The solution is not three separate reports built independently. It is one data layer feeding three report tiers, each derived from the same underlying eval results but structured for its specific audience.

**Tier 1: The Technical Detail Report** contains everything the customer's AI team needs. Accuracy by document type and by week. Latency percentiles by request category. Error breakdowns by error type. Judge agreement rates. Drift trend analysis with month-over-month comparisons. Model version information if the customer has opted into technical transparency. This report is typically 8 to 15 pages and is delivered to the customer's technical contact. It uses precise language: "Accuracy on clinical discharge summaries was 93.7 percent for the reporting period, computed as the mean score across 14,200 evaluated outputs using the customer-approved dual-judge rubric. Inter-judge agreement was 91.2 percent. Outputs where judges disagreed were resolved by majority voting with a third judge."

**Tier 2: The Executive Summary** distills the technical report into business language. It is one to three pages. It opens with an SLA compliance status — green, yellow, or red for each quality dimension. Green means all commitments met. Yellow means a metric approached but did not breach the threshold. Red means a breach occurred. Below the status indicators, the report provides a trend narrative: "Quality has been stable across all document types for the past three months. The platform team proactively addressed a minor accuracy fluctuation in week three that did not reach SLA thresholds." It closes with a forward-looking section: "No risks identified for the upcoming period. A planned model upgrade in February has been pre-tested against your workload with positive results." The executive summary never includes raw numbers without interpretation. Every metric is paired with its SLA threshold and its business meaning.

**Tier 3: The Legal and Compliance Report** is structured for regulatory submission and contractual evidence. It includes a methodology section documenting how quality is measured — the eval pipeline architecture, the judge models used, the rubric design process, the sampling methodology if applicable. It includes a data provenance section documenting where the evaluated data came from, how it was selected, and the chain of custody from production to evaluation. It includes a compliance mapping section that connects each quality metric to the specific contractual clause or regulatory requirement it satisfies. For customers operating under the EU AI Act, this section maps quality measurements to Article 50 transparency obligations and the GPAI Code of Practice requirements, demonstrating that the platform's quality assurance process meets the regulatory standard for AI system transparency. It includes a reproducibility statement confirming that the reported results can be independently verified by re-running the eval pipeline against the same data. This report can run to 20 pages or more for heavily regulated customers, but most of its content is boilerplate methodology and compliance mapping that changes infrequently. The dynamic section — the actual quality results — is compact.

## What Confuses Each Audience

Understanding what confuses each audience is as important as understanding what they need.

Technical teams are confused by **inconsistent methodology**. If this month's report uses a different judge model than last month's, the technical team cannot compare the results. Every methodology change — judge model update, rubric revision, sampling strategy adjustment — must be flagged explicitly in the technical report with an explanation of the impact on score comparability. Without this, the technical team's trust in the trend data collapses.

Executives are confused by **detail without interpretation**. A chart showing weekly accuracy scores for six document types over three months contains 72 data points. An executive looking at that chart does not see a story. They see noise. The executive summary must synthesize those 72 data points into a narrative: "Quality has been consistent across all categories except claims processing, where a brief dip in week eight was caused by a known issue that has been resolved." The narrative is derived from the data but is not the data itself.

Legal teams are confused by **quality claims without methodological backing**. A report that states "accuracy was 93 percent" without explaining how accuracy is defined, how it was measured, who designed the rubric, and whether the measurement process is auditable, is legally useless. It cannot be submitted as evidence in a regulatory review because there is no way for a reviewer to assess whether "93 percent" means what the vendor claims it means. Every quality claim in the legal report must be traceable to a documented methodology.

## The Technical Precision Trap

A counterintuitive risk: showing too much technical detail to executives makes them less confident in your platform, not more. When an executive sees a detailed breakdown of every error type, every quality fluctuation, and every anomaly the eval pipeline detected, their natural response is concern. "Are all these issues real problems?" "Should I be worried about the 2.1 percent decline in category four?" "Why did the judge agreement rate drop below 90 percent in week seven?"

The issues may be minor. The fluctuations may be within normal variance. The agreement rate drop may have been a one-week artifact. But the executive does not have the context to assess severity. They see a long list of things that look like problems, and they conclude that the platform has problems. The technical team, reading the same data, sees a healthy system with normal operational variance. The difference is not in the data. It is in the interpretive context that each audience brings.

The solution is not to hide data from executives. It is to present only the data that changes the executive's decision. If no metric breached an SLA, if no trend line is concerning, and if no intervention was required, the executive summary should communicate exactly that — stability and compliance. If a metric did breach, or a trend does warrant attention, the executive summary should present it with severity, context, and resolution. The goal is signal, not noise.

## Templating at Scale With Per-Customer Customization

Generating three report tiers for 300 customers means producing 900 report variants per month. This is only feasible with a templating system that separates structure from data.

The **template layer** defines the layout, language, and structure for each report tier. Technical report templates include sections for each quality dimension, with placeholders for the customer's specific metrics, thresholds, and trends. Executive summary templates include the traffic-light SLA status indicators and narrative sections with fill-in-the-blank structures. Legal report templates include the methodology boilerplate and compliance mapping sections that are identical across most customers, with tenant-specific sections for custom contractual clauses.

The **data layer** populates the templates with each customer's actual eval results. The same eval data feeds all three templates for a given customer. The technical report gets the full granularity. The executive summary gets aggregated versions. The legal report gets the auditable subset with methodology annotations.

The **customization layer** handles per-customer overrides. One customer wants their technical report to include adapter performance details. Another customer's legal team has specified a particular format for compliance evidence that aligns with their internal audit process. A third customer wants the executive summary to include a competitive benchmark section comparing their quality metrics to their industry's typical range. These customizations are stored as per-tenant report configuration objects that the templating engine applies during generation. The key design principle is that customizations modify the template, not the data pipeline. The data pipeline produces the same structured output for every customer. The template decides what to show, how to show it, and what to emphasize.

## The Report Nobody Reads

The most dangerous report is the one that arrives in an inbox and is never opened. Platforms invest heavily in report content and barely think about report delivery. A beautifully designed quality report that arrives as a 15-page PDF attachment on the fifth business day of the month will be read by exactly one person — the customer's designated technical contact — and ignored by everyone else.

Timing matters. Deliver the executive summary the day after month-end, when the executive team is reviewing operational metrics. Deliver the technical report three days later, when the technical team has had time to surface any questions from their own internal data. Deliver the legal report on a quarterly cadence aligned with the customer's audit calendar, not your reporting calendar.

Format matters. Executives read email summaries and Slack messages. They do not open PDF attachments unless something is wrong. The executive summary should arrive as inline content in the delivery email or as a notification with the key metrics visible without opening the attachment. Technical teams prefer structured data they can import into their own dashboards. Offer the technical report as both a PDF and a machine-readable format — a JSON endpoint or a CSV export — so the customer's data team can ingest your quality metrics into their own monitoring tools. Legal teams need documents they can file. PDF with digital signatures and tamper-evident metadata serves this need.

Channel matters. Some customers want reports pushed to a shared Slack channel. Others want them uploaded to a secure document portal with access logging. Others want them delivered via API to their internal reporting system. The delivery infrastructure should support multiple channels, with the channel preference stored in the per-tenant configuration alongside the report template preferences.

A report that arrives at the right time, in the right format, through the right channel, to the right person has a 90 percent read rate. The same report arriving as an email attachment to a generic distribution list has a 20 percent read rate. The difference is not the content. It is the delivery design.

## Closing the Loop Between Report and Action

The quality report is not the end of the conversation. It is the beginning. The report tells the customer what happened. What happens next — the customer's feedback, their questions, their corrections — is the signal that makes the eval system smarter over time. The next subchapter examines how to build the feedback loop that lets customers tell you when your quality scores are wrong and how to incorporate their corrections without destabilizing the eval system that serves everyone else.
