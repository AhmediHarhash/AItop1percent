# 28.61 — The Eval Chargeback Model: Attributing Eval Costs to the Customers They Serve

If you cannot tell your CFO exactly how much your evaluation system costs to operate for each customer, you are subsidizing your least valuable tenants with the budget meant for your most valuable ones. Every eval run consumes compute, judge API calls, storage, and engineering time. In a single-product company, those costs vanish into a shared infrastructure line item. In a multi-tenant platform serving hundreds of enterprise customers, that invisibility is a strategic liability. The **eval chargeback model** is the discipline of mapping every eval expense to the tenant that generated it — not to spread blame, but to enable unit economics visibility, tier-appropriate investment, and defensible budget allocation.

Without per-tenant cost attribution, you cannot answer the questions that determine your platform's financial health: which customers cost more to evaluate than they pay you, which tier of service justifies which level of eval investment, and whether your pricing reflects the actual cost of quality assurance. Platforms that skip this step inevitably discover, usually during an annual planning review, that their eval infrastructure budget has silently grown 40 percent while nobody can explain which customers drove the increase.

## The Five Cost Categories of Evaluation

Evaluation costs are not a single number. They decompose into five categories, each with different attribution characteristics.

**Judge API calls** are typically the largest variable cost. Every time your eval pipeline sends an output to GPT-5, Claude Opus 4.6, or Gemini 3 for quality judgment, you pay per token. A customer whose outputs are long-form clinical summaries averaging 2,000 tokens per document costs roughly four times more per judge call than a customer whose outputs are short product descriptions averaging 500 tokens. Judge costs scale linearly with eval volume and non-linearly with output complexity. For a platform running 2 million eval judgments per month across 300 tenants, judge API costs often reach $15,000 to $40,000 monthly — and the distribution across tenants is wildly uneven.

**Compute for eval orchestration** covers the CPU and memory consumed by the eval pipeline itself — routing inputs to judges, collecting results, computing aggregate metrics, running drift detection, and generating reports. This cost is lower than judge calls but harder to attribute because the orchestration infrastructure is shared. A Kubernetes cluster running eval workers processes jobs from all tenants on the same nodes.

**Storage** includes eval results, historical scores, golden set maintenance, audit logs, and evidence packages. Storage costs are small per record but accumulate. A tenant with a 5,000-example golden set that runs monthly full-set evaluation and retains three years of history for regulatory compliance generates substantially more storage than a tenant with a 200-example golden set and six months of retention.

**Human review hours** are the most expensive per-unit cost. When eval pipelines flag outputs for human review — disagreements between judges, low-confidence scores, regulatory-sensitive content — the cost is measured in analyst hours at $35 to $120 per hour depending on domain expertise. A healthcare customer requiring physician review of flagged clinical outputs costs dramatically more per review cycle than a retail customer whose flagged outputs can be reviewed by a general-purpose analyst.

**Engineering time** is the hidden category. Custom eval configurations, per-tenant judge prompt development, golden set curation support, and quality report customization all consume engineering hours. This cost is real but often untracked, absorbed into the platform team's general capacity rather than attributed to the tenant that requested the work.

## Attribution Methods: Direct, Proportional, and Hybrid

Three methods exist for mapping eval costs to tenants, and the right choice depends on the cost category.

**Direct attribution** tags every cost event with a tenant identifier at the point of origin. When the eval pipeline sends a judge API call, it includes tenant_id in the request metadata. When the result is stored, it is written to a tenant-scoped partition. When a human reviewer spends 45 minutes on a flagged output, the review ticket carries the tenant identifier. Direct attribution is the most accurate method and the only defensible one for variable costs like judge calls and human review. The infrastructure requirement is that your entire eval pipeline propagates tenant context through every operation — from the initial eval trigger through judge execution, result storage, and reporting. OpenTelemetry's baggage propagation, where tenant_id travels as a context attribute through every span in the trace, is the standard implementation pattern in 2026. Every downstream service inherits the tenant context automatically, and your cost rollup queries simply aggregate by the tenant_id dimension.

**Proportional allocation** divides shared infrastructure costs by a usage-based ratio. If tenant A generates 12 percent of total eval jobs and tenant B generates 3 percent, they are allocated 12 percent and 3 percent respectively of the shared compute costs. Proportional allocation is appropriate for shared infrastructure that cannot be directly tagged — the Kubernetes control plane overhead, the shared message queue, the monitoring stack. It is a reasonable approximation but should never be used for costs that can be directly attributed. Using proportional allocation for judge API calls when you could directly tag them is choosing convenience over accuracy, and the inaccuracy compounds as your tenant base grows.

**Hybrid attribution** combines direct and proportional methods. Judge calls, storage, and human review are directly attributed. Shared compute and engineering overhead are proportionally allocated. This is the standard approach for mature multi-tenant platforms. The direct portion typically accounts for 70 to 80 percent of total eval cost, giving you accurate per-tenant visibility where it matters most.

## Cost Tracking vs Cost Billing

A critical distinction that many platform teams conflate: attributing costs to tenants is not the same as billing tenants for those costs. Cost tracking is an internal capability — it tells your finance team and your platform team where eval dollars go. Cost billing is an external decision — it determines whether and how customers pay for eval consumption.

Most multi-tenant platforms in 2026 do not bill eval costs directly to customers. Eval is bundled into the platform subscription or the per-query pricing. But cost tracking is still essential because it feeds three internal decisions. First, it informs pricing — if you know that healthcare tenants cost 3.2 times more to evaluate than retail tenants, you can price healthcare tiers accordingly, as Section 30 on pricing and unit economics explores in depth. Second, it justifies budget — when the eval team requests a 25 percent budget increase, cost attribution data shows exactly which customer segments drove the increase. Third, it reveals subsidy patterns — cost tracking exposes when your self-service tier is being subsidized by your enterprise tier's margins, or when a single whale customer's eval requirements are consuming 15 percent of your total eval budget while paying 4 percent of total revenue.

## Infrastructure for Per-Tenant Cost Rollups

The technical foundation for eval cost attribution requires four components.

The **tenant context propagation layer** ensures that every eval operation carries a tenant identifier from trigger to completion. In a well-instrumented pipeline, the eval job is created with a tenant_id field, the judge API call includes tenant_id in its metadata, the result record includes tenant_id in its schema, and the OpenTelemetry trace carries tenant_id as a resource attribute. If any operation in the chain drops the tenant context, cost attribution for that operation falls back to proportional allocation — a gap that grows more expensive to fix the longer it persists.

The **cost event stream** captures every billable event with its tenant association and unit cost. Judge API calls are logged with input token count, output token count, model used, and per-token price. Compute consumption is logged with CPU-seconds, memory-seconds, and the per-unit cost of the underlying infrastructure. Human review sessions are logged with duration, reviewer tier, and hourly rate. This event stream feeds both real-time dashboards and monthly cost rollup jobs.

The **per-tenant cost aggregation service** computes daily, weekly, and monthly cost totals per tenant, broken down by cost category. It joins the cost event stream with tenant metadata — contract tier, pricing plan, account manager — to produce the reports that finance, sales, and platform engineering each need. The aggregation runs as a batch job, typically daily, with an end-of-month reconciliation that handles any late-arriving events or retroactive cost adjustments.

The **cost anomaly detection layer** monitors per-tenant eval costs for unexpected spikes. If tenant A's eval costs suddenly triple in a week, the anomaly detector alerts the platform team before the monthly report reveals the surprise. The spike might be legitimate — the tenant onboarded a new document type and eval volume increased accordingly — or it might indicate a pipeline bug generating duplicate eval jobs. Either way, early detection prevents budget surprises and enables proactive customer communication.

## Why Invisible Eval Costs Destroy Platform Economics

Platforms that cannot attribute eval costs per customer make predictable mistakes. They offer the same eval depth to every tenant regardless of contract value, because they have no data to differentiate. They approve eval pipeline upgrades — more sophisticated judges, more frequent drift detection, richer evidence packages — without understanding which customers benefit and at what cost. They set pricing based on compute and API margins while ignoring eval overhead, which for complex enterprise customers can reach 15 to 25 percent of the total cost of serving that customer.

The most dangerous pattern is **eval cost regression to the mean** — over time, the platform gravitates toward a one-size-fits-all eval configuration because differentiating by tenant seems like too much operational complexity. Every tenant gets the same judge model, the same eval frequency, the same report depth. This means low-value self-service tenants receive eval investment that their contract value does not justify, while high-value enterprise tenants receive less eval investment than their contract value demands. The platform over-spends on the bottom of the customer base and under-spends on the top, exactly inverting the economically rational allocation.

The eval chargeback model reverses this. When you can see that your top 20 customers generate 60 percent of eval costs and 70 percent of revenue, the investment math is clear. When you can see that your bottom 100 customers generate 8 percent of eval costs but only 3 percent of revenue, the subsidy is visible and can be addressed through pricing adjustments or eval depth tiering, as Section 25 on cost-quality tradeoffs details.

Cost attribution tells you where the money goes. But knowing where it goes raises an uncomfortable question: what happens when a customer's eval costs exceed what they pay you? The next subchapter examines the unit economics problem — when evaluation itself makes a customer unprofitable.
