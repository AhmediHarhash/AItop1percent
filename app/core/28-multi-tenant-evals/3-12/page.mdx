# 28.12 — Cache Poisoning and Shared State Attacks in Eval Infrastructure

**The Shared Cache Shortcut** is the most common and most dangerous performance optimization in multi-tenant evaluation infrastructure. The logic is seductive: if tenant A's eval pipeline scores an output against a rubric dimension, and tenant B's eval pipeline needs to score a similar output against the same dimension, why not cache the result and reuse it? The judge model is expensive. Inference is slow. Caching saves money and reduces latency. Every engineering instinct says to cache aggressively. And that instinct, applied without tenant isolation, creates a security vulnerability that can corrupt eval results, expose proprietary scoring patterns, and destroy the integrity of your entire evaluation system.

This subchapter covers the attack vectors specific to multi-tenant eval infrastructure — cache poisoning, shared state contamination, and model response reuse across tenant boundaries — and the architectural patterns that prevent each one.

## Cache Poisoning: How It Happens

Cache poisoning in evaluation infrastructure occurs when one tenant's cached eval result is served to another tenant. The mechanism is straightforward. The eval pipeline caches judge model responses to reduce redundant inference calls. The cache key is typically derived from the input — the output being scored, the rubric dimension, and the scoring prompt. If two tenants submit similar outputs for evaluation against similar rubric dimensions, the cache may return the cached response from tenant A's evaluation when processing tenant B's evaluation.

This is not a theoretical vulnerability. Research published at NDSS in 2025 demonstrated that semantic caching in LLM-serving infrastructure — caching based on semantic similarity rather than exact string matching — creates cross-tenant information leakage in multi-tenant environments. The attack surface in eval infrastructure is analogous: the cache does not check whether the cached response belongs to the requesting tenant, because the cache was designed as a performance optimization, not a security boundary.

The consequences are severe in three dimensions. First, the accuracy dimension: tenant B receives a score that was computed for tenant A's output. If the outputs are similar but not identical, the score may be approximately correct, which makes the poisoning harder to detect. Tenant B's quality dashboard shows a plausible but incorrect score. Decisions are made based on wrong data. Second, the confidentiality dimension: the cached response may contain information about tenant A's evaluation criteria, scoring patterns, or proprietary rubric language that should never be visible to tenant B. Third, the integrity dimension: if tenant B's eval results include cached responses from tenant A, the entire eval provenance chain for tenant B is compromised. The config hash says the evaluation used tenant B's configuration, but the actual scoring used tenant A's cached result. The provenance is a lie.

## Semantic Cache Attacks

The risk intensifies with semantic caching, which is increasingly common in 2026 as platforms optimize for cost. Exact-match caching only serves cached results when the input is byte-identical. Semantic caching serves cached results when the input is semantically similar — when a vector embedding of the input falls within a threshold distance of a cached entry's embedding. Semantic caching provides higher cache hit rates, which reduces judge inference costs by 30 to 60 percent in typical deployments. It also massively expands the attack surface.

A 2026 study on semantic cache key collisions demonstrated that semantically different inputs can unexpectedly map to the same cache key when the embedding model groups them as similar. In an eval context, this means that tenant A's output about medication dosing and tenant B's output about dietary supplements — two semantically related but factually different topics — could collide in a semantic cache. Tenant B receives a judge score that was computed for tenant A's medication dosing output. The score is wrong. If tenant B is a healthcare company whose eval results feed into compliance reporting, the wrong score can cascade into a compliance failure.

The fundamental problem with semantic caching in multi-tenant eval is that similarity is not identity. Two outputs may be semantically similar and still require completely different evaluation treatment — different rubric dimensions, different scoring criteria, different domain-specific expectations. The cache treats them as equivalent. The evaluation system should not.

## Shared State Contamination

Cache poisoning is the most visible shared state attack, but it is not the only one. Multi-tenant eval infrastructure contains several shared state vectors that can contaminate results across tenant boundaries.

**Judge model context state** is a subtle contamination vector. When a judge model processes a batch of eval requests, the model's internal state — particularly for models that maintain conversation-level context or use key-value cache internally — can carry residual information from one request to the next. In a multi-tenant environment where eval requests from different tenants are batched together for efficiency, the residual state from tenant A's evaluation can influence the scoring of tenant B's evaluation. Research from 2025 demonstrated that KV-cache sharing in multi-tenant LLM serving creates cross-tenant side channels that allow information extraction through carefully timed requests. In eval infrastructure, this manifests as subtle scoring drift rather than direct information extraction — but the effect on score integrity is real.

**Pipeline state variables** are another contamination vector. An eval pipeline that tracks running statistics — average scores, dimension-level distributions, anomaly detection thresholds — across tenants creates a shared state that tenant-specific operations can influence. If the anomaly detector uses a global baseline that includes all tenants' scores, a large tenant with unusual score patterns can shift the baseline and cause false anomaly detection (or missed anomalies) for other tenants. The anomaly detector's internal state is contaminated by cross-tenant data.

**Temporary file systems and scratch space** create contamination when eval jobs write intermediate results to shared file systems. Tenant A's eval job writes scoring artifacts to a temporary directory. Tenant B's eval job runs on the same worker node and reads from the same temporary directory. If the cleanup between jobs is incomplete — a common occurrence when jobs fail mid-execution — tenant B's pipeline may read tenant A's artifacts. The result is not just incorrect evaluation. It is a data breach: tenant B's pipeline has accessed tenant A's scoring data.

**Queue state** in message-based eval architectures creates contamination when a dead letter queue or retry queue processes messages from multiple tenants in a shared context. A failed eval event from tenant A is retried and lands in the same processing context as tenant B's events. The retry handler, which may reconstruct context from the queue rather than from tenant-specific storage, can cross-wire tenant contexts.

## The Shared Cache Shortcut in Detail

To understand why cache poisoning is so prevalent, you need to understand how eval caching is typically implemented and where the isolation failures occur.

In the early stages of a platform, the eval pipeline has no cache. Every eval request sends a fresh inference call to the judge model. As the platform grows, the judge inference cost becomes significant — at 500 tenants processing 10,000 eval events per day each, the platform is making 5 million judge inferences per day. At two cents per inference, that is $100,000 per day in judge costs. The engineering team adds a cache. The cache key is a hash of the input text, the rubric dimension, and the scoring prompt. The cache dramatically reduces costs — a 40 percent hit rate cuts daily judge costs by $40,000.

The cache was built as a performance optimization. Nobody asked "should tenant A's cached response be served to tenant B?" because the question seemed absurd — the cache is keyed on the input, and if two inputs are identical, the scoring should be identical. But this reasoning has two flaws. First, it ignores tenant-specific calibration. Tenant A's judge pipeline may have different calibration parameters than tenant B's. An identical input scored under different calibration produces different scores. The cache key does not include the calibration parameters, so the cached score from tenant A's calibration is served to tenant B's evaluation, which expects tenant B's calibration. Second, it ignores the confidentiality implication. The cached response contains the judge's reasoning — its explanation of why it assigned a particular score. That reasoning may reference tenant A's rubric language, scoring criteria, or domain context. Serving it to tenant B exposes tenant A's proprietary evaluation criteria.

## Prevention: Cache Partitioning by Tenant

The primary defense against cache poisoning is cache partitioning. Every tenant gets their own cache namespace. Tenant A's cached results are stored in a partition that tenant B's eval pipeline cannot access. The cache key includes the tenant identifier as a mandatory prefix. A cache lookup for tenant B never searches tenant A's partition. The partitions are enforced at the cache infrastructure level — separate Redis databases, separate cache key prefixes with access controls, or separate cache clusters entirely.

Cache partitioning reduces the cache hit rate because it eliminates cross-tenant cache reuse. A platform with 500 tenants that shares a single cache might achieve a 40 percent hit rate. The same platform with per-tenant cache partitions might achieve a 15 to 25 percent hit rate, because each tenant's cache is smaller and hits are limited to exact matches within that tenant's own history. The cost difference is real — perhaps $15,000 to $25,000 per day in additional judge inference costs. The cost of a cache poisoning incident — customer trust destruction, potential regulatory action, breach notification requirements — is orders of magnitude higher.

For platforms that cannot absorb the full cost increase of per-tenant caching, a hybrid approach uses per-tenant partitions for scored results while sharing a cross-tenant cache for intermediate computations that do not contain tenant-specific information. The embedding computation for semantic similarity, for example, can be cached across tenants because the embedding itself does not contain scoring information. The judge inference result, which contains the score and reasoning, must always be tenant-partitioned. This hybrid approach recovers some of the cost savings of shared caching while maintaining scoring isolation.

## Prevention: Stateless Eval Pipeline Design

The defense against shared state contamination is statelessness. Every eval job starts with a clean environment, accesses only the data it is authorized to access, maintains no state that persists between jobs, and writes its results exclusively to tenant-scoped storage.

Stateless eval pipelines use container-based execution where each eval job runs in a fresh container that is destroyed after the job completes. The container has no access to the previous job's file system, memory, or network connections. The container's environment variables, mounted volumes, and network policies are configured per-tenant, drawn from the tenant's isolation profile. A job for tenant A runs in a container with access to tenant A's golden set storage, tenant A's result storage, and tenant A's cache partition. A job for tenant B runs in a different container with access to tenant B's resources. The containers share the underlying node but not the runtime environment.

For the judge model context state issue, the mitigation is request isolation at the inference layer. Each eval scoring request to the judge model must be an independent, stateless API call that carries no context from previous requests. This means avoiding batching of cross-tenant eval requests into a single model context. If batching is necessary for throughput, the batches must be tenant-homogeneous — all requests in a batch belong to the same tenant. Tenant-heterogeneous batching, where requests from different tenants are interleaved in the same batch, creates the KV-cache side channel vulnerability identified in the 2025 NDSS research.

For pipeline state variables, the mitigation is per-tenant state scoping. The anomaly detector, the running statistics, the drift detector — all of these must operate on per-tenant data exclusively. The anomaly detector for tenant A uses tenant A's historical score distribution as its baseline, not a global distribution that includes all tenants. This is computationally more expensive because you maintain N state machines for N tenants rather than one global state machine. But a global state machine is a shared state attack surface. A per-tenant state machine is an isolation boundary.

## Prevention: Per-Tenant Encryption for Cached Data

Even with cache partitioning, a defense-in-depth strategy adds per-tenant encryption to cached data. Each tenant's cache partition is encrypted with a tenant-specific key. Even if a software bug or infrastructure misconfiguration allows cross-partition access, the data retrieved is encrypted with a key the unauthorized accessor does not possess. The result is garbage — unreadable ciphertext rather than a competitor's evaluation scores.

Per-tenant encryption keys are managed through the platform's key management system — AWS KMS, Azure Key Vault, or an equivalent. Each tenant has a dedicated encryption key that is used exclusively for their eval data, including cached results, stored eval events, and intermediate artifacts. Key rotation follows the platform's standard rotation schedule, typically every 90 days. When a key is rotated, cached data encrypted with the old key is invalidated and must be recomputed — which is the correct behavior, since cache data is ephemeral by design.

The encryption overhead is minimal for cache operations. Encrypting and decrypting a cached judge response adds sub-millisecond latency. The key lookup from the key management service is the primary overhead, and it can be mitigated with key caching at the application layer — caching the encryption key itself, which is a security-internal operation that does not create the cross-tenant exposure that caching eval results does.

## Testing for Shared State Vulnerabilities

Prevention is necessary but insufficient. You must also verify that your isolation mechanisms work through active testing, as described in the previous subchapter on testing isolation boundaries. For cache and shared state specifically, three test categories apply.

**Cache isolation tests** submit identical eval requests for two different tenants and verify that each receives a freshly computed result rather than a cached result from the other tenant. The test measures both correctness — the scores are independently computed — and timing — the second request takes the same time as the first, indicating it was not served from cache. A cached response is typically 10 to 100 times faster than a freshly computed response, so a timing difference is a strong signal of cache sharing.

**State contamination tests** run eval jobs for two tenants sequentially on the same worker node and verify that the second job's results are independent of the first job's state. The test compares the second job's results when run in isolation versus when run after the first job. Any statistical difference in scores suggests that residual state from the first job influenced the second job's evaluation.

**Encryption verification tests** attempt to read cached data from a tenant's partition using a different tenant's encryption key. The test should fail — the data should be unreadable. If the test succeeds, the encryption boundary is misconfigured. These tests run as part of the regular isolation verification suite, not just during initial deployment. Configuration drift, infrastructure updates, and cache system upgrades can all introduce regressions in isolation that were not present during the initial setup.

## The Organizational Temptation

The hardest part of preventing cache poisoning and shared state attacks is organizational, not technical. The engineering team knows that per-tenant caching is more expensive. They know that stateless pipelines are slower. They know that per-tenant encryption adds operational complexity. And they face constant pressure to reduce eval infrastructure costs, increase eval throughput, and simplify operations. The shared cache shortcut is a seductive answer to all three pressures — cheaper, faster, simpler. The security team says no. The engineering team says "but the cost savings." The leadership team asks "what is the actual risk?"

The actual risk is this: a single cache poisoning incident in which one tenant's eval scores are contaminated by another tenant's cached results will trigger a data breach investigation, a customer notification obligation under GDPR and the EU AI Act, and a loss of trust that no amount of post-incident communication can fully repair. The customer whose scores were contaminated will question every historical eval result. The customer whose cached data was exposed will question every isolation claim the platform has ever made. And every other customer who learns about the incident — which they will, because breach notifications are public — will question whether their own data is safe.

The cost comparison is not "per-tenant caching costs $15,000 more per day." It is "per-tenant caching costs $15,000 more per day, and the alternative is betting the platform's entire enterprise customer base on the assumption that a shared cache will never leak." For a platform with $40 million in annual revenue, the per-tenant caching premium is roughly $5.5 million per year. The first cache poisoning incident puts all $40 million at risk. The math is not complicated.

This subchapter concludes the chapter on tenant isolation in evaluation systems. The isolation requirements covered across these twelve subchapters — data isolation, compute isolation, reporting isolation, ground truth separation, privacy compliance, resource contention prevention, audit trails, contamination prevention in judge systems, isolation testing, metadata contracts, config provenance, evaluation locality, and cache security — form the foundation that everything else in multi-tenant evaluation depends on. Without isolation, per-tenant quality measurement is unreliable. Without reliable measurement, per-tenant quality improvement is impossible. Without quality improvement, the platform is selling a promise it cannot keep. The next chapter shifts from isolation to a different challenge: evaluating each tenant's actual configuration rather than platform defaults — the discipline of configuration-aware evaluation.
