# 28.7 — What Adjacent Sections Do Not Cover: The Unique Territory of Per-Customer Evaluation

Why does this section exist when the book already dedicates hundreds of pages to evaluation strategy, eval pipelines, scaling evaluation, infrastructure, governance, and pricing? The answer is that every one of those sections solves a different problem than the one you face when five hundred enterprise customers each define quality differently and expect proof on their own terms. The adjacent sections give you the machinery. This section gives you the customer-complexity layer that none of them address — the layer where a single platform must maintain, measure, and demonstrate distinct quality standards for every tenant without collapsing under the combinatorial weight of that diversity.

Understanding where the adjacent sections end and Section 28 begins is not an academic exercise. It is the difference between building evaluation systems that serve your platform and building evaluation systems that serve each customer. Teams that blur this boundary end up with one of two failure modes: they either over-engineer their generic eval infrastructure to accommodate per-customer concerns it was never designed to handle, or they under-invest in per-customer evaluation because they assume their existing eval stack already covers it. Both paths lead to lost contracts and silent churn. This subchapter maps the boundaries precisely so you can build in the right place.

## Section 3: Evaluation Strategy Is Not Tenant Strategy

Section 3 covers how to build an evaluation strategy for an AI system. It teaches you to define what quality means, select the right metrics, design rubrics, build golden sets, and establish the operating model that turns evaluation from an afterthought into a first-class engineering discipline. Everything in Section 3 assumes a single definition of quality. One rubric. One set of thresholds. One golden set. One quality bar that the entire product is measured against.

That assumption holds when you are building a single product for a general user base. It breaks the moment you sign enterprise customers with contractual quality requirements. A legal-tech platform serving a conservative insurance company and an aggressive litigation firm needs two different quality definitions for the same task — one that penalizes any speculative language and one that rewards creative legal arguments. Section 3 teaches you how to build one rubric well. Section 28 teaches you how to manage five hundred rubrics that all run on the same platform without creating five hundred independent evaluation systems.

The distinction is architectural, not cosmetic. Section 3's frameworks — the evaluation operating model, the rubric design process, the golden set methodology — are prerequisites for Section 28. You must know how to build one good eval before you can build hundreds. But the per-customer challenge introduces problems that Section 3 never encounters: rubric inheritance and override hierarchies, tenant-scoped judge calibration, cross-customer score comparability, and the organizational question of who owns the quality definition when the customer and the platform disagree.

## Section 15: Eval Pipeline Mechanics Are Not Tenant-Aware Pipelines

Section 15 covers the mechanics of automated evaluation pipelines. It teaches you how to build CI/CD-style eval systems that run automatically on every model update, every prompt change, every data pipeline modification. It covers pipeline orchestration, judge model integration, result storage, alerting, and the operational discipline of treating eval pipelines as production infrastructure.

Every pipeline in Section 15 evaluates against a single set of criteria. The pipeline ingests outputs, scores them against a rubric, compares results to a baseline, and fires alerts if quality degrades. This works perfectly when your eval criteria are uniform. But in a multi-tenant platform, the same model output might score 0.92 for Customer A and 0.71 for Customer B — not because the output changed, but because the customers weight different quality dimensions differently. Customer A cares primarily about factual accuracy and tolerates verbose responses. Customer B demands conciseness above all and will flag anything longer than three sentences as a quality failure.

Section 15 teaches you to build a pipeline that runs one evaluation per output. Section 28 teaches you to build a pipeline that runs tenant-scoped evaluations where the same output is assessed against different criteria depending on which customer it was generated for. That means tenant-aware routing in the eval pipeline, per-customer rubric resolution, tenant-scoped baselines, and per-customer alerting thresholds. The pipeline mechanics from Section 15 are the foundation. The tenant-awareness from Section 28 is the layer on top that makes those pipelines useful when your customers don't agree on what quality means.

## Section 26: Volume Scaling Is Not Complexity Scaling

This is the boundary that teams blur most often, and the confusion costs real money. Section 26 covers scaling evaluation systems to handle high volume. It addresses the question: how do you evaluate one million outputs per day without your eval infrastructure becoming your largest cost center? The answers involve sampling strategies, distributed eval compute, LLM-as-judge optimization, batch processing, and the economics of eval at scale.

Section 26 asks "how do we evaluate a million outputs a day?" Section 28 asks "how do we evaluate outputs for five hundred customers who each define good differently?" These are not the same question. A platform with one customer generating a million requests per day is a Section 26 problem. A platform with five hundred customers generating two thousand requests each per day is a Section 28 problem — even though the total volume is the same one million requests.

The difference is **customer-complexity scaling** versus volume scaling. Volume scaling is about throughput, sampling efficiency, and compute cost. Customer-complexity scaling is about maintaining distinct quality contexts for each tenant — distinct rubrics, distinct thresholds, distinct golden sets, distinct baselines, distinct alerting configurations, distinct reporting formats. Volume scaling is a compute problem with well-understood engineering solutions. Customer-complexity scaling is a combinatorial problem where every new customer multiplies the evaluation surface area. A platform with five hundred customers and ten quality dimensions per customer has five thousand distinct quality configurations to maintain, monitor, and report on. That complexity does not decrease when you solve the volume problem. It sits on top of the volume problem and compounds it.

Teams that mistake their challenge for a volume problem when it is actually a complexity problem end up building fast eval pipelines that evaluate everything against the same criteria. The pipeline processes a million outputs a day efficiently and reports a healthy aggregate score. Meanwhile, forty-seven customers are experiencing quality problems that the aggregate hides, because the aggregate averages across customer-specific concerns that diverge in ways the single rubric cannot capture. This is the Global Average Trap from subchapter 1.3, and it is a direct consequence of applying Section 26's volume solutions without Section 28's complexity solutions.

## Section 27: Infrastructure Supports Tenants but Does Not Evaluate Them

Section 27 covers global infrastructure and Kubernetes for AI workloads. It teaches you how to deploy AI systems across regions, manage GPU scheduling, handle model artifacts, and build the platform layer that keeps everything running. Multi-tenancy appears in Section 27 as an infrastructure concern — tenant isolation at the compute layer, resource quotas per tenant, network segmentation, and data residency compliance.

But infrastructure isolation and evaluation isolation are different problems. Section 27 ensures that Customer A's inference traffic does not compete with Customer B's for GPU resources. Section 28 ensures that Customer A's quality is measured against Customer A's standards, not Customer B's. You can have perfect infrastructure isolation — dedicated compute, separate network paths, independent scaling — and still have zero evaluation isolation because every customer's outputs are scored against the same rubric by the same judge with the same thresholds. The infrastructure is tenant-aware. The evaluation is tenant-blind.

The reverse also happens. Teams build sophisticated per-customer evaluation systems that run distinct rubrics and report per-customer quality — but deploy them on shared infrastructure where a spike in one customer's eval workload delays eval results for other customers. The eval is tenant-aware, but the infrastructure running it is not. Section 27 and Section 28 are complementary layers that must be designed together but solve different problems. Infrastructure isolation is necessary for evaluation isolation but not sufficient.

## Section 29: Governance Frames the Rules, Not the Measurement

Section 29 covers enterprise governance, organizational design, and compliance. It teaches you how to build the policies, processes, and organizational structures that govern AI systems across an enterprise. This includes regulatory compliance frameworks, audit trail requirements, model risk management, and the governance bodies that oversee AI decisions.

Governance tells you what must be evaluated and how often. It establishes the compliance requirements that per-customer evaluation must satisfy. For a healthcare customer subject to HIPAA, governance dictates that evaluation must verify PHI handling. For a financial services customer under SOX, governance requires that evaluation results constitute auditable evidence. Section 29 defines these requirements. Section 28 implements the evaluation systems that satisfy them on a per-customer basis.

The gap between governance and per-customer evaluation is the gap between policy and operations. Governance says "every customer in a regulated industry must have quarterly eval reports demonstrating compliance." Section 28 builds the evaluation infrastructure that produces those reports — the per-customer eval runs, the compliance-specific rubrics, the evidence packages, the audit trails that trace every score back to every input. Teams that have governance without per-customer evaluation have policies they cannot enforce. Teams that have per-customer evaluation without governance have capabilities without direction. You need both, and neither replaces the other.

## Section 30: Pricing Needs Eval Data, Not Eval Architecture

Section 30 covers pricing, packaging, and unit economics for AI products. It teaches you how to structure pricing that reflects your actual costs, how to create quality tiers, and how to build the economic model that makes your platform sustainable. Per-customer evaluation data is a critical input to Section 30's pricing decisions — you need to know what quality each customer is actually receiving to price quality tiers honestly and to justify premium pricing with evidence.

But Section 30 does not teach you how to produce that per-customer quality data. It assumes the data exists. Section 28 builds the systems that generate it. A platform that offers a "premium quality tier" at $0.08 per request and a "standard tier" at $0.03 per request needs per-customer evaluation to prove the premium tier actually delivers higher quality. Without Section 28's infrastructure, the quality tier is a pricing fiction — a number on an invoice with no measurement behind it. Section 30 tells you what quality data you need for pricing. Section 28 tells you how to generate that data at scale across hundreds of customers.

## The Unique Territory: Where Customer Complexity Lives

The territory that belongs to Section 28 and no other section is the intersection of all these boundaries. It is the space where evaluation strategy meets per-customer diversity. Where pipeline mechanics meet tenant-scoped routing. Where volume scaling meets complexity scaling. Where infrastructure isolation meets evaluation isolation. Where governance requirements meet per-customer evidence. Where pricing tiers meet per-customer quality measurement.

This territory has its own failure modes that do not appear in any adjacent section. **Rubric drift** happens when a customer's quality definition evolves but the eval system still runs the original rubric — a problem that does not exist when you have one rubric for one product. **Cross-customer contamination** happens when eval data from one tenant influences scoring for another — a problem that does not exist in single-tenant systems. **The coverage illusion** happens when platform-wide eval coverage looks healthy but individual customers have zero dedicated evaluation — a problem that only emerges at the intersection of volume and complexity. **The onboarding gap** happens when new customers go live without configured eval systems because the platform's default rubric is close enough until it isn't — a problem that does not exist when every customer uses the same rubric.

These failure modes are what make multi-tenant evaluation a distinct discipline. Not the volume. Not the infrastructure. Not the governance. The customer complexity — the fact that each tenant carries its own definition of quality, its own risk profile, its own regulatory constraints, and its own expectation of proof. The adjacent sections give you the building blocks. Section 28 teaches you how to assemble those blocks into a system that serves every customer on their own terms without building a custom eval system for each one.

## The Integration Requirement

Understanding these boundaries is not about drawing turf lines. It is about knowing where to build what. If you try to solve per-customer rubric management inside your generic eval pipeline from Section 15, you will contort the pipeline into something it was never designed to be. If you try to solve eval infrastructure isolation inside your per-customer rubric system from Section 28, you will duplicate infrastructure work that Section 27 already handles.

The practical architecture is layered. Section 27 provides the infrastructure layer — compute, networking, storage, isolation. Section 15 provides the pipeline layer — orchestration, execution, result storage. Section 3 provides the methodology layer — rubric design, golden sets, operating models. Section 26 provides the scaling layer — sampling, distributed compute, cost management. Section 28 provides the tenant-complexity layer — per-customer rubrics, tenant-scoped evaluation, customer reporting, quality contracts. Section 29 provides the governance layer — policies, compliance, audit requirements. Section 30 provides the economic layer — pricing, packaging, cost attribution.

Each layer depends on the ones below it and serves the ones above it. Skip any layer and the system fails. But conflate two layers — try to make one solve the other's problem — and the system becomes unmaintainable. The clearest sign that a team understands multi-tenant evaluation is that they can name which layer each component belongs to and explain why it does not belong anywhere else.

The distinction between these layers matters most when things go wrong. When Customer 247 reports a quality drop, you need to diagnose which layer the problem lives in. Is it infrastructure — did their inference latency spike, causing timeouts that degraded output quality? Is it the pipeline — did an eval pipeline failure mean their quality regression went undetected? Is it the rubric — did their quality definition change and nobody updated the eval configuration? Is it governance — did a new compliance requirement create a quality dimension that was never being measured? Each answer points to a different system, a different team, and a different fix. The platform that cannot make this diagnosis quickly is the platform that loses Customer 247.

The next subchapter closes Chapter 1 with the concrete costs of ignoring per-customer evaluation — the contracts lost, the churn accelerated, and the regulatory exposure created when a platform treats all customers the same.
