# 28.69 — The Evidence Package: Audit-Ready Exports and Trace Replay

An **evidence package** is the single artifact that proves your eval system did what you claim it did, for a specific customer, over a specific time period. It is not a quality report. It is not a dashboard screenshot. It is an audit-grade, exportable bundle that contains every configuration, every rubric version, every golden set version, every judge model identifier, every raw score, and every aggregated result that contributed to the customer's quality assessment. When an auditor, a regulator, or a customer's legal team asks "show me how you measured quality for this customer in Q3 2026," the evidence package is the answer. If you cannot produce one within 48 hours of the request, you have a compliance gap that no amount of good eval engineering can paper over.

The distinction between an evidence package and a quality report matters. A quality report tells the customer "your accuracy was 91 percent last month." An evidence package tells an auditor "here is the exact rubric definition used to measure accuracy, the version of that rubric active during the measurement period, the 340 golden set examples scored, the judge model that scored them including its model identifier and version hash, the raw score for every example, the aggregation method that produced the 91 percent figure, and the timestamps proving when each evaluation ran." The quality report communicates a result. The evidence package proves the result is legitimate.

## What Goes Into an Evidence Package

The contents of an evidence package fall into six categories, and omitting any of them creates an audit gap.

The first category is **eval configuration state**. This includes the rubric version active during the reporting period, the weight assigned to each quality dimension, the scoring scale used, any customer-specific overrides to the default rubric, and the date each configuration element was last modified. Auditors care about configuration state because they need to verify that the same rules were applied consistently throughout the period. If your rubric changed mid-quarter and the evidence package does not capture both versions with their effective dates, the auditor cannot determine which version produced which scores.

The second category is **golden set provenance**. Every reference example used in the evaluation must be traceable: when it was added to the golden set, who approved it, what use case variant it represents, and whether it was modified during the period. For customers in regulated industries like healthcare or financial services, golden set provenance is particularly sensitive because reference examples may contain domain-specific content that itself is subject to data governance requirements. The evidence package must document the golden set's composition without requiring the auditor to access live production systems.

The third category is **judge model identification**. If your eval pipeline uses LLM-as-judge scoring, the evidence package must identify exactly which judge model scored each evaluation. This means model name, version, provider, and any configuration parameters like temperature settings or system prompts used by the judge. When Anthropic updates Claude Opus 4.6 or OpenAI pushes a GPT-5 patch, the judge's behavior may shift. The evidence package must prove which judge version was active for every score in the package. Platforms that use a single string like "claude-opus" without version pinning create an audit gap that is invisible in daily operations and devastating during a compliance review.

The fourth category is **raw and aggregated scores**. The evidence package includes every individual score produced during the period — not just the aggregates, but every evaluation of every example. Aggregated scores are derived from raw scores, and auditors need to verify the derivation independently. This means the package includes the raw score matrix (each example scored on each dimension), the aggregation formula applied, and the resulting composite and per-dimension scores. If your aggregation uses weighted averages, the weights must be documented. If your aggregation excludes outliers or applies statistical corrections, the method must be specified.

The fifth category is **sample traces**. A trace is a complete input-output-score triplet: the exact input the model received, the exact output the model produced, and the exact score the judge assigned, along with any judge reasoning if your judge produces chain-of-thought explanations. Evidence packages typically include a stratified sample of traces — examples from each quality band (high-scoring, mid-scoring, low-scoring) and each use case variant. The number of sample traces depends on the customer's audit requirements, but 50 to 200 traces per reporting period is common for enterprise customers. These traces allow an auditor to independently verify that the scoring appears reasonable by examining concrete examples rather than trusting aggregate numbers.

The sixth category is **operational metadata**. Timestamps for every eval job, the identity of the operator or automation that triggered each job, job completion status, any errors or retries that occurred, and the infrastructure region where the eval executed. This metadata establishes the chain of custody — a continuous record showing that evaluations ran as scheduled, completed successfully, and were not tampered with after completion.

## Regulatory Drivers

Three regulatory frameworks are driving evidence package requirements for multi-tenant AI platforms in 2026, and their requirements overlap but are not identical.

The EU AI Act's Article 11 requires providers of high-risk AI systems to maintain technical documentation that demonstrates how the system was tested, what the results were, and how quality is monitored throughout the system's lifecycle. For multi-tenant platforms serving customers who deploy AI in high-risk categories — healthcare, financial services, legal, employment — the evidence package is how you satisfy Article 11's technical documentation requirement on a per-customer basis. The August 2026 compliance deadline for high-risk system obligations means platforms must have evidence generation capabilities operational before that date, not after.

HIPAA audit requirements for healthcare customers demand that any system processing protected health information maintain audit trails showing who accessed what data, when, and for what purpose. When your eval pipeline processes healthcare customer data — even to score it — the eval jobs become part of the HIPAA audit surface. The evidence package must demonstrate that eval data was processed in compliant infrastructure, accessed only by authorized personnel or systems, and retained according to the customer's data retention policy.

SOC 2 evidence collection requirements affect platforms that undergo SOC 2 audits. The evidence package feeds directly into SOC 2's monitoring and logging controls. Auditors reviewing your SOC 2 compliance will ask for proof that your quality monitoring operates as documented, that controls are applied consistently, and that anomalies are detected and investigated. A well-structured evidence package provides this proof without requiring the auditor to access your production systems.

## Trace Replay and Determinism

**Trace replay** is the capability to take a specific evaluation — a specific input, a specific model configuration, a specific judge, a specific rubric — and re-run it to verify that the same score is produced. Trace replay is what turns your evidence package from a historical record into a verifiable claim.

Determinism is the hard requirement. If an auditor picks a trace from your evidence package and asks you to re-produce the score, you must be able to do so. This requires version-pinned judge models (not "latest"), frozen rubric configurations (the exact version active at the time of the original eval), and reproducible scoring conditions. Non-deterministic judge models — LLMs with temperature above zero, for instance — create a challenge. You have two options: run judges at temperature zero for audit-critical evaluations, or run judges at your standard temperature but log enough judge metadata (including the random seed if available) to explain expected variance. Most platforms choose temperature zero for evidence-grade evaluations because explaining score variance to an auditor is more expensive than the marginal quality difference between temperature zero and temperature 0.3.

Trace replay also requires that the golden set examples used in the original eval are still available in their original form. If you update a golden set and delete the previous version, you lose the ability to replay traces from the previous period. This is why evidence packages must include frozen snapshots of golden set state, not just references to the current golden set.

## Storage, Retention, and Immutability

Evidence packages must be stored immutably. Once generated, the contents of an evidence package cannot be modified. This is not just a policy — it must be enforced technically through write-once storage, cryptographic hashing, or append-only data stores. When an auditor asks "how do I know this evidence package was not modified after the fact?" your answer must be a technical mechanism, not a process assurance.

The standard approach in 2026 is to generate a cryptographic hash of the complete evidence package at creation time, store the hash in a separate tamper-evident log (such as an immutable ledger or a blockchain-anchored timestamping service), and provide the hash to the customer as part of their quality report. The customer can independently verify that the evidence package they receive matches the hash, and the tamper-evident log proves the hash was recorded at creation time.

Retention periods vary by regulation and contract. HIPAA-governed evidence typically requires six-year retention. EU AI Act technical documentation must be maintained for the lifetime of the system plus ten years. SOC 2 evidence is typically retained for seven years. Customer contracts may specify their own retention requirements. Your evidence storage infrastructure must support per-customer retention policies — one customer's evidence may need six years while another's needs twelve. Deleting evidence early is a compliance violation. Retaining it beyond the required period creates unnecessary storage costs and data liability. Per-customer retention scheduling is not optional.

## Format Standards

Auditors expect structured, self-describing exports. They do not want raw database dumps, proprietary binary formats, or instructions to log into your dashboard. The evidence package should be a portable, self-contained export in a format that can be read without access to your platform.

Common formats include structured CSV or Parquet files for score data, JSON-lines for trace data (described in prose rather than rendered with literal syntax characters), and PDF or plain-text summaries for configuration state and metadata. Each file within the package should include a header or manifest describing its contents, the schema of each field, and the relationship between files. A manifest file at the root of the package should list all included files with their cryptographic hashes, creation timestamps, and descriptions.

The goal is that a compliance analyst at the customer's organization — someone who has never used your platform — can open the evidence package, read the manifest, understand what each file contains, and verify the contents against the aggregated quality report without any assistance from your team. If the evidence package requires a phone call to interpret, it is not self-describing enough.

## Automated Generation From the Eval Pipeline

The final requirement is that evidence packages are generated automatically, not assembled manually. Manual assembly introduces three risks: human error in selecting which data to include, delays in producing the package when requested, and inconsistency between packages produced by different team members.

The eval pipeline should produce evidence packages as a standard output of every eval cycle. When a customer's weekly eval run completes, the pipeline automatically generates the evidence package for that period, computes the cryptographic hash, stores the package in immutable storage, and records the hash in the tamper-evident log. The customer's quality report includes a reference to the evidence package, and the package is retrievable on demand through your customer portal or API.

For platforms serving hundreds of customers, automated generation is not just a convenience — it is a necessity. A platform with 400 customers running weekly evaluations produces 20,800 evidence packages per year. Manual assembly of even a fraction of those is operationally impossible. Automation ensures that every evaluation is backed by a complete, consistent, audit-ready evidence package from the moment it completes.

The evidence package proves what your eval system measured and what it found. The next subchapter addresses the companion question: what changed since the last time you measured, and why the difference matters more than the absolute numbers.
