# 3.4 â€” Recovery and Rollback: Memory as a Fault Tolerance Primitive

In September 2025, a financial services company deployed an agent-based reconciliation system to match incoming wire transfers against customer accounts. The system used a multi-step workflow: first, an agent would parse the wire transfer message and extract account numbers, amounts, and reference codes. Second, an agent would query the customer database to find matching accounts. Third, an agent would validate the transfer against fraud rules and compliance policies. Fourth, an agent would post the transaction to the ledger. The workflow processed over fifteen thousand transfers per day. In the first month of operation, the team discovered that roughly two percent of transfers entered an unrecoverable error state. The parsing agent would successfully extract account information. The lookup agent would find the matching customer. The validation agent would approve the transfer. But the ledger agent would fail because the ledger API timed out after thirty seconds. When the ledger agent failed, the entire workflow halted. The transfer sat in limbo. The customer's money had left their bank but had not arrived in their account. The team had no automated way to retry just the ledger step. They had to manually identify the failed transfer, manually verify that the first three steps had completed successfully, and manually resubmit to the ledger API. By late October, the operations team was spending four hours per day on manual reconciliation. The root cause was architectural: the system had no memory of intermediate state and no mechanism to retry individual steps. When any step failed, the entire workflow was lost. The team had built a pipeline without checkpoints.

Memory is what makes agent systems fault-tolerant. When an agent fails halfway through a multi-step task, memory preserves the work completed so far. When an external API returns an error, memory allows the agent to retry that specific operation without re-executing prior steps. When a model hallucinates a plan that leads to a dead end, memory allows the agent to backtrack and try a different path. Without memory, every failure is catastrophic. The agent must start over from the beginning, wasting time, tokens, and API calls. With memory, failures are recoverable. The agent resumes from the last known good state. This is not a luxury for high-reliability systems. In 2026, production agent systems run thousands of workflows per day. Even a one percent failure rate means hundreds of failures. If recovery requires human intervention, the system does not scale. Memory-based recovery is how agent systems achieve the same operational maturity as traditional software: automatic retries, graceful degradation, and self-healing.

## Checkpoint-Based Recovery

Checkpointing is the simplest and most effective recovery pattern. Before executing a risky operation, the agent saves its current state to memory. If the operation succeeds, the agent continues. If the operation fails, the agent restores from the checkpoint and retries with a different approach or escalates to a human. A legal research agent might checkpoint before querying an expensive legal database API. The checkpoint includes the research question, the search terms, the documents already reviewed, and the findings so far. The agent queries the API. If the API returns results, the agent processes them and continues. If the API times out, the agent restores from the checkpoint. It knows exactly where it was: it had reviewed twelve documents, extracted six relevant cases, and was about to query the database for cases citing those six. The agent does not need to re-review the twelve documents or re-extract the six cases. It retries the database query, possibly with a longer timeout or a fallback to a different database.

Checkpoint granularity is a design decision. Fine-grained checkpoints reduce the amount of work lost on failure but increase storage and latency overhead. Coarse-grained checkpoints reduce overhead but lose more work on failure. A content generation agent writing a ten-section report might checkpoint after each section. If the agent fails while writing section seven, it restores from the checkpoint after section six and resumes. Only section seven is lost. If the agent checkpoints only after completing the entire report, a failure on section seven means re-generating all ten sections. The right granularity depends on the cost of re-execution and the likelihood of failure. If individual steps are fast and cheap, coarse-grained checkpoints are fine. If steps are slow or expensive, checkpoint more frequently. The financial reconciliation system should checkpoint after each of the four agents completes its step. The parsing agent writes its output to memory and marks a checkpoint. If the lookup agent fails, the system restores from that checkpoint and retries the lookup without re-parsing.

Checkpoint storage determines where checkpoints are written. In-memory checkpoints are fast but volatile. If the agent process crashes, the checkpoints are lost. Disk-based checkpoints survive process crashes but are slower to write. Cloud storage checkpoints survive machine failures but introduce network latency and cost. The choice depends on failure modes. If you are protecting against transient errors like API timeouts, in-memory checkpoints suffice. The agent retries immediately in the same process. If you are protecting against process crashes or machine failures, use disk or cloud storage. A distributed agent system might write checkpoints to a shared database so any instance can resume a failed workflow. Some systems use tiered checkpointing: write to memory for fast access, asynchronously flush to disk for durability. If the agent needs to restore within the same process, it reads from memory. If the process has restarted, it reads from disk.

Checkpoint compression reduces storage cost for large states. An agent processing a fifty-page document might accumulate megabytes of intermediate state: extracted entities, inferred relationships, partially constructed knowledge graphs. Checkpointing the raw state every few operations would consume gigabytes. Use compression. Before writing the checkpoint, serialize the state to JSON or Protocol Buffers and compress with gzip or zstd. A fifty-megabyte state compresses to five megabytes. The trade-off is CPU cost. Compressing and decompressing on every checkpoint adds milliseconds to latency. For long-running workflows where storage cost dominates, compression is worth it. For short workflows where latency matters, skip compression.

Checkpoint expiration prevents unbounded storage growth. A customer support agent might create dozens of checkpoints per conversation. If conversations run for thirty minutes, the system accumulates hundreds of checkpoints per hour. Most checkpoints are never used. The agent completes successfully and never needs to restore. Implement expiration policies. Delete checkpoints older than one hour. Delete checkpoints for workflows that completed successfully. Keep checkpoints for failed workflows for post-mortem analysis. Some systems use a garbage collection process that runs periodically and purges expired checkpoints. Others use time-to-live fields in the database. The checkpoint record includes a created-at timestamp. The database automatically deletes records older than one hour.

## Rollback Strategies

Rollback is the inverse of checkpointing. Instead of resuming forward from a saved state, the agent undoes recent actions and returns to a prior state. A data enrichment agent might query three external APIs to gather information about a company: one for financial data, one for news articles, one for social media mentions. The agent queries the financial API, processes the response, and updates its internal state. It queries the news API, processes that response, and updates its state again. It queries the social media API, but that API returns an error indicating rate limiting. The agent could retry, but retrying will also hit the rate limit. Instead, the agent rolls back to the state before querying the social media API. It preserves the financial and news data but marks the social media data as unavailable. It completes the workflow with partial data rather than failing entirely. Rollback enables graceful degradation. When full success is impossible, the agent salvages what it can.

Undo logs record every state mutation with enough information to reverse it. Before the agent adds an item to a list, it logs a remove-item operation. Before it updates a field, it logs the old value. If the agent needs to roll back, it reads the undo log in reverse order and applies each undo operation. This is the same pattern used by database transaction logs. The undo log is append-only. Every mutation appends an undo record. Rollback reads the log backward and applies the inverses. The challenge is defining inverses for complex operations. If the agent calls an external API that creates a resource, the inverse is calling the API to delete that resource. If the API does not support deletion or the deletion call fails, the rollback is incomplete. Some systems use compensating transactions instead of true rollback. The agent cannot undo the API call, but it can mark the resource as invalid or notify a cleanup service to handle it asynchronously.

Rollback depth defines how far back the agent can go. Shallow rollback undoes the last operation. Deep rollback can undo the last N operations or return to a named checkpoint. A planning agent might explore multiple branches of a decision tree. It pursues branch A for three steps, realizes it leads to a dead end, and rolls back all three steps. It pursues branch B for two steps, hits another dead end, and rolls back those two. It pursues branch C and succeeds. Deep rollback requires maintaining a stack of undo logs or a linked list of checkpoints. Each checkpoint points to its parent. Rolling back to checkpoint C means restoring checkpoint C's state and discarding all checkpoints created after C. The stack depth is limited by memory. If the agent explores a hundred branches, maintaining a hundred checkpoints may be infeasible. Use bounded depth. Keep the last ten checkpoints. If the agent needs to roll back further, it cannot. This forces the agent to commit occasionally. After exploring ten branches, the agent must decide whether to commit to the best one or abandon the entire search.

Partial rollback undoes some state changes but not others. A multi-agent workflow might have Agent A query a database, Agent B call an external API, and Agent C write results to a file. If Agent C fails, you might want to roll back the file write and the API call but not the database query, because the database query is expensive and idempotent. Implementing partial rollback requires labeling state changes as rollback-eligible or not. Critical mutations that are expensive or have side effects are pinned. Tentative mutations are rollback-eligible. The agent marks the database query result as pinned and the API call result as tentative. On rollback, the agent discards tentative state but keeps pinned state. This requires careful design. If downstream steps depend on tentative state, rolling it back might leave the system in an inconsistent state.

Optimistic rollback assumes rollback will rarely be needed and optimizes for the happy path. The agent does not write undo logs for every mutation. Instead, it periodically snapshots its state. If rollback is needed, the agent restores the most recent snapshot and re-executes from there. This is faster than maintaining detailed undo logs but loses more work on rollback. A code generation agent might snapshot every fifty lines of generated code. If the agent generates three hundred lines and then detects a logic error at line two hundred and eighty, it rolls back to the snapshot at line two hundred and fifty and regenerates the last fifty lines. It does not have the fine-grained undo information to roll back just the bad thirty lines. Optimistic rollback trades off precision for performance. Use it when rollback is rare and re-execution is cheap.

## Replay from Known Good States

Replay is a recovery strategy where the agent re-executes a workflow from a known good state using the same inputs. A fraud detection agent processes a transaction, flags it as suspicious, and triggers a manual review. The human reviewer determines the flag was incorrect. The agent should re-process the transaction with updated rules to produce the correct output. Replay reads the original transaction from memory, applies the updated rules, and produces a new result. Replay requires storing inputs along with outputs. If the agent only stores the final decision without the input transaction, replay is impossible. Some systems store the full input and output for every workflow execution. Others store only inputs and recompute outputs on demand. The trade-off is storage cost versus compute cost. Storing outputs avoids recomputation but consumes more storage. Storing only inputs saves storage but requires recomputing every output on access.

Deterministic replay requires that re-executing the same workflow with the same inputs produces the same output. This is trivial for pure functions but challenging for agents that call external APIs or use non-deterministic models. If the agent queries a weather API during the original execution, replaying the workflow will query the API again and might get different results if the weather has changed. If the agent uses a language model with temperature greater than zero, replaying produces different outputs because the model samples randomly. To achieve deterministic replay, log all external inputs. When the agent queries the weather API, log both the request and the response. During replay, instead of calling the API, read the logged response. When the agent calls the language model, log the exact tokens sampled. During replay, force the model to produce those tokens. This transforms a non-deterministic workflow into a deterministic one. The logged inputs and outputs become part of the replay state.

Replay for debugging allows engineers to reproduce failures. A summarization agent fails on a particular document, producing a summary that omits key information. The engineer triggers a replay using the same document and the same agent version. The replay reproduces the failure. The engineer can now step through the agent's execution, inspect intermediate state, and identify the bug. Without replay, reproducing the failure might be impossible. The document might no longer be available. The agent might have been updated. External APIs might return different data. Replay isolates the failure by freezing all inputs. Some systems provide replay-with-modification. The engineer replays the workflow but overrides certain parameters. Replay with temperature zero instead of temperature 0.7. Replay with a different prompt. Replay with verbose logging enabled. This allows hypothesis testing. Does lowering temperature fix the issue? Does rephrasing the prompt change the output?

Replay for auditing provides a tamper-proof record of agent decisions. A loan approval agent makes thousands of decisions per day. Regulators require the ability to audit any decision and understand how the agent reached its conclusion. The system logs every workflow execution: the loan application, the credit score, the agent's reasoning trace, the final decision. If the regulator asks why loan application 47392 was denied, the team replays that workflow execution. The replay shows exactly which rules the agent applied, which thresholds were exceeded, and which factors contributed to the denial. The replay is read-only. It does not modify any state or trigger any actions. It simply reconstructs the agent's reasoning from logged memory. Some jurisdictions require that audit logs be immutable and verifiable. Write logs to append-only storage with cryptographic hashes. Each log entry includes a hash of the previous entry, forming a chain. Tampering with any entry invalidates the chain.

Replay for regression testing validates that agent updates do not break existing workflows. The team updates the fraud detection agent to incorporate a new rule about cryptocurrency transactions. Before deploying, they replay the last ten thousand fraud detection workflows using the new agent version. They compare the new outputs to the original outputs. If more than one percent of outputs differ, the update is flagged for review. This catches unintended side effects. The new rule might interact with existing rules in unexpected ways, causing false positives on non-cryptocurrency transactions. Replay-based testing is only as good as the test coverage. If the replayed workflows do not include edge cases, the testing misses bugs. Some teams use production replay: continuously replay a sample of production traffic against the latest agent version in a staging environment. If the staging agent produces significantly different outputs, flag the differences for human review before deploying.

## The Memory Journal Pattern

A memory journal is an append-only log of every action the agent takes, every decision it makes, and every observation it receives. The agent does not overwrite state. It appends new entries. Each entry includes a timestamp, an action type, and a payload. The agent reads a document: append a read-document entry with the document ID and timestamp. The agent calls an API: append a call-api entry with the endpoint, parameters, and response. The agent updates its plan: append an update-plan entry with the old plan and the new plan. The journal becomes a complete history of the agent's execution. Replaying the journal from the beginning reconstructs the agent's state at any point in time. Rolling back to a prior state means replaying the journal up to a certain entry and ignoring everything after.

Event sourcing is a software architecture pattern where the journal is the primary source of truth. Instead of storing the current state in a database, the system stores the sequence of events that led to that state. The agent's current state is the result of replaying all events. This is the same pattern used by version control systems like git. The repository does not store the current file contents. It stores a sequence of commits. Checking out a branch means replaying commits up to the branch head. Event sourcing provides perfect auditability. Every state change is an event. You can query the journal to see when and why the state changed. It provides time travel. Replay the journal up to a specific timestamp to see what the state was at that moment. It provides branching. Replay the journal up to event N, then continue with different events to explore alternate histories.

Journal compaction prevents unbounded growth. A long-running agent might accumulate millions of journal entries. Replaying from the beginning becomes slow. Periodically snapshot the current state and truncate the journal. Append a snapshot entry containing the full state. Delete all entries before the snapshot. Replaying now starts from the snapshot instead of the beginning. The trade-off is that you lose fine-grained history before the snapshot. Some systems keep both. Compact the journal for operational use but archive the full journal to cold storage for compliance and auditing. A healthcare agent might compact the journal every hour for performance but archive the full journal to S3 for the legally required seven-year retention period.

Journal branching enables speculative execution. The agent checkpoints the journal, explores a speculative branch, and either commits the branch or discards it. A negotiation agent interacting with an API might speculatively propose a set of terms, see how the API responds, and decide whether to commit. If the API accepts, the agent commits the speculative branch and continues. If the API rejects, the agent discards the branch and tries a different approach. The journal tracks both the main branch and speculative branches. Committing a branch merges its entries into the main journal. Discarding a branch deletes its entries. This is analogous to database transactions. Begin a transaction, execute speculative operations, commit if successful, rollback if not.

Journal-based replay is easier to implement than state-based replay because the journal is self-contained. To replay a workflow, you only need the journal. You do not need to reconstruct database states, API responses, or file contents. Everything is in the journal. The challenge is journal size. A workflow that makes a hundred API calls and processes a thousand documents generates a hundred thousand journal entries. Storing and replaying a hundred thousand entries is expensive. Use summarization. Periodically replace a sequence of low-level entries with a high-level summary. Instead of logging read-chunk entries for a thousand document chunks, log a single read-document entry with a summary of what was read. Summarization loses fidelity but makes replay feasible for long workflows.

## Graceful Degradation When Full Recovery Is Impossible

Not every failure is recoverable. An agent scheduling a meeting sends calendar invites to ten participants. Eight accept, two decline. The agent cannot recover to a state where all ten accept. The best it can do is report partial success and escalate the two declines to a human. Graceful degradation is about completing the workflow with reduced functionality when full success is impossible. Memory enables graceful degradation by tracking what succeeded and what failed. The agent writes to its journal: sent invite to participant A, status accepted. Sent invite to participant B, status declined. At the end, the agent reviews the journal and produces a summary: eight accepted, two declined, meeting scheduled with eight participants. The human receives actionable information rather than a generic error.

Partial results are often more valuable than no results. A research agent tasked with gathering information from five sources queries all five in parallel. Three return results. Two time out. Without memory, the agent might report total failure. With memory, the agent reports the three successful results and notes that two sources were unavailable. The user decides whether three sources are sufficient or whether to retry the two failed sources manually. This requires the agent to distinguish between critical and optional operations. If the agent is writing a financial report and the stock price API fails, that is critical. The report cannot be completed. If the sentiment analysis API fails, that is optional. The report can be completed without sentiment data. The agent's plan should annotate operations as critical or optional. Memory tracks which operations completed. On failure, the agent checks whether all critical operations succeeded. If yes, produce partial results. If no, escalate.

Fallback strategies use memory to select alternative approaches when the primary approach fails. A translation agent tries to translate a document using GPT-4o. The API returns a rate limit error. The agent consults its memory to see what alternatives are available: Claude 3.5 Sonnet, Gemini 2, or a human translator. The agent tries Claude 3.5 Sonnet. That succeeds. The agent completes the workflow with a fallback model. Memory tracks which fallback was used. If the user asks why the translation took longer than expected, the agent explains that the primary model was unavailable and a fallback was used. Some agents learn from failures. If GPT-4o rate limits persist, the agent updates its memory to prefer Claude 3.5 Sonnet for future translations. This is a simple form of adaptation. The agent is not training a model. It is updating a preference table stored in memory.

Compensation is an alternative to rollback when the action cannot be undone. The agent books a flight, then discovers the hotel at the destination is unavailable. It cannot un-book the flight, but it can book a different hotel or notify the user to choose a different destination. Compensation requires creative problem-solving. The agent's memory includes the goal: book travel to the destination. It includes the constraints: flight already booked, hotel unavailable. The agent searches for compensating actions: book a different hotel, change the flight, cancel everything and start over. The agent evaluates each option and selects the one that best satisfies the goal. Memory provides the context for evaluating options. If the user prioritized cost over convenience, the agent compensates by booking a cheaper hotel further from the city center. If the user prioritized convenience, the agent compensates by changing the flight to a different city with available hotels.

Escalation to humans is the ultimate fallback. When the agent exhausts all automated recovery strategies, it writes a detailed failure report to memory and escalates to a human. The report includes the workflow goal, the steps completed successfully, the step that failed, the recovery strategies attempted, and the current state. The human reads the report, decides how to proceed, and either manually completes the workflow or instructs the agent to retry with different parameters. Memory makes escalation actionable. The human does not waste time reconstructing what the agent did. The memory provides a complete history. The human focuses on the decision: retry, abort, or manually intervene.

The next subchapter explores how agent systems use memory to personalize interactions and adapt to user preferences over time.
