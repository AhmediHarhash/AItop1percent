# 6.2 — Detecting Hallucinated Recall and Memory Contamination

In March 2025, a healthcare technology company discovered that its patient engagement system had been giving advice based on medical conditions that patients never had. The system, which used memory to maintain context across multiple visits, confidently referenced a heart condition discussion from a previous appointment. The patient, confused, explained they had never discussed heart problems. The engineering team investigated and found the model had hallucinated the memory entirely. It wasn't retrieving stored information—it was fabricating plausible-sounding medical history based on patterns in its training data. The contamination spread across 1,200 patient interactions over six weeks before detection. The company faced regulatory scrutiny, spent $890,000 on remediation, and implemented a complete memory audit system. The root cause was treating model output as ground truth without validating against actual stored memories.

This failure reveals a fundamental challenge in memory systems: models don't just retrieve what you store. They confabulate, they blend training data with retrieved context, they hallucinate memories that feel real but never existed. Your memory system becomes a contamination vector when you can't distinguish between legitimate recall and fabricated context. The distinction matters because hallucinated recall looks exactly like real memory to the model. It states the fabricated information with high confidence, weaves it naturally into conversation, references specific details that sound plausible. Users trust it. Downstream systems act on it. The contamination compounds until someone notices the impossibility.

Detecting hallucinated recall requires treating memory retrieval as an adversarial process. You're not just fetching data—you're validating that what the model claims to remember actually exists in your store, was written correctly, and hasn't been corrupted. This subchapter covers how to build detection systems that catch fabricated memories, measure contamination rates, and prevent poisoned data from entering your memory store in the first place.

## What Hallucinated Recall Looks Like in Production

Hallucinated recall manifests as the model confidently referencing information that was never stored. You see it when the model says "as we discussed last week" about a conversation that never happened. You see it when it recalls user preferences that were never set. You see it when it cites previous interactions with details that sound right but don't exist in your logs. The hallucination feels coherent because the model generates it using the same mechanisms it uses for legitimate recall. It pulls from training data patterns, from similar conversations, from plausible completion of the current context.

The challenge is that hallucinated recall doesn't look like gibberish. It looks like memory. A customer service system might recall that a user complained about shipping delays when they actually complained about product defects. A financial advisor system might remember a risk tolerance conversation that never occurred. A medical system might reference allergies that were never mentioned. The fabricated memories fit the narrative flow. They're contextually appropriate. They just aren't true.

You detect hallucinated recall by comparing model claims against your actual memory store. When the model says "you told me you prefer email updates," you query the memory system for that preference. If it doesn't exist, you have hallucination. When the model references "our conversation about pricing on January 15th," you check conversation logs for that date and topic. If there's no match, you have fabrication. The detection requires making model memory claims explicit and verifiable.

The false confidence problem makes this detection critical. Models state hallucinated memories with the same confidence as real ones. They don't flag uncertainty. They don't say "I think you mentioned" versus "you definitely mentioned." They present fabricated recall as fact. Your system must assume the model is overconfident and validate every memory claim against ground truth. Trust but verify becomes verify then trust.

Hallucinated recall also appears when models blend multiple real memories incorrectly. They might take a preference from user A and a context from user B and merge them for user C. They might remember the right topic but wrong date, right person but wrong conversation, right preference but wrong intensity. These blended hallucinations are harder to detect because parts of them are true. Your validation logic must check not just existence but correct attribution and association.

## Distinguishing Model Confabulation from Actual Memory Retrieval

Model confabulation happens when the model generates plausible content that fills gaps in its actual knowledge. It's the AI equivalent of a human making up details to complete a story they half-remember. The model has partial context, recognizes a pattern, and generates what should come next based on training data. The generated content feels like memory because it's contextually coherent, but it's actually statistical completion.

You distinguish confabulation from retrieval by implementing strict memory provenance. Every piece of information the model references must trace back to a specific storage event. When the model says "you prefer concise responses," your system checks for a stored preference with that exact claim. If it exists, you have retrieval. If it doesn't, you have confabulation. Provenance requires structured memory writes with timestamps, source attribution, and unique identifiers that can be validated.

The distinction matters because confabulation and retrieval require different interventions. Retrieval errors indicate problems with your memory query logic, ranking, or context assembly. Confabulation errors indicate prompt engineering problems, insufficient grounding instructions, or model selection issues. A system that hallucinates frequently despite having good memory storage needs better prompts that emphasize only referencing verified information. A system that fails to retrieve stored memories needs better search and ranking.

You implement confabulation detection through memory citation requirements. Your prompt instructs the model to cite memory sources when making claims about previous interactions. The citation format is simple: "According to stored preference ID 1847, you prefer email updates." Your validation layer checks that ID 1847 exists and contains that preference. If the model makes a claim without citation, you reject it as unverified. If it cites a non-existent ID, you flag it as hallucination. This approach makes confabulation explicit and catchable.

Some confabulation is subtle. The model might correctly recall that a user likes short responses but fabricate the reason why. It remembers the preference but invents the justification. Your detection system must validate not just the existence of memories but their completeness and accuracy. If the model says "you prefer short responses because you're usually on mobile," you check whether that reason was ever stored. If your memory only has the preference without the justification, the model confabulated the explanation. This level of validation requires granular memory structure where preferences, reasons, contexts, and metadata are separately verifiable.

## Contamination Vectors: Bad Writes, Poisoned Inputs, Stale Data

Memory contamination enters through three primary vectors: bad writes that store incorrect information, poisoned inputs that trick the system into storing malicious content, and stale data that becomes incorrect over time. Each vector requires different detection and prevention strategies.

Bad writes happen when your memory extraction logic misinterprets user input and stores something the user never meant. A user says "I don't want promotional emails" and your system stores "user wants promotional emails" because of a parsing error. A user says "maybe I'll consider the premium plan" and your system stores "user committed to premium plan." These bad writes contaminate your memory with false information that the model then confidently retrieves. You detect bad writes through write-time validation. Before storing a preference or fact, you show it back to the user for confirmation when stakes are high. You implement confidence scoring on extracted information and flag low-confidence writes for review. You maintain write logs that let you trace contamination back to the source conversation and identify systematic extraction errors.

Poisoned inputs are adversarial attempts to inject false information into memory. A user might say "for your records, I'm allergic to penicillin" when they're not, testing whether your system blindly stores self-reported medical information. An attacker might claim "I previously requested full admin access" to see if your system stores and acts on fabricated history. Poisoned inputs exploit the assumption that users tell the truth and that stated preferences should be stored as-is. You defend against poisoning through input validation rules. Medical information requires clinical verification before storage. Security-relevant preferences require authentication checks. High-stakes claims require multi-modal confirmation.

Stale data contamination happens gradually. A user's preferences change, their contact information updates, their circumstances shift, but your memory retains the old information. The model retrieves it as if it's current. A customer who moved addresses gets deliveries sent to their old location because your memory stored it two years ago. A user who changed careers gets job-related recommendations based on their previous role. Stale data contamination is insidious because it was correct when written. You detect it through staleness scoring, which we'll cover in depth in the next subchapter, and through contradiction detection, where new inputs conflict with stored memories and trigger refresh workflows.

All three contamination vectors share a common pattern: they create a gap between what the memory system believes is true and what is actually true. Your detection strategy is to continuously validate stored memories against ground truth sources, user corrections, and logical consistency checks. You build trust scores for memories based on how they were sourced, how recently they were verified, and whether they've been contradicted.

## Detection Techniques: Comparing Model Claims Against Actual Memory Store

The foundational detection technique is claim extraction and validation. You parse model outputs to identify memory claims—statements that reference previous interactions, stored preferences, or historical context. Each claim gets extracted into a structured format: what was claimed, what time period it references, what entity it's about. You then query your memory store for evidence supporting that claim. If evidence exists and matches, the claim is validated. If evidence is missing, contradictory, or incomplete, you flag the claim as potential hallucination.

Claim extraction requires prompt engineering and output parsing. You instruct the model to mark memory references explicitly. One approach is to have the model wrap memory claims in a specific format: "Based on previous interaction, user prefers concise responses." Your parser identifies "based on previous interaction" as a memory claim marker and extracts "user prefers concise responses" as the claim to validate. You query your preference store for a concise response preference. If it exists with sufficient confidence, you allow the claim. If not, you reject it and instruct the model to respond without that assumption.

Another detection technique is parallel verification. You run two inference paths: one with memory context and one without. You compare the outputs. Claims that appear only in the memory-enabled path are candidates for validation. If the model says something with memory that it wouldn't say without memory, that's a memory claim. You validate it against your store. This approach catches implicit memory usage where the model doesn't explicitly mark claims but changes its behavior based on retrieved context.

You implement automated validators that run after every model response. The validator receives the model output and the memory context that was provided. It checks that every fact stated in the output either appears in the provided context or is general knowledge. If the model claims something specific about the user that wasn't in the context, the validator flags it. This creates a closed-loop system where memory usage is continuously audited. Over time, you build a flagged claim dataset that reveals systematic hallucination patterns.

Detection also requires negative testing. You deliberately withhold known memories and check whether the model fabricates them anyway. If you have a stored preference that the user likes concise responses, you run a test conversation without providing that preference in context. If the model still claims to know the preference, it's hallucinating. These absence tests reveal when the model is relying on training data patterns rather than actual retrieval. They're especially important for common preferences and facts that appear frequently in training data.

## Automated Hallucination Detection Pipelines

An automated hallucination detection pipeline continuously monitors memory claims in production and flags anomalies. The pipeline consists of claim extraction, validation, scoring, and alerting components that run in near real-time as users interact with your system.

The claim extraction component uses a combination of rule-based parsing and a secondary model. Rules catch explicit memory markers like "you told me," "last time," "previously," "you prefer," "your account shows." The secondary model identifies implicit memory claims where the model states specific user facts without explicit markers. The extractor outputs a structured claim set for each conversation: what was claimed, confidence in the extraction, and relevant context.

The validation component queries your memory store for each claim. It uses fuzzy matching because claims might be paraphrased. If the model says "you like brief answers" and your store has "preference for concise responses," the validator recognizes these as equivalent. The validator returns match status: exact match, fuzzy match, partial match, no match, contradiction. Each status has different implications. Exact matches are validated. Fuzzy matches are flagged for review. No matches are flagged as hallucinations. Contradictions are flagged as high-priority contamination.

The scoring component assigns hallucination risk scores based on validation results and claim characteristics. A claim with no memory match gets a high risk score. A claim about high-stakes information like medical conditions or financial commitments gets elevated risk even with a fuzzy match. A claim about low-stakes preferences gets lower risk. The scoring incorporates claim specificity—specific claims like "you called on January 15th at 2pm" are higher risk than vague claims like "we've discussed this before." Specific hallucinations are more dangerous because they're more convincing.

The alerting component sends notifications when hallucination scores exceed thresholds. High-risk hallucinations in production trigger immediate alerts to on-call engineers. Moderate-risk hallucinations batch into daily reports. The alerts include the conversation ID, the specific claim, the validation result, and suggested remediation. Engineers can review flagged conversations, determine if hallucination occurred, and update prompts or memory logic to prevent recurrence.

The pipeline also includes a feedback loop. When human reviewers confirm a hallucination, that example enters a training dataset. You use it to improve your claim extraction model, refine your validation logic, and update your prompts with better grounding instructions. Over time, the pipeline learns which types of claims are prone to hallucination and applies stricter validation to those patterns.

## Measuring Contamination Rates Over Time

Contamination rate is the percentage of memory claims that don't validate against your memory store. You measure it continuously to track memory system health. A rising contamination rate indicates degrading prompt effectiveness, memory retrieval failures, or increasing adversarial inputs. A falling rate indicates your detection and prevention systems are working.

You calculate contamination rate per conversation, per user, and per time period. Per-conversation rate is the number of invalid claims divided by total claims in that conversation. Per-user rate is aggregated across all conversations for that user. Per-time-period rate is aggregated across all users in a day, week, or month. Each aggregation level reveals different issues. A single conversation with high contamination might indicate a prompt regression. A single user with persistent contamination might indicate poisoning attempts. A system-wide contamination spike might indicate a model version change or memory store corruption.

You track contamination by memory type. Preferences might have a two percent contamination rate while conversation history has five percent. The difference tells you where to focus remediation. If preferences are clean but history is contaminated, your conversation summarization logic is flawed. If preferences are contaminated but history is clean, your preference extraction is misinterpreting user statements.

Contamination trends matter more than absolute rates. A system at three percent contamination that's dropping to two percent is healthy. A system at one percent that's climbing to two percent is degrading. You set up trend monitoring with week-over-week comparisons. A 25 percent increase in contamination rate triggers investigation even if absolute rate is low. The investigation examines recent changes: model updates, prompt modifications, memory schema changes, increased traffic patterns.

You also measure contamination severity. Not all contamination is equal. Hallucinating a user's preferred greeting is low severity. Hallucinating medical history or financial commitments is critical. Your measurement system weights contamination by impact. A single critical contamination event might outweigh a hundred minor ones. Your dashboards show both rate and severity-weighted contamination scores.

## The False Confidence Problem: Fabricated Memory with High Confidence

The false confidence problem is that models state fabricated memories with the same certainty as real ones. They don't express doubt. They don't hedge. They present hallucinated recall as established fact. This makes contamination dangerous because users and downstream systems trust the model's confidence.

You see false confidence when the model says "you definitely told me you prefer email" with no memory evidence. When it says "according to your previous request" referencing a request that never happened. When it provides specific dates, times, and details for events that didn't occur. The confidence isn't calibrated to evidence. It's calibrated to completion probability. The model generates text that sounds confident because that's what naturally follows the context, not because it has verified information.

You combat false confidence through confidence calibration requirements. Your prompt instructs the model to express uncertainty when memory evidence is weak. If a memory claim has low match confidence, the model should say "I think you mentioned" rather than "you told me." If no memory exists, the model should say "I don't have a record of that preference" rather than fabricating one. Calibration turns false confidence into useful uncertainty signals.

You also implement confidence scoring on memory retrieval. When your memory system returns a result, it includes a confidence score based on recency, source reliability, and match quality. The model receives this score and adjusts its language accordingly. High-confidence memories get stated as facts. Medium-confidence memories get hedged phrasing. Low-confidence memories get flagged as uncertain. This creates alignment between memory evidence strength and model output confidence.

Another approach is forced citation. The model must cite memory IDs when making claims. If it says "you prefer email updates per memory ID 2891," you can validate that ID. If the model can't provide an ID, it can't make the claim. This prevents the model from stating fabricated memories confidently because fabricated memories don't have IDs to cite. The citation requirement makes confidence verifiable.

You measure false confidence through calibration analysis. You take all model claims, score their stated confidence, and check their validation rate. Perfectly calibrated confidence means 95 percent confident claims validate 95 percent of the time. False confidence shows up as high stated confidence with low validation rates. If the model speaks with 95 percent confidence but only 60 percent of those claims validate, you have a 35 percentage point calibration gap. You track this gap over time and use it to tune prompt instructions and confidence thresholds.

## Red-Teaming Memory Recall: Adversarial Testing

Red-teaming memory recall means deliberately attempting to trick your system into hallucinating or retrieving contaminated memories. You run adversarial tests that probe for weaknesses in your detection and validation systems. The goal is to find failure modes before users and attackers do.

One red-team technique is fabrication injection. You have a test user claim a false memory: "last time I told you I'm allergic to shellfish" when no such conversation occurred. You check whether your system stores this claim without verification, whether the model retrieves it in future conversations, and whether your detection pipeline flags it as suspicious. If the system blindly accepts and recalls the fabricated memory, you have a contamination vulnerability.

Another technique is memory poisoning at scale. You simulate an attacker who consistently provides false information across many interactions, attempting to establish a contaminated memory profile. You check whether your trust scoring and validation systems eventually catch the pattern and flag the user or the memories as unreliable. If repeated poisoning succeeds, your system lacks contamination defenses.

Confusion attacks test whether you can make the model blend memories from different users or contexts. You provide similar but distinct memories for different users and check whether the model sometimes retrieves the wrong one. If user A prefers email and user B prefers SMS, you verify the model never tells user A they prefer SMS. These cross-contamination tests reveal memory isolation failures.

Temporal attacks test whether you can exploit stale memories. You establish a preference, change it explicitly, and check whether the model sometimes retrieves the old preference. If your staleness detection is weak, the model might recall outdated information confidently. These tests reveal memory refresh and invalidation gaps.

You also run plausibility attacks. You claim memories that sound plausible but are logically impossible. "I told you I'm allergic to water." "I mentioned I'm 200 years old." "I said I need responses in a language that doesn't exist." Your system should reject or flag these as implausible even if detection rules don't explicitly forbid them. Plausibility checking requires semantic validation beyond simple existence checks.

Red-team results feed into your threat model. Each discovered vulnerability gets documented, assigned severity, and prioritized for remediation. You track mean time to detect adversarial inputs and measure improvement as you strengthen validation. Over time, red-teaming becomes continuous background testing that probes your production system with safe synthetic attacks.

## Building Test Harnesses that Inject Known-False Memories

A test harness for memory hallucination detection works by deliberately injecting false memories into test environments and verifying your system catches them. You create synthetic conversations with known-false claims, run them through your memory pipeline, and check whether validation flags them.

The harness maintains a ground truth dataset of what should and shouldn't be stored. It includes conversations where users make false claims, where the model should confabulate if poorly prompted, where memories should be rejected as implausible. Each test case has expected outcomes: should this be stored, should this be flagged, should this trigger alerts.

You inject false memories at different stages. Write-time injection tests whether bad extractions get stored. You provide a conversation where the user clearly says "I don't want marketing emails" and test whether your extraction logic might store "user wants marketing emails." If your validator catches the inversion, the test passes. If the false memory gets stored, the test fails and you fix your extraction logic.

Retrieval-time injection tests whether the model hallucinates even when memory is empty. You run conversations with no stored context and check whether the model fabricates user preferences anyway. These tests catch over-reliance on training data patterns. You expect the model to say "I don't have information about your preferences" rather than inventing plausible ones.

Contamination injection tests whether poisoned inputs get properly flagged. You provide adversarial conversations where users claim false histories or impossible preferences. Your system should reject or flag them. If they get stored and recalled, your trust scoring and validation logic needs strengthening.

The harness runs continuously in your CI/CD pipeline. Every prompt change, every model update, every memory schema modification triggers the test suite. If contamination detection regresses, the build fails. This prevents you from deploying changes that increase hallucination risk. Over time, your test suite grows as you encounter new hallucination patterns in production. Each real-world contamination event becomes a test case that prevents recurrence.

You measure test coverage by cataloging known hallucination patterns and verifying each has corresponding test cases. Fabricated preferences, blended memories, impossible claims, adversarial poisoning, stale data retrieval—each pattern needs explicit tests. Coverage gaps indicate areas where hallucination might occur without detection.

## Building Detection into Memory Write and Retrieval Paths

Detection shouldn't be a separate audit process—it should be embedded in your memory write and retrieval code paths. Every write operation validates input before storage. Every retrieval operation validates output before providing it to the model. This creates defense in depth where contamination is caught at multiple stages.

Write-path detection starts with input validation. Before storing a user preference, you check plausibility. If a user claims to prefer responses in a language your system doesn't support, you reject it. If they claim a communication preference that conflicts with previous preferences, you flag the conflict and ask for confirmation. If they provide information that requires verification like medical data or financial facts, you mark it as unverified until confirmation.

Write-path detection also includes extraction confidence scoring. When your NLU system extracts a preference or fact from conversation, it provides a confidence score. Low-confidence extractions don't get stored immediately—they get queued for review or shown back to the user for confirmation. This prevents misinterpretations from contaminating your memory.

Retrieval-path detection validates that retrieved memories are relevant and current. When your memory system returns results for a query, it includes relevance scores and staleness indicators. Memories that are marginally relevant or potentially stale get flagged. The model receives these flags and can choose to ignore questionable memories or express uncertainty about them.

Retrieval-path detection also includes contradiction checking. If a retrieved memory contradicts information in the current conversation, you flag it. If the user says "I prefer phone calls" but your memory says they prefer email, you surface the contradiction to the model and ask the user which is current. This turns contradictions into refresh opportunities rather than silent contamination.

You implement detection through middleware layers. Every memory write passes through a validation middleware that applies plausibility checks, confidence thresholds, and conflict detection. Every memory read passes through a validation middleware that applies staleness checks, relevance scoring, and contradiction detection. These middleware layers are instrumented with metrics that feed into your contamination rate dashboards.

Your memory system becomes a verified system where nothing enters or exits without validation. This architecture assumes contamination will be attempted and defends against it systematically. The overhead is acceptable because the cost of contaminated memory—wrong decisions, user distrust, regulatory violations—far exceeds the cost of validation.

You're now equipped to detect when your model hallucinates memories, when your memory store gets contaminated, and when stored information becomes unreliable. The next challenge is measuring how memories degrade over time—how preferences drift, how facts become outdated, and how to maintain memory freshness as circumstances change. The next subchapter covers staleness detection and drift measurement.
