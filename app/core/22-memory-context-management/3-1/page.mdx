# 3.1 â€” Agent Memory Architecture: Plans, Progress, and Checkpoints

In mid-2025, a logistics technology company deployed an autonomous scheduling agent to optimize delivery routes across their network of 340 warehouses. The agent was responsible for creating daily routing plans, monitoring execution, and adapting when delays or cancellations occurred. The system worked flawlessly in testing with simulated scenarios that completed in minutes. In production, the first real workday took fourteen hours to complete because the agent kept restarting its planning process from scratch every time it encountered a delay. By the third hour, the agent had generated seventeen distinct routing plans, each one abandoning the previous plan's progress entirely. The warehouse operations team watched in disbelief as trucks that had already been dispatched under Plan 7 received new instructions from Plan 11, creating a cascade of confusion that cost the company $840,000 in overnight shipping fees and customer credits. The post-mortem revealed that the agent had no persistent memory of its own plans or progress. Each time it called the LLM to adapt to new information, it started fresh, treating every problem as if it were the first moment of the day.

The agent had no memory architecture. It was functionally lobotomized, unable to remember what it had decided five minutes earlier. This is the difference between an agent that works in demos and an agent that works in production. Stateless agents are unreliable at best and dangerous at worst. Stateful agents with proper memory architecture are the only agents you should deploy in environments where decisions compound over time.

## The Three Types of Agent Memory

Agent memory divides into three distinct categories, each serving a different purpose in the execution lifecycle. **Plan memory** holds the current plan and its structure: the steps the agent intends to execute, the order of execution, the dependencies between steps, and the rationale for the plan. This is not a simple to-do list. It is a structured representation of the agent's intended approach, including conditional branches, fallback strategies, and success criteria for each step. Plan memory must persist across LLM calls because the agent will need to reference it repeatedly as it executes.

**Progress memory** tracks which steps have been attempted, which have completed successfully, which are currently in progress, and which have failed. Progress memory is the agent's record of what has actually happened versus what was planned. Without progress memory, an agent cannot distinguish between a step it has not yet attempted and a step it has already tried three times and failed. Progress memory prevents the agent from repeating failed attempts indefinitely and enables it to recognize when it needs to invoke a fallback strategy or escalate to a human operator.

**Checkpoint memory** captures snapshots of the agent's state at critical junctures. A checkpoint includes the plan at that moment, the progress at that moment, the working memory contents at that moment, and the context that led to that state. Checkpoints enable recovery when something goes wrong. If the agent crashes, if the LLM API times out, if the user cancels execution mid-stream, checkpoint memory allows the agent to resume from the most recent consistent state rather than starting over. Checkpoints also enable human review and intervention. An operator can inspect a checkpoint to understand what the agent was thinking at a particular moment, then decide whether to let the agent continue or take manual control.

These three types of memory work together. Plan memory defines the intent. Progress memory tracks the reality. Checkpoint memory provides the safety net. An agent without all three is not production-ready.

## External Persistent Storage, Not Context Window Memory

The naive approach to agent memory is to keep everything in the context window and hope the LLM remembers. This fails in production for three reasons. First, context windows have token limits. An agent executing a complex multi-hour task will exceed even the largest context windows if it tries to keep the full history of every decision and result in context. Second, context window contents are ephemeral. If the process crashes or the API connection drops, the context is lost. Third, context window memory is expensive. Keeping thousands of tokens of memory in every API call multiplies your cost per decision by a factor that grows linearly with task duration.

The professional approach is external persistent storage. Plan memory, progress memory, and checkpoint memory live in a database or key-value store outside the LLM context. Each time the agent needs to make a decision, it selectively retrieves the relevant memory and injects it into the context window for that specific call. After the call completes and the agent takes an action, it updates the external memory store with the new state. This pattern decouples memory capacity from context window size, ensures durability across failures, and minimizes token costs.

The memory store design depends on the agent's task structure. For agents executing linear workflows with well-defined steps, a simple relational schema works: a table for the current plan with rows for each step, a table for progress entries with timestamps and status codes, and a table for checkpoints with serialized state blobs. For agents executing graph-based workflows with conditional branches and parallel execution, a document store or graph database provides more flexibility. The key requirement is that the store supports atomic updates. When the agent marks a step as complete or updates the plan, that write must be transactional so that concurrent processes or recovery logic see a consistent view of state.

The selective injection pattern determines what memory gets loaded into context for each decision. You do not inject the entire plan and progress history into every call. You inject the current step, the immediate dependencies, the recent progress entries relevant to the decision at hand, and the most recent checkpoint if recovery is in progress. This keeps context tight and focused while still giving the agent the information it needs to act coherently.

## Stateless Agents Versus Stateful Agents

A **stateless agent** treats every invocation as independent. It receives a prompt, generates a response, executes an action, and forgets everything. If you invoke the same agent again five minutes later with new information, it has no memory of the previous invocation. Stateless agents work for simple one-shot tasks where there is no continuity requirement. They do not work for multi-step tasks, adaptive planning, or any scenario where decisions build on prior decisions.

A **stateful agent** maintains memory across invocations. It knows what it has already tried, what worked, what failed, and what it plans to do next. Stateful agents are the only agents suitable for production use in environments where tasks take more than one LLM call to complete. The logistics company's scheduling agent was stateless despite being asked to perform a task that inherently required hours of sequential decision-making. This is professional negligence. If the task has multiple steps, the agent must be stateful.

Stateful agents require infrastructure that stateless agents do not. You need a memory store, you need logic to load and save state, you need checkpoint snapshots, and you need recovery mechanisms. This adds complexity. The complexity is not optional. It is the price of reliability. Teams that skip this complexity because they want to ship quickly end up shipping agents that work in demos and fail in production.

The transition from stateless to stateful agent architecture is not incremental. You cannot start with a stateless agent and add memory later as a feature. Memory architecture shapes the entire design: how you structure the task, how you handle errors, how you manage context budgets, how you test. You decide at the beginning whether this is a stateless one-shot agent or a stateful multi-step agent, and you build the architecture accordingly.

## Plan Memory Evolution and Adaptive Planning

Plan memory is not static. Agents adapt their plans based on results, feedback, and changing conditions. A plan that made sense at step one may be obsolete by step five. The agent's ability to evolve its plan while maintaining coherence is what separates a rigid script-follower from an adaptive problem-solver.

Plan evolution works through a structured update cycle. The agent executes a step, evaluates the result against the success criteria for that step, and decides whether to continue with the current plan or revise. If the step succeeded and the world state matches expectations, the agent proceeds to the next step with the plan unchanged. If the step failed or the world state diverged from expectations, the agent invokes a planning LLM call to generate a revised plan. The revised plan replaces the current plan in memory, but the old plan is preserved in checkpoint memory so that operators can trace the evolution of the agent's thinking.

The planning LLM call receives context that includes the original plan, the progress so far, the result of the most recent step, and the current world state. The prompt asks the LLM to generate a new plan that accounts for the new information while maintaining continuity with the work already completed. This prevents the agent from discarding valuable progress just because one step failed. If the agent successfully completed steps one through four and step five failed, the revised plan should start from step six, not step one.

Plan evolution introduces a risk of plan thrashing, where the agent revises its plan so frequently that it never makes forward progress. This happens when the agent's replanning threshold is too sensitive or when the environment is so volatile that every step reveals new information that invalidates the plan. The fix is to add plan stability constraints: the agent can only replan a maximum of N times per task, or it must execute at least M steps before it is allowed to replan again, or it must detect a significant divergence from expectations rather than replanning on every minor deviation.

Plan memory should also track the rationale for each plan version. Why did the agent choose this approach? What alternatives did it consider? What assumptions underlie the plan? This rationale is metadata stored alongside the plan structure. When the agent replans, the new rationale explains what changed and why. When operators review the agent's behavior after the fact, the rationale history provides a narrative of the agent's decision-making process. This transparency is critical for debugging and for building trust with stakeholders who need to understand why the agent made the choices it made.

## Checkpoint Memory for Recovery and Rollback

Checkpoint memory is your safety net when things go wrong. A checkpoint captures the entire state of the agent at a particular moment: the plan at that moment, the progress at that moment, the working memory at that moment, the context that led to that state, and a timestamp. Checkpoints are immutable once written. They represent a point in time that the agent can return to if needed.

Checkpoints are created at specific trigger points during execution. The most common triggers are step completion, plan revision, error detection, and time-based intervals. When the agent completes a step successfully, it writes a checkpoint before proceeding to the next step. When the agent revises its plan, it writes a checkpoint that captures the state before the revision and another checkpoint after the revision. When the agent detects an error or unexpected condition, it writes a checkpoint before attempting recovery logic. For long-running tasks, the agent writes checkpoints every N minutes regardless of progress, ensuring that even if no steps complete, there is a recent recovery point.

The checkpoint format must be serializable and deserializable. The simplest approach is JSON, but JSON has size limitations if the state includes large working memory contents. For agents with substantial state, use a binary serialization format or a hybrid approach where the checkpoint stores references to large objects in blob storage rather than embedding the objects directly. The key requirement is that you can recreate the exact agent state from the checkpoint alone, without needing external context or historical logs.

Recovery from checkpoint happens in two scenarios: crash recovery and manual rollback. In crash recovery, the agent process dies unexpectedly due to a code bug, an infrastructure failure, or a resource limit. When the agent restarts, it queries the checkpoint store for the most recent checkpoint, deserializes it, and resumes execution from that point. The agent does not re-execute steps that were already completed before the crash. It picks up where it left off. This requires that the checkpoint includes enough information to reconstruct the agent's understanding of the world state, not just its plan and progress.

In manual rollback, a human operator decides that the agent made a bad decision and should return to an earlier state. The operator selects a checkpoint from the history, and the agent loads that checkpoint and discards all subsequent state. This is a deliberate rollback to a known good state. Rollback is powerful but dangerous. If the agent has taken actions in the external world based on the discarded state, rolling back the agent's memory does not undo those actions. You cannot rewind time. Rollback is safe only when the agent's actions are idempotent or when you have compensating logic to undo side effects. For agents that control physical systems or financial transactions, rollback may not be feasible at all.

## Designing the Memory Schema for Agent Frameworks

In 2025 and 2026, agent frameworks like LangGraph, AutoGen, CrewAI, and Semantic Kernel have introduced memory management abstractions, but most of them still require you to design the schema yourself. The framework provides the plumbing to load and save state, but it does not dictate how you structure the plan, progress, and checkpoint records. You must make schema design decisions based on your task's specific requirements.

For plan memory, the schema must represent the task's control flow. If the task is a linear sequence of steps, the plan schema can be a simple ordered list with step identifiers, descriptions, success criteria, and status fields. If the task is a directed acyclic graph with conditional branches, the plan schema must include edges between steps and conditional predicates that determine which path to follow. If the task is a stateful process with loops and retry logic, the plan schema must include iteration counters and exit conditions. The schema is a data structure that mirrors the task's logical structure.

For progress memory, the schema must support querying by step, by status, and by timestamp. Common queries include: What is the current step? Which steps are complete? Which steps have failed? How long has the current step been running? The schema typically includes a progress log table with columns for step identifier, status, timestamp, result summary, and error message if applicable. Each time the agent updates progress, it appends a new row. This creates an immutable audit trail of execution history.

For checkpoint memory, the schema must support snapshot isolation. Each checkpoint is a distinct record with a unique identifier, a timestamp, and a serialized state blob. The agent can query for the most recent checkpoint, for all checkpoints within a time range, or for the checkpoint immediately before a specific event. The schema should also include metadata fields that describe the checkpoint: what triggered it, what plan version it represents, what step was active at the time. This metadata enables operators to navigate checkpoint history without deserializing every checkpoint to inspect its contents.

The memory schema must also handle concurrent access if multiple agents or agent instances are working on related tasks. If two agents are coordinating on a shared plan, they both read and write the same memory store. You need locking or optimistic concurrency control to prevent race conditions where one agent's update overwrites another agent's update. The simplest approach is pessimistic locking: an agent acquires a lock on the memory record before reading it, holds the lock while making a decision, updates the record, and releases the lock. This serializes access but limits parallelism. Optimistic concurrency control allows concurrent reads and detects conflicts at write time using version numbers or timestamps. If a conflict is detected, the write fails and the agent must retry with fresh memory state.

## Agent Frameworks and Memory Patterns in 2026

By January 2026, the leading agent frameworks have converged on a few common memory patterns. LangGraph uses a state graph model where each node in the graph can read and write to a shared state object. The state object is the plan and progress memory. LangGraph persists the state object to a database after each node execution, creating implicit checkpoints at every step. This is convenient but can be inefficient for high-frequency tasks because it writes to the database on every action. For production use, you configure LangGraph to checkpoint only at specific nodes or after N steps.

AutoGen uses a message-based memory model where agents communicate by sending messages to each other, and the message history is the memory. Progress memory is implicit in the message log. Plan memory is whatever the agent encoded in its messages. This works well for conversational agents but poorly for task-oriented agents because the plan is not first-class. You end up with agents that re-explain their plan in every message, bloating context and reducing coherence. AutoGen works best when you layer a structured memory store on top of the message log, using the messages as a communication layer but storing plan and progress in a separate schema.

CrewAI uses a task queue model where agents pull tasks from a queue, execute them, and push results back. Memory is tied to the task object. Each task has a context field that holds the plan, progress, and working memory relevant to that task. CrewAI's memory is task-scoped, which works well for parallelizable tasks but poorly for sequential tasks where context accumulates over many steps. For sequential workflows, you configure CrewAI to use a session-scoped memory store that persists across tasks in the same session.

Semantic Kernel uses a plugin-based memory model where memory is provided by memory plugins that implement a standard interface. You choose a plugin based on your persistence requirements: in-memory for ephemeral agents, Redis for distributed agents, PostgreSQL for durable agents. Semantic Kernel's abstraction is powerful but requires you to implement the memory schema yourself. The framework does not prescribe a plan or progress structure. You define it, and the memory plugin stores it.

Regardless of framework, the principles are the same. Plan memory defines intent. Progress memory tracks reality. Checkpoint memory enables recovery. External persistent storage ensures durability. Selective injection controls context costs. These are not framework features. They are architectural requirements for any stateful agent that must execute multi-step tasks reliably in production.

The logistics company rebuilt their scheduling agent with a stateful memory architecture in late 2025. They used LangGraph with PostgreSQL persistence, a plan schema that represented the routing graph as a DAG, and checkpoint triggers at every warehouse assignment decision. The agent's first production run after the rebuild completed in nine hours with zero plan restarts and zero contradictory instructions. The operations team reported that the agent's behavior was coherent and understandable. When a delay occurred, the agent adapted the plan in a way that preserved the work already done. This is what agent memory architecture enables: coherence, adaptability, and reliability in environments where decisions compound over time.

Working memory for tool-using agents presents a different set of challenges, particularly around token budgets and intermediate result management.

