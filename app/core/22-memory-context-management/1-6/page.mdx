# 1.6 — Summarization and Compression Strategies

In November 2025, a legal research assistant built for a mid-sized law firm began producing incoherent case summaries after six months of operation. The system had been designed to maintain full conversation history for each case, appending every exchange between attorneys and the AI to a growing session memory. By November, some cases had accumulated more than 200 exchanges spanning dozens of research sessions. The memory for a single case was consuming 85,000 tokens, far exceeding the 32,000-token context window of the model they were using. The engineering team had implemented a crude solution: they truncated memory at 30,000 tokens, keeping the most recent exchanges and discarding everything older. The result was catastrophic. Attorneys found that the system had forgotten critical precedents identified in early research sessions. Follow-up questions that relied on prior findings produced answers that contradicted earlier advice. One senior partner described it as "talking to someone with amnesia who confidently makes things up." The firm suspended the system in December and demanded a full rewrite.

The root cause was not that the system remembered too much. The root cause was that the team treated memory as an append-only log with no strategy for managing growth. They assumed that if memory exceeded the context window, the solution was to truncate, cutting off old information as if it had no value. This is the mistake teams make when they conflate memory storage with memory presentation. **Summarization** is the process of condensing accumulated memory into shorter, denser representations that preserve the essential information while reducing token cost and noise. It is how production systems keep memory useful as it grows beyond what can fit in a single context window. Summarization is not optional for systems with session-horizon or user-horizon memory. It is a mandatory component of any memory architecture designed to operate for more than a few hours.

## Why Summarization Is Necessary

As memory accumulates, three problems emerge. First, raw history becomes prohibitively expensive. If you are maintaining full conversation logs for 10,000 active users and each user has an average of 50 exchanges, you are storing 500,000 exchanges. If the average exchange is 300 tokens, that is 150 million tokens of memory. Injecting even a fraction of that into context windows on every request will bankrupt your API budget. Second, signal gets diluted in noise. A conversation that started with a critical piece of information and then wandered through 40 tangential exchanges is hard for the model to parse. The critical information is buried. The model must sift through noise to find signal, and models are not perfect at this. Attention is finite. The more tokens you inject, the more diffuse that attention becomes.

Third, you eventually hit hard limits. Claude 3.5 Sonnet supports a 200,000-token context window. GPT-4o supports 128,000 tokens. Gemini 2.0 Flash supports 1 million tokens. These are large windows, but they are not infinite. If your memory for a single user grows to 500,000 tokens over a year of interaction, you cannot inject all of it, even with the largest available windows. You must decide what to keep and what to discard, and summarization is the mechanism for making that decision intelligently rather than arbitrarily.

Summarization allows you to compress a large volume of past interactions into a compact representation that retains the essential facts, decisions, and context. Instead of injecting 50,000 tokens of raw conversation history, you inject a 3,000-token summary that captures the key points and then append the most recent 2,000 tokens of raw exchanges. The model sees both the high-level arc of the conversation and the immediate recent context, and it can produce coherent, informed responses. This is the pattern that works at scale.

## Rolling Summaries: Continuous Compression

The simplest and most common summarization strategy is the **rolling summary**, where you periodically summarize a window of recent memory and replace that window with the summary. The process works like this: you define a summary trigger, such as "every 20 exchanges" or "every 10,000 tokens of accumulated memory." When the trigger fires, you take the oldest unsummarized memory segment, feed it to the model with a summarization prompt, and replace that segment with the summary output. The summary is typically much shorter than the original — a 5,000-token conversation might compress to a 500-token summary. You store the summary and discard the raw segment, or you archive the raw segment to cold storage for auditing and keep only the summary in active memory.

Rolling summaries are **continuous** because they happen incrementally as memory grows, not all at once at the end of a session. This keeps your active memory size bounded. If you summarize every 20 exchanges and your summary compression ratio is 10-to-1, your active memory will never exceed the size of 20 exchanges plus the accumulated summaries. Over 100 exchanges, you will have one summary covering exchanges 1 through 20, another summary covering exchanges 21 through 40, another covering 41 through 60, another covering 61 through 80, and the raw text of exchanges 81 through 100. The total memory is approximately 20 raw exchanges plus four summaries, which is far smaller than 100 raw exchanges.

The summarization prompt is critical. You do not ask the model to "summarize this conversation." You provide specific instructions about what to preserve. A production summarization prompt for a customer support system might read: "Summarize the following customer conversation segment. Preserve all facts about the customer's issue, any commitments made by the agent, any actions taken, and any unresolved questions. Omit greetings, small talk, and procedural language. Output a concise summary in third person past tense." The specificity ensures that the summary retains what matters and discards what does not.

The failure mode of rolling summaries is that each summarization step loses information. A 10-to-1 compression ratio means you are discarding 90 percent of the original tokens. Some of that 90 percent is filler, but some of it is nuance, detail, and context that might become important later. If a user mentions a minor detail in exchange five that becomes critical in exchange 95, and that detail was omitted from the summary, the model will not have access to it. This is the **information loss problem**, and it is unavoidable with lossy compression. You manage it by tuning your summarization prompts to be conservative about what they discard, and by keeping raw memory available in cold storage so that you can retrieve it if necessary.

## Hierarchical Summaries: Multi-Level Compression

For long-lived memory that spans weeks or months, rolling summaries alone are insufficient because you eventually accumulate too many summaries. If you generate one summary per 20 exchanges and a user has 500 exchanges over six months, you have 25 summaries. At 500 tokens each, that is 12,500 tokens just for summaries, which is still large. The next level of sophistication is **hierarchical summarization**, where you summarize the summaries.

Hierarchical summarization works by creating multiple levels of abstraction. At level zero, you have raw exchanges. At level one, you have summaries of 20-exchange windows. At level two, you have summaries of five level-one summaries, each covering 100 exchanges. At level three, you have summaries of five level-two summaries, each covering 500 exchanges. Each level compresses by a factor of 5 or 10, and the result is a pyramid structure where the top level is a very short, high-level summary of the entire conversation history, and lower levels provide progressively more detail.

When you inject memory, you include the top-level summary to give the model a broad overview, then you include progressively more detailed summaries from lower levels for the most recent or most relevant time periods. For example, if a user has 500 exchanges and asks a question, you might inject the level-three summary covering all 500 exchanges, the level-two summary covering the most recent 100 exchanges, and the raw text of the most recent 10 exchanges. The model sees the big picture, the recent arc, and the immediate context, all within a manageable token budget.

Hierarchical summarization is more complex to implement than rolling summarization because you must track multiple summary levels, trigger summarization at each level independently, and manage the relationships between levels. The implementation typically involves a background job that runs periodically, checks which memory segments are eligible for higher-level summarization, generates those summaries, and updates the memory store. This is not something you run synchronously on the request path. It is asynchronous background processing.

The failure mode of hierarchical summarization is that the top-level summaries become so abstract that they lose all specificity. A summary of 500 exchanges compressed to 200 tokens will necessarily omit almost everything. The result is a summary that says "the user discussed billing issues and technical support over six months" without any actionable detail. This is why hierarchical summarization must be paired with selective retrieval. You use the top-level summary for global context, but you retrieve detailed lower-level summaries or raw memory for the specific topic the user is asking about. The hierarchy provides structure, but retrieval provides specificity.

## Importance Scoring and Selective Compression

Not all memory is equally important. A user saying "thank you" is less important than a user saying "I want to cancel my subscription." A conversation about the weather is less important than a conversation about a security incident. **Importance scoring** is the process of assigning a numeric importance value to each memory item at the time it is created, and using that score to decide what to summarize aggressively and what to preserve in detail.

Importance scores can be assigned using heuristics, classifiers, or model-based evaluation. Heuristic scoring uses rules: any message containing the word "cancel" gets a high importance score, any message containing "thanks" gets a low importance score. Classifier-based scoring uses a small classification model trained to predict importance based on message content and context. Model-based scoring uses the LLM itself: you send the memory item to the model with a prompt asking "on a scale of 1 to 10, how important is this message to retain for future reference?" and you use the model's response as the score.

Once you have importance scores, you apply selective compression. High-importance memory items are preserved in full or summarized lightly. Low-importance memory items are summarized aggressively or discarded entirely. Medium-importance items are summarized with moderate compression. This allows you to keep critical information accessible while aggressively compressing filler. The result is a memory store that is much smaller than the raw history but retains the high-value information that the model needs.

In practice, importance scoring is most effective when combined with domain-specific logic. A healthcare assistant should assign high importance to any message mentioning symptoms, medications, or allergies, and low importance to scheduling small talk. A legal assistant should assign high importance to case citations and legal arguments, and low importance to procedural confirmations. You do not use generic importance scoring. You tune it to your domain, and you validate it by sampling scored items and checking whether the scores align with human judgment.

The failure mode of importance scoring is that it introduces another layer of potential error. If your scoring logic misclassifies an important message as unimportant, that message will be over-compressed or discarded, and the model will not have access to it later. If your scoring logic over-assigns importance, you end up preserving too much and your compression gains are minimal. Importance scoring is not fire-and-forget. It is a tuned system that requires validation and iteration.

## Trigger Strategies: When to Summarize

Summarization must be triggered at the right time. If you summarize too early, you waste compute on small memory segments that do not need compression yet. If you summarize too late, you accumulate too much raw memory and blow your context budget or storage costs. Production systems use three trigger strategies: time-based, size-based, and event-based.

**Time-based triggers** summarize memory at fixed intervals. Every 24 hours, you summarize all memory that has accumulated in the past 24 hours. This is simple to implement and ensures that memory never grows unbounded for more than a day. Time-based triggers work well for systems where memory growth is steady and predictable, such as daily assistant bots or periodic report generators. The downside is that time is not always the right proxy for memory size. A user who has one exchange per day does not need daily summarization. A user who has 100 exchanges per hour does.

**Size-based triggers** summarize memory when it exceeds a token threshold. When a user's active memory exceeds 10,000 tokens, you trigger summarization to compress it back down to 2,000 tokens. This ensures that memory size stays within a bounded range regardless of interaction frequency. Size-based triggers are the most common strategy in 2026 production systems because they directly manage the metric you care about: token count. You set the threshold based on your context budget and desired compression ratio, and the system handles the rest.

**Event-based triggers** summarize memory when specific events occur. At the end of a conversation session, you summarize the entire session. When a user closes a support ticket, you summarize the ticket history. When a user completes a transaction, you summarize the transaction dialogue. Event-based triggers align summarization with natural boundaries in user activity, producing summaries that are semantically coherent and easy to interpret. The downside is that event detection is not always reliable. If your session detection logic misses a session boundary, you might summarize mid-conversation, producing a summary that cuts off in the middle of an exchange.

Most production systems use a combination of triggers. Size-based triggers provide a safety net to ensure memory never grows unbounded. Event-based triggers provide high-quality summaries at natural boundaries. Time-based triggers provide a fallback for users with low activity who would otherwise never hit size thresholds. You implement all three and let them coexist.

## Quality Risks of Summarization

Summarization is lossy, and every lossy process introduces quality risks. The three most common risks are information loss, bias amplification, and hallucinated summaries.

**Information loss** is the unavoidable consequence of compression. When you compress a 5,000-token conversation to a 500-token summary, you discard 4,500 tokens of content. Some of that content is redundant or irrelevant, but some of it is meaningful detail that might matter in future exchanges. A user mentioning a dietary restriction in passing, a customer specifying a delivery address nuance, a patient noting a medication side effect — these are the details that get lost in aggressive summarization. You mitigate information loss by tuning your summarization prompts to be explicit about what to preserve, by maintaining importance scores so high-value details are preserved in full, and by keeping raw memory in cold storage so you can retrieve it if a user later references something that was omitted from the summary.

**Bias amplification** occurs when summarization prompts or models introduce or amplify biases present in the original text. If a conversation contains subtle gender or racial bias, and the summarization prompt asks for a concise summary, the model might amplify that bias by stripping away the nuance and context that would otherwise temper it. If a customer support conversation involves a frustrated customer and a defensive agent, the summary might characterize the customer as "angry" and omit the agent's role in escalating the situation. Bias amplification is difficult to detect because summaries are shorter and harder to audit than raw logs. You mitigate it by testing your summarization process on diverse conversation samples, by comparing summaries to source material for distortion, and by incorporating fairness review into your summarization pipeline.

**Hallucinated summaries** are summaries that include information that was not present in the original text. Models sometimes generate plausible-sounding details that are inferred or confabulated rather than quoted. A conversation about a billing error might be summarized as "the customer requested a refund" when the customer actually asked for an explanation, not a refund. Hallucinations are dangerous because they corrupt your memory with false information, and that false information propagates forward into future model responses. You catch hallucinations by validating summaries against source text. Some teams use a second model call to verify the summary: they send the summary and the source text to the model with a prompt asking "does this summary accurately reflect the source text, yes or no?" If the model says no, you regenerate the summary. This doubles the cost of summarization but significantly reduces hallucination rates.

## Evaluating Summarization Quality

You cannot improve what you do not measure. Summarization quality must be evaluated continuously, and evaluation requires both automated and human components. Automated evaluation uses metrics like **compression ratio**, **ROUGE scores**, and **factual consistency scores**. Compression ratio is the ratio of source token count to summary token count. A ratio of 10-to-1 means you are compressing by a factor of ten. You track this to ensure your summarization is achieving the desired compression. ROUGE scores measure n-gram overlap between the summary and the source, giving you a proxy for whether the summary is capturing the key phrases from the original text. Factual consistency scores use a model to check whether facts in the summary are supported by the source text, catching hallucinations and distortions.

Human evaluation involves sampling summaries, reading them alongside the source material, and rating them on accuracy, completeness, and conciseness. You do this weekly or monthly, depending on your summarization volume. A human reviewer reads ten summaries and their source conversations and answers three questions: does the summary accurately reflect the source? Does the summary omit any critical information? Is the summary concise and well-written? You aggregate these ratings and track trends over time. If accuracy drops, you investigate whether your summarization prompt needs adjustment or whether your model has degraded. If completeness drops, you investigate whether your compression ratio is too aggressive.

Some teams also evaluate summarization quality indirectly by measuring downstream task performance. If users start reporting that the system is forgetting things, and you trace those reports back to summarized memory, you have a summarization quality problem. If your model's answer accuracy decreases after you deploy a new summarization strategy, you have a summarization quality problem. Downstream metrics are lagging indicators, but they are the ultimate test of whether your summarization is working.

## Progressive Compression: Adapting Compression Over Time

Memory does not age uniformly. Recent memory is accessed frequently and should be preserved in detail. Old memory is accessed rarely and can be compressed aggressively. **Progressive compression** is the strategy of applying lighter compression to recent memory and heavier compression to old memory, with compression intensity increasing over time.

The implementation works like this: when memory is first created, it is stored in full with no compression. After 24 hours, it is lightly summarized with a 3-to-1 compression ratio. After one week, it is summarized again with a 10-to-1 ratio. After one month, it is summarized again with a 50-to-1 ratio or archived to cold storage. The result is a tiered memory system where recent memory is detailed, medium-age memory is summarized, and old memory is highly compressed or archived.

Progressive compression mirrors human memory, where recent events are vivid and detailed and old events are remembered as high-level impressions. It also aligns with access patterns. Users are far more likely to reference something they said yesterday than something they said six months ago, so it makes sense to keep yesterday's memory in full and compress six-month-old memory aggressively. This reduces average memory size without sacrificing access to recent context.

The failure mode of progressive compression is complexity. You must track the age of every memory item, apply different compression strategies based on age, and manage transitions between compression tiers. This requires background jobs, state tracking, and careful testing. Progressive compression is not a starting point. It is an optimization you implement after you have a working rolling or hierarchical summarization system and you need to further reduce costs or improve quality.

## Current Approaches in 2025-2026

As of early 2026, the most common summarization strategies in production systems are rolling summaries with size-based triggers, hierarchical summaries for long-lived sessions, and importance-weighted compression. Most teams use the same model for summarization that they use for task execution — if you are using GPT-4o for your assistant, you use GPT-4o for summarization. This simplifies infrastructure and ensures that the summarization style is consistent with the model's output style.

Some teams experiment with using smaller, cheaper models for summarization. They use GPT-4o mini or Claude 3.5 Haiku for summarization and reserve the larger models for task execution. This reduces costs but introduces the risk that the smaller model produces lower-quality summaries. Early data from teams running these experiments shows that smaller models are adequate for straightforward conversations but struggle with complex, multi-threaded dialogues where nuance and context are critical. The trade-off is cost versus quality, and the answer depends on your domain and your quality bar.

A minority of teams use retrieval-augmented summarization, where the summarization prompt includes not just the text to summarize but also retrieved context from prior summaries or knowledge bases. This helps the model produce summaries that are consistent with prior summaries and that reference external knowledge appropriately. Retrieval-augmented summarization is more expensive and more complex, but it produces higher-quality summaries for knowledge-intensive domains like legal research and medical advising.

Another emerging pattern is **summary validation**, where every generated summary is checked for factual consistency before it is stored. The validation step uses a separate model call with a prompt like "compare this summary to the source text and identify any factual errors or omissions." If errors are found, the summary is regenerated with additional constraints. This doubles the cost of summarization but reduces hallucination rates by 60 to 80 percent, according to early reports from teams using this approach.

Summarization is not a solved problem. It is an active area of engineering iteration. The teams that treat summarization as a first-class concern — that measure quality, test strategies, and tune prompts — are the teams whose memory systems scale reliably. The teams that treat summarization as an afterthought are the teams whose systems collapse under memory growth or produce incoherent outputs after a few weeks of operation.

The next question is where memory lives and how it moves through your system. Memory architecture is not just about what you store and how you compress it. It is also about where you store it, how you retrieve it, and how you integrate it with the rest of your application stack. That is the topic we turn to next.
