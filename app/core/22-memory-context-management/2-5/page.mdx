# 2.5 â€” Multi-Turn Context: Managing Conversation State Across Turns

In mid-2025, a healthcare technology company deployed a clinical documentation assistant that worked beautifully in testing but degraded badly in production. The assistant helped physicians document patient encounters through natural conversation, asking follow-up questions and building comprehensive notes. In testing with scripted 5-turn conversations, the system was fast and accurate. In production, real physicians had conversations that averaged 18 turns and sometimes exceeded 40. By turn 20, the assistant would take 8 seconds to respond instead of 2, cost per conversation ballooned from 4 cents to 37 cents, and physicians complained that the system seemed to forget earlier details they had provided. The engineering team discovered their context management strategy was naive: they sent the entire conversation history with every turn. By turn 20, they were sending 52,000 tokens of conversation history plus 8,000 tokens of medical context on every request. The system was paying to re-process the same conversation turns dozens of times, and the extended context was causing the model to lose track of details mentioned 15 turns earlier. The root cause was treating multi-turn conversations as stateless when they are fundamentally stateful systems requiring deliberate memory management across turns.

Multi-turn conversations are where context management transitions from a technical challenge to a product-defining capability. Single-turn interactions are straightforward: you construct context once, send one request, return one response. Multi-turn interactions compound complexity with every exchange. Each turn adds new tokens to the history. Each turn requires access to previous context but not necessarily all of it. Each turn risks exceeding context limits if you keep everything or losing coherence if you discard too much. The conversation state grows linearly with turns but your context window is fixed. Something has to give, and what gives determines whether your assistant remains coherent and responsive across long interactions or degrades into an expensive, slow, forgetful system that frustrates users.

## The Full History Approach and Its Collapse

The simplest multi-turn strategy is full history retention: send every previous turn with every new request. This is how most teams start because it requires no sophistication. You append each user message and assistant response to a conversation array and send the entire array with each turn. For the first five turns, this works fine. Turn 1 might be 800 tokens, turn 2 adds 600 tokens for 1,400 total, turn 3 reaches 2,100 tokens, turn 4 hits 2,800 tokens, turn 5 reaches 3,600 tokens. You are well within context limits, response times are fast, costs are reasonable.

The collapse happens gradually then suddenly. Turn 10 might reach 8,000 tokens of conversation history. Turn 15 hits 15,000 tokens. Turn 20 crosses 25,000 tokens. By turn 25, you are sending 35,000 tokens of conversation history alone, before adding any system instructions, retrieved documents, or other context components. You have consumed your entire context window with conversation history, leaving no room for the knowledge the assistant needs to answer questions. Or you are using a large context model like Claude Opus with 200,000 token capacity and technically you fit, but you are paying to re-process 35,000 tokens of old conversation on every turn. If you are using GPT-4o at January 2026 pricing of 2.50 dollars per million input tokens, turn 25 costs 8.75 cents in input tokens alone just for conversation history, compared to 0.9 cents at turn 5. The cost growth is super-linear because both the per-turn token count and the number of turns being re-processed are increasing.

Beyond cost, full history retention degrades model performance in long conversations. Models excel at using recent context and struggle with distant context, even when technically within their context window. A detail mentioned at turn 3 is 22 turns away by turn 25. The model sees that detail in its context but weighted far less than recent turns. If the user references something from turn 3 at turn 25, the model may fail to connect it, producing answers that feel like the assistant forgot earlier parts of the conversation. Users describe this as the assistant having a short memory or losing the thread. The technical reality is that the model has access to the full history but is cognitively prioritizing recent context over distant context, which is actually correct behavior for most use cases but feels wrong when users expect perfect recall of everything they said.

Full history retention makes sense only for very short conversations with known turn limits. If your use case is genuinely 5 turns or fewer, send everything every time. If your conversations can grow to 15, 25, or 50 turns, full history retention is a time bomb. You will hit the wall on cost, latency, or context limits, and when you do, your options are all breaking changes. You cannot suddenly start truncating history without users noticing that the assistant forgot everything. You need a conversation state management strategy from the start, designed for the conversation lengths you expect in production, not the conversation lengths you tested in development.

## Sliding Window: Recency Bias with Hard Cutoffs

The **sliding window** approach keeps only the N most recent turns in the conversation history. If your window is 10 turns, you send turns 11 through 20 at turn 20, discarding turns 1 through 10. This caps your conversation history tokens at a predictable maximum. If each turn averages 1,200 tokens, a 10-turn window caps history at 12,000 tokens regardless of total conversation length. You can have 50-turn conversations without exceeding 12,000 tokens of history. Costs become predictable, latency stabilizes, context window pressure is controlled.

The sliding window works well for conversations where recent context is vastly more important than distant context. Customer support is a good fit: the current issue being discussed matters, the pleasantries exchanged 15 turns ago do not. Troubleshooting conversations are excellent fits: the recent diagnostic steps matter, the initial complaint description can be discarded once you have moved into solution mode. Casual chatbot interactions work fine: users do not expect the bot to remember what they said 20 turns ago. The sliding window aligns with natural human conversation, where we remember recent exchanges clearly and older exchanges fade unless particularly significant.

The sliding window fails when conversations have long-range dependencies or when users expect the assistant to remember details from early turns. A financial advisory conversation might start with the user providing their income, savings, and goals in turns 1 through 5, then spend turns 6 through 25 exploring different scenarios. If you drop turns 1 through 5 at turn 15, the assistant loses the foundational information and starts giving generic advice instead of personalized recommendations. The user says "based on my situation" at turn 25 and the assistant has no idea what their situation is because it was described 20 turns ago and has been evicted from the window. This feels like profound system failure to the user, even though it is working exactly as designed.

The window size is the critical tuning parameter. Too small and you lose important context too quickly. Too large and you do not solve the token growth problem. A 5-turn window is aggressive: conversations feel disjointed, the assistant seems to have amnesia. A 10-turn window is moderate: most conversations stay coherent, occasional long-range references fail. A 20-turn window is conservative: you still cap growth but allow substantial history. The right size depends on your conversation structure. Analyze real conversation transcripts from your domain. Identify how far back users typically reference previous exchanges. If 90% of references are within 8 turns, a 10-turn window is safe. If users routinely reference details from 15 turns ago, you need a larger window or a more sophisticated strategy.

Some teams implement adaptive windows: larger windows for shorter conversations, smaller windows as conversations grow long. At turns 1 through 10, keep everything. At turns 11 through 20, keep the last 15 turns. At turns 21 and beyond, keep the last 10 turns. This gives you full context early when the conversation is establishing shared understanding, then tightens as the conversation extends. The adaptive approach requires more implementation complexity but produces better user experience than fixed windows.

## Summarized History: Compressing the Past

The **summarized history** approach keeps recent turns verbatim and older turns in compressed form. At turn 20, you might keep turns 15 through 20 as full conversation exchanges and replace turns 1 through 14 with a 400-token summary. The model sees recent context in full detail and older context as a digest. This balances memory capacity with memory access: the assistant remembers what happened early in the conversation but not every exact phrasing and exchange.

Turn-level summarization requires deciding when to compress and how much detail to retain. A common pattern is to summarize every N turns or when conversation history exceeds a token threshold. At turn 10, if history exceeds 8,000 tokens, you trigger summarization. You take turns 1 through 5, send them to the model with a prompt that says "summarize the key information and decisions from this conversation segment," receive a 300-token summary, and replace turns 1 through 5 with that summary in your conversation state. Turns 6 through 10 remain verbatim. At turn 15, if history again exceeds 8,000 tokens, you summarize turns 6 through 10 and merge with the existing summary. You now have a single summary covering turns 1 through 10 and verbatim turns 11 through 15.

The summarization prompt is critical. Naive summarization asks the model to summarize generally and loses critical details. Effective summarization prompts specify what to retain: user-provided facts, decisions made, preferences stated, entities introduced, unresolved questions. A financial advisory conversation should retain income figures, account balances, risk tolerance, investment goals, and any decisions about allocation. It does not need to retain the exact phrasing of how the user greeted the assistant or thanked them for explanations. A healthcare documentation conversation should retain symptoms, onset dates, severity scales, medications mentioned, allergies disclosed, and examination findings. It does not need to retain the back-and-forth clarifications about terminology.

Summarized history introduces a new failure mode: lossy compression. If your summary drops a critical detail, that detail is gone forever from the conversation context. The user mentioned they are allergic to penicillin at turn 3. The summarization happens at turn 12 and the summary says "discussed medical history" without specifically noting the allergy. At turn 25, the assistant suggests a medication containing penicillin because the allergy is not in the recent verbatim context and was lost in the summary. This is a catastrophic failure in high-stakes domains. You must test your summarization prompts extensively, review sample summaries for dropped details, and potentially maintain separate high-priority fact lists that are never compressed.

Some teams use layered summarization: keep the last 5 turns verbatim, keep a medium-detail summary of turns 6 through 15, keep a high-level summary of turns 16 and older. This creates a gradient of detail where recent context is fully preserved, mid-range context is moderately compressed, and distant context is highly compressed. The model can still access information from 30 turns ago, but in abstract form. This works well for open-ended conversations where users might reference distant topics but typically care more about recent exchanges.

The cost of summarization itself must be factored in. Summarizing turns 1 through 5 requires sending those turns to the model with a summarization prompt and paying for both input and output tokens. If those turns total 4,000 tokens and the summary is 300 tokens, you spent 4,000 input tokens plus 300 output tokens to save approximately 3,700 tokens on future turns. If the conversation continues for 20 more turns, you save 74,000 tokens across those turns, making summarization cost-effective. If the conversation ends at turn 8, you spent tokens on summarization and got no benefit. Summarization makes sense for conversations expected to be long, not for conversations that might end at any turn.

## Conversation Compaction: Hybrid Strategies

**Conversation compaction** combines sliding windows, summarization, and selective retention into a unified strategy. Instead of applying one approach uniformly, you apply different strategies to different parts of the conversation history based on importance and recency. Recent turns are kept verbatim. Mid-range turns are selectively retained if they contain high-value information and discarded if routine. Distant turns are summarized. The result is a compacted conversation history that fits within token budgets while preserving the information most likely to be needed.

Compaction requires classifying turns by importance. Not all turns are equal. A turn where the user provides critical information is high-value. A turn where the assistant asks a clarifying question and the user says "yes, that's correct" is low-value. A turn where the user states a constraint or preference is high-value. A turn where the assistant provides a verbose explanation is medium-value: the conclusion matters, the reasoning can be compressed. You can implement importance scoring manually with rules or automatically with a classifier. The manual approach uses pattern matching: if the turn contains numbers, dates, entity names, or decision keywords, mark it high-value. If the turn is under 50 tokens and contains only confirmations, mark it low-value. The automatic approach sends each turn to a small model with a prompt asking whether this turn contains information that might be referenced later in the conversation.

A compaction policy might say: keep the last 5 turns verbatim always, keep any turn from the last 15 turns that is marked high-value, summarize turns 16 through 30 into a single digest, discard turns beyond 30 unless marked high-value. Applied to a 40-turn conversation, this might result in 5 verbatim recent turns, 3 verbatim high-value turns from the mid-range, 1 summary covering 15 mid-range turns, and 2 verbatim high-value turns from early in the conversation. You have compressed 40 turns down to 11 components, dramatically reducing tokens while retaining critical information across the entire conversation span.

The implementation complexity of compaction is significant. You need turn classification, policy logic, summarization triggers, and reassembly of the compacted history into a coherent prompt. You need to test extensively to ensure important information is not lost. You need to monitor production conversations for cases where users reference something that was compacted away. But the payoff is substantial: you can support genuinely long conversations without runaway costs or context exhaustion. A SaaS company running a customer onboarding assistant in late 2025 supported conversations averaging 35 turns with compaction, where a sliding window approach capped them at 20 turns and full history retention was economically infeasible beyond 15 turns.

## Maintaining Coherence When Older Context Is Summarized Away

When you summarize or discard older turns, you risk breaking conversation coherence. Coherence means the conversation feels like a continuous interaction where the assistant remembers the shared context and users do not need to repeat themselves. Losing coherence is the cardinal sin of conversation systems: users immediately notice and trust collapses. If a user says at turn 25 "what did I tell you about my budget at the start" and the assistant says "I don't have information about your budget," the conversation is broken, even if technically that information was summarized away according to a well-designed policy.

Maintaining coherence requires preserving not just facts but also the narrative thread of the conversation. Facts are explicit: the user's budget is 50,000 dollars, the user is allergic to penicillin, the user prefers evening appointments. The narrative thread is implicit: the user is price-sensitive, the user is frustrated with previous providers, the user is in exploratory mode not decision mode. Summaries that focus only on extracting facts lose the narrative thread. The assistant sees "user has 50,000 dollar budget" in the summary but does not see that the user expressed anxiety about whether that budget is sufficient, which changes how the assistant should frame recommendations.

Effective summarization preserves both facts and framing. The summary should include user sentiment, expressed concerns, stated preferences, and conversational tone, not just data points. A summary that says "user provided income of 120,000 dollars, savings of 80,000 dollars, looking to buy a home in the next 18 months, concerned about interest rates" is vastly better than a summary that says "income 120,000 dollars, savings 80,000 dollars, goal is home purchase." The second summary has the facts, the first has the context that makes the facts meaningful.

Another coherence risk is pronoun and reference resolution. Users say "it" and "that" and "the one we discussed earlier" frequently in conversation. If the referent is in the verbatim recent context, the model resolves it correctly. If the referent was in a turn that has been summarized or discarded, the model has no way to resolve it. The user says at turn 22 "let's go with that approach" referring to an approach discussed at turn 8. If turn 8 is summarized as "discussed three approaches to portfolio allocation," the model does not know which approach "that" refers to. You have broken reference resolution.

Some teams address this by maintaining a separate entity and decision register alongside the conversation history. When the assistant presents options or the user makes decisions, you log them in a structured register: "Turn 8: presented aggressive, moderate, conservative allocation approaches. Turn 9: user expressed interest in moderate approach." At turn 22 when the user says "let's go with that approach," your system can check the decision register to resolve "that approach" to the moderate allocation from turn 9, even if turn 9 itself has been compacted. This is more complex but preserves coherence across long conversations with compaction.

## Reference Resolution Across Turns When History Is Compressed

Reference resolution is the process of determining what a pronoun or reference phrase points to. In single-turn interactions, references are always within the current input. In multi-turn interactions, references can span turns. The user says "the blue one" at turn 10, referring to something presented at turn 7. If turn 7 is still in context, the model resolves it. If turn 7 has been compacted, the model cannot.

Explicit references are easier to handle. If the user says "the budget I mentioned at the beginning," you know they are referring to a specific fact from early turns. If you maintain a fact register or your summaries preserve key facts, the model can retrieve it. If the summary says "user stated budget of 50,000 dollars in initial turns" and the user asks "is my budget realistic," the model can connect the reference even without the original turn in context.

Implicit references are harder. The user says "like I said before" or "based on what we discussed" without specifying what they are referring to. The model must infer the referent from conversational context. If recent turns are about project timelines and the user says "based on what we discussed," the model infers they mean the timeline discussion. But if the conversation has covered multiple topics and recent turns are about pricing while "what we discussed" refers to feature prioritization from 10 turns ago, the model may misresolve the reference. The assistant assumes the user is referring to pricing when they meant features.

You can reduce reference resolution failures by designing your compaction strategy to preserve topic boundaries. If the conversation shifts from discussing budget to discussing features to discussing timelines, keep at least one turn from each topic segment in the compacted history. This gives the model anchors for resolving implicit references. When the user says "based on what we discussed about features," the model sees the feature segment in the compacted history and can resolve the reference even if most feature turns have been summarized.

Another approach is to use the assistant's responses to create explicit grounding. When the user provides important information, the assistant's next response should acknowledge and confirm it: "Got it, your budget is 50,000 dollars. Let's explore options within that range." This confirmation serves two purposes. First, it ensures the user and assistant have shared understanding in the moment. Second, it creates a self-contained exchange that is easier to preserve in summaries. If you later compress that exchange, the confirmation makes it clear what was discussed, even if the full user input is lost.

Some systems implement reference tracking explicitly. Each time the assistant introduces an option, entity, or concept, it is logged with a unique ID. The assistant says "I can offer you three approaches: aggressive, moderate, and conservative" and internally logs those three options with IDs. When the user later says "tell me more about the moderate one," your system uses the model to extract that the user is asking about the moderate option, looks up the ID in the reference log, and retrieves the full context for that option even if the original turn has been compacted. This is sophisticated and rare in production systems but highly effective for complex domains where users frequently reference earlier options.

## Turn Attribution and Conversation State Boundaries

In compacted conversations, you need clear turn attribution: which parts of the conversation history came from which turns. This matters for debugging, evaluation, and compliance. If the assistant gives a wrong answer at turn 25, you need to know whether the error was caused by information from turn 3, turn 12, or turn 22. If all of those turns have been merged into summaries, you cannot trace the error source.

Maintain metadata for each component in your compacted history. Tag verbatim turns with their original turn number. Tag summaries with the turn range they cover. Tag high-value preserved turns with their original position. When the model generates a response, you can log which components were in the context for that turn. If the response is wrong, you review those specific components to identify the source of the error. Without turn attribution, debugging long conversations is nearly impossible.

Turn attribution also matters for compliance. In regulated domains, you may need to prove what information was available to the model when it made a recommendation. If the model recommends a medication at turn 25 and the patient has an adverse reaction, you need to show whether the patient's allergy was in the context. If turn 3 where the patient mentioned the allergy was summarized and the summary dropped the allergy, you need to know that. If the allergy was in the summary and the model ignored it, you need to know that too. Turn attribution creates an audit trail for conversation state at every turn.

Conversation state boundaries define where one conversation ends and another begins. In customer support, each support ticket is a separate conversation. In a personal assistant, the conversation might span days or weeks with no clear end. You need to decide when to reset conversation state. Resetting too frequently loses continuity: the user has to re-explain their context every time. Never resetting leads to unbounded state growth and stale information poisoning new interactions.

A common pattern is session-based boundaries: a conversation is active for a session, typically defined by continuous interaction within a time window. If 30 minutes pass with no user input, the session ends and the next interaction starts a fresh conversation. This aligns with user expectations: if they come back hours later, they do not expect the assistant to remember the previous conversation unless explicitly resuming it. Some systems offer explicit resume: the user can say "continue our earlier conversation about X" and the system retrieves that conversation state from storage and loads it into context.

## The Cost-Coherence Tradeoff in Multi-Turn Systems

Every multi-turn conversation strategy is a tradeoff between cost and coherence. Full history retention maximizes coherence at the cost of unbounded token growth. Aggressive sliding windows minimize cost at the expense of coherence in long conversations. Summarization balances both but introduces lossy compression risks. Compaction optimizes the tradeoff but requires significant implementation complexity.

You must define your coherence requirements explicitly. What does success look like? Is it acceptable for the assistant to forget details from 20 turns ago, or must it retain everything? Can you tolerate occasional reference resolution failures, or is perfect recall required? The answer depends on your domain. In casual chatbots, users tolerate imperfect memory. In financial advisory, healthcare, or legal applications, forgetting critical details is unacceptable. Your conversation state management strategy must match your coherence requirements.

Cost sensitivity also varies by domain. A free consumer chatbot must minimize cost aggressively, even if it means shorter memory. An enterprise product sold per-seat can afford higher per-conversation costs to deliver better coherence. A usage-based pricing model must balance cost and quality to avoid surprises for customers. You need actual cost modeling: take your expected conversation length distribution, apply your conversation state strategy, calculate token usage per turn, and project monthly costs at expected volume. If the numbers are untenable, you need a more aggressive compaction strategy. If you have budget headroom, you can prioritize coherence.

The right strategy for most production systems in 2026 is conversation compaction with turn-level importance scoring and layered summarization. Keep recent turns verbatim, preserve high-value turns from mid-range, summarize distant turns with fact-preserving prompts, and discard low-value routine exchanges. This delivers strong coherence for conversations up to 30 or 40 turns, contains costs within predictable bounds, and degrades gracefully beyond that length. It requires more engineering than naive full history or simple sliding windows, but the product quality and economic sustainability justify the investment.

The next challenge is reusing context across requests, not just across turns within a conversation. Many context components are shared between conversations, and re-processing them every time is wasteful. Context caching and reuse strategies become critical for systems operating at scale, where even small per-request savings compound into substantial cost reductions.
