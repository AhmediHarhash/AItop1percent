# 6.1 â€” Evaluating Memory Relevance and Recall Accuracy

In August 2025, a customer service platform for a large telecommunications company began receiving complaints that its AI support agent was surfacing outdated information during conversations. Customers who had changed their service plans months earlier were being offered promotions for the old plans they no longer had. Customers who had resolved billing disputes were being asked about issues that no longer existed. The engineering team investigated and discovered that the memory retrieval system had high recall but terrible precision. It was retrieving memories correctly from storage, but it was also retrieving far too many irrelevant memories and ranking them poorly. The system would fetch 200 potentially relevant memories per conversation and present the top 10 to the language model for context. The problem was that memories ranked in positions 3 through 10 were frequently irrelevant, but the language model used them anyway, leading to confused and outdated responses. The team had no systematic way to measure retrieval quality. They had built evaluation datasets during initial development but had not updated them in six months. Production traffic had evolved. User intents had shifted. The memory store had grown by 40 percent. The evaluation dataset no longer represented reality. By the time the team built a new evaluation set and recalibrated the retrieval algorithm, the platform had already handled over 100,000 conversations with degraded memory quality. The root cause was not a technical failure in retrieval but a process failure in evaluation. The team treated memory evaluation as a launch requirement, not a continuous practice.

Memory systems fail in two distinct ways. They retrieve memories that are irrelevant to the current context, creating noise that confuses downstream processing. They fail to retrieve memories that are relevant, creating gaps in context that lead to repetitive questions, lost personalization, and missed opportunities. Measuring these failure modes requires a rigorous evaluation framework that goes beyond infrastructure monitoring. You must measure not just whether the retrieval system is running but whether it is returning the right memories at the right time with the right ranking. This is a fundamentally different problem than monitoring database query performance or API latency. It requires golden datasets, relevance scoring methodologies, continuous feedback loops, and the discipline to treat memory evaluation as an ongoing operational practice rather than a one-time development task.

## Precision and Recall for Memory Retrieval

Precision and recall are the foundational metrics for evaluating memory retrieval quality. Precision measures what fraction of retrieved memories are actually relevant. Recall measures what fraction of relevant memories were successfully retrieved. Both metrics are essential because optimizing for one at the expense of the other creates different failure modes. High precision with low recall means you retrieve only the most obviously relevant memories but miss important context. High recall with low precision means you retrieve everything potentially relevant but bury the signal in noise.

Precision is calculated as the number of relevant memories in the retrieved set divided by the total number of memories retrieved. If your retrieval system returns 10 memories and 7 are relevant to the current context, your precision is 70 percent. This tells you how much noise you are introducing into the downstream context. Low precision means the language model will receive irrelevant information that may lead to hallucinations, off-topic responses, or confusion. Precision failures are visible in production as strange non-sequiturs, outdated information being used, or the system referencing facts that do not apply to the current user or conversation.

Recall is calculated as the number of relevant memories retrieved divided by the total number of relevant memories that exist in the memory store. This is harder to measure because it requires knowing the ground truth of what should have been retrieved. If there are 15 memories in the store that are relevant to the current query, and your system retrieves 7 of them, your recall is 47 percent. Low recall means you are missing context that would improve the user experience. Recall failures manifest as the system asking questions it should already know the answer to, failing to personalize responses based on known user preferences, or missing cross-conversation continuity.

The trade-off between precision and recall is controlled by your retrieval threshold and ranking algorithm. If you retrieve only the top-ranked memory, you maximize precision but sacrifice recall. If you retrieve the top 50 memories, you maximize recall but sacrifice precision. The optimal operating point depends on your downstream consumption model. If the language model is sensitive to noise, you prioritize precision. If the language model is good at filtering noise but needs comprehensive context, you prioritize recall. Most production systems operate in the middle, retrieving 5 to 20 memories with a balanced threshold that achieves 60 to 80 percent precision and 60 to 80 percent recall.

You must measure precision and recall separately for different memory types and different query types. Precision and recall for retrieving user preferences may be very different from precision and recall for retrieving conversation history. Precision and recall for explicit factual queries may differ from ambient context queries. Aggregating these into a single metric obscures important failure modes. You need per-category metrics that allow you to diagnose where retrieval is strong and where it is weak.

## Relevance Scoring Methodologies

Relevance is not binary. A memory can be highly relevant, somewhat relevant, tangentially relevant, or completely irrelevant. Your evaluation framework must capture this spectrum. Binary relevance labels are easier to collect but lose important information about degree of relevance. Multi-level relevance labels provide richer signal but are harder to define consistently and more expensive to collect.

The most common relevance scale is a four-level system: highly relevant, relevant, marginally relevant, and not relevant. Highly relevant memories directly address the current query or conversation topic and should always be included in the retrieved set. Relevant memories provide useful context that improves response quality but are not strictly necessary. Marginally relevant memories have some connection to the current context but are unlikely to improve the response. Not relevant memories have no meaningful connection to the current context and should never be retrieved.

When evaluating retrieved memories, you assign each one a relevance label and calculate metrics based on these labels. Precision at highly relevant becomes the fraction of retrieved memories that were labeled highly relevant. Recall at relevant or above becomes the fraction of all highly relevant and relevant memories that were successfully retrieved. These metrics give you more nuanced insight into retrieval quality than binary precision and recall.

Relevance scoring requires human judgment. Automated relevance scoring using model-based similarity is circular reasoning. You are using one model to evaluate another model, which means you measure consistency, not accuracy. True relevance evaluation requires human raters who understand the user's intent, the conversation context, and the purpose of the memory system. These raters must be trained on your relevance definitions, calibrated against each other to ensure consistency, and refreshed regularly as the product evolves.

The challenge in relevance scoring is inter-rater reliability. Two human raters will not always agree on whether a memory is highly relevant or merely relevant. They may disagree on whether a marginally relevant memory crosses the threshold into not relevant. You must measure inter-rater agreement and establish minimum thresholds for consistency. If raters agree less than 75 percent of the time, your relevance definitions are too vague or your raters need more training. If agreement is below 50 percent, your relevance labels are essentially random, and your evaluation is not measuring anything meaningful.

You improve inter-rater reliability by providing detailed rubrics with concrete examples. Do not just define highly relevant as directly addresses the query. Define it with examples: A memory is highly relevant if it contains the specific fact being requested, describes the user's preference for the exact topic being discussed, or provides necessary context without which the response would be incomplete. Give raters positive and negative examples. Show them borderline cases and explain how to classify them. Treat relevance labeling as a skill that requires training and practice, not a task that anyone can do with a simple definition.

## Golden Sets for Memory Evaluation

A golden set is a curated collection of queries paired with the correct set of relevant memories that should be retrieved for each query. It is your ground truth for evaluation. You build golden sets by sampling real production queries, manually identifying all relevant memories for each query, and labeling them with relevance scores. The golden set becomes your regression test for memory retrieval quality. Any change to retrieval algorithms, embedding models, ranking logic, or memory storage must be evaluated against the golden set before deployment.

The size of your golden set depends on the diversity of your memory types and query patterns. A minimum viable golden set for a production memory system is 200 to 500 query-memory pairs covering the major categories of queries and memory types. Larger systems with more complex memory models need 1,000 to 5,000 pairs. The key is coverage, not volume. Your golden set must include examples of every query type, every memory type, and every edge case that you care about. A golden set with 5,000 examples that all test the same query pattern is less useful than a golden set with 500 examples that cover diverse scenarios.

You construct your golden set by stratified sampling from production logs. Identify the major query categories based on intent, topic, or user segment. Sample proportionally from each category. For each sampled query, retrieve the top 50 candidate memories using your current retrieval system. Then have human raters review all 50 memories and label each one with a relevance score. This process is labor-intensive. Labeling 500 queries with 50 candidate memories each requires reviewing 25,000 query-memory pairs. At 10 seconds per pair, this is 70 hours of human effort. You cannot shortcut this. Quality evaluation requires quality ground truth.

Golden sets decay over time. User behavior shifts. Memory stores grow. New memory types are added. Query patterns evolve. A golden set that accurately represented your system six months ago may no longer represent it today. You must refresh your golden set on a regular cadence, typically quarterly or semi-annually. Refreshing does not mean discarding the old set and building a new one from scratch. It means sampling new queries, adding them to the existing set, and retiring queries that are no longer representative. A well-maintained golden set grows slowly over time, accumulating coverage of edge cases while staying current with production patterns.

You must version your golden sets and track evaluation metrics over time. When you evaluate a new retrieval algorithm, you run it against the current golden set and compare the results to the previous algorithm's performance on the same set. When you refresh the golden set, you re-evaluate the current production system against the new set to establish a new baseline. This allows you to distinguish between changes in retrieval quality and changes in the evaluation dataset. If precision drops after a golden set refresh, it may indicate that production traffic has shifted and your retrieval system has not adapted, even if the system itself has not changed.

## Automated versus Human Evaluation of Memory Quality

Automated evaluation is fast, scalable, and continuous. Human evaluation is accurate, nuanced, and expensive. You need both. Automated evaluation runs continuously in production, measuring proxy metrics like retrieval latency, cache hit rates, and embedding similarity scores. Human evaluation runs periodically on golden sets, measuring true relevance and recall. The automated metrics give you real-time signals that something may be wrong. The human metrics tell you what is actually wrong and how bad it is.

The most common automated metric is embedding similarity between the query and retrieved memories. If you use vector embeddings for retrieval, you already have similarity scores. You can track the distribution of these scores over time. If the average similarity of retrieved memories drops, it may indicate that retrieval quality is degrading. If the variance increases, it may indicate inconsistency in retrieval. These are useful signals, but they are not the same as relevance. High embedding similarity does not guarantee relevance. Low embedding similarity does not guarantee irrelevance. Embeddings capture semantic proximity, not task-specific utility.

Another automated metric is retrieval stability. For the same query issued multiple times, does the system retrieve the same memories? If retrieval is non-deterministic or fluctuates based on small changes in query phrasing, this indicates a problem with ranking stability. Users expect consistency. If they ask the same question in two different ways, they should get memory from the same context. Measuring retrieval stability requires logging queries with their retrieved memories and comparing results across similar queries. This can be automated but requires careful normalization of queries to identify true duplicates versus semantically similar but distinct queries.

Human evaluation provides the ground truth that automated metrics approximate. You periodically sample production queries, retrieve memories using your live system, and have human raters label the relevance of each retrieved memory. You calculate precision, recall, and relevance-weighted metrics. You compare these to your golden set performance to detect drift. You compare them to previous human evaluation rounds to detect trends. This process is slow and expensive, but it is the only way to know with confidence whether your memory system is retrieving the right information.

The cadence of human evaluation depends on your rate of change. If you deploy new retrieval algorithms weekly, you need weekly or bi-weekly human evaluation to catch regressions quickly. If your system is stable and changes infrequently, monthly or quarterly evaluation may be sufficient. The key is consistency. You cannot evaluate heavily during development, then stop after launch, then resume only when users complain. Evaluation must be continuous, predictable, and built into your operational rhythm.

## Measuring Retrieval Latency Alongside Accuracy

Retrieval quality is only meaningful if retrieval completes within your latency budget. A perfectly accurate retrieval system that takes 10 seconds to return results is unusable in a conversational application. You must measure latency alongside accuracy and understand the trade-offs. Improving accuracy often increases latency. Reducing latency often decreases accuracy. Your job is to find the operating point where both are acceptable.

The relevant latency metrics are p50, p95, and p99 retrieval time. The p50 tells you the typical case. The p95 tells you the degraded but not catastrophic case. The p99 tells you the tail behavior that affects a small but meaningful fraction of requests. In most conversational systems, p95 latency matters more than p50 because users notice inconsistency. If retrieval usually takes 100 milliseconds but occasionally takes 2 seconds, the 2-second cases create a perception of unreliability.

You must measure latency separately for each stage of the retrieval pipeline. Time to generate query embedding. Time to search the vector index. Time to fetch full memory content from storage. Time to re-rank retrieved candidates. Each stage has different performance characteristics and different optimization strategies. If your p99 latency is dominated by vector search, you need to optimize indexing. If it is dominated by re-ranking, you need to optimize the re-ranking model or reduce the candidate set.

Latency and accuracy interact in complex ways. If you reduce the number of memories retrieved, latency decreases but recall may suffer. If you use a more expensive re-ranking model, accuracy improves but latency increases. If you increase the vector index size for better coverage, recall improves but search time increases. You must explicitly model these trade-offs and measure them empirically. Run experiments where you vary retrieval parameters and measure the impact on both latency and accuracy. Plot the Pareto frontier. Identify the configurations that are not dominated by any other configuration. Choose your operating point from this frontier based on product requirements.

Your latency budget must account for the entire request path, not just memory retrieval. If your total response time budget is 1 second, and the language model takes 600 milliseconds to generate a response, you have 400 milliseconds for everything else, including memory retrieval, prompt construction, preprocessing, and postprocessing. If memory retrieval consistently takes 300 milliseconds, you are leaving only 100 milliseconds for everything else, which creates fragility. A safe rule of thumb is that memory retrieval should consume no more than 20 to 30 percent of your total latency budget.

## False Positive Memories: Retrieved but Irrelevant

False positives are retrieved memories that are not relevant to the current context. They create noise in the context window, waste tokens, and can mislead the language model into generating responses based on incorrect assumptions. High false positive rates are a symptom of overly aggressive retrieval thresholds, poor ranking algorithms, or low-quality embeddings that cluster unrelated memories together.

The impact of false positives depends on the downstream consumer. If the language model is robust to noise and can ignore irrelevant context, false positives are a performance cost but not a correctness cost. If the language model is sensitive to noise and treats all retrieved context as authoritative, false positives cause hallucinations and incorrect responses. You must understand your language model's noise tolerance and calibrate your precision requirements accordingly.

You measure false positive rate as one minus precision. If precision is 70 percent, false positive rate is 30 percent. This tells you that 30 percent of retrieved memories are irrelevant. Whether this is acceptable depends on your application. A customer service system that surfaces irrelevant billing history may confuse agents. A recommendation system that includes irrelevant product preferences may reduce click-through rates. A personalization system that uses irrelevant conversation context may feel uncanny or intrusive.

Reducing false positives requires tightening retrieval thresholds, improving ranking algorithms, or using better embeddings. Tightening thresholds is the simplest approach but trades recall for precision. If you only retrieve memories with embedding similarity above 0.8 instead of 0.7, you reduce false positives but also reduce the total number of memories retrieved, which may hurt recall. Improving ranking algorithms requires training or tuning a re-ranker that uses more features than embedding similarity. Better embeddings require fine-tuning your embedding model on domain-specific data or switching to a higher-quality base model.

You should track false positive rate separately by memory type and query type. False positives in user preference retrieval may be less harmful than false positives in factual retrieval. False positives in ambient context queries may be more tolerable than false positives in explicit question-answering queries. By segmenting your false positive analysis, you can prioritize which categories to optimize first.

## False Negative Memories: Existed but Not Retrieved

False negatives are relevant memories that exist in the memory store but were not retrieved. They create gaps in context that lead to poor user experiences. The system asks questions it should already know the answer to. It fails to personalize responses based on known user preferences. It repeats information the user has already provided. High false negative rates indicate that your retrieval system has low recall.

False negatives are harder to detect than false positives because they require knowing what should have been retrieved but was not. In production, you do not have ground truth for every query. You rely on periodic human evaluation using golden sets to estimate false negative rates. You also look for indirect signals like user frustration, repeated questions, or explicit user feedback that the system is missing context.

The impact of false negatives is often worse than false positives. Users notice when the system forgets things they have told it. They notice when personalization fails. They notice when they have to repeat themselves. False negatives erode trust in the memory system and in the AI system overall. A system that occasionally surfaces irrelevant context is mildly annoying. A system that consistently forgets important context is perceived as incompetent.

Reducing false negatives requires lowering retrieval thresholds, retrieving more candidate memories, or improving the embedding model to capture more diverse forms of relevance. Lowering thresholds increases recall but also increases false positives. Retrieving more candidates increases computational cost and latency. Improving embeddings is the most sustainable approach but requires investment in model development and fine-tuning.

You must balance false positives and false negatives based on product priorities. In customer service, false negatives are often more damaging. Users expect agents to have access to their full history. In content recommendation, false positives may be more damaging. Users have limited attention, and irrelevant recommendations waste it. The optimal balance is not universal. It is product-specific and must be informed by user research and business metrics.

## A/B Testing Memory Retrieval Algorithms

Memory retrieval algorithms evolve over time. You upgrade embedding models, tune ranking algorithms, adjust thresholds, add new memory types, and change storage backends. Each change has the potential to improve or degrade retrieval quality. The only way to know the actual impact is to run controlled experiments in production. A/B testing allows you to compare the new retrieval algorithm against the current production algorithm using real traffic and real user outcomes.

The basic A/B test structure is to route a percentage of production traffic to the new retrieval algorithm while the rest continues using the current algorithm. You measure retrieval quality metrics for both groups and compare them. If the new algorithm improves precision, recall, or latency without degrading other metrics, you roll it out to all traffic. If it improves some metrics but degrades others, you make a product decision about which trade-off is acceptable. If it degrades quality, you reject it and investigate why.

The challenge in A/B testing memory retrieval is choosing the right success metrics. Retrieval metrics like precision and recall are important, but they are proxies for user outcomes. The metrics that actually matter are user engagement, task success, user satisfaction, and downstream business metrics. Does the new retrieval algorithm lead to more successful conversations? Faster resolutions? Higher user ratings? More conversions? These are the metrics that justify deploying a change.

You must also measure whether the new retrieval algorithm changes the distribution of memory types or topics surfaced. If the new algorithm retrieves more recent memories and fewer historical memories, this may improve recency but reduce long-term continuity. If it retrieves more user preferences and fewer conversation history entries, this may improve personalization but reduce contextual coherence. These distributional shifts can be detected by analyzing the memory types and content of retrieved memories across both experiment groups.

A/B tests for memory retrieval must run long enough to capture diverse usage patterns. A test that runs for only one day may not capture weekly patterns. A test that runs during only business hours may not capture evening or weekend usage. A test with only 5 percent of traffic may not have statistical power to detect small but meaningful differences. Typical memory retrieval A/B tests run for one to two weeks with 10 to 50 percent of traffic allocated to the treatment group.

You must have automated systems for detecting and rolling back bad experiments. If the new retrieval algorithm causes latency spikes, error rate increases, or user satisfaction drops, you need to detect this within hours and automatically revert to the previous algorithm. Waiting for weekly review cycles is too slow. Memory quality regressions can affect thousands or millions of users in a short time. Your experimentation platform must include real-time monitoring, automated anomaly detection, and circuit breakers that disable experiments that degrade key metrics.

## Building Evaluation Datasets from Production Logs

The best evaluation datasets come from production traffic because they represent real user queries, real conversation contexts, and real memory stores. Building evaluation datasets from production logs requires sampling queries, retrieving candidate memories, and collecting relevance labels. This process is ongoing. Every month or quarter, you sample new queries, label them, and add them to your golden set.

The sampling strategy must balance representativeness and diversity. You want your evaluation dataset to reflect the distribution of production queries, but you also want it to include rare edge cases that are important for quality. A purely random sample will over-represent common queries and under-represent rare but important scenarios. You use stratified sampling to ensure coverage. Sample proportionally from each query category, but also include targeted samples of edge cases, failure modes, and newly introduced features.

You must anonymize production logs before using them for evaluation. Queries may contain personal information, sensitive topics, or proprietary data. Before you hand queries to human raters for labeling, you must strip or redact any information that should not be shared. This is particularly important if you use third-party labeling services. You cannot send raw production logs to external raters without anonymization. GDPR, HIPAA, and other privacy regulations require you to minimize data exposure.

The labeling process for production-derived evaluation data must account for context. A query in isolation may be ambiguous. The same question may have different relevant memories depending on who asked it, when they asked it, and what conversation preceded it. Your labeling interface must provide raters with enough context to make informed relevance judgments. This typically means showing the query, the user's recent conversation history, the user's profile or preferences, and the candidate memories to be labeled. Without this context, relevance labels will be noisy and unreliable.

You should also collect metadata about why certain memories were labeled as relevant or irrelevant. Free-text justifications from raters provide valuable insight into edge cases, ambiguities, and gaps in your relevance definitions. These justifications help you refine your rubrics, train new raters, and identify systematic issues in your retrieval algorithm. A memory labeled as irrelevant with the justification too old, no longer applies indicates that your recency weighting may be too weak. A memory labeled as relevant with the justification provides necessary background even though not directly mentioned indicates that your recall threshold may be too conservative.

## Memory Evaluation as a Continuous Process

Memory evaluation is not a phase of development. It is not a task you complete before launch and then forget about. It is a continuous operational process that must be embedded into your development and deployment workflows. You evaluate before every significant change. You evaluate after every deployment. You evaluate on a fixed cadence regardless of whether anything changed. You treat evaluation as a first-class operational responsibility alongside monitoring, incident response, and capacity planning.

The continuous evaluation workflow includes automated regression testing, periodic human evaluation, and real-time production monitoring. Automated regression testing runs your golden set evaluation against every code change before it reaches production. If precision or recall drops below acceptable thresholds, the deployment is blocked. Periodic human evaluation runs monthly or quarterly to refresh golden sets, measure absolute quality, and detect drift. Real-time production monitoring tracks proxy metrics like latency, error rates, and embedding similarity distributions to detect anomalies as they happen.

You must allocate dedicated resources to evaluation. This is not work that happens in someone's spare time. It requires people, infrastructure, and budget. People to design evaluation methodologies, collect labels, analyze results, and communicate findings. Infrastructure to run automated evaluations, store golden sets, and integrate evaluation into CI/CD pipelines. Budget for human labeling, either internal raters or external labeling services. Memory evaluation for a production system costs tens of thousands to hundreds of thousands of dollars per year depending on scale and complexity. This is not optional. It is the cost of operating a memory system responsibly.

The cultural shift required for continuous evaluation is moving from build and ship to build, measure, and improve. Many teams treat launch as the finish line. After launch, they shift to maintenance mode and only revisit evaluation when something breaks or when users complain loudly. This reactive approach guarantees degradation over time. Memory systems degrade silently. Users adapt to poor memory quality by lowering expectations, asking fewer questions, or abandoning the system. By the time the degradation is visible in user complaints, you have already lost trust and engagement. Continuous evaluation allows you to detect and fix degradation before users notice, maintaining quality as a sustainable operational practice rather than a one-time achievement.

Your evaluation practice must produce actionable insights, not just metrics. It is not enough to know that recall is 65 percent. You must know why it is 65 percent, which query types are suffering, which memory types are being missed, and what changes would improve it. Every evaluation cycle should produce a prioritized list of improvement opportunities with estimated impact. This list feeds into your roadmap and sprint planning. Evaluation becomes the evidence base for technical decisions, not just a compliance checkbox.

The next challenge in memory observability is understanding a particularly dangerous failure mode: hallucinated recall, where the system confidently retrieves or constructs memories that never existed.
