# 1.9 â€” Memory Write Policy: What Qualifies as Memory, What Never Gets Stored

In mid-2025, a financial services chatbot serving wealth management clients began storing transaction amounts as user preferences. Over three months, the system accumulated seventeen thousand memory entries, most of them noise. When a client asked "what's my usual transfer amount," the bot confidently replied with a number from someone else's account, mixing up user IDs in a deduplicated memory store that had collapsed similar-looking entries. The compliance team discovered the breach during a routine audit. The company paid $4.8 million in regulatory fines and spent nine months rebuilding trust with high-net-worth clients. The root cause was not a security failure but a conceptual one: the engineering team had never defined what qualifies as memory. They stored everything, assuming more data meant better personalization. They learned the hard way that memory is not a garbage dump for conversational debris. It is a curated store of stable, verified, user-controlled information, and the write pipeline is where you enforce that discipline.

Elite teams treat memory as two separate pipelines with different responsibilities and failure modes. The write pipeline decides what to store, how to validate it, and how to handle conflicts. The read pipeline decides when stored memory is allowed to influence outputs. Most production failures in memory systems originate in the write pipeline. You store junk, then retrieve junk confidently. You store sensitive data without consent, then leak it in a different session. You store speculative inferences as facts, then compound errors across conversations. The write policy is your first and most important line of defense. It determines what enters your memory store, and everything downstream depends on getting this boundary right.

## What Qualifies as Memory

Memory is not everything the user says. It is not everything the model infers. Memory is stable information that improves future interactions and that the user expects the system to remember. This definition has three requirements: stability, utility, and user expectation. Stability means the information is unlikely to change frequently. A user's name is stable. A user's preference for formal tone is stable. A user's current location during a single trip is not stable. Utility means the information makes future interactions better. Remembering a user's dietary restrictions improves restaurant recommendations. Remembering that a user asked about weather in Paris yesterday does not improve today's conversation unless there is a clear continuity need. User expectation means the user would reasonably expect the system to remember this. When a user says "my name is Sarah," they expect you to remember. When a user says "I'm feeling tired today," they do not expect you to store that as a permanent mood profile.

Stable preferences are the clearest category of valid memory. Communication style preferences, accessibility needs, language preferences, domain expertise levels, recurring goals, notification preferences, privacy settings. These are facts about how the user wants to interact with your system, and they change infrequently. When a user tells you they prefer bullet points over paragraphs, that is a write. When a user tells you they are colorblind and need high-contrast mode, that is a write. When a user tells you to never use their data for training, that is a write. These preferences shape every interaction, and forgetting them degrades the user experience immediately.

Confirmed facts are the second category. These are factual statements the user has explicitly provided and that are verifiable or at least specific enough to be wrong. A user's job title, company name, team size, tech stack, project names, product names, family structure if relevant to the application domain. The key word is confirmed. If a user says "I work in healthcare," that is a confirmed fact. If the model infers from context that the user might work in healthcare, that is speculation and does not qualify for memory. Speculation goes into session context, not long-term memory. You store what the user told you, not what you guessed.

Explicit user requests to remember something are the third category, and they override your heuristics. When a user says "remember that I hate cilantro," you write that even if your system does not normally track food preferences. When a user says "save this for later," you write it. When a user says "don't forget I'm left-handed," you write it. The user is explicitly invoking the memory contract. They are telling you this information is important to them, and you are obligated to honor that. Some systems implement explicit memory commands: "remember this," "forget that," "what do you know about me." These commands make the write boundary explicit and give users control, which is both a usability win and a compliance advantage under GDPR and the EU AI Act's transparency requirements.

Entity extraction for work context is a fourth category common in professional tools. When a user mentions a codebase name, a bug tracker ID, a customer name, a project codename, your system may choose to store these as structured entities if they recur and if storing them improves future work. This is domain-specific. A coding assistant benefits from remembering repository names and file paths the user works with frequently. A legal research assistant benefits from remembering case names and statute references. A customer support assistant benefits from remembering product SKUs and common issue types. The heuristic is recurrence plus relevance. If the user mentions the same entity three times across different sessions, and that entity is central to their work, it qualifies. If they mention it once in passing, it does not.

## What Never Gets Stored

Sensitive data without explicit consent is the first exclusion, and it is non-negotiable. Passwords, API keys, credit card numbers, social security numbers, health information, biometric data, credentials of any kind. Even if the user pastes a password into the chat, you do not store it. You process it if the task requires it, then discard it immediately after the session ends. Some systems implement runtime filters that detect and redact sensitive patterns before they reach the memory write pipeline. Regular expressions for credit card numbers, API key formats, email-password pairs. This is not perfect, but it catches the obvious cases and prevents catastrophic leaks.

Personal information that triggers regulatory obligations is the second exclusion unless your system is specifically designed and compliant for it. Under GDPR, storing personal data requires a lawful basis, and "we thought it would be helpful" is not a lawful basis. If your memory system stores user names, email addresses, phone numbers, physical addresses, you need consent, a privacy policy that covers memory, and a data retention policy that specifies how long you keep it and how users can delete it. Most general-purpose assistants avoid this entirely by not storing PII in long-term memory. They store preferences and work context, but not identity attributes. If you do store PII, you need a compliance review, data encryption at rest, access controls, audit logs, and a process for handling data subject requests.

Volatile or temporary information is the third exclusion. The user's current emotional state, transient context like "I'm at the airport," time-sensitive information like "I have a meeting in ten minutes," speculative plans that have not been confirmed. These belong in session context, not long-term memory. Writing them to memory creates noise and leads to embarrassing errors. A user who said "I'm thinking about switching jobs" in March does not want that surfaced in July when they are happily settled in a new role. A user who said "I'm having a bad day" does not want that treated as a permanent mood baseline. The test is durability: if this information is likely to be obsolete in a week, do not write it to long-term memory.

Speculative inferences are the fourth exclusion, and this is where many teams fail. The model infers from the user's questions that they might be a software engineer. That is speculation. The model infers from the user's tone that they prefer direct communication. That is speculation. Speculation is useful within a session. It helps the model adapt its responses in real time. But it should not be written to long-term memory unless confirmed. If you want to store an inference, you surface it to the user first: "It seems like you prefer concise answers. Should I remember that?" If the user confirms, you write it. If not, you discard it. This prevents compounding errors where a wrong inference becomes a permanent fact, then influences future inferences, then creates a false user profile that diverges further from reality with every interaction.

Third-party information about other people is the fifth exclusion unless your system is designed for CRM or collaboration. If a user says "my manager Lisa prefers email over Slack," you are now storing information about Lisa without her consent. If your system is a personal assistant, you probably should not write that. If your system is a team collaboration tool where Lisa is also a user, you might write it, but you need to think through the privacy model. Who can see that memory? Just the user who wrote it, or everyone on the team? If Lisa contradicts it, whose version wins? Most systems sidestep this by only storing first-party memory: information the user provides about themselves, not about others.

## Write Validation: Deduplication and Conflict Checking

Deduplication prevents memory bloat and ensures that updates replace old information instead of accumulating duplicates. When a user says "I prefer Python," and you already have a memory entry that says "prefers JavaScript," you have a conflict. Deduplication logic decides whether to replace, merge, or flag for review. Simple deduplication uses semantic similarity. Before writing a new memory, you retrieve the top five most similar existing memories. If any of them have a cosine similarity above 0.9 on the same attribute, you treat it as an update, not a new write. You replace the old entry with the new one, preserving the timestamp and incrementing an update counter. This prevents the same preference from being stored ten times with slight wording variations.

Conflict checking is more sophisticated. Some attributes are single-valued: a user has one name, one primary language, one job title at a time. If you already have a value and a new write contradicts it, you need a resolution policy. The simplest policy is last-write-wins: the new value replaces the old one, and you log the change. This works for preferences. If a user says "I prefer formal tone" on Monday and "I prefer casual tone" on Friday, the Friday preference wins. But last-write-wins is dangerous for factual information. If you stored "works at Google" and the user casually mentions "when I was at Google," you should not overwrite the current employer. You need temporal reasoning. Past tense suggests historical information, present tense suggests current information. Some systems maintain versioned memory with effective dates: "worked at Google from 2022 to 2024," "works at Microsoft from 2025 onward." This requires more storage and more complex retrieval logic, but it prevents data loss and supports better temporal reasoning.

Multi-valued attributes are easier. A user can have multiple hobbies, multiple programming languages, multiple projects. New writes append rather than replace. But you still deduplicate within the list. If the user says "I like hiking" three times across different sessions, you store one entry, not three. You might increment a confidence score or a mention counter, but you do not duplicate the value. Some systems use set semantics: memory is a set of facts, and adding a fact that already exists is a no-op. This prevents redundancy but loses information about recurrence, which can be useful for ranking relevance during retrieval.

Confidence thresholds filter low-quality writes. Not every statement the user makes is worth storing. If the model's confidence in the extraction is below 0.7, you discard it. If the statement is ambiguous, you discard it. If the statement is a question rather than a declaration, you discard it. Some systems implement a two-stage write: low-confidence extractions go into a staging area, and after three confirmations across different sessions, they graduate to permanent memory. This reduces noise but adds complexity. The tradeoff depends on your domain. High-stakes applications like healthcare cannot afford false positives in memory, so they use high thresholds and require explicit confirmation. Consumer applications tolerate more noise in exchange for smoother UX, so they use lower thresholds and rely on users to correct errors.

## Sanitization and Format Normalization

Sanitization removes content that should not persist even if the user provided it. Profanity filters, hate speech detection, PII redaction. If a user says "remember my credit card ends in 1234," your sanitization layer strips the number before the write. If a user says something that violates your content policy, you do not store it, and you may surface a warning. Sanitization also normalizes formatting. Users provide information in messy natural language. "I live in NYC," "I'm in New York," "I'm based in New York City" are all the same fact. Your write pipeline normalizes these to a canonical form: "location: New York City." This improves retrieval precision and reduces redundancy.

Entity resolution is part of sanitization for structured domains. If your memory schema includes entity types like companies, products, locations, you resolve user-provided text to canonical entity IDs. "I work at OpenAI" resolves to a company entity with a stable ID, so that future mentions of OpenAI in different phrasings all link to the same entity. This requires an entity database or API, but it dramatically improves memory coherence. Instead of storing raw text snippets, you store structured facts with entity links, which support richer queries and better conflict detection.

Format normalization also applies to dates, times, quantities, and units. "I have a meeting next Tuesday" is not a good memory because "next Tuesday" is relative to the conversation date. Your write pipeline resolves it to an absolute date, or it does not write it at all if the task is transient. "I usually run 5k" normalizes to "running distance: 5 kilometers." "I prefer temperatures in Fahrenheit" normalizes to a unit preference attribute. This normalization happens before the write, so your memory store contains clean, queryable data, not raw conversational fragments.

## The Memory Write Contract

The write contract is the set of guarantees your memory system provides to users and to downstream components. First, you guarantee that memory is user-scoped. User A's memory never leaks into User B's context. This sounds obvious, but it requires careful implementation. User IDs must be validated on every write and every read. Multitenant systems must enforce strict partitioning. A 2025 incident at a healthcare SaaS company leaked patient preferences across accounts because the memory write pipeline used session IDs instead of authenticated user IDs, and session IDs were not securely bound to users. The fix required a full memory wipe and a painful disclosure process.

Second, you guarantee that memory is append-only or versioned, not silently mutable. If a memory entry changes, you log the change. Users can see their memory history and understand what the system knows about them. This is not just good UX, it is a compliance requirement under GDPR Article 15 and the EU AI Act's transparency obligations. Users have the right to access their data, and memory is data. You need a UI or API endpoint that shows all stored memory entries, with timestamps and sources. Some systems implement a memory dashboard where users can review, edit, and delete entries. This gives users control and reduces support burden because users fix their own memory errors instead of filing tickets.

Third, you guarantee that memory respects deletion requests immediately. When a user deletes a memory entry or requests account deletion, that data is purged from the live store within seconds and from backups within your retention policy window. You do not soft-delete and keep serving the memory. You do not wait for the next batch job. You delete it synchronously, and the next request from that user reflects the deletion. This is both a legal requirement and a trust requirement. Users who cannot delete their memory will not trust your system with sensitive preferences.

Fourth, you guarantee that memory writes are auditable. Every write logs the user ID, timestamp, source conversation ID, extracted content, confidence score, and write decision. If a memory entry causes a problem, you can trace it back to the conversation where it was created. If a user disputes a memory, you can show them the source. If regulators ask how a particular decision was made, you can reconstruct the memory state at the time of the decision. Audit logs are not optional in production memory systems. They are the foundation of accountability and debugging.

## Failure Mode: Storing Junk, Then Retrieving Junk Confidently

The most common failure mode in memory systems is garbage-in-garbage-out at scale. You store low-quality inferences, ambiguous statements, transient context, and noise. Your memory store balloons to thousands of entries per user, most of them irrelevant. Your retrieval system pulls the top ten most similar entries for every query, and now half of them are junk. The model's context window fills with noise, and output quality degrades. Users start seeing bizarre responses based on things they never said or things they said months ago in a completely different context. They lose trust, and your team spends weeks cleaning up the memory store and tightening the write policy.

The worst version of this failure mode is storing speculative inferences as facts, then using those facts to generate new inferences, compounding errors across sessions. A user asks several questions about machine learning. The system infers they are a data scientist and writes that to memory. In the next session, the system retrieves "user is a data scientist" and adjusts its tone and complexity accordingly. The user, who is actually a product manager learning about ML, finds the responses confusing and overly technical. They disengage. The system never learns the truth because the false inference is now a permanent fact, and every interaction reinforces it. Breaking this cycle requires explicit confirmation loops or periodic memory review prompts, both of which add friction but prevent runaway error accumulation.

Another version of this failure mode is storing sensitive data without realizing it. A user mentions their child's name in passing. Your system extracts it as a family member entity and writes it to memory. You now have PII about a minor without consent. A user mentions a medical condition in the context of explaining why they need flexible work hours. Your system extracts it as a health attribute and writes it to memory. You now have health information that triggers HIPAA obligations if your system operates in healthcare. These writes seem harmless in isolation, but they create legal and ethical liabilities. The fix is conservative write policies with explicit exclusion lists for sensitive entity types, and runtime filters that block writes containing PII patterns.

## Designing a Write Policy for Production

A production-grade write policy starts with explicit inclusion criteria. Define the categories of information you store: preferences, work context, entity references, explicit user requests. Define the confidence threshold for automatic writes: 0.8 or higher. Define the confirmation requirement for ambiguous or sensitive information: surface to user, require explicit approval. Define the retention period: how long does memory persist before it expires or requires revalidation. Some systems expire memory entries after six months unless the user reaffirms them. This prevents outdated preferences from persisting indefinitely.

Next, define explicit exclusion criteria. List the categories you never store: PII without consent, sensitive credentials, speculative inferences, transient context, third-party information. Implement runtime filters that catch these patterns before they reach the write pipeline. Use regex for obvious patterns, use a classifier for ambiguous cases. Log exclusions so you can audit false positives. If you blocked a legitimate write, you need to know so you can refine your filters.

Then, define conflict resolution policies for each attribute type. Single-valued attributes use last-write-wins with logging. Multi-valued attributes use set semantics with deduplication. Temporal attributes use versioning with effective dates. Numeric attributes use update-with-delta if appropriate: if a user says "I usually run 5k" and later says "I usually run 10k," you replace the old value, but you might also log the change as a trend if that is relevant to your application.

Finally, define user controls. Every memory system needs a way for users to view, edit, and delete their memory. This is not just compliance, it is quality control. Users are the ground truth. If they tell you a memory entry is wrong, they are right, and you fix it. If they delete an entry, you honor it. If they want to export their memory data, you provide it in a readable format. These controls should be accessible from your UI, not buried in settings. Memory is part of the core user experience, and users should be able to inspect and manage it easily.

The discipline you enforce in the write pipeline determines the quality of your memory system. Store only what qualifies, validate rigorously, sanitize thoroughly, and give users control. The alternative is a memory store that degrades trust instead of building it. In the next subchapter, we cover the read pipeline: when memory is allowed to influence answers, and how to balance helpfulness with safety in retrieval policies.
