# 2.2 — Context Prioritization: What Gets In and What Gets Cut

In mid-2025, a healthcare technology company deployed an AI assistant to help nurses document patient encounters. The system used GPT-4o with a 128K token context window and retrieved relevant clinical guidelines, patient history snippets, and prior encounter notes to help nurses write accurate documentation. The retrieval system returned everything that might be relevant, and the context window filled with fifteen retrieved documents, twelve prior encounter notes, and eight clinical guidelines. The system worked well until a nurse treating a diabetic patient with a complex medication history asked the assistant to generate a discharge summary. The retrieval system returned 47 documents totaling 89K tokens. The system prompt, conversation history, and user input added another 22K tokens, bringing the total to 111K tokens. The system detected that the context budget was exceeded and applied its fallback strategy: it truncated the context by removing the oldest conversation turns. Unfortunately, the oldest turns included the nurse's initial instructions specifying that the patient had a severe penicillin allergy and that all medication recommendations must avoid beta-lactam antibiotics. The assistant generated a discharge summary that included amoxicillin for a suspected respiratory infection. The nurse caught the error before the prescription was sent, but the incident triggered a safety review that grounded the system for two weeks while the team redesigned the prioritization logic. The failure was not a lack of information—the allergy was documented in the patient history. The failure was that the system cut the wrong content when the context budget was exceeded.

When you have more content than context budget, something must be removed. The question is what. Naive truncation strategies cut content based on arbitrary criteria like age, position, or size, without regard for relevance or importance. This causes systems to discard critical information while retaining irrelevant filler, degrading response quality and creating safety risks. The solution is context prioritization: a deliberate strategy for ranking content by importance and cutting the lowest-priority items first when the budget is exceeded. Prioritization ensures that the model always sees the most relevant, most critical, and most task-appropriate content, even when the full set of available content cannot fit.

## The Failure Modes of Naive Truncation

Naive truncation is any cutting strategy that ignores content relevance and applies a simple mechanical rule. The most common naive strategies are truncate-oldest, truncate-longest, and truncate-last. Truncate-oldest removes the earliest items in a sequence, such as the first conversation turns or the first retrieved documents. Truncate-longest removes the largest items, such as the longest retrieved document or the longest memory snippet. Truncate-last removes the most recent items, such as the latest conversation turn or the most recently retrieved document. All three strategies fail because they conflate order, size, or recency with importance, which is rarely true.

Truncate-oldest is the default strategy in many systems because it seems intuitive for conversation history. The assumption is that recent turns are more relevant than old turns, so when the conversation history exceeds the budget, you discard the oldest turns and keep the most recent ones. This works for casual conversations where each turn is independent, but it fails catastrophically for task-oriented workflows where early turns establish critical context. The healthcare assistant that cut the penicillin allergy warning used truncate-oldest. The nurse's first message was "Patient is a 67-year-old male with type 2 diabetes, severe penicillin allergy, and history of hospital-acquired MRSA. Generate discharge summary." That message was turn one. By turn eight, after the nurse had asked several follow-up questions about medication dosing and formatting, the conversation history exceeded the budget and the system discarded turn one. The model lost the allergy warning and generated unsafe output. The fix was to assign each turn a relevance score based on whether it mentioned allergies, contraindications, critical diagnoses, or explicit constraints, and to retain high-relevance turns regardless of age.

Truncate-longest is common in document retrieval systems. The assumption is that long documents are verbose and redundant, while short documents are concise and information-dense, so when retrieved documents exceed the budget, you discard the longest documents and keep the shortest ones. This works for news articles and blog posts, where length often correlates with fluff, but it fails for technical documents, research papers, and legal contracts, where length correlates with detail and precision. A legal AI assistant that used truncate-longest discarded a 15K token merger agreement in favor of five 2K token blog posts about merger best practices. The user asked a specific question about indemnification clauses in the merger agreement, and the assistant responded with generic advice from the blog posts because it never saw the actual contract. The fix was to prioritize documents by source type, ranking primary sources like contracts and filings above secondary sources like blog posts and summaries, regardless of length.

Truncate-last is rare but appears in systems that construct context incrementally and abort when the budget is exceeded. The assumption is that if you cannot fit all content, you stop adding content and work with what you have. This works when content is added in priority order, but it fails when content is added in arbitrary order. A research assistant that retrieved documents in order of publication date rather than relevance filled the context with recent blog posts before it could add older seminal research papers. When the budget was exceeded, the system stopped adding content, leaving the model with shallow recent content and no foundational references. The user asked a question that required understanding the theoretical basis of a research area, and the assistant provided a superficial answer based on recent popularizations. The fix was to retrieve documents in relevance order, ensuring that the most important documents were added first and the least important documents were truncated if the budget was exceeded.

Naive truncation fails because it treats all content as fungible, as if a conversation turn, a retrieved document, and a memory snippet have equivalent value and can be swapped freely. In reality, different content types have different roles, and removing critical content is worse than removing supplementary content regardless of age, size, or position. Prioritization fixes this by assigning each piece of content an importance score and cutting based on importance rather than metadata.

## Recency-Weighted Prioritization

Recency-weighted prioritization assigns higher importance to recent content and lower importance to older content, but it does so with a decay function rather than a hard cutoff. Instead of discarding all content older than a threshold, you assign a score to each item based on how recently it was created or accessed, and you cut the lowest-scoring items when the budget is exceeded. This preserves the intuition that recent content is usually more relevant while allowing older high-value content to be retained if it scores well on other dimensions.

The decay function can be linear, exponential, or step-based. A linear decay assigns a score that decreases proportionally with age: an item from one turn ago gets a score of 1.0, an item from two turns ago gets 0.9, an item from three turns ago gets 0.8, and so on. An exponential decay assigns a score that decreases rapidly at first and then levels off: an item from one turn ago gets 1.0, two turns ago gets 0.8, three turns ago gets 0.64, four turns ago gets 0.51. A step-based decay assigns the same score to items within a time window and then drops sharply: items from the last three turns get 1.0, items from four to six turns ago get 0.5, items older than six turns get 0.1. The choice depends on your use case. Linear decay works well for workflows where relevance declines steadily over time. Exponential decay works well for workflows where recent context is critical and older context is rarely useful. Step-based decay works well for workflows where relevance is stable within a session phase and drops sharply when the phase changes.

A customer support system built in late 2025 used exponential recency weighting for conversation history. Each turn was assigned a recency score based on how many turns ago it occurred, with a decay factor of 0.8. The most recent turn scored 1.0, the previous turn scored 0.8, the turn before that scored 0.64, and so on. When the conversation history exceeded the 15K token budget, the system ranked all turns by recency score and removed the lowest-scoring turns until the budget was met. This preserved recent turns while allowing older high-importance turns to be retained if they scored well on other dimensions. The system also applied a relevance multiplier: if a turn mentioned account numbers, security questions, or escalation requests, its score was multiplied by 1.5, boosting it above less critical recent turns. A turn from five turns ago that mentioned an account number scored higher than a turn from two turns ago that asked about business hours, ensuring that critical context was retained even if it was older.

Recency weighting alone is insufficient for complex workflows, but it is a useful baseline. It prevents the worst failure mode of truncate-oldest—discarding all old content indiscriminately—while maintaining the intuition that recent context is usually more relevant. It works best when combined with other prioritization dimensions like relevance, source priority, and task type.

## Relevance-Weighted Prioritization

Relevance-weighted prioritization assigns higher importance to content that is semantically or topically related to the current user query and lower importance to content that is tangential or unrelated. Instead of cutting content based on age or size, you cut content that does not help answer the question. This is the most effective prioritization strategy for retrieval-heavy systems like RAG, where the volume of potentially relevant content exceeds the context budget and the goal is to surface the most useful subset.

The relevance score is typically computed using semantic similarity between the content and the user query. You embed the query using the same model you used to embed your documents during indexing, compute the cosine similarity or dot product between the query embedding and each document embedding, and rank documents by similarity score. The highest-scoring documents are the most relevant, and the lowest-scoring documents are the least relevant. When the context budget is exceeded, you include the highest-scoring documents and discard the rest. This is the standard approach in vector search systems and works well for most use cases.

The challenge is deciding how to handle multi-component contexts where you have retrieved documents, conversation history, memory injections, and system prompts all competing for space. You cannot compute a single relevance score across all components because they serve different roles. The solution is to compute relevance scores within each component and then apply a priority ranking across components. For example, you might compute relevance scores for retrieved documents and include only the top five most relevant documents. You might compute relevance scores for memory injections and include only the top three most relevant memory facts. You might compute relevance scores for conversation history turns and include only the turns that mention entities or topics in the current query. Each component is filtered by relevance independently, and then the filtered components are combined within the total context budget.

A financial services company building an investment research assistant used relevance-weighted prioritization for retrieved documents in 2025. Their knowledge base included earnings reports, analyst notes, news articles, and regulatory filings. When a user asked "What drove the revenue growth in Q3 for the healthcare sector," the system embedded the query, computed similarity scores for all documents in the knowledge base, and retrieved the top fifty documents. The top fifty totaled 82K tokens, exceeding the 60K token budget for retrieved documents. The system ranked the documents by similarity score and included documents in order until the budget was reached. The top twelve documents, totaling 59K tokens, were included. The remaining thirty-eight documents were discarded. The response was focused and accurate because the included documents were the most relevant to the query. When the team tested a naive truncate-longest strategy, the system discarded a highly relevant 8K token earnings report in favor of less relevant 2K token news snippets, and the response quality dropped significantly.

Relevance weighting works best when your embeddings are high-quality and your queries are specific. If your embeddings are poor, similarity scores will be noisy and relevance ranking will be unreliable. If your queries are vague, many documents will have similar relevance scores and ranking will be arbitrary. In those cases, you need to combine relevance weighting with other prioritization dimensions to break ties and ensure critical content is retained.

## Source-Priority Prioritization

Source-priority prioritization assigns higher importance to content from authoritative or high-value sources and lower importance to content from speculative or low-value sources. The assumption is that not all documents are equally trustworthy or useful, and when the context budget is exceeded, you should prefer primary sources over secondary sources, official documents over informal notes, and verified content over user-generated content. This is particularly important in domains like legal, medical, and financial services, where the provenance of information affects its credibility and usability.

The source priority is typically defined as a tiered ranking. Tier one sources are authoritative and must be included if available: regulatory filings, signed contracts, clinical guidelines, certified financial statements. Tier two sources are reliable but supplementary: internal memos, analyst reports, meeting notes, technical documentation. Tier three sources are contextual but not authoritative: blog posts, news articles, user comments, forum discussions. When the context budget is exceeded, you include all tier one sources, then add tier two sources until the budget is nearly full, then add tier three sources if space remains. This ensures that the model always sees the most authoritative content, even if it means excluding large volumes of less reliable content.

A legal technology company building a contract analysis assistant used source-priority ranking to manage retrieved documents. Tier one sources were signed contracts, executed amendments, and court filings. Tier two sources were internal contract templates, negotiation memos, and legal opinions. Tier three sources were blog posts about contract best practices and general legal commentary. When a user asked a question about a specific contract term, the system retrieved all relevant documents, ranked them by source tier, and included tier one documents first. If the context budget allowed, it added tier two documents. Tier three documents were included only if space remained after all higher-priority sources were added. This prevented the system from filling the context with generic blog posts while excluding the actual contract being analyzed, which had been a recurring problem in early testing.

Source-priority ranking also applies to conversation history and memory injections. A conversation turn where the user explicitly states a constraint or requirement is higher priority than a turn where the user asks a clarifying question. A memory fact derived from a user's explicit instruction is higher priority than a memory fact inferred from usage patterns. By assigning priorities across content types and sources, you ensure that critical, authoritative, and user-specified content is retained when the budget is tight.

Source priority is orthogonal to relevance. A highly relevant blog post is still lower priority than a moderately relevant signed contract. The way to reconcile the two is to compute a composite score that combines relevance and source priority. One approach is to multiply the relevance score by a source priority multiplier: tier one sources get a 2.0 multiplier, tier two sources get a 1.0 multiplier, tier three sources get a 0.5 multiplier. A tier one document with a relevance score of 0.7 gets a composite score of 1.4, outranking a tier three document with a relevance score of 0.9, which gets a composite score of 0.45. This ensures that source priority dominates when the difference in relevance is small, but highly relevant lower-tier sources can still outrank barely relevant higher-tier sources.

## Task-Type-Specific Prioritization

Different tasks require different types of context, and prioritization strategies should reflect those differences. A summarization task needs the full source document but minimal conversation history. A question-answering task needs highly relevant retrieved documents but can tolerate truncated conversation history if the question is self-contained. A code generation task needs the code specification, relevant library documentation, and recent conversation turns, but older conversation history is rarely useful. Task-type-specific prioritization adjusts the budget allocation and cutting strategy based on what the user is trying to accomplish.

The way to implement task-type-specific prioritization is to detect the task type from the user query or conversation context and apply a task-specific prioritization profile. A summarization task profile allocates 80 percent of the context budget to the source document, 10 percent to the system prompt, and 10 percent to conversation history. A question-answering task profile allocates 60 percent to retrieved documents, 20 percent to conversation history, 10 percent to memory injections, and 10 percent to the system prompt. A code generation task profile allocates 50 percent to the code specification, 30 percent to library documentation, and 20 percent to recent conversation turns. Each profile defines which components are essential and which are expendable.

A SaaS company building a developer assistant used task-type-specific prioritization to handle varied workflows. When the user asked a question about API behavior, the system classified the task as question-answering and allocated 60 percent of the context budget to retrieved API documentation. When the user asked the system to generate a function, the system classified the task as code generation and allocated 50 percent of the context budget to the function specification and 30 percent to relevant library documentation. When the user asked the system to summarize a log file, the system classified the task as summarization and allocated 80 percent of the context budget to the log file content. This dynamic reallocation ensured that the model always had the context it needed for the specific task, even when the total volume of available content exceeded the budget.

Task detection can be rule-based, keyword-based, or model-based. Rule-based detection uses explicit signals: if the user query starts with "summarize," classify as summarization; if the user query starts with "generate," classify as code generation; if the user query ends with a question mark, classify as question-answering. Keyword-based detection uses lexical analysis: if the query mentions "summary," "overview," or "recap," classify as summarization. Model-based detection uses a lightweight classifier or few-shot prompt to classify the query into a task type. The choice depends on accuracy requirements and latency constraints. Rule-based detection is fast but brittle. Model-based detection is accurate but adds latency. Most teams start with rule-based detection and upgrade to model-based detection when the rule set becomes unwieldy.

## The Priority Stack Framework

The priority stack is a unified framework for managing context prioritization across all components. It treats context construction as a stack where each item has a priority rank, and items are added to the context in priority order until the budget is exhausted. The highest-priority items are guaranteed to be included, and the lowest-priority items are cut first when space is tight. The stack is dynamic—priority ranks change based on relevance, recency, source, and task type—but the construction logic is always the same: sort by priority, add items in order, stop when the budget is reached.

The stack starts empty. You add the system prompt, which has the highest fixed priority because it defines the model's behavior. You add the user query, which also has high fixed priority because it defines what the user wants. You then add items from other components in priority order. Retrieved documents are sorted by composite score, which combines relevance and source priority. Conversation history turns are sorted by composite score, which combines recency and importance. Memory injections are sorted by relevance to the current query. You add items one by one, incrementing the token count, until the budget is reached. Any items that do not fit are logged and discarded.

A healthcare AI assistant built in early 2026 used the priority stack framework to manage context for clinical decision support. The system prompt was 8K tokens and had fixed priority 1. The user query was variable length, up to 5K tokens, and had fixed priority 2. Retrieved clinical guidelines were ranked by relevance and source tier, with composite scores ranging from 0.1 to 2.0, and had dynamic priority based on score. Conversation history turns were ranked by recency and importance, with composite scores ranging from 0.1 to 1.5, and had dynamic priority based on score. Memory injections about patient allergies and contraindications had fixed priority 3, just below the user query. The system constructed the context by adding the system prompt, then memory injections, then the user query, then retrieved guidelines in score order, then conversation history in score order. When the context budget of 90K tokens was reached, any remaining guidelines and history were discarded. This ensured that the model always saw the system prompt, patient allergies, the current query, and the most relevant clinical guidelines, even if it meant losing older conversation turns or lower-relevance documents.

The priority stack framework makes trade-offs explicit and auditable. You can log which items were included, which were cut, and why. You can analyze whether quality degradation correlates with specific items being cut. You can adjust priority ranks based on observed outcomes. If cutting certain conversation turns causes the model to lose critical context, you increase the priority of those turns. If including certain low-relevance documents does not improve quality, you decrease their priority or exclude them entirely. The stack is a living system that evolves as you learn what matters.

## Handling Multi-Turn Dependency in Conversation History

Conversation history poses a unique prioritization challenge: turns are not independent. A user's follow-up question in turn five may reference a decision made in turn two, and cutting turn two makes turn five incomprehensible. A user's instruction in turn one may establish constraints that apply to all subsequent turns, and cutting turn one invalidates the entire conversation. Prioritizing conversation turns based solely on recency or relevance ignores these dependencies and causes coherence failures.

The solution is **dependency-aware prioritization**, where you analyze conversation turns for references, pronouns, and contextual dependencies, and you retain all turns in a dependency chain even if some of them have low individual relevance scores. A turn that references "the analysis we discussed earlier" creates a dependency on the earlier turn, and both turns must be retained. A turn that includes a pronoun like "it" or "that" creates a dependency on the antecedent, and both turns must be retained. A turn that modifies a prior instruction, such as "actually, ignore the previous constraint about time zones," creates a dependency on the prior instruction, and both turns must be retained.

A customer support system built in 2025 used dependency-aware prioritization for conversation history. The system parsed each conversation turn for references to prior turns, including pronouns, phrases like "as I mentioned," and explicit turn references like "in my first question." When a turn was identified as dependent on a prior turn, both turns were assigned a shared priority group, and the entire group was retained or discarded together. This prevented the system from cutting turn two while retaining turn five, which would have left the model with an incomprehensible query. In testing, dependency-aware prioritization reduced user-reported context loss errors by 60 percent compared to naive recency-based truncation.

Dependency detection can be rule-based or model-based. Rule-based detection uses regex patterns to identify pronouns, reference phrases, and turn numbers. Model-based detection uses a lightweight NLP model to identify coreference chains and discourse relations. Rule-based detection is fast but misses complex dependencies. Model-based detection is accurate but adds latency. Most teams start with rule-based detection and upgrade to model-based detection when false negatives become a problem.

## Monitoring Prioritization Decisions

Prioritization is a prediction problem: you predict which content will be most useful for generating a high-quality response, and you include that content while excluding the rest. Like all predictions, prioritization decisions can be wrong. A document you excluded might have contained the answer. A conversation turn you discarded might have included a critical constraint. The only way to know whether your prioritization strategy works is to monitor outcomes and correlate prioritization decisions with quality metrics.

The key metrics are inclusion rate, exclusion impact, and rank correlation. Inclusion rate measures what percentage of available content was included in the context. Exclusion impact measures how often excluded content would have improved the response if it had been included. Rank correlation measures how well your priority ranking matches the ideal ranking, which can be estimated by having humans review contexts and responses and identify which content was most useful. High inclusion rates with low exclusion impact indicate your budget is appropriately sized. Low inclusion rates with high exclusion impact indicate your budget is too small or your prioritization strategy is poor. Low rank correlation indicates your scoring function is miscalibrated.

A legal tech company monitored exclusion impact by logging all excluded documents and periodically sampling responses to see whether the answer could have been improved by including excluded content. They found that 8 percent of responses would have been more complete if a specific excluded document had been included. They analyzed those cases and discovered that the excluded documents were tier two sources with moderate relevance scores that were cut in favor of tier three sources with high relevance scores. The source priority multiplier was too weak—tier two sources needed a 1.5x multiplier instead of 1.0x. They adjusted the multiplier, retested, and saw exclusion impact drop to 3 percent.

Rank correlation is harder to measure because it requires ground truth rankings. The practical approach is to sample a set of requests, manually review the included and excluded content, and have domain experts rank the content by usefulness. You then compute the correlation between the expert rankings and your system's priority scores. Low correlation indicates your scoring function does not capture what experts consider important. High correlation indicates your scoring function aligns with expert judgment. A financial services company used this approach to validate their prioritization strategy for investment research and found a 0.72 correlation between system rankings and expert rankings, which they considered acceptable. When they tested a naive relevance-only strategy, the correlation dropped to 0.51, confirming that their composite scoring approach was better aligned with expert judgment.

Knowing how to prioritize content ensures that the model always sees what matters most, even when the full set of available content exceeds the context budget. The next challenge is managing how much context you retrieve in the first place, which requires retrieval strategy and chunking decisions that we will explore in the next chapter.
