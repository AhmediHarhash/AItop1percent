# 1.10 â€” Memory Read Policy: When Memory Is Allowed to Influence Answers

In November 2025, a customer support assistant at a SaaS company began surfacing irrelevant account history in every response. A user asking about a new feature would receive a paragraph about a billing issue from four months ago. A user asking for documentation would hear about a team member who left the company in March. The retrieval system was pulling the top eight memory entries for every query, regardless of relevance, and the model was dutifully weaving them into responses. Customer satisfaction scores dropped nineteen points in two weeks. The support team escalated to engineering, who discovered that the memory read pipeline had no gating logic. Every query triggered retrieval, and every retrieved memory influenced the output. The fix required a complete redesign of the read policy: task-type gating, relevance thresholds, recency requirements, and confidence minimums. Within a month, satisfaction scores recovered, and memory became useful again instead of annoying. The lesson was clear: writing good memory is half the problem, reading it correctly is the other half.

The read pipeline decides when memory is allowed to influence the model's output. Not every request should trigger memory retrieval. Not every retrieved memory should make it into the model's context. The read policy is a set of rules that filter and rank memory based on the task, the query, the user's context, and safety requirements. A poorly designed read policy produces two failure modes: over-retrieval, where memory drowns out the actual question, and under-retrieval, where useful context is ignored. Elite teams design read policies that balance helpfulness with safety, relevance with coverage, and personalization with task integrity.

## Task-Type Gating: When Memory Should Stay Out

Some tasks should never be influenced by memory. Factual questions, math problems, code execution, objective analysis, compliance checks. When a user asks "what is the capital of France," the answer is Paris, regardless of what is in their memory. If your memory contains a user preference for detailed explanations, you might adjust the response style, but you do not adjust the fact. When a user asks "calculate 15 percent of 240," the answer is 36, and memory is irrelevant. When a user asks "does this code have a security vulnerability," you analyze the code on its merits, not through the lens of their past preferences or projects.

Task-type gating is the first filter in the read pipeline. You classify the incoming query into task categories: factual lookup, calculation, creative generation, personalized recommendation, conversational continuation, work assistance. Each category has a memory policy. Factual lookup: no memory, or only style preferences. Calculation: no memory. Creative generation: memory allowed for style, tone, and domain preferences, but not for factual constraints. Personalized recommendation: memory required, but filtered for relevance. Conversational continuation: memory allowed if relevant to the current topic. Work assistance: memory allowed for project context, team structure, and tool preferences.

Implementing task-type gating requires a classifier or a rule-based dispatcher. The classifier takes the user query and predicts the task type. If the confidence is high, you apply the corresponding memory policy. If the confidence is low, you default to conservative: allow memory for style preferences only, not for factual content. Some systems use explicit user signals: if the user phrase includes "remember" or "you know," that is a signal to use memory. If it includes "calculate" or "what is," that is a signal to skip memory or use it minimally. These signals are not perfect, but they reduce the error rate compared to always-on retrieval.

The rationale for task-type gating is twofold. First, it prevents memory from corrupting objective tasks. If a user has a memory entry that says "prefers optimistic estimates," you do not want that influencing a financial calculation or a risk assessment. The task requires objectivity, and memory is subjective. Second, it reduces noise. Retrieving memory for every query is expensive and pollutes the context window. If memory is not relevant to the task, skipping retrieval saves tokens, reduces latency, and improves output quality by giving the model more room for task-specific reasoning.

## Relevance Thresholds: Filtering by Semantic Similarity

Even when memory is allowed for a task type, not all memory entries are relevant to the current query. Relevance thresholds filter memory based on semantic similarity between the query and the memory content. You embed the query and each memory entry, compute cosine similarity, and discard anything below the threshold. A common threshold is 0.5: memory entries with similarity below 0.5 are considered irrelevant and excluded. Higher thresholds like 0.7 reduce noise but risk excluding useful context. Lower thresholds like 0.3 increase recall but introduce more irrelevant entries.

The optimal threshold depends on your memory schema and your embedding model. If your memory entries are short and highly specific, you can use a higher threshold because relevant entries will have strong semantic overlap with the query. If your memory entries are longer and more general, you need a lower threshold because relevant information might be buried in a paragraph with lower average similarity. Some systems use adaptive thresholds: start at 0.5, and if no entries pass, lower it to 0.4, then 0.3. If more than ten entries pass, raise it to 0.6. This dynamic adjustment balances recall and precision based on the retrieval distribution.

Relevance filtering also considers query intent. A user asking "how do I configure logging" might have a memory entry about their tech stack: "uses Python, Flask, PostgreSQL." That memory has moderate semantic similarity to the query because both involve technical configuration, but the intent is different. The user is asking how to configure logging in general, not how to configure it for their specific stack. A naive retrieval system pulls the tech stack memory, and the model generates a Flask-specific answer, which might be helpful or might be confusing if the user was asking about a different project. Intent-aware retrieval requires understanding whether the query is general or contextual. Explicit signals help: "how do I configure logging in Flask" is contextual, "how do I configure logging" is general. For general queries, you apply stricter relevance thresholds or exclude memory entirely unless the user explicitly invokes it.

Relevance thresholds also apply to multi-hop reasoning. If the query is "what is the best database for my use case," and the memory contains "user is building a real-time analytics platform," the memory is highly relevant. But if the memory contains "user prefers concise answers," that is low relevance to the technical question, though it might influence the response style. Some systems separate content-level relevance from style-level relevance. Content-level memory must meet a high similarity threshold to influence factual reasoning. Style-level memory can have a lower threshold because it only affects formatting and tone. This separation prevents style preferences from leaking into technical decisions.

## Recency Requirements: Temporal Filtering

Recency requirements ensure that outdated memory does not influence current responses. Memory decays. A user's preferences change, their projects evolve, their context shifts. A memory entry from two years ago about a project that is no longer active should not surface in response to a current query unless it is explicitly historical. Recency filtering adds a time dimension to relevance. You retrieve memory entries that are both semantically similar and temporally recent.

The simplest recency policy is a sliding window: only retrieve memory from the last six months, or the last year. Anything older is archived or weighted down. This works for volatile domains like software development, where tech stacks and projects change frequently. It is too aggressive for stable domains like personal preferences or professional credentials, which change infrequently. A better approach is category-specific recency. Preferences have no recency requirement: they persist until explicitly updated. Work context has a six-month recency window: projects and tools from more than six months ago are deprioritized. Factual information has versioning: you retrieve the most recent version, but you keep historical versions for temporal queries.

Some systems implement exponential decay for memory relevance. Each memory entry has a base relevance score computed from semantic similarity. You multiply that score by a decay factor based on age: `decayed_score = base_score * exp(-lambda * age_in_days)`. Lambda is a tuning parameter. A small lambda like 0.001 means memory decays slowly, remaining relevant for years. A large lambda like 0.01 means memory decays quickly, becoming irrelevant after a few months. You set lambda based on your domain and your users' feedback. If users complain about outdated memory surfacing, you increase lambda. If they complain about useful historical context being ignored, you decrease lambda.

Recency filtering also prevents temporal confusion. A user asks "what are my current priorities," and the system retrieves a memory from eight months ago listing old priorities. The user is confused because those priorities are no longer current. The fix is to tag memory entries with temporal scope: "current as of January 2025," "valid from March 2025 onward," "historical, no longer active." When retrieving memory for a query about current state, you filter for entries tagged as current. When retrieving memory for a query about past state, you filter for entries tagged as historical. This requires more sophisticated memory tagging at write time, but it prevents a large class of temporal errors.

## Confidence Minimums: Filtering by Extraction Quality

Not all memory entries are equally reliable. Some were written with high confidence based on explicit user statements. Others were written with lower confidence based on inferred preferences or ambiguous statements. Confidence minimums filter memory based on the confidence score recorded at write time. If a memory entry was written with confidence below 0.6, you do not retrieve it for high-stakes tasks. If it was written with confidence above 0.9, you retrieve it even for sensitive tasks.

Confidence-based filtering is especially important for inferred memory. If your write pipeline allows inferred preferences to be stored after multiple confirmations, those entries have lower intrinsic confidence than explicitly stated preferences. When retrieving memory for a task that requires high reliability, you exclude inferred entries and use only explicit entries. When retrieving memory for a task where mistakes are low-cost, you include inferred entries because they improve personalization even if they are occasionally wrong.

Some systems maintain a confidence score that evolves over time. A memory entry starts with the confidence from the initial write. Each time it is confirmed by user behavior, the confidence increases. Each time it is contradicted or corrected, the confidence decreases. If confidence drops below a threshold, the entry is flagged for review or auto-deleted. This dynamic confidence scoring turns memory into a self-correcting system. Entries that prove useful over time gain confidence and are retrieved more often. Entries that prove incorrect lose confidence and are eventually purged.

Confidence minimums also apply to retrieval ranking. You retrieve the top K memory entries by similarity, but you rank them by a composite score that includes similarity, recency, and confidence. High-confidence recent entries rank higher than low-confidence old entries, even if the old entries have slightly higher semantic similarity. This ranking ensures that the most reliable and relevant memory influences the model's output, while noisy or outdated memory is deprioritized.

## The Dangers of Over-Retrieval: Memory Drowning Out the Question

Over-retrieval happens when you pull too much memory into the context, and the model focuses on the memory instead of the actual question. The user asks a simple question, and the response is a rambling synthesis of everything the system knows about them. The user asks "what is the weather today," and the response starts with "based on your preference for detailed explanations and your interest in meteorology, here is a comprehensive breakdown of today's atmospheric conditions." The user did not ask for a comprehensive breakdown. They asked for the weather. The memory drowned out the intent.

Over-retrieval also happens when memory is irrelevant but semantically similar. The user asks "how do I reset my password," and the system retrieves a memory entry about a previous password issue from six months ago. The response references the old issue, confusing the user who has no idea what the system is talking about. The memory was similar enough to pass the relevance threshold, but it was not relevant to the current task, which is a simple procedural question with a standard answer.

The fix for over-retrieval is conservative retrieval limits. Retrieve at most three to five memory entries per query, even if more entries pass the relevance threshold. Rank them by composite score, take the top few, and discard the rest. This prevents context pollution and keeps the model focused on the task. Some systems use dynamic limits based on task complexity. Simple queries get one or two memory entries. Complex queries get up to five. Conversational queries where the user explicitly references past context get up to ten. The limit adapts to the need.

Another fix is memory summarization. Instead of injecting raw memory entries into the context, you summarize them into a concise bullet list or a single paragraph. "User preferences: formal tone, prefers examples, works with Python and Flask." This summary is much shorter than the original memory entries, and it gives the model the necessary context without overwhelming the task prompt. Summarization requires an additional LLM call or a pre-computed summary stored with each memory cluster, but it reduces token usage and improves output quality.

Over-retrieval also degrades user trust. Users notice when the system brings up irrelevant information. It feels intrusive, like the system is trying too hard to prove it remembers everything. It breaks the conversational flow. Users start to distrust memory because it makes responses worse, not better. They might start asking the system to forget things or avoid mentioning personal details because they do not want them surfaced inappropriately. This is the opposite of the intended effect. Memory should feel helpful and natural, not forced and creepy.

## The Dangers of Under-Retrieval: Ignoring Useful Context

Under-retrieval happens when you are too conservative and skip memory that would have improved the response. The user asks "what should I work on next," and the system gives generic advice about prioritization frameworks, ignoring the memory entry that lists the user's current projects and deadlines. The user is frustrated because the system is not using the context it already has. They have to repeat themselves, which defeats the purpose of memory.

Under-retrieval also happens when your relevance thresholds are too strict. The user asks a question that is semantically similar to stored memory but not identical, and the memory is excluded because the cosine similarity is 0.48 instead of 0.5. The system generates a generic response, and the user thinks the system has forgotten their context. They lose trust in memory and stop providing information because it does not seem to matter.

The fix for under-retrieval is to tune your thresholds based on user feedback. If users frequently ask "do you remember when I told you X," that is a signal that you are under-retrieving. You lower your relevance threshold or increase the number of retrieved entries. If users complain about irrelevant information surfacing, you raise your threshold. This is an empirical tuning process, and the optimal settings depend on your embedding model, your memory schema, and your user base.

Another fix is to implement fallback retrieval. If the primary retrieval strategy returns zero results, you try a secondary strategy with relaxed constraints. You lower the similarity threshold, expand the time window, or include lower-confidence entries. This ensures that you retrieve something if relevant memory exists, even if it does not meet the strict criteria. Fallback retrieval should be logged and monitored. If you are using fallback frequently, it means your primary strategy is miscalibrated, and you need to adjust your thresholds.

Under-retrieval is less visible than over-retrieval because the symptom is absence, not presence. Users do not always notice when memory is missing. They just get a generic response and move on. But over time, under-retrieval erodes the value proposition of memory. If the system rarely uses stored context, users stop bothering to provide it. The memory store becomes stale and useless. You need proactive monitoring to detect under-retrieval. Track the fraction of queries that trigger memory retrieval, the fraction of retrieved memory that influences the output, and the fraction of user follow-ups that reference unaddressed context. If these metrics are low, you are likely under-retrieving.

## Read Policies for Multi-User and Team Contexts

In multi-user systems, the read policy must account for permissions and data boundaries. If User A and User B are on the same team, should User A's memory be visible to User B? Should shared project context be pooled? Should personal preferences stay private? The read policy defines these boundaries and enforces them at retrieval time.

The simplest model is strict per-user isolation. Each user has their own memory, and no cross-user retrieval is allowed. This is appropriate for consumer applications where privacy is paramount. It is too restrictive for team collaboration tools, where shared context improves productivity. A more flexible model is namespaced memory. Each user has personal memory that is private, and each team or project has shared memory that is accessible to all members. When retrieving memory, you pull from both the user's personal namespace and the relevant shared namespaces. You rank entries by relevance and deduplicate if there is overlap.

Shared memory introduces new risks. If User A writes a false or misleading entry to shared memory, it affects User B's experience. You need moderation or validation for shared writes. Some systems require approval from multiple team members before a shared memory entry is committed. Others implement reputation-based trust: entries written by users with high reliability scores are auto-approved, while entries from new users are flagged for review. This prevents malicious or accidental pollution of shared memory.

Another consideration is role-based retrieval. In an organization, different roles have different information needs. An engineer needs memory about code repositories and architecture. A product manager needs memory about user feedback and roadmap priorities. A customer success manager needs memory about account history and escalations. Role-based retrieval filters memory by role relevance. When an engineer asks a question, the system retrieves engineering-relevant memory and deprioritizes product or customer success memory. This prevents information overload and improves response precision.

## Designing a Read Policy for Production

A production-grade read policy starts with task-type classification. Define the task categories your system handles, and define the memory policy for each: no memory, style-only, filtered memory, full memory. Implement a classifier or rule-based dispatcher that applies the correct policy based on the query. Log classification decisions so you can audit errors and refine the classifier over time.

Next, define relevance thresholds and recency windows for each memory category. Preferences have no recency window and a moderate relevance threshold. Work context has a six-month recency window and a high relevance threshold. Factual memory has versioning and a high confidence minimum. Explicit user requests to remember something have the highest priority and bypass most filters. Tune these parameters based on user feedback and retrieval metrics.

Then, define retrieval limits and ranking logic. Retrieve at most five entries per query, ranked by a composite score of similarity, recency, confidence, and user priority. If the user explicitly references past context, increase the limit to ten. If the task is simple and factual, decrease the limit to two or use no memory at all. Implement summarization if raw memory entries are too verbose for the context window.

Finally, implement safety checks on retrieved memory. Before injecting memory into the model's context, scan for PII, sensitive data, or policy violations. If you detect a problematic entry, exclude it and log the event. This is a defense-in-depth measure. Ideally, your write pipeline prevents bad data from entering memory, but the read pipeline provides a second layer of protection in case something slipped through.

Monitor retrieval metrics continuously. Track the fraction of queries that trigger retrieval, the average number of entries retrieved, the average similarity score, the fraction of retrieved entries that are recent versus old, and the fraction of user feedback that indicates memory errors. These metrics tell you whether your read policy is working. If retrieval rates are too low, you are under-retrieving. If similarity scores are too low, your thresholds are too loose. If users frequently correct or contradict retrieved memory, your confidence minimums are too low. Use these signals to iterate on your read policy and keep it aligned with user needs.

The read policy is the gatekeeper between stored memory and live reasoning. It determines what context the model sees, and therefore what responses the user gets. A well-designed read policy makes memory feel seamless and helpful. A poorly designed read policy makes memory feel intrusive and broken. The discipline you enforce in the read pipeline determines whether memory is an asset or a liability. In the next subchapter, we cover memory decay and refresh strategies: how to handle aging information and keep memory current without user intervention.
