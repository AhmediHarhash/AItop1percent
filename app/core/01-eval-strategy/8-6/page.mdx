# 8.6 — Multi-Turn & Stateful Evaluation

A friend once told me about his first serious relationship. "The first date was perfect," he said. "Great conversation, lots of chemistry. But by date five, we were running out of things to talk about. By date ten, I realized we had completely different values." Single interactions lie. Extended interactions reveal truth.

Your agent aces the demo. Beautiful first response. Then a customer uses it for twenty turns, and it forgets what they said in turn three, contradicts itself in turn twelve, and completely abandons the original goal by turn eighteen. You just learned what every dating app knows: chemistry over one interaction tells you nothing about compatibility over twenty.

Most evaluation frameworks test single-turn quality. Your agent gets a question, produces an answer, and you score it. That's not how real agents work. Real agents operate across conversations—five turns, ten turns, fifty turns. They maintain state, remember context, track goals across multiple exchanges. If you only test single turns, you're flying blind.

**Multi-turn evaluation** tests agent performance across extended conversations. **Stateful evaluation** tests whether the agent correctly manages information, context, and goals over time. This chapter shows you how to build both.

---

## Why Single-Turn Evaluation Misses Agent Reality

You test a customer service agent on a hundred individual questions. Ninety-five percent accuracy. Ship it. Within a week, customers are furious. The agent can't remember what they said five turns ago. It asks for the same information twice. It starts solving a billing problem, then inexplicably switches to discussing product features.

Your single-turn eval missed the entire reality of how the agent gets used.

**Single-turn evaluation assumptions that break for agents:**

- Each interaction is independent (false—customers expect continuity)
- Context doesn't matter beyond the current turn (false—previous turns shape current expectations)
- Goals are achieved in one exchange (false—most agent tasks require multiple turns)
- State management isn't part of quality (false—it's often the primary quality driver)

Real agent interactions have **conversational dependencies**. Turn five only makes sense in the context of turns one through four. The quality of turn twelve depends on whether the agent remembered turn three. You can't evaluate these interactions one turn at a time any more than you can evaluate a movie by watching random scenes.

**What single-turn eval misses:**

- **Context retention failures** — agent forgets critical information from earlier turns
- **State corruption** — agent mixes up information between conversations or users
- **Goal drift** — agent starts with one objective, abandons it halfway through
- **Contradiction accumulation** — agent says X in turn two, not-X in turn eight
- **Recovery capability** — agent hits an error in turn six, can't recover by turn ten
- **Progressive complexity** — task difficulty increases over turns, agent fails at scale

By 2026, this isn't theoretical. Every major agent platform—OpenAI Assistants, Anthropic Claude with extended context, LangChain agents—operates statelessly across turns. The framework maintains conversation history, but it's your job to design state management. If you're not testing multi-turn behavior, you're not testing your agent.

---

## State Management Evaluation

Your agent is helping a customer troubleshoot their account. Turn one: customer provides their account ID. Turn three: customer mentions they're on the premium plan. Turn seven: agent asks for their account ID again. Customer rage-quits.

**State management evaluation** tests whether your agent correctly remembers, updates, and uses information across turns.

**Three categories of state to test:**

**1. Context retention** — does the agent remember information from previous turns?

Test by providing information early in the conversation, then requiring it later. The agent should retrieve and use it without asking again.

Example test pattern:

```yaml
turn_1:
  user: "My order number is 12345"
  agent: "Got it, looking up order 12345..."
turn_5:
  user: "When will it arrive?"
  agent_must_reference: "12345"  # Should not ask for order number again
  failure_if: "what.*order number|which order"
```

**2. State updates** — does the agent correctly update its understanding as new information arrives?

Test by providing information that supersedes earlier information. The agent should use the latest state, not outdated data.

Example: Customer says they're on the basic plan (turn two), then upgrades to premium mid-conversation (turn six). Turn eight should reference premium features, not basic limitations.

**3. State consistency** — does the agent maintain coherent state across turns?

Test by checking for contradictions. If the agent determines fact X in turn three, it should not contradict X in turn nine unless the user explicitly corrected it.

**Common state management failures:**

- **Recency bias** — agent only remembers the last 2-3 turns, forgets earlier context
- **State leakage** — information from one conversation bleeds into another (catastrophic for multi-user systems)
- **Partial updates** — agent updates some state but not related dependent state
- **Context window overflow** — conversation exceeds context window, agent loses early state
- **Hallucinated state** — agent "remembers" things the user never said

Build test cases that explicitly check each failure mode. For state leakage, run two conversations in parallel and verify information doesn't cross-contaminate. For context window overflow, design conversations that intentionally exceed your model's context limit and test degradation behavior.

**Measurement approach:**

Track **state accuracy** per turn—what percentage of previously-established facts does the agent correctly retain and use? A proper multi-turn eval should score state accuracy separately from response quality.

Turn-by-turn state scorecard:

- Turn 1: 0 facts established (baseline)
- Turn 3: 4 facts established, agent must maintain all 4
- Turn 7: 8 facts established, agent must maintain all 8
- Turn 10: State accuracy = facts correctly retained / facts established

If your agent drops below 90% state accuracy past turn five, you have a retention problem.

---

## Conversation Coherence

Your agent starts turn one discussing a refund. By turn four, it's recommending products. By turn eight, it's asking about the weather. The user never changed topics. Your agent has **topic drift**.

**Conversation coherence** measures whether the agent maintains a consistent narrative and goal across turns. Humans expect conversations to have **thematic continuity**—topics flow logically, goals persist, and transitions make sense.

**Three coherence failures to test:**

**1. Topic drift** — agent wanders off the original subject without user prompt

Caused by: overly broad agent instructions, poorly scoped tool calls, retrieval systems returning tangentially related content.

Test: Establish a clear goal in turn one. Verify every subsequent turn advances that goal or directly responds to user redirection. Flag any turn where the agent introduces a new topic unprompted.

**2. Goal abandonment** — agent stops pursuing the stated objective

Example: User asks to "find the cheapest flight to New York." Agent shows flights in turn two, then starts discussing hotels in turn four without completing the flight search.

Test: Define explicit goals at conversation start. Score each turn for goal progress—advancing, maintaining, or abandoning. Conversations should end with goal completion or explicit user pivot.

**3. Contradiction** — agent says X in turn A, contradicts X in turn B

Example: Agent says "your account has a $50 credit" in turn three, then says "you have no credits" in turn nine.

Test: Extract factual claims per turn. Check for logical contradictions across the conversation. This is hard to automate—2026 standard is to use a second LLM as a consistency checker.

**Coherence scoring rubric:**

For each conversation, score 1-5:

- **5 — Excellent coherence:** Clear goal progression, no drift, no contradictions, natural transitions
- **4 — Good coherence:** Minor tangents, but user can redirect easily, goal stays in focus
- **3 — Acceptable coherence:** Some drift, occasional contradictions, goal is completed but path is messy
- **2 — Poor coherence:** Significant drift, multiple contradictions, goal completion questionable
- **1 — Failed coherence:** Complete topic abandonment, contradictions prevent task completion

This maps to the multi-turn scoring approach from Chapter 2.6—you're applying conversation-level rubrics, not turn-level grades.

**Enterprise expectation:** For customer-facing agents, coherence scores below 4.0 trigger user frustration. For internal tools, teams tolerate 3.5+ if the agent eventually completes tasks. You need baseline coherence metrics before launch.

---

## Turn-Level vs Conversation-Level Metrics

You score every turn individually. Average score: 4.2 out of 5. Great, right? Then you read actual conversations and realize the agent delivers high-quality responses that, taken together, make no sense. You optimized the wrong metric.

**Turn-level metrics** measure quality per individual response. **Conversation-level metrics** measure quality of the entire interaction. Both matter. You need both.

**Turn-level metrics:**

- Response accuracy per turn
- Latency per turn
- Tool call success rate per turn
- Hallucination rate per turn

These are easy to measure—score each turn independently, aggregate. But they miss the forest for the trees.

**Conversation-level metrics:**

- **Goal completion rate** — did the conversation achieve the user's objective?
- **Total conversation time** — how long from start to goal completion?
- **User intervention count** — how many times did the user have to correct or redirect?
- **Conversation coherence score** — rated holistically across all turns
- **State accuracy at conversation end** — what percentage of facts were retained?
- **Error recovery success** — when the agent failed (turn-level), did it recover by conversation end?

These require evaluating the entire conversation as a single unit. You can't average turn scores to get conversation quality—a conversation with nine perfect turns and one catastrophic turn (e.g., data leakage) is a failed conversation, even if turn-level average is 90%.

**The relationship:**

- High turn-level quality is necessary but not sufficient for high conversation-level quality
- Low turn-level quality usually causes low conversation-level quality
- You can have high turn-level scores and low conversation-level scores (incoherent but accurate responses)
- You cannot have low turn-level scores and high conversation-level scores

**Measurement approach:**

Score both. Report both. Optimize conversation-level metrics, but monitor turn-level metrics for debugging.

Example metrics dashboard:

```yaml
turn_level:
  average_response_quality: 4.3
  average_latency_ms: 1200
  tool_call_success_rate: 0.94

conversation_level:
  goal_completion_rate: 0.78  # This is what actually matters
  average_turns_to_completion: 8.2
  coherence_score: 3.9
  state_accuracy_final: 0.91
```

If conversation-level goal completion is below 80%, your agent isn't ready for production, regardless of turn-level scores.

---

## Progressive Complexity Testing

Your agent handles simple requests beautifully. Then a user starts with a simple request, adds constraints in turn three, changes requirements in turn five, and asks for a comparison in turn eight. Your agent collapses.

**Progressive complexity testing** starts simple and adds difficulty over turns. Real conversations escalate—users refine, pivot, add constraints, request alternatives. If your agent only handles static complexity, it fails in production.

**Complexity escalation patterns:**

**1. Additive constraints** — each turn adds a new requirement

- Turn 1: "Find me a flight to New York"
- Turn 3: "Under $300"
- Turn 5: "Departing after 2pm"
- Turn 7: "With checked bag included"

Agent must maintain all constraints, not just the latest.

**2. Pivots and comparisons** — user asks for alternatives mid-conversation

- Turn 1-4: Discussing option A
- Turn 5: "What about option B instead?"
- Turn 8: "Compare A and B"

Agent must remember details of A while researching B.

**3. Multi-step task composition** — simple tasks combine into complex workflow

- Turn 1: "Check my account balance"
- Turn 3: "Transfer $500 to savings"
- Turn 5: "Then pay my credit card bill"
- Turn 7: "What's my new checking balance?"

Agent must track state across sequential operations.

**4. Error correction and refinement** — user corrects agent misunderstandings

- Turn 2: Agent misunderstands user intent
- Turn 3: User clarifies
- Turn 4: Agent must incorporate correction and continue

Agent must update its understanding mid-stream.

**Test design:**

Build conversation templates that systematically increase complexity. Score the agent on final goal completion, not early-turn quality. An agent that aces turn one but fails turn ten gets a failing grade.

```yaml
test_case: "progressive_flight_search"
complexity_curve:
  turns_1_3: "basic search"
  turns_4_6: "add constraints"
  turns_7_9: "compare alternatives"
  turns_10_12: "finalize booking with edge cases"
success_criteria:
  must_complete: true
  state_accuracy_turn_12: ">= 0.90"
  no_constraint_loss: true
```

**Failure modes:**

- **Early constraint loss** — agent forgets constraints from turns 1-3 by turn 10
- **Complexity ceiling** — agent handles 3 constraints but breaks at 5
- **Sequential failure** — agent can't chain multi-step operations
- **Recovery failure** — agent can't incorporate mid-conversation corrections

By 2026, progressive complexity is the standard for agent eval. Static test cases don't cut it. Conversations evolve—your eval must evolve with them.

---

## User Simulation for Multi-Turn Testing

You need to test a thousand multi-turn conversations. You can't manually role-play a thousand simulated users. You need automation.

**User simulation** is the 2026 standard approach—build an LLM-based user simulator that interacts with your agent over multiple turns. The simulator plays the role of the end user, issuing requests, responding to agent questions, and evaluating agent behavior.

**How it works:**

1. Define a user goal and persona
2. User simulator issues the initial request
3. Agent responds
4. User simulator evaluates the response and generates the next user message
5. Repeat for N turns or until goal completion
6. Score conversation-level metrics

**User simulator prompt template:**

```yaml
role: "You are simulating a customer with the following profile and goal."
profile:
  persona: "Frustrated customer seeking refund"
  goal: "Get refund for order 12345"
  constraints: "Will not accept store credit"

instructions: |
  - Issue requests based on your goal
  - Respond naturally to agent questions
  - Add realistic details when asked
  - Escalate frustration if agent is unhelpful
  - End conversation when goal is met or after 15 turns

turn_behavior:
  turn_1: "State your goal clearly"
  turn_3_plus: "Add details or constraints as needed"
  if_agent_asks_for_info: "Provide requested info"
  if_agent_is_unhelpful: "Express frustration, repeat goal"
```

The simulator needs a **policy** for generating turns. Simple version: template-based responses. Advanced version: full LLM-generated turns with persona consistency.

**Persona-driven simulation:**

Different users behave differently. Your eval should cover multiple personas:

- **Cooperative user** — provides clear information, follows instructions
- **Frustrated user** — starts annoyed, escalates if agent fails
- **Confused user** — provides vague or contradictory information
- **Adversarial user** — tries to break the agent or extract unintended behavior

Each persona tests different agent capabilities. Cooperative users test happy-path performance. Frustrated users test de-escalation and recovery. Adversarial users test robustness and safety.

**2026 tooling:**

- **LangSmith** supports multi-turn eval traces with built-in user simulators
- **OpenAI Evals framework** includes conversation replay and user simulation
- **BrainTrust** offers persona-based multi-turn eval harnesses
- **Anthropic** provides reference user simulator implementations for Claude-based agents

**Common pitfall:**

User simulators that are too predictable. If your simulator always asks for the same information in the same order, you're not testing realistic variability. Inject randomness—users don't follow scripts.

**Enterprise pattern (2026):**

Run 100+ automated multi-turn conversations per eval run. Use user simulators for volume, but manually review 10-20 conversations per sprint to catch edge cases automation misses.

---

## State Corruption Detection

Your agent works perfectly in testing. You deploy to production. Within hours, users report that the agent knows things they never told it. Information is leaking between conversations. You have **state corruption**.

**State corruption** is when the agent's state management breaks down—mixing information across conversations, retaining stale data, or hallucinating facts. This is catastrophic for multi-user systems.

**Three corruption patterns to test:**

**1. Cross-conversation leakage** — information from conversation A appears in conversation B

Cause: Shared state, improperly cleared context, caching bugs.

Test: Run two conversations in parallel with distinct, non-overlapping information. Verify that facts from conversation A never appear in conversation B.

```yaml
conversation_A:
  user_id: "user_123"
  order_id: "A-99999"

conversation_B:
  user_id: "user_456"
  order_id: "B-11111"

test:
  run_parallel: true
  assert: "Order A-99999 never mentioned in conversation B"
  assert: "Order B-11111 never mentioned in conversation A"
```

If you detect leakage, you have a serious security and privacy problem. Fix before launch.

**2. Stale state retention** — agent uses outdated information from previous conversations

Cause: State not properly cleared between sessions, cached data persisting incorrectly.

Test: Run conversation A, end it, start conversation B with the same user but different context. Verify agent doesn't reference conversation A.

Example: User asks about Order 1 in conversation A (Monday). User asks about Order 2 in conversation B (Tuesday). Agent should not mention Order 1 unless user explicitly references it.

**3. Context window overflow effects** — agent behavior degrades when conversation exceeds context limits

Cause: Agent frameworks often truncate early turns when context window fills. This causes state loss.

Test: Design conversations that intentionally exceed your model's context window. Measure state accuracy and response quality as turns increase.

```yaml
test_case: "context_overflow"
model_context_limit: 128000  # tokens
conversation_design:
  turns: 50
  tokens_per_turn: 3000  # Intentionally exceed limit

measure:
  state_accuracy_per_turn: true
  identify_degradation_point: true
  assert: "Graceful degradation or explicit warning to user"
```

**2026 best practice:**

Agent frameworks should either summarize early turns, move to external memory (Chapter 22), or explicitly warn users when approaching context limits. If your agent silently loses state, users lose trust.

**Detection automation:**

Use assertion-based testing. After each conversation, check:

- State reset assertions (no data from previous conversation)
- Fact retention assertions (all user-provided facts still accurate)
- No hallucinated facts (agent doesn't "know" things user never said)

Run these as part of your CI/CD pipeline. State corruption is too dangerous to catch in production.

---

## Long-Horizon Evaluation

Most agents operate in sessions—a few minutes to an hour. But some agents run tasks over hours, days, or weeks. A research agent that monitors papers over a month. A sales agent that nurtures leads over weeks. A project management agent that tracks tasks over sprints.

**Long-horizon evaluation** tests agent performance over extended time scales.

**Challenges:**

**1. Time compression for testing** — you can't wait weeks to run an eval

Solution: Simulate time progression. Inject events at compressed intervals. A "week-long" eval runs in an hour by triggering day-level events every few minutes.

**2. State persistence across sessions** — agent must maintain state between interactions

Test: Run multi-session conversations with gaps. Verify agent correctly resumes context after hours or days of inactivity.

```yaml
test_case: "long_horizon_lead_nurture"
sessions:
  session_1:
    day: 1
    turns: 5
    action: "Initial contact, qualify lead"
  session_2:
    day: 3
    turns: 4
    action: "Follow up, share resources"
  session_3:
    day: 7
    turns: 6
    action: "Schedule demo"

success_criteria:
  agent_remembers_session_1_facts: true
  goal_completion: "demo scheduled"
  no_redundant_questions: true
```

**3. Event-driven interactions** — agent responds to external events over time

Example: Monitoring agent checks for new data daily, alerts user when thresholds are met.

Test: Simulate event streams at compressed time scales. Verify agent correctly triggers actions based on temporal patterns.

**Measurement:**

- **Task completion rate over horizon** — what percentage of long-running tasks complete successfully?
- **State accuracy across sessions** — does agent remember context after days of inactivity?
- **Temporal reasoning quality** — does agent correctly understand time-based relationships?

**Enterprise use case (2026):**

Sales and customer success teams deploy agents that maintain relationships over weeks. These agents must remember past interactions, track sentiment over time, and know when to escalate. Long-horizon eval is mandatory for these deployments.

**Tooling gap:**

As of 2026, most eval frameworks assume short sessions. Long-horizon eval is still largely custom-built. Expect dedicated tooling to emerge by 2027.

---

## Conversation Branching

Users don't follow linear paths. They interrupt. They change topics. They return to previous discussions. Your agent must handle **conversation branching**.

**Branching patterns:**

**1. Topic change** — user pivots to a new subject mid-conversation

- Turns 1-5: Discussing billing
- Turn 6: "Actually, I have a technical question"
- Turns 7-10: Discussing technical issue

Agent must switch context cleanly, then potentially return to billing if user asks.

**2. Interruption and resumption** — user pauses one task to handle another

- Turns 1-4: Searching for flights
- Turn 5: "Wait, first I need to check my PTO balance"
- Turn 6-8: Checking PTO
- Turn 9: "Okay, back to flights"

Agent must pause the flight search, handle PTO, then resume the flight search with all prior state intact.

**3. Multi-threaded conversation** — user is tracking multiple parallel goals

- Odd turns: Planning a work trip
- Even turns: Planning a personal trip

Agent must maintain separate state for both threads and not mix them.

**Testing approach:**

Build test cases that explicitly branch. Score the agent on:

- **Branch detection** — does agent recognize the topic change?
- **Context switching** — does agent cleanly switch to the new topic?
- **State preservation** — does agent maintain state for the previous topic?
- **Resume quality** — when user returns to the original topic, does agent resume correctly?

```yaml
test_case: "interrupted_task"
conversation:
  turns_1_4:
    topic: "flight_search"
    state: ["destination: NYC", "budget: $300"]
  turn_5:
    user: "Hold on, what's my PTO balance?"
    expected: "agent switches to PTO, preserves flight state"
  turns_6_7:
    topic: "pto_check"
  turn_8:
    user: "Okay, back to the flight search"
    expected: "agent resumes with NYC, $300 still in context"
```

**Failure modes:**

- **State mixing** — agent blends information from both topics
- **Lost context** — agent forgets original topic entirely
- **Forced linearity** — agent refuses to switch topics, insists on completing current task

**2026 pattern:**

Advanced agents maintain explicit conversation threads—labeled topics with independent state. When the user switches topics, the agent creates or switches to the appropriate thread. This requires explicit thread management in your agent architecture (Chapter 22).

---

## Multi-Turn Scoring Rubrics

You can't score a multi-turn conversation the same way you score a single response. You need rubrics adapted for conversational dynamics.

Chapter 2.6 introduced multi-turn scoring. Here's how to adapt it for agent-specific concerns.

**Agent-specific rubric dimensions:**

**1. Goal completion** — did the conversation achieve the user's objective?

- **5:** Goal fully achieved, efficient path, no user frustration
- **4:** Goal achieved, minor inefficiencies or dead ends
- **3:** Goal partially achieved, required significant user guidance
- **2:** Goal mostly unmet, agent made progress but didn't finish
- **1:** Goal completely unmet, agent failed to understand or pursue goal

**2. State accuracy** — did the agent correctly maintain information across turns?

- **5:** Perfect state retention, no repeated questions, no contradictions
- **4:** Minor state slips, user had to remind agent once
- **3:** Moderate state issues, agent asked for information twice
- **2:** Significant state loss, agent forgot key facts multiple times
- **1:** Severe state corruption, agent hallucinated or leaked information

**3. Recovery capability** — when the agent hit errors, did it recover?

- **5:** No errors, or immediate successful recovery
- **4:** Errors occurred, agent recovered within 1-2 turns
- **3:** Errors occurred, agent recovered but required user help
- **2:** Errors occurred, agent partially recovered, task degraded
- **1:** Errors occurred, agent never recovered, conversation failed

**4. Conversation efficiency** — did the agent achieve goals without wasting turns?

- **5:** Minimal turns, no unnecessary questions, optimal path
- **4:** Reasonable turn count, minor tangents
- **3:** Higher turn count than necessary, some redundancy
- **2:** Significantly inefficient, repeated loops
- **1:** Extremely inefficient, conversation went in circles

**5. Coherence** — did the conversation maintain logical flow?

(Use the 1-5 scale from the Conversation Coherence section above.)

**Aggregate scoring:**

You can average these dimensions, but **goal completion** should be weighted highest—an agent that fails to achieve goals is useless, regardless of other scores.

Weighted example:

```yaml
rubric_weights:
  goal_completion: 0.40
  state_accuracy: 0.25
  recovery_capability: 0.15
  efficiency: 0.10
  coherence: 0.10
```

**Enterprise standard (2026):**

Agents must score 4.0+ on goal completion and 4.0+ on state accuracy to ship to production. Recovery and efficiency can be lower for internal tools, but customer-facing agents need 3.5+ across all dimensions.

---

## 2026 Patterns and Tooling

By 2026, multi-turn evaluation is standardized across major agent platforms. Here's what's working in production.

**LangSmith multi-turn eval traces:**

LangSmith's tracing system captures full conversation history, tool calls, and state transitions. You can replay conversations, fork them at any turn, and run automated evals on entire traces.

Pattern:

1. Capture production traces
2. Sample interesting conversations (failures, edge cases, long conversations)
3. Turn them into regression tests
4. Re-run on new agent versions

This creates a living eval set that grows from real usage.

**Conversation replay testing:**

Record real user conversations (with consent and privacy protection). Replay them against new agent versions to detect regressions.

```yaml
replay_test:
  source: "production_conversation_12345"
  turns: 18
  original_outcome: "success"
  test:
    replay_against: "agent_v2.3"
    assert: "outcome is success"
    assert: "turn count is less than or equal to 18"
```

If the new version fails a conversation the old version passed, you have a regression.

**Automated user simulators with personas:**

Build a library of persona-based simulators—cooperative, frustrated, confused, adversarial. Run 100+ conversations per eval covering all personas.

2026 best practice: Use a mixture of scripted and LLM-generated simulators. Scripted simulators ensure coverage of known edge cases. LLM-generated simulators discover new failure modes through variability.

**Failure mode tracking:**

Instrument your agent to detect and log specific failure modes:

- State retention failures
- Goal abandonment
- Contradiction detection
- Cross-conversation leakage
- Context overflow

Aggregate these across eval runs. If state retention failures increase from 2% to 8% between versions, you have a regression even if overall scores look fine.

**Enterprise governance:**

For regulated industries, multi-turn eval results are part of the compliance record. Every deployed agent version must have documented multi-turn eval results showing state accuracy, goal completion, and failure rates.

2026 pattern: Store eval results in a versioned database tied to agent releases. Auditors can trace back from any production incident to the eval that should have caught it.

---

## Failure Modes

**State leakage between users:** Information from User A's conversation appears in User B's conversation. Cause: improperly scoped state, shared memory, caching bugs. Impact: catastrophic privacy violation.

**Context window silent failure:** Agent silently loses early context when conversation exceeds limits. Users don't realize the agent has forgotten critical information. Cause: frameworks truncate without warning. Impact: state loss, user trust loss.

**Goal drift over turns:** Agent starts with a clear goal, gradually drifts to tangential topics. Cause: poorly scoped tool calls, overly broad agent instructions, retrieval systems surfacing related but off-topic content. Impact: task incompletion, user frustration.

**Compounding hallucinations:** Agent hallucinates a fact in turn three, then builds on that hallucination in turns five, seven, and nine. Cause: agent's own outputs become part of context, model confabulates. Impact: cascading errors.

**Recovery loops:** Agent hits an error, tries to recover, hits the same error again, loops indefinitely. Cause: no error deduplication, no escalation strategy. Impact: conversation deadlock.

**Simulator brittleness:** Your user simulator only tests happy paths or follows rigid scripts. Cause: over-constrained simulation logic. Impact: eval misses edge cases, production failures.

---

## Enterprise Expectations

**Pre-launch requirements:**

- Multi-turn eval suite covering 100+ conversations
- State accuracy metrics above 90% for conversations up to 20 turns
- Goal completion rate above 80% for target use cases
- Documented failure modes and mitigation strategies
- User simulator testing across multiple personas

**Ongoing monitoring:**

- Weekly multi-turn regression tests on production traces
- Monthly manual review of sampled long conversations
- Automated alerts for state leakage, goal completion drops, or coherence degradation

**Governance and auditability:**

- Every agent version tagged with multi-turn eval results
- Failed conversations logged and categorized by failure mode
- Privacy-preserving conversation replay for debugging

**Team expectations:**

By 2026, product teams expect multi-turn eval as standard. Shipping an agent without conversation-level testing is like shipping a web app without integration tests—technically possible, irresponsible in practice.

---

## Template: Multi-Turn Test Case

```yaml
test_case: "customer_refund_request"
description: "Multi-turn conversation testing goal completion and state retention"

user_profile:
  persona: "Frustrated customer"
  goal: "Obtain refund for damaged product"
  order_id: "ORD-78901"
  product: "Laptop stand"
  issue: "Arrived broken"

conversation_structure:
  turn_1:
    user: "I need a refund for my order"
    expected_agent_behavior: "Ask for order number"

  turn_2:
    user: "Order ORD-78901"
    expected_agent_behavior: "Look up order, confirm product"

  turn_3:
    user: "The laptop stand arrived broken"
    expected_agent_behavior: "Express empathy, explain refund process"
    state_check: ["order_id: ORD-78901", "product: laptop stand", "issue: damaged"]

  turn_5:
    user: "How long will the refund take?"
    expected_agent_behavior: "Provide timeline, should NOT ask for order number again"
    state_check: ["order_id still in context"]

  turn_7:
    user: "Can I get expedited shipping on the replacement?"
    expected_agent_behavior: "Handle request, maintain refund context"

  turn_9:
    user: "Confirm the refund amount"
    expected_agent_behavior: "State refund amount, should reference ORD-78901"
    state_check: ["order_id, product, issue all retained"]

success_criteria:
  goal_completion: "Refund processed or clearly scheduled"
  state_accuracy_final: ">= 0.95"
  no_repeated_questions: true
  coherence_score: ">= 4"
  turn_count: "<= 10"

failure_conditions:
  - "Agent asks for order number more than once"
  - "Agent forgets product or issue by turn 7"
  - "Goal not completed by turn 10"
  - "Agent contradicts previous statements"
```

Use this template to build domain-specific multi-turn tests. Adapt the conversation structure, success criteria, and state checks to your agent's use cases.

---

## Interview: Multi-Turn Evaluation

**Q: We're testing our agent on individual queries and it's scoring well. Why do we need multi-turn evaluation?**

A: Because your users aren't asking individual queries—they're having conversations. An agent can ace single-turn eval and completely fail at multi-turn. You're testing a proxy metric, not the real thing. Single-turn eval misses state management, goal persistence, conversation coherence, and recovery capability. If your agent operates across multiple turns in production—and almost every agent does—you must evaluate multi-turn behavior. Testing one turn at a time is like evaluating a movie by rating random scenes. You'll miss the narrative, the continuity, the flow. Real quality only emerges over time.

**Q: How do we measure state accuracy across a conversation?**

A: Track the facts established in each turn, then verify the agent retains and correctly uses those facts in later turns. Fact extraction can be manual or automated—use a secondary LLM to parse facts from each turn. Then check: does the agent reference those facts when relevant? Does it avoid asking for information already provided? Does it avoid contradicting established facts? Measure state accuracy as retained facts divided by established facts. If a user provides their order number in turn one, the agent should never ask for it again. If the agent asks again in turn five, that's a state retention failure. Score each conversation, aggregate across your eval set. Target: 90%+ state accuracy for conversations up to 20 turns.

**Q: How do we test for cross-conversation state leakage without manually running hundreds of conversations?**

A: Automate it. Run parallel conversations with distinct, non-overlapping information. Use unique identifiers—different user IDs, order IDs, account numbers. Assert that facts from conversation A never appear in conversation B. This is assertion-based testing, easy to automate. Run it in CI/CD on every deployment. If you detect leakage even once, stop everything and fix it—state leakage is a catastrophic privacy and security failure. For production monitoring, log all conversations and run automated checks for cross-contamination. You can also use synthetic data: create obviously fake facts (e.g., "Order CANARY-123") in one conversation and scan all other conversations for that string. If it appears, you have leakage.

**Q: Our conversations can get very long—30, 40, 50 turns. How do we evaluate those efficiently?**

A: You don't need to manually review every turn. Use automated conversation-level metrics: goal completion, state accuracy at turn 50, contradiction count, coherence score. Use user simulators to generate long conversations at scale. Then sample—manually review 10-20 long conversations per sprint to catch issues automation misses. Focus your manual review on failures and edge cases flagged by automated metrics. If your automated eval shows a conversation failed goal completion or had low coherence, read that one. Don't read the 95% that passed. Also, test progressive complexity—design conversations that intentionally escalate difficulty. If your agent handles 10 turns but breaks at 20, you've found your ceiling. Test there.

**Q: We don't have real user conversations yet—we're pre-launch. How do we build a multi-turn eval set?**

A: Build synthetic conversations based on your expected use cases. Use user simulators with defined goals and personas. Start with scripted conversations covering your core workflows—happy path, common errors, edge cases. Then layer in LLM-generated variability: give the simulator a goal and let it generate realistic multi-turn interactions. You can also adapt conversations from adjacent domains—if you're building a customer service agent, study real customer service transcripts (anonymized), extract patterns, and build test cases that match those patterns. Pre-launch, you're necessarily synthetic. Post-launch, convert real production conversations (with privacy controls) into regression tests. Your eval set will grow and improve as you learn from production.

---

## Bridge to 8.7

You've built multi-turn eval. Your agent handles conversations, maintains state, achieves goals. But it's still running in a controlled test environment. Real agents don't just converse—they act on the world. They call APIs, query databases, execute code, manipulate files.

Your next question: how do you evaluate agents that interact with real or simulated environments? How do you test tool calls, side effects, and environmental state changes safely and reproducibly?

**Chapter 8.7 — Sandbox & Environment Design** covers evaluation environments for agents that act on systems—isolated sandboxes, mock services, environment reset strategies, and safety controls for testing agents that can do real damage if they fail.

Turn the page. Let's talk about testing agents in the wild.

