# Chapter 2 â€” Quality Definitions & Scoring

### Plain English

Quality is meaningless until you define it.

**What does "good" mean for your specific AI system and task?**

In traditional software, quality often means "does it match the spec?"

In AI systems, quality is multi-dimensional:
- Is the answer correct?
- Is it grounded in the right sources?
- Is it complete enough to be useful?
- Is it safe?
- Does it refuse when uncertain?
- Does it maintain quality across multiple turns?

Without clear, measurable definitions of quality, your evals measure nothing.

This chapter teaches you how to define quality dimensions, build scoring rubrics humans can apply consistently, and create scorecards that work in production.

---

### Why This Chapter Exists

AI outputs are not binary pass/fail.

The same response might be:
- factually correct but unhelpful
- helpful but not grounded
- grounded but incomplete
- complete but unsafe

Traditional test assertions do not capture this complexity.

This chapter exists to:
- replace vague quality judgments with clear dimensions
- make human scoring reproducible
- align engineers, product, and domain experts on definitions
- prevent arguments about whether something "works"
- give you the vocabulary to express nuanced quality requirements

In 2026, **quality definitions are system design artifacts**.

They belong in your architecture review, not just your testing docs.

---

### What Quality Definitions Actually Are (2026 Meaning)

Quality definitions are **not**:
- a single accuracy percentage
- subjective opinions
- what the model happens to produce

Quality definitions **are**:
- task-specific
- multi-dimensional
- measurable by humans or automated judges
- documented and versioned
- tied to product requirements

Technically, they define:
- what "correct" means for this task
- what "good enough" means for this task
- what "unsafe" means for this task
- how to handle edge cases and ambiguity
- how to score partial success

---

### Core Components of Quality Definitions

#### 1. Task-Specific Definitions of "Good"

Quality is never universal.

Different tasks require different definitions:
- Extraction tasks: did you extract the right fields?
- Summarization tasks: did you capture the key points without hallucination?
- Classification tasks: did you pick the right category?
- RAG tasks: did you ground your answer in the provided documents?
- Agent tasks: did you complete the workflow correctly?
- Conversational tasks: did you maintain context and tone?

Each task type needs its own quality framework.

Your eval system must define quality **per task category**, not globally.

---

#### 2. Rubrics Humans Can Score Consistently

A rubric is a decision tree for humans.

Good rubrics:
- have clear criteria
- provide examples of each score level
- minimize judgment calls
- achieve high inter-rater reliability

Example rubric for grounding:
- Score 3: All claims are directly supported by source documents
- Score 2: Most claims are supported, minor details unsupported
- Score 1: Some claims are supported, significant unsupported content
- Score 0: Claims contradict or ignore source documents

Rubrics fail when:
- criteria are vague
- examples are missing
- edge cases are undefined
- humans must guess intent

If two annotators consistently disagree, your rubric is broken.

---

#### 3. Good vs Bad Patterns

Patterns are concrete examples of quality dimensions.

Good patterns show:
- what ideal outputs look like
- what acceptable outputs look like
- what edge cases are handled correctly

Bad patterns show:
- hallucinations
- grounding failures
- incomplete responses
- unsafe content
- tone violations
- refusal failures (refusing when it should answer, or answering when it should refuse)

Patterns are used to:
- train human annotators
- calibrate LLM judges
- create synthetic test cases
- document system behavior

In mature systems, patterns are versioned and reviewed like code.

---

#### 4. Uncertainty, Refusal, and Safety Scoring

Not all questions should be answered.

Your quality definitions must cover:
- when the system should refuse
- how it should express uncertainty
- what qualifies as unsafe

Uncertainty scoring:
- Did the system express appropriate confidence?
- Did it hedge when evidence was weak?
- Did it escalate when uncertain?

Refusal scoring:
- Did it refuse dangerous requests?
- Did it refuse requests outside its scope?
- Did it explain why it refused?

Safety scoring:
- Did it avoid generating harmful content?
- Did it resist jailbreak attempts?
- Did it protect sensitive information?

These dimensions are **equally important** as correctness in production systems.

---

#### 5. Enterprise Scorecards

Enterprises need standardized scorecards across systems.

A scorecard defines:
- which quality dimensions are measured
- how each dimension is scored (scale, rubric)
- what thresholds must be met
- who reviews results
- how often reviews happen

Common scorecard dimensions:
- Correctness (0-3 scale)
- Grounding (0-3 scale)
- Completeness (0-3 scale)
- Safety (pass/fail)
- Tone appropriateness (0-3 scale)
- Latency (milliseconds)
- Cost (tokens or dollars)

Scorecards allow:
- consistent evaluation across teams
- comparison across systems
- roll-up reporting for leadership
- evidence for compliance audits

Without scorecards, every team invents their own metrics.

---

#### 6. Multi-Turn Conversation Scoring

Conversations are not single-turn tasks.

Multi-turn scoring evaluates:
- context retention across turns
- handling of clarifications
- correction of mistakes
- escalation when stuck
- tone consistency

Example conversation quality dimensions:
- Does the system remember what the user said 3 turns ago?
- Does it acknowledge corrections gracefully?
- Does it avoid repeating failed strategies?
- Does it detect when the user is frustrated?

Multi-turn evals require:
- conversation-level datasets
- turn-by-turn annotation
- aggregate conversation scores

Single-turn evals will miss critical failures in conversational systems.

---

### Enterprise Perspective

Enterprises need:
- standardized quality frameworks across teams
- defensible scoring for audits
- consistent definitions across geographies
- alignment between engineering and compliance

Quality definitions allow:
- predictable evaluation outcomes
- evidence for risk reviews
- clear communication with non-technical stakeholders
- consistent customer experience

In regulated industries, quality definitions are reviewed by legal and compliance teams.

They are treated like specifications, not opinions.

---

### Founder / Startup Perspective

Startups need:
- fast but rigorous quality definitions
- clear communication of product expectations
- alignment between founders and engineers
- confidence to ship without fear

Good quality definitions:
- clarify what "done" means
- reduce back-and-forth debates
- allow you to delegate eval work
- create institutional knowledge even with turnover

Early-stage teams that skip quality definitions ship based on vibes.

They discover quality problems only after users complain.

---

### Common Failure Modes

- Using a single quality score for all tasks
- Vague rubrics that humans interpret differently
- No examples of good vs bad outputs
- Ignoring uncertainty and refusal as quality dimensions
- Evaluating single turns instead of full conversations
- No versioning of quality definitions
- Scorecards that measure what is easy, not what matters
- Quality definitions owned by no one and updated never

Recognizing these failures is the first step to fixing them.

---
