# Chapter 2.5 — Enterprise Scorecards (What Leadership Cares About)

**What we're doing here:**
So far, we defined "good" (2.1), built human rubrics (2.2), learned patterns (2.3), and scored uncertainty/safety (2.4).
Now we translate all of that into **enterprise scorecards** — the dashboards and decision signals that leadership uses to approve launches, track risk, and justify investment.

**Big idea:**
Engineers care about model behavior.
Leadership cares about **business outcomes + risk + cost + reliability**.
A scorecard bridges both.

---

## 1) What leadership actually wants (in plain English)

Most execs want answers to these questions:

1. **Is it safe?** (Will it cause harm, legal risk, privacy incidents?)
2. **Does it work reliably?** (Will customers trust it day-to-day?)
3. **Does it help the business?** (Revenue, retention, efficiency, CSAT)
4. **What does it cost?** (AI spend, infra, support load)
5. **Can we scale it?** (Ops, monitoring, governance, consistency)

Your job is to express model quality in those terms.

---

## 2) Mechanics: the 3-layer scorecard

A good enterprise scorecard has three layers:

### Layer A — Quality (Does it do the job well?)
- Task success rate
- Correctness score
- Grounding score (for RAG)
- Tool correctness (for agents)
- Voice UX (for voice)

### Layer B — Risk (Can it hurt us?)
- Safety fail rate (hard gate)
- Privacy/PII incident rate (hard gate)
- Over-refusal rate (blocked safe requests)
- Confident-wrong rate (high trust damage)

### Layer C — Economics (Is it worth it?)
- Cost per successful task
- Latency / time-to-resolution
- Escalation rate to humans
- Ticket deflection / agent containment
- Support burden created by the AI

Leadership uses Layer B as a gate, and Layers A/C to decide "ship, pause, or invest."

---

## 3) Knobs & defaults (what you actually set)

These are the standard enterprise settings for scorecards:

### 3.1 Release gates (hard minimums)
- **Safety fail rate:** must be ~0 on your safety suite
- **PII/privacy failures:** must be ~0
- **Critical task success:** must meet target (set per product)
- **Regression threshold:** "no more than X% drop" on key tasks

### 3.2 Target bands (operational goals)
- Latency target (p50 / p95)
- Cost per task target
- Escalation rate target
- Over-refusal ceiling (too many refusals = business loss)

### 3.3 Red/yellow/green thresholds
Make it simple:
- **Green:** safe + meets targets
- **Yellow:** safe but needs tuning (cost/latency/quality)
- **Red:** safety/PII risk or major regression

---

## 4) The core scorecard metrics (recommended)

Below is a practical "starter scorecard" used across many enterprise AI products.

### 4.1 Reliability & Quality
- **Task Success Rate (%)** — Did the user achieve the goal?
- **Average Quality Score (0–3)** from your rubric
- **Top Intent Pass Rate** — Your top 10 intents should have explicit pass rates

### 4.2 Safety & Trust (hard focus)
- **Safety Fail Rate (%)** (hard gate)
- **PII Leak Rate (%)** (hard gate)
- **Confident-Wrong Rate (%)** — wrong answers delivered with high confidence
- **Abstain Correctness Rate (%)** — when it abstains/refuses, was that correct?
- **Over-refusal Rate (%)** — refused safe requests that should be supported

### 4.3 Performance & Cost
- **Latency p50 / p95**
- **Cost per successful task**
- **Tokens/compute per task** (or "AI credits" per task if you bill that way)
- **Tool calls per task** (for agents)
- **Human escalation rate (%)**
- **Average time-to-resolution** (end-to-end)

### 4.4 Business outcomes (choose what fits)
- **CSAT / NPS impact**
- **Ticket deflection rate** (support)
- **Conversion lift** (sales assistant)
- **Retention impact**
- **Revenue influenced** (careful: define attribution clearly)

---

## 5) Scorecards by product type (quick templates)

### 5.1 Scorecard for Chat Support Bot

**Must-have**
- Safety fail rate
- Correctness score
- Resolution rate
- Escalation rate
- CSAT
- Cost per resolved ticket

**What usually goes wrong**
- Looks "helpful" but increases escalations
- Answers confidently but wrong (trust collapse)

---

### 5.2 Scorecard for RAG (internal knowledge assistant)

**Must-have**
- Retrieval success rate (did it fetch relevant docs?)
- Grounding score (did it stay within evidence?)
- Abstain correctness rate
- Citation correctness rate (if required)
- Time-to-answer
- Cost per answer

**What usually goes wrong**
- Hallucinations hidden under good writing
- Retrieval filters/permissions silently block the right docs

---

### 5.3 Scorecard for Agents (tool-using automations)

**Must-have**
- Task completion rate
- Tool correctness rate
- Duplicate action rate (idempotency failures)
- Recovery success rate (handles tool errors)
- Human approval requests (how often it needs a human)
- Cost per completed workflow

**What usually goes wrong**
- Great text, poor execution
- Hidden loops and retries inflate cost

---

### 5.4 Scorecard for Voice AI

**Must-have**
- Containment rate (how many calls resolved without human)
- Transfer rate to human agents
- Caller satisfaction (CSAT) or sentiment
- Critical-field confirmation compliance
- Latency feel proxy (silence gaps, interruption handling)
- Cost per resolved call-minute

**What usually goes wrong**
- Misheard critical details without confirmation
- Awkward turn-taking leads to hang-ups

---

## 6) How to present the scorecard (so execs actually use it)

### 6.1 One-page view (recommended)
A single page with:
- **Green/Yellow/Red**
- 5–10 top metrics
- 3 biggest risks
- 3 recommended actions

### 6.2 Trendlines matter more than single numbers
Leadership cares about:
- "Is it improving?"
- "Did last release cause regressions?"
- "Where are the risks increasing?"

So always include:
- last release vs current
- 7-day and 30-day trends

---

## 7) Failure modes: what breaks enterprise scorecards

### 7.1 Vanity metrics
**Symptom:** "Average rubric score is high" but customers complain
**Root cause:** scoring rewards style, not outcomes
**Fix:** include business metrics + confident-wrong + escalation rate

### 7.2 No separation between "quality" and "risk"
**Symptom:** a model looks good overall but has rare severe failures
**Fix:** safety/PII must be hard gates with their own suite

### 7.3 No slice analysis
**Symptom:** overall looks fine, but one customer segment is suffering
**Fix:** slice by:
- intent
- language
- region
- user tier
- device/channel
- tenant/customer

### 7.4 Cost surprises
**Symptom:** quality up, but costs explode
**Fix:** track cost per successful task and tool calls per task, not just tokens

---

## 8) Debug playbook: when leadership asks "Why did this drop?"

When a key metric drops, do this in order:

1. **Identify the slice** — Which intent? Which user segment? Which channel?
2. **Localize the layer**
   - Chat: prompt vs knowledge vs policy
   - RAG: retrieval vs reranker vs generator
   - Agents: tool layer vs planning layer vs state
   - Voice: ASR vs reasoning vs TTS vs latency
3. **Pull 20 examples** — 10 successes, 10 failures. Compare what changed.
4. **Ship a targeted fix** — Add regression tests for that slice. Add gold examples so it never returns unnoticed.

---

## 9) Enterprise expectations (what serious teams do)

- Scorecards are used as **release gates**
- Every metric is tied to:
  - an owner
  - a target
  - an alert threshold
  - an escalation path
- They maintain:
  - Quality eval suite (task success)
  - Safety eval suite (red-team)
  - Regression suite (critical flows)
- They run weekly business reviews:
  - "What improved?"
  - "What regressed?"
  - "What is the next risk?"

---

## 10) Interview-ready talking points

> "I build scorecards that map model evals to business outcomes, risk, and cost."

> "Safety and privacy are hard gates; quality averages can't hide severe failures."

> "I track confident-wrong and over-refusal because they directly impact trust and revenue."

> "I slice scorecards by intent and segment so we don't miss localized regressions."

> "I report cost per successful task, not just token usage."
