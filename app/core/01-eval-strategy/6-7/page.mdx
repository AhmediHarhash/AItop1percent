# 6.7 — Rater Drift, Fatigue & Bias Control

I once worked with a team that had a brilliant group of raters evaluating customer support responses. Three months in, something strange happened. The lead noticed that approval rates had quietly climbed from 72% to 89%, yet production complaints hadn't changed. When they audited the labels, they found that raters were now accepting responses they would have rejected in week one. The rubric hadn't changed. The examples hadn't changed. But the raters had drifted like a ship slowly pulled off course by an invisible current.

Human judgment isn't a fixed instrument. It's a living thing that shifts with time, context, fatigue, and the thousands of micro-decisions that pile up over evaluation sessions. Your raters start sharp. They study the rubric. They debate edge cases. But by hour three of session fifty, their mental model has quietly mutated. They're faster, yes, but they're no longer measuring the same thing they measured on day one.

This chapter is about the silent enemies of evaluation quality: rater drift, fatigue, and the constellation of biases that corrupt judgment when you're not watching. We'll talk about what causes them, how to detect them before they ruin your dataset, and how to build systems that keep human judgment calibrated over months of labeling work.

---

## Why This Matters

You can have perfect rubrics, gold-standard examples, and expert raters, and still end up with garbage labels. Why? Because humans aren't consistent across time. A rater who gave a "4" to an answer in January might give a "3" to the same answer in March, not because they changed their mind, but because their internal reference point shifted without them noticing.

**Rater drift** is the silent killer of evaluation datasets. It introduces noise that looks like signal. You think you're seeing model improvement when really you're seeing rater leniency creep. You think you're catching regressions when really you're seeing a harsh rater replacing a lenient one.

**Fatigue** makes this worse. A rater in hour one catches nuance. A rater in hour four clicks buttons on autopilot. The labels look similar, but one set reflects careful judgment and the other reflects cognitive exhaustion wearing a mask of productivity.

And then there are the **biases** — anchoring effects, position preferences, confirmation tendencies — that sneak in even when raters are trying their hardest to be objective. These aren't character flaws. They're features of human cognition operating in conditions that make bias inevitable.

If you don't actively fight these forces, your evaluation program slowly becomes a elaborate ritual that produces numbers instead of truth. Let me show you how to keep the signal clean.

---

## What Is Rater Drift?

**Rater drift** is the gradual, unconscious shift in a rater's standards or interpretation of a rubric over time. It's not intentional. It's not defiance. It's the natural result of humans adapting their mental models as they see more examples.

Here's what causes it:

**Pattern adaptation.** After seeing 500 examples, raters unconsciously adjust their sense of "normal." If most responses are mediocre, mediocre starts to feel acceptable. Their threshold for "good" shifts upward without them realizing it.

**Rubric compression.** Early on, raters use the full scale. By month two, they've informally collapsed it. A five-point scale becomes effectively three points because extreme scores feel uncomfortable to assign repeatedly.

**Memory decay.** Calibration sessions imprint a standard, but that standard fades. By week three, raters are working from a blurry recollection of the gold examples, and they've filled in the gaps with their own intuition.

**Contextual anchoring.** If a rater evaluates ten terrible responses in a row, the eleventh merely-bad response looks good by comparison. Their scoring drifts upward because recent context overshadows the rubric.

The danger is that drift is **invisible to the rater**. They don't feel like they've changed. They'll swear they're applying the same standard they always have. Only when you show them their own scores over time do they see the shift.

---

## Detecting Rater Drift

You can't fix what you can't see. Here's how to make drift visible before it corrupts your dataset:

### Gold Question Tracking

Embed **gold questions** — examples with known correct labels — into your evaluation workflow. Not at the start as calibration, but scattered throughout, unlabeled so raters don't know which ones are tests.

Track each rater's accuracy on gold questions **over time**. If a rater was 95% accurate in week one and they're 78% accurate in week eight, they've drifted. Their internal model no longer matches your rubric.

Plot this on a dashboard. Drift often shows up as a slow downward trend in gold accuracy, or as sudden drops after long evaluation sessions.

### Agreement Trend Analysis

Measure **inter-rater agreement** not just once during calibration, but continuously throughout the labeling process. If agreement between raters was 85% in the first batch and it's 68% in the current batch, someone has drifted (or everyone has, in different directions).

Look for:
- **Declining pairwise agreement**: Two raters who used to agree are now diverging
- **Widening score distributions**: Some raters are using a narrow range while others use the full scale
- **Cluster formation**: Raters split into factions with different unspoken standards

### Per-Rater Score Distribution Monitoring

Track the **distribution of scores** each rater assigns over time. If Rater A gave 15% "5" ratings in January and 35% "5" ratings in March, they've drifted lenient. If Rater B's average score drops from 3.8 to 2.9, they've drifted harsh.

This is different from just tracking averages. You want to see the **shape** of their distribution. Are they still using the full scale? Have they collapsed into a narrow band? Are they avoiding extreme scores?

### Recalibration Spot Checks

Every few weeks, pull a random sample of labels from each rater and have them **re-label** the same examples without knowing they're duplicates. Compare their new labels to their old ones. High disagreement means drift.

This is gold for catching drift because you're comparing each rater to themselves, not to others. It removes the "different interpretation" excuse. If they can't reproduce their own labels, they're not operating with a stable standard.

---

## The Fatigue Problem

Let's talk about what happens when you ask humans to make the same type of judgment for hours at a stretch.

### How Fatigue Degrades Labels

Cognitive fatigue doesn't make raters stupid. It makes them **take shortcuts**. After prolonged evaluation, you see:

**Default bias.** When unsure, fatigued raters pick the middle option or the first option instead of deliberating. You'll see a spike in "3" ratings on a five-point scale in later session hours.

**Pattern matching instead of reasoning.** Fresh raters read carefully and think about edge cases. Fatigued raters scan for familiar patterns and click based on surface features. They miss nuance.

**Reduced discrimination.** Early in a session, raters can distinguish between a "3" and a "4" reliably. Late in a session, everything borderline gets rounded to the nearest convenient score.

**Increased noise.** Fatigue doesn't just bias scores downward or upward — it makes them **inconsistent**. The same rater will score similar examples differently because they're not fully processing each one anymore.

### Session Length Limits

So how long is too long?

The research and industry practice converge around **90–120 minutes** as the maximum effective session length for focused evaluation work. After that, quality drops faster than speed increases.

Better structure:
- **45–60 minute focused blocks** with mandatory 10–15 minute breaks between them
- **Maximum 4 hours of evaluation work per day**, spread across morning and afternoon
- **Variety within sessions**: mix task types, switch between pairwise and absolute scoring, inject discussion prompts

Some teams use **micro-sessions**: 20–30 minute sprints with hard stops. This keeps raters sharp but requires more overhead to ramp in and out.

Track **quality by session hour**. If you see gold accuracy or inter-rater agreement drop in hours three and four, that's fatigue. Shorten your sessions.

### Ideal Session Structure

A well-designed evaluation session looks like this:

**Warmup (5 minutes).** Start with a few easy examples or a quick review of the rubric. This gets raters into the right mental mode without burning them out on hard cases immediately.

**Focused work (45 minutes).** The core labeling block. Minimize interruptions. Raters should be able to enter a flow state.

**Break (10 minutes).** Mandatory, not optional. Walk away from the screen. Fatigue is cumulative and breaks barely dent it if you don't enforce them.

**Second block (45 minutes).** Another focused session, ideally on a slightly different task or dataset to maintain engagement.

**End-of-session review (5 minutes).** Quick spot check of their own labels, or a calibration question to close the loop. This keeps quality top-of-mind.

Don't schedule marathon sessions before deadlines. The labels you gain in speed you lose in quality, and you'll pay for it in downstream model performance.

---

## Anchoring Bias

Here's a fun experiment. Show a rater ten examples. The first five are all excellent. The sixth is mediocre. Watch them score it higher than they would if you'd shown mediocre examples first.

**Anchoring bias** means the first few examples a rater sees set an unconscious baseline for everything that follows. If the anchor is high, subsequent scores drift upward. If the anchor is low, scores drift downward.

This is especially dangerous in **batched evaluation**. If you assign Rater A a batch that happens to contain mostly good outputs and Rater B a batch with mostly poor outputs, they'll develop different internal scales even if they started with the same rubric.

### How to Mitigate Anchoring

**Randomize presentation order.** Don't sort examples by score, by model, or by any other feature that creates patterns. Shuffle them so each rater sees a representative mix in unpredictable order.

**Seed sessions with calibrated examples.** Start each session with 2–3 gold questions from across the score range. This resets the rater's mental anchor to the rubric standard instead of whatever random examples appear first.

**Blind raters to batch composition.** Don't tell them "this batch is all edge cases" or "these are candidate production outputs." Let them approach each example fresh.

**Interleave tasks.** If you're evaluating outputs from multiple models, randomly interleave them rather than evaluating Model A, then Model B. This prevents model-level anchoring.

---

## Leniency and Severity Bias

Some raters are **consistently generous**. They see the good in every output. Their average score is half a point higher than everyone else's, and they rarely use the bottom of the scale.

Other raters are **consistently harsh**. They see flaws in everything. Their average score is half a point lower, and they treat "5" like a theoretical construct that no real output deserves.

This isn't about accuracy — a harsh rater might correctly identify flaws that lenient raters miss. The problem is **inconsistency across raters**. If you aggregate raw scores, you're mixing different scales.

### Detecting Leniency/Severity Bias

Plot the **average score per rater** over a shared evaluation set. If everyone evaluated the same 100 examples, you should see similar averages. If Rater A averages 4.2 and Rater B averages 2.8, one of them is systematically biased (or both are, in opposite directions).

More sophisticated: track each rater's score on **gold questions**. If Rater A consistently scores gold "4" examples as "5" and gold "2" examples as "3", they're lenient by exactly one point. If Rater B does the opposite, they're harsh.

### Correcting Leniency/Severity Bias

You have three options:

**Score normalization.** Convert each rater's scores to z-scores (standardized scores) within their own distribution, then map back to a common scale. This removes systematic bias while preserving their relative judgments.

This works well when you have enough data per rater to estimate their mean and variance reliably. It fails when raters see different subsets of examples with different true distributions.

**Recalibration.** Show biased raters their scoring patterns compared to peers and gold standards. Walk through examples where they diverged. Often, just making them aware of the bias reduces it.

This is most effective when bias comes from misunderstanding the rubric, not from personality. You can't recalibrate someone's temperament, but you can realign their interpretation.

**Rater replacement.** If a rater can't or won't align with the target standard after recalibration, remove them from the pool. One persistently biased rater can corrupt aggregated scores even if everyone else is calibrated.

Harsh: yes. Necessary: sometimes. You're optimizing for label quality, not for preserving everyone's role.

---

## Position Bias

If you show raters two outputs side by side and ask "which is better?", they'll pick the **left one** (or the top one, or whichever is presented first) more often than chance, even when the outputs are equivalent.

**Position bias** is the tendency to prefer the first option in a comparison task. It's unconscious and persistent. Even when you warn raters about it, the bias remains.

Measured across thousands of pairwise judgments, position bias can skew win rates by **5–15 percentage points**. That's enough to flip conclusions about which model is better.

### Mitigation

**Randomize position.** For every pairwise comparison, randomly assign which output appears first. Over many judgments, position bias averages out.

**Counterbalancing.** Show each pair twice, swapping positions. If output A beats output B in position 1 but loses when it's in position 2, you know position mattered more than quality.

This doubles your labeling cost, so use it for high-stakes decisions or when you see unexplained asymmetry in pairwise results.

**Forced justification.** Require raters to write a sentence explaining their choice. The act of justifying reduces impulsive positional preference because it engages deliberative reasoning.

---

## Confirmation Bias

Raters see what they expect to see. If you label an output as "Generated by Model A (known to be good at math)", raters will judge it more favorably on math tasks than if you label it "Generated by Model B (experimental)".

**Confirmation bias** means prior beliefs shape perception, especially on ambiguous or borderline cases. If a rater expects a model to be strong at a task, they'll interpret unclear outputs charitably. If they expect weakness, they'll interpret the same outputs critically.

This is catastrophic for A/B testing. If raters know which outputs came from which model, their expectations contaminate the comparison.

### Mitigation

**Blind raters to source.** Don't tell them which model, which prompt, or which experimental condition produced each output. Present everything unlabeled.

**Randomize metadata.** If you must show some context (like the user query), randomize which output gets which label so raters can't infer patterns.

**Audit edge cases.** Confirmation bias concentrates in borderline examples where judgment is hard. Track inter-rater agreement on the hardest 10% of examples. Low agreement often signals that bias is tipping close calls.

**Use within-rater designs.** Have each rater evaluate outputs from all conditions. This makes their biases apply equally across all models rather than systematically favoring one.

---

## Cultural and Demographic Bias

Rater background affects judgment, especially on subjective dimensions like tone, formality, politeness, and appropriateness.

A rater from a high-context culture might score a direct, blunt response as rude. A rater from a low-context culture might score the same response as refreshingly clear. Neither is wrong — they're applying different cultural standards of politeness.

Similarly, age, education, regional dialect, and professional background all shape how raters interpret language. A legal professional will read "reasonable" differently than a software engineer. A Gen Z rater interprets informal tone differently than a Boomer rater.

You **cannot eliminate** cultural and demographic bias. It's baked into human judgment. But you can manage it.

### Managing Cultural and Demographic Bias

**Diversify your rater pool.** If your product serves global users, your raters should reflect that diversity. Don't let one demographic group define "correct" for everyone.

**Stratify by rater demographics.** Track scores separately for different rater subgroups. If you see systematic differences (e.g., raters over 50 score formality higher than raters under 30), that's signal, not noise. It tells you how different user populations will perceive your outputs.

**Align rater demographics to target users.** If you're building a product for enterprise CFOs, having college students rate financial advice is a mismatch. Recruit raters who represent your actual user base.

**Flag culturally sensitive content.** For evaluation of content involving culture-specific norms (politeness, humor, appropriateness), use raters from the relevant culture. Don't have American raters judge whether a Japanese response is appropriately polite.

**Consensus via diversity.** When aggregating scores, don't try to suppress demographic differences. Instead, accept that "quality" is partially subjective and aim for consensus across diverse perspectives. If all demographic groups agree an output is good, it's robustly good.

---

## Detection Methods: Building a Monitoring Dashboard

You need a **rater quality dashboard** that makes drift, fatigue, and bias visible in real time. Here's what to track:

### Per-Rater Metrics (Updated Continuously)

**Gold question accuracy over time.** Line chart showing each rater's accuracy on gold questions by week or by batch. Declining trends indicate drift.

**Score distribution.** Histogram of scores assigned by each rater in recent batches. Compare to overall distribution. Flag raters whose distribution is significantly narrower or shifted.

**Agreement with consensus.** For each rater, measure how often their labels match the majority vote. Low agreement means they're either drifting or they have a different interpretation.

**Average scores over time.** Track mean score per rater per week. Sharp changes indicate drift toward leniency or severity.

**Session-level quality.** For each evaluation session, plot gold accuracy or agreement by hour within session. Drops in hours 2–4 indicate fatigue.

### Cohort-Level Metrics

**Inter-rater agreement trends.** Track Fleiss' kappa or pairwise agreement across all raters over time. Declining agreement means the group is drifting apart.

**Score variance.** Measure the variance in scores assigned to the same example by different raters. Increasing variance means raters are diverging.

**Position bias check.** For pairwise tasks, measure win rate by position. If the first option wins 60% of the time, you have position bias.

### Alerts and Flags

Set up **automatic alerts** for:
- Rater gold accuracy drops below 80%
- Inter-rater agreement drops below 70%
- Individual rater's score distribution deviates more than 0.5 points from cohort mean
- Session quality drops in later hours

Dashboards without alerts are just decoration. You need systems that notify you when drift or bias crosses thresholds so you can intervene.

---

## Correction Methods: Bringing Raters Back on Track

When you detect drift or bias, here's how to fix it:

### Recalibration Sessions

Every 2–4 weeks, run a **recalibration session**. Gather raters (in person or via video), walk through 10–15 examples that span the scoring range, and discuss edge cases where raters disagreed.

Show them **their own score distributions** compared to peers. Visualize where they're diverging. Often, just making drift visible is enough to correct it.

Focus recalibration on **examples where drift appeared**. If gold question accuracy dropped on a specific type of case, drill into those cases and realign standards.

Don't make recalibration punitive. Frame it as collaborative refinement, not correction of failure. Raters who feel defensive won't internalize the new standard.

### Rater Rotation

Avoid having the same rater evaluate the same type of content for months on end. **Rotate raters** across different tasks, datasets, or evaluation dimensions every few weeks.

Rotation has two benefits:
- **Breaks monotony**, which reduces fatigue-driven autopilot labeling
- **Diversifies perspective**, so one rater's drift doesn't compound over thousands of labels on the same slice of data

Trade-off: rotation reduces specialization. A rater who's evaluated summarization tasks for eight weeks has built expertise that a fresh rater lacks. Balance rotation with continuity.

### Score Normalization

If you detect systematic leniency or severity, **normalize scores** post-hoc. Convert each rater's scores to z-scores within their own distribution, then scale back to the target range.

Formula:
- Compute rater's mean (μ) and standard deviation (σ) over all their labels
- For each label score s, compute z = (s - μ) / σ
- Map z back to target scale: s_normalized = target_mean + z × target_std

This removes systematic bias while preserving each rater's ability to discriminate between examples.

Warning: normalization assumes the true score distribution is the same across all raters. If Rater A genuinely saw harder examples than Rater B, normalization will incorrectly adjust their scores.

### Removing Outlier Raters

If a rater consistently fails to align with the rubric after multiple recalibration attempts, **remove them** from the evaluation pool.

Criteria for removal:
- Gold accuracy below 75% after recalibration
- Agreement with consensus below 60%
- Extreme leniency/severity (mean score >1 point away from cohort)
- Pattern of labeling on autopilot (duplicate labels, no variance, suspiciously fast)

Don't keep a rater just because you've invested in training them. One bad rater introduces noise that contaminates every aggregate metric they touch.

---

## Prevention Strategies: Designing Against Drift and Bias

Better to prevent drift than to correct it. Here's how to structure evaluation work to minimize degradation from the start:

### Session Length Limits (Enforced)

**Hard-code session limits** into your evaluation tooling. After 60 or 90 minutes, the tool locks and requires a break. Don't trust raters to self-regulate. Fatigue is imperceptible to the person experiencing it.

Track **quality by session duration** and use that data to refine your limits. If your team maintains quality for 75 minutes but drops off sharply at 80, set the limit to 75.

### Mandatory Breaks

Between evaluation sessions, require **at least 10 minutes away from the screen**. Not "take a break," which people interpret as checking email. Physically stand up, walk, look at something distant.

Better: build in **buffer tasks** between evaluation blocks. Have raters write feedback on the rubric, discuss edge cases with peers, or review their own score distributions. This keeps them engaged without the cognitive load of more judgments.

### Variety in Task Types

Don't have raters do the same evaluation task for eight hours. **Mix task types** within a day:
- Absolute scoring in the morning
- Pairwise comparison after lunch
- Open-ended feedback on edge cases before end of day

Variety reduces monotony and keeps raters mentally engaged. It also prevents overfitting to one type of judgment.

### Regular Calibration Refreshes

Don't calibrate once at the start and assume it sticks. Run **mini-calibrations** every week: 5–10 minutes, 3–5 examples, quick discussion of edge cases.

This continually reinforces the standard and gives raters a chance to ask questions before drift compounds.

### Gold Question Injection

Scatter **gold questions** throughout every evaluation session, not just at the start. Aim for 5–10% of all examples being gold questions.

Don't tell raters which ones are gold. Track their accuracy on these and give feedback immediately (or at end of session) so they can course-correct before drift accumulates.

---

## The Burned-Out Rater Problem

Here's a paradox: your most experienced raters are sometimes your worst raters.

After months of evaluation, expert raters develop **autopilot efficiency**. They recognize patterns instantly. They label quickly. But they've also stopped *thinking* about each example. They're applying cached judgments instead of fresh analysis.

A burned-out rater will:
- Label consistently with themselves, creating the illusion of reliability
- Miss edge cases because they're pattern-matching instead of reasoning
- Drift slowly because their mental model ossified weeks ago
- Resist recalibration because they're confident (wrongly) that they're still accurate

### Fresh Eyes vs. Experienced Judgment

There's a trade-off:

**Fresh raters** are slower, make more mistakes on easy cases, and need more supervision. But they see edge cases with fresh eyes and haven't yet drifted.

**Experienced raters** are fast, consistent, and reliable on routine cases. But they're vulnerable to drift, fatigue, and autopilot labeling.

Best structure: **blend both**. Use experienced raters for volume and routine cases, but inject fresh raters regularly to catch drift and to provide a control group that hasn't been shaped by your system's accumulated biases.

### Recognizing Burnout

Watch for:
- **Declining session duration**: Rater used to work 90-minute blocks, now quits after 45 minutes
- **Flat score distributions**: Rater used to use full scale, now clusters around mode
- **Decreased commentary**: Rater used to flag edge cases and ask questions, now labels silently
- **Speed without accuracy**: Rater is faster than ever but gold accuracy is dropping

When you see this, **rotate the rater to different tasks** or give them a break. A week off resets mental fatigue better than any amount of recalibration.

---

## Enterprise Expectations: What Leadership Wants to Hear

When you present rater quality to executives or governance boards, they care about:

**"How do we know our labels are reliable?"**
Walk through your drift detection dashboard. Show gold question accuracy trends, inter-rater agreement over time, and score distribution monitoring. Demonstrate that you're actively tracking quality, not assuming it.

**"What happens when a rater drifts?"**
Explain your correction process: automatic alerts, recalibration sessions, score normalization, and rater removal thresholds. Show that drift is caught and corrected before it corrupts downstream decisions.

**"How long can raters work before quality degrades?"**
Share your session length limits (60–90 minutes), break requirements, and the data showing quality drop-off in later hours. Emphasize that you've optimized for quality, not throughput.

**"Are our raters biased?"**
Acknowledge that bias is inevitable, then explain your mitigation strategies: randomization, blinding, demographic diversity, and within-rater designs. Position bias management as a strength, not a liability.

**"How do you prevent burnout?"**
Describe rater rotation, task variety, session limits, and the balance between fresh and experienced raters. Show that you're managing human factors proactively.

Leadership isn't expecting perfection. They're expecting **visibility and control**. Show them you're monitoring the right metrics and intervening when they degrade.

---

## 2026 Patterns: Automated Drift Detection and AI-Assisted Calibration

The cutting edge in 2026:

### Automated Drift Detection Dashboards

Modern evaluation platforms include **real-time drift detection**. Every label flows through statistical models that:
- Compare each rater's scores to gold standards
- Flag unusual score distributions or sudden shifts in mean/variance
- Track agreement trends and alert when cohort alignment degrades
- Surface examples where individual raters diverged from consensus

These dashboards auto-generate **rater quality reports** weekly, highlighting who needs recalibration and which types of cases are causing drift.

### AI-Assisted Bias Flagging

Some teams use **LLM-based bias detection**. After each evaluation session, an AI reviews a sample of the rater's labels and flags patterns:
- "This rater consistently scores informal tone lower than peers"
- "Position bias detected: first option chosen 68% of the time"
- "Potential confirmation bias: higher scores on outputs labeled 'Model A'"

The AI doesn't replace human judgment, but it surfaces patterns that would take weeks to notice manually. Raters review the flagged cases during recalibration.

### Adaptive Calibration

Instead of fixed weekly recalibration, some platforms use **adaptive calibration**: when a rater's gold accuracy drops below a threshold, the system automatically injects more gold questions and schedules a recalibration session.

This concentrates recalibration effort on raters who need it, rather than treating everyone the same.

### Fatigue Prediction Models

A few teams have built **fatigue prediction models** that track keystroke patterns, mouse movement, response time variance, and label entropy within a session. When the model detects fatigue signatures, it suggests a break or ends the session early.

Early results show 10–15% improvement in late-session label quality compared to fixed time limits.

---

## Failure Modes: What Goes Wrong When You Ignore Rater Quality

Let me show you what happens when you don't actively manage drift, fatigue, and bias:

**Silent dataset corruption.** Your labels slowly diverge from the rubric without triggering any alarms. By the time you notice, you've accumulated months of drifted labels that contaminate every model decision built on them.

**Regression test failures.** You think you've regressed because evals show declining performance, but really you've just swapped a lenient rater for a harsh one. You waste weeks debugging a phantom problem.

**Model comparison bias.** You A/B test two models, but position bias systematically favors Model A. You ship the wrong model because your evaluation process was broken.

**Rater churn.** You push raters too hard without breaks or recalibration support. They burn out and quit. You lose institutional knowledge and spend months training replacements who will also burn out.

**Undetected cultural bias.** All your raters come from one demographic, so your rubric encodes that demographic's preferences as universal truth. Your product alienates users from other backgrounds.

**Gold question gaming.** Raters figure out which examples are gold questions (because you use the same ones repeatedly) and memorize the "correct" answers without understanding the rubric. Gold accuracy looks great, but real label quality is terrible.

Rater quality isn't an optional nice-to-have. It's the foundation that everything else rests on. Ignore it and your entire evaluation program becomes an expensive way to generate confident nonsense.

---
