# 2.6 Multi-Turn Conversation Scoring

A customer support agent handled seven turns beautifully — polite acknowledgment, gathered account details, checked history, offered a solution, confirmed understanding, processed the fix, and closed warmly. Then analytics flagged it: the solution offered in turn 4 contradicted information the system had already shared in turn 2. The customer noticed. The conversation failed. **Which turn do you score as broken?**

This is the attribution problem in multi-turn conversations. Unlike single exchanges where you evaluate one input and one output, conversations build context across turns. Turn 7's success depends on turns 1 through 6. A mistake in turn 2 might not surface until turn 5. An agent might nail every individual response yet fail the overall goal.

Let me walk you through how to score conversations when context carries across turns, how to decide what to measure at the turn level versus the conversation level, and how to handle the messy reality that quality in multi-turn systems is both local and cumulative.

---

## Why Multi-Turn Scoring Is Different

In Chapter 2.1 we covered task-specific definitions for single exchanges. You evaluate one user message, one assistant response, done. But conversations don't work that way.

**Turn-level dependencies.** Turn 7 references "the account we discussed earlier." If you score turn 7 in isolation, you miss that it relies on turn 3 successfully capturing the account number. The response quality at turn 7 depends on memory from turn 3.

**Context carryover.** The model doesn't just respond to the current message — it responds to the current message plus the full conversation history. A factually correct answer in turn 4 becomes factually incorrect in turn 6 if the user corrected a detail in turn 5 that the model ignored.

**Cumulative goals.** A customer service conversation isn't successful because turn 8 was polite. It's successful because by turn 8, the customer's problem was solved, they felt heard, and no contradictory information was given along the way. You need conversation-level success metrics, not just turn-level quality checks.

This creates a scoring architecture question: do you evaluate each turn independently, evaluate the whole conversation as one unit, or both?

---

## Turn-Level vs Conversation-Level Scoring

You'll typically need both. They measure different things.

**Turn-level scoring** evaluates each assistant response against criteria that can be judged in isolation or with minimal context. Examples:

- **Tone.** Is this turn polite, professional, empathetic?
- **Factual accuracy.** Does this specific claim match your knowledge base?
- **Format adherence.** Did the assistant follow the required response structure?
- **Safety.** Does this turn contain harmful content or jailbreak attempts?

Turn-level scoring is efficient. You can parallelize it across turns. You can catch problems early in long conversations. You can slice by turn number (Chapter 3.4) to see if quality degrades after turn 10.

**Conversation-level scoring** evaluates whether the entire exchange accomplished the goal. Examples:

- **Task completion.** Did the conversation resolve the customer's issue?
- **Information consistency.** Did the assistant contradict itself across turns?
- **Conversation coherence.** Did the flow make logical sense start to finish?
- **User satisfaction.** Would a human judge rate this whole conversation as helpful?

Conversation-level scoring requires reading the full context. It's slower. But it's the only way to measure what actually matters: did the conversation succeed?

**Default heuristic:** score turn-level for real-time safety and format checks. Score conversation-level for quality, coherence, and success metrics. Use both in your eval pipeline.

---

## The Attribution Problem

A seven-turn technical support conversation ends with the user frustrated. Your eval flags it as a failure. Now you need to know: which turn caused the problem?

**Turn 7 might have failed because of turn 2.** The assistant asked for the wrong diagnostic info in turn 2. The user provided it. Turns 3 through 6 built on that incorrect foundation. By turn 7, the assistant is confident but wrong. If you only score turn 7, you'll miss that the root cause was turn 2's bad question.

**How to handle attribution:**

**1. Track context dependencies.** When scoring turn N, note which prior turns it references. If turn 7 says "as I mentioned earlier," trace back to the earlier turn. If turn 7 fails, check whether the failure was in turn 7's logic or in the earlier turn it relied on.

**2. Score both local and cumulative quality.** For each turn, score:
   - **Local quality:** is this response good given perfect prior context?
   - **Cumulative quality:** is this response good given the actual prior context (including any mistakes already made)?

If turn 7 has high local quality but low cumulative quality, the problem is upstream.

**3. Use conversation-level rubrics that surface root causes.** Instead of "conversation failed," use rubrics like:
   - Did the assistant gather the right information early? (Checks turns 1-3)
   - Did the assistant maintain consistency across turns? (Checks all turns for contradictions)
   - Did the assistant recover from user corrections? (Checks if mistakes were fixed)

This gives you attribution: you know which capability broke, which points to which turns.

---

## Conversation-Level Success Metrics

Once you've scored individual turns, you still need to answer: did the conversation succeed?

**Goal completion.** Did the user's original intent get satisfied? For customer support: was the issue resolved? For a sales assistant: did the user get product recommendations and next steps? For a tutoring agent: did the student understand the concept?

Define success as a binary or rubric score applied to the full conversation. Don't average turn-level scores and call it conversation success. A conversation with eight great turns and one critical failure is a failure.

**Coherence across turns.** Did the conversation maintain logical flow? Check:
   - **Consistency:** no contradictions between turns
   - **Memory:** the assistant remembers and references earlier turns appropriately
   - **Context tracking:** the assistant stays on topic and doesn't lose the thread

These are conversation-level properties. You can't score them turn by turn.

**Error accumulation.** Small mistakes compound. An assistant mishears a name in turn 1. Uses the wrong name in turn 3. The user corrects it in turn 4. The assistant reverts to the wrong name in turn 6. Each individual mistake might be minor, but the cumulative effect is "this system doesn't listen."

Score for error accumulation by tracking whether mistakes are corrected and stay corrected. If a user corrects the assistant, all subsequent turns should reflect that correction.

---

## Scoring Multi-Turn by Channel

Different system types have different multi-turn patterns. Let me walk you through chat, RAG, agents, and voice.

**Chat (context carryover).** The simplest case. The model receives the full conversation history and generates the next turn. Score:
   - **Turn-level:** tone, safety, factual accuracy per response
   - **Conversation-level:** consistency (no contradictions across turns), context tracking (references earlier turns correctly), goal completion

Watch for long-context degradation. In 2026, models handle thousands of tokens, but quality can drift after many turns. Slice by turn number (Chapter 3.4) to detect this.

**RAG (multi-hop retrieval).** The assistant retrieves documents to answer each turn. But turn 7's query might depend on information retrieved in turn 3. Score:
   - **Turn-level:** retrieval quality (did the system pull the right docs for this turn?), grounding (did the response cite retrieved content?)
   - **Conversation-level:** multi-hop success (did the system connect information across turns?), no hallucination from outdated context (if turn 5 retrieved updated info, does turn 7 use it or still reference stale data from turn 2?)

Example failure mode: user asks "What's the return policy?" in turn 1, assistant retrieves and answers. User asks "Does that apply to electronics?" in turn 3. If the system re-retrieves and gets a different document that contradicts turn 1, you have a consistency failure. Score for this.

**Agents (multi-step workflows).** The agent takes actions across turns: search, tool calls, API requests. A task might span five turns. Score:
   - **Turn-level:** action correctness (did this tool call use the right parameters?), intermediate output quality (did this turn's result make sense?)
   - **Conversation-level:** workflow success (did the full sequence accomplish the goal?), action coherence (did the agent's plan make sense step by step?), error recovery (if a tool call failed in turn 3, did the agent adapt in turn 4?)

Agent evals (Chapter 8) go deeper here, but for multi-turn scoring the key is: individual actions might succeed while the overall plan fails. Don't just check turn-level action correctness.

**Voice (dialog state plus turn-taking).** Voice conversations add timing and turn-taking. The system must track dialog state across turns and handle interruptions. Score:
   - **Turn-level:** transcription accuracy, response latency (per turn), turn-taking appropriateness (did the system cut off the user or wait too long?)
   - **Conversation-level:** dialog state consistency (does the system track what's been discussed?), interruption recovery (if the user cuts off the assistant, does the conversation recover?), conversation flow (does it feel natural start to finish?)

Voice evals (Chapter 10) cover real-time metrics, but for multi-turn scoring focus on: does the system maintain coherent state across turns, especially after interruptions?

---

## Knobs and Defaults

Here's when to use turn-level, conversation-level, or both.

**Use turn-level scoring when:**
- You need real-time monitoring and can't wait for the conversation to finish
- You're checking safety, compliance, or format rules that apply per response
- You want to slice by turn number to detect degradation over long conversations
- You're debugging and need to pinpoint exactly which turn broke

**Use conversation-level scoring when:**
- You're measuring task success or goal completion
- You're evaluating coherence, consistency, or memory across turns
- You're assessing user satisfaction or overall conversation quality
- You're building evals for system-level releases (Chapter 13)

**Use both when:**
- You're running full eval pipelines (most of the time)
- You need turn-level metrics for debugging and conversation-level metrics for success tracking
- You want to attribute failures (conversation-level flags a problem, turn-level helps you find the cause)

**Default setup for multi-turn evals:**
- Run turn-level scoring for safety, tone, factual accuracy
- Run conversation-level scoring for goal completion, consistency, coherence
- Store both in your eval dataset (Chapter 3.3) so you can analyze at both granularities
- Use conversation-level metrics for release gates (Chapter 13), turn-level metrics for debugging

---

## Failure Modes

Here's what breaks in multi-turn scoring and how to catch it.

**Scoring turns in isolation when context matters.** You evaluate turn 7 and mark it accurate. But turn 7 says "yes that account is active" when turn 4 already established the account was closed. If your turn-level eval doesn't check consistency with prior turns, you miss this.

**Fix:** for turns that reference earlier context, include relevant prior turns in the eval prompt. Don't score turn 7 alone — score turn 7 given turns 1-6.

**Averaging turn-level scores to get conversation-level quality.** Eight turns scored 9 out of 10, one turn scored 1 out of 10 (gave the wrong account number). Average: 7.9 out of 10. But the conversation failed because the wrong account number broke everything downstream.

**Fix:** use conversation-level rubrics (Chapter 2.2) that explicitly check for critical failures. Don't average turn scores.

**Ignoring error accumulation.** Each turn is "mostly fine" but small mistakes compound. By turn 10, the conversation is incoherent even though no single turn was terrible.

**Fix:** score for cumulative errors. Check whether mistakes are corrected and stay corrected. Track consistency across all turns, not just pairwise.

**Attributing conversation failure to the wrong turn.** You flag turn 7 as broken, retrain the model, improve turn 7's accuracy. The conversation still fails because the problem was turn 2's question, not turn 7's answer.

**Fix:** use the attribution techniques from earlier. When a conversation-level eval fails, trace back to find the root cause turn. Check context dependencies.

**Not slicing by turn number.** Your multi-turn eval passes on average, but quality degrades significantly after turn 12. You don't notice because you're only looking at conversation-level averages.

**Fix:** slice by turn number (Chapter 3.4). Check if accuracy, consistency, or coherence drops as conversations lengthen. This is especially important in 2026 with long-context models where subtle degradation can hide in averages.

---

## Enterprise Expectations

Here's what multi-turn scoring looks like in production systems that need to scale.

**Turn-level and conversation-level metrics in dashboards.** Enterprises run both continuously. Turn-level metrics power real-time monitoring dashboards (Chapter 11) that alert if safety or tone violations spike. Conversation-level metrics power weekly quality reviews and release gates (Chapter 13).

**Attribution tooling.** When a conversation-level metric drops, teams need to quickly identify which turns are causing failures. Expect tooling that surfaces:
   - Which turn numbers have the most failures?
   - What patterns of turns (e.g., after user corrections, after tool calls) correlate with conversation failure?
   - Which conversation-level failures trace back to which turn-level errors?

**Conversation slicing.** Not just by turn number. Slice by conversation length (short vs long), by user segment (new vs returning), by conversation type (simple query vs multi-step task). Different slices may have different failure modes.

**Replay and debugging.** Enterprises store full conversation logs and can replay them through updated models or prompts. This requires:
   - Logging full conversation history, not just final outcomes
   - Eval infrastructure that can score archived conversations, not just live ones
   - Version control for prompts and models so you can attribute changes in multi-turn quality to specific updates

**Multi-turn regression tests.** Every release runs conversation-level evals on a curated set of multi-turn scenarios (Chapter 12). These aren't just "does turn 7 work" tests. They're "does this ten-turn customer service flow complete successfully" tests.

**Human eval integration.** Conversation-level quality often requires human judgment. Enterprises run regular human evals (Chapter 6) on full conversations, not just individual turns. Use turn-level automated evals to filter which conversations humans should review.

---

## Interview Q&A

**Q: When should I score at the turn level versus the conversation level?**

**A:** Use turn-level scoring for properties that can be judged per response — safety, tone, factual accuracy, format adherence. Use conversation-level scoring for properties that require full context — goal completion, consistency across turns, coherence, error accumulation. In practice, you'll run both. Turn-level metrics help you debug and monitor in real-time. Conversation-level metrics tell you if the system actually succeeded at its job. If you had to pick one, pick conversation-level for success metrics and turn-level for debugging.

**Q: How do I handle the attribution problem when a conversation fails?**

**A:** Start with conversation-level evals to detect failure. Then trace back through turns to find the root cause. Use rubrics that surface which capability broke (information gathering, consistency, recovery from corrections). Check context dependencies — if turn 7 references turn 3, and turn 7 fails, check if turn 3 set it up to fail. Store turn-level scores alongside conversation-level scores so you can correlate them. Build tooling that surfaces patterns like "conversation failures after user corrections often trace back to turn 2 not capturing the correction." The goal is to move from "this conversation failed" to "turn 2's lack of context tracking causes downstream failures by turn 6."

**Q: My evals pass on average, but some conversations degrade after many turns. How do I catch this?**

**A:** Slice by turn number. Calculate your conversation-level success rate separately for conversations that end at turn 5, turn 10, turn 15, turn 20. If success drops significantly after turn 12, you have long-context degradation. Also slice turn-level metrics by turn number — does factual accuracy or consistency drop as turn number increases? This is critical in 2026 with long-context models that technically support hundreds of turns but may have subtle quality degradation. Don't just look at averages. Look at performance curves across turn counts.

---

Next, Chapter 2.7 will cover how to define and score quality for multi-modal interactions — when your system handles not just text but images, audio, video, and structured data, and quality means different things for each modality.
