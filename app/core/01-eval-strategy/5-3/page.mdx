# Chapter 5.3 — Balancing & Difficulty (Easy / Normal / Hard / Adversarial)

**What we're doing here:**
If your eval dataset is too easy, your metrics lie.
If it's too hard, your team ignores the results.
If difficulty is unbalanced across slices, you'll ship confidently in one area and silently fail in another.

Difficulty balancing is how you make your evaluation dataset **honest** — reflecting the real spread of easy, moderate, hard, and adversarial cases your system will face in production.

**Enterprise outcome:**
A dataset where difficulty is labeled, tracked, and balanced — so your metrics tell the truth about quality across all risk tiers, slices, and channels.

---

## 1) Mechanics: why difficulty mix matters

Most teams start with whatever examples they have.
The result: 70–80% of cases are "normal" or easy, and the hard/adversarial tail is barely represented.

What happens next:
- **Metrics look great** — because most cases are easy
- **Leadership approves a release** — based on those metrics
- **Production fails** — on the hard, ambiguous, multi-constraint, adversarial cases that real users actually send

This is the "easy dataset trap," and it's one of the most common eval failures in enterprise AI.

**The 2026 reality:**
Models have become very good at easy cases. By mid-2026, most frontier models saturate traditional benchmarks (MMLU scores above 90%, for example). The signal now lives almost entirely in the hard and adversarial tail. If your dataset doesn't include that tail, your eval is worthless for differentiating model versions or catching regressions.

---

## 2) The four difficulty levels (definitions + examples)

### 2.1 Easy
**Definition:** Single-step, clear intent, unambiguous, well-covered by training data and docs.

Examples by channel:
- **Chat:** "What are your business hours?" — single fact, no ambiguity
- **RAG:** Query with one obvious matching doc, answer is a direct quote
- **Agent:** Single tool call, no branching, no confirmation needed
- **Voice:** Clear speech, quiet environment, simple command ("Check my balance")

### 2.2 Normal
**Definition:** Typical production complexity. May require moderate reasoning, context use, or multi-step logic, but not edge-case territory.

Examples by channel:
- **Chat:** "I want to change my plan but keep my current billing date" — requires understanding plan rules
- **RAG:** Query requires synthesizing 2–3 chunks, answer needs slight rephrasing
- **Agent:** 2–3 tool calls in sequence, standard parameters, expected success
- **Voice:** Some background noise, natural speech with filler words, standard request

### 2.3 Hard
**Definition:** Requires complex reasoning, handles ambiguity, involves multiple constraints, or tests rare-but-real scenarios.

Examples by channel:
- **Chat:** Multi-constraint request with conflicting rules ("I want the cheapest plan that includes international and has no contract, but I'm a student")
- **RAG:** Query is semantically close to a wrong doc (retrieval trap), correct answer requires careful grounding
- **Agent:** Multi-tool workflow with a partial failure mid-sequence, requires fallback logic
- **Voice:** Heavy accent, barge-in mid-sentence, noisy environment, multi-turn with corrections

### 2.4 Adversarial
**Definition:** Intentionally designed to break or manipulate the system. Tests safety boundaries, prompt injection resistance, policy edge cases, and failure resilience.

Examples by channel:
- **Chat:** Prompt injection attempt, social engineering to bypass refusal, requests that sound reasonable but violate policy
- **RAG:** Queries designed to make the model cite non-existent sources or hallucinate authority
- **Agent:** Requests for prohibited tool sequences, attempts to bypass confirmation requirements
- **Voice:** Spoofed identity attempts, abusive language, rapid topic-switching to confuse dialog state

---

## 3) Default mix ratios (recommended starting points)

These ratios are practical defaults. Adjust based on your product's risk profile.

### 3.1 General assistant (Tier 0–1 heavy)

| Difficulty | Ratio |
|---|---|
| Easy | 15–20% |
| Normal | 55–65% |
| Hard | 15–20% |
| Adversarial | 5–10% |

### 3.2 High-stakes product (Tier 2–3 heavy)

| Difficulty | Ratio |
|---|---|
| Easy | 10% |
| Normal | 45–50% |
| Hard | 25–30% |
| Adversarial | 15–20% |

### 3.3 Safety/red-team suite (dedicated)

| Difficulty | Ratio |
|---|---|
| Easy | 0% |
| Normal | 10% |
| Hard | 30% |
| Adversarial | 60% |

**Why adversarial gets its own heavy allocation in safety suites:**
Safety failures almost never happen on easy cases. If you're testing safety, you need adversarial-dominant datasets — otherwise you're testing the absence of attacks, not resistance to them.

---

## 4) How to assign difficulty labels

Difficulty labeling is not "just guess." Use a structured approach.

### 4.1 Heuristic signals (automated first pass)

Score each case on:
- **Query complexity:** number of constraints, multi-hop reasoning, ambiguity
- **Context requirements:** how many documents/chunks needed, how much conversation history matters
- **Action complexity (agents):** number of tools, branching, error handling paths
- **Environmental factors (voice):** noise level, accent diversity, interruptions

Simple heuristic: count the "hard factors" present.
- 0 factors → easy
- 1–2 factors → normal
- 3–4 factors → hard
- Intentionally adversarial design → adversarial

### 4.2 Human review pass (required for gold sets)

After automated labeling:
- Sample 20% of each difficulty bucket
- Have a domain expert confirm or override
- Pay special attention to the easy/normal boundary (the most commonly mislabeled)
- Pay special attention to hard vs adversarial (adversarial must be intentional, not just hard)

### 4.3 Production signals (feedback loop)

Use production data to validate difficulty labels:
- **High retry rate** → likely hard or mislabeled as normal
- **High escalation rate** → likely hard
- **High thumbs-down rate** → could be hard, could be model failure on normal
- **Flagged/reported** → potentially adversarial

This feedback loop lets you re-calibrate difficulty labels over time based on real-world performance.

---

## 5) Balancing across slices (the cross-dimensional problem)

Difficulty must be balanced **within each slice**, not just overall.

If your English cases are 60% easy but your Spanish cases are 80% hard, your per-language comparison is broken — any quality difference might just reflect difficulty imbalance.

### 5.1 The balancing matrix

For each slice dimension (language × tier × tenant × channel), check:
- Does the difficulty distribution roughly match the target mix?
- Are any slices missing adversarial cases entirely?
- Are any slices dominated by easy cases?

### 5.2 Practical approach

You don't need perfect balance. You need:
- Every tier has representation at every difficulty level
- No slice has more than 2× the target ratio for any difficulty level
- Adversarial cases exist for every Tier 2–3 slice

If a slice is under-represented at a difficulty level, use synthetic generation to fill the gap (5.1 source rules apply).

---

## 6) Knobs & defaults (what you actually set)

### 6.1 The difficulty_label field
Add to every eval example:
```
difficulty: easy | normal | hard | adversarial
difficulty_source: heuristic | human | production_signal
difficulty_version: v1.0
```

### 6.2 Rebalance cadence
- **Monthly:** check difficulty distribution per slice, adjust if drifted
- **Per model upgrade:** re-run difficulty heuristics (what was "hard" for v1 may be "easy" for v2)
- **Per dataset expansion:** ensure new cases don't skew the mix

### 6.3 Over-sampling and under-sampling rules
- Never drop hard/adversarial cases to "fix" low scores — that's gaming, not balancing
- Over-sample hard cases in Tier 2–3 suites
- Under-sample easy cases if the dataset is too large (cut easy first, never hard)

---

## 7) Failure modes (symptoms + root causes)

### 7.1 "Our metrics look great but production quality is bad"
Root causes:
- Dataset is 70%+ easy cases
- Hard/adversarial tail is under-represented
- No difficulty-stratified reporting

Fix:
- Audit difficulty distribution
- Add hard + adversarial packs
- Report metrics sliced by difficulty (not just overall)

---

### 7.2 "Our eval scores are so low the team stopped trusting them"
Root causes:
- Dataset is too adversarial-heavy
- Hard cases dominate normal production distribution
- Team confuses "hard eval" with "bad model"

Fix:
- Rebalance toward production-realistic mix
- Separate safety/adversarial suite from quality tracking suite
- Report difficulty-level scores separately so teams see where failures actually are

---

### 7.3 "Scores improved but nothing changed"
Root causes:
- Difficulty drift — model got better, so "hard" cases became effectively "normal"
- Dataset difficulty was never recalibrated after model upgrade

Fix:
- Re-assess difficulty labels after every major model change
- Track "model pass rate by difficulty" over time — if "hard" pass rate hits 90%, those aren't hard anymore

---

### 7.4 "Adversarial cases leak into the normal test set"
Root causes:
- No clear labeling distinction between "hard real scenario" and "intentionally adversarial"
- Cases created for red-teaming were added to the general eval set

Fix:
- Enforce adversarial tag at creation time
- Separate adversarial suite from general quality suite
- Gate adversarial cases: only add to general eval if they represent realistic production risk

---

## 8) Debug playbook: auditing difficulty distribution

Run this check monthly (or per dataset version):

1. **Pull difficulty distribution** — overall and per slice
2. **Compare to target mix** — flag any slice more than 2× off-target
3. **Check adversarial coverage** — every Tier 2–3 task must have adversarial cases
4. **Check for difficulty drift** — compare model pass rate by difficulty level across last 3 runs
5. **Spot-check labels** — sample 50 cases, confirm difficulty label matches actual complexity
6. **Review production signals** — any "easy" cases with high retry/escalation rates?

If issues found:
- Relabel drifted cases
- Generate synthetic fills for under-represented difficulty × slice combos
- Update difficulty heuristics if systematic mislabeling detected

---

## 9) Enterprise expectations (what serious teams do)

- They label every eval case with difficulty and track difficulty distribution as a dataset health metric
- They maintain separate suites: general quality (balanced mix), safety/adversarial (adversarial-heavy), regression (stable mix)
- They re-calibrate difficulty after every major model or prompt change
- They report metrics sliced by difficulty level — leadership sees "hard-case quality" as a distinct metric
- They treat difficulty balance as a release gate: "if adversarial coverage < X%, dataset is not ready"

---

## 10) Ready-to-use templates

### 10.1 Difficulty assignment checklist (per case)

```
case_id: ___
difficulty: easy | normal | hard | adversarial
difficulty_source: heuristic | human | production_signal

heuristic_factors:
  query_complexity: low | medium | high
  context_requirements: single | multi | complex
  action_complexity: none | linear | branching | failure_handling
  environmental_challenge: none | moderate | severe
  adversarial_intent: false | true

human_override: yes/no
override_reason: ___
```

### 10.2 Difficulty mix audit table (per slice)

| Slice | Easy (target) | Easy (actual) | Normal (t) | Normal (a) | Hard (t) | Hard (a) | Adv (t) | Adv (a) | Status |
|---|---|---|---|---|---|---|---|---|---|
| en-US, Tier 1 | 20% | ___% | 60% | ___% | 15% | ___% | 5% | ___% | OK / Flag |
| es-MX, Tier 2 | 10% | ___% | 50% | ___% | 25% | ___% | 15% | ___% | OK / Flag |
| (add rows) | | | | | | | | | |

---

## 11) Interview-ready talking points

> "I label every eval case with difficulty — easy, normal, hard, adversarial — and track the mix per slice."

> "Easy-only datasets produce fake-high scores. I balance difficulty to match production reality plus adversarial stress."

> "I re-calibrate difficulty labels after every model upgrade because what was hard for v1 may be easy for v2."

> "I report metrics sliced by difficulty level so leadership sees hard-case quality separately from easy wins."

> "Adversarial cases get their own suite with 60%+ adversarial density — you can't test safety with easy cases."
