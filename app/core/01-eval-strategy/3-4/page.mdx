# Chapter 3.4 — Slice Strategy (Segments, Languages, Tiers)

**What we're doing here:**
Averages lie. You can have "good overall quality" while one segment is suffering badly (and that segment might be your highest-paying customers).

So we use **slice strategy**: we measure quality, safety, and cost across the slices that matter:
- customer segments (enterprise vs SMB)
- user tiers (free vs paid)
- languages (English/Arabic/etc.)
- regions
- channels (chat/RAG/agent/voice)
- risk tiers (Tier 0–3)

**Outcome of this chapter:**
You'll know exactly which slices to track, how to pick "must-track" slices, and how to turn slice failures into release blocks and regression tests.

---

## 1) Mechanics: what a "slice" is (simple definition)

A **slice** is a subset of your usage defined by a label, like:
- Language = Arabic
- Tier = Enterprise
- Channel = Voice
- Intent = Refund policy
- Risk = Tier 3

When you slice your eval results, you stop asking:
- "How are we doing overall?"

And start asking:
- "Where are we failing, and who is impacted?"

That is how enterprise teams avoid hidden disasters.

---

## 2) The slice categories you should always consider

### 2.1 Customer segment slices
- **Enterprise vs SMB vs Consumer**
- **Top tenants** (top 10–50 revenue customers)
- **New customers vs long-term customers**
- **High-support customers** (heavy usage, many tickets)

Why it matters:
- Enterprises demand stability, auditability, and low incident rates.
- One enterprise tenant failure can kill a renewal.

---

### 2.2 User tier slices (product plan)
- Free vs paid
- Standard vs Pro vs Enterprise
- Add-ons enabled vs disabled (RAG, voice, tools)

Why it matters:
- Different tiers often have different policies, tools, and latency budgets.
- Enterprise often has stricter privacy rules and custom knowledge bases.

---

### 2.3 Language slices
- English vs Arabic (or any top languages you support)
- Dialects (where relevant)
- Mixed-language messages (code-switching)

Why it matters:
- Safety and refusal behavior can vary across languages.
- Retrieval performance can drop sharply in multilingual corpora.
- "Same intent" can be expressed very differently.

**Minimum expectation:**
If you operate in multiple languages, you must have:
- multilingual eval sets
- multilingual safety packs
- multilingual "must abstain" cases for RAG

---

### 2.4 Region / market slices
- MENA vs EU vs US (or your markets)
- Country-specific policies, holidays, regulations
- Local payment methods, phone formats, addresses

Why it matters:
- Voice and support bots fail on local formats (names, addresses, numbers).
- Policy content differs by region (refund, tax, compliance).

---

### 2.5 Channel slices (product surface)
- Chat vs RAG vs Agent vs Voice
- Multi-modal (if applicable)

Why it matters:
- Each channel has different failure modes:
  - RAG: retrieval/grounding
  - Agents: tool correctness/state/idempotency
  - Voice: ASR + turn-taking + confirmation

---

### 2.6 Risk tier slices (Tier 0–3)
- Tier 0–1: low/medium stakes
- Tier 2–3: high/regulatory/critical

Why it matters:
- Tier 3 should have strict release gates.
- You never want "overall is good" to hide Tier 3 failures.

---

### 2.7 Difficulty slices
- easy / normal / hard
- adversarial / red-team

Why it matters:
- A system can look great on easy cases and still be fragile.

---

### 2.8 Device / environment slices (especially for voice)
- mobile vs desktop
- noisy environments vs quiet
- speakerphone vs headset
- interruption heavy vs calm

Why it matters:
- Voice quality is often dominated by real-world conditions, not the model.

---

## 3) Knobs & defaults (what you actually set)

### 3.1 "Must-track slices" (default list)
If you want a strong 2026 baseline, always track these:

- Top intents (head) x Tier 2–3
- Top 10–50 enterprise tenants (if B2B)
- Each supported language
- Each channel (chat/RAG/agent/voice)
- Safety suite results by language + channel
- Cost per successful task by tier (free vs paid vs enterprise)

### 3.2 Slice size minimums (practical)
Slices that are too small will mislead you.

Use these minimums per slice in eval datasets:
- Tier 0–1: 20–50 examples
- Tier 2: 50–150 examples
- Tier 3: 150–300+ examples + safety suite

If you can't get enough real data:
- supplement with expert-written and synthetic cases,
- but keep a stable "real logs pack" where possible.

### 3.3 Slice gating rules (when a slice blocks release)
A simple enterprise rule:
- Tier 3 slice failure can block a release even if overall looks fine.
- Safety/PII failure in any slice blocks release.

---

## 4) Failure modes (symptoms + root causes)

### 4.1 "Average is fine, but a big customer is furious"
Root causes:
- no top-tenant slice
- tenant-specific knowledge base differences
- unique workflows not represented in evals

Fix:
- keep a "Top Tenants Pack"
- add tenant-specific regression tests
- slice scorecards per tenant for Tier 2–3 tasks

---

### 4.2 "English is good, Arabic is unreliable"
Root causes:
- dataset is mostly English
- safety suite not multilingual
- retrieval embeddings/chunking not tuned for Arabic content

Fix:
- multilingual eval sets by intent
- multilingual safety variants
- bilingual RAG tests (Arabic question, English docs, and vice versa)

---

### 4.3 "Voice works in the lab, fails in real calls"
Root causes:
- no slice for noise/interruptions
- no critical-field confirmation enforcement
- latency budget ignored in tool-heavy flows

Fix:
- voice eval packs for real environments
- enforce confirmation rule for critical fields
- monitor silence gaps and interruption behavior

---

### 4.4 "Paid tier is worse than free tier"
Root causes:
- paid tier has extra tools/features that fail
- stricter policies cause over-refusal
- heavier context windows cause confusion

Fix:
- treat tier as a first-class slice
- separate eval sets per tier configuration

---

## 5) Debug playbook: how to build slice-aware evaluation

### Step 1 — Pick your "slice shortlist"
Don't track 200 slices. Track the ones that matter most.

Pick:
- 5–10 "must-track" slices
- 10–30 "watch" slices

### Step 2 — Ensure each slice has representation
- enforce minimum examples per slice
- enforce difficulty mix per slice (easy/normal/hard)

### Step 3 — Report slice results alongside overall
In dashboards:
- show overall
- show worst 5 slices
- show Tier 3 slices separately

### Step 4 — When a slice fails, turn it into action
- add regression tests for that slice
- add new examples to gold set
- assign an owner and deadline

---

## 6) Enterprise expectations (what serious teams do)

- Slice scorecards are reviewed weekly (or per release)
- They maintain:
  - **Top Tenants Pack**
  - **Multilingual Pack**
  - **Safety Pack by language**
  - **Tier 3 Pack**
  - **Voice Real-World Pack** (if voice exists)
- They use canaries:
  - release to 1–5% traffic or specific tenants first
  - monitor slices before full rollout

---

## 7) Ready-to-use templates

### 7.1 Slice scoreboard (copy/paste)

| Slice | Metric | Target | Current | Trend | Status | Owner | Notes |
|---|---|---:|---:|---|---|---|---|
| Enterprise Tier | Task success | ≥ 90% | __ | __ | __ | __ | __ |
| Arabic | Safety pass rate | 100% | __ | __ | __ | __ | __ |
| Voice (noisy) | Completion | ≥ 80% | __ | __ | __ | __ | __ |
| Tier 3 tasks | Confident-wrong | ~0 | __ | __ | __ | __ | __ |
| Top Tenant #1 | Escalation rate | ≤ X% | __ | __ | __ | __ | __ |

### 7.2 Slice selection checklist
- Revenue impact (top tenants)
- Risk impact (Tier 2–3)
- Market coverage (languages/regions)
- Channel differences (chat/RAG/agent/voice)
- Known pain points (support complaints, past incidents)

---

## 8) Interview-ready talking points

> "I don't trust averages. I slice results by tenant, language, tier, channel, and risk."

> "Tier 3 and safety/PII are hard gates in every slice."

> "I maintain dedicated packs: top tenants, multilingual safety, tool failures, and voice real-world conditions."

> "Slice failures become regression tests and ownership tickets."
