# Chapter 3.3 — Sampling Strategy (Head vs Long-Tail)

**What we're doing here:**
A coverage map tells you *what* you should test. Sampling strategy tells you *which real examples* to include so your evals actually match production.

**Big reality:**
Most AI systems look great on "head" traffic (common requests) and fail on the "long tail" (weird phrasing, rare intents, messy edge cases).
Enterprise evals must cover both — on purpose.

---

## 1) Mechanics: what "head vs long-tail" means

### Head (the common stuff)
The top intents that happen all day, every day. Examples:
- "Reset password"
- "What's your refund policy?"
- "Summarize this"
- "Draft an email"

### Long-tail (rare, messy, risky)
Rare intents, unusual phrasing, weird contexts, edge conditions. Examples:
- "My account is locked but I changed my email and don't have my old phone"
- "Refund policy when a reseller is involved"
- "User asks the same thing in 3 languages in one message"
- "Tool returned partial data and then timed out"

**Why long-tail matters:**
A small number of long-tail failures can create:
- safety incidents
- brand damage
- escalations
- legal risk
- big customer churn (especially enterprise tenants)

---

## 2) The goal: match production risk, not just production volume

Bad sampling:
- 100% sampled by frequency (you only test the head)

Better sampling:
- sample by **frequency + risk + known failure modes**

Enterprise sampling is always a mix:
- enough head cases to track day-to-day quality
- enough long-tail and adversarial cases to prevent surprises

---

## 3) Knobs & defaults (what you actually set)

### 3.1 Default sampling mix (strong starter)

A practical starting point for most products:

- **50–70% Head (top intents)**
- **20–35% Long-tail (rare intents + hard cases)**
- **10–15% Adversarial / Safety / Abuse**

If your product is high-stakes (Tier 2–3 heavy), shift toward risk:
- **40–60% Head**
- **25–40% Long-tail**
- **15–20% Safety/Adversarial**

### 3.2 Per-intent quotas (so head doesn't dominate)

For the top 10 intents, set a quota like:
- 30–100 examples each (depends on your scale)

Then force the rest of the dataset to include:
- rare intents
- edge cases
- multi-turn conversations

### 3.3 Freshness window
- Update sampling regularly (weekly or per release)
- Always include the most recent failure patterns from production

---

## 4) How to actually sample (practical methods)

### 4.1 Frequency-based sampling (for head)
- Count intent frequency from logs
- Sample proportionally, but cap each top intent so it doesn't eat the dataset

### 4.2 Risk-based sampling (for long-tail)
Over-sample:
- Tier 2–3 tasks (account, payment, identity)
- Regulated domains
- Anything involving PII
- Agent actions (tool calls)
- Voice critical fields (numbers, addresses)

### 4.3 Failure-mode sampling (learn from reality)
Every time something breaks in production:
- add similar cases immediately
- create "near-miss" variants (same bug, different phrasing)

### 4.4 Slice-based sampling (fairness + coverage)
Ensure representation across:
- language and dialect
- region
- device/channel
- enterprise tenants (top customers)
- novice vs power users

---

## 5) Failure modes (symptoms + root causes)

### 5.1 "We test a lot but still get surprised"
Root causes:
- too much head sampling
- no tool failure cases for agents
- no "must abstain" cases for RAG
- no noisy audio / interruption cases for voice

### 5.2 "Metrics look stable but users complain"
Root causes:
- dataset is stale and not updated
- sampling doesn't include new intents/features
- customer-specific issues hidden by global averages

### 5.3 "Safety incidents happen rarely but severely"
Root causes:
- safety suite too small
- no adversarial sampling
- missing multilingual abuse phrasing

---

## 6) Debug playbook: build a sampling pipeline that works

### Step 1 — Start from real logs (when possible)
- Pull a large log sample (last 7–30 days)
- Classify by intent/task using your taxonomy

### Step 2 — Apply quotas
- Set caps for top intents
- Reserve budget for long-tail and safety

### Step 3 — Add "hard packs"
Create fixed packs that always stay in the dataset:
- **Tool failure pack** (agents)
- **Abstain pack** (RAG)
- **Ambiguity pack** (multiple valid answers)
- **Safety pack** (red-team prompts)
- **Voice pack** (noise + interruptions + critical fields)

### Step 4 — Track drift
Every week:
- compare the real intent distribution vs eval distribution
- update if new intents appear

### Step 5 — Turn incidents into regression tests
Every incident must produce:
- 1–5 new eval cases
- a new regression test
- a coverage map update

---

## 7) Enterprise expectations (what serious teams do)

- Maintain a "living dataset" refreshed weekly
- Maintain a separate "never break" regression suite:
  - critical flows
  - known incident cases
  - contractual requirements (SLA, compliance)
- Slice sampling by:
  - top enterprise tenants
  - revenue impact
  - support volume
  - high-risk workflows

---

## 8) Ready-to-use templates

### 8.1 Sampling plan (copy/paste)

**Eval Dataset Name:**
**Time window:** last 14/30 days + fixed regression packs
**Total examples:** ___

**Mix targets:**
- Head: ___%
- Long-tail: ___%
- Safety/adversarial: ___%

**Head intents (caps):**
- Intent A: max ___ examples
- Intent B: max ___ examples
- Intent C: max ___ examples

**Long-tail rules (must include):**
- Tier 2–3 tasks: ___ examples minimum
- Multi-turn: ___ examples minimum
- Tool failures (agent): ___ examples minimum
- Must-abstain (RAG): ___ examples minimum
- Voice interruptions/noise: ___ examples minimum

**Slices required:**
- Languages: ___
- Regions: ___
- Customer tiers: ___
- Top tenants: ___

---

### 8.2 "Good sampling" checklist

- Head intents represented but capped
- Long-tail reserved budget exists (not accidental)
- Tier 2–3 tasks are oversampled
- Safety/adversarial pack included every time
- Tool failures included for agents
- Must-abstain cases included for RAG
- Noisy/interrupt voice cases included for voice
- Dataset refreshed regularly with real logs
- Incidents become regression tests

---

## 9) Interview-ready talking points

> "I don't sample only by frequency. I sample by frequency + risk + failure modes."

> "I cap top intents so head traffic doesn't hide long-tail failures."

> "I maintain fixed 'hard packs' for tool failures, abstain behavior, safety, and voice interruptions."

> "Every production incident becomes new eval cases and regression tests."
