# 8.5 — Error Recovery & Robustness

## The Glass Bridge Test

In 2019, a team at a major cloud provider ran a disaster recovery drill. They simulated a catastrophic database failure in production. The on-call engineer followed the runbook perfectly—restored from backup, verified data integrity, brought services back online. Two hours later, they discovered the drill had accidentally run against the real production database. The simulated failure became a real one. But because the engineer had practiced recovery so thoroughly, the actual incident was resolved faster than most routine outages.

This is the paradox of reliability: **you get good at handling failures by deliberately causing them.**

For AI agents, the same principle applies. An agent that passes 99% of your happy-path test cases will fail spectacularly in production the moment an API times out, a user changes their mind mid-task, or a tool returns corrupted data. Recovery isn't a backup plan—it's the main event. Because in production, failures aren't edge cases. They're Tuesday afternoon.

This chapter is about evaluating agents under adversity. Not just whether they complete tasks when everything works, but how they behave when everything breaks.

---

## Why Recovery Matters More Than Happy Paths

Most agent evaluations focus on capability: can the agent solve the problem? But production agents face a different question: can the agent stay functional when the environment is hostile?

Consider a customer service agent booking a flight. Happy-path evaluation tests whether it can find flights, compare prices, and complete the booking. But production reality looks different:

The airline API times out. The customer changes their destination mid-conversation. The payment service returns a cryptic error code. The agent checks availability, but by the time it tries to book, the seat is gone. The customer's saved payment method was deleted. The session expires during a long deliberation.

**An agent that handles the first scenario perfectly but crashes on any of the others is not production-ready.** It doesn't matter if it scores 95% on your test suite if it creates a support ticket every time it hits a timeout.

Recovery capability determines operational cost. Every unhandled failure becomes a human escalation. Every infinite retry loop burns tokens and time. Every state corruption creates subtle bugs that propagate through future interactions.

The best production agents aren't the ones that never fail—they're the ones that fail elegantly, recover quickly, and maintain context through chaos.

---

## Chaos Testing for Agents

In 2011, Netflix introduced Chaos Monkey, a tool that randomly terminates production instances to ensure systems can survive failures. The insight was profound: if you don't deliberately break things, you never learn how they break. And if you don't know how they break, you can't build resilience.

**Chaos testing for agents applies the same philosophy.** Instead of hoping your agent handles failures gracefully, you inject failures deliberately and measure the response.

The core pattern: run your agent evaluation suite, but randomly inject failures into the environment. Then measure not just task success, but recovery quality.

Chaos testing reveals failure modes that normal testing misses:

- Agents that succeed when tools work but spiral into infinite loops when they fail
- Recovery strategies that work for timeouts but not for malformed responses
- State management that breaks when operations partially complete
- Graceful degradation that works for data errors but not permission errors

The key difference from traditional testing: **chaos testing measures resilience, not capability.** You already know the agent can complete the task. The question is whether it can complete the task while the ground is shifting beneath it.

---

## Failure Injection Categories

Effective chaos testing requires structured failure injection. Random errors are useful for discovery, but systematic evaluation requires categorizing failure modes.

**Tool failures** simulate infrastructure problems. The API is down—the tool returns a 503 error. The API is slow—the tool times out after 30 seconds. The API is broken—it returns a 200 status but malformed JSON. The API changed—the endpoint moved, the schema evolved, or authentication requirements shifted.

These failures test retry logic, timeout handling, fallback strategies, and error interpretation. Does the agent distinguish between transient failures worth retrying and permanent failures that need a different approach?

**Data failures** simulate unexpected or corrupted information. Required fields are missing. Data types don't match expectations—a number field contains a string, a date is malformed. Returned data is incomplete—a search returns zero results, an object is missing critical attributes. Data is inconsistent—a customer record shows conflicting information.

These failures test input validation, null handling, partial data processing, and graceful degradation. Can the agent work with what it has, or does it require perfect data?

**User failures** simulate human unpredictability. The user changes their mind mid-task—"actually, I want the blue one instead." The user provides contradictory instructions—first asking for the cheapest option, then demanding premium features. The user is vague—"something like what I ordered last time" without specifying what that was. The user abandons the interaction mid-stream, then returns hours later.

These failures test context management, instruction reconciliation, assumption validation, and session recovery. Can the agent adapt to shifting requirements without losing coherence?

**Environment failures** simulate platform and permission issues. Rate limits are hit—the agent makes too many requests too quickly. Quotas are exceeded—the user's account can't perform the requested operation. Permissions are denied—the agent lacks access to required resources. Dependencies are unavailable—a required service is in maintenance mode.

These failures test resource awareness, permission handling, alternative pathways, and user communication. Can the agent recognize environmental constraints and adjust its approach?

---

## Recovery Strategies to Test

When failures occur, agents have several response patterns. Effective evaluation measures which strategies the agent employs and whether it chooses appropriately for the failure type.

**Retry with backoff** is the most common pattern. The agent attempts the operation again, waiting longer between each attempt. Evaluation criteria: Does the agent retry transient failures but not permanent ones? Does backoff timing scale appropriately? Is there a maximum retry limit? Does the agent inform the user about retry attempts?

Example evaluation: Inject a timeout on first attempt, success on second. Measure whether the agent retries and task completes. Then inject three consecutive timeouts. Measure whether the agent gives up rather than retrying forever.

**Fallback to alternative tool** means using a different approach when the primary fails. Evaluation criteria: Does the agent recognize when fallback is appropriate? Are alternative tools functionally equivalent? Does the agent maintain context across tools?

Example evaluation: Make the primary search API unavailable. Measure whether the agent switches to an alternative search method and completes the task. Verify the results are comparable to the primary path.

**Graceful degradation** means completing what's possible when full completion isn't. Evaluation criteria: Does the agent identify which parts of the task are still achievable? Does it complete partial work rather than abandoning entirely? Does it clearly communicate what was and wasn't completed?

Example evaluation: Request a report with five sections, but make data for two sections unavailable. Measure whether the agent generates the three available sections and reports the gaps.

**Escalation to human** means recognizing when automation has reached its limits. Evaluation criteria: Does the agent escalate appropriate failures? Does it provide enough context for human handoff? Does it avoid over-escalating solvable problems?

Example evaluation: Inject a permission error the agent can't resolve. Measure whether it escalates with clear context. Then inject a temporary timeout. Measure whether it retries rather than escalating prematurely.

**Partial completion with status report** means documenting progress when interruption occurs. Evaluation criteria: Does the agent track completed steps? Can it resume from interruption point? Does the status report include enough detail for continuation?

Example evaluation: Interrupt a multi-step task after three of five steps. Measure whether the agent reports completed work and can resume without repeating steps.

The sophistication isn't in having these strategies—it's in choosing the right one for each failure type and context.

---

## The Infinite Loop Problem

One of the most common and dangerous failure modes: **the agent that never gives up.**

The pattern looks like this: the agent tries an operation. It fails. The agent, trained to be helpful and persistent, tries again. It fails again. The agent tries a slight variation. Still fails. The agent tries the original approach one more time. Fails. The cycle continues until token limits, timeouts, or human intervention stop it.

Infinite loops waste resources, create poor user experience, and indicate deeper reasoning problems. An agent stuck in a loop isn't just failing at the task—it's failing to recognize its own failure.

**Loop detection** requires identifying repeated patterns in agent behavior. Evaluation approaches:

Action sequence analysis: Track the sequence of tools called. If the same tool is called with the same parameters more than three times, flag as potential loop. If the same sequence of tools repeats, flag as probable loop.

State change tracking: Monitor whether agent actions change system state. If ten consecutive actions produce no state changes, the agent may be stuck.

Reasoning pattern analysis: Examine the agent's internal reasoning between attempts. If reasoning text is nearly identical across attempts, the agent isn't adapting—it's repeating.

Termination conditions: Does the agent have explicit conditions for giving up? After how many failures does it try a different approach? Is there a maximum attempt limit?

**Evaluation scenarios:**

Present an impossible task—one that cannot be completed with available tools. Measure whether the agent recognizes impossibility and how many attempts it makes before concluding this.

Present a task where the first approach fails but an alternative succeeds. Measure how quickly the agent pivots to the alternative.

Present a task where transient failures resolve themselves. Measure whether the agent retries appropriately without over-persisting.

Circular dependencies: Create a scenario where Tool A requires output from Tool B, but Tool B requires output from Tool A. Measure whether the agent detects the circular dependency or loops indefinitely.

The goal isn't to prevent all repeated attempts—persistence is often valuable. The goal is to ensure persistence is strategic, not mechanical.

---

## Graceful Degradation

When a task cannot be fully completed, the difference between a good agent and a bad one is what happens next.

**Bad agents** fail binary: they either complete the entire task or report complete failure. If a single component is unavailable, the whole operation fails. The user gets nothing.

**Good agents** degrade gracefully: they complete what's possible, clearly report what isn't, and provide value even in partial success.

Consider an agent generating a weekly analytics report with five sections: sales performance, customer acquisition, support metrics, marketing ROI, and product usage. Data for marketing ROI is unavailable due to an API outage.

Poor degradation: "Unable to generate report due to data unavailability."

Good degradation: "Report generated with four of five sections. Sales performance, customer acquisition, support metrics, and product usage are included. Marketing ROI section could not be completed because the advertising platform API is currently unavailable. This section will show 'Data Unavailable' in the report."

**Evaluation criteria for graceful degradation:**

Partial completion: Does the agent complete achievable portions of the task?

Clear communication: Does the user understand exactly what was and wasn't completed?

Useful output: Is the partial result valuable, or is it too incomplete to be useful?

Recovery path: Does the agent explain how the missing portions could be obtained later?

Appropriate judgment: Does the agent correctly assess which failures permit partial completion versus which require full abortion?

**Evaluation scenarios:**

Multi-part tasks with one component failing. Measure whether the agent completes other components.

Data aggregation with some sources unavailable. Measure whether the agent aggregates available data and notes gaps.

Workflow with optional and required steps. Measure whether the agent distinguishes between failures in required versus optional steps.

Quality thresholds: A report with 80% of data might be valuable. A financial calculation with 80% of data might be dangerous. Does the agent make appropriate judgments about when partial completion is acceptable?

The key insight: graceful degradation requires the agent to understand not just how to complete tasks, but how to assess partial value and communicate limitations clearly.

---

## State Consistency After Failure

Failures often create a more insidious problem than incomplete tasks: **corrupted agent state.**

When an operation fails, the agent's internal representation of the world should remain accurate. But in practice, partial failures often leave inconsistent state.

Example scenario: An agent is booking a meeting. It reserves a conference room, sends calendar invites, and then fails to update the booking system due to a database timeout. The agent's context now believes the meeting is fully booked, but the booking system has no record of it. In subsequent actions, the agent might reference this "booked" meeting that doesn't exist.

**State corruption patterns:**

Partial writes: The agent updates its internal state but external systems fail to update, creating divergence.

Stale reads: The agent caches information that becomes invalid after a failure.

Assumption persistence: The agent assumes an operation succeeded because it didn't see an error, when in fact it failed silently.

Context accumulation: Failed operations leave artifacts in context that future reasoning incorrectly incorporates.

**Evaluation approaches:**

State verification after failure: After injecting a failure, query the agent about system state. Does its understanding match reality?

Sequential task testing: Run a multi-step task with failure in step two. Does the agent correctly assess step one's status while acknowledging step two's failure?

Resume and retry testing: Interrupt a task, then ask the agent to continue. Does it correctly identify what was already completed?

Contradiction testing: After a failure, provide new information that contradicts the agent's assumed state. Does it recognize and reconcile the contradiction?

**Example evaluation:**

Task: Create a customer record, then send a welcome email.

Injection: Customer creation succeeds, email sending fails due to invalid email address.

State check: Ask the agent, "Was the customer created?" Expected: Yes. "Was the welcome email sent?" Expected: No, with explanation.

Follow-up: "Try sending the welcome email again." Does the agent recognize the customer already exists and only retry the email?

State consistency evaluation reveals whether the agent maintains an accurate mental model through adversity—a critical capability for multi-turn interactions and long-running tasks.

---

## Rollback and Compensation

When a multi-step task partially fails, simply stopping isn't always sufficient. Sometimes completed steps must be undone because they're now invalid without the failed steps.

This pattern, called **compensating transactions** in distributed systems, applies equally to agents. When you can't complete the whole workflow, you may need to undo the parts you already did.

Example: An agent books a flight, reserves a hotel, and schedules ground transportation. The transportation booking fails because the service area doesn't include the destination. Now the hotel reservation is problematic—it was chosen based on proximity to a transportation hub that won't be used. A sophisticated agent recognizes this and may cancel or modify the hotel reservation.

**Rollback scenarios:**

Atomic operations: All steps must succeed or all must be undone. Example: Processing a refund requires crediting the customer and updating inventory. If inventory update fails, the credit must be reversed.

Cascading dependencies: Later steps depend on earlier ones. If later steps fail, earlier ones may need revision. Example: Scheduling a meeting requires booking a room and inviting participants. If key participants decline, the room booking should be released.

Resource allocation: Steps consume limited resources. If the workflow fails, resources should be released. Example: Reserving inventory during a multi-step checkout. If payment fails, inventory should be unreserved.

State isolation: Some failures corrupt state in ways that make resumption impossible. Rollback and restart from clean state may be safer than attempting to continue.

**Evaluation criteria:**

Rollback identification: Does the agent recognize when rollback is necessary?

Rollback execution: Can the agent actually undo completed steps?

Partial rollback: Can the agent selectively undo specific steps while preserving others?

Rollback reporting: Does the agent communicate what was undone and why?

Compensation logic: When perfect rollback is impossible, does the agent take appropriate compensating actions?

**Example evaluation:**

Task: Book a business trip with flight, hotel, and car rental.

Injection: Flight and hotel succeed, car rental fails due to driver's license verification issue.

Expected behavior: Agent recognizes the car rental failure and asks whether to proceed with flight and hotel only, or cancel the entire trip. If cancellation is chosen, agent cancels hotel and flight reservations.

Measure: Does the agent recognize the dependency relationship? Does it execute cancellations correctly? Does it verify cancellation success?

This connects to **Chapter 3.7's rollback patterns** for agent workflows. Evaluation confirms whether architectural rollback capabilities are actually used correctly in practice.

---

## Robustness Scoring

Individual recovery tests measure specific capabilities. But for production readiness, you need an aggregate view: **how robust is this agent overall?**

A robustness score combines multiple recovery dimensions into a single metric that answers: "How well does this agent handle adversity?"

**Robustness score components:**

Error recovery rate: Percentage of injected failures where the agent successfully recovers and completes the task. Target: above 70% for production readiness.

Loop avoidance rate: Percentage of impossible or failing tasks where the agent terminates gracefully without looping. Target: 100%.

Degradation quality: For tasks that cannot be fully completed, average value of partial completion on a 0-5 scale. Target: above 3.5.

State consistency: Percentage of failure scenarios where the agent's internal state remains accurate. Target: above 95%.

Rollback accuracy: For scenarios requiring rollback, percentage where the agent correctly identifies and executes rollback. Target: above 80%.

Escalation appropriateness: Precision and recall for escalation decisions. Precision: percentage of escalations that were necessary. Recall: percentage of necessary escalations that occurred. Target: precision above 70%, recall above 90%.

**Aggregate calculation:**

Robustness Score = weighted average of component scores, typically weighted as:
- Error recovery: 30%
- Loop avoidance: 25%
- State consistency: 20%
- Degradation quality: 15%
- Rollback accuracy: 5%
- Escalation appropriateness: 5%

Weights adjust based on application domain. A customer service agent might weight degradation quality higher. A financial agent might weight state consistency higher.

**Scoring interpretation:**

Above 85: Production-ready robustness. Agent handles most failures gracefully.

70-85: Acceptable for supervised production. Requires monitoring and occasional intervention.

50-70: Development stage. Major robustness gaps exist.

Below 50: Not ready for production. Likely to create more problems than it solves.

**Benchmark across failure categories:**

Overall robustness scores are useful, but segment by failure type to identify specific weaknesses:

- Tool failures: 82%
- Data failures: 78%
- User failures: 65%
- Environment failures: 71%

This reveals the agent handles infrastructure issues well but struggles with user unpredictability. Training and prompt engineering can target the weak area.

Robustness scoring transforms chaos testing from exploratory to quantitative, enabling tracking over time and comparison across agent versions.

---

## Building Chaos Eval Suites

Ad-hoc failure injection is useful for exploration, but production chaos testing requires systematic organization.

**Chaos eval suite structure:**

Start with your existing task evaluation suite—the set of tasks the agent should be able to complete. This is your baseline.

For each task, create failure variants by injecting each failure category. A task that involves three tool calls might have:
- Failure variant 1: First tool call times out
- Failure variant 2: Second tool call returns malformed data
- Failure variant 3: Third tool call returns permission denied
- Failure variant 4: All tools succeed but with degraded data quality

If you have 50 baseline tasks and create four failure variants each, you now have 200 chaos test cases.

**Severity levels:**

Organize failures by severity to balance coverage and execution time:

Severity 1 - Critical failures: Complete unavailability, hard errors, impossible tasks. Every agent should handle these gracefully.

Severity 2 - Common failures: Timeouts, rate limits, missing data, common user changes. Production agents encounter these frequently.

Severity 3 - Edge failures: Rare but possible scenarios. Malformed data, cascading failures, complex contradictions.

Run Severity 1 tests on every build. Run Severity 2 weekly. Run Severity 3 monthly or before major releases.

**Failure realism:**

Early chaos testing often uses unrealistic failure patterns. APIs that are always down, data that's completely corrupt, users who constantly contradict themselves.

Mature chaos testing uses realistic failure distributions:

Transient timeouts: 60% of timeout failures resolve on retry.

Intermittent availability: Services are down for 5-15 minutes, not permanently.

Partial data: 20% of fields missing, not entire records.

User changes: One or two changes per session, not constant reversal.

Realistic distributions reveal different agent behaviors than extreme scenarios.

**Chaos test maintenance:**

Failure scenarios evolve with your system:

New tools introduce new failure modes. Add failure variants for new tools when they're integrated.

Production incidents reveal real failure patterns. When something breaks in production, add that scenario to your chaos suite.

Tool interfaces change. Failure modes that were common become rare, and vice versa. Review and update failure scenarios quarterly.

User behavior shifts. Track which user failure patterns actually occur and weight your chaos tests accordingly.

A chaos eval suite is not a one-time creation—it's a living reflection of how your system actually fails.

---

## Chaos Testing Frameworks for AI Agents

In 2026, specialized frameworks have emerged for agent chaos testing, moving beyond manual failure injection to automated, systematic robustness evaluation.

**Framework capabilities:**

Declarative failure injection: Define failure scenarios in configuration rather than code.

Example YAML:

```yaml
chaos_scenario:
  name: "search_api_timeout"
  base_task: "find_customer_by_email"
  inject_at: "tool_call:search_database"
  failure_type: "timeout"
  failure_rate: 0.3
  recovery_expectation: "retry_with_backoff"
  max_retries: 3
```

The framework runs the base task repeatedly, injecting the specified failure at the specified point with the specified probability, and measures whether the agent's recovery behavior matches expectations.

**Stochastic failure patterns:** Rather than deterministic injection, introduce probabilistic failures that mimic real-world unpredictability. An API that's 95% reliable, data that's 10% incomplete, users who change their mind 5% of the time.

**Multi-agent chaos testing:** When evaluating agent systems with multiple collaborating agents, chaos frameworks inject failures that require cross-agent coordination to resolve. Agent A's tool fails, forcing Agent B to provide an alternative path.

**Continuous chaos testing:** Frameworks that run chaos tests continuously in staging environments, automatically discovering new failure modes and regression testing robustness improvements.

**Recovery pattern libraries:** Frameworks maintain libraries of expected recovery patterns for different failure types, enabling automated assessment of recovery quality without manual evaluation.

**Temporal failure injection:** Failures that occur not just at specific operation points but at specific times—simulating time-dependent issues like quota resets, scheduled maintenance windows, or time-zone-dependent availability.

These frameworks treat robustness evaluation as a first-class engineering discipline, not an afterthought.

---

## Production Resilience Scoring

Development chaos testing measures potential robustness. **Production resilience scoring** measures actual robustness in the wild.

The pattern: instrument your production agent to track real failures and recovery outcomes, then calculate resilience metrics based on actual operational experience.

**Tracked metrics:**

Failure encounter rate: How often does the agent encounter failures? Broken down by failure category.

Recovery success rate: When failures occur, how often does the agent successfully recover versus escalating or failing?

Time to recovery: How long does recovery take? Quick retries versus extended troubleshooting.

Degradation frequency: How often does the agent complete partial versus full tasks?

Loop incidents: Frequency of loop detection triggers in production.

**Production resilience score calculation:**

Similar to robustness score but weighted by actual failure distribution. If 80% of production failures are timeouts and 5% are permission errors, timeout recovery performance should dominate the score.

Production Resilience = weighted average of recovery success rates across failure types, weighted by frequency of each failure type in production.

**Drift detection:**

Development robustness scores often don't match production resilience scores. Reasons:

Test-production mismatch: Chaos tests don't include failure modes that occur in production.

Failure distribution shift: Actual failures aren't distributed as expected.

Context differences: Production tasks are more complex, with longer histories and more intricate state.

Tracking drift between development robustness and production resilience reveals gaps in your chaos testing strategy.

**Resilience SLIs:**

Treat resilience as a Service Level Indicator:

Target: 90% of failures should be recovered without human intervention.

Measurement: Weekly resilience score above 90%.

Alerting: Alert when resilience score drops below 85% for sustained periods.

This is **SRE practices applied to agent reliability**—treating agent robustness with the same rigor as system reliability.

---

## Enterprise Expectations for Agent Robustness

For enterprise deployment, robustness isn't optional—it's a requirement.

**Enterprise robustness standards:**

Documented failure modes: Complete catalog of known failure scenarios and agent behavior for each.

Recovery SLAs: Defined maximum recovery time for different failure categories. Example: transient tool failures should recover within 30 seconds.

Escalation paths: Clear criteria for when agent escalates to human, and defined escalation procedures.

State audit trails: Complete logging of agent state changes through failure and recovery sequences for debugging and compliance.

Rollback capabilities: Verified ability to undo actions when workflows fail, with rollback verification testing.

Incident playbooks: When agents encounter failures they can't recover from, operational teams need playbooks for intervention.

**Governance requirements:**

Robustness testing before deployment: Minimum robustness score required for production deployment.

Regression testing: Continuous verification that updates don't degrade robustness.

Production monitoring: Real-time visibility into failure rates and recovery performance.

Quarterly resilience reviews: Regular assessment of whether chaos test suites still reflect production reality.

**Risk management:**

Different agent applications have different robustness risk profiles:

Low stakes: A documentation assistant that fails occasionally is annoying. Minimum robustness: 60%.

Medium stakes: A customer service agent that fails frequently damages brand reputation. Minimum robustness: 75%.

High stakes: A financial transaction agent that fails can cause monetary loss or compliance issues. Minimum robustness: 90%.

Critical stakes: A medical or safety-critical agent where failures could cause harm. Minimum robustness: 95%+, with extensive testing of failure modes.

Enterprise deployments require matching robustness standards to application risk profiles and enforcing those standards through testing and monitoring.

---

## Evaluation Template: Agent Chaos Testing Protocol

**Objective:** Systematically evaluate agent robustness through structured failure injection across multiple categories and severity levels.

**Phase 1: Baseline Task Definition**
- Catalog: List all tasks the agent should complete successfully
- Success criteria: Define what constitutes successful completion for each task
- Coverage: Ensure tasks span all major agent capabilities and common user scenarios

**Phase 2: Failure Scenario Design**
- Tool failures: API timeouts, errors, unavailability, malformed responses
- Data failures: Missing fields, type mismatches, empty results, inconsistencies
- User failures: Mid-task changes, contradictions, vague instructions, session abandonment
- Environment failures: Rate limits, quota exhaustion, permission denials, dependency outages

**Phase 3: Chaos Test Execution**
- Injection: For each baseline task, inject failures from each category
- Measurement: Record recovery outcome, time to recovery, degradation quality, state consistency
- Severity: Categorize by Severity 1 (critical), 2 (common), 3 (edge)

**Phase 4: Recovery Assessment**
- Strategy appropriateness: Did the agent choose the right recovery approach?
- Execution quality: Was recovery executed correctly?
- Communication clarity: Did the agent explain failures and recovery to users?
- State integrity: Was internal state consistent after recovery?

**Phase 5: Robustness Scoring**
- Component scores: Calculate error recovery, loop avoidance, degradation quality, state consistency, rollback accuracy, escalation appropriateness
- Aggregate score: Weighted average across components
- Segmentation: Break down scores by failure category to identify weaknesses

**Phase 6: Iteration and Hardening**
- Gap analysis: Identify failure scenarios with low recovery rates
- Prompt engineering: Adjust prompts to improve recovery strategies
- Tool enhancement: Add tool-level error handling where needed
- Retest: Verify improvements with focused chaos testing on previously weak areas

**Success Metrics:**
- Robustness score above 85 for production deployment
- Zero infinite loop incidents in testing
- State consistency above 95% across all scenarios
- Escalation precision above 70%, recall above 90%

---

## Looking Forward: Multi-Turn and Stateful Evaluation

Chaos testing evaluates how agents handle failures within individual tasks. But production agents rarely operate in isolation. They engage in multi-turn conversations, maintain state across sessions, and build context over time.

This introduces new evaluation challenges:

**State propagation:** When a failure occurs in turn three of a ten-turn conversation, does it corrupt the context for turns four through ten? Or does the agent isolate the failure and continue cleanly?

**Recovery across sessions:** If an agent fails to complete a task today and the user returns tomorrow asking "did you finish that thing?", does the agent correctly recall the failure and resume appropriately?

**Conversational coherence:** When an agent encounters a failure during a complex dialog, does it maintain conversational flow or does the recovery attempt break the interaction?

**Long-term state consistency:** Over dozens or hundreds of interactions, does the agent maintain an accurate model of completed and failed tasks? Or do small state inconsistencies accumulate into major confusion?

**Chapter 8.6—Multi-Turn and Stateful Evaluation** extends robustness testing into the temporal dimension, examining not just whether agents recover from individual failures, but whether they maintain coherence across extended interactions and persistent state.

The chaos test that really matters isn't whether your agent can handle a timeout. It's whether it can handle a timeout in conversation turn seven, recover gracefully, and continue the conversation without losing the plot.

---

## Interview Questions: Error Recovery and Robustness

**Q: We ran chaos testing and found our agent gets stuck in retry loops about 15% of the time. What's the fastest way to fix this?**

The retry loop is almost always a prompt engineering problem, not an architecture problem. The agent doesn't know when to give up because it hasn't been explicitly told.

Fastest fix: Add clear termination criteria to your prompt. "If an operation fails three times consecutively, try a different approach. If three different approaches all fail, escalate to human." Make retry limits explicit and behavioral.

But also diagnose why loops happen in that 15%. Is it specific failure types? Specific tasks? Often you'll find loops cluster around particular scenarios—say, permission errors where the agent doesn't understand that retrying won't fix a permission issue. Add failure-type-specific guidance.

Longer-term fix: Add loop detection to your agent framework. Track operation sequences and intervene when patterns repeat. But prompt engineering should handle this—framework intervention is a safety net, not the primary solution.

**Q: Our agent has 85% error recovery rate overall, but only 40% for user-initiated failures like changing their mind mid-task. Why such a gap?**

User failures are fundamentally different from tool failures. When an API times out, the task is clear—the API request. When a user changes their mind, the task itself is shifting.

Tool failures are problems to solve. User failures are requirements changes. Your agent is probably treating them the same way—as errors to recover from rather than as new information to integrate.

Look at your prompt's instruction handling. Does it have explicit patterns for instruction updates? "When the user provides new information that contradicts previous instructions, acknowledge the change, confirm the new intent, and adjust the plan accordingly."

Also check whether your agent is holding too tightly to its original plan. Good agents treat user input as authoritative. If there's a conflict between the original task and new user input, the user wins. Make this explicit in your prompt.

The gap suggests your agent is good at engineering recovery but weak at adaptive planning.

**Q: We test graceful degradation by making parts of our agent's data unavailable, but in production it still fails completely rather than degrading. What are we missing?**

Your chaos tests probably inject clean, obvious failures—entire APIs unavailable, complete data sources down. Production degradation is messier. Data is there but incomplete. APIs return success but with degraded quality. The agent doesn't recognize these as degradable failures.

Expand your chaos testing to include partial failures, not just total ones. API returns successfully but with only three of five expected fields. Database query succeeds but returns stale data. Tool completes but takes five times longer than normal.

Also check your degradation criteria. In tests, you probably have clear "this data is available, this isn't" scenarios. In production, availability is probabilistic and quality is a spectrum. Your agent needs to assess data quality and make judgment calls about whether it's sufficient.

Add explicit quality thresholds to your prompt. "If data is incomplete but covers at least 70% of requirements, proceed with available data and note limitations. If data is less than 70% complete, escalate for human review."

**Q: How do you balance robustness with latency? Our chaos testing rewards retry attempts, but each retry adds seconds to response time.**

This is the robustness-performance tradeoff, and it's context-dependent. For a research agent, spending 30 seconds on retries to successfully complete a task is fine. For a customer service agent, 30 seconds of retries feels like the agent is broken.

First, optimize retry efficiency. Don't retry immediately—that's likely to hit the same failure. Use exponential backoff: retry after 1 second, then 2, then 4. But also don't wait unnecessarily—if the failure is permanent (like a 404 or permission error), don't retry at all.

Second, parallelize where possible. If you're fetching data from three sources and one fails, don't block the others. Continue with what succeeds and retry the failure in parallel.

Third, set time budgets. "Attempt recovery for up to 10 seconds, then degrade or escalate." This prevents unlimited retry loops while still allowing reasonable recovery attempts.

Fourth, communicate during recovery. If retry will take time, tell the user: "The booking system is responding slowly, retrying..." This transforms wait time from confusion into understanding.

The goal isn't maximum robustness regardless of cost—it's optimal robustness within acceptable latency bounds.

**Q: Should we chaos test in production, or is that too risky?**

Chaos testing in production is the gold standard for resilience, but requires maturity and safeguards.

Start with shadow chaos testing: run your production agent twice for a sample of requests, once normally and once with failures injected. Compare outcomes but only return the normal result to users. This reveals how production traffic would respond to failures without impacting users.

Next, limited production chaos: inject failures for a small percentage of production traffic (1-5%) with careful monitoring. Choose low-stakes interactions first—read-only operations, internal tools, or users who opted into beta programs.

Full production chaos: once you're confident in your agent's recovery capabilities, systematic failure injection becomes part of normal operations. But maintain circuit breakers—if recovery rate drops below threshold, disable chaos injection automatically.

Never chaos test critical paths without safeguards. Don't inject failures in payment processing, security operations, or any scenario where failure could cause monetary loss or data corruption.

The question isn't "should we chaos test in production?" It's "how do we chaos test in production safely?" Start small, monitor carefully, build confidence gradually.

