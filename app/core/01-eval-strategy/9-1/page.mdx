# 9.1 — Why RAG Evaluation Is a Separate Discipline

A financial services company launched their AI assistant in October 2025. It answered questions about mortgage rates, loan terms, and refinancing options. The model was GPT-4, the prompts were carefully tuned, and their standard LLM evaluation showed 94% accuracy on a test set of 500 questions.

Three weeks into production, a customer complained. The assistant told them their fixed-rate mortgage could convert to an adjustable rate at any time — completely false. The compliance team investigated. The model had retrieved an internal document about construction loans (a different product) and confidently applied those rules to residential mortgages. The answer sounded perfect. The retrieval was wrong.

They ran their evaluation again. Same 94% score. Their eval only tested whether the model could generate good answers when given the right context. It never checked whether the system found the right context in the first place. They had been testing half their system.

Here's what I see teams get wrong: they treat RAG like it's just an LLM with some documents attached. It's not. RAG is two separate systems — a retrieval engine and a language model — that have to work together perfectly. When you only evaluate the language model, you're flying blind on the half of the system that determines whether your answers are grounded in reality or fiction.

---

## The Split-Brain Problem

RAG stands for **Retrieval-Augmented Generation**. The name tells you everything. There are two brains:

**Brain one: Retrieval.** Your system searches through documents, databases, knowledge bases, or APIs to find relevant information. This is vector search, keyword matching, semantic similarity, reranking — whatever method you use to find the right content.

**Brain two: Generation.** Your language model reads the retrieved content and generates an answer. This is prompt engineering, model selection, temperature tuning — all the LLM work you already know.

Here's the problem: these two brains can fail independently.

Your retrieval can fail while your generation succeeds. The model gets the wrong documents but writes a beautiful, coherent, completely wrong answer.

Your generation can fail while your retrieval succeeds. The system finds the perfect documents but the model ignores them and hallucinates anyway.

Both can fail at once. Wrong documents, wrong answer.

Both can succeed individually but fail together. The right documents are retrieved, the model generates a factually correct answer, but the answer comes from the model's training data instead of the retrieved documents — so you can't prove where it came from.

Standard LLM evaluation only tests brain two. It gives the model some context and checks whether the answer is good. It never asks whether the context was the right context to retrieve in the first place.

This is why you need RAG evaluation as a separate discipline. You're not just testing a model. You're testing a system.

---

## Why Standard LLM Eval Misses RAG Failures

Let's say you're evaluating a customer support assistant. Your standard LLM eval looks like this:

**Input:** A question and some context documents.

**Output:** The model's answer.

**Evaluation:** Does the answer correctly use the context?

This works fine if you hand-pick the context. But in production, the system picks the context. And that's where everything breaks.

Here's a real example from a legal tech company in 2026. They built a contract analysis assistant. Their LLM evaluation showed 91% accuracy when given the right contract clauses. But when they tested the full RAG pipeline — user question, retrieval, generation — accuracy dropped to 68%. The retrieval was failing 23% of the time.

The model was great. The retrieval was not. Their evaluation never caught it because they never tested retrieval.

Here's another failure mode: **retrieval is correct but the model doesn't use it.**

A healthcare assistant retrieves the correct drug interaction warnings. The model generates an answer that sounds confident and professional. The answer is wrong. The model hallucinated instead of reading the retrieved documents. Your standard LLM eval would mark this as a generation failure, but the root cause is a **faithfulness failure** — the model isn't faithful to the retrieved context.

You need to test both: did you retrieve the right stuff, and did you use it correctly?

---

## The Three Evaluation Dimensions

RAG evaluation breaks down into three core dimensions:

### 1. Retrieval Quality

Did you find the right documents?

This is **precision** and **recall** for information retrieval. Precision means the documents you retrieved are relevant. Recall means you didn't miss important documents.

Example: A user asks "What's the return policy for electronics?" Your system retrieves five documents. Three are about electronics returns (relevant). Two are about clothing returns (not relevant). You missed one document about restocking fees for electronics (incomplete).

Your retrieval has 60% precision (3 out of 5 are relevant) and missed key information (incomplete recall).

### 2. Generation Quality

Did you use the retrieved documents correctly?

This is **faithfulness** and **answer relevance**. Faithfulness means the answer is grounded in the retrieved documents — no hallucinations. Answer relevance means the answer actually addresses the user's question.

Example: The system retrieves the correct return policy. The model generates an answer that says "Electronics can be returned within 30 days" but the document says "within 14 days." That's a faithfulness failure — the model didn't stick to the source material.

Or: The system retrieves the correct policy, the model quotes it accurately, but the answer is three paragraphs about shipping costs when the user asked about returns. That's an answer relevance failure — correct information, wrong question.

### 3. Attribution Quality

Did you cite your sources accurately?

This is **citation accuracy** and **source traceability**. Citation accuracy means the citations point to content that actually supports the answer. Source traceability means you can trace every claim back to a specific document and location.

Example: The model says "Electronics must be in original packaging (Source: Return Policy v2.3, Section 4)." You check Section 4. It's about refund timelines, not packaging. That's a citation failure. The answer might be correct, but the attribution is wrong — and in regulated industries, that's a compliance violation.

All three dimensions matter. You can't evaluate RAG by only testing one.

---

## The RAG Pipeline: Where Each Stage Can Fail

Let's walk through the full RAG pipeline and see where failures happen:

**Stage 1: Query Processing**

The user asks a question. Your system might rewrite it, expand it, or break it into sub-queries. If this fails, everything downstream is doomed.

Example: User asks "What are the fees for early loan payoff?" Your query processor rewrites it as "loan application fees." Now retrieval is searching for the wrong thing.

**Stage 2: Retrieval**

Your system searches the knowledge base. This is vector search, keyword search, hybrid search, whatever method you use. If this fails, you get wrong documents.

Example: Vector search finds semantically similar documents about "loan penalties" but misses the specific document titled "Prepayment Fees" because the embedding didn't capture that synonym relationship.

**Stage 3: Context Assembly**

Your system takes the top retrieved documents and assembles them into a prompt. This includes reranking, truncation, and formatting. If this fails, the model gets the right documents in the wrong order or truncated at the wrong spot.

Example: You retrieve five documents. The most important one is ranked fifth. Your prompt only includes the top three. The model never sees the key information.

**Stage 4: Generation**

The model reads the context and generates an answer. This is standard LLM behavior. If this fails, you get hallucinations or irrelevant answers even with perfect context.

Example: The model has the correct prepayment fee information but generates an answer about refinancing options instead.

**Stage 5: Response Formatting**

Your system formats the answer and adds citations. If this fails, the answer is correct but the user experience is broken.

Example: The model generates the right answer with inline citations, but your formatter strips them out or links them to the wrong documents.

Each stage can fail independently. Your evaluation needs to catch failures at every stage.

---

## RAG in 2026: Context Engines and Agentic Retrieval

In 2026, RAG is evolving beyond simple "retrieve then generate" into what teams call **Context Engines**.

**Hybrid Retrieval:** Combine dense vector search (semantic similarity), sparse search (keyword matching), and reranking models. This improves precision but adds complexity — now you have three retrieval systems that can each fail differently.

**Multi-Source Retrieval:** Pull from databases, APIs, documents, and real-time data streams. Your evaluation needs to test whether the system is choosing the right sources for each query.

**Agentic RAG:** The agent decides what to retrieve, when to retrieve, and whether to retrieve again based on the initial results. This is common in 2026 for complex queries that need multiple rounds of information gathering. Now your evaluation has to test the agent's decision-making, not just the retrieval quality.

Example: A user asks "Compare the pricing of our Premium and Enterprise plans for teams larger than 50 people, and tell me which includes SSO." An agentic RAG system might:

1. Retrieve pricing documents.
2. Realize it needs SSO feature information.
3. Retrieve feature comparison docs.
4. Realize the pricing doc is from 2024 and search for updated pricing.
5. Synthesize an answer.

Your evaluation needs to test: Did the agent retrieve all necessary sources? Did it recognize when information was missing? Did it choose the most up-to-date sources? Standard RAG eval doesn't cover this. You need agent evaluation patterns (see Chapter 8) on top of RAG evaluation.

---

## The TRACe Framework

Teams in 2026 use a structured framework called **TRACe** to organize their RAG evaluation:

**T — Trust:** Can users trust the answer? This covers faithfulness (no hallucinations), citation accuracy (sources are correct), and source quality (you're pulling from authoritative documents).

**R — Relevance:** Is the answer relevant to the user's question? This covers answer relevance (addresses the query) and retrieval relevance (the documents you pulled are related to the query).

**A — Accuracy:** Is the answer factually correct? This covers factual correctness (claims match reality), context correctness (claims match retrieved documents), and up-to-date information (not using stale data).

**C — Completeness:** Does the answer cover everything the user needs? This covers recall (didn't miss important documents), comprehensiveness (answer addresses all parts of a multi-part question), and edge case handling (what happens with ambiguous or out-of-scope queries).

Each letter maps to specific metrics. Trust maps to faithfulness scores and citation precision. Relevance maps to answer relevance scores and retrieval precision. Accuracy maps to factual consistency and LLM-judge accuracy scores. Completeness maps to recall and coverage metrics.

The framework gives you a checklist: "Did we test trust? Did we test relevance?" If you're missing a dimension, your evaluation is incomplete.

---

## Common RAG Failure Modes

Here are the failure modes I see most often in production RAG systems:

**1. Wrong Documents Retrieved**

The retrieval pulls irrelevant or partially relevant documents. The model generates an answer based on wrong information. This looks like a hallucination but it's a retrieval failure.

Example: User asks about "Python dictionaries" (the data structure). System retrieves documents about Python (the snake). Model generates an answer about reptiles.

**2. Right Documents, Wrong Answer Extracted**

Retrieval is perfect. The model misreads or misinterprets the documents. This is a generation failure, but you need RAG eval to distinguish it from a retrieval failure.

Example: The document says "Refunds are processed within 5-7 business days after we receive the item." The model says "Refunds take 5-7 days" (missing the "after we receive" part). The answer is technically correct but misleading.

**3. Hallucination Despite Correct Context**

Retrieval is perfect. The model ignores the documents and hallucinates. This is a faithfulness failure.

Example: The system retrieves the correct pricing document that says "Enterprise plan is $99/month." The model says "$149/month" because that was in the training data from an older version of the pricing page.

**4. Citation to Wrong Source**

The answer is correct. The citation is wrong. This is an attribution failure. In regulated industries, this can be a compliance violation even if the answer is factually correct.

Example: The model says "Our return policy allows 30 days for electronics (Source: Terms of Service, Section 12)." Section 12 is about privacy, not returns. The return policy is in Section 8. The answer is right, the citation is wrong.

**5. Outdated Information Retrieved**

The retrieval pulls an old version of a document. The model generates an answer that was correct in 2023 but is wrong in 2026.

Example: User asks about visa requirements for Japan. System retrieves a 2019 document. Japan changed their visa policy in 2024. The answer is confidently wrong because the source is stale.

You need separate metrics to catch each failure mode. A single accuracy score won't tell you which of these five things went wrong.

---

## Why RAG Eval Is Harder Than It Looks

Here's the trap: retrieval and generation have **interaction effects**. The whole system is more than the sum of its parts.

You can have perfect retrieval in isolation. You can have perfect generation in isolation. But when you put them together, new failure modes emerge:

**Over-reliance on retrieved context:** The model trusts the documents too much. If you accidentally retrieve a document with an error, the model will repeat that error even if it knows better from training.

**Under-reliance on retrieved context:** The model trusts its training data more than the documents. It ignores retrieved information that contradicts what it learned during pretraining.

**Context length failures:** The model performs well with two retrieved documents. Add a third and performance drops because the key information gets lost in the middle (the "lost in the middle" problem identified in 2023 and still relevant in 2026).

**Query-document mismatch:** The retrieval pulls documents that are semantically similar but pragmatically wrong. Example: User asks "How do I reset my password?" System retrieves "How to create a secure password" (similar topic, wrong task).

These failures don't show up when you test retrieval alone or generation alone. You have to test the full system under realistic conditions.

---

## Enterprise RAG Evaluation Requirements

If you're building RAG for healthcare, finance, legal, or government, your evaluation requirements are stricter:

**Provable grounding:** Every claim must trace back to a specific document and location. You can't just say "the model is accurate." You need to show exactly where each fact came from.

**Citation accuracy:** Citations must be precise and verifiable. If you cite Section 4, a compliance officer needs to open Section 4 and see the exact text that supports your claim.

**Source traceability:** You need a full audit trail from user query to retrieved documents to generated answer. If a user challenges an answer, you need to reconstruct exactly what the system retrieved and why.

**Version control:** Your knowledge base changes over time. You need to track which version of which document was used to generate each answer. This matters for regulatory audits.

**Bias and fairness:** If your retrieval preferentially surfaces documents that favor certain demographics or outcomes, that's a fairness issue even if the generation is unbiased.

These requirements make RAG evaluation 10x harder than standard LLM eval. You're not just checking accuracy. You're building a provenance system.

---

## The 2026 RAG Evaluation Landscape

Here are the tools and frameworks teams use in 2026 for RAG evaluation:

**RAGAS (Retrieval-Augmented Generation Assessment):** Open-source framework with metrics for faithfulness, answer relevance, context precision, context recall, and more. Widely used as a starting point.

**TruLens:** Evaluation and observability for RAG systems. Provides feedback functions (like LLM judges) that score groundedness, relevance, and toxicity. Integrates with LangChain and LlamaIndex.

**DeepEval:** Testing framework for LLMs with specific RAG metrics including faithfulness, contextual relevance, and hallucination detection. Includes pre-built test cases.

**LangSmith:** LangChain's tracing and evaluation tool. Tracks every step of the RAG pipeline (query rewriting, retrieval, generation) and lets you evaluate each stage independently.

**Arize AI:** RAG monitoring for production. Tracks retrieval quality, generation quality, and user feedback in real time. Detects drift in retrieval relevance or generation accuracy.

Most teams in 2026 use a combination: RAGAS for offline evaluation during development, TruLens or DeepEval for integration testing, and Arize or LangSmith for production monitoring.

No single tool covers everything. You need retrieval metrics (precision, recall, MRR, NDCG), generation metrics (faithfulness, relevance), attribution metrics (citation accuracy), and system metrics (latency, cost, user satisfaction).

---

## Failure Modes and Enterprise Expectations

Let's talk about what happens when RAG evaluation fails in the real world.

A telecommunications company deployed a billing assistant in early 2026. Their LLM evaluation showed 89% accuracy. They didn't evaluate retrieval. In production, the assistant retrieved outdated billing documents 14% of the time. Customers received incorrect information about their bills. The company faced regulatory scrutiny because they couldn't prove where the AI's information came from.

They rebuilt their evaluation system with three layers:

**Layer 1: Retrieval Testing.** Measure precision at K (are the top K retrieved documents relevant?), recall (did we miss critical documents?), and freshness (are we retrieving the most up-to-date version?).

**Layer 2: Generation Testing.** Measure faithfulness (does the answer stick to retrieved documents?), answer relevance (does it address the query?), and completeness (does it cover all necessary information?).

**Layer 3: Attribution Testing.** Measure citation accuracy (do citations point to correct sources?), citation coverage (does every claim have a citation?), and traceability (can we reconstruct the full retrieval-to-answer pipeline?).

They ran this evaluation on 5,000 customer queries. Retrieval accuracy improved from 86% to 94%. Generation faithfulness improved from 89% to 96%. Citation accuracy went from 72% (pre-testing) to 99%.

The lesson: RAG evaluation isn't optional. It's the difference between an AI system that works and one that fails under regulatory review.

---

## Template: RAG Evaluation Checklist

```yaml
rag_evaluation_checklist:
  retrieval_quality:
    - metric: "Precision at K"
      description: "Percentage of top K documents that are relevant"
      target: "above 90%"
    - metric: "Recall"
      description: "Percentage of relevant documents retrieved"
      target: "above 85%"
    - metric: "MRR (Mean Reciprocal Rank)"
      description: "Rank position of first relevant document"
      target: "above 0.8"

  generation_quality:
    - metric: "Faithfulness"
      description: "Answer grounded in retrieved context"
      target: "above 95%"
    - metric: "Answer Relevance"
      description: "Answer addresses the user query"
      target: "above 90%"
    - metric: "Completeness"
      description: "Answer covers all aspects of query"
      target: "above 85%"

  attribution_quality:
    - metric: "Citation Accuracy"
      description: "Citations point to correct sources"
      target: "above 98%"
    - metric: "Citation Coverage"
      description: "All claims have citations"
      target: "100% for factual claims"
    - metric: "Source Traceability"
      description: "Full audit trail available"
      target: "100%"

  system_metrics:
    - metric: "End-to-End Latency"
      description: "Query to response time"
      target: "under 3 seconds"
    - metric: "Retrieval Latency"
      description: "Time to retrieve documents"
      target: "under 500ms"
    - metric: "Cost per Query"
      description: "Retrieval plus generation cost"
      target: "under $0.02"
```

---

## Interview Q&A: RAG Evaluation

**Q: We're using GPT-4 for RAG. Isn't the model good enough that we don't need to test retrieval separately?**

No. The model quality doesn't matter if the retrieval is broken. GPT-4 can't generate accurate answers from incorrect documents. Think of it like having a brilliant lawyer with the wrong case files — the brilliance doesn't help. You need to test retrieval independently because retrieval failures create a ceiling on your system's accuracy. If retrieval is 80% accurate, your system can never be better than 80% accurate no matter how good your model is. I've seen teams with GPT-4 launch systems at 70% accuracy because they never tested retrieval. Test the full pipeline, not just the model.

**Q: How is RAG eval different from just using LLM judges to score answers?**

LLM judges are part of RAG eval, but they're not the whole thing. Judges can score whether the answer is faithful to the context (generation quality), but they can't tell you whether the context was the right context to retrieve (retrieval quality). You need retrieval metrics like precision and recall, which measure whether you pulled the right documents. You also need attribution metrics to check if citations are accurate. LLM judges help with generation evaluation. They don't replace retrieval evaluation. Use judges for faithfulness and relevance scoring (see Chapter 7.2), but add retrieval metrics on top.

**Q: Can we just use accuracy as a single metric for RAG?**

No. Accuracy hides too much. If your accuracy is 80%, you don't know if that's because retrieval failed 20% of the time, generation failed 20% of the time, or both failed 10% of the time each. You need to decompose the metric. Measure retrieval precision and recall separately. Measure generation faithfulness and answer relevance separately. Measure citation accuracy separately. When something breaks in production, you need to know which component failed so you can fix it. A single accuracy score doesn't tell you where to look. Break it down by dimension: retrieval, generation, attribution.

**Q: We're using agentic RAG where the agent decides what to retrieve. How do we evaluate that?**

Agentic RAG needs agent evaluation on top of RAG evaluation. First, evaluate the agent's decisions: Did it retrieve the right sources? Did it recognize when it needed more information? Did it stop retrieving at the right time? Second, evaluate the RAG pipeline for each retrieval step: Was each individual retrieval accurate and relevant? Finally, evaluate the end-to-end output: Is the final answer correct and complete? You're testing three layers: agent decision-making (Chapter 8), retrieval quality per step, and overall generation quality. Agentic RAG is harder to evaluate because failures can come from the agent's planning, the retrieval execution, or the final generation.

**Q: How often should we run RAG evaluation in production?**

Continuously, but with different frequencies for different checks. Run lightweight metrics (faithfulness, answer relevance) on every query or a sample of queries. Run expensive metrics (citation accuracy, full retrieval recall) on a smaller sample daily or weekly. Run full human evaluation monthly or when you update the knowledge base. Also run regression tests whenever you change the retrieval logic, reranking model, or prompt. Production monitoring should track retrieval latency and precision in real time using something like Arize or LangSmith. If retrieval quality drops below your threshold, you need to investigate immediately — it usually means your knowledge base has drifted or your embeddings are stale.

---

In the next section (9.2 — Retrieval Quality Metrics), we'll break down exactly how to measure retrieval performance: precision, recall, MRR, NDCG, and how to calculate them without needing a massive labeled dataset.
