# 9.9 RAG Evaluation in Production

A customer support RAG system at a SaaS company passed all offline evaluations with 92% faithfulness and 88% answer relevance. The team celebrated and deployed to production. Within two weeks, support tickets about "wrong answers" tripled. Users complained the system kept citing outdated pricing from the knowledge base. One query about a new feature returned "I don't have information about that" even though the feature had launched three days earlier. The offline eval dataset, carefully curated over months, had never seen queries about recent changes or edge cases that real users naturally asked.

Production RAG evaluation is not just running your offline evals on live traffic. It's fundamentally different because you're dealing with the real world: queries you never anticipated, knowledge bases that change daily, user expectations that evolve, and failure modes that only emerge at scale. Your carefully curated test set can't predict what users will ask tomorrow or when your documentation goes stale. Production eval is about continuous monitoring, real-time detection of quality degradation, and building feedback loops that make your system better every day.

This chapter covers how to monitor and evaluate RAG quality in live production systems. We'll explore real-time faithfulness monitoring, retrieval quality tracking, user feedback signals, query analysis, data freshness detection, A/B testing RAG configurations, drift detection, and alert design. By the end, you'll know how to catch hallucinations in production, detect retrieval degradation before users notice, and build a production eval system that keeps your RAG quality high.

---

## Why Production RAG Eval Is Different

Offline evaluation uses curated datasets. You write questions, label correct answers, validate retrieval results, and measure performance on a fixed test set. This tells you how your system performs on known queries with known answers.

Production evaluation faces reality. Users ask questions you never thought of. They use different phrasing, make typos, ask about edge cases, and expect answers about events from yesterday. Your knowledge base grows and changes—new documents are added, old ones are updated, some information becomes outdated. User needs evolve as your product changes.

The gap between offline and production shows up in several ways. **Query distribution shift** means the types of queries users actually ask differ from your test set. Your offline eval might focus on factual product questions, but users ask troubleshooting questions, comparison questions, or vague exploratory questions. **Knowledge base drift** means your retrieval index changes as documents are added or updated, but your eval set stays static. A query that retrieved perfectly last month might fail today because the relevant document was reorganized. **Temporal relevance** means some queries require recent information. A question about "current pricing" or "latest features" can't be answered correctly from offline eval data that's weeks old.

Production eval also surfaces failure modes that offline eval misses. **No-result queries** where retrieval returns nothing relevant are rare in curated test sets but common in production. **Low-confidence responses** where the system hedges or gives vague answers indicate retrieval didn't find strong evidence. **User frustration signals** like follow-up questions rephrasing the same query or escalations to human support reveal quality problems that metrics don't capture.

The goal of production RAG eval is not to achieve a perfect score on a fixed test set. It's to maintain quality as conditions change, detect degradation quickly, understand what users actually need, and continuously improve the system based on real usage.

---

## Real-Time Faithfulness Monitoring

Faithfulness is whether the generated answer is grounded in the retrieved context. In production, you can't manually check every response, but you can sample and run automated faithfulness checks to catch hallucinations.

**Sampling strategy** determines which responses to evaluate. Evaluate a random sample of all responses to get a baseline quality measure—typically 1-5% of traffic depending on volume. Sample more heavily from high-impact areas like first-time users, paying customers, or queries in critical product areas. Flag and evaluate all low-confidence responses where the model expressed uncertainty. Flag unusual queries that fall outside your normal distribution—these are more likely to hallucinate.

**Automated faithfulness scoring** uses LLM-as-judge to evaluate if the answer is supported by the retrieved context. Send the query, retrieved chunks, and generated answer to a judge model with a prompt like "Does the answer only include information found in the context? Answer yes or no and explain." Run this asynchronously after serving the response—don't add latency to the user experience. Store faithfulness scores in your monitoring system and track trends over time.

**Hallucination patterns** emerge from production data. Some queries consistently hallucinate because the knowledge base has incomplete information—the model fills gaps with plausible-sounding but wrong details. Some topics trigger hallucination more than others, like numerical details, dates, or proper names. Some users phrase questions in ways that lead to hallucination, revealing prompt weaknesses.

**Alerting on faithfulness degradation** requires setting thresholds. Alert if faithfulness score drops below 85% in the last hour on a sample of at least 20 responses. Alert if any response is flagged as extremely unfaithful by the judge model with high confidence. Alert if faithfulness for a specific product area or document type degrades significantly. These alerts let you catch hallucination spikes before they affect many users.

**Production faithfulness challenges** include evaluator agreement—LLM-as-judge may disagree with human judgment in edge cases. Latency and cost—running judge models on every response is expensive at scale, so sampling is essential. False positives—some flagged responses are actually fine, so alerts need investigation. Despite these challenges, real-time faithfulness monitoring is critical for catching hallucination in production before it damages user trust.

---

## Retrieval Quality Monitoring

Retrieval is the foundation of RAG quality. If retrieval fails, no amount of clever prompting will save the answer. Monitoring retrieval quality in production means tracking whether the right documents are being found.

**Retrieval metrics in production** are harder than offline because you often don't have ground truth for what should be retrieved. You can track **precision at K** by sampling retrieved chunks and having evaluators judge relevance. You can track **recall** by identifying queries that should have retrieved certain documents (based on user feedback or manual review) and checking if those documents were in the top K. You can track **relevance scores** from your retrieval system—if confidence scores are calibrated, low scores indicate poor retrieval.

**No-result detection** catches queries where retrieval returns nothing relevant. If your retrieval system returns relevance scores, set a threshold—queries where the top result scores below 0.3 are likely no-results. Track the percentage of no-result queries over time. An increase indicates knowledge base gaps, query distribution shift, or retrieval model degradation. Log these queries and analyze them to find missing content.

**Retrieval degradation signals** show up in several ways. The average relevance score of top retrieved chunks drops over time. The percentage of queries requiring more than 5 chunks to find relevant information increases. The distribution of retrieved documents changes—certain documents stop being retrieved, or retrieval becomes more diffuse. User follow-up questions increase, suggesting the first answer wasn't helpful.

**Per-document retrieval analytics** help identify problems. Track how often each document is retrieved. If a newly added document is never retrieved, it might have poor metadata or use terminology that doesn't match user queries. If a previously popular document stops being retrieved after an update, the update might have broken its searchability. Track retrieval rank—if important documents slip from position 1 to position 5, retrieval quality is degrading even if they're still in the top K.

**Retrieval latency monitoring** is also critical. RAG systems often have tight latency budgets. If retrieval slows down—due to index growth, infrastructure issues, or expensive reranking—the user experience degrades. Track p50, p95, and p99 retrieval latency and alert if percentiles cross thresholds.

---

## User Feedback Signals

Users tell you when RAG quality is poor, but often indirectly. Explicit feedback like thumbs up and thumbs down is valuable but rare—most users don't rate responses. Implicit feedback from user behavior is more abundant and often more honest.

**Explicit feedback mechanisms** include thumbs up and thumbs down buttons on responses. Collect these ratings and calculate positive feedback rate. Track how feedback rate changes over time—a drop in positive ratings indicates quality degradation. Allow users to provide free-text feedback explaining what was wrong. Categorize this feedback to identify common failure modes. Prioritize fixing issues that generate the most negative feedback.

**Implicit feedback signals** reveal quality without requiring user action. **Follow-up queries** that rephrase the same question suggest the first answer wasn't helpful. If a user asks "What is feature X?" and then immediately asks "How does feature X work?" the first answer likely didn't fully address their need. Track the percentage of sessions with rephrased queries. **Search refinements** where users add more details or change keywords indicate the initial retrieval didn't match their intent. **Escalations to human support** after interacting with the RAG system are a strong negative signal—the system failed to answer the query.

**Session-level metrics** provide quality signals. Track **session success rate**—did the user accomplish their goal without escalating or leaving frustrated? Track **time to resolution**—how long did it take the user to get a satisfactory answer? Track **abandon rate**—what percentage of users leave after one unhelpful response? These metrics correlate with RAG quality but also depend on UI, task complexity, and user patience.

**Correlating feedback with system behavior** helps diagnose problems. If queries with low retrieval scores get more negative feedback, your relevance threshold might be too low. If queries about a specific topic get more thumbs down, you might have a knowledge gap in that area. If certain user cohorts give more negative feedback, their needs might differ from your primary use case.

**Challenges with user feedback** include sparsity—most users don't provide explicit feedback. Bias—users who had extreme experiences (very good or very bad) are more likely to rate. Noise—some negative feedback is about product decisions, not RAG quality. Despite these challenges, user feedback is the ultimate ground truth for production quality.

---

## Query Analysis

Understanding what users actually ask in production is essential for improving RAG systems. Query analysis reveals knowledge gaps, surfaces common failure modes, and guides prioritization.

**Query categorization** groups queries by type or topic. Categorize by intent: factual questions, how-to questions, troubleshooting, comparison, exploratory. Categorize by product area: billing, features, integrations, account management. Categorize by complexity: simple lookup, multi-hop reasoning, open-ended. Use clustering algorithms or LLM-based classification to automatically categorize queries at scale.

**Query frequency analysis** shows what users care about most. Track the top 100 most common queries. If a frequent query has low answer quality, fixing it will have high impact. Track query frequency over time—spikes in certain query types might correlate with product launches, marketing campaigns, or outages. Track the long tail—rare queries that each occur once or twice but collectively represent significant traffic. The long tail often reveals edge cases your offline eval missed.

**No-result query analysis** identifies knowledge base gaps. If 5% of queries are no-result and many of them ask about the same topic, you're missing documentation for that topic. Log all no-result queries and manually review a sample weekly. Prioritize adding content to address common no-result queries.

**Low-confidence query analysis** flags queries where the system is unsure. These queries might have ambiguous phrasing, require information not in the knowledge base, or involve complex reasoning the system struggles with. Low-confidence queries are opportunities to improve prompts, add clarifying questions, or expand documentation.

**Temporal query patterns** reveal seasonality or trends. Billing questions might spike at month-end. Troubleshooting queries might spike after a new feature launch. Understanding these patterns helps you anticipate load and prioritize eval coverage for high-traffic periods.

**Query drift detection** uses statistical tests to detect when the query distribution changes. If the distribution of query topics in the last week is significantly different from the last month, something changed—a new feature launched, a competitor released something, or user behavior shifted. Query drift often precedes quality degradation because your system was optimized for the old distribution.

---

## No-Result and Low-Confidence Detection

No-result queries and low-confidence responses are explicit signals that your RAG system is struggling. Detecting and analyzing these failures helps you improve coverage and capability.

**No-result detection methods** depend on your retrieval system. If you use vector similarity, set a threshold—queries where the top result has similarity below 0.3 are likely no-results. If you use keyword search, queries that return no hits are obvious no-results. If you use hybrid retrieval, queries where both vector and keyword retrieval fail are high-confidence no-results. Track the no-result rate over time and alert if it increases significantly.

**Low-confidence detection** flags responses where the system is uncertain. If your generation prompt asks the model to output a confidence score, track queries with confidence below 0.5. If you use retrieval scores, flag queries where the top retrieved chunk has low relevance. If your model outputs hedging language like "I'm not sure" or "I don't have information about that," flag these responses automatically.

**Analyzing no-result queries** reveals what you're missing. Group no-result queries by topic using clustering or LLM classification. If many no-results ask about a feature that's not documented, add documentation. If many no-results use terminology not in your knowledge base, add synonyms to metadata. If many no-results are vague or exploratory, consider adding clarifying questions to guide users.

**Handling no-results gracefully** improves user experience even when you can't answer. Respond with "I couldn't find information about that. Can you rephrase or provide more details?" rather than hallucinating. Offer related documents or topics the user might be interested in. Log the query so you can improve later. Escalate to human support if the query seems urgent.

**Trends in no-result and low-confidence rates** indicate system health. If no-result rate increases suddenly, you might have had a retrieval index failure, a knowledge base update that broke retrieval, or a query distribution shift. If low-confidence rate increases, your generation model might be degrading or the knowledge base might be less comprehensive than before.

---

## Data Freshness Monitoring

RAG systems are only as good as the knowledge base they retrieve from. If your knowledge base is stale, users get outdated answers. Monitoring data freshness is critical for production RAG quality.

**Freshness requirements** depend on your domain. A customer support RAG system needs up-to-date documentation—outdated pricing or feature descriptions are unacceptable. A historical Q&A system might have looser freshness requirements. Define freshness SLAs for different content types: product documentation updated within 24 hours of changes, blog posts indexed within 1 hour of publication, internal knowledge refreshed weekly.

**Automated freshness checks** detect stale information. Track the last update timestamp for each document in your knowledge base. Alert if critical documents haven't been updated in weeks or months. Compare your knowledge base to source systems—if your CMS has newer versions of documents, your retrieval index is stale. Use content hashing to detect when documents change and trigger re-indexing.

**Version control for knowledge base** helps track freshness. Tag documents with version numbers or timestamps. When a document is updated, mark the old version as deprecated and add the new version. If a user query retrieves a deprecated document, flag it for review. Track how often deprecated documents are retrieved—this indicates your index isn't prioritizing fresh content.

**User feedback on stale information** is a critical signal. If users complain about outdated answers, log the query and the document that was retrieved. Investigate why the stale document was returned instead of the fresh one. This might reveal indexing lag, retrieval ranking issues, or ambiguous queries that match both old and new content.

**Alerting on freshness issues** prevents stale information from reaching users. Alert if critical documents are older than their freshness SLA. Alert if the average age of retrieved documents increases significantly—this indicates your index is getting stale. Alert if users report outdated information more frequently. These alerts let you fix freshness problems before they damage trust.

**2026 patterns for freshness** include incremental indexing where new content is indexed within minutes, streaming updates where changes propagate to retrieval in near real-time, and automated change detection that monitors source systems for updates and triggers re-indexing automatically.

---

## A/B Testing in Production

A/B testing lets you compare different RAG configurations on live traffic to find the best setup. You can test chunking strategies, retrieval methods, prompt changes, generation models, and more.

**What to A/B test in RAG** includes chunking strategy—comparing 512-token chunks versus 256-token chunks with more overlap. Retrieval method—comparing pure vector search versus hybrid retrieval. Prompt variations—testing different system prompts or few-shot examples. Generation models—comparing GPT-4 versus Claude for answer quality and cost. Reranking—testing whether a reranker improves relevance versus raw vector similarity. Number of retrieved chunks—comparing top-3 versus top-5 retrieval.

**A/B test setup** assigns users to variants randomly and consistently. Use user ID or session ID to assign variants so each user sees the same configuration throughout their session. Split traffic 50/50 for the initial test, or use 90/10 if you're testing a risky change. Ensure assignment is logged so you can correlate outcomes with variants.

**Metrics for A/B testing RAG** include answer quality metrics like faithfulness and relevance, measured by LLM-as-judge on a sample of responses. User feedback metrics like thumbs up rate and session success rate. Behavioral metrics like follow-up question rate and escalation rate. Operational metrics like latency and cost. Define a primary metric (often thumbs up rate or session success) and guardrail metrics (faithfulness must stay above 85%, latency must stay below 2 seconds).

**Running the experiment** requires statistical rigor. Calculate sample size needed to detect a meaningful difference—typically thousands of queries for behavior metrics. Run the test long enough to account for day-of-week and time-of-day effects—at least one week. Monitor metrics daily to catch issues early. If a variant causes a significant drop in guardrail metrics, stop the experiment.

**Analyzing results** uses statistical tests. For behavior metrics like thumbs up rate, use a proportion test or chi-squared test. For continuous metrics like latency, use a t-test. Account for multiple comparisons if you're testing many metrics. Look for segment effects—a change might improve outcomes for power users but hurt new users.

**Shipping the winner** means rolling out the better variant to 100% of users. Monitor metrics post-launch to ensure the improvement holds. Document the test and results so you can build on learnings. Iterate with new A/B tests to continuously improve.

---

## Drift Detection

Drift is when the behavior or quality of your RAG system changes over time. Detecting drift early lets you intervene before users notice degradation.

**Types of drift in RAG** include query distribution drift, where the types of queries users ask changes. Retrieval drift, where the distribution of retrieved documents changes. Quality drift, where answer quality metrics degrade. Latency drift, where response time increases. Data drift, where the knowledge base content changes in ways that affect retrieval.

**Query distribution drift detection** compares the distribution of queries over time. Use statistical tests like the Kolmogorov-Smirnov test or chi-squared test to compare this week's query distribution to last month's. Use topic modeling to track changes in query topics. If the query distribution shifts significantly, your offline eval set might no longer be representative, and you should update it.

**Retrieval drift detection** tracks changes in retrieval behavior. Monitor the distribution of retrieved documents—if certain documents are retrieved much more or less frequently, something changed. Track average retrieval scores—if they drop, retrieval quality might be degrading. Track the number of unique documents retrieved per day—if it decreases, retrieval might be getting stuck on a small set of documents.

**Quality drift detection** monitors answer quality metrics over time. Track faithfulness, relevance, and user feedback rate in rolling windows (daily, weekly). Use statistical process control to detect when metrics fall outside expected ranges. Alert if quality drops more than 5% compared to the previous month's baseline.

**Latency drift detection** tracks response time. Monitor p50, p95, and p99 latency in rolling windows. Alert if percentiles increase significantly. Latency drift often indicates infrastructure issues, index growth making retrieval slower, or increased load.

**Root cause analysis for drift** requires investigating logs and correlating changes. If quality drifts, check if there was a knowledge base update, a model deployment, or a query distribution shift. If retrieval drifts, check if indexing changed or documents were added or removed. If latency drifts, check infrastructure metrics like CPU, memory, and network.

**2026 patterns for drift detection** include automated drift monitoring pipelines that run statistical tests daily, anomaly detection models that learn normal behavior and flag deviations, and dashboards that visualize drift metrics over time with automatic alerts.

---

## Alert Design for RAG

Effective alerting balances catching real problems quickly without overwhelming the team with false positives. RAG-specific alerts need to account for inherent variability and user behavior.

**What to alert on** includes hallucination spike where faithfulness drops below 85% in the last hour on a sample of at least 20 responses. Retrieval degradation where average retrieval relevance score drops more than 10% compared to the previous week. No-result rate spike where the percentage of no-result queries doubles in the last day. User feedback crash where thumbs up rate drops below 50% or negative feedback rate increases by 50%. Latency breach where p95 latency exceeds 3 seconds. Freshness violation where critical documents are older than their SLA.

**Setting thresholds** requires balancing sensitivity and specificity. Too sensitive and you get false alarms. Too loose and you miss real problems. Use historical data to understand normal variability—set thresholds at 2-3 standard deviations from the mean. Use rolling baselines that adjust for trends and seasonality. Start with loose thresholds and tighten them as you gain confidence in your monitoring.

**Alert severity levels** help prioritize response. Critical alerts for severe quality degradation or system outages that affect all users. High alerts for significant quality drops or issues affecting a segment of users. Medium alerts for early warning signals or issues with moderate impact. Low alerts for anomalies that need investigation but aren't urgent.

**Alert context** makes alerts actionable. Include the metric value and threshold. Include the time window and sample size. Include links to dashboards showing detailed metrics. Include suggested troubleshooting steps—check recent deployments, check knowledge base updates, check query distribution. Include example queries or responses that triggered the alert.

**Alert fatigue prevention** keeps the team responsive. Aggregate similar alerts—don't send 10 alerts for the same issue. Use snooze or acknowledge features so alerts don't repeat after the team is aware. Regularly review alerts to tune thresholds and eliminate noisy alerts. Celebrate when alerts catch real problems early to reinforce their value.

**On-call workflow for RAG alerts** should prioritize by severity. Critical alerts wake someone up and require immediate action. High alerts interrupt work but can wait for business hours in some cases. Medium and low alerts go to a queue for investigation during scheduled monitoring time. Document the alert response playbook so anyone on-call knows what to do.

---

## 2026 Patterns in Production RAG Monitoring

The cutting edge of RAG monitoring in 2026 involves specialized tools, automated evaluation pipelines, and sophisticated drift detection.

**Arize RAG monitoring** provides purpose-built observability for retrieval-augmented generation. It automatically tracks retrieval relevance, faithfulness, and answer quality. It detects drift in query distribution, retrieval behavior, and quality metrics. It correlates quality degradation with specific documents, query types, or user segments. It supports root cause analysis by linking quality issues to retrieval failures, knowledge base changes, or prompt problems.

**LangSmith production tracing** instruments RAG pipelines end-to-end. Every query is logged with full trace data: the query, retrieved chunks, retrieval scores, generated answer, latency breakdown, and user feedback. Traces can be filtered and analyzed to find patterns. You can replay traces in development to debug issues. You can run evals on production traces to measure quality on real traffic.

**Continuous eval pipelines** run evaluations automatically on a schedule. Every hour, sample recent production queries and run LLM-as-judge evals for faithfulness and relevance. Every day, run retrieval quality evals on a curated test set. Every week, run end-to-end evals on a comprehensive test set and compare to last week's results. Results feed into dashboards and trigger alerts if quality degrades.

**Automated knowledge base gap detection** analyzes no-result and low-confidence queries to find missing content. It clusters similar queries to identify common topics without coverage. It prioritizes gaps by frequency and user impact. It generates suggestions for new documents or FAQ entries to fill gaps. It tracks whether gaps are addressed and measures the impact on no-result rate.

**Hybrid human-AI evaluation at scale** combines automated evals with human review. Automated evals run on all traffic to flag potential issues. Human evaluators review a sample of flagged responses to confirm quality problems. Human feedback is used to fine-tune automated evals, improving agreement over time. This approach scales human judgment to high-volume production systems.

**Real-time quality scoring** computes quality scores for every response as it's generated. Scores are displayed to users—responses with high confidence get a checkmark, low confidence responses get a disclaimer. Scores inform routing—low-quality responses trigger escalation to human support. Scores feed into monitoring—trends in quality scores indicate system health.

---

## Failure Modes in Production RAG Eval

Production RAG monitoring has its own failure modes that can undermine effectiveness.

**Monitoring blind spots** occur when you only monitor what's easy to measure. You track faithfulness but not user satisfaction. You track latency but not retrieval relevance. You track aggregate metrics but miss segment-specific problems. Regularly audit your monitoring to ensure you're covering all aspects of quality.

**Alert fatigue** happens when alerts are noisy or non-actionable. The team starts ignoring alerts or disabling them. When a real problem occurs, no one responds quickly. Prevent alert fatigue by tuning thresholds, aggregating alerts, and demonstrating value when alerts catch issues early.

**Lag in detection** means you don't catch problems until hours or days after they start. This can happen if you run evals in batches instead of real-time, if you don't monitor aggressively enough, or if you set loose thresholds. Reduce detection lag by increasing eval frequency, tightening thresholds on critical metrics, and using real-time scoring.

**False positives in automated evals** occur when LLM-as-judge flags responses as poor quality when they're actually fine. This wastes time investigating non-issues and undermines trust in automated evals. Reduce false positives by using stronger judge models, providing clear rubrics, and validating automated judgments with human review on a sample.

**Sampling bias** happens when the queries you evaluate aren't representative of overall traffic. You might oversample errors, undersample high-confidence responses, or miss rare but important queries. Use stratified sampling to ensure coverage across query types, confidence levels, and user segments.

**Lack of actionability** is when you detect a problem but don't know what to do about it. Faithfulness drops, but you don't know which documents or queries are problematic. Alerts should include enough context to guide investigation and remediation.

---

## Enterprise Expectations

Enterprises have specific requirements for production RAG monitoring that go beyond quality metrics.

**Audit trails** are mandatory in regulated industries. Every query, retrieved document, generated answer, and user feedback must be logged with timestamps. Logs must be immutable and retained for compliance periods. You must be able to trace any response back to the specific retrieved content that informed it.

**Privacy and PII handling** in monitoring requires care. User queries might contain sensitive information. Don't log PII in plain text—redact or hash it. Ensure logs are access-controlled and encrypted at rest. Anonymize data before using it for analysis or model improvement. Comply with GDPR, CCPA, and industry-specific regulations.

**Multi-tenant monitoring** in SaaS environments requires isolating metrics by tenant. Each customer's RAG quality is monitored separately. Alerts are sent to the appropriate customer admin or your support team. You track SLA compliance per customer. You can compare quality across customers to identify outliers.

**Explainability for monitoring** means users can understand why quality degraded. When an alert fires, you can show which queries failed, which documents were retrieved, and why the answer was poor. This transparency builds trust and helps users collaborate on fixes.

**Incident response integration** connects RAG monitoring to your broader incident management. RAG quality alerts create incidents in your ticketing system. Incidents are triaged, assigned, and resolved following standard procedures. Post-incident reviews analyze root causes and implement preventive measures.

---

## Production Monitoring Template

Here's a production monitoring configuration for a RAG system deployed with LangSmith and Arize:

```yaml
production_monitoring:
  sampling:
    faithfulness_eval_rate: 0.03  # 3% of responses
    retrieval_eval_rate: 0.05     # 5% of queries
    high_risk_sampling: 1.0       # 100% of low-confidence queries

  real_time_scoring:
    faithfulness_threshold: 0.85
    retrieval_relevance_threshold: 0.3
    confidence_threshold: 0.5

  metrics:
    - name: faithfulness_score
      source: llm_judge
      frequency: continuous
      alert_threshold: 0.85

    - name: retrieval_relevance
      source: retrieval_system
      frequency: continuous
      alert_threshold: 0.3

    - name: thumbs_up_rate
      source: user_feedback
      frequency: hourly
      alert_threshold: 0.5

    - name: no_result_rate
      source: retrieval_system
      frequency: hourly
      alert_threshold: 0.1

  drift_detection:
    query_distribution:
      window: 7d
      comparison: 30d
      test: chi_squared
      significance: 0.05

    quality_metrics:
      window: 24h
      comparison: 7d
      test: t_test
      significance: 0.01

  alerts:
    - name: hallucination_spike
      condition: faithfulness_score < 0.85 AND sample_size >= 20
      window: 1h
      severity: critical

    - name: retrieval_degradation
      condition: avg_retrieval_score drops 10% vs last_week
      window: 1d
      severity: high

    - name: no_result_spike
      condition: no_result_rate > 2x baseline
      window: 1d
      severity: medium

  freshness_checks:
    critical_docs_sla: 24h
    product_docs_sla: 48h
    blog_posts_sla: 1h
    alert_on_sla_breach: true
```

---

## Interview: Production RAG Evaluation

**Q: We deployed our RAG system after thorough offline evaluation with 90% faithfulness and 85% answer relevance. In production, users are complaining about wrong answers. How is this possible?**

Offline eval uses a curated test set that represents your expectations, not reality. You tested on questions you anticipated, with documents you knew were relevant, in scenarios you could control. Production throws everything at you: queries phrased in unexpected ways, questions about edge cases not in your test set, users asking about recent events not in your knowledge base, and failure modes that only emerge at scale.

The gap between offline and production performance comes from several sources. Query distribution shift means users ask different types of questions than your test set. Your offline eval might focus on straightforward factual questions, but users ask vague exploratory questions, troubleshooting questions with incomplete context, or comparison questions that require retrieving multiple documents. Knowledge base drift means your retrieval index changes as content is added or updated, but your eval set stays static. Temporal relevance means some queries require fresh information that your offline eval data doesn't have.

To bridge this gap, implement production monitoring that samples real queries and runs automated faithfulness checks. Analyze the queries where users complain—are they asking about topics not in your test set? Are they using phrasing that breaks your retrieval? Use these insights to expand your offline eval set and improve your system. Production and offline eval should inform each other in a continuous loop.

**Q: What's the most important metric to monitor for a production RAG system?**

There's no single most important metric—you need a balanced scorecard. But if I had to choose one, it's user feedback rate, specifically the percentage of queries that get positive feedback (thumbs up or equivalent). This metric directly measures whether users found the answer helpful, which is the ultimate goal.

User feedback captures many aspects of quality in one signal. If your answer hallucinates, users will give negative feedback. If retrieval fails to find relevant documents, the answer will be unhelpful and users will downvote. If your answer is technically correct but poorly phrased or doesn't address the user's actual need, feedback will be negative. User feedback also accounts for factors that automated metrics miss, like tone, clarity, and completeness.

However, user feedback alone isn't enough because it's sparse and delayed. Most users don't rate responses, so you're making inferences from a small sample. Feedback comes after the response is served, so you can't use it for real-time quality control. You need to supplement user feedback with automated metrics like faithfulness score, retrieval relevance, no-result rate, and latency. Monitor all of these, but prioritize user feedback as the ultimate ground truth.

**Q: How do we detect when our knowledge base has become stale or outdated?**

Detecting stale knowledge requires both automated checks and user feedback signals. On the automated side, track the last update timestamp for every document in your knowledge base. Alert if critical documents haven't been updated in weeks or months relative to their freshness SLA. Compare your knowledge base to source systems—if your CMS or documentation site has newer versions, your retrieval index is stale. Use content hashing to detect changes and trigger re-indexing automatically.

User feedback is often the first signal of staleness. Users complain about outdated pricing, deprecated features, or "that information is old." Log these complaints and correlate them with the documents that were retrieved. Track patterns—if many users report stale information about the same topic, that area of your knowledge base needs updating. Implement a feedback mechanism that lets users flag outdated information directly on responses.

You can also use automated freshness scoring. For documents that should change frequently (like pricing, feature lists, or event information), check if they've been updated recently. For documents with explicit version numbers or dates, validate that retrieved documents match the current version. If a query about "current pricing" retrieves a document from six months ago, flag it as potentially stale.

Finally, implement incremental indexing so new content is available quickly. If it takes days for documentation updates to reach production, you'll always be serving stale information. Streaming updates or near-real-time indexing keeps your knowledge base fresh.

**Q: We want to A/B test a new chunking strategy—512-token chunks instead of 256-token chunks. How do we design this experiment?**

Start by defining your hypothesis and metrics. Your hypothesis might be "512-token chunks will improve answer relevance because they provide more context, without significantly increasing latency." Your primary metric is answer relevance, measured by LLM-as-judge on a sample of responses. Your guardrail metrics are faithfulness (must stay above 85%), latency (must stay below 2 seconds), and user feedback rate (must not drop).

Set up the experiment infrastructure. Randomly assign users to variant A (256-token chunks) or variant B (512-token chunks) based on user ID or session ID, ensuring consistency—each user should see the same variant throughout their session. Split traffic 50/50 initially. Ensure assignment is logged so you can correlate outcomes.

Calculate sample size. To detect a 5% improvement in answer relevance with 80% power and 95% confidence, you'll need several thousand queries per variant, depending on baseline variance. Plan to run the test for at least one week to account for day-of-week effects and accumulate enough data.

Monitor metrics daily during the test. Check that both variants are serving traffic correctly and that guardrail metrics haven't crashed. If 512-token chunks cause latency to spike above 2 seconds, stop the test early. If faithfulness drops significantly, investigate why—larger chunks might include irrelevant information that confuses the model.

After the test concludes, analyze results with statistical rigor. Use a t-test or proportion test to compare answer relevance between variants. Check for segment effects—does the new chunking help power users but hurt new users? Look at other metrics like retrieval relevance, no-result rate, and cost. If 512-token chunks improve answer relevance by 7% without hurting guardrails, roll out to 100% of users and monitor post-launch.

**Q: Our RAG system's quality has been degrading slowly over the past month, but we didn't catch it until users complained. How do we detect drift earlier?**

You need automated drift detection that monitors key metrics in rolling windows and alerts when they deviate from baseline. Set up daily monitoring for quality metrics like faithfulness, answer relevance, and user feedback rate. Use statistical process control to define expected ranges—typically within 2-3 standard deviations of the rolling 30-day mean. Alert if metrics fall outside this range for two consecutive days.

Implement query distribution drift detection. Use statistical tests like chi-squared or Kolmogorov-Smirnov to compare this week's query distribution to the previous month's. If the distribution shifts significantly, your system might be facing new types of queries that your offline eval didn't cover. This is an early warning that quality could degrade even if current metrics look fine.

Monitor retrieval drift by tracking the distribution of retrieved documents. If certain documents are retrieved much more or less frequently, or if average retrieval scores drop, retrieval behavior is changing. This often precedes answer quality degradation because poor retrieval leads to poor answers.

Set up freshness monitoring to catch knowledge base staleness. If critical documents aren't being updated or if your retrieval index is lagging behind source systems, you'll start serving outdated information. Alert on freshness violations before users notice.

Finally, implement continuous evaluation pipelines that run offline evals on production traffic samples daily. Compare this week's eval results to last month's. Even a 5% drop in faithfulness over a month should trigger investigation. By catching drift early through automated monitoring, you can intervene before quality degradation reaches users and damages trust.

---

## Bridge to Chapter 9.10

You've now learned how to monitor and evaluate RAG quality in production—sampling responses for faithfulness, tracking retrieval quality, analyzing user feedback, detecting drift, and alerting on degradation. Production monitoring is fundamentally different from offline eval because you're dealing with real queries, changing data, and evolving needs. By building real-time monitoring, feedback loops, and drift detection, you can maintain high RAG quality as your system scales.

Next, in **Chapter 9.10: RAG Eval Maturity and Best Practices**, we'll synthesize everything you've learned about RAG evaluation into a maturity model. We'll explore how RAG eval practices evolve from ad-hoc manual checks to sophisticated automated pipelines, what best practices separate great RAG systems from mediocre ones, and how to build an organizational culture of continuous RAG quality improvement. You'll learn how to assess your current maturity, identify gaps, and build a roadmap to world-class RAG evaluation.

---

**End of Chapter 9.9**
*Next: 9.10 — RAG Eval Maturity and Best Practices*
