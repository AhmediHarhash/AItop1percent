# Chapter 11.5 — Production Sampling Strategies

A friend of mine runs the quality team for a healthcare chatbot that processes 4 million conversations per month.

When they first launched production monitoring, they tried to review everything. They hired twenty contractors, built a review portal, and set a goal: "every user interaction gets a quality score within 24 hours."

They lasted nine days.

By day three, the review backlog hit 400,000 cases. By day seven, contractors were clicking through evaluations without reading them. By day nine, the budget was blown and the team lead was in crisis meetings explaining why they'd spent $180,000 to learn almost nothing about quality.

The problem wasn't the reviewers. It wasn't the tooling. It was the strategy.

They were trying to drink from a fire hose. At scale, you cannot review everything. You have to sample — and you have to sample smart. Not just randomly, but strategically, so you find the signal that matters.

Let me show you how teams actually do this in 2026.

---

## Why You Cannot Review Everything (The Math Problem)

Let's start with cold reality.

At any meaningful scale, manual review of all production traffic is financially and operationally impossible.

A trained reviewer can carefully evaluate 10-20 interactions per hour, depending on task complexity. At $30 per hour, that's $1.50-$3.00 per case.

If you process 100,000 interactions per day:
- Full review would cost $150,000-$300,000 per day
- Require 5,000-10,000 contractor hours per day
- Need 600-1,200 full-time contractors working around the clock

Even companies with massive budgets don't do this. It's not just expensive — it's slow, unscalable, and produces diminishing returns.

Here's the insight that changes everything: **most of your interactions are fine, and most of your quality signal comes from a small subset of cases.**

You don't need to review the millionth "What's your refund policy?" interaction this month. It's the same as the previous 999,999. You need to review:

- The edge cases where your model struggled
- The interactions that triggered errors or user complaints
- The unusual inputs that might expose failure modes
- The high-stakes tasks where mistakes cost you customers

Smart sampling isn't about seeing everything. It's about seeing the **right things** — the cases that tell you where your system is weak, where it's improving, and where it's about to break.

---

## Random Sampling: The Baseline (Necessary But Insufficient)

The simplest approach is **random sampling**: take X percent of all production traffic and review it.

Example: sample 1% of daily traffic. If you process 100,000 interactions per day, you review 1,000 cases.

### Why random sampling matters

Random sampling is unbiased. It gives you a representative slice of your actual traffic distribution — common cases, rare cases, easy tasks, hard tasks, everything in proportion.

This makes it valuable for:

- **Establishing baseline quality metrics** across your whole product
- **Tracking trends over time** (are things getting better or worse overall?)
- **Detecting systemic issues** that affect all traffic types equally

If you only use targeted sampling strategies, you'll never know how typical users are experiencing your system.

### Why random sampling alone is insufficient

The problem with random sampling is **it wastes most of your review budget on easy cases.**

If 90% of your traffic is straightforward requests that your model handles well, then 90% of your random sample will be boring. You'll spend $1.50 per case reviewing things like:

- "What are your hours?" → model gives correct answer
- "Track my order" → model retrieves correct tracking info
- "Change my password" → model provides correct instructions

These reviews tell you almost nothing. You already knew the model was good at common cases. That's not where failure happens.

The signal-to-noise ratio is terrible. You're finding maybe 5-10 interesting cases out of every 100 reviewed.

### The right role for random sampling

Use random sampling as **one component** of a multi-strategy approach:

- Allocate 20-30% of your review budget to random sampling
- Use it for trend tracking and baseline metrics
- Combine it with targeted sampling to find edge cases

Think of random sampling as your "control group" — it keeps you honest about overall quality while other strategies hunt for problems.

---

## Stratified Sampling: Ensuring Coverage Across Slices

**Stratified sampling** means dividing your traffic into meaningful groups (strata) and sampling proportionally from each group.

Example strata:
- Task type (FAQ, account management, order tracking, agent workflows)
- User segment (free tier, paid tier, enterprise customers)
- Risk tier (Tier 0: low-stakes read-only, Tier 1: medium-stakes actions, Tier 2: high-stakes financial/identity)
- Language (English, Spanish, French, etc.)
- Channel (web, mobile, voice, API)

Instead of taking 1% of all traffic randomly, you take 1% from each stratum.

### Why this matters

Imagine your traffic is:
- 80% FAQ questions (easy)
- 15% account management (medium)
- 5% payment-related (hard, high-stakes)

Pure random sampling would give you:
- 800 FAQ reviews
- 150 account reviews
- 50 payment reviews

That 50-case payment sample might be too small to detect problems. If your payment flow has a 2% failure rate, you'd expect to see only 1 failure in your sample — easy to miss as statistical noise.

With stratified sampling, you set minimums:
- 500 FAQ reviews (0.6% sample rate)
- 200 account reviews (1.3% sample rate)
- 300 payment reviews (6% sample rate)

Now you have enough signal from each critical slice to make confident quality assessments.

### Practical stratification strategies

**By task type:**
- Ensure every task in your taxonomy gets at least N samples per week (e.g., 50-100)
- Over-sample rare but important tasks

**By user segment:**
- Sample more heavily from enterprise customers (they pay more, expect more)
- Sample more from first-time users (they're forming impressions)
- Sample less from power users who've already validated your product

**By risk tier:**
- Tier 0 (informational): 0.5% sample rate
- Tier 1 (account actions): 2% sample rate
- Tier 2 (financial, identity): 5-10% sample rate

**By geography/language:**
- If you serve 10 languages, ensure each language has sufficient samples
- Don't let English-only reviews hide problems in other languages

### The formula

For each stratum, set a **minimum sample size** needed for statistical confidence (typically 30-100 cases for rough signal, 300+ for precise estimates).

Then allocate your budget proportionally, respecting minimums.

---

## Uncertainty-Based Sampling: High Signal Per Review Dollar

This is where modern production monitoring gets smart.

**Uncertainty-based sampling** means sampling interactions where your model was least confident. These are the cases most likely to be wrong.

### The mechanics

Many production systems emit confidence scores alongside outputs:
- RAG systems show retrieval scores
- Classification models output probability distributions
- LLMs can be prompted to self-assess confidence

Interactions with **low confidence** are red flags:
- The model retrieved documents but none were highly relevant
- The classification was 51% Class A vs 49% Class B (near the decision boundary)
- The LLM said "I'm not entirely certain, but I think..."

These are exactly the cases you want human reviewers to check.

### Why this works

Let's say your model is 95% accurate overall, but only 60% accurate when confidence is below 0.7.

If you sample randomly:
- 95% of reviews show the model was correct (wasted effort)
- 5% catch errors (useful signal)

If you sample only low-confidence cases:
- 40% of reviews show the model was correct (still useful validation)
- 60% catch errors (high signal density)

You've multiplied your error-detection rate by 12x while spending the same amount on review.

### Implementation patterns

**Simple threshold-based:**
- Sample 100% of interactions where confidence is below 0.6
- Sample 10% where confidence is 0.6-0.8
- Sample 1% where confidence is above 0.8

**Proportional sampling:**
- Sample rate = 1 minus confidence score
- Low confidence (0.5) → 50% sample rate
- High confidence (0.95) → 5% sample rate

**Budget-constrained:**
- Rank all interactions by confidence (lowest first)
- Review the bottom N percent that fits your budget

### Calibration is critical

This strategy only works if your confidence scores are **calibrated** — meaning a confidence of 0.7 actually corresponds to 70% accuracy.

Many models produce overconfident scores (says 0.9 but is only 70% accurate) or underconfident scores (says 0.7 but is 95% accurate).

Before relying on uncertainty-based sampling:
- Review a random sample and measure actual accuracy vs predicted confidence
- If miscalibrated, adjust your thresholds accordingly

---

## Edge-Case Sampling: Finding the Weird Stuff

**Edge-case sampling** means deliberately looking for interactions that are unusual, unexpected, or outside normal patterns.

These cases are rare in production but disproportionately likely to expose failure modes.

### What counts as an edge case

**Very long inputs:**
- User sends a 3,000-word message when typical messages are 20 words
- Possible issues: truncation, context window limits, latency spikes

**Very short inputs:**
- User sends "help" or "???" or just an emoji
- Possible issues: the model needs more context to help but has none

**Unusual topics:**
- Detect inputs that don't match any of your known intents
- Use clustering or anomaly detection on embeddings
- Sample from the outlier clusters

**Rare languages or dialects:**
- If 95% of traffic is English, sample more heavily from the other 5%
- Multilingual models often degrade on low-resource languages

**First-time users:**
- Users who have never interacted before might phrase things differently
- They haven't learned your system's quirks yet

**Unusual timing:**
- Requests at 3 AM local time (could be unusual use cases)
- Requests during outages or degraded service windows

### Detection strategies

**Statistical outliers:**
- Track distributions for: input length, output length, latency, retrieved document scores
- Flag interactions in the 95th+ percentile or below 5th percentile

**Embedding-based anomaly detection:**
- Embed all inputs using a sentence transformer
- Cluster and identify low-density regions
- Sample from those regions

**Intent classification confidence:**
- If your system classifies intents, sample cases where no intent scored above a threshold
- These are "what even is this?" cases

### Why edge cases matter

Most users experience your product on the "happy path" — they ask normal questions, get normal answers, and everything works.

But edge cases are where:
- Users get frustrated and churn
- Safety incidents happen
- Compliance violations occur
- Your brand reputation takes hits

One bizarre failure that goes viral on social media costs you more than a thousand successful common cases help you.

---

## Error-Triggered Sampling: Catch Every Failure Signal

The highest-signal cases are the ones where **something already went wrong.**

**Error-triggered sampling** means automatically flagging and reviewing interactions that showed signs of failure.

### What triggers an error

**System errors:**
- API timeouts
- Tool call failures
- Database query errors
- Exceptions in your code

**User error signals:**
- User clicked "thumbs down" or gave negative feedback
- User immediately rephrased the same question (suggests first answer was unsatisfactory)
- User asked for a human agent or escalation
- User abandoned the interaction mid-conversation

**Moderation flags:**
- Your content filter triggered
- Safety classifier flagged the input or output
- Abuse detection system raised an alert

**Heuristic failures:**
- Output contained "I don't know" or "I can't help with that" (when it shouldn't)
- Output was suspiciously short or repetitive
- Latency exceeded SLA thresholds

### Sample rate for error-triggered cases

Unlike other strategies, error-triggered sampling often has a **100% review rate.**

Why? Because these are your highest-value learning opportunities:
- They tell you exactly where the system is breaking
- They often represent user-facing failures you need to fix urgently
- The volume is usually small enough to afford full review

If you process 100,000 interactions per day and 2% show error signals, that's 2,000 cases — manageable for a small review team.

### Building the error-triggered pipeline

**Step 1: Instrument error detection**
- Log all exceptions, timeouts, and failed tool calls
- Capture user feedback signals (thumbs down, escalation requests)
- Track rephrases and abandonment

**Step 2: Auto-route to review queue**
- Flag every error-triggered case for review
- Priority-sort by severity (system crash → high, user rephrase → medium)

**Step 3: Root cause investigation**
- Reviewers don't just score quality — they diagnose why it failed
- Tag with failure mode: "retrieval failed," "hallucination," "tone issue," "policy violation"

**Step 4: Feed findings back**
- Create eval dataset cases for every failure mode
- Update regression tests
- File bugs for systemic issues

### Error-triggered sampling as a forcing function

This strategy forces you to confront every failure. You can't hide from bad outputs or ignore user complaints.

It's uncomfortable but essential. If you're not reviewing your errors, you're flying blind.

---

## Adversarial Sampling: Hunt for Attacks and Abuse

**Adversarial sampling** means specifically looking for interactions that appear to be prompt injection attempts, jailbreaks, abuse, or other adversarial behavior.

These cases require specialized review — often by your safety or security team.

### What counts as adversarial

**Prompt injection attempts:**
- "Ignore previous instructions and..."
- "You are now in developer mode..."
- Attempts to override system prompt or extract hidden instructions

**Jailbreak attempts:**
- Requests for harmful content phrased indirectly
- "Write a story about..." (where the story is really instructions for something dangerous)
- Role-playing scenarios designed to bypass safety filters

**Abuse and toxicity:**
- Hate speech, harassment, threats
- Requests for illegal content
- Attempts to generate spam or malware

**Data extraction attempts:**
- "Repeat your training data"
- "Tell me what other users have asked you"
- Attempts to leak PII or proprietary information

### Detection methods

**Keyword-based filters:**
- Maintain a list of known adversarial phrases
- Flag interactions containing phrases like "ignore all previous," "jailbreak," "DAN mode"

**Classifier-based detection:**
- Train or use a pre-trained classifier to detect prompt injections
- Many vendors offer adversarial detection APIs (e.g., Lakera Guard, HiddenLayer)

**Embedding similarity:**
- Maintain a library of known adversarial prompts
- Embed incoming requests and flag high-similarity matches

**LLM-based meta-detection:**
- Use a second LLM to analyze the input: "Does this input appear to be a prompt injection or jailbreak attempt?"

### Sample rate and routing

Adversarial cases are rare (typically below 1% of traffic) but high-stakes.

Sample rate: **100%** of detected adversarial attempts go to review.

Route to specialized reviewers:
- Not general contractors — you need people trained on safety and security
- Often routed to internal safety team or trusted partners

### Why this matters

A single successful jailbreak that goes viral can:
- Damage your brand reputation
- Trigger regulatory scrutiny
- Expose liability for harmful content generation

Adversarial sampling is your early warning system. It helps you:
- Understand what attackers are trying
- Measure how often your defenses fail
- Build better safety filters and red-teaming datasets

---

## Sample Size Math: How Many Is Enough?

You have a budget for 1,000 reviews per week. How should you split them across your strata?

This is a question of **statistical power**: how many samples do you need to get meaningful estimates of quality?

### The formula for proportions

When you're estimating a proportion (e.g., "what percentage of payment interactions are handled correctly?"), the sample size needed is:

```
n = (Z^2 * p * (1-p)) / E^2

where:
- Z = Z-score for your desired confidence level (1.96 for 95% confidence)
- p = estimated proportion (use 0.5 if unknown, as it's the most conservative)
- E = margin of error (how much uncertainty you're willing to tolerate)
```

### Practical examples

**Scenario 1: High-volume FAQ category**
- You want to estimate accuracy within plus-or-minus 5 percentage points
- 95% confidence level
- n = (1.96^2 * 0.5 * 0.5) / 0.05^2 = 384 samples

**Scenario 2: Low-volume payment category**
- You want to estimate accuracy within plus-or-minus 10 percentage points
- 90% confidence level (Z = 1.645)
- n = (1.645^2 * 0.5 * 0.5) / 0.10^2 = 68 samples

**Scenario 3: Safety-critical interactions**
- You need very tight bounds: plus-or-minus 2 percentage points
- 99% confidence level (Z = 2.576)
- n = (2.576^2 * 0.5 * 0.5) / 0.02^2 = 4,147 samples

### Rules of thumb for 2026

If you don't want to do the math every time:

**Rough signal (directional sense):**
- 30-50 samples per stratum
- Good for "is this getting better or worse?"

**Moderate confidence (typical production monitoring):**
- 100-200 samples per stratum
- Good for tracking metrics with roughly plus-or-minus 7-10% error bars

**High confidence (release decisions, SLA verification):**
- 300-500+ samples per stratum
- Good for precise estimates with plus-or-minus 5% error bars

### Budget allocation heuristic

If you have N total reviews per week:
- Reserve 20-30% for random sampling (baseline)
- Count your strata (task types, risk tiers, user segments)
- Allocate remaining budget to ensure each stratum gets at least 50-100 samples
- If budget allows, boost critical strata to 300+ samples

---

## Combining Strategies: A Practical Budget Allocation

Real-world production sampling uses **multiple strategies simultaneously.**

Here's a proven allocation that works for most teams:

### Budget split

Assume you can afford 1,000 reviews per week:

**30% Random sampling (300 reviews)**
- Pure random sample across all traffic
- Used for baseline metrics and trend tracking
- No filtering, no weighting

**30% Uncertainty-based sampling (300 reviews)**
- Sample cases where model confidence was below 0.7
- These have high error rates and provide learning signal
- Helps you understand model limitations

**20% Error-triggered sampling (200 reviews)**
- Every interaction that hit an error, timeout, or user complaint
- 100% review rate for this category (200 assumes 0.2% error rate on 100k interactions)
- Highest-priority cases for fixes

**10% Edge-case sampling (100 reviews)**
- Outliers by length, topic, language, user type
- Sample from low-density regions of embedding space
- Catches tail-risk failures

**10% Adversarial sampling (100 reviews)**
- Detected prompt injection, jailbreak, or abuse attempts
- 100% review rate for detected adversarial cases
- Routed to safety team

### Adjustments by product type

**High-stakes enterprise product (payments, healthcare, legal):**
- Increase error-triggered to 30%
- Increase adversarial to 15%
- Reduce random to 20%

**Consumer chatbot (low-stakes, high-volume):**
- Increase random to 40%
- Increase uncertainty-based to 40%
- Reduce edge-case to 5%

**RAG system (knowledge retrieval):**
- Add a "grounding check" category (20%): sample cases with low retrieval scores
- Reduce random to 20%

**Agent system (tool calls, multi-step workflows):**
- Add a "tool failure" category (20%): sample any interaction where a tool call returned an error
- Reduce random to 20%

### Dynamic reallocation

As your system matures, adjust allocations:

**Month 1 (learning phase):**
- Heavy on random and uncertainty-based to understand baseline
- Less on adversarial (you haven't seen many attacks yet)

**Month 3 (optimization phase):**
- Reduce random as baselines stabilize
- Increase uncertainty-based and edge-case to hunt for improvements

**Month 6+ (maintenance phase):**
- Maintain error-triggered and adversarial at high rates
- Reduce others as quality stabilizes and you've covered the main failure modes

---

## The Review Pipeline: From Sample to Action

Sampling is just step one. Here's the full pipeline:

### Step 1: Automated sampling logic

Your system runs continuously, tagging interactions for review based on the strategies above:
- Random: 1% of all traffic gets tag "random_sample"
- Uncertainty: all cases with confidence below 0.7 get tag "uncertainty_sample"
- Error: all cases with errors get tag "error_sample"
- Edge: statistical outliers get tag "edge_sample"
- Adversarial: detected attacks get tag "adversarial_sample"

Interactions can have multiple tags.

### Step 2: LLM-as-Judge automated scoring

Before human review, run an LLM-as-Judge on all sampled cases:
- Score on key dimensions: correctness, helpfulness, safety, tone
- Flag low-scoring cases for priority human review
- Generate draft explanations for reviewers

This pre-triage reduces human workload. Reviewers focus on:
- Cases where LLM-as-Judge scored poorly (most likely to be real issues)
- High-stakes cases (error, adversarial) regardless of LLM score
- Random QC checks on LLM-as-Judge accuracy

### Step 3: Human review queue

Reviewers see a prioritized queue:
1. Adversarial (safety-critical, to safety team)
2. Error-triggered (user-facing failures, to product team)
3. Low-scoring LLM-as-Judge cases (likely quality issues)
4. Random and uncertainty samples (for calibration)

Reviewers:
- Score on your standard rubric (Chapter 2.2)
- Tag failure modes
- Write notes on what went wrong

### Step 4: Findings feed back to eval dataset

Every reviewed case becomes a candidate for your eval dataset:
- If it exposed a failure, add it to your regression suite (Chapter 12)
- If it showed unusual behavior, add it to edge-case pack (Chapter 3.3)
- If it was adversarial, add it to red-teaming suite (Chapter 14)

Your production samples continuously refresh your offline evals.

### Step 5: Metrics and alerting

Aggregate review scores into metrics:
- Overall quality by day/week
- Quality by task type, user segment, risk tier
- Trends over time

Alert when metrics dip below thresholds:
- Overall accuracy drops below 90% → page on-call
- Safety violations spike → alert safety team
- High-value customer segment quality degrades → alert product lead

---

## 2026 Patterns: Active Learning and Intelligent Triage

In 2026, the most advanced teams use **active learning** principles to continuously improve sampling strategies.

### Active learning-based sampling

Traditional sampling: fixed rules (sample 1% randomly, sample all low-confidence cases).

Active learning: your system learns which cases provide the most information and adapts sampling dynamically.

**How it works:**
1. You review an initial batch using your fixed sampling strategy
2. Train a model to predict "how likely is this case to reveal a new failure mode?"
3. Sample future cases based on predicted learning value

**Signals that indicate high learning value:**
- Model has never seen similar inputs before (novelty)
- Model disagrees with LLM-as-Judge score (confusion)
- Case sits at decision boundary between two intents (ambiguity)
- Case involves features your model rarely uses

This is cutting-edge but increasingly practical. If you have a strong ML team, it's worth exploring.

### LLM-powered triage before human review

Use an LLM as a **first-pass triage layer** before humans see anything:

**Prompt example:**
```
You are triaging production interactions for human review.

Given this interaction, determine:
1. Severity: Critical / High / Medium / Low
2. Category: Error / Safety / Quality / Edge-case
3. Suggested reviewer: Safety team / Product team / General contractor
4. Priority: Urgent / Standard / Low
5. One-line summary of why this needs review

Interaction:
[user input and model output]

Output your triage assessment.
```

This LLM:
- Routes cases to the right reviewer (safety team for adversarial, product team for functional errors)
- Prioritizes urgent cases (user complaints, safety issues)
- Filters out false positives (cases flagged by heuristics but actually fine)

Human reviewers now see a much cleaner, better-prioritized queue.

### Automated sample routing by expertise needed

Not all reviewers should see all cases.

**Routing logic:**
- **Safety-critical cases** (adversarial, content moderation) → specialized safety reviewers
- **Domain-specific cases** (medical, legal, financial) → domain expert reviewers
- **Multilingual cases** → native speakers of that language
- **General quality cases** → general contractor pool

This improves review quality and reduces costs (you pay domain experts only for cases that need them).

Implementation:
- Tag each sampled case with required expertise
- Route to appropriate review pool
- Track reviewer performance by expertise area

---

## Failure Modes: What Goes Wrong

### Failure Mode 1: Sampling the wrong things

**Symptom:** You're reviewing 1,000 cases per week but quality incidents still surprise you.

**Root causes:**
- Too much random sampling, not enough targeted sampling
- Stratification doesn't match your actual risk profile
- Missing entire categories (e.g., you don't sample tool failures for agents)

**Fix:** Audit your sampling strategy against production incidents from the last quarter. Are the cases you sample similar to the cases that broke? If not, adjust your strata and targeted sampling.

### Failure Mode 2: Sample size too small

**Symptom:** Your metrics swing wildly week-to-week. One week accuracy is 92%, next week it's 83%, week after it's 89%.

**Root causes:**
- Your sample size is too small to be statistically stable
- Variance dominates signal

**Fix:** Increase sample size per stratum until trends stabilize. Use the sample size formulas above. If you can't afford larger samples, widen your time windows (monthly instead of weekly).

### Failure Mode 3: Over-indexing on errors

**Symptom:** You spend all your review budget on error cases and adversarial attempts. You have no idea how the 98% of interactions that don't error are performing.

**Root causes:**
- You've set error and adversarial sampling to 100% review but haven't budgeted for random baseline

**Fix:** Cap error and adversarial categories at a fixed percentage of your budget (20-30%). Reserve budget for random and uncertainty-based sampling to track overall quality.

### Failure Mode 4: Sampling without action

**Symptom:** You have beautiful dashboards showing quality metrics by stratum, but nothing improves.

**Root causes:**
- Sampling and review happen in isolation
- Findings don't feed back into eval datasets, model training, or product fixes
- No ownership or accountability for acting on review findings

**Fix:** Establish a weekly ritual: review the top 10 failure modes from sampled cases, assign owners to fix or add to regression tests. If a failure mode isn't actioned within two weeks, stop reviewing that category (it's wasted effort if no one acts on it).

---

## Enterprise Expectations: What Serious Teams Do

By 2026, enterprise production monitoring looks like this:

### Sampling strategy is documented and versioned

Your sampling strategy is not an ad-hoc script. It's a documented policy:
- What percentage of each stratum do you sample?
- What triggers error-based sampling?
- What thresholds define uncertainty-based sampling?
- How often is the strategy reviewed and updated?

This documentation lives in your knowledge base, not someone's head.

### Sample sizes meet statistical thresholds

You don't guess at sample sizes. You calculate them based on required confidence intervals:
- For SLA-tracked metrics: 300+ samples per stratum per measurement period
- For directional tracking: 100+ samples per stratum
- For safety-critical categories: 100% review

### Stratification aligns with business risk

Your strata are not just task types. They include:
- Top enterprise customers (sample more from your highest-value accounts)
- Regulated workflows (compliance-sensitive paths get 100% review)
- Revenue-impacting interactions (checkout flows, upgrade prompts)

### Findings feed back into eval datasets within one week

Every production failure discovered via sampling becomes:
- A new test case in your offline eval dataset (Chapter 3)
- A regression test (Chapter 12)
- A candidate for root cause analysis and bug fix

The loop closes fast. No "we'll add that to the backlog" — it happens within days.

### Sampling strategy evolves with product

As you launch new features, add new tasks, or serve new user segments:
- Update your stratification within one week
- Add new categories to sampling strategy
- Ensure new areas get sufficient sample coverage from day one

---

## Ready-to-Use Template: Production Sampling Plan

**Product:** [your product name]
**Date:** [YYYY-MM-DD]
**Owner:** [quality team lead]

### Traffic Profile
- **Daily volume:** _____ interactions
- **Top task types:** [list top 5-10]
- **Risk tiers:** [% of traffic in Tier 0, Tier 1, Tier 2]
- **Error rate:** ____%
- **Adversarial detection rate:** ____%

### Review Budget
- **Total reviews per week:** _____
- **Cost per review:** $_____
- **Total weekly cost:** $_____

### Sampling Strategy

**Random sampling (30% of budget = ___ reviews/week):**
- Pure random across all traffic
- Used for baseline metrics

**Uncertainty-based sampling (30% of budget = ___ reviews/week):**
- Sample all interactions where model confidence is below 0.7
- Cap at ___ reviews/week; if volume exceeds, take lowest-confidence cases first

**Error-triggered sampling (20% of budget = ___ reviews/week):**
- 100% review of: system errors, user complaints, escalations, rephrases
- Expected volume: ___ per week

**Edge-case sampling (10% of budget = ___ reviews/week):**
- Sample outliers by: input length (above 90th percentile, below 10th percentile), embedding novelty (low-density clusters), rare languages

**Adversarial sampling (10% of budget = ___ reviews/week):**
- 100% review of: detected prompt injections, jailbreak attempts, abuse
- Expected volume: ___ per week
- Routed to: [safety team email/queue]

### Stratification Requirements

**By task type:**
- Each top-10 task type gets minimum ___ samples/week
- Long-tail tasks: sample at ___% rate

**By risk tier:**
- Tier 0: ___% sample rate
- Tier 1: ___% sample rate
- Tier 2: ___% sample rate

**By user segment:**
- Enterprise customers: ___% sample rate
- Paid tier: ___% sample rate
- Free tier: ___% sample rate

**By language:**
- Each supported language gets minimum ___ samples/week

### Review Pipeline

1. **Automated sampling:** interactions tagged by strategy
2. **LLM-as-Judge triage:** all sampled cases scored automatically
3. **Human review queue:** prioritized by severity
4. **Routing logic:** [safety team gets adversarial, product team gets errors, contractors get general quality]
5. **Findings feedback:** all failures added to eval dataset within 1 week

### Metrics and Alerts

- **Overall quality:** alert if below ____%
- **Safety violations:** alert if above ___ per week
- **Error rate:** alert if above ____%
- **Tier 2 quality:** alert if below ____%

### Review Cadence

- Strategy reviewed and updated: [monthly/quarterly]
- Metrics dashboard reviewed: [daily/weekly]
- Deep-dive analysis: [weekly/monthly]

---

## Interview Q&A: Real Questions You'll Get Asked

**Q: How do you decide what percentage of production traffic to review when you're processing millions of interactions per day?**

A: You can't review everything — the math doesn't work. I use a multi-strategy approach: allocate 30% of my review budget to random sampling for baseline metrics, 30% to uncertainty-based sampling for cases where the model was least confident, 20% to error-triggered sampling for interactions that hit errors or user complaints, 10% to edge-case sampling for outliers, and 10% to adversarial sampling for prompt injection attempts. This ensures I'm seeing both typical traffic and the highest-risk cases. For sample sizes, I calculate based on statistical confidence intervals — typically 100-200 samples per stratum for moderate confidence, 300+ for high-confidence metrics.

---

**Q: What's the difference between random sampling and stratified sampling, and when would you use each?**

A: Random sampling takes a uniform percentage of all traffic — it's unbiased and gives you a representative slice, but it wastes budget on common easy cases. Stratified sampling divides traffic into meaningful groups (task types, risk tiers, user segments) and samples proportionally from each group, ensuring you have sufficient coverage of rare but important categories. In practice, I use both: random sampling for baseline trend tracking, and stratified sampling to ensure I get enough signal from high-stakes, low-volume categories like payment flows or enterprise customer interactions. Stratified sampling is especially critical when your traffic distribution is uneven.

---

**Q: How do you use model confidence scores to improve your sampling strategy?**

A: Uncertainty-based sampling is one of my highest-signal strategies. I sample heavily from interactions where the model had low confidence — these are cases most likely to be wrong. For example, if confidence is below 0.6, I might review 100% of those cases; 0.6-0.8 gets 10% sampling; above 0.8 gets 1%. This multiplies my error-detection rate by 10-12x compared to random sampling. The key is ensuring your confidence scores are calibrated — I validate on a random sample first to confirm that a 0.7 confidence score actually corresponds to roughly 70% accuracy. If scores are miscalibrated, I adjust thresholds accordingly.

---

**Q: What role does error-triggered sampling play, and how do you implement it?**

A: Error-triggered sampling is about reviewing every interaction where something already went wrong — system errors, user complaints, escalations, or rephrases. These are your highest-value learning opportunities because they represent user-facing failures. I typically set a 100% review rate for this category since volumes are small (usually 1-2% of traffic) and the signal is critical. Implementation involves instrumenting error detection: logging exceptions, timeouts, failed tool calls, negative feedback, and abandonment. These cases are auto-routed to a priority review queue, and findings feed directly into regression tests and bug fixes. This forces you to confront every failure rather than hiding from them.

---

**Q: How do you balance your review budget across different sampling strategies?**

A: I use a 30/30/20/10/10 split as a starting point: 30% random for baselines, 30% uncertainty-based for learning signal, 20% error-triggered for failures, 10% edge-case for outliers, and 10% adversarial for safety. Then I adjust based on product type — for high-stakes enterprise products, I increase error-triggered to 30% and adversarial to 15%, reducing random to 20%. For low-stakes consumer chatbots, I go heavier on random and uncertainty-based. The key is documenting your allocation, tracking which strategies yield the most actionable findings, and reallocating budget toward higher-signal strategies over time. If a strategy consistently finds issues that lead to fixes, give it more budget; if it's not actionable, reduce it.

---

Now that you know how to sample production traffic strategically, the next question is: what do you do with all the user feedback you're getting? Thumbs up, thumbs down, escalations, complaints — these are rich signals, but only if you systematically integrate them into your quality loop. Let's talk about user feedback integration and how to turn implicit signals into actionable improvements.

