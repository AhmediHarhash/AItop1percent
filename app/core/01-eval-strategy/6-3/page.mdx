# Chapter 6.3 — Calibration & Inter-Rater Agreement

**What we're doing here:**
Let me walk you through how to make sure your human raters are actually measuring the same thing — because if they're not, every number you get from them is meaningless.

You hired five smart people to rate your chatbot. You gave them a rubric. They scored 200 examples. You looked at the dashboard and saw an average quality score of 3.2 out of 5.

Great news, right?

Then you checked: the same example got a 5 from one rater and a 2 from another. The "average" was hiding the fact that nobody agreed on what "good" meant. You shipped the model anyway because the dashboard looked fine. Two weeks later, users complained about responses that half your raters loved and half your raters hated.

This is what happens when you skip **calibration** and ignore **inter-rater agreement**.

**The core truth:**
Human evaluation is only useful if humans agree with each other — and with your quality standards. Calibration is how you get them there. Agreement metrics are how you prove they stayed there.

**Enterprise goal:**
Build a human eval system where raters are consistent, repeatable, and auditable — so the numbers you get mean something real, not just averaged noise.

---

## 1) What Calibration Actually Means

**Calibration** is the process of getting your raters to agree on what "good" looks like before they start scoring real data.

It's not:
- sending raters a rubric and hoping they read it
- assuming smart people will naturally agree
- averaging out disagreements and calling it done

It is:
- walking through examples together
- discussing why you scored them the way you did
- aligning on edge cases and borderline calls
- updating the rubric when confusion appears
- creating a shared mental model of quality

Think of it like tuning musical instruments before a concert. You can have five talented violinists, but if their instruments are tuned differently, the music will sound terrible no matter how well they play.

**Mechanics:**
Calibration happens in sessions where raters:
1. Score the same set of examples independently
2. Compare their scores
3. Discuss disagreements
4. Update their understanding of the rubric
5. Re-score to check alignment

You repeat this until scores stabilize.

---

## 2) The Calibration Session (How to Run One)

Here's how a real calibration session works in practice.

### Step 1 — Prepare gold examples (before the session)

Create a set of **20–50 examples** that include:
- **Easy cases:** obvious good and bad (10–15 examples)
- **Normal cases:** typical production quality (10–15 examples)
- **Hard cases:** borderline calls, edge cases, ambiguity (5–10 examples)
- **Controversial cases:** examples where you expect disagreement (5–10 examples)

These should be real or realistic — not synthetic toy cases.

### Step 2 — Independent scoring (15–30 minutes)

Each rater scores all examples independently using the rubric. No discussion yet.

They record:
- scores per dimension
- overall score
- brief reason (1 sentence) for borderline calls

### Step 3 — Compare scores (5 minutes)

Pull the scores into a simple table:

| Example ID | Rater A | Rater B | Rater C | Agreement? |
|---|---:|---:|---:|---|
| EX-001 | 3 | 3 | 3 | Yes |
| EX-002 | 2 | 3 | 2 | Close |
| EX-003 | 1 | 4 | 2 | No |

Flag cases where:
- scores differ by 2+ points
- any rater marked it "hard to score"
- safety/correctness disagreements appear

### Step 4 — Discuss disagreements (20–40 minutes)

Go through flagged cases one by one.

For each:
- ask raters to explain their reasoning
- point to evidence in the rubric
- identify what's unclear
- agree on the "right" score (or acceptable range)
- update the rubric or add an anchor if needed

**Critical rule:**
Don't just vote and move on. The goal is to understand why people disagree, not just to pick a winner.

### Step 5 — Update rubric and anchors (10 minutes)

If disagreements came from:
- vague dimension wording → clarify it
- missing examples → add anchors
- edge case confusion → add a tie-break rule

Document changes in the rubric version log.

### Step 6 — Optional re-score (10 minutes)

If disagreements were large, have raters re-score the same examples with the updated rubric to confirm alignment improved.

### Cadence (how often to run calibration)

- **Week 1–4:** weekly (while ramping up)
- **Month 2–3:** every 2 weeks
- **After that:** monthly, or whenever:
  - a new rater joins
  - rubric changes
  - agreement metrics drop
  - a major policy update happens

---

## 3) Inter-Rater Agreement Metrics (Plain English)

Once raters have scored data, you need to measure how much they agree. This is where **inter-rater agreement metrics** come in.

There are four main ones used in production systems. Let me walk you through each in plain English — no formulas, just intuition.

---

### 3.1 Percent Agreement (Simple but Misleading)

**What it is:**
The percentage of cases where all raters gave the exact same score.

**Example:**
You have 100 examples. Raters A and B agreed on 70 of them.
Percent agreement = 70%.

**Why it's useful:**
It's easy to calculate and easy to explain.

**Why it's misleading:**
It doesn't account for **chance agreement** — the fact that raters might agree just by randomly picking the same score.

Imagine scoring with a 0–1 scale (pass/fail). If both raters just flip a coin, they'll agree 50% of the time by pure luck. A 60% agreement sounds okay, but it's only 10% better than random guessing.

**When to use it:**
As a quick sanity check. If percent agreement is below 60%, something is very wrong. But don't trust it as your only metric.

---

### 3.2 Cohen's Kappa (Accounts for Chance Agreement)

**What it is:**
A measure of agreement that corrects for the agreement you'd expect by random chance.

**Intuition:**
Imagine two raters scoring 100 examples on a 0–3 scale. They agree on 75 cases. Sounds good — but what if 60 of those agreements were just luck?

Cohen's Kappa asks:
"How much better than random chance are these raters?"

It adjusts the raw agreement by subtracting the "expected by chance" agreement.

**The scale:**
- **Below 0:** worse than random (raters are anti-correlated — very bad)
- **0.0–0.2:** slight agreement (barely better than chance)
- **0.2–0.4:** fair agreement
- **0.4–0.6:** moderate agreement
- **0.6–0.8:** substantial agreement (good)
- **0.8–1.0:** near-perfect agreement (excellent)

**Why it's better than percent agreement:**
It's harder to game. If raters just guess, Kappa will be near zero.

**When to use it:**
When you have **two raters** scoring the same set of examples on a categorical or ordinal scale (like 0–3 or pass/fail).

**Enterprise default:**
Aim for **Kappa ≥ 0.6** for production use. Below 0.4 means your rubric or raters need work.

---

### 3.3 Krippendorff's Alpha (Multiple Raters, Scales, and Missing Data)

**What it is:**
A more flexible version of Cohen's Kappa that works for:
- **3 or more raters** (not just 2)
- different types of scales (binary, ordinal, interval, ratio)
- missing data (not all raters scored all examples)

**Intuition:**
Cohen's Kappa only works for two raters. But in real systems, you often have 3–5 raters, and not everyone scores everything (some raters are out sick, some examples are assigned to subsets).

Krippendorff's Alpha handles all of this.

**The scale (same as Kappa):**
- **Below 0.4:** poor
- **0.4–0.6:** moderate
- **0.6–0.8:** good
- **Above 0.8:** excellent

**When to use it:**
When you have **3+ raters** or when raters don't all score the same set.

**Enterprise default:**
Aim for **Alpha ≥ 0.6** for launch. Below 0.4 is unacceptable for any Tier 2–3 task.

---

### 3.4 Weighted Kappa (For Ordinal Scales — Close Disagreements Are Less Bad)

**What it is:**
A version of Cohen's Kappa that treats near-misses differently from far-misses.

**Intuition:**
Imagine a 0–3 quality scale:
- Rater A scores an example as 2
- Rater B scores it as 3

That's a 1-point disagreement. It's not great, but it's much better than:
- Rater A: 0
- Rater B: 3

Both are "disagreements," but the second one is way worse. The raters aren't even in the same ballpark.

**Weighted Kappa** gives partial credit for close disagreements and heavier penalties for far disagreements.

**When to use it:**
When your scale is ordinal (0–3, 1–5) and you care about magnitude of disagreement, not just binary agree/disagree.

**Enterprise default:**
Use weighted Kappa when you're using scales like 0–3 or 1–5. Aim for **≥ 0.65**.

---

## 4) What Agreement Numbers Actually Mean (Reference Guide)

Here's a simple lookup table for interpreting agreement metrics in production:

| Kappa / Alpha | Interpretation | What to Do |
|---:|---|---|
| < 0.0 | Worse than random | Major rubric failure. Start over. |
| 0.0–0.2 | Slight | Rubric is too vague or raters untrained |
| 0.2–0.4 | Fair | Not usable for decisions. More calibration needed. |
| 0.4–0.6 | Moderate | Acceptable for early iteration, not for launch |
| 0.6–0.8 | Substantial | Good. Safe for production and release gates. |
| 0.8–1.0 | Near-perfect | Excellent. Rare except on very clear tasks. |

**Enterprise thresholds (common defaults):**
- **Tier 0–1 tasks:** Kappa ≥ 0.5 acceptable
- **Tier 2 tasks:** Kappa ≥ 0.6 required
- **Tier 3 tasks:** Kappa ≥ 0.7 required, with multi-rater adjudication

---

## 5) What to Do When Agreement Is Low

You ran calibration. You calculated Kappa. It's 0.38. Now what?

Low agreement has three root causes. Your job is to figure out which one.

---

### Cause 1 — Bad raters (wrong skills, no attention)

**Symptoms:**
- one rater consistently disagrees with everyone
- raters are rushing, skipping instructions
- raters don't understand the domain

**Debug steps:**
- check if one rater is an outlier (remove them and recalculate Kappa)
- review rater notes — are they thoughtful or empty?
- add an attention-check example (obvious fail case) — if raters miss it, they're not paying attention

**Fix:**
- replace low-quality raters
- add incentives for accuracy (bonus for matching gold labels)
- add feedback loops: show raters where they disagreed and why

---

### Cause 2 — Bad rubric (too vague, missing anchors)

**Symptoms:**
- all raters are smart and trying hard, but still disagree
- disagreements cluster around specific dimensions or edge cases
- raters often write "unclear what to score here"

**Debug steps:**
- look at high-disagreement cases — what's ambiguous?
- check if dimension names are vague ("helpful," "good")
- check if anchors exist for edge cases

**Fix:**
- rewrite vague dimensions with observable criteria
- add 5–10 anchored examples per dimension
- add tie-break rules for borderline cases
- run a new calibration session with updated rubric

---

### Cause 3 — Genuinely ambiguous task (multiple valid answers)

**Symptoms:**
- rubric is clear, raters are good, but disagreement persists
- the task allows multiple correct answers (creative writing, tone, style)
- domain experts also disagree

**What this means:**
The task itself is subjective. Low agreement isn't a bug — it's reality.

**Fix:**
- accept the ambiguity
- report disagreement rate alongside scores
- use a "must-include" checklist instead of expecting exact scores
- switch to a behavior-based rubric (did it ask? did it refuse? did it cite?) instead of quality-based

**When to accept low agreement:**
- creative tasks (writing, design, tone)
- subjective user preference (humor, friendliness)
- tasks where domain experts also can't agree

In these cases, aim for **moderate agreement (0.4–0.6)** and report score distributions, not just averages.

---

## 6) Ongoing Calibration (Not Just Once)

Calibration isn't a one-time event. Raters drift over time.

**Why drift happens:**
- raters get tired or bored
- new edge cases appear that weren't in the original rubric
- raters start applying personal preferences
- rubric gets updated but raters miss the change
- team turnover (new raters join)

**How to prevent drift:**

### 6.1 Periodic recalibration sessions
- monthly or quarterly
- use a fixed "benchmark set" of 20–50 examples
- track agreement over time

### 6.2 Spot-check with gold examples
- inject 5–10 gold examples into every batch
- compare rater scores to gold labels
- flag raters who drift below threshold

### 6.3 Rater feedback loops
- after each batch, show raters:
  - cases where they disagreed with others
  - cases where they missed gold labels
  - updated rubric sections

### 6.4 Version control for rubrics
- every time you update the rubric, tag it (v1.0, v1.1, v2.0)
- record which version was used for each batch
- re-calibrate raters when major changes happen

---

## 7) Calibration with Gold Examples (Accuracy, Not Just Agreement)

So far we've talked about **agreement** — do raters agree with each other?

But there's another question: **accuracy** — do raters agree with the correct answer?

You can have high agreement and still be wrong. Imagine all your raters consistently score a bad response as "good" because they misunderstood the rubric. Kappa will be high, but your evals are useless.

**Solution: gold examples**

A **gold example** is a pre-labeled case where you've locked the "correct" score with high confidence (usually via expert adjudication or policy review).

**How to use gold examples for calibration:**

### Step 1 — Build a gold set
- 50–200 examples
- cover easy, normal, hard, and edge cases
- include must-abstain, must-refuse, and safety-critical cases
- lock labels via expert review or multi-rater adjudication

### Step 2 — Measure rater accuracy
For each rater, calculate:
- **percent match with gold:** how often they match the gold label exactly
- **average distance from gold:** average score difference (for ordinal scales)

### Step 3 — Set accuracy thresholds
- **≥ 80% match:** good
- **70–80%:** acceptable, needs coaching
- **< 70%:** replace or retrain

### Step 4 — Use gold examples for ongoing quality control
- inject 10% gold examples into every rater's batch
- flag raters who drop below threshold
- use this as a performance metric (not just agreement)

**Enterprise pattern:**
Combine agreement (Kappa) with accuracy (gold match rate). Both must be high.

---

## 8) Resolving Disagreements (Adjudication Workflows)

Even after calibration, raters will sometimes disagree on hard cases. You need a process to resolve it.

Here are the three main patterns:

---

### Pattern 1 — Majority vote (fast, risky)

**How it works:**
3 or 5 raters score each example. The majority score wins.

**When to use it:**
- Tier 0–1 tasks
- low-risk decisions
- you need speed over perfection

**Risks:**
- majority can be wrong (groupthink, shared misconception)
- doesn't work if there's no majority (3 different scores)

**Mitigation:**
- flag cases with no majority for manual review
- require evidence for Tier 2–3 decisions

---

### Pattern 2 — Expert tiebreak (better quality)

**How it works:**
2 raters score independently. If they disagree beyond a threshold (e.g., 2+ points apart), escalate to a senior reviewer or domain expert.

**When to use it:**
- Tier 2–3 tasks
- safety/policy decisions
- expensive to get wrong

**Pros:**
- higher quality final labels
- domain expertise applied to hard cases

**Cons:**
- slower
- expert becomes a bottleneck

**Mitigation:**
- only escalate large disagreements (not all disagreements)
- train experts to update rubric when confusion appears

---

### Pattern 3 — Discussion-based consensus (highest quality)

**How it works:**
Raters who disagree meet, discuss the case, review evidence, and come to consensus.

**When to use it:**
- building gold sets
- major policy-sensitive cases
- calibration sessions

**Pros:**
- rubric improvements happen naturally
- shared understanding deepens

**Cons:**
- very slow
- doesn't scale to thousands of examples

**Best practice:**
Use this for your gold set and calibration examples, not for every production batch.

---

## 9) The 2026 Angle: LLM-as-Judge as a Calibration Baseline

Here's a modern twist: in 2026, many teams use **LLM-as-Judge** not just for automated eval, but as a **calibration baseline** for human raters.

**How it works:**

### Step 1 — Run LLM-as-Judge on your calibration set
Use a strong model (GPT-4, Claude Opus) to score the same examples your raters will score.

### Step 2 — Measure human-LLM agreement
Calculate Kappa between:
- Human Rater A vs LLM
- Human Rater B vs LLM
- Humans vs Humans

### Step 3 — Identify patterns
If humans agree with each other but disagree with the LLM:
- the LLM might be wrong (common for domain-specific tasks)
- or humans are applying personal preferences

If humans disagree with each other AND with the LLM:
- rubric is probably too vague
- task is genuinely ambiguous

If one human agrees with the LLM but others don't:
- that human might be the most accurate
- or the LLM is overfitting to a certain style

### Step 4 — Use LLM as a "tie-break baseline"
For borderline cases, check:
- what did the LLM score?
- what was its reasoning?
- does it align with the rubric?

This doesn't mean the LLM is always right — but it gives you a third perspective that's consistent and auditable.

**Enterprise best practice:**
Treat LLM-as-Judge like another rater. Track its agreement with humans. Use it to catch drift and calibrate edge cases. But don't blindly trust it — especially on Tier 2–3 tasks where domain expertise matters.

---

## 10) When Low Agreement Is Acceptable (Some Tasks Are Just Subjective)

Not every task needs Kappa ≥ 0.7.

Some tasks are genuinely subjective, and forcing consensus would distort the truth.

**Examples:**
- "Is this response friendly?"
- "Would you personally prefer answer A or B?"
- "Rate the creativity of this ad copy."

For these tasks:
- moderate agreement (0.4–0.6) is realistic
- you should report disagreement, not hide it
- use score distributions (quartiles, variance) instead of just means

**What to do:**
- switch from "one correct score" to "acceptable range"
- use a "must-include" checklist for objective criteria
- report both average score and disagreement rate
- consider scoring behavior (did it follow the instruction?) instead of subjective quality

**Enterprise truth:**
On subjective tasks, transparency about disagreement is more valuable than fake precision.

---

## 11) Knobs & Defaults (What You Actually Set)

Here's a reference for practical calibration and agreement settings:

### 11.1 Calibration cadence
- **Week 1–4:** weekly sessions
- **Month 2–3:** every 2 weeks
- **Ongoing:** monthly + whenever rubric changes or new rater joins

### 11.2 Gold set size
- **Small team:** 50–100 examples
- **Mid team:** 200–500
- **Enterprise:** 500–2,000 (across tasks and slices)

### 11.3 Agreement thresholds
- **Tier 0–1:** Kappa ≥ 0.5
- **Tier 2:** Kappa ≥ 0.6
- **Tier 3:** Kappa ≥ 0.7

### 11.4 Adjudication triggers
Escalate when:
- scores differ by 2+ points (on 0–3 scale)
- safety gate disagrees (one says safe, one says unsafe)
- "abstain vs answer" disputes (critical in RAG)

### 11.5 Accuracy thresholds (gold match rate)
- **≥ 80%:** good
- **70–80%:** coaching needed
- **< 70%:** replace or intensive retraining

---

## 12) Failure Modes (Symptoms → Root Causes)

### 12.1 High agreement, wrong answers
**Symptoms:**
- Kappa is 0.8+, but examples are clearly mislabeled
- all raters consistently make the same mistake

**Root causes:**
- rubric rewards the wrong thing (style over correctness)
- shared misconception from training
- no gold set to validate accuracy

**Fix:**
- build a gold set with expert labels
- measure accuracy, not just agreement
- add adversarial cases where the obvious answer is wrong

---

### 12.2 Low agreement despite calibration
**Symptoms:**
- ran 3 calibration sessions
- raters are smart and engaged
- Kappa still below 0.5

**Root causes:**
- task is genuinely ambiguous
- rubric dimensions are vague or overlapping
- missing edge case anchors

**Fix:**
- switch to behavior-based scoring (ask/abstain/refuse)
- use must-include checklists instead of holistic scores
- accept moderate agreement and report disagreement rate

---

### 12.3 Agreement drops over time (rater drift)
**Symptoms:**
- Kappa was 0.7 in month 1, now it's 0.5
- scores on benchmark set are changing

**Root causes:**
- no recalibration
- raters bored or tired
- new edge cases not added to rubric

**Fix:**
- monthly recalibration sessions
- inject gold examples into every batch
- update rubric when new edge cases appear

---

### 12.4 One rater is always an outlier
**Symptoms:**
- four raters agree, one always diverges
- removing that rater's data raises Kappa significantly

**Root causes:**
- rater is undertrained or misunderstands task
- rater is applying personal standards
- rater has domain expertise others lack (and might be right)

**Fix:**
- review outlier's notes — are they thoughtful or confused?
- check if they're right and others are wrong (compare to gold set)
- retrain or replace if consistently inaccurate

---

## 13) Enterprise Expectations (What Serious Teams Do)

- They track **both agreement and accuracy** — Kappa measures consistency, gold match rate measures correctness
- They run **monthly calibration sessions** even after raters are trained
- They maintain **versioned gold sets** tied to policy and rubric versions
- They **inject gold examples** into every rater batch for quality control
- They escalate disagreements on Tier 2–3 tasks to domain experts, not just majority vote
- They report **disagreement rates** alongside scores for subjective tasks
- They use **LLM-as-Judge** as a third perspective to catch drift
- They treat calibration as a continuous process, not a one-time event

---
