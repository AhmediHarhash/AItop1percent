# 9.4 — Attribution & Citation Accuracy

A junior analyst at a pharmaceutical company asked the company's RAG system a question about drug interactions. The system returned a confident answer with three citations to published research papers. The analyst included the answer in a regulatory filing. Six months later, during an audit, a reviewer tried to verify the citations. Two of the three papers didn't exist. The third paper existed but said the opposite of what the system claimed. The company faced a multi-million dollar fine, the filing was rejected, and the RAG system was shut down for six months.

The model's answer was technically correct. The information existed somewhere in the knowledge base. But the **attribution** was completely wrong. In high-stakes environments, "the model said so" isn't good enough. Users need to verify answers. Auditors need to trace claims back to authoritative sources. Regulators need to see the provenance of every assertion.

Attribution accuracy is the discipline of ensuring that every claim in a generated response points to the correct source and that every source cited actually supports the claim it's attached to. It's the difference between a useful research assistant and a liability generator.

---

## Why Attribution Matters — The Trust Layer

When you ask a human expert a question, you can probe: "How do you know that?" A good expert points to evidence. A RAG system should do the same.

**Attribution** is the practice of linking every claim in a generated response to the source document that supports it. Without attribution, users have three bad options: blind trust, manual re-research, or ignoring the answer entirely.

With proper attribution, users can verify answers, auditors can trace decisions, and compliance teams can demonstrate due diligence. Attribution transforms a black box into an auditable process.

In regulated industries—healthcare, finance, legal—attribution isn't a nice-to-have. It's a legal requirement. The EU AI Act and similar regulations increasingly mandate transparency and traceability for high-risk AI systems. If your system makes a claim, you must be able to show where it came from.

Even in lower-stakes environments, attribution builds trust. A customer support agent who can say "According to our Q3 policy update, section 2.4" sounds more credible than one who just states facts without sources.

**Citation accuracy** is the measurable quality of those attributions. Are the citations correct? Do they actually support the claims? Are all claims cited?

---

## Citation Precision — No Fake Citations

**Citation precision** measures whether every citation in the response points to a source that actually supports the claim.

The failure mode is simple: the model invents plausible-sounding citations. It says "According to Smith et al. (2023)" when no such paper exists. Or it cites a real paper that doesn't say what the model claims it says.

This happens because language models are trained to predict text, not to verify facts. If the training data contains citation patterns, the model learns to generate citation-like strings. But those strings aren't grounded in the retrieved documents unless you explicitly enforce grounding.

**How to evaluate citation precision:**

Manual verification: A human evaluator reads each citation and checks whether the source actually supports the claim. This is the gold standard but doesn't scale.

LLM-based verification: A second LLM reads the claim and the cited source and judges whether the source supports the claim. Prompt: "Does the following passage support the claim? Passage: [source_text]. Claim: [claim]. Answer: Yes/No/Partial."

Automated string matching: Check whether key phrases from the claim appear in the cited source. Limited but fast. Works well for detecting completely fabricated citations.

**Precision formula:**

```
Citation Precision = (Number of correct citations) / (Total number of citations)
```

A correct citation is one where the cited source actually contains information that supports the claim.

**Example failure:**

Claim: "The Q4 revenue target is 50 million dollars (Internal Memo, Dec 2023)."

Cited source: Internal Memo, Dec 2023 says "We exceeded our Q3 target of 40 million."

The citation exists, but it doesn't support the claim. Precision fails.

---

## Citation Recall — No Uncited Borrowed Information

**Citation recall** measures whether every claim that comes from a source has a citation.

The failure mode: the model paraphrases information from retrieved documents but doesn't cite them. The answer is factually correct and grounded in the knowledge base, but there's no attribution. Users can't verify the claim.

This is particularly problematic when the model synthesizes information from multiple documents. It might cite one source but quietly borrow facts from three others.

**How to evaluate citation recall:**

Manual annotation: A human reads the response and the retrieved documents, then marks which claims come from which sources. Compare the human annotations to the model's citations.

Automated claim extraction: Use an LLM to extract factual claims from the response. For each claim, check whether there's a corresponding citation. Then verify whether the claim appears in the retrieved documents.

**Recall formula:**

```
Citation Recall = (Number of cited claims from sources) / (Total number of claims from sources)
```

A claim "from sources" is any statement that isn't common knowledge and appears in the retrieved documents.

**Example failure:**

Response: "The company was founded in 1998 in Seattle. Our current CEO is Jane Smith. We have 12,000 employees."

Retrieved documents contain all three facts. The model cites the source for the founding date but not for the CEO or employee count. Recall is 33%.

---

## Citation Granularity — The Right Paragraph, Not Just the Right Document

Document-level citations are easy: "Source: Q4_report.pdf." But they're not very useful. A 50-page PDF has thousands of claims. Which one supports your assertion?

**Granular citations** point to specific sections, paragraphs, or sentences. They're harder to generate but far more useful.

**Granularity levels:**

- Document-level: "Source: Annual Report 2023"
- Section-level: "Source: Annual Report 2023, Section 3.2"
- Paragraph-level: "Source: Annual Report 2023, Section 3.2, Paragraph 4"
- Sentence-level: "Source: Annual Report 2023, Page 12, Line 7"

The right granularity depends on context. For a legal contract, sentence-level citations might be necessary. For a customer support answer, section-level is usually enough.

**Evaluating granularity:**

Time-to-verify: How long does it take a human to verify the claim using the citation? Granular citations should reduce verification time.

User satisfaction: Ask users whether the citations were specific enough to be useful.

---

## Source Verification — Hallucinated Citations

The most dangerous failure mode: the model invents a citation that doesn't exist.

User: "What's the refund policy?"

Model: "According to the Employee Handbook, Section 4.7, refunds are issued within 14 days."

Problem: There is no Section 4.7. Or there is, but it's about vacation policy, not refunds.

This happens because the model has learned citation patterns from training data. It knows that authoritative answers often include citations. So it generates citation-like strings even when it doesn't have a real source.

**How to detect hallucinated citations:**

Source lookup: After generating a citation, verify that the cited source exists in the knowledge base and that the cited section/page/paragraph is real.

Cross-check content: Extract the text from the cited location and verify it matches the claim.

**Automated verification pipeline:**

1. Parse citations from the response (regex or structured output)
2. Look up each cited source in the knowledge base
3. Extract the cited passage
4. Use an LLM or semantic similarity to check whether the passage supports the claim
5. Flag mismatches for human review

In 2026, **automated attribution verification** is a standard component of production RAG systems. Tools like RAGAS, TruLens, and LangSmith include built-in citation verification metrics.

---

## The "Close Enough" Problem — Paraphrasing and Attribution

The model rarely quotes sources verbatim. It paraphrases, summarizes, and rephrases.

Source: "Our quarterly revenue increased by 23% compared to the same period last year."

Model: "The company saw a 23% year-over-year revenue boost in Q3."

Is this correctly attributed? Yes. The claim is the same, even though the wording is different.

Now a harder case:

Source: "Revenue grew significantly in Q3."

Model: "The company saw a 23% year-over-year revenue boost in Q3."

The model added a specific number not present in the source. If the source is cited, the attribution is incorrect.

**Evaluating paraphrased attribution:**

Semantic similarity: Use embeddings to measure whether the claim and the source passage are semantically equivalent.

Entailment checking: Use an NLI (natural language inference) model to check whether the source entails the claim. If the source entails the claim, the attribution is correct.

Human judgment: For edge cases, manual review is the only reliable method.

**Threshold tuning:** You need to decide how similar is "similar enough." Too strict, and you flag correct attributions as errors. Too loose, and you miss subtle misattributions.

---

## Multi-Source Attribution — Synthesized Answers

A powerful RAG system doesn't just retrieve and regurgitate. It synthesizes information from multiple sources.

User: "Compare our Q3 and Q4 performance."

The model retrieves the Q3 report and the Q4 report, then generates:

"Q3 revenue was 40 million (Q3 Report, Section 2). Q4 revenue was 50 million (Q4 Report, Section 2). This represents a 25% increase."

**Challenge:** Each part of the answer needs a separate citation. The model must track which source supports which claim.

**Evaluation approach:**

Claim-level annotation: Break the response into individual claims. For each claim, identify the supporting source. Check whether the model cited the correct source for each claim.

**Multi-source precision and recall:** Calculate precision and recall at the claim level, not the response level.

**Common failure:** The model cites only one source when the answer draws from three. Or it cites all three sources at the end, without specifying which source supports which claim.

**Best practice:** Inline citations with claim-specific attribution. "Q3 revenue was 40 million [1]. Q4 revenue was 50 million [2]. This represents a 25% increase [calculated]."

---

## Attribution Formats — Inline, Footnotes, Source Lists

Different use cases call for different citation formats.

**Inline citations:** "The policy (Section 4.2) states that refunds are processed within 14 days."

Pros: Clear, immediate, easy to verify.
Cons: Can clutter the text.

**Footnotes:** "The policy states that refunds are processed within 14 days. [1]"

[1] Customer Service Policy, Section 4.2

Pros: Cleaner text, detailed citations at the end.
Cons: Requires the user to scroll or click.

**Source lists:** "Answer: Refunds are processed within 14 days. Sources: Customer Service Policy (Section 4.2), Employee Handbook (Page 17)."

Pros: Comprehensive, shows all sources used.
Cons: Doesn't specify which source supports which claim.

**Confidence indicators:** "Refunds are processed within 14 days (Confidence: High, Source: Customer Service Policy, Section 4.2)."

Pros: Adds transparency, helps users judge reliability.
Cons: Requires confidence estimation, which is hard to calibrate.

**Evaluating citation format:**

User testing: Which format do users find most useful? Measure time-to-verify, user satisfaction, trust ratings.

Compliance requirements: Legal and regulatory contexts often mandate specific citation formats.

---

## Legal and Compliance Implications — High-Stakes Attribution

In regulated industries, incorrect attribution can have serious consequences.

**Healthcare:** A clinical decision support system cites a study that doesn't support the recommendation. A doctor relies on the citation, makes a treatment decision, and the patient is harmed. The health system faces a malpractice lawsuit.

**Legal:** A legal research assistant cites a case that was overturned. A lawyer includes the citation in a brief. The opposing counsel points out the error. The lawyer's credibility is damaged, and the case is weakened.

**Finance:** An investment research tool attributes a claim to a regulatory filing that doesn't contain that information. The firm makes an investment decision based on the false attribution. The SEC investigates.

In these contexts, **attribution accuracy is a hard requirement**, not a quality-of-life improvement.

**Compliance standards:**

FDA 21 CFR Part 11: Electronic records must be traceable and auditable. If an AI system generates a recommendation, you must be able to show the source.

GDPR Article 22: Automated decision-making requires transparency. Users have a right to understand how decisions are made.

EU AI Act (2024-2026 rollout): High-risk AI systems must provide transparency, traceability, and human oversight. Attribution is a key component of transparency.

**Enterprise expectations:**

Audit trails: Every response should log which sources were retrieved, which were cited, and how they were used.

Version control: If a source document changes, attribution must update accordingly. Citing "Q3 Report" is ambiguous if there are multiple versions.

Human review: High-stakes outputs should include a human verification step where a domain expert checks citations before the response is used.

---

## 2026 Patterns — Automated Attribution Verification

**Automated attribution verification pipelines:** Modern RAG systems include automated checks that run after every response is generated. The pipeline extracts claims, verifies citations, and flags potential errors before the response reaches the user.

**Source-grounding scores in RAGAS and TruLens:** These tools compute a **faithfulness score** that measures whether the response is grounded in the retrieved documents. They also provide **citation precision** and **citation recall** metrics.

Example RAGAS output:

```
Faithfulness: 0.92
Citation Precision: 0.88
Citation Recall: 0.85
Hallucinated Citations: 1
```

**Provenance tracking:** Each generated claim is tagged with metadata: source document ID, section, paragraph, retrieval timestamp, model version. This metadata is logged and stored for audit purposes.

**Confidence-weighted attribution:** The model assigns a confidence score to each citation. Low-confidence citations are flagged for human review.

**Citation correction loops:** If the automated verification pipeline detects a bad citation, it triggers a re-generation step with additional constraints: "You previously cited Source X for Claim Y, but Source X does not support Claim Y. Revise your response."

**EU AI Act transparency requirements:** For high-risk systems, attribution logs must be retained and made available to users and regulators upon request. This drives demand for automated, scalable attribution verification.

---

## Failure Modes — What Goes Wrong

**Phantom citations:** The model cites a source that doesn't exist. Often happens with academic papers, legal cases, or internal documents.

**Wrong document:** The model cites Document A when the information actually comes from Document B. Common when multiple documents discuss similar topics.

**Wrong section:** The model cites the correct document but the wrong section or page. The information is there, but the user can't find it.

**Over-citing:** The model cites every sentence, even common knowledge. The response becomes cluttered and hard to read.

**Under-citing:** The model only cites the first claim but synthesizes information from five sources. Users can't verify the rest.

**Ambiguous citations:** "Source: Internal Memo." Which memo? When? By whom?

**Outdated citations:** The model cites the 2022 version of a policy document when the 2023 version has different information.

**Synthesized claims without attribution:** The model makes an inference based on multiple sources but doesn't cite any of them. Example: "Based on trends in Q3 and Q4, Q1 will likely exceed 60 million." This is a prediction, not a cited fact.

---

## Enterprise Expectations — Attribution as a Product Requirement

In 2026, **citation accuracy is a tier-one product requirement** for enterprise RAG systems. Procurement teams ask for attribution SLAs. Compliance teams require attribution audits. Users expect citations as part of the answer, not an optional add-on.

**What enterprises expect:**

**100% citation coverage:** Every factual claim has a citation. No uncited assertions.

**Granular citations:** Not just document-level, but section or paragraph-level.

**Verification UI:** Users should be able to click a citation and see the original source text highlighted in context.

**Audit logs:** Every response, every citation, every source retrieval logged and stored.

**Automated quality gates:** Responses with low attribution scores are flagged or blocked before delivery.

**Human-in-the-loop for high-stakes:** Critical decisions require a human to verify citations.

**Citation testing in pre-deployment:** Before rolling out a new model or prompt, run attribution benchmarks on a test set.

---

## Evaluation Template — Attribution Testing

```yaml
attribution_eval:
  dataset: "./data/attribution_test_set.jsonl"
  # Each example: query, ground_truth_answer, ground_truth_citations

  metrics:
    - citation_precision  # Are cited sources correct?
    - citation_recall     # Are all claims cited?
    - hallucinated_citations  # Fake sources?
    - granularity_score   # Paragraph-level or document-level?

  automated_checks:
    - source_exists: true  # Verify cited sources exist
    - passage_extraction: true  # Extract cited passages
    - semantic_match: 0.8  # Min similarity between claim and source
    - entailment_check: true  # NLI model verification

  human_review:
    sample_size: 50
    reviewers: 2
    inter_rater_agreement_threshold: 0.85

  quality_gates:
    min_precision: 0.95
    min_recall: 0.90
    max_hallucinations: 0
```

---
