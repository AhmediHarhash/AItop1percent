# Chapter 5.5 — Privacy, Sanitization & Data Governance

**What we're doing here:**
Privacy in evaluation is not just a legal checkbox.
It's a technical risk that affects dataset quality, system bias, and regulatory compliance.

When you build eval datasets from production logs, expert sessions, or third-party data, you inherit **personally identifiable information (PII)**, behavioral traces, and re-identification risk.
If you don't handle this properly, you create liability for your company, bias in your models, and compliance violations that can block product launch.

This chapter covers the practical mechanics of **sanitization, access control, retention policies, and compliance frameworks** — the infrastructure required to use real data safely in 2026 enterprise AI evaluation.

**Enterprise outcome:**
A governed pipeline where evaluation datasets are sanitized, access-controlled, audited, and compliant with GDPR, CCPA, HIPAA, SOC 2, and EU AI Act requirements — letting you use production data without creating legal or ethical risk.

---

## 1) Why privacy is an evaluation problem (not just a legal problem)

Most teams treat privacy as something the legal team handles.
That's wrong in three ways.

### 1.1 Privacy failures create liability
If PII leaks into eval reports, demo outputs, or shared datasets:
- You've violated data protection laws (GDPR, CCPA, HIPAA)
- You've exposed customer trust to breach
- You've created audit trail problems for SOC 2, ISO 27001, and EU AI Act

**2026 reality:** Regulators increasingly treat eval datasets as "processing" under GDPR. If your eval set contains production data with PII, you need legal basis (consent or legitimate interest), data minimization, and retention limits.

### 1.2 Privacy failures create bias
PII is not random. It correlates with protected attributes.

If your eval set includes:
- names (ethnicity signals)
- zip codes (income/race proxies)
- medical history (disability/health status)

...then your model may overfit to those correlations. Sanitization isn't just privacy — it's fairness engineering.

### 1.3 Privacy failures create compliance blockers
In 2026, enterprise procurement requires:
- SOC 2 Type II (with data handling audits)
- GDPR Article 30 processing records
- EU AI Act conformity assessments (for high-risk systems)

If your evaluation pipeline can't prove data governance, you can't sell to regulated customers.

---

## 2) What PII looks like in evaluation datasets (channel-specific)

### 2.1 Direct identifiers (obvious PII)
- **Names:** "Hi, I'm Sarah Johnson, how do I reset my password?"
- **Email addresses:** "Send the receipt to sarah.j@company.com"
- **Phone numbers:** "My number is 555-0123"
- **Government IDs:** SSN, passport numbers, driver's license
- **Financial data:** credit card numbers, account numbers
- **Health data:** diagnoses, medications, lab results (HIPAA-regulated)

### 2.2 Quasi-identifiers (re-identification risk)
Combinations of seemingly non-PII fields that uniquely identify individuals:
- **Job title + location + date:** "VP of Engineering in Boulder, started Jan 2025" → often unique
- **Rare interests:** "I teach underwater basket weaving in rural Montana"
- **Behavioral patterns:** user who always queries at 3am, specific multi-turn sequences

**2026 research note:** Re-identification attacks succeed on 80%+ of "anonymized" datasets with 3+ quasi-identifiers (Sweeney, Narayanan, etc.). Simple redaction isn't enough.

### 2.3 Behavioral traces (sequential PII)
- **Session sequences:** multi-turn conversations reveal identity over time
- **Tool call parameters (agents):** database queries, API calls with customer IDs
- **Voice biometrics (voice evals):** voiceprints are biometric PII under GDPR and BIPA (Illinois)

### 2.4 Channel-specific PII patterns

**Chat:**
- User mentions names, contact info in free-text queries
- Multi-turn dialogs accumulate identity signals

**RAG:**
- Retrieved documents contain PII (contracts, emails, customer records)
- Queries reveal internal knowledge ("What did we agree with Acme Corp in the May contract?")

**Agents:**
- Tool parameters: `lookup_customer(email="pii@example.com")`
- Logs of actions taken on behalf of users

**Voice:**
- Audio contains voiceprints (biometric PII)
- Transcripts contain direct PII plus speech patterns
- Background audio may capture third parties

---

## 3) The sanitization pipeline (detection → redaction → validation)

### 3.1 Step 1: Detection (finding PII)

**Three detection methods in 2026:**

#### 3.1.1 Named Entity Recognition (NER)
- Use fine-tuned models (spaCy, Flair, commercial APIs like AWS Comprehend, Google DLP)
- Detects: names, locations, organizations, dates
- **Strengths:** fast, good at common entities
- **Weaknesses:** misses context-specific PII, struggles with slang/typos

#### 3.1.2 Regex patterns
- Pattern-match for structured PII: emails, phones, SSNs, credit cards
- Example patterns:
  ```
  email: \b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b
  phone: \b\d{3}[-.]?\d{3}[-.]?\d{4}\b
  SSN: \b\d{3}-\d{2}-\d{4}\b
  ```
- **Strengths:** precise for known formats
- **Weaknesses:** misses obfuscated variants ("five five five, one two three four")

#### 3.1.3 LLM-assisted scanning (2026 standard)
- Pass text through an LLM with prompt: "Identify all PII in this text. Return: type, span, confidence."
- **Strengths:** catches context-dependent PII (nicknames, implied identifiers), handles natural language variations
- **Weaknesses:** slower, requires model inference, can hallucinate false positives

**Practical default: combine all three**
- Regex for structured PII (high precision)
- NER for entity-level PII (speed + recall)
- LLM scan for ambiguous/context cases (catch edge cases)

### 3.2 Step 2: Redaction strategy (what to do with detected PII)

**Three approaches, choose based on use case:**

#### 3.2.1 Redaction (removal)
Replace PII with placeholder: `"Hi, I'm [NAME]"` or `"Email me at [EMAIL]"`

**Pros:** safe, simple
**Cons:** loses signal for multi-turn consistency, breaks some reasoning patterns

**Use when:** safety is paramount, no need for semantic preservation

#### 3.2.2 Replacement (synthetic substitution)
Replace PII with realistic fake values: `"Hi, I'm Jane Doe"` (where original was "Sarah Johnson")

**Pros:** preserves sentence structure, maintains realism
**Cons:** requires careful generation to avoid re-introducing bias (don't always replace Hispanic names with Anglo names)

**Use when:** you need realistic text for model training/testing

#### 3.2.3 Generalization (abstraction)
Replace specific values with general categories: `"30-year-old from [CITY_100K_250K]"` instead of "Boulder"

**Pros:** reduces re-identification risk while preserving distributional signal
**Cons:** lossy, requires careful bucketing

**Use when:** you need aggregate patterns but not individual specifics

**2026 best practice:** Use replacement for most eval datasets (preserves readability + realism), with generalization for high-risk quasi-identifiers.

### 3.3 Step 3: Validation (spot-check redacted output)

Sanitization is fallible. Always validate.

**Validation checklist:**
1. **Automated scan:** re-run PII detection on sanitized output (should find zero direct identifiers)
2. **Human spot-check:** sample 1–2% of sanitized examples, manual review for missed PII
3. **Re-identification test:** can a human reviewer identify the original user from the sanitized version? If yes, over-sanitize.
4. **Utility check:** does the sanitized text still make sense for evaluation? If no, adjust redaction strategy.

**Frequency:**
- First sanitization: 100% validation on sample dataset
- Production pipeline: 1–2% ongoing spot-check + quarterly full audit

---

## 4) Compliance frameworks (2026 regulatory landscape)

### 4.1 GDPR (EU General Data Protection Regulation)
**Applies to:** any company processing EU residents' data

**Key requirements for eval datasets:**
- **Legal basis:** consent or legitimate interest (evaluation typically falls under legitimate interest, but document it)
- **Data minimization:** only collect/retain PII necessary for evaluation
- **Purpose limitation:** don't repurpose production logs for unrelated eval without new legal basis
- **Right to erasure:** if user requests deletion, you must purge their data from eval sets (requires data lineage tracking)
- **Retention limits:** specify and enforce max retention periods

**Practical enforcement:**
- Pseudonymization or anonymization is a "safeguard" but doesn't exempt you from GDPR
- Eval datasets are "processing" — you need Article 30 records

### 4.2 CCPA / CPRA (California Consumer Privacy Act + amendment)
**Applies to:** companies serving California residents

**Key requirements:**
- Right to know what data is collected
- Right to delete
- Right to opt out of "sale" (broad definition, may include some data sharing)

**Practical enforcement:**
- If your eval dataset contains California users' production logs, track lineage and support deletion requests

### 4.3 HIPAA (Health Insurance Portability and Accountability Act)
**Applies to:** healthcare providers, payers, and their vendors (BAAs)

**Key requirements:**
- Protected Health Information (PHI) includes: names, dates, addresses, medical record numbers, diagnoses, treatments
- **Safe Harbor de-identification:** remove 18 identifier types (names, dates, zip codes, etc.) + no re-identification risk
- **Expert determination de-identification:** statistical proof that re-identification risk is very low

**Practical enforcement:**
- If you're building healthcare AI, your eval pipeline must meet HIPAA Safe Harbor or Expert Determination
- Business Associate Agreements (BAAs) required for third-party eval vendors

### 4.4 SOC 2 Type II
**Applies to:** SaaS companies selling to enterprises

**Key requirements:**
- Access controls (who can see raw vs sanitized data)
- Audit logs (track who accessed eval datasets when)
- Data retention policies (document and enforce)
- Incident response (what happens if PII leaks)

**Practical enforcement:**
- Your SOC 2 audit will review eval dataset governance as part of "data security" controls
- Expect auditors to spot-check: access logs, retention enforcement, sanitization validation

### 4.5 EU AI Act (in effect 2026)
**Applies to:** high-risk AI systems (employment, credit, law enforcement, critical infrastructure, etc.)

**Key requirements for evaluation:**
- **Training data governance:** Article 10 requires datasets be "relevant, representative, and free of errors"
- **Logging and traceability:** evaluation results must be logged and auditable
- **Human oversight:** high-risk systems need human review of evaluation results before deployment
- **Conformity assessment:** some systems require third-party audit of evaluation process

**Practical enforcement:**
- If your AI system is "high-risk," your eval pipeline itself becomes a compliance artifact
- Expect regulators to ask: "How do you ensure eval datasets don't contain bias or PII?"

### 4.6 Consent models (when you need explicit consent)

**You typically need consent if:**
- Eval dataset includes data from minors
- Data is highly sensitive (health, biometrics, financial)
- You're repurposing data beyond original collection purpose

**Consent checklist:**
- Specific (not blanket "we use your data")
- Informed (user knows it's for evaluation)
- Freely given (not coerced)
- Revocable (user can withdraw)

**2026 default:** Most enterprise eval pipelines rely on **legitimate interest** (not consent), but document your legal basis in writing and get it reviewed by legal counsel.

---

## 5) Access control (who can see what)

### 5.1 Least-privilege principle
Default rule: no one sees raw production logs unless they need to.

**Access tiers:**

| Role | Raw logs | Sanitized eval sets | Gold sets (long-lived) | Reports (aggregates) |
|---|---|---|---|---|
| Data annotators | No | Yes (sanitized) | Yes | No |
| Eval engineers | No | Yes (sanitized) | Yes | Yes |
| Data engineers (sanitization team) | Yes (time-limited) | Yes | Yes | Yes |
| Researchers | No | Case-by-case | Yes | Yes |
| Leadership | No | No | No | Yes |

### 5.2 Role-based access control (RBAC)
Implement via:
- IAM roles (AWS, GCP, Azure)
- Dataset access policies (S3 bucket policies, BigQuery table ACLs)
- Audit logs (CloudTrail, GCP Audit Logs, Azure Monitor)

**Practical setup:**
```yaml
raw_logs_bucket:
  access: data_engineering_team_only
  retention: 30 days
  audit_log: required

sanitized_eval_sets_bucket:
  access: eval_team + annotators + researchers (on request)
  retention: 1 year
  audit_log: required

gold_eval_sets_bucket:
  access: eval_team + approved_researchers
  retention: 3 years
  audit_log: required
```

### 5.3 Audit logs (who accessed what when)
Log every access to eval datasets:
- User ID
- Timestamp
- Action (read, write, download)
- Dataset name + version
- Result (success/failure)

**Use cases:**
- Compliance audits (SOC 2, GDPR Article 30)
- Incident response (PII leak investigation)
- Anomaly detection (unusual access patterns)

**Retention:** 1–3 years (check your compliance requirements)

---

## 6) Retention policies (how long to keep data)

### 6.1 The retention hierarchy

**Raw production logs:**
- **Default retention:** 30–90 days
- **Rationale:** needed for incident investigation, but high PII risk
- **After retention:** delete permanently (don't archive)

**Sanitized eval sets:**
- **Default retention:** 1 year
- **Rationale:** needed for regression testing, quarterly audits
- **After retention:** archive or delete based on compliance needs

**Gold eval sets (reference datasets):**
- **Default retention:** 3 years
- **Rationale:** long-lived benchmarks, used across model versions
- **After retention:** review for continued relevance, delete if obsolete

**Aggregate reports / metrics:**
- **Default retention:** indefinite (no PII, low risk)
- **Rationale:** historical quality tracking, trend analysis

### 6.2 Retention enforcement mechanisms
- **Automated deletion:** use bucket lifecycle policies (S3, GCS) to auto-delete after retention period
- **Access revocation:** revoke access to expired datasets (don't just hide them)
- **Compliance tracking:** log when datasets are deleted (for audit trail)

### 6.3 Right to erasure (GDPR Article 17)
If a user requests deletion:
1. Identify all datasets containing their data (requires lineage tracking)
2. Delete from raw logs (immediate)
3. Delete from sanitized eval sets (within 30 days)
4. If data is in gold sets and cannot be deleted (would break benchmark continuity), document why and ensure it's anonymized to the point of non-re-identification

**2026 best practice:** maintain a `user_id → dataset_version` lineage table so you can honor deletion requests without manual search.

---

## 7) Knobs & defaults (what you actually set)

### 7.1 PII detection confidence threshold
```yaml
pii_detection:
  method: combined (NER + regex + LLM)
  confidence_threshold: 0.75
    # 0.75 = balanced (recall + precision)
    # 0.90 = conservative (fewer false positives, may miss PII)
    # 0.50 = aggressive (more false positives, safer)
  false_positive_tolerance: low  # prefer over-detection
```

### 7.2 Redaction method
```yaml
redaction_strategy:
  direct_identifiers: replacement  # names, emails → synthetic
  quasi_identifiers: generalization  # rare job titles → [ROLE_CATEGORY]
  behavioral_traces: session_id_hash  # preserve multi-turn linkage without user ID
```

### 7.3 Retention windows
```yaml
retention_policies:
  raw_logs: 90 days
  sanitized_eval_sets: 1 year
  gold_sets: 3 years
  audit_logs: 3 years
  aggregate_reports: indefinite
```

### 7.4 Access control defaults
```yaml
access_control:
  default_policy: least_privilege
  raw_logs_access: data_engineering_team_only
  sanitized_access: eval_team + annotators
  audit_log: required for all access
  review_cadence: quarterly
```

---

## 8) Failure modes (symptoms + root causes + fixes)

### 8.1 "PII leaked into an eval report shared with leadership"

**Symptoms:**
- Customer name, email, or phone number visible in demo output
- Compliance team flags data breach
- Customer trust incident

**Root causes:**
- Sanitization pipeline missed PII (detection failure)
- Eval report pulled from raw logs instead of sanitized set
- Human reviewer shared unsanitized examples

**Fix:**
1. **Immediate:** retract report, notify affected users (if GDPR/CCPA applies), log incident
2. **Short-term:** re-run sanitization validation, patch detection rules
3. **Long-term:** enforce policy that all shared outputs must come from sanitized datasets only, add automated PII scan to report generation

---

### 8.2 "Over-redaction destroyed signal, model evaluation is now meaningless"

**Symptoms:**
- Eval examples are mostly `[NAME] asked [REDACTED] about [REDACTED]` — unreadable
- Scores drop dramatically after sanitization
- Annotators can't make sense of examples

**Root causes:**
- Too-aggressive PII detection (low confidence threshold)
- Redaction instead of replacement (lost semantic structure)
- No utility validation after sanitization

**Fix:**
1. **Adjust threshold:** increase confidence threshold to reduce false positives
2. **Switch to replacement:** use synthetic substitution to preserve readability
3. **Add utility check:** sample sanitized examples, confirm they're still scorable

---

### 8.3 "GDPR deletion request cannot be fulfilled because we don't know which datasets contain the user's data"

**Symptoms:**
- User submits Article 17 deletion request
- Data team can't locate all instances of user's data in eval sets
- Compliance violation

**Root causes:**
- No lineage tracking (user_id → dataset_version mapping)
- Eval datasets were anonymized without preserving deletion capability

**Fix:**
1. **Immediate:** manual search + delete (slow, error-prone)
2. **Long-term:** implement lineage table: `user_id, dataset_id, created_at, deleted_at`
3. **Architecture change:** use pseudonymization (reversible hash) instead of anonymization during sanitization, so you can map deletion requests

---

### 8.4 "SOC 2 audit failed because we can't prove who accessed eval datasets"

**Symptoms:**
- Auditor requests access logs for sanitized eval sets
- No logs available (or incomplete)

**Root causes:**
- Audit logging not enabled on data storage (S3, BigQuery, etc.)
- Logs not retained long enough

**Fix:**
1. **Immediate:** enable audit logging on all eval dataset buckets/tables
2. **Retention:** set audit log retention to 3 years (SOC 2 standard)
3. **Review:** quarterly access review (who accessed what, any anomalies?)

---

### 8.5 "Sanitization pipeline is too slow, blocks eval workflow"

**Symptoms:**
- Sanitization takes hours for large datasets
- Eval team bypasses sanitization to meet deadlines (creates risk)

**Root causes:**
- LLM-based PII detection is slow (100+ tokens/sec per call)
- No caching or batch processing

**Fix:**
1. **Parallelize:** run NER + regex + LLM detection in parallel, aggregate results
2. **Cache:** store PII detection results per example (keyed by hash), reuse across runs
3. **Tiered approach:** use fast methods (regex + NER) first, LLM only for ambiguous cases

---

## 9) Debug playbook: building a compliant sanitization pipeline

**Step-by-step setup (first time):**

1. **Inventory your data sources**
   - What data will you use for eval? (prod logs, expert sessions, third-party)
   - What channels? (chat, RAG, agent, voice)
   - What PII types are present?

2. **Choose detection methods**
   - Start with: NER + regex (fast, good coverage)
   - Add: LLM scan for high-risk datasets (HIPAA, financial)

3. **Choose redaction strategy**
   - Default: replacement (synthetic substitution)
   - Use generalization for quasi-identifiers
   - Document your strategy for audit trail

4. **Implement validation**
   - Automated re-scan of sanitized output
   - Manual spot-check (1% sample)
   - Quarterly full audit

5. **Set up access control**
   - RBAC: raw logs = engineering only, sanitized = eval team
   - Enable audit logs
   - Quarterly access review

6. **Define retention policies**
   - Raw logs: 90 days
   - Sanitized: 1 year
   - Gold sets: 3 years
   - Implement automated deletion

7. **Document for compliance**
   - Legal basis (legitimate interest, consent)
   - Article 30 record (GDPR)
   - SOC 2 control documentation
   - EU AI Act conformity (if high-risk)

8. **Test with deletion request**
   - Simulate a GDPR deletion request
   - Verify you can locate and delete user data within 30 days
   - If you can't, implement lineage tracking

---

## 10) Enterprise expectations (what serious teams do)

- They treat eval datasets as **regulated data assets** with the same rigor as production databases
- They implement **multi-layer detection** (NER + regex + LLM) for PII, not just regex
- They use **replacement over redaction** to preserve semantic signal while removing PII
- They maintain **lineage tracking** (user_id → dataset_version) to support deletion requests
- They enforce **role-based access** with audit logs for every dataset access
- They document **legal basis and retention policies** for GDPR, CCPA, HIPAA, and SOC 2 compliance
- They run **quarterly audits** of sanitization quality (spot-check for missed PII)
- They separate **raw, sanitized, and gold datasets** with different access and retention rules
- They integrate sanitization validation into CI/CD: no eval can run on unsanitized data

---

## 11) Ready-to-use templates

### 11.1 Sanitization pipeline checklist

```yaml
sanitization_pipeline:
  input: raw_logs_dataset
  output: sanitized_eval_dataset

  detection:
    - method: regex
      patterns: [email, phone, SSN, credit_card]
      confidence: 0.95
    - method: NER
      model: spacy_en_core_web_trf
      entities: [PERSON, ORG, GPE, DATE]
      confidence: 0.75
    - method: LLM_scan
      model: gpt-4o-mini
      prompt: "Identify all PII. Return: type, span, confidence."
      confidence: 0.70
    - aggregation: union (any detection triggers redaction)

  redaction:
    strategy: replacement
    direct_identifiers:
      PERSON: synthetic_name (preserve ethnicity distribution)
      EMAIL: fake_email@example.com
      PHONE: 555-0100 to 555-0199
    quasi_identifiers:
      rare_job_title: generalize to [ROLE_CATEGORY]
      zip_code: generalize to [REGION_STATE]

  validation:
    - automated_rescan: true (re-run PII detection on output)
    - manual_spot_check: 1% sample
    - utility_check: annotators confirm examples are scorable

  lineage:
    - track: user_id → dataset_version → example_id
    - purpose: support GDPR deletion requests
```

### 11.2 Access control matrix

| Dataset Type | Raw Logs | Sanitized Eval Sets | Gold Sets | Aggregate Reports |
|---|---|---|---|---|
| **Access** | Data eng only | Eval team + annotators | Eval team + approved researchers | All (public or internal BI) |
| **Retention** | 90 days | 1 year | 3 years | Indefinite |
| **Audit log** | Required | Required | Required | Optional |
| **Deletion policy** | Auto-delete after 90d | Supports GDPR deletion | Anonymized (no re-ID risk) | No PII (no deletion needed) |

### 11.3 Retention policy configuration

```yaml
retention_policies:
  raw_logs:
    bucket: s3://company-raw-logs/
    retention: 90 days
    lifecycle_rule: auto_delete
    access: data_engineering_team_only
    audit_log: enabled

  sanitized_eval_sets:
    bucket: s3://company-eval-sanitized/
    retention: 1 year
    lifecycle_rule: auto_delete
    access: eval_team, annotators
    audit_log: enabled
    supports_deletion_requests: true

  gold_eval_sets:
    bucket: s3://company-eval-gold/
    retention: 3 years
    lifecycle_rule: archive_then_delete
    access: eval_team, approved_researchers
    audit_log: enabled
    anonymization_required: true

  audit_logs:
    bucket: s3://company-audit-logs/
    retention: 3 years
    lifecycle_rule: archive_then_delete
    access: security_team, compliance_team
```

### 11.4 GDPR deletion request workflow

```yaml
gdpr_deletion_workflow:
  trigger: user submits Article 17 deletion request

  step_1_locate:
    - query lineage table: user_id → datasets
    - output: list of dataset_ids containing user data

  step_2_delete:
    - raw_logs: delete immediately
    - sanitized_eval_sets: delete within 30 days
    - gold_sets: verify anonymization (no re-ID risk)
      - if re-ID risk exists: delete
      - if no risk: document why retention is justified

  step_3_log:
    - record deletion in audit log
    - confirm to user (GDPR Article 17 requires confirmation)

  step_4_verify:
    - re-scan all datasets for user_id (should return zero results)
```

---

## 12) Interview-ready talking points

> "I treat eval datasets as regulated data assets with PII risk, not just measurement tools."

> "I use a three-layer detection pipeline: regex for structured PII, NER for entities, and LLM scan for context-dependent identifiers."

> "I prefer replacement over redaction — synthetic substitution preserves semantic signal while removing PII."

> "I maintain lineage tracking (user_id to dataset_version) so I can honor GDPR deletion requests within 30 days."

> "I enforce least-privilege access: raw logs are engineering-only, sanitized sets are eval-team-wide, with full audit logs."

> "I separate raw logs (90-day retention), sanitized eval sets (1 year), and gold sets (3 years, anonymized)."

> "I run quarterly audits: re-scan sanitized datasets for missed PII, review access logs for anomalies."

> "For SOC 2 compliance, I document access controls, retention policies, and audit logs for every eval dataset."

> "For GDPR compliance, I document legal basis (legitimate interest), implement data minimization, and support right to erasure."

> "For HIPAA (healthcare), I use Safe Harbor de-identification: remove 18 identifier types, validate no re-identification risk."
