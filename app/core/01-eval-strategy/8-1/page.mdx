# 8.1 — Why Agent Evaluation Is Different

Let me tell you about the day a customer service agent deleted 14,000 customer records.

Not a human agent. An AI agent. It was supposed to clean up duplicate entries in a CRM system. The company had tested it thoroughly—gave it sample data, watched it identify duplicates correctly, verified the output looked good. They deployed it on a Friday afternoon.

By Monday morning, the agent had marked every customer who shared a last name as a "duplicate" and deleted all but one. The Johnsons, the Smiths, the Garcias—all gone except for one randomly selected record. The final output? "Successfully removed 14,000 duplicate entries." Technically accurate. Catastrophically wrong.

The company's evaluation strategy was built for chatbots. They checked if the agent could identify duplicates in a test set. It could. They verified the output format was correct. It was. What they didn't evaluate was the **trajectory**—the sequence of decisions, tool calls, and state changes that led to that output.

When you move from chatbots to agents, you're not just adding features. You're crossing a fundamental threshold. Chatbots answer questions. Agents **take actions**. And actions have consequences in the real world—booking flights, purchasing equipment, deleting records, sending emails to customers, modifying production databases.

This changes everything about how you evaluate.

---

## The Chatbot-to-Agent Shift

For the first three years of the LLM era (2022-2025), most production systems were **chatbots**. You sent a prompt, you got text back. The worst-case scenario was a bad answer. Embarrassing, maybe. Harmful to trust, definitely. But contained.

Agents are different. An **agent** is an LLM-powered system that can:

- **Call tools** (APIs, databases, external services)
- **Take actions** that modify state (create, update, delete operations)
- **Operate autonomously** over multiple turns without human intervention
- **Make decisions** that cascade into further actions

The canonical example in 2026 is a customer support agent that can look up orders, process refunds, update shipping addresses, and escalate complex cases. It's not just retrieving information—it's **doing things** on behalf of users.

This shift introduces three fundamental challenges for evaluation:

**1. Real-world consequences.** When a chatbot hallucinates, you get a wrong answer. When an agent hallucinates mid-execution, it might cancel the wrong subscription, send a refund to the wrong account, or delete production data.

**2. Multi-step reasoning.** Agents operate over many turns. They maintain context, make sequential decisions, adapt plans based on intermediate results. A failure in step 3 might not surface until step 7.

**3. Non-deterministic paths.** There isn't one "right" way to complete most tasks. An agent might check inventory before checking payment info, or vice versa. Both sequences can be valid. Your evaluation must handle this.

By mid-2025, the industry realized that **output-only evaluation**—the foundation of chatbot testing—was fundamentally inadequate for agents. You can't just check the final result. You have to evaluate the journey.

---

## Why Output-Only Evaluation Fails for Agents

Here's a scenario. You're building an agent to help users book conference rooms. A user asks: "Book a room for 10 people tomorrow at 2pm."

**Scenario A:** The agent books Conference Room B, capacity 12, available tomorrow at 2pm. Output: "Booked Conference Room B for 10 people tomorrow at 2pm."

**Scenario B:** The agent books Conference Room B without checking capacity (it holds 6 people) or availability (it's already booked). By luck, the existing booking gets cancelled five minutes later and the meeting organizer doesn't notice the capacity issue because only 5 people show up. Output: "Booked Conference Room B for 10 people tomorrow at 2pm."

Same output. Radically different quality.

An **output-only evaluation** would mark both as correct. But Scenario B is a ticking time bomb. The agent didn't follow the correct process. It got lucky. Next time, it won't be lucky.

This is the **lucky path problem**. An agent that achieves the right outcome through wrong steps is dangerous. It appears to work in testing, then fails catastrophically in production under slightly different conditions.

Traditional LLM evaluation focuses on the final generation. You compare the output against a reference answer, measure similarity (BLEU, ROUGE, semantic similarity), maybe have a human rate quality. This works for question-answering, summarization, translation—tasks where the output is the entire artifact you care about.

For agents, **the output is the least interesting part**. What matters is:

- Did the agent check availability before booking?
- Did it verify capacity matched requirements?
- Did it handle the case where no rooms were available?
- Did it confirm with the user before finalizing?
- If the booking failed, did it retry or suggest alternatives?

You can't answer these questions by looking at "Booked Conference Room B for 10 people tomorrow at 2pm."

You need to examine the **trajectory**.

---

## The Trajectory Matters

A **trajectory** is the complete sequence of steps an agent takes from initial input to final output. In the conference room example:

**Good trajectory:**
1. Parse user request (10 people, tomorrow, 2pm)
2. Query available rooms for tomorrow at 2pm
3. Filter rooms by capacity (greater than or equal to 10)
4. Select Conference Room B (capacity 12, available)
5. Create booking
6. Return confirmation

**Bad trajectory:**
1. Parse user request (10 people, tomorrow, 2pm)
2. Select Conference Room B (without checking anything)
3. Create booking (without validation)
4. Return confirmation

When you evaluate agents, **the trajectory IS the primary evaluation target**. Not the final answer. The sequence of tool calls, decisions, state changes, and recovery actions.

This is a conceptual shift. In chatbot evaluation, you treat the model as a black box. Input goes in, output comes out, you measure output quality. In agent evaluation, you **open the black box**. You instrument every step. You log every tool call, every decision point, every state transition.

Modern agent frameworks (LangGraph, CrewAI, AutoGPT successors in 2026) all emit **execution traces**—structured logs of the agent's trajectory. These traces are the raw material for evaluation.

A minimal trace entry looks like this:

```yaml
step_id: 3
timestamp: 2026-01-15T14:23:45Z
action_type: tool_call
tool_name: check_room_availability
tool_inputs:
  date: "2026-01-16"
  time: "14:00"
  min_capacity: 10
tool_output:
  available_rooms: ["Conference Room B", "Conference Room D"]
  capacities: [12, 15]
next_step: 4
```

You build assertions against these traces:

- "Step 2 must be a capacity check before step 5 (booking)"
- "If tool call X fails, step N+1 must be error handling, not proceeding"
- "Total steps for booking flow must be between 4 and 8" (sanity check against infinite loops)

By 2026, **trajectory-based evaluation** is the standard for agent testing. Platforms like LangSmith, Maxim AI, and Galileo provide built-in trace viewers and assertion frameworks. The AGENCYBENCH benchmark (released late 2025) includes reference trajectories, not just input-output pairs.

If you're still evaluating agents by checking final outputs, you're flying blind.

---

## Multi-Turn Complexity

Chatbots typically operate in **single-turn** interactions. User sends a message, model generates a response, done. Even in multi-turn conversations, each turn is largely independent—the model uses conversation history as context, but each response is a discrete generation task.

Agents operate over **many turns** within a single task execution. A travel booking agent might:

1. Search for flights (turn 1)
2. Present options to user (turn 2)
3. Wait for user selection (turn 3)
4. Check seat availability (turn 4)
5. Process payment (turn 5)
6. Handle payment failure (turn 6)
7. Retry with alternative payment (turn 7)
8. Confirm booking (turn 8)

Each turn depends on the previous turns. The agent maintains **state** across turns—selected flight, user preferences, payment method, error history. It makes decisions based on accumulated context.

This introduces temporal dependencies that single-turn evaluation can't capture:

**State management:** Does the agent correctly track information across turns? If the user says "I prefer aisle seats" in turn 2, does the agent remember that in turn 6?

**Error propagation:** If a tool call fails in turn 4, does that contaminate decisions in turn 7? Does the agent get stuck in a retry loop?

**Plan adaptation:** If the agent's initial plan (book flight A) becomes impossible (flight sells out), does it adapt (offer flight B) or fail?

**Context window management:** Over 15 turns, does the agent overflow its context? Does it summarize or truncate appropriately?

In 2024, most teams tested agents with single-task scenarios: "Book this flight." By 2025, practitioners realized they needed **multi-turn test scenarios** that exercised state management and error recovery across extended interactions.

The OSUniverse benchmark (released mid-2025) includes scenarios that run for 20-30 turns, deliberately introducing failures mid-execution to test recovery. Example: "Book a round-trip flight to Paris, but the payment API will fail twice, and the preferred hotel will be unavailable."

Multi-turn evaluation requires:

- **Scenario scripts** that define the full interaction sequence, including simulated failures
- **State assertions** that verify the agent maintains correct state across turns
- **Rollback testing** to ensure the agent can recover from failures without corrupting state

If your agent eval only tests single-turn interactions, you're not evaluating the agent—you're evaluating a chatbot with tool access.

---

## Non-Determinism at Scale

Here's a question: What's the "correct" trajectory for booking a flight?

There isn't one. There are many valid trajectories. An agent might:

- Check flight availability, then check user's calendar for conflicts, then book
- Check user's calendar first, then check flight availability, then book
- Check flight availability and calendar in parallel, then book
- Check availability, present options, wait for user confirmation, then book

All valid. The optimal trajectory might depend on API latency, user preferences, or booking urgency.

This is **non-deterministic execution**. The same input can produce different valid trajectories. Agents make decisions based on LLM generation (inherently stochastic), API response times (variable), and user interactions (unpredictable).

Traditional software testing assumes determinism. Given input X, the system should always produce output Y. You write a test: `assert booking_agent("flight to Paris") == "Booked flight AF123"`. If the output changes, the test fails.

Agent testing can't work this way. You need to evaluate **classes of valid trajectories**, not exact sequence matching.

**Example:** For a refund processing agent, valid trajectories might include:

- **Trajectory class 1:** Check order exists → Verify refund eligibility → Process refund → Send confirmation
- **Trajectory class 2:** Check order exists → Verify refund eligibility → Verify refund eligibility fails → Escalate to human
- **Trajectory class 3:** Check order exists → Order doesn't exist → Ask user for order number → Loop to start

Your evaluation asserts: "The trajectory must match one of these three classes." You don't assert an exact sequence.

By 2026, agent evaluation frameworks support **trajectory constraints** rather than exact matching:

```yaml
required_steps:
  - tool: check_order_exists
    before: [process_refund]
  - tool: verify_eligibility
    before: [process_refund]

optional_steps:
  - tool: send_confirmation

prohibited_steps:
  - tool: process_refund
    when:
      eligibility: false
```

This says: "You must check order existence and eligibility before processing refund. Sending confirmation is optional. You must never process a refund when eligibility is false." The exact order of the first two steps doesn't matter.

This shift—from exact matching to constraint satisfaction—is essential for evaluating non-deterministic agents at scale.

---

## Real-World Side Effects

When you test a chatbot, the test is isolated. You send a prompt, you get a response, nothing else changes. You can run the test a thousand times without side effects.

When you test an agent, the agent **modifies the world**. It calls APIs. It writes to databases. It sends emails. Each test execution has side effects.

This creates a fundamental evaluation problem: **You can't safely test agents in production environments.**

If your booking agent is connected to a live payment API, testing it means processing real charges. If your email agent is connected to SendGrid, testing it means sending real emails to real people. If your data management agent is connected to your production database, testing it means actually deleting records.

The industry standard solution is **sandboxing**: Run agents against mock environments that simulate production APIs without real side effects.

But sandboxing introduces its own problems:

**Fidelity gap:** Mock APIs don't perfectly replicate production behavior. Edge cases, latency patterns, error modes—all differ. An agent that works in the sandbox might fail in production.

**Maintenance burden:** Every production API change requires updating the mock. If your payment API adds a new required field, your sandbox needs to reflect that, or your tests become invalid.

**State management:** Some agent tasks span multiple systems. "Book a flight and add it to my calendar." Your sandbox needs to mock both the booking API and the calendar API, and correctly simulate their interaction.

By 2026, most organizations use a **hybrid testing strategy**:

**1. Sandbox testing (80% of tests):** Run agents against mock environments for rapid iteration and comprehensive trajectory testing.

**2. Production testing with guardrails (15% of tests):** Run agents in production with human-in-the-loop oversight. The agent executes, but a human approves any state-modifying action before it commits.

**3. Shadow mode (5% of tests):** Run agents in parallel with human operators. The agent executes fully, but its actions aren't committed. Compare agent decisions to human decisions.

The GAIA benchmark (General AI Assistants, major benchmark released 2024-2025) doesn't test real-world side effects at all—it focuses on reasoning and tool use in simulated environments. This is a known limitation. Enterprise teams building production agents need **integration test suites** that exercise real API interactions in controlled environments.

If your agent evaluation strategy doesn't account for side effects, your first production deployment will be a learning experience you don't want.

---

## The Planning Dimension

Modern agents don't just react. They **plan**.

The **ReAct pattern** (Reasoning + Acting, formalized 2023, standardized by 2025) has agents generate explicit plans before executing:

**Thought:** "The user wants to book a flight to Paris next week. I need to: 1) Search for available flights, 2) Check user's calendar for conflicts, 3) Present options, 4) Book selected flight."

**Action:** Execute step 1.

**Observation:** Results from step 1.

**Thought:** "Step 1 succeeded. Proceeding to step 2."

The agent maintains a plan and executes it step-by-step. When something goes wrong, it **replans**.

This introduces a new evaluation dimension: **You need to evaluate the plan AND the execution separately.**

A bad plan executed well leads to wrong outcomes. A good plan executed poorly leads to failures. Both matter.

**Plan evaluation:**
- Is the plan logically coherent?
- Does it include all necessary steps?
- Are steps in a valid order (check availability before booking)?
- Does it handle known failure modes (what if no flights are available)?

**Execution evaluation:**
- Does the agent follow the plan?
- When execution diverges from the plan (unexpected API error), does it replan appropriately?
- Are individual actions executed correctly (correct tool calls, correct parameters)?

In Chapter 3.7, we covered **agentic task decomposition**—breaking complex tasks into subtasks. For agents, this decomposition happens at runtime. The agent generates the decomposition (the plan), then executes it.

Your evaluation must verify both layers.

By 2026, advanced agent frameworks expose the plan as a structured artifact:

```yaml
plan_id: plan_a3f2
task: "Book round-trip flight to Paris"
steps:
  - id: 1
    description: "Search outbound flights"
    tool: flight_search
    dependencies: []
  - id: 2
    description: "Search return flights"
    tool: flight_search
    dependencies: []
  - id: 3
    description: "Check calendar for conflicts"
    tool: calendar_check
    dependencies: [1, 2]
  - id: 4
    description: "Present options to user"
    tool: user_interface
    dependencies: [3]
```

You can evaluate this plan before execution:

- "Step 3 should depend on steps 1 and 2" ✓
- "Step 4 should depend on step 3" ✓
- "Plan includes calendar check" ✓

Then evaluate execution:

- "Agent completed steps 1, 2, 3, 4 in dependency order" ✓
- "Agent skipped step 3" ✗

This separation—plan quality vs. execution quality—is critical for debugging. If the plan is wrong, you have a reasoning problem (prompt engineering, model selection). If the plan is right but execution fails, you have an integration problem (tool configuration, API reliability).

Don't evaluate agents as monolithic black boxes. Evaluate planning and execution as distinct phases.

---

## Error Recovery as a First-Class Metric

Here's a truth about production systems: **Things go wrong.**

APIs time out. Databases become unavailable. Third-party services return errors. Users provide malformed input. Authentication tokens expire.

For chatbots, error handling is simple: Return an error message. "Sorry, I couldn't complete that request."

For agents, error handling is **mission-critical**. An agent might be 5 steps into an 8-step process when an API fails. What does it do?

**Bad agent:** Gives up. "I encountered an error. Please try again later." The user has to restart from step 1.

**Good agent:** Retries with exponential backoff. "The payment API timed out. Retrying... Success. Booking confirmed."

**Great agent:** Adapts the plan. "The payment API is unavailable. I've saved your flight selection. Would you like me to try a different payment method, or should I hold the reservation and retry in 5 minutes?"

The ability to **recover gracefully from errors** is often more important than happy-path performance. Users forgive agents that handle failures well. They abandon agents that break at the first problem.

Yet most agent evaluation in 2024-2025 focused on **happy paths**—scenarios where everything works. By 2026, the industry recognized that **error recovery must be a first-class metric**.

Modern agent benchmarks deliberately inject failures:

**AGENCYBENCH error scenarios:**
- API returns 500 error on first call, succeeds on retry
- Tool returns incomplete data (missing required field)
- User provides ambiguous input mid-execution
- Network partition isolates agent from one service for 30 seconds

Your agent's performance under these conditions is as important as its performance when everything works.

**Error recovery metrics:**
- **Recovery rate:** Percentage of injected errors the agent successfully recovers from
- **Recovery time:** How long it takes the agent to recover (immediate retry vs. exponential backoff vs. giving up)
- **State consistency:** After recovery, is the agent's state still valid? (No corrupted bookings, no duplicate charges)
- **User experience:** Does the agent explain what went wrong and what it's doing? Or does it silently retry 47 times?

The best agent teams in 2026 spend more time testing failure scenarios than happy paths.

If your eval suite doesn't include deliberate failures, you're not evaluating production readiness.

---

## The 2026 Landscape

By early 2026, agent evaluation has matured into a distinct discipline with its own benchmarks, platforms, and best practices.

**Key benchmarks:**

**GAIA** (General AI Assistants Benchmark): Multi-step reasoning tasks requiring tool use. 500+ questions spanning web search, file manipulation, calculations. Released 2024, now industry standard for general agent capabilities.

**OSUniverse** (Open-Source Universe Benchmark): Agents interact with real software environments (web apps, CLIs, APIs). Tasks include "Deploy a web app to staging, run tests, and roll back if tests fail." Released mid-2025.

**AGENCYBENCH** (Agent Benchmark): Focus on error recovery, multi-turn interactions, and trajectory quality. Includes reference trajectories and trajectory constraints. Released late 2025.

**Specialized benchmarks:** WebArena (web navigation), ToolBench (tool use), SWE-bench (software engineering agents).

**Key platforms:**

**LangSmith** (LangChain): Agent tracing, trajectory visualization, evaluation harness. Supports custom trajectory assertions.

**Maxim AI**: Enterprise agent evaluation platform. Trajectory diffing, automated regression detection, production monitoring.

**Galileo AI**: Agent observability and evaluation. Real-time trace analysis, anomaly detection, human-in-the-loop review.

**Braintrust**: LLM evaluation platform expanded to agents in 2025. Dataset versioning, trajectory comparisons, CI/CD integration.

**Industry adoption patterns:**

By mid-2026, approximately **60% of enterprises restrict agent deployment without human oversight**. The remaining 40% allow autonomous agent actions, but typically in low-risk domains (internal tools, data retrieval) or with extensive sandboxing.

**Common agent deployment gates** (from industry surveys):

1. Pass regression test suite (95%+ pass rate on trajectory constraints)
2. Human evaluation of 50+ trajectories (to catch edge cases automated tests miss)
3. Red team assessment (covered in Chapter 14)
4. Staged rollout with monitoring (1% traffic → 10% → 50% → 100%)
5. Kill switch and rollback plan (immediate shutdown if error rate exceeds threshold)

The most mature teams treat agent deployment like high-stakes infrastructure deployment: extensive testing, gradual rollout, continuous monitoring, instant rollback capability.

The **ReAct pattern** has become standardized. Most agent frameworks (LangGraph, CrewAI, Autogen 2.0) support it natively. Evaluation tools understand ReAct's thought-action-observation structure and can parse traces automatically.

By 2026, **"trajectory-first evaluation"** is the consensus best practice. Output-only evaluation is recognized as inadequate. If you're building agents and not evaluating trajectories, you're behind the curve.

---

## Failure Modes Specific to Agent Evaluation

When teams first move from chatbot evaluation to agent evaluation, they make predictable mistakes. Here are the top failure modes in 2026:

**1. Testing only happy paths.**

The team writes test cases where everything works—APIs return data, users provide valid input, the environment is stable. They achieve 98% pass rate and deploy. In production, the agent encounters an API timeout and breaks catastrophically.

**Fix:** Allocate at least 40% of test scenarios to error injection. Deliberately fail APIs, corrupt data, provide invalid inputs.

**2. Ignoring trajectory, evaluating only outputs.**

The team checks if the agent books the right flight. They don't check if it verified availability first, or handled payment failures, or confirmed with the user. The agent gets lucky in testing, fails in production.

**Fix:** Shift to trajectory-based evaluation. Assert on step sequences, not just final outputs.

**3. Not testing multi-turn interactions.**

The team tests each capability in isolation: "Can it search flights?" "Can it process payments?" They don't test the full end-to-end flow over 10 turns, maintaining state, adapting to user changes.

**Fix:** Build multi-turn test scenarios that exercise state management and plan adaptation.

**4. Exact trajectory matching.**

The team records one "golden" trajectory and asserts all future runs must match it exactly. When the agent's LLM provider updates and changes generation slightly, all tests break, even though behavior is still correct.

**Fix:** Use trajectory constraints (required/optional/prohibited steps) instead of exact matching.

**5. Sandboxing without fidelity checks.**

The team builds a mock API for testing. The mock diverges from production (different error codes, different latency). Tests pass, but the agent fails in production.

**Fix:** Periodically validate sandbox fidelity against production. Use shadow mode to catch divergence.

**6. Not versioning evaluation datasets.**

The team adds test cases over time but doesn't version them. When performance drops, they can't tell if the agent regressed or the tests changed.

**Fix:** Treat eval datasets as code. Version them, track changes, run regression tests against historical versions.

**7. Over-reliance on automated metrics.**

The team measures success rate, error rate, trajectory length. They don't do human evaluation. They miss failure modes that metrics don't capture (rude responses, inefficient trajectories, poor user experience).

**Fix:** Combine automated metrics with regular human review (covered in Chapter 6).

If you recognize your team in these failure modes, you're in good company. Most teams hit them. The difference is whether you learn from them before or after production incidents.

---

## Enterprise Expectations for Agent Evaluation Programs

By 2026, enterprises deploying autonomous agents have converged on a standard evaluation program structure. If you're building an agent evaluation strategy, this is the baseline expectation:

**Pre-deployment:**

1. **Unit tests for individual tools:** Each tool the agent can call has its own test suite. If the agent can call `book_flight`, there are tests ensuring `book_flight` works correctly in isolation.

2. **Trajectory test suite:** 100+ test scenarios covering happy paths, error cases, edge cases. Each scenario includes trajectory constraints. Minimum 95% pass rate required for deployment.

3. **Multi-turn integration tests:** 20+ scenarios testing extended interactions (15-30 turns) with state management and error recovery.

4. **Human evaluation:** Domain experts review 50-100 trajectories, rating on correctness, efficiency, user experience. Minimum 85% "acceptable or better" rating required.

5. **Red team assessment:** Security team attempts to manipulate the agent into harmful actions (covered in Chapter 14). No critical vulnerabilities allowed.

6. **Shadow mode validation:** Run agent in parallel with existing process (human agent, legacy system) for 1-2 weeks. Compare decisions. Resolve discrepancies before launch.

**Post-deployment:**

1. **Production monitoring:** Real-time trajectory logging and analysis. Alert on anomalies (unusual step sequences, high error rates, long execution times).

2. **Weekly evaluation runs:** Re-run the pre-deployment test suite weekly. Catch regressions from LLM provider updates, API changes, or prompt modifications.

3. **Monthly human review:** Human evaluators review a sample of production trajectories (typically 100-200 per month). Identify failure modes not caught by automated tests.

4. **Quarterly benchmark evaluation:** Run the agent on public benchmarks (GAIA, AGENCYBENCH) to compare performance against industry baselines and track improvements.

5. **Continuous dataset expansion:** When production failures occur, convert them into test cases. The eval dataset grows to cover real-world edge cases.

This is the mature state in 2026. Most teams are still building toward it. But if you're deploying agents that handle high-value or high-risk tasks (financial transactions, healthcare decisions, legal processes), this is the expected standard.

The regulatory environment is also evolving. In the EU, the AI Act (phased enforcement 2025-2027) requires documentation of evaluation procedures for high-risk AI systems. In the US, industry groups have published voluntary agent evaluation frameworks. Expect mandatory evaluation requirements within 2-3 years.

If you're treating agent evaluation as an afterthought, you're accumulating technical and regulatory debt.

---

## Template: Agent Trajectory Evaluation Spec

When you design an agent evaluation, document what you're testing using a structured spec. Here's a lean template:

```yaml
agent_eval_spec:
  agent_name: "Flight Booking Agent"
  version: "2.1.0"
  eval_date: "2026-01-15"

  scenario:
    id: "booking_happy_path_001"
    description: "User books round-trip flight, no errors"
    initial_state:
      user_logged_in: true
      user_has_payment_method: true
      flights_available: true

    user_inputs:
      - turn: 1
        message: "Book a round-trip flight to Paris, leaving Jan 20, returning Jan 27"
      - turn: 4
        message: "Select the 10am flight"

    expected_trajectory:
      required_steps:
        - tool: search_flights
          before: [present_options]
        - tool: present_options
          before: [book_flight]
        - tool: verify_payment
          before: [book_flight]
        - tool: book_flight

      prohibited_steps:
        - tool: book_flight
          when:
            user_confirmed: false

      max_total_steps: 10

    success_criteria:
      - flight_booked: true
      - correct_destination: "Paris"
      - correct_dates: ["2026-01-20", "2026-01-27"]
      - user_confirmation_obtained: true
      - payment_processed: true

    error_injection: none
```

This spec defines the scenario, the expected trajectory constraints, and success criteria. It's executable (your eval harness can parse and run it) and human-readable (stakeholders can review it).

Maintain a library of these specs. Version them. Review them when the agent changes or requirements evolve.

---

## Interview Questions: Agent Evaluation Fundamentals

**Q1: Why can't we evaluate agents the same way we evaluate chatbots?**

**A:** Chatbots produce text outputs; agents take actions with real-world consequences. For chatbots, you evaluate output quality (accuracy, fluency, relevance). For agents, you must evaluate the entire trajectory—the sequence of tool calls, decisions, and state changes—because an agent that gets the right result through wrong steps is dangerous. An agent might book the correct flight without checking availability first; it got lucky, but it's not reliable. Output-only evaluation misses this completely. You can't judge an agent's quality by its final answer; you must examine every step it took to get there.

**Q2: What is a trajectory in agent evaluation, and why does it matter?**

**A:** A trajectory is the complete sequence of steps an agent takes from initial input to final output: the tool calls, reasoning steps, state changes, and decisions. It matters because the trajectory reveals whether the agent is following correct processes, even if it happens to produce the right output. For example, an agent that deletes customer records without verifying permissions might succeed in testing (lucky path) but fail catastrophically in production. By evaluating trajectories—asserting that certain checks must happen before certain actions—you ensure the agent is robust, not just lucky. Trajectory evaluation is the foundation of reliable agent testing.

**Q3: How does multi-turn complexity change agent evaluation requirements?**

**A:** Multi-turn interactions introduce state management, error propagation, and plan adaptation that single-turn tests can't capture. An agent might maintain context across 20 turns, remember user preferences from turn 2 when making decisions in turn 15, and recover from failures in turn 8 that affect turn 12. Single-turn evaluation treats each interaction as independent; multi-turn evaluation tests whether the agent correctly maintains state, adapts its plan when conditions change, and handles errors without corrupting accumulated context. If you only test single-turn scenarios, you're not evaluating the agent's most critical capabilities—its ability to operate autonomously over extended interactions.

**Q4: Why is error recovery considered a first-class metric for agent evaluation?**

**A:** Because production systems always encounter errors—API timeouts, missing data, invalid inputs—and an agent's ability to recover gracefully is often more important than its happy-path performance. Users forgive agents that handle failures well (retry with backoff, adapt the plan, explain what's happening). They abandon agents that break at the first problem. Yet most early agent evaluation focused only on scenarios where everything worked. By 2026, the industry recognized that error recovery must be explicitly tested: inject API failures, corrupt data, provide ambiguous inputs, and measure whether the agent retries, replans, or fails gracefully. An agent that performs perfectly under ideal conditions but collapses when an API times out is not production-ready.

**Q5: What are the biggest mistakes teams make when first evaluating agents?**

**A:** The most common failure modes are: (1) Testing only happy paths, ignoring error scenarios; (2) Evaluating only final outputs instead of trajectories; (3) Testing capabilities in isolation instead of multi-turn end-to-end flows; (4) Requiring exact trajectory matching instead of trajectory constraints, which breaks when LLMs produce slight variations; (5) Using mock environments without validating they match production behavior; and (6) Relying solely on automated metrics without human review to catch subtle failure modes. Most teams hit these mistakes and learn from production incidents. The mature approach is trajectory-first evaluation with extensive error injection, multi-turn scenarios, trajectory constraints (not exact matching), sandbox fidelity checks, and regular human review.

---

In Chapter 8.2, we'll go deep on **Trajectory and Action Trace Evaluation**—the practical techniques for logging, parsing, and asserting on agent execution traces. You'll learn how to instrument agents for observability, define trajectory constraints that handle non-determinism, and build evaluation harnesses that scale to hundreds of test scenarios. Let's make trajectory evaluation concrete.
