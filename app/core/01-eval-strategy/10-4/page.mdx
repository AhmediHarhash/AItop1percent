# Speech Synthesis & Voice Quality Evaluation

## The Last Mile Problem

A customer support AI perfectly understands a frustrated caller's problem. It retrieves the right policy. It crafts an empathetic response. Then it speaks — and the voice sounds like a GPS unit from 2009. Flat. Robotic. No warmth. The caller hangs up.

**Text-to-speech is the last mile.** Everything upstream can be flawless — intent recognition, reasoning, response generation — but if the voice fails, the entire experience collapses. A healthcare voice agent needs calm reassurance. A sales voice needs enthusiasm without being overbearing. A tutoring voice needs patience and clarity. Get the voice wrong, and users feel like they are talking to a machine, not an assistant.

Evaluating TTS quality is harder than evaluating text. You cannot run regex checks. You cannot count tokens. You need human ears, subjective judgments, and systematic protocols. But in 2026, with real-time voice agents becoming the interface for customer support, telehealth, and sales, voice quality evaluation is no longer optional. It is the final gate between development and production.

---

## Mean Opinion Score: The Gold Standard

**Mean Opinion Score (MOS)** is the gold standard for voice quality evaluation. The protocol is simple: human listeners hear a speech sample, then rate it on a scale from 1 to 5.

**MOS Scale:**
- **5** — Excellent. Indistinguishable from human speech.
- **4** — Good. Natural, minor imperfections.
- **3** — Fair. Clearly synthetic, but understandable.
- **2** — Poor. Robotic, difficult to listen to.
- **1** — Bad. Nearly unintelligible or extremely unpleasant.

You play 20 TTS samples to 15 human raters. Each rater scores each sample. You average the scores. A production-ready TTS system typically needs **MOS above 4.0** for consumer applications, **above 4.3** for healthcare or high-stakes domains.

MOS originated in telecom quality testing in the 1990s. It remains the definitive metric because human perception is the ultimate arbiter. If users say a voice sounds robotic, it is robotic — no matter what your acoustic features report.

**MOS testing is expensive.** Recruiting raters, playing samples, collecting scores, ensuring demographic diversity — a full MOS evaluation for 100 samples can cost thousands of dollars and take days. But for launch decisions, brand-sensitive deployments, or regulatory compliance, MOS is non-negotiable.

In Chapter 6, we covered human evaluation protocols. MOS follows the same principles: **rater instructions matter.** If you tell raters "rate naturalness," you get different scores than "rate clarity." If your raters are all under 25, you miss issues that older users notice. Demographics, listening environment (headphones versus laptop speakers), and fatigue all influence MOS.

---

## Automated MOS Prediction Models

Because human MOS is slow and expensive, researchers have built **automated MOS prediction models** — models that listen to TTS output and predict what MOS score humans would give.

**Common automated metrics:**

1. **PESQ (Perceptual Evaluation of Speech Quality)** — developed for telecom. Compares degraded speech to a reference signal. Outputs a score from 1 to 4.5. Works well for codec quality, less so for TTS naturalness.

2. **POLQA (Perceptual Objective Listening Quality Assessment)** — successor to PESQ. Better for wideband and super-wideband audio. Still requires a reference signal, limiting its use for generative TTS where no reference exists.

3. **UTMOS (Universal Text-to-Speech Mean Opinion Score)** — a neural model trained on thousands of TTS samples with human MOS labels. Predicts MOS from 1 to 5 without needing a reference. Developed by researchers in 2022, now widely used in TTS pipelines.

4. **MOSNet** — another neural MOS predictor. Trained on VoiceMOS challenge datasets. Correlates well with human MOS for English, less so for tonal languages.

**Automated MOS predictors are fast but imperfect.** They catch egregious failures — glitches, distortion, robotic monotone. But they miss subtle issues: unnatural pauses, emotional mismatch, uncanny valley effects. A model might give 4.2 to a voice that sounds creepy, because the acoustic features are technically clean.

**Use automated metrics for continuous monitoring, human MOS for launch gates.** Run UTMOS on every TTS sample in production to catch regressions. Run human MOS quarterly, or whenever you change TTS providers, to validate that automated scores still align with user perception.

---

## Naturalness Evaluation: Prosody, Intonation, Breathing

**Naturalness** is the degree to which TTS sounds like a real human speaking. It is the primary dimension users care about. A natural voice feels alive. An unnatural voice feels like a machine reading a script.

**Key components of naturalness:**

**Prosody** — the rhythm and stress patterns of speech. Humans emphasize certain words, pause at clause boundaries, speed up when excited. TTS systems that ignore prosody sound monotone. Evaluating prosody requires listening for **appropriate emphasis** (does the voice stress the right words?) and **natural phrasing** (does it pause at commas, not mid-phrase?).

**Intonation** — the pitch variation across an utterance. Questions rise at the end. Statements fall. Excitement increases pitch range. Sadness flattens it. Early TTS systems had flat intonation, making every sentence sound like a statement. Modern TTS models learn intonation from data, but they still fail on edge cases: sarcasm, rhetorical questions, lists with parallel structure.

**Breathing** — humans breathe while speaking. Long sentences have natural breath pauses. TTS systems that generate audio without breath sounds feel uncanny. Advanced systems like ElevenLabs and Play.ht inject subtle breath noise to increase naturalness. Evaluate this by listening to long utterances: does the voice sound like it could speak indefinitely without air?

**Micro-pauses and filler words** — real humans say "um," "uh," "like," not constantly, but occasionally. Some conversational TTS models add filler words to sound more human. Evaluate whether fillers improve naturalness or feel forced.

**Evaluating naturalness requires A/B tests with human raters.** Play two versions of the same utterance (different TTS systems or settings). Ask: "Which sounds more natural?" Collect pairwise preferences. This is cheaper than MOS but still requires humans.

---

## Intelligibility: Understanding Versus Naturalness

**Intelligibility** is the degree to which listeners can understand the spoken words. It is distinct from naturalness. A robotic voice can be highly intelligible. A natural-sounding voice that mumbles is unintelligible.

**Measuring intelligibility:**

1. **Word Error Rate (WER) in reverse** — play TTS output to humans, ask them to transcribe what they heard. Run ASR on their transcription. High WER means low intelligibility. This is expensive.

2. **Clarity ratings** — ask human raters: "On a scale of 1 to 5, how easy was it to understand this voice?" Faster than transcription, less precise.

3. **Pronunciation accuracy checks** — for domain-specific terms (medical jargon, product names, abbreviations), verify that TTS says them correctly. "Dr." should expand to "doctor" in "Dr. Smith," but stay as "Drive" in "123 Main Dr."

Intelligibility matters more than naturalness in high-noise environments (call centers, emergency services) and for accessibility (screen readers for visually impaired users). A slightly robotic voice that is always clear beats a natural voice that blurs syllables.

**Common intelligibility failures:**
- **Number ambiguity** — "fifteen" versus "fifty," "thirteen" versus "thirty."
- **Homophones** — "their" versus "there," "to" versus "too."
- **Acronyms** — "API" as "A-P-I" versus "ay-pee-eye."
- **Proper nouns** — name pronunciation, especially non-English names.

**Evaluate intelligibility with challenge sets.** Create a dataset of 100 utterances with numbers, acronyms, proper nouns, homophones. Generate TTS. Have human listeners transcribe. Measure accuracy. If accuracy is below 95%, you have an intelligibility problem.

---

## Emotional Expressiveness and Context Matching

A support agent says "I understand that must be frustrating" with no warmth — the words are right, the voice is wrong. A sales agent pitches a product with the cadence of a funeral director. **Emotional expressiveness** is the ability of TTS to convey the appropriate emotion for the context.

**Emotions to evaluate:**
- **Empathy** — warmth, understanding, validation. Critical for healthcare and support.
- **Enthusiasm** — energy, excitement, positivity. Important for sales and marketing.
- **Calm** — reassurance, steadiness. Needed for crisis lines, telehealth.
- **Urgency** — seriousness without panic. Useful for alerts and warnings.
- **Neutrality** — professional, informative. Default for transactional interactions.

**How to evaluate emotional expressiveness:**

1. **Human raters judge emotion match** — give raters the text and context (e.g., "This is a response to a frustrated customer"). Play the TTS. Ask: "Does the voice convey the appropriate emotion?" Rate 1 to 5.

2. **Acoustic feature analysis** — measure pitch variance (high variance suggests emotion), energy (loud versus soft), tempo (fast versus slow). Compare to reference emotional speech. This is faster but less reliable than human judgment.

3. **User perception studies** — in production, survey users: "Did the voice feel empathetic?" "Did the agent sound like they cared?" Perception matters more than acoustic features.

**Modern TTS APIs offer emotion controls.** ElevenLabs supports "stability" and "similarity" sliders. Play.ht offers emotion tags like "cheerful" or "serious." Cartesia's Sonic model supports real-time emotion modulation. But these controls are imprecise. "Cheerful" might sound manic. "Serious" might sound depressed. Evaluate actual output, not just settings.

**Failure modes:**
- **Emotional mismatch** — excited voice delivering bad news.
- **Uncanny positivity** — overly cheerful voice that feels fake.
- **Flat affect** — no emotion at all, sounds robotic even if acoustically clean.

---

## Consistency Across Conversations

A voice agent starts a call with a warm, mid-pitched voice. Five minutes later, after a function call, the voice shifts — slightly higher pitch, different cadence. The user notices. The experience feels broken.

**Consistency** means the voice sounds the same across turns, sessions, and edge cases. It is critical for multi-turn voice agents.

**What to evaluate:**

1. **Voice identity persistence** — does the same voice model sound the same across different utterances? If you regenerate the same text, do you get the same voice?

2. **Cross-turn consistency** — in a 10-turn conversation, does the voice remain stable? Early systems would drift as context grew.

3. **Edge case consistency** — does the voice handle errors, apologies, or clarifications with the same tone as normal responses? Some systems switch to a different voice for error messages.

**Testing consistency:**
- Generate 20 different utterances with the same TTS voice ID. Have human raters listen and report if they hear voice shifts.
- Record a full conversation (10 turns). Ask raters: "Does this sound like one speaker or multiple speakers?"
- Test error paths: generate a happy-path response, then an error message. Compare voice characteristics.

**Streaming TTS introduces consistency challenges.** If TTS generates audio chunk-by-chunk (Chapter 10.2), prosody can shift mid-sentence as new context arrives. Evaluate first-chunk quality: does the voice sound stable even when TTS only has partial text?

**Voice cloning consistency** — if you clone a customer's voice (for accessibility or personalization), evaluate that the clone sounds like the same person across different sentences. Inconsistency in cloned voices is more jarring than in generic TTS, because users expect their own voice to be stable.

---

## Pronunciation Accuracy for Domain Terms

A medical voice agent says "diabetes" as "dee-ah-bee-tees." A financial voice says "$1.5M" as "one point five M." A customer support agent mispronounces the customer's name. **Pronunciation accuracy** for domain-specific terms is a reliability issue, not just a quality issue.

**High-risk categories:**

1. **Proper nouns** — names of people, companies, products. "Nguyen" is not "en-goo-yen." "Porsche" is not "porsh."

2. **Numbers and units** — "$1.2B" should be "one point two billion dollars," not "dollar sign one point two B." "5'10"" is "five feet ten inches."

3. **Abbreviations** — "Dr." is "doctor" in "Dr. Lee" but "drive" in "Main Dr." "St." is "saint" or "street."

4. **Domain jargon** — medical terms (angioedema, tachycardia), legal terms (voir dire, amicus curiae), tech terms (Kubernetes, OAuth).

**How to fix pronunciation issues:**

1. **Phoneme dictionaries** — most TTS APIs support SSML (Speech Synthesis Markup Language) phoneme tags. You can specify exact pronunciation using IPA notation. But this requires manual work per word.

2. **Text normalization preprocessing** — before sending text to TTS, run a normalization layer that expands "$1.5M" to "one point five million dollars," "Dr." to "doctor" where appropriate. Maintain a domain-specific lexicon.

3. **Name pronunciation services** — APIs like NameShouts or custom name pronunciation models. Useful for customer names in CRM-integrated voice agents.

**Evaluating pronunciation accuracy:**
- Build a challenge set of 100 domain terms.
- Generate TTS for each.
- Have domain experts (not just random raters) judge correctness.
- Track error rate. Acceptable threshold is below 2% for production.

**Failure modes:**
- **Homograph errors** — "read" (present tense) versus "read" (past tense). "lead" (metal) versus "lead" (verb).
- **Contextual expansion** — "US" as "United States" versus "us" (pronoun).
- **Number formatting** — "1-800-555-1234" should be "one eight hundred five five five one two three four," not "one minus eight hundred minus..."

---

## Streaming TTS and First-Chunk Quality

In real-time voice agents (Chapter 10.2), TTS must stream audio incrementally. The system cannot wait for the full response before speaking — that would add seconds of latency. Instead, it generates audio chunk-by-chunk as the LLM produces tokens.

**Streaming TTS evaluation challenges:**

1. **First-chunk quality** — the first 500ms of audio set user expectations. If the first chunk is robotic or glitchy, users form a negative impression even if the rest is fine. Evaluate first-chunk MOS separately from full-utterance MOS.

2. **Prosody discontinuities** — when TTS generates the first chunk, it does not know what comes next. If the sentence ends with a question mark, but TTS already committed to a statement intonation in chunk 1, the prosody feels broken. Test by comparing streaming TTS to non-streaming TTS of the same utterance.

3. **Chunk boundary artifacts** — some TTS systems produce clicks, pops, or abrupt cutoffs at chunk boundaries. Listen for smooth transitions. Automate detection with spectral analysis (sudden energy spikes indicate artifacts).

**How to evaluate streaming TTS:**
- Record streaming TTS output for 50 real conversations.
- Have human raters listen for "any weird pauses or breaks."
- Compare MOS of streaming versus batch TTS. If streaming MOS is more than 0.2 points lower, you have a quality problem.

**Modern streaming TTS systems** (Cartesia, ElevenLabs Turbo, Play.ht Streaming) handle chunk boundaries well, but they still struggle with prosody lookahead. They cannot stress a word in chunk 1 if the reason for emphasis only appears in chunk 3. This is a fundamental tradeoff between latency and quality.

---

## Voice Cloning and Custom Voices

**Voice cloning** enables creating a synthetic voice that sounds like a specific person. Use cases: accessibility (preserving a user's voice after surgery), personalization (a brand's signature voice), localization (same speaker in multiple languages).

**Evaluating cloned voices:**

1. **Similarity to target** — play the original speaker's voice and the clone side-by-side. Ask raters: "Are these the same speaker?" Measure identification accuracy. Professional voice clones should achieve above 80% identification.

2. **Naturalness of the clone** — even if the clone sounds like the target, does it sound natural? Or does it fall into the uncanny valley (recognizable but creepy)? Measure MOS separately for cloned voices.

3. **Range and expressiveness** — can the clone handle different emotions, prosody, and contexts? Some clones sound perfect for neutral narration but fail on emotional speech.

4. **Robustness to novel text** — clones are often trained on limited data (5-10 minutes of audio). Test them on text the original speaker never said. Do they generalize, or do they sound robotic on out-of-distribution utterances?

**Uncanny valley risk** — voice clones can sound almost-but-not-quite right, triggering discomfort. Evaluate with qualitative feedback: "Does this voice sound creepy or off-putting?" If more than 10% of raters say yes, the clone is not ready for production.

**Ethical and legal considerations** — cloning a voice without consent is risky. Evaluate not just quality, but also user perception: "If this is a cloned voice, does it feel deceptive?" Some users find cloned voices unethical even when disclosed.

---

## 2026 TTS Landscape and Evaluation Patterns

In 2026, TTS has converged on neural generative models, but the ecosystem is fragmented. Different providers excel at different axes.

**Major TTS providers:**

1. **ElevenLabs** — highest naturalness MOS (4.5+), excellent emotional expressiveness, slower latency. Best for pre-recorded content and high-quality narration.

2. **Play.ht** — strong balance of quality and speed. Good streaming support. Wide voice library. Popular for customer-facing voice agents.

3. **Cartesia Sonic** — ultra-low latency (80-100ms first chunk), real-time emotion control. Optimized for live voice agents. Slightly lower MOS (4.0-4.2) than ElevenLabs.

4. **OpenAI TTS** — integrated with ChatGPT voice. Good quality, moderate latency. Limited emotion controls.

5. **Google Cloud TTS / Azure TTS** — enterprise-grade, multilingual, SSML support. More robotic than newer models (MOS 3.5-4.0).

**Evaluation patterns for multi-provider setups:**

- **Run MOS benchmarks quarterly** — TTS models improve rapidly. A provider that scored 3.8 last quarter might now score 4.2. Re-evaluate when switching versions.

- **Use provider-specific evaluation sets** — ElevenLabs handles British accents well but struggles with Mandarin. Azure excels at Asian languages. Build language-specific eval sets.

- **A/B test in production** — route 10% of traffic to a new TTS provider. Measure user satisfaction, conversation completion rate, escalation to human agents. Quality metrics matter, but business metrics matter more.

**Emerging patterns:**

- **Emotion-controllable TTS** — APIs that accept emotion parameters (valence, arousal, dominance). Evaluate by testing extreme settings: does "high arousal" sound excited or manic?

- **Multi-language voice consistency** — same voice ID across languages. Evaluate whether the Spanish version sounds like the same speaker as the English version.

- **Real-time voice-to-voice** — systems that transform input speech to output speech without intermediate text (useful for accent conversion, emotion modulation). Evaluate latency and identity preservation.

---

## Failure Modes and Red Flags

**Common TTS quality failures:**

1. **Robotic monotone** — no prosody variation. Every sentence sounds the same. MOS below 3.5.

2. **Uncanny valley** — technically clean but emotionally wrong. Overly smooth, no micro-imperfections. Users report "creepiness."

3. **Pronunciation glitches** — mispronounces common words, inconsistent across utterances. "Live" as "lyve" instead of "liv" in "live demo."

4. **Emotional mismatch** — cheerful voice delivering bad news, flat voice in a high-energy context.

5. **Consistency drift** — voice changes mid-conversation, different prosody after an API retry.

6. **Artifacts and distortion** — clicks, pops, buzzing, clipping. Often caused by audio codec mismatches or poor network transmission.

7. **Gender or accent bias** — TTS performs worse on non-Western names, non-standard dialects. Evaluate with diverse test sets.

**Enterprise expectations for TTS quality:**

- **MOS above 4.0** for customer-facing applications.
- **Intelligibility above 95%** for accessibility and safety-critical domains.
- **Pronunciation accuracy above 98%** for domain-specific terms.
- **Consistency across 100-turn conversations** without voice drift.
- **Emotional expressiveness rated "appropriate" by 80% of human evaluators.**

If any of these thresholds are missed, TTS is a blocker to launch.

---

## Minimal Evaluation Template

```yaml
tts_quality_eval:
  dimensions:
    - naturalness_mos:
        method: human_rating
        scale: 1_to_5
        raters_per_sample: 10
        sample_size: 50
        target_score: 4.0

    - intelligibility:
        method: transcription_accuracy
        sample_size: 100
        target_accuracy: 0.95
        challenge_categories: [numbers, proper_nouns, acronyms]

    - emotional_expressiveness:
        method: context_match_rating
        contexts: [empathy, enthusiasm, calm, urgency]
        raters_per_sample: 5
        target_match_rate: 0.80

    - consistency:
        method: cross_turn_comparison
        conversation_length: 10_turns
        raters: 5
        question: "Does this sound like one speaker?"

    - pronunciation_accuracy:
        method: expert_review
        domain_terms: 100
        target_accuracy: 0.98

  automated_monitoring:
    - utmos_prediction:
        frequency: all_samples
        alert_threshold: 3.5

    - artifact_detection:
        method: spectral_analysis
        check_for: [clicks, pops, clipping]

  evaluation_frequency:
    human_mos: quarterly
    automated_mos: continuous
    challenge_sets: on_provider_change
```

---

## Interview Questions: TTS Quality Evaluation

**Q1: Your voice agent's MOS score is 4.2, but users complain it sounds "robotic." What is happening?**

MOS measures overall quality, but it averages across multiple dimensions — naturalness, intelligibility, clarity. A voice can score high on intelligibility and clarity (clean audio, no artifacts) but low on naturalness (flat prosody, no emotion). Users perceive "robotic" as lack of naturalness, not lack of clarity.

**Fix:** Run a targeted naturalness evaluation. Ask raters specifically: "Does this sound like a real person?" not "How would you rate this voice?" Analyze prosody: is there variation in pitch, stress, and tempo? If not, switch TTS providers or tune emotion settings. ElevenLabs and Play.ht typically score higher on naturalness than Azure or Google Cloud TTS. Alternatively, the mismatch might be emotional expressiveness — users expect empathy in support contexts, and a neutral voice feels robotic even if technically natural.

**Q2: You are evaluating two TTS providers. Provider A has MOS 4.3, latency 800ms. Provider B has MOS 3.9, latency 150ms. Which do you choose?**

**It depends on the use case.** For pre-recorded content (podcasts, audiobooks, onboarding flows), choose Provider A. Quality matters more than latency when users are not in real-time conversation. For real-time voice agents (customer support, telehealth), 800ms latency is unacceptable. Users will perceive delays as "the agent is thinking too long." Choose Provider B and work to improve quality (tune settings, add SSML, test different voice IDs).

**Better approach:** Test if Provider A offers a streaming mode. Latency metrics often assume batch TTS. Streaming can reduce time-to-first-audio significantly. If Provider A can stream and achieve 200ms first-chunk latency, you get both quality and speed. Also evaluate user perception: does the 0.4 MOS difference matter to your users? Run an A/B test in production. If conversation completion rate and satisfaction are similar, choose the faster option.

**Q3: How do you evaluate pronunciation of customer names in a CRM-integrated voice agent?**

**Build a name pronunciation pipeline.** First, integrate a name pronunciation service (NameShouts, custom model trained on phonetic name data). Second, create an evaluation set: pull 200 customer names from your CRM, covering diverse origins (Chinese, Arabic, Eastern European, etc.). Generate TTS for "Hello, [Name]" for each. Have human raters (ideally native speakers of each name's origin language) judge correctness.

**Measure accuracy per demographic.** If you get 95% accuracy for Western names but 60% for Asian names, your TTS has a bias issue. Fix by adding phoneme hints (SSML tags), using a TTS provider with better multilingual training (Azure, Google), or allowing customers to record their own name pronunciation and splicing it into TTS output.

**In production, monitor escalations.** If a customer asks "Can you spell my name?" or "You are saying it wrong," log it. Track which names cause issues. Build a correction dictionary over time.

**Q4: Your streaming TTS has artifacts (clicks, pops) at chunk boundaries. How do you debug and fix this?**

**Debugging steps:**

1. **Record the raw audio stream.** Capture each chunk separately and the merged stream. Listen to identify whether artifacts are in individual chunks or only at boundaries.

2. **Check audio format consistency.** Streaming TTS often sends chunks in different sample rates or bit depths. If chunk 1 is 24kHz and chunk 2 is 16kHz, merging will produce artifacts. Verify all chunks are in the same format.

3. **Analyze spectral plots.** Use a tool like Audacity to visualize waveforms. Clicks and pops show as sudden spikes. If spikes align with chunk timestamps, the issue is boundary handling.

4. **Test chunk overlap settings.** Some streaming TTS APIs support overlap (chunk 2 starts slightly before chunk 1 ends). If overlap is too small, you get gaps. If too large, you get echoes. Tune overlap duration.

**Fixes:**
- Enable crossfading between chunks (if supported by TTS API).
- Buffer chunks and apply smoothing at boundaries (audio engineering technique called "windowing").
- Switch to a TTS provider with better streaming quality (Cartesia Sonic is optimized for clean chunk boundaries).
- If artifacts persist, fall back to batch TTS for high-stakes interactions (e.g., reading medical instructions).

**Q5: You are launching a voice agent in a new language (Mandarin). How do you evaluate TTS quality when you do not speak the language?**

**Hire native-speaker raters.** MOS evaluation requires human judgment, and only native speakers can assess naturalness, prosody, and emotional expressiveness accurately. Recruit Mandarin-speaking raters through platforms like Prolific, Scale AI, or in-house bilingual staff.

**Build language-specific challenge sets.** Mandarin has tonal distinctions (mā, má, mǎ, mà), so pronunciation accuracy is critical. Create a challenge set of tone-minimal pairs. Generate TTS for each. Have raters judge if tones are correct.

**Use bilingual quality checks.** If you have a bilingual team member, they can spot-check for major failures (completely wrong pronunciation, inappropriate emotion). But full evaluation still requires native speakers.

**Benchmark against local providers.** US-based TTS providers (ElevenLabs, OpenAI) may underperform on Mandarin compared to Chinese providers (Alibaba Cloud, iFlytek). Run MOS comparisons. If a local provider scores 4.2 and your US provider scores 3.6, switch.

**Monitor production feedback.** Even with pre-launch evaluation, users will surface issues you missed. Track Mandarin-language escalations, user complaints, and satisfaction scores. If satisfaction is significantly lower for Mandarin than English, you have a quality gap.

---

**Next:** Chapter 10.5 — End-to-End Voice Agent Testing (simulating full conversations, evaluating multi-turn coherence and user experience).
