# Section 1 — Eval Strategy & Operating Model

## Chapter 1

### Plain English

An evaluation strategy answers one simple but critical question:

**How do we know our AI system is actually good, and how do we decide when it is safe to ship?**

Without a clear eval strategy, teams argue emotionally, ship based on vibes, and only discover problems after users complain or incidents happen.

An eval operating model defines:
- what we evaluate
- why we evaluate it
- when we evaluate it
- who owns the results
- how decisions are made from those results

This section is the foundation of every serious AI system in 2026.

If this is weak, everything built on top of it collapses.

---

### Why This Section Exists

Modern AI systems fail in subtle ways:
- they sound confident but are wrong
- they work well in demos but fail in production
- they regress silently when prompts or models change
- they behave differently across customers or use cases

Traditional software testing does not catch these failures.

An eval strategy exists to:
- replace intuition with evidence
- make quality measurable
- prevent silent regressions
- align engineers, product, leadership, and compliance
- make shipping decisions defensible

In 2026, **evaluation is not a QA activity**.
It is a **core system design discipline**.

---

### What an Eval Strategy Actually Is (2026 Meaning)

An eval strategy is **not**:
- a single accuracy score
- a benchmark leaderboard
- a one-time test before launch

An eval strategy **is**:
- a continuous decision system
- tied directly to product goals
- integrated into development and deployment
- owned like infrastructure, not experiments

Technically, it defines:
- quality dimensions
- task categories
- acceptance thresholds
- failure definitions
- release criteria
- feedback loops

---

### Core Components of an Eval Strategy

#### 1. Eval Charter (Purpose & Scope)

The eval charter states:
- what the system is responsible for
- what it is not responsible for
- what success means at a high level

Examples:
- This system provides grounded answers from internal documents.
- This system may assist but must never make irreversible decisions.
- This system must escalate uncertainty instead of guessing.

The charter prevents scope creep and overconfidence.

---

#### 2. Quality Dimensions

Quality is multi-dimensional in 2026.

Common dimensions include:
- correctness
- grounding (uses the right source)
- completeness
- safety
- usefulness
- clarity
- latency
- cost efficiency

Not every task uses all dimensions equally.
Your eval strategy defines **which dimensions matter for which tasks**.

---

#### 3. Task Taxonomy

Different tasks require different evaluation logic.

Typical task categories:
- conversational chat
- retrieval-augmented answers
- extraction and classification
- tool-calling workflows
- multi-step agent tasks
- real-time voice interactions

A strong eval strategy **never evaluates everything the same way**.

---

#### 4. Release Gates

Release gates define:
- what must pass before something can ship
- what blocks a release
- what requires human review

Examples:
- No critical safety failures allowed
- No regression against the golden dataset
- Latency must stay under X for voice tasks

Release gates turn evals into **real decision power**, not reports.

---

#### 5. Ownership & Accountability

In mature systems:
- evals have owners
- failures have escalation paths
- metrics are reviewed regularly

Ownership answers:
- who maintains datasets
- who reviews failures
- who signs off on releases
- who responds to incidents

Without ownership, evals rot.

---

#### 6. Continuous Improvement Loop

Eval strategy is never static.

A proper loop:
- collect failures from production
- feed them into eval datasets
- update metrics and thresholds
- review results on a fixed cadence (weekly or bi-weekly)

This is how systems improve instead of repeating mistakes.

---

### Enterprise Perspective

Enterprises care about:
- defensibility (why did this ship?)
- auditability
- consistency across customers
- risk containment

An eval strategy allows:
- consistent quality standards
- evidence for compliance teams
- predictable behavior at scale

This is why eval systems are often reviewed at **staff or principal level**, not junior level.

---

### Founder / Startup Perspective

Founders care about:
- shipping fast without breaking trust
- avoiding catastrophic mistakes
- learning quickly from real usage

A good eval strategy:
- reduces fear of shipping
- allows fast iteration with guardrails
- creates leverage even with small teams

Startups that skip eval strategy move fast until they stop entirely.

---

### Common Failure Modes

- Treating evals as benchmarks instead of decision systems
- Using one metric for all tasks
- Ignoring cost and latency as quality dimensions
- No clear release gates
- No owner for eval failures
- Running evals only before launch, not continuously

Recognizing these failures is already senior-level thinking.

---

### Interview-Grade Talking Points

You should be able to explain:

- Why evals are different from traditional tests
- How eval strategy connects to product decisions
- Why quality is multi-dimensional
- How release gates prevent silent regressions
- Why ownership matters more than metrics

If you can do that clearly, you already outperform most candidates.

---

### Completion Checklist

You are done with this section when you can:

- Explain what an eval strategy is in plain English
- Describe its core components without notes
- Explain why evals must be continuous
- Explain how evals affect shipping decisions
- Identify eval strategy failures in real systems

Do not move on until this feels obvious.

---

### How This Section Connects Forward

This section defines **the rules of the game**.

Next sections will answer:
- what data we evaluate on (Section 02)
- how we label and measure quality (Sections 03-07)
- how evals work for agents, RAG, and voice (Sections 08-10)

Everything else depends on this.

---

## Chapter 2

### 2.1 — Task-Specific Definitions of "Good" (Chat / RAG / Agents / Voice)

When teams say "the model is good," they usually mean different things depending on the product.

In enterprise evals, we don't start with metrics. We start with a clear definition of "good" for each task type. That becomes your "contract" for building rubrics, datasets, automated tests, and release gates.

You'll see this idea across modern eval tooling too: curated datasets + task-specific evaluators + regression checks (OpenAI Evals, LangSmith evals, RAGAS metrics, etc.).

---

### The Core Idea: "Good" = Outcome + Constraints

For any AI feature, define "good" like this:

**Outcome:** what the user must get

**Constraints:** what must NOT happen (safety, privacy, policy, cost, latency, tone)

**Evidence:** what we can check in an eval (logs, citations, tool calls, transcripts, human scoring)

If you only define outcomes, you'll ship something that "sounds right" but breaks in production.

---

### A) "Good" for Chat (General Assistant / Support Bot)

#### Mechanics (How It Works)

Chat is mostly: interpret intent → decide what info is needed → respond clearly → stay safe.

#### What "Good" Means (Task Definition)

A "good" chat answer is:

- Directly answers the user's question
- Correct enough for the use case
- Easy to understand
- Honest about uncertainty
- Safe + policy-aligned
- Doesn't waste time (no fluff when the user wants speed)

#### Knobs & Defaults (What You Set)

- **Answer style:** short vs detailed, formal vs friendly
- **Ask-clarify behavior:** when to ask questions vs make best-effort assumptions
- **Uncertainty behavior:** when to say "I'm not sure" vs "Here's the best estimate"
- **Safety behavior:** when to refuse, when to redirect, when to give safer alternatives
- **Latency/cost targets:** "fast enough" matters in enterprise support

#### Failure Modes (Symptoms → Root Causes)

- Sounds confident but wrong → no grounding, poor knowledge boundaries, missing verification
- Over-explains → no "brevity" dimension in the definition of good
- Asks too many questions → "clarify-first" default too aggressive
- Too agreeable (sycophant behavior) → reward signals favor "pleasantness" over correctness/safety (this has been a real production lesson in the industry)

#### Debug Playbook

- Collect 10–30 real conversations per top user intent
- For each: label intent, what a great answer would contain, what would be unacceptable
- Add "hard cases": angry user, vague question, unsafe request, conflicting constraints

#### Enterprise Expectations

- A supportable definition: different reviewers score similarly
- A measurable definition: ties to ticket resolution, CSAT, escalations, handle time
- A regression-proof definition: you can say "this got worse" before customers do

#### Interview-Ready Talking Points

"Chat quality isn't one thing. I define 'good' per intent: accuracy, clarity, actionability, safety, and efficiency — then I build rubrics and datasets per intent."

---

### B) "Good" for RAG (Retrieval-Augmented Generation)

RAG is two systems glued together: retrieval + generation. "Good" must cover both.

#### Mechanics

User asks → system retrieves sources → model answers using sources → (optionally) cites them.

#### What "Good" Means (Task Definition)

A "good" RAG answer is:

- Relevant to the question (answers the right thing)
- Grounded in retrieved context (no invented facts)
- Uses the best parts of the sources (not random snippets)
- Admits when sources don't support an answer
- Cites/quotes appropriately when required
- Keeps scope tight (doesn't "freestyle" outside the documents)

This is why RAG evals commonly split into dimensions like answer relevancy and faithfulness/grounding.

#### Knobs & Defaults

**Retriever knobs:**
- chunk size, overlap
- top-k
- filters (time, permissions, tenant/org)
- reranking on/off
- query rewriting on/off

**Generator knobs:**
- "use only provided context"
- refusal/abstain behavior when context is missing
- citation format requirement
- verbosity level

#### Failure Modes

- Hallucination with high confidence → generator not constrained, weak grounding checks
- Right docs retrieved, wrong answer → model didn't use the evidence (prompt/format issue)
- Wrong docs retrieved, answer looks plausible → retrieval failure masked by fluent text
- Good for easy queries, fails on long-tail → coverage gap in dataset + taxonomy

#### Debug Playbook

- Check retrieval first (did we fetch the right evidence?)
- If retrieval is good, check instruction/prompt (does it force evidence use?)
- Add eval cases where the correct behavior is "I can't answer from these sources."
- Track: "how often do we answer anyway when we shouldn't?"

#### Enterprise Expectations

- Permission-safe retrieval (no data leaks across tenants)
- Auditability: "show me why the bot said that"
- Repeatability: same question should not randomly flip answers
- Clear "abstain" rules for compliance-heavy domains (health, finance, legal)

#### Interview-Ready Talking Points

"In RAG, 'good' must be defined across retrieval and generation. I score relevancy + grounding separately so I can tell whether regressions come from the retriever or the model."

---

### C) "Good" for Agents (Tool-Using, Multi-Step Workflows)

Agents are not just "chat." They are decision systems that take actions.

#### Mechanics

Interpret goal → plan → call tools (API/DB/browser) → handle errors → update state → finish.

#### What "Good" Means (Task Definition)

A "good" agent:

- Completes the user's goal (not just "talks about it")
- Uses tools correctly (right tool, right params, right sequence)
- Is safe with actions (no risky operations without confirmation)
- Is reliable under failure (timeouts, bad data, tool errors)
- Keeps clean state (doesn't lose context or repeat actions)
- Leaves an audit trail (what it did, why, results)

#### Knobs & Defaults

- Allowed tools + permissions
- Confirmation rules ("always ask before sending email / charging card / deleting")
- Retry rules (how many retries, backoff)
- Time budget (max steps, max tool calls)
- Memory rules (what can be stored, retention time)
- Escalation rules (when to hand off to a human)

#### Failure Modes

- Tool thrashing (calls tools in loops) → no stop conditions, poor planning
- Silent partial success → missing "done criteria"
- Repeated actions (duplicate charges/messages) → no idempotency keys, weak state
- Unsafe actions → missing confirmations and role-based permissions

#### Debug Playbook

- Require structured traces: plan → actions → results → final outcome
- Add "chaos tests": tool timeout, malformed API response, empty search results
- Add "must-confirm" tests: agent tries a risky action and should stop

#### Enterprise Expectations

- Permissioning, audit logs, and least-privilege by default
- "Break glass" controls and human approval for high-risk actions
- SLA thinking: success rate, time-to-complete, cost-per-task

#### Interview-Ready Talking Points

"For agents, 'good' is action correctness + safety + recoverability. I evaluate not only the final answer, but the tool calls, state handling, and confirmation behavior."

---

### D) "Good" for Voice AI (Calls, Real-Time Assistants)

Voice adds constraints that text doesn't: time, turn-taking, audio quality, and human comfort.

#### Mechanics

Audio in → speech-to-text → reasoning + policy → text-to-speech → audio out (fast).

#### What "Good" Means (Task Definition)

A "good" voice agent:

- Understands the caller (even with accents, noise, interruptions)
- Responds fast enough to feel natural
- Handles barge-in (caller interrupts) without breaking
- Stays calm and clear (tone matters more in voice)
- Never leaks private info (voice is high-risk for identity data)
- Escalates gracefully when unsure (handoff to human or fallback)

#### Knobs & Defaults

- Latency targets (end-to-end)
- Barge-in on/off, interruption policy
- "Ask to repeat" thresholds
- Voice style (speed, warmth, formality)
- Verification rules (identity checks before account actions)

#### Failure Modes

- Feels slow → pipeline latency, long prompts, too many tool calls
- Talks over the user → barge-in handling is wrong
- Mishears names/numbers → ASR weaknesses, no confirmation loop for critical fields
- Over-confident on low-quality audio → missing "confidence gating"

#### Debug Playbook

- Test with: noisy audio, speakerphone, fast talkers, interruptions, accents
- Add "critical fields" checklist: phone numbers, addresses, dates must be confirmed
- Keep transcripts + timestamps for every turn (so you can find where it went wrong)

#### Enterprise Expectations

- Compliance: call recording rules, consent, PII redaction, retention controls
- Monitoring: abandonment rate, transfer rate, time-to-resolution
- Clear escalation paths and safe "can't help with that" scripts

#### Interview-Ready Talking Points

"Voice quality is mostly latency + turn-taking + correct capture of critical info. I define 'good' with comfort and safety constraints, not only 'correct answers.'"

---

### A Practical Template You Can Reuse (Definition of "Good" Card)

Copy this into your notes for each feature:

- **Feature / Task:**
- **Primary user goal:**
- **User context (who / where / why):**
- **What a great output must include (3–7 bullets):**
- **What is unacceptable (3–7 bullets):**
- **Edge cases we must handle:**
- **When we must refuse / escalate:**
- **Evidence we log for audits (traces, citations, transcripts):**
- **Success signals (business):**
- **Failure signals (business):**

---

### 2.2 — Rubrics Humans Can Score Consistently

**Goal:** Turn "good" into a scoring system that different people can apply the same way, even at scale.

A rubric is not "rate 1–5."
A real rubric is a **shared contract**: *what to look for, what counts as success, what counts as failure, and examples that anchor each score.*

---

#### 1) What a Rubric Is (and What It Is Not)

##### Mechanics (How It Works)

A strong rubric has 4 parts:
1. **Dimensions** (the "quality lenses" you score)
2. **Scales** (clear score meanings, usually 0–3 or 1–5)
3. **Anchors** (examples of what each score looks like)
4. **Rules** (how to decide the final score, what to do when unsure)

##### Knobs & Defaults (What You Actually Set)

- **Scale size:** default **0–3** for speed and consistency
  - 0 = fail, 1 = weak, 2 = good, 3 = great
- **Number of dimensions:** default **4–7** (more than that becomes noisy)
- **Weighting:** default **no weighting** at first
  - Start simple. Add weights only when you've proven you need them.
- **Pass/Fail gate:** always define a **hard fail dimension** for safety/PII
  - If that fails, the whole response fails regardless of other scores.

##### Failure Modes (Symptoms → Root Causes)

- Reviewers disagree a lot → vague dimension names ("helpful"), no anchors
- Scores drift over time → no calibration, no "gold" reference set
- People game the rubric → rubric rewards style over correctness
- Rubric is too slow → too many dimensions, too long instructions

##### Debug Playbook (Fix the Rubric)

- Reduce dimensions, simplify wording, add anchors
- Add a "tie-break rule" for borderline cases
- Run calibration sessions weekly until stable

##### Enterprise Expectations

- Rubrics must be:
  - **Fast enough** for throughput
  - **Consistent enough** for decisions (release gates)
  - **Auditable** (you can explain why a score happened)

##### Interview-Ready Talking Points

"I build rubrics with clear dimensions, anchored examples, and calibration so scoring is repeatable across raters and time."

---

#### 2) The Golden Rule: Score Observable Behavior, Not Vibes

Bad dimension:
- "Was it smart?"

Good dimension:
- "Did it answer the user's question correctly using only the allowed sources?"

If a reviewer can't point to evidence in the output/logs, it's not rubric-friendly.

---

#### 3) The Universal Rubric Skeleton (Copy/Paste)

Use this as the base for most AI tasks.

##### Dimension A — Correctness / Validity (0–3)

- **0 (Fail):** Wrong, misleading, or contradicts known facts/context
- **1 (Weak):** Partly correct but key errors or missing critical pieces
- **2 (Good):** Correct for the task with minor gaps
- **3 (Great):** Fully correct + covers edge cases appropriately

##### Dimension B — Relevance / Goal Fit (0–3)

- **0:** Doesn't answer the question / goes off-topic
- **1:** Partially addresses the goal but misses main intent
- **2:** Addresses the intent directly
- **3:** Addresses intent + anticipates obvious next step without drifting

##### Dimension C — Clarity / Usability (0–3)

- **0:** Confusing, hard to follow, poorly structured
- **1:** Some clarity but messy or too dense
- **2:** Clear, structured, easy to act on
- **3:** Clear + "copy/paste ready" where needed + great formatting

##### Dimension D — Safety / Policy / Privacy (Hard Gate)

Score as **PASS / FAIL** (not 0–3).
- **FAIL examples:** unsafe instructions, PII leaks, disallowed content, policy bypass guidance
- **PASS:** safe handling + correct refusal/redirection when required

##### Optional Dimension E — Grounding (for RAG) (0–3)

- **0:** Makes claims not supported by sources
- **1:** Some grounding but mixes in unsupported claims
- **2:** Mostly grounded; minor unsupported wording
- **3:** Fully grounded; cites/uses sources correctly

##### Optional Dimension F — Efficiency (for Agents/Ops) (0–3)

- **0:** Wasteful loops or excessive tool calls
- **1:** Inefficient but completes
- **2:** Reasonably efficient
- **3:** Efficient + minimal steps + good defaults

**Final scoring rule (simple default):**
- If Safety = FAIL → overall = FAIL
- Else overall score = average of selected dimensions, rounded down
- Record "main reason" in one short sentence

---

#### 4) Task-Specific Rubrics (Chat / RAG / Agents / Voice)

##### 4.1 Chat Rubric (Human Scoring)

**Dimensions (recommended):**
- Correctness (0–3)
- Relevance (0–3)
- Clarity (0–3)
- Safety gate (PASS/FAIL)
- Optional: Helpfulness / Next-step value (0–3)

**Anchors (quick examples):**
- **Score 0 correctness:** confident wrong answer
- **Score 1:** mostly correct but misses the user's main need
- **Score 2:** correct and usable
- **Score 3:** correct + adds the exact next step the user likely needs

**What usually goes wrong:**
- Reviewers reward "polite tone" over correctness
- Reviewers punish short answers even when user asked for speed

**Debug playbook:**
- Add examples of "good short answer"
- Add examples of "polite but wrong" as hard fails on correctness

---

##### 4.2 RAG Rubric (Human Scoring)

**Dimensions (recommended):**
- Answer Relevance (0–3)
- Grounding / Faithfulness (0–3)
- Citation Quality (0–3 or PASS/FAIL depending on your needs)
- Clarity (0–3)
- Safety gate (PASS/FAIL)

**Citation Quality anchors:**
- **0:** No citations when required, or cites wrong source
- **1:** Cites something but not tied to key claims
- **2:** Cites key claims; minor formatting issues
- **3:** Citations directly support the important claims and are easy to audit

**What usually goes wrong:**
- Reviewers forget to check whether sources actually support the claim
- Models sound correct but are not source-backed

**Debug playbook:**
- Force reviewers to answer: "Which sentence is supported by which source?"
- Add "must abstain" examples: correct behavior is "not enough info in sources"

---

##### 4.3 Agent Rubric (Human Scoring)

Agents must be evaluated on both **outcome** and **behavior**.

**Dimensions (recommended):**
- Task Completion (0–3)
- Tool Correctness (0–3)
- Safety gate (PASS/FAIL)
- Efficiency (0–3)
- Recovery / Robustness (0–3)

**Tool Correctness anchors:**
- **0:** wrong tool, wrong params, or unsafe action taken
- **1:** eventually reaches correct tool use but messy/retries
- **2:** correct tool use with minor inefficiency
- **3:** clean tool sequence + good checks + idempotent behavior

**Recovery anchors:**
- **0:** fails on a tool error and gives up or loops forever
- **1:** notices error but responds poorly (no workaround)
- **2:** recovers with a reasonable fallback
- **3:** recovers + explains briefly + continues safely

**What usually goes wrong:**
- "It explained the steps" gets scored as success even if it didn't do them
- Duplicate actions happen because state/idempotency isn't enforced

**Debug playbook:**
- Require a trace summary: tool calls + outputs + final result
- Add tests where the correct behavior is "ask for confirmation"

---

##### 4.4 Voice Rubric (Human Scoring)

Voice needs extra dimensions because UX is different.

**Dimensions (recommended):**
- Understanding Accuracy (0–3)
- Turn-taking (0–3)
- Latency feel (0–3) *human perception, not exact timing*
- Safety/Privacy gate (PASS/FAIL)
- Tone / Comfort (0–3)
- Task Completion (0–3)

**Turn-taking anchors:**
- **0:** talks over user / ignores interruptions
- **1:** awkward interruptions handled inconsistently
- **2:** mostly smooth
- **3:** smooth barge-in + confirms critical info naturally

**What usually goes wrong:**
- Mishears critical fields (numbers, names) and doesn't confirm
- Over-confident when audio quality is poor

**Debug playbook:**
- Add a rule: "Critical fields must be confirmed"
- Test noisy audio, accents, interruptions, fast speakers

---

#### 5) Consistency System: How You Make Humans Score the Same

##### 5.1 Rater Instructions (Must Be Short)

- Read the user request first
- Score each dimension independently
- Use anchors; do not invent new criteria
- When unsure, choose the lower score and write why in 1 line

##### 5.2 Calibration Workflow (Enterprise Default)

- Create a **Gold Set** of ~50 examples:
  - 20 easy, 20 normal, 10 hard/edge cases
- Two raters score the same items
- Discuss disagreements, update anchors
- Repeat weekly until scores stabilize, then monthly

##### 5.3 Adjudication Rules (When Raters Disagree)

- If Safety gate differs: safety reviewer wins by default
- If correctness differs by 2+ points: escalate to senior reviewer
- Record the final decision and add it to the Gold Set

##### 5.4 Keep the Rubric from Rotting

- Every time you see a new failure type, add:
  - 1–3 new examples
  - a new anchor or rule
- Version the rubric (v1, v1.1, v2) so changes are trackable

---

#### 6) Ready-to-Use Assets

##### 6.1 One-Page Rubric Sheet (Copy/Paste)

**Task type:** Chat / RAG / Agent / Voice

**Dimensions:**
- Correctness (0–3)
- Relevance (0–3)
- Clarity (0–3)
- Safety/Privacy (PASS/FAIL)
- (Optional) Grounding (0–3)
- (Optional) Tool Correctness (0–3)
- (Optional) Turn-taking (0–3)

**Overall rule:** Safety FAIL => Overall FAIL

**Main reason:** (1 sentence)

##### 6.2 Reviewer Checklist (Fast)

- Did it answer the question?
- Is it correct?
- Is anything invented (especially in RAG)?
- Is anything unsafe or private?
- Is it clear and usable?

---

#### 7) What Usually Goes Wrong (and How to Prevent It)

**Common problems:**
- "Helpful" becomes "long"
- Style gets rewarded more than truth
- Reviewers apply personal preferences
- Rubric is too slow to run at scale

**Fixes:**
- Keep rubric short (4–7 dimensions)
- Use anchors and a Gold Set
- Train reviewers with calibration
- Make Safety a hard gate

---

#### 8) Interview-Ready Summary

"I don't rely on vibes. I define task-specific dimensions, use a small scale like 0–3, add anchors and a gold dataset, calibrate raters regularly, and enforce safety/privacy as a hard fail. That makes human eval repeatable and usable for release decisions."

---

### 2.3 — Good vs Bad Patterns (Examples)

**What we're doing here:**
Now that we can *define* "good" (2.1) and *score* it (2.2), we need examples people can learn from fast.
This chapter gives **repeatable patterns**: what strong outputs look like, what weak outputs look like, and why.

**How to use this chapter in real teams:**
- Put these examples in your "Gold Set" for calibrating reviewers
- Use them in onboarding for new raters and new engineers
- Use them as "unit tests" for prompts, tools, and policies

---

#### 1) Chat Patterns (General Assistant)

##### Pattern: Direct + Structured Answer

**Good:**
- Starts with the answer (or the next action)
- Uses short steps or bullets
- Matches the user's requested style (short vs detailed)

**Bad:**
- Long preamble
- Explains the world before answering
- Adds unnecessary assumptions

**Why it matters:**
- Users judge quality by "Did it help me now?" not "Was it fancy?"

**Mini example:**
User: "Write a short LinkedIn message to a recruiter asking for the JD."
- Good: 4–6 lines, clear ask, friendly tone, no fluff
- Bad: 3 paragraphs about your background before asking

---

##### Pattern: Honest Uncertainty (Without Being Useless)

**Good:**
- States what is known vs unknown
- Offers a safe next step: "Here's how to verify"
- Gives best-effort help without pretending certainty

**Bad:**
- Confident guessing
- Or the opposite: "I can't help" when you clearly can

**What usually goes wrong:**
- Models learn "confidence sounds good"
- Reviewers reward confident tone even when wrong

---

##### Pattern: Clarify Only When It Truly Blocks Progress

**Good:**
- Makes a reasonable assumption and proceeds
- Asks 1–2 key questions only if necessary

**Bad:**
- Asks 5 questions before doing anything
- Or never asks and outputs something unusable

**Debug tip:**
- Track "clarification rate" and whether clarifications were actually needed

---

#### 2) RAG Patterns (Retrieval + Grounded Answers)

##### Pattern: "Answer Only from Sources" Behavior

**Good:**
- Uses only retrieved context
- Clearly separates: "What sources say" vs "What we don't know"
- Cites key claims when required

**Bad:**
- Adds external facts not in the documents
- Uses citations that do not support the claim (fake grounding)

**Mini example:**
Question: "What is our refund policy for annual plans?"
- Good: quotes/paraphrases exact policy language and cites it
- Bad: invents "30-day refund" because it's common elsewhere

---

##### Pattern: Correct Abstain When Evidence Is Missing

**Good:**
- "I don't have enough info in the provided documents to answer."
- Suggests what doc/field is needed, or offers to search again with a better query

**Bad:**
- Makes up the policy anyway
- Or blames the user: "Your question is unclear" when it's not

**What usually goes wrong:**
- Teams forget to include "must abstain" cases in eval datasets — so the model never learns that abstaining can be "correct."

---

##### Pattern: Retrieval Failure Diagnosis (Don't Blame the Model Too Early)

**Good:**
- When answer is bad, checks:
  - Did we retrieve the right chunks?
  - Is chunking too big or too small?
  - Do we need reranking?
  - Are filters blocking relevant docs?

**Bad:**
- Prompt-tweaks forever while retrieval is the real issue

---

#### 3) Agent Patterns (Tools + Multi-Step)

##### Pattern: Plan → Act → Verify → Finish

**Good:**
- Brief plan (1–3 steps)
- Executes tools
- Verifies outputs
- Declares done with a clear result

**Bad:**
- Endless planning, no execution
- Executes but never verifies
- Stops mid-way without saying what happened

**Mini example:**
Task: "Find the latest invoice and summarize line items."
- Good: searches → opens invoice → extracts items → summarizes → links/IDs
- Bad: explains how to find invoices but doesn't do it

---

##### Pattern: Safe Confirmation Before Irreversible Actions

**Good:**
- "I'm about to send an email / delete data / charge a card. Confirm?"
- Uses least privilege
- Uses idempotency keys for actions that can be duplicated

**Bad:**
- Takes action without user confirmation
- Repeats action due to retries and causes duplicates

**What usually goes wrong:**
- Tool retries without idempotency
- State not stored (agent forgets it already sent something)

---

##### Pattern: Graceful Tool Failure Recovery

**Good:**
- Detects tool error
- Retries with backoff (limited)
- Switches to fallback approach
- If blocked, escalates clearly

**Bad:**
- Loops tool calls
- Pretends it succeeded
- Dumps raw error logs without interpretation

---

#### 4) Voice Patterns (Real-Time Calls)

##### Pattern: Confirm Critical Fields (Always)

Critical fields: names, phone numbers, dates, addresses, amounts.

**Good:**
- "Just to confirm, that's 0-1-2-… correct?"
- Uses natural confirmation, not robotic repetition

**Bad:**
- Assumes it heard correctly
- Doesn't confirm when audio is noisy

**Why it matters:**
- Voice errors are high-impact and feel "creepy" when wrong

---

##### Pattern: Smooth Turn-Taking (Barge-In)

**Good:**
- Stops speaking when user interrupts
- Responds to the interruption, not the old plan

**Bad:**
- Talks over user
- Ignores interruption and continues script

---

##### Pattern: Fast, Calm, Human Pacing

**Good:**
- Short sentences
- Small pauses
- Doesn't read long paragraphs

**Bad:**
- Long monologues
- Slow pipeline responses that feel like awkward silence

---

#### 5) "Gold Examples" Pack (Ready to Build Your Calibration Set)

Create a shared folder/page with:
- 10 examples of "great"
- 10 examples of "acceptable"
- 10 examples of "fail"

For each example, store:
- The user request
- The system output
- The rubric scores
- 1–2 sentences: "why this is scored that way"

This becomes your reviewer training kit and your regression baseline.

---

#### 6) What Usually Goes Wrong (Across All Task Types)

- Teams confuse "sounds good" with "is correct"
- No abstain cases, so the model learns to guess
- Reviewers score based on style preferences
- Agents are judged on text quality instead of action correctness
- RAG bugs get misdiagnosed as model bugs

---

#### 7) Debug Playbook (Quick)

When quality drops:
1. Identify task type: Chat, RAG, Agent, Voice
2. Check the main failure dimension:
   - Correctness? Relevance? Grounding? Tool correctness? Turn-taking?
3. Localize:
   - Retrieval vs generation (RAG)
   - Tool layer vs planning layer (agents)
   - ASR vs reasoning vs TTS (voice)
4. Add 3–10 new gold examples that represent the failure
5. Add them to regression tests

---

#### 8) Interview-Ready Talking Points

- "I use pattern libraries and gold examples so reviewers score consistently and engineers fix the right layer."
- "For RAG, I separate retrieval failures from grounding failures."
- "For agents, I evaluate tool correctness, state, idempotency, and safe confirmations — not just the final text."
- "For voice, I prioritize critical-field confirmation and turn-taking quality."
