# Chapter 5.2 — Dataset Formats (Prompts, Traces, Citations, Audio)

**What we're doing here:**
If you store datasets in the wrong format, you can't evaluate correctly.

In 2026 enterprise systems, evaluation is not just "prompt → text."
You often need:
- multi-turn conversations,
- retrieved documents and citations,
- tool traces and system state,
- audio + transcript + timing,
- and metadata for slices and risk.

This chapter defines the formats you should use for chat, RAG, agents, and voice — and how to keep them consistent and versionable.

---

## 1) Mechanics: what a dataset example must contain

A good eval example is a **self-contained test case**.

At minimum, every example should include:
- **ID** (stable identifier)
- **Task label** (taxonomy task_id)
- **Inputs** (messages, docs, tool context, audio, etc.)
- **Expected behavior** (answer / ask / abstain / refuse / escalate)
- **Scoring hooks** (rubric fields, required elements, forbidden elements)
- **Slice metadata** (language, tier, tenant, channel, risk, difficulty)
- **Version info** (dataset_version, policy_version if relevant)

**Enterprise mindset:**
If a case can't be replayed reliably, it's not a good eval case.

---

## 2) Core dataset "shapes" by channel

### 2.1 Chat format (single-turn and multi-turn)

**Best practice: store as a conversation array**
Each message includes:
- role (user/assistant/system/tool)
- content
- timestamp (optional)
- attachments (optional)

**What usually goes wrong**
- People store only the last user message and ignore prior turns.
- Then the eval is unrealistic.

**Enterprise rule**
If the product is multi-turn, the dataset must include full context windows that match production patterns.

---

### 2.2 RAG format (query + retrieved context + citations)

RAG eval cases must include:
- user query (and conversation context if multi-turn)
- retrieved chunks (the exact text chunks used)
- doc identifiers (doc_id, title, URL/path, section)
- retrieval metadata (optional but useful):
  - rank order
  - chunk boundaries
  - embedding version / retriever version
- expected answer constraints:
  - required facts
  - required citations (if your product uses citations)
  - abstain rules if evidence missing

**What usually goes wrong**
- Datasets store "the docs" but not the exact retrieved chunks.
- That makes the eval non-reproducible because retrieval changes over time.

**Enterprise rule**
Store the retrieved context snapshot used for evaluation, even if the live retriever changes later.

---

### 2.3 Agent format (tool traces + state transitions)

Agent eval cases must include:
- user goal
- allowed tools list (and tool schemas)
- tool calls + tool responses (trace)
- intermediate assistant thoughts are NOT required for eval; focus on:
  - actions
  - parameters
  - order
  - retries/fallback
- final state (what "done" looks like)
- safety requirements:
  - confirmations required
  - approvals required
  - prohibited actions

**What usually goes wrong**
- Only scoring the final text, ignoring whether the agent actually did the right thing.
- Missing tool failure scenarios (timeouts, partial results, permission denied).

**Enterprise rule**
Agent datasets must be trace-first:
- "Did it do the right steps safely?" not just "did it say something nice?"

---

### 2.4 Voice format (audio + transcript + timing + outcome)

Voice eval cases must include:
- audio file reference (or audio snippet)
- transcript(s):
  - ASR output transcript
  - human reference transcript (gold) if possible
- segmentation:
  - speaker turns
  - timestamps
- environment labels (slices):
  - noise level
  - accent/dialect
  - barge-in present
  - interruptions present
- expected dialogue behavior:
  - confirmations for critical fields
  - escalation rules
- final task outcome:
  - ticket created / appointment booked / etc.

**What usually goes wrong**
- Only measuring transcript accuracy while ignoring task success and UX behavior.
- Not storing timing, so latency "feel" can't be evaluated.

**Enterprise rule**
Voice truth is multi-layer: ASR accuracy + dialog policy + task outcome + latency/turn-taking behavior.

---

## 3) Knobs & defaults (what you actually set)

### 3.1 The "unified schema" approach (recommended)
Even if you have different formats per channel, enforce a shared top-level structure:

Top-level fields (always):
- example_id
- task_id
- channel
- risk_tier
- difficulty
- slices (language, tier, tenant, region)
- inputs (channel-specific)
- expected (behavior + constraints)
- scoring (rubric hooks)
- provenance (source type, created_by, created_date)
- versions (dataset_version, policy_version, retriever_version, tool_version)

This makes cross-channel reporting possible.

### 3.2 Reproducibility defaults
- store retrieval snapshots for RAG
- store tool traces for agents
- store audio references + timestamps for voice
- store prompt/system config references if needed for replay

### 3.3 Privacy defaults
- production data must be sanitized
- store minimum needed data
- separate sensitive fields into locked storage (access-controlled)
- (we'll go deep in 5.5)

---

## 4) Failure modes (symptoms + root causes)

### 4.1 "We can't reproduce the eval result"
Root causes:
- retrieval context changed
- tool responses changed
- missing config references
- cases lack IDs/versioning

Fix:
- snapshot retrieval chunks
- record tool I/O
- version datasets + tool schemas

---

### 4.2 "Agent looks good in eval but fails in production"
Root causes:
- eval lacks tool failure traces
- eval doesn't validate state transitions
- only final response scored

Fix:
- add tool-failure packs
- score traces + state outcomes

---

### 4.3 "Voice passes ASR but users still hate it"
Root causes:
- no scoring for confirmations
- no scoring for interruptions
- no latency/turn-taking evaluation

Fix:
- add voice behavior rubric + timing metrics

---

## 5) Debug playbook: choose the correct format quickly

Ask:
1. Does correctness depend on previous turns? → store multi-turn chat.
2. Does correctness depend on evidence? → store RAG chunks + doc IDs + cite rules.
3. Does correctness depend on actions/tools? → store traces + final state.
4. Does correctness depend on speech/timing? → store audio + transcripts + timestamps + outcome.

If "yes" to more than one:
- use a combined format (common in real products).

---

## 6) Enterprise expectations (what serious teams do)

- They maintain a unified schema with channel-specific payloads
- They store replayable artifacts:
  - retrieval snapshots
  - tool traces
  - audio timestamps
- They keep datasets compatible with CI:
  - deterministic replays for regression suites
- They enforce access control and auditing for sensitive fields

---

## 7) Ready-to-use templates (copy/paste)

### 7.1 Unified example skeleton (minimal)

```
example_id: ___
task_id: ___
channel: chat | rag | agent | voice
risk_tier: 0 | 1 | 2 | 3
difficulty: easy | normal | hard | adversarial
slices:
  language: ___
  region: ___
  tier: ___
  tenant: ___
provenance:
  source_type: prod | expert | synthetic
  created_by: ___
  created_date: ___
versions:
  dataset_version: ___
  policy_version: ___
  retriever_version: ___
  tool_schema_version: ___
inputs: (channel-specific)
expected:
  behavior: answer | clarify | abstain | refuse | escalate
  required_elements: [...]
  forbidden_elements: [...]
scoring:
  rubric_version: ___
  dimensions: [...]
```

---

### 7.2 RAG inputs snippet (drop-in)

```
inputs:
  conversation: [...]
  retrieved_chunks:
    - doc_id: ___
      title: ___
      chunk_id: ___
      rank: 1
      text: "..."
    - ...
expected:
  behavior: answer | abstain
  cite_required: true
  must_be_supported_by_chunks: true
```

---

### 7.3 Agent trace snippet (drop-in)

```
inputs:
  user_goal: "..."
  tools_allowed: [toolA, toolB]
  tool_trace:
    - tool: toolA
      params: {...}
      response: {...}
    - tool: toolB
      params: {...}
      response: {...}
expected:
  behavior: complete_task
  final_state_requirements: [...]
  confirmations_required: [...]
```

---

### 7.4 Voice inputs snippet (drop-in)

```
inputs:
  audio_ref: "..."
  asr_transcript: "..."
  gold_transcript: "..."
  timestamps: [...]
  environment:
    noise: low | medium | high
    interruptions: true/false
expected:
  behavior: complete_task | escalate
  critical_fields_to_confirm: [date, phone, address]
```

---

## 8) Interview-ready talking points

> "I store eval cases in replayable formats: RAG chunks, agent traces, and voice timestamps."

> "I use a unified schema so we can slice quality across channel, tier, language, and tenant."

> "Non-reproducible evals are a major failure mode — I snapshot retrieval and tool I/O."
