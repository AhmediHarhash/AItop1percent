# Chapter 3.6 — Practical Coverage Math (No Algebra, Just Decision Rules)

**What we're doing here:**
You need to answer questions like:
- "How many test cases are enough?"
- "How sure are we that quality didn't drop?"
- "How big should the regression suite be?"

Without turning this into algebra.

This chapter gives **simple sizing rules** and **confidence habits** used in real enterprise eval programs.

---

## 1) The uncomfortable truth: small eval sets lie

If your eval set is small, you'll see this:
- One release "improves" by 8 points
- Next release "drops" by 10 points
- But it's mostly noise.

**Why:** A small number of examples is too sensitive to random variation:
- prompt changes
- model randomness
- sampling changes
- different difficulty mix

**Enterprise mindset:**
We don't chase tiny ups/downs. We build enough coverage so changes are meaningful.

---

## 2) The "coverage budget" concept (simple mental model)

Think of eval cases as a budget you spend across:
- **Tasks** (taxonomy)
- **Risk tiers**
- **Difficulty levels**
- **Slices** (language, customer, channel)
- **Failure-mode packs** (safety, tool failures, abstain)

You don't need "a huge dataset everywhere."
You need **big where it matters** and **small where it doesn't**.

---

## 3) Default sizing rules (strong, practical)

### 3.1 Per leaf task (baseline)
- **Tier 0–1 tasks:** 10–30 cases per leaf task to start
- **Tier 2 tasks:** 30–80 cases per leaf task
- **Tier 3 tasks:** 80–200+ cases per leaf task (plus safety packs)

Why this works:
- Tier 0–1: you mainly need "does it behave normally?"
- Tier 2–3: you need stability and fewer surprises

### 3.2 For your top intents (the head)
For each of your **top 10 intents**, aim for:
- **50–200** cases each (depending on traffic and risk)

This is how you prevent "we broke the main thing" from slipping through.

### 3.3 For long-tail
Create a long-tail bucket of:
- **200–1000** cases total (depending on product size)

Focus on:
- rare but costly failures
- multi-turn edge cases
- tricky language

---

## 4) The "difficulty mix" rule (so the dataset isn't fake-easy)

A safe default mix:
- **20% easy**
- **60% normal**
- **20% hard**

For Tier 2–3:
- shift to more hard:
  - **15% easy**
  - **55% normal**
  - **30% hard**

**What usually goes wrong:**
Teams accidentally build "easy-only" evals and think they're winning.

---

## 5) The "minimum meaningful change" rule (stop chasing noise)

Instead of obsessing over tiny changes, set a threshold:

### Default rule
Treat changes as meaningful only if:
- the drop is seen in **multiple slices**, and
- it shows up in **enough cases** (not just 2–3 examples), and
- it repeats in the next run (not a one-off)

### Practical thresholds (no math)
- **Tier 0–1:** ignore tiny changes; only act on clear regressions users feel
- **Tier 2:** act if you see consistent drop in the task + a business signal (more escalations, more retries)
- **Tier 3:** act even on small shifts if it touches safety/PII/critical actions

---

## 6) The "confidence through repetition" trick

If you run the same eval suite multiple times and results swing wildly:
- your suite is too small, or
- your model/system is too unstable, or
- your sampling is inconsistent.

### Simple enterprise practice
- Run the same suite **3 times**
- If results vary a lot, don't trust the score — fix the suite or stabilize the system.

This is one of the easiest "no-math" stability checks.

---

## 7) Regression suite sizing (the "never break" set)

Your regression suite is not your full dataset.
It's the small set that must pass in CI before shipping.

### Default sizes
- **Small product:** 150–400 tests
- **Mid product:** 400–1500 tests
- **Large enterprise:** 1500–5000+ tests

### What goes into regression (always)
- Top intents (head)
- Tier 2–3 tasks
- Past incident cases
- Safety "red" cases
- Tool failure cases (agents)
- Must-abstain cases (RAG)
- Critical-field confirmation cases (voice)

---

## 8) Slice sizing rules (languages, customers, tiers)

You don't need full duplication across all slices. Use this rule:

### The "slice minimum"
For every important slice (example: language = Arabic/English):
- **Tier 0–1:** 20–50 cases per slice
- **Tier 2:** 50–150 cases per slice
- **Tier 3:** 150–300+ cases per slice

For top enterprise tenants:
- keep a "top customers pack" even if it's only 30–100 cases each — because a single enterprise tenant can be worth more than your whole long-tail combined.

---

## 9) The "safety math" rule (no guessing allowed)

Safety and privacy are different:
- you don't average them
- you don't accept "mostly safe"

### Practical rule
Maintain a dedicated safety suite with:
- hundreds to thousands of adversarial prompts (scaled by product risk)
- multilingual variants
- realistic phrasing (not only obvious bad prompts)

Release gate:
- **Any safety/PII failure = block release** (for gated systems)

---

## 10) What usually goes wrong (and how to fix it)

### Problem: "We have 500 tests but still miss bugs"
Likely causes:
- tests are duplicated variants of the same easy pattern
- missing tool failures, abstain cases, or edge slices

Fix:
- add "hard packs"
- add incident-based cases
- enforce difficulty mix

### Problem: "Quality looks worse after we improved the system"
Likely causes:
- your sampling changed (dataset drift)
- your new data is harder (which is good)

Fix:
- keep a stable regression suite
- track "difficulty label" so you can compare fairly

### Problem: "Scores don't match user complaints"
Likely causes:
- you're not sampling production
- you don't have business metrics in the loop

Fix:
- align with production slices + add scorecard links (2.5)

---

## 11) Copy/paste quick-start rules (the whole chapter in 12 lines)

- Tier 0–1: 10–30 cases per leaf task
- Tier 2: 30–80 cases per leaf task
- Tier 3: 80–200+ cases per leaf task + safety suite
- Difficulty mix: 20/60/20 (easy/normal/hard)
- Top 10 intents: 50–200 cases each
- Long-tail bucket: 200–1000 cases
- Regression suite: 150–5000 tests depending on product size
- Run suite 3 times to check stability
- Safety/PII: dedicated suite, any fail blocks release
- Keep "hard packs": tool failures, abstain, voice critical fields
- Keep "top customers pack" for enterprise tenants
- Every incident becomes regression tests + new cases
