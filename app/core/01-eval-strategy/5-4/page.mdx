# Chapter 5.4 — Splits & Holdouts (Train/Dev/Eval Separation + Leakage Prevention)

**What we're doing here:**
Here's a horror story that happens more than you'd think. A team fine-tunes their model and runs eval. Scores: 97%. They celebrate, ship to production. Users complain immediately. The model memorized the eval answers because the eval set leaked into fine-tuning data.

Data contamination is the silent killer of eval credibility. You can have perfect metrics, perfect labeling, perfect coverage — but if your eval set leaked into training, you're measuring memorization, not capability.

This chapter covers how to keep training, development, and evaluation data completely separate — and how to make every eval run reproducible.

---

## 1) Why separation is non-negotiable

In classical ML, the train/test split rule is gospel. In enterprise AI (2026), the contamination surface is much larger:

- **Fine-tuning data** overlaps with eval cases
- **Prompt templates** include examples verbatim from the eval set
- **RAG corpus** contains the exact text you're testing retrieval against
- **Synthetic generation** echoes eval patterns because the generator saw them
- **Embedding spaces** leak signal between splits if not properly isolated

What happens when contamination occurs:
- **Metrics inflate** — model scores 95% because it memorized the answers
- **Regressions go undetected** — the model passes eval but fails new production cases
- **Trust collapses** — when production quality doesn't match eval quality, nobody believes evals anymore

With models trained on internet-scale corpora and fine-tuned on user data, contamination is not a rare edge case — it's the default unless you actively prevent it.

---

## 2) The three splits

### Training / Fine-tuning data
Data used to train or fine-tune the model — including base pre-training, instruction tuning, RLHF, and domain adaptation. This data must **never** appear in dev or eval sets, even paraphrased.

### Dev / Validation data
Data used during development to tune prompts, adjust retrieval strategies, debug workflows, and iterate on system design. This data is "burned" for objective evaluation — because you've optimized against it. It should **never** be used for final quality metrics or release gates.

### Eval / Test data (the holdout)
Data held out from all training, fine-tuning, and development activities — used only for final evaluation and release decisions. This data is **read-only**. Once you evaluate against it, you cannot modify the system based on those results and re-evaluate on the same set. If you do, it becomes dev data.

---

## 3) How contamination happens in 2026

**Prompt template overlap.** You write a prompt template, then use the same phrasing in your eval cases. The model has now seen the structure and specific phrasings during system design. Fix: treat prompt templates as versioned artifacts in the dataset registry.

**Fine-tuning data includes eval cases.** A customer escalation becomes training data. Later, you add the same escalation to your eval set. Fix: hash-based deduplication across all splits before any data is used.

**Synthetic generation echoes eval patterns.** You generate synthetic cases using a prompt that samples from internet text including your public docs or leaked eval examples. Fix: track synthetic generation prompts and seeds; ensure the generator has not seen eval data.

**RAG corpus contains eval answers verbatim.** Your eval tests "What is our return policy?" and your RAG corpus includes a doc with that exact question as a header and exact answer below. Fix: hash document chunks in the RAG corpus and check for overlap with eval queries or expected answers.

**Shared embedding spaces leak signal.** You embed all your data (training, dev, eval) into a single vector store. Retrieval at inference time can now pull eval examples as context. Fix: isolate eval embeddings into separate stores with strict access control.

---

## 4) Separation disciplines

**Dataset registry (single source of truth).** Every example must be registered with a unique ID (hash-based), split assignment (train/dev/eval), creation date and source, access permissions, and lock status. No data enters training, prompts, or RAG without passing through this registry.

**Hash-based deduplication.** Before assigning splits: hash every example (input + expected output), hash all prompt templates and few-shot examples, hash all RAG corpus chunks, and check for collisions across splits. Priority order: eval is highest priority (never removed), then dev, then training.

**Holdout management with access controls.** Eval data must be stored separately with read-only access, versioned and locked (once eval v1.0 ships, it's immutable), audited (track who accessed eval data and when), and time-gated for rotating holdouts.

**Prompt templates as versioned artifacts.** Every prompt template gets a unique version ID, registration in the dataset registry, dedup check against eval examples, and lock status after deployment. When you update a prompt, create a new version and re-check for contamination.

---

## 5) Determinism & replayability

Contamination prevention is half the battle. The other half is ensuring you can **reproduce** an eval run exactly.

**Fixed random seeds.** For any operation involving randomness — sampling from datasets, model temperature, retrieval tie-breaking, tool order selection in agents — set explicit seeds and log them with the eval run.

**Tool mocks for agent eval replay.** Agent evals are notoriously non-deterministic because external tools return different results over time. Fix: record all tool calls and responses during first run, store as a "tool response tape," and in replay mode, mock tools return recorded responses. Compare agent decisions, not tool outputs.

**Config version pinning.** Every eval run must lock: model version, prompt template version, retriever config (vector store snapshot ID, K value, rerank setting), and tool schema versions. Log all versions in a single config file per eval run.

---

## 6) Knobs & defaults

**Split ratios:** For most enterprise datasets: 60–70% training/fine-tuning, 15–20% dev/validation, 15–20% eval/test holdout. If you're not fine-tuning, training split is 0% and dev + eval split the data 50/50. If your dataset is small (fewer than 500 examples), use k-fold cross-validation and rotate holdouts.

**Dedup thresholds:** Exact hash match means reject across splits. Fuzzy near-match (over 95% similarity) means flag for human review.

**Holdout rotation cadence:** If your eval set is small or your product evolves quickly, rotate holdouts periodically. Example: Q1 uses holdout A for eval and B+C for dev. Q2 rotates to holdout B. Q3 to holdout C. Q4 refreshes all holdouts with new production data.

---

## 7) Failure modes

**"Eval scores are suspiciously high."**
Model scores 95%+ but doesn't match production quality. Eval data leaked into training or fine-tuning. Prompt templates contain eval examples. Fix: run contamination audit — hash all training data against eval data. Rebuild eval set from fresh production logs or expert-written cases.

**"We can't reproduce last month's eval results."**
Same model, same dataset, different scores. No random seed pinning. Model version auto-updated. Tool responses changed. Fix: enforce config version pinning for all runs. Use tool mocks for agent replay. Lock eval corpus snapshots for RAG.

**"We ran out of holdout data."**
Team has iterated so much that eval data is effectively dev data now. No holdout rotation plan. Fix: implement holdout rotation. Expand dataset size (aim for 500+ eval examples minimum). Use separate datasets for dev iteration vs final release gate.

**"Prompt changes broke eval comparability."**
After updating prompts, scores changed dramatically. Can't tell if model regressed or prompt changed the task. Fix: version all prompt templates. When prompt changes, re-run eval on previous model version with new prompt to isolate prompt impact from model impact.

---

## 8) Enterprise expectations

- They maintain a **dataset registry** where every example is logged with split assignment, hash, source, and access controls
- They run **hash-based deduplication** across train/dev/eval before any data is used and after every update
- They **version and lock** eval sets — once deployed, immutable and read-only
- They enforce **deterministic replay** by pinning random seeds, model versions, prompt versions, and tool response mocks
- They implement **holdout rotation** for long-lived products where eval sets risk burnout
- They treat **contamination audits** as a mandatory pre-release gate
- They track **eval data provenance** so they can trace any example back to its source

---
