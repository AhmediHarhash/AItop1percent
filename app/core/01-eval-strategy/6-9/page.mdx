# 6.9 Crowdsource vs In-House vs Expert Panels

A medical AI startup I worked with in early 2025 had a painful lesson about evaluator sourcing. They'd been using crowdsourced raters on Mechanical Turk to evaluate their diagnostic suggestion system—getting labels on 1,000 cases per day at $0.15 each. Then an emergency room physician tested the system and found it suggesting contraindicated medications in 8% of cases—medications that previous reviewers had marked as "appropriate." The crowd raters had no medical training. They were pattern-matching based on superficial features. The company immediately switched to a panel of board-certified physicians, saw their evaluation costs jump from $150 to $8,000 per thousand cases, but finally got accurate quality signals. Six months later, they'd built a hybrid model: crowd for basic safety checks, in-house nurses for routine evaluations, and physician panels for edge cases and regulatory validation.

The decision of who evaluates your AI system is just as important as how you evaluate it. Get the sourcing wrong and you're flying blind, no matter how sophisticated your rubric.

---

## Why This Decision Matters

**The evaluator sourcing decision determines**:

- **Quality ceiling** — you can't evaluate what your raters can't understand
- **Velocity ceiling** — some models scale to thousands of raters, others to five
- **Cost structure** — ranging from $5/hour to $500/hour depending on expertise
- **Data security posture** — what you can safely share with external parties
- **Institutional knowledge** — whether insights compound over time or evaporate
- **Consistency** — whether quality improves or regresses as you scale

This isn't a one-time choice. Most mature organizations use all three models simultaneously for different types of evaluation tasks.

The key is knowing which model fits which use case, and having clear criteria for when to switch.

---

## The Three Sourcing Models

Let me walk you through the characteristics, strengths, and limitations of each approach.

### Crowdsourcing Platforms

**The model**: You post tasks to a platform like Scale AI, Surge AI, or Amazon Mechanical Turk, and thousands of independent contractors pick up work on demand.

**Common platforms in 2026**:
- **Scale AI** — premium platform with quality controls, used by OpenAI, Anthropic, Meta
- **Surge AI** — competitive alternative with flexible pricing tiers
- **Amazon MTurk** — original crowdsourcing marketplace, cheapest but requires heavy quality management
- **Appen/Lionbridge** — traditional data labeling companies with managed crowd workforces
- **Prolific** — research-focused platform with vetted, higher-quality participants

**Strengths**:

- **Speed** — spin up hundreds or thousands of raters in hours, not weeks
- **Scale** — can process 100K+ evaluations per week without hiring
- **Cost** — typically $5-15/hour effective rate, sometimes lower
- **Geographic diversity** — can recruit raters from specific regions or languages
- **No HR overhead** — platform handles recruiting, payments, compliance
- **Experimentation friendly** — can test new rubrics without long-term commitment

**Limitations**:

- **Quality inconsistency** — raters range from excellent to actively harmful, high variance
- **No domain expertise** — general crowd can't evaluate medical, legal, financial, or technical content accurately
- **Data security risk** — sharing production data with thousands of anonymous contractors
- **High management overhead** — requires sophisticated quality control, qualification tests, review pipelines
- **Motivation misalignment** — raters optimize for throughput, not accuracy, since they're paid per task
- **No institutional knowledge** — different rater every time, no learning across evaluations
- **Platform lock-in** — switching platforms means rebuilding quality control infrastructure

**Best for**:
- High-volume, low-stakes evaluations where domain expertise isn't required
- Initial dataset creation and baseline labeling
- Preference evaluations where "average user" perspective is the goal
- Safety checks that don't require specialized knowledge (toxicity, basic coherence)
- Rapid iteration on rubric design before committing to in-house resources

### In-House Evaluation Teams

**The model**: You hire full-time employees whose job is to evaluate your AI system. They become domain experts in your product, rubrics, and quality standards.

**Typical team structure**:
- **Evaluation specialists** (IC role) — perform evaluations, give feedback on rubric clarity
- **Senior evaluators/leads** — handle edge cases, train new team members, refine rubrics
- **Evaluation program manager** — oversee team, set standards, interface with product/ML teams
- **Evaluation tooling engineers** — build internal platforms for efficient evaluation workflows

**Strengths**:

- **Deep product context** — raters understand your system's goals, architecture, and common failure modes
- **Consistent quality** — same people evaluating over time with shared training and standards
- **Data security** — evaluations stay internal, no risk of leaking proprietary or sensitive information
- **Institutional knowledge** — insights compound, raters develop intuition for quality
- **Tight feedback loops** — evaluators sit near engineers, can flag issues immediately
- **Rubric refinement** — daily users of rubrics can identify ambiguities and suggest improvements
- **Flexibility** — can quickly pivot to evaluate new features without renegotiating contracts
- **Cultural alignment** — evaluators understand company values and can spot misalignment

**Limitations**:

- **Expensive** — fully-loaded cost typically $80K-150K per evaluator per year (salary, benefits, overhead)
- **Slow to scale** — hiring and training takes months, not hours
- **Fixed cost** — you're paying salaries whether there's evaluation work or not
- **Echo chamber risk** — team can develop blind spots or drift from real user perspectives
- **Burnout** — evaluation work is cognitively demanding and repetitive, high turnover risk
- **Limited expertise** — unless you hire domain specialists, may lack specialized knowledge
- **Capacity constraints** — can't easily handle 10x spike in evaluation needs

**Best for**:
- Core product evaluations where quality and consistency are critical
- Evaluations involving proprietary or sensitive data
- Complex systems where deep context improves evaluation quality
- Organizations with steady, predictable evaluation volume
- Building institutional knowledge about quality over time
- High-stakes decisions where you need accountability and traceability

### Expert Panels

**The model**: You contract with recognized domain experts—doctors, lawyers, engineers, subject matter specialists—to evaluate specific aspects of your AI system.

**Common expert types**:
- **Licensed professionals** — physicians, attorneys, CPAs for regulated domains
- **Academic researchers** — professors or PhD researchers for scientific accuracy
- **Industry specialists** — ex-FAANG engineers for code evaluation, financial analysts for market predictions
- **Certified practitioners** — teachers, therapists, translators with professional credentials
- **Security researchers** — for red-teaming and adversarial evaluation

**Strengths**:

- **Domain expertise** — can evaluate nuances that general raters and in-house teams miss
- **High accuracy** — fewer false positives/negatives on specialized content
- **Credibility** — regulatory and external stakeholders trust expert validation
- **Specialized knowledge** — access to expertise you can't afford to hire full-time
- **Quality signal on edge cases** — expert judgment on rare but critical scenarios
- **Regulatory compliance** — some domains legally require expert review

**Limitations**:

- **Very expensive** — typically $100-500/hour, sometimes more for rare specialties
- **Limited availability** — experts have day jobs, limited time for evaluation work
- **Slow turnaround** — may take weeks to schedule, coordinate, and complete evaluations
- **Small scale** — can't hire 100 radiologists to label 10K scans in a week
- **Consistency challenges** — different experts may disagree on subjective judgments
- **Expertise verification** — ensuring claimed credentials are legitimate
- **Over-qualification** — paying expert rates for tasks that don't require expertise

**Best for**:
- Evaluations in regulated domains (healthcare, legal, financial)
- High-stakes decisions where accuracy is paramount
- Validation of in-house or crowd evaluation quality
- Edge cases and escalations that stumped other raters
- Regulatory submissions and external audits
- Establishing ground truth for specialized tasks
- Building training datasets for in-house teams

---

## The Hybrid Approach

The most sophisticated organizations don't choose one model—they use all three strategically for different evaluation needs.

**A typical hybrid architecture**:

**Tier 1: Crowdsourced volume screening**
- Handle 80% of evaluation volume
- Basic safety, toxicity, and coherence checks
- Fast feedback for routine model improvements
- Cost: $0.10-0.50 per evaluation

**Tier 2: In-house core evaluations**
- 15% of volume, higher complexity
- Product-specific quality dimensions
- Nuanced judgments requiring context
- Weekly quality reviews and rubric refinement
- Cost: $3-8 per evaluation (amortized)

**Tier 3: Expert validation and edge cases**
- 5% of volume, highest stakes
- Regulatory-critical evaluations
- Escalations from Tier 1 and 2
- Quarterly audits of lower-tier quality
- Ground truth establishment
- Cost: $25-100 per evaluation

**The routing logic**:

All evaluations start in Tier 1 (crowd). If:
- Crowd confidence scores are low (e.g., 3+ raters disagree)
- Content flagged as potentially specialized
- Random sample for quality monitoring (10% of crowd evals)
→ Escalate to Tier 2 (in-house)

If in-house evaluators:
- Mark evaluation as requiring domain expertise
- Encounter rare/novel scenario outside training
- Flag for regulatory or legal sensitivity
→ Escalate to Tier 3 (expert)

This creates a **quality pyramid**: broad, cheap coverage at the base, and expensive, specialized judgment at the top.

---

## Data Security Considerations

**What determines whether you can use external raters**?

The sensitivity of the data you need to show them.

**Green light for external (crowd or expert panels)**:
- Public domain content (Wikipedia, news articles, published books)
- Synthetic or heavily anonymized examples
- Generic tasks with no proprietary information
- Content users have already consented to share

**Yellow light (requires NDA and restricted access)**:
- Non-PII user inputs (questions, prompts without identifying info)
- Internal documents if sanitized of trade secrets
- Product features in limited beta
- Low-sensitivity business data

**Red light (in-house only)**:
- Personally identifiable information (names, emails, locations)
- Protected health information (HIPAA-regulated)
- Financial records or payment information
- Trade secrets, proprietary algorithms, unreleased products
- Content under NDA from enterprise customers
- Anything that would violate GDPR, CCPA, or other privacy regulations

**Common mitigation strategies for external raters**:

1. **Anonymization** — strip PII, replace with synthetic placeholders
2. **Aggregation** — show raters summary statistics, not raw records
3. **Sample filtering** — only send pre-screened "clean" examples externally
4. **Contractual protections** — NDAs, data processing agreements, platform security certifications
5. **Geographic restrictions** — only use raters in specific jurisdictions
6. **Monitoring** — audit what external raters can access, track data exports

If you're in a regulated industry (healthcare, finance, government), assume you need in-house evaluation unless your legal team explicitly approves external sharing.

---

## NDA and Compliance Requirements

**Standard NDA provisions for external evaluators**:

- **Confidentiality clause** — raters cannot share evaluation content or insights
- **Data use restrictions** — evaluation data cannot be used for other purposes
- **Return or destruction** — data must be deleted after evaluation completes
- **Subcontracting limits** — platforms cannot sub-delegate without approval
- **Breach notification** — requirement to report security incidents
- **Jurisdiction** — which courts govern disputes

**Additional compliance for regulated industries**:

**Healthcare (HIPAA)**:
- Business Associate Agreement (BAA) with platform
- De-identification per Safe Harbor or Expert Determination standards
- Minimum necessary data principle
- Audit logs of all data access

**Financial (GLBA, PCI-DSS)**:
- No sharing of non-public personal information without consent
- Encrypted transmission and storage
- Restricted access to cardholder data
- Regular security assessments

**EU/UK (GDPR)**:
- Data Processing Agreement specifying lawful basis
- Explicit consent if relying on consent
- Right to erasure provisions
- Data transfer mechanisms (Standard Contractual Clauses)

**Government (FedRAMP, ITAR)**:
- Platform must have appropriate security authorizations
- US-person restrictions for controlled information
- Air-gapped or on-prem evaluation infrastructure

Many enterprises maintain **pre-approved vendor lists** of labeling platforms that have passed security review. If your platform isn't on the list, expect 3-6 months for vendor assessment before you can use external raters.

---

## Platform Comparison: What to Look For

**Key features when evaluating labeling platforms** (2026 landscape):

**Quality control mechanisms**:
- Qualification tests and ongoing certification
- Consensus requirements (3+ raters per task)
- Expert review layers and escalation paths
- Rater performance tracking and feedback
- Honeypot questions to catch bad actors

**Rater management**:
- Pools segmented by language, region, expertise
- Custom qualification workflows
- Rater training and onboarding support
- Performance incentives tied to accuracy

**Integration and workflow**:
- API for programmatic task submission
- Webhook notifications for completed work
- Integration with evaluation platforms (Braintrust, Evidently)
- Support for complex, multi-step evaluations

**Security and compliance**:
- SOC 2 Type II certification
- GDPR and CCPA compliance
- Data encryption in transit and at rest
- Geographic restrictions and IP allowlisting
- BAA availability for HIPAA

**Cost structure**:
- Pay-per-task vs subscription models
- Volume discounts and enterprise pricing
- Overage handling and flex capacity

**Transparency and support**:
- Real-time task progress dashboards
- Quality metrics and inter-rater agreement stats
- Dedicated account management for enterprise
- SLA guarantees on turnaround time

**Red flags**:
- No rater qualification or quality controls ("we just send to the crowd")
- Opaque pricing with surprise overage fees
- No security certifications or vague answers about data handling
- Can't provide references from companies in your industry
- Platform owns IP rights to evaluation data

**Top-tier platforms in 2026** (by use case):

- **Scale AI** — premium quality, highest cost, best for safety-critical applications
- **Surge AI** — flexible middle tier, good balance of cost and quality
- **Prolific** — research-grade, vetted participants, moderate cost
- **Appen** — large scale, established, good for multi-language
- **Labelbox** — self-serve with bring-your-own raters, great for hybrid models

---

## Building an In-House Evaluation Team

If you're ready to invest in full-time evaluators, here's what it takes.

**Roles and structure**:

**Evaluation Specialist (IC, entry-level)**
- Performs 50-200 evaluations per day depending on complexity
- Provides feedback on rubric clarity and edge cases
- Documents novel failure modes
- Participates in calibration exercises
- Typical background: recent grads, content moderators, QA testers, customer support
- Salary range: $50K-75K depending on location

**Senior Evaluator / Team Lead**
- Handles escalations and ambiguous cases
- Trains and onboards new team members
- Refines rubrics based on team feedback
- Runs calibration sessions and quality reviews
- Conducts inter-rater agreement analyses
- 2-5 years experience in evaluation or related field
- Salary range: $75K-110K

**Evaluation Program Manager**
- Sets overall evaluation strategy and priorities
- Interfaces with product and ML teams on quality standards
- Manages team capacity and hiring plans
- Oversees tooling and process improvements
- Reports on evaluation metrics to leadership
- 5+ years in operations, QA, or product management
- Salary range: $110K-160K

**Evaluation Tooling Engineer (optional, for larger teams)**
- Builds internal platforms for efficient evaluation workflows
- Integrates evaluation data with ML training pipelines
- Creates dashboards and analytics on eval metrics
- Software engineering background
- Salary range: $120K-180K

**Hiring profile for evaluation specialists**:

Look for:
- **Attention to detail** — spots inconsistencies and edge cases
- **Communication skills** — can articulate why they made a judgment call
- **Learning agility** — quickly absorbs new rubrics and domain knowledge
- **Judgment** — balances rubric letter vs spirit, escalates appropriately
- **Resilience** — handles repetitive work without quality degradation
- **Collaboration** — works well in calibration sessions, takes feedback constructively

Don't overindex on:
- Domain expertise (you can train this)
- Technical background (unless evaluating code or technical systems)
- Prior AI/ML knowledge (can be taught)

**Training program** (first 2-4 weeks):

Week 1: Product training
- Understand the AI system being evaluated
- Learn common use cases and failure modes
- Shadow customer support or user research

Week 2: Rubric training
- Deep dive on evaluation dimensions and rating scales
- Practice on pre-labeled gold standard examples
- Calibration session with senior evaluators

Week 3: Supervised evaluation
- Perform live evaluations with senior review
- Discuss disagreements and edge cases
- Refine understanding of rubric edge cases

Week 4: Independent evaluation with monitoring
- Full evaluation load with spot-check quality review
- Weekly 1:1s to address questions
- Join calibration sessions with full team

**Retention strategies**:

Evaluation work is demanding and can lead to burnout. To keep good people:

- **Career progression** — clear path from IC to lead to program manager
- **Variety** — rotate evaluators across different product areas or evaluation types
- **Input** — give evaluators voice in rubric design and process improvements
- **Impact visibility** — show how their work improves the product (model metrics, user feedback)
- **Avoid monotony** — mix evaluation work with calibration, training, and analysis projects
- **Compensation** — market-rate pay plus bonuses tied to quality metrics
- **Well-being** — adequate breaks, mental health support, content moderation protections if evaluating harmful content

Expect 20-40% annual turnover in entry-level roles, 10-20% in senior roles. Plan hiring pipeline accordingly.

---

## Managing External Rater Relationships

**Setting clear expectations upfront**:

**Service Level Agreements (SLAs)**:
- **Turnaround time** — how quickly evaluations are returned (e.g., 80% within 24 hours, 95% within 48 hours)
- **Quality targets** — inter-rater agreement thresholds, accuracy on gold standard examples
- **Availability** — minimum and maximum task volume per week
- **Responsiveness** — how quickly platform responds to quality issues or questions

**Quality expectations**:
- Minimum accuracy on qualification tests (e.g., 85% on gold standard)
- Inter-rater agreement targets (e.g., Cohen's kappa > 0.6)
- Acceptable error types vs unacceptable (missing nuance vs completely wrong)
- Escalation criteria (when to ask for help vs guessing)

**Communication channels**:
- Primary point of contact at platform
- How to submit feedback or flag quality issues
- Emergency escalation path for critical problems
- Regular business reviews (monthly or quarterly)

**Feedback mechanisms**:

Give raters feedback so they improve over time:

- **Immediate feedback on errors** — if gold standard question is failed, explain why
- **Weekly quality reports** — show accuracy trends and common mistakes
- **Bonus incentives** — reward high-performing raters with higher pay or priority access to tasks
- **Positive reinforcement** — highlight examples of excellent evaluations
- **Calibration materials** — provide updated examples as rubrics evolve

**Escalation paths**:

Define clear process for issues:

**Quality concerns**:
- Low agreement or accuracy → Platform investigates specific raters → Retraining or removal
- Systematic rubric misunderstanding → Review training materials → Update examples

**Data security incidents**:
- Suspected breach or misuse → Immediate suspension → Legal/security investigation

**Capacity shortfalls**:
- Cannot meet SLA turnaround → Platform adds raters or prioritizes your tasks → Consider backup platform

**Cost overruns**:
- Unexpected volume charges → Review usage and adjust task submission → Negotiate pricing

**Monitor these metrics monthly**:
- Inter-rater agreement trends (improving or degrading?)
- Accuracy on gold standard check questions
- Turnaround time distribution (meeting SLA?)
- Cost per evaluation (drifting up or down?)
- Escalation rate (what % of tasks require expert review?)

---

## When to Switch Models

**Signs you've outgrown crowdsourcing**:

- **Quality ceiling** — inter-rater agreement plateaus below acceptable levels no matter how much training
- **Domain complexity** — frequent errors on domain-specific content that requires expertise
- **Security requirements** — compliance or business needs prevent sharing data externally
- **Strategic importance** — evaluation quality is now mission-critical, needs dedicated focus
- **Volume predictability** — evaluation needs are steady enough to justify fixed costs
- **Feedback loop value** — institutional knowledge from long-term evaluators would significantly improve quality

**Signs you need expert panels**:

- **Regulatory requirements** — audits or submissions require credentialed professional review
- **Domain-critical failures** — in-house team lacks expertise to catch dangerous errors
- **Accuracy vs cost** — cost of mistakes outweighs expert evaluation expense
- **Establishing ground truth** — need authoritative labels to train or validate other models
- **Competitive differentiation** — expert-validated quality is a market advantage

**Signs you need to scale back to crowd**:

- **Over-capacity** — in-house team idle 30%+ of the time, underutilized
- **Cost pressure** — evaluation budget unsustainable, need to reduce spend
- **Exploratory work** — rapid iteration on new features where lower quality is acceptable temporarily
- **Volume spike** — short-term need for 10x evaluations that in-house can't handle

**Migration strategies**:

**Crowd to in-house**:
1. Hire first senior evaluator or program manager
2. Run pilot with 2-3 junior evaluators on subset of tasks
3. Measure quality improvement vs crowd baseline
4. Gradually shift volume, keep crowd as overflow capacity
5. Maintain crowd relationship for experimentation

**In-house to experts** (usually additive, not replacement):
1. Identify high-stakes evaluation tasks requiring domain expertise
2. Contract with 2-3 experts for pilot evaluation
3. Compare expert judgments to in-house baseline
4. Establish routine expert review cadence (weekly or monthly)
5. Use expert feedback to train in-house team on domain knowledge

**Experts to in-house** (rare, but happens):
1. Hire domain experts as full-time employees if volume justifies
2. Create training program where experts teach in-house generalists
3. Reserve contracted experts for edge cases and validation only

The most mature approach is having **clear task routing criteria** that automatically send evaluations to the right tier based on sensitivity, complexity, and stakes.

---

## 2026 Patterns and Emerging Models

The evaluator sourcing landscape is evolving rapidly.

**Specialized AI evaluation firms**:

Companies like **Anthropic Evals** (fictional, but representative) offer managed evaluation services specifically for LLM and AI applications:
- Deep expertise in AI failure modes
- Pre-trained evaluators familiar with common rubrics
- Turnkey evaluation infrastructure
- Faster than in-house, higher quality than generic crowd
- Cost: $2-5 per evaluation

**Domain-certified rater networks**:

Platforms building **credentialed rater pools**:
- Medical labeling networks with verified RN/MD credentials
- Legal evaluation pools with active bar licenses
- Engineering assessment teams with FAANG experience
- Teacher networks for educational content review

You get expert quality with crowd-like scalability.

**Rater-as-a-service with SLA guarantees**:

Traditional staffing firms (Upwork, Toptal) entering evaluation space:
- Dedicated evaluators assigned to your account
- Contractual SLAs on quality and turnaround
- More flexible than full-time, more consistent than crowd
- Cost: $25-50/hour

**Hybrid human-AI evaluation**:

Platforms using **AI as first pass**, escalate to human only when:
- AI confidence below threshold
- Disagreement between multiple AI evaluators
- Random sample for calibration

This dramatically reduces human evaluation volume (80-90% reduction) while maintaining quality oversight.

**Evaluation co-ops and consortiums**:

Companies in the same industry **pooling evaluation resources**:
- Shared rater training for domain-specific tasks
- Joint development of standardized rubrics
- Collaborative benchmarking

Common in healthcare, legal, and financial services where regulatory alignment is valuable.

**On-demand expert marketplaces**:

Platforms like **Maven** or **GLG** that connect you with domain experts for short consultations now offer **evaluation services**:
- Post evaluation task with required credentials
- Experts bid or are matched algorithmically
- Complete evaluation with expert commentary
- Pay per evaluation or hourly

**Geographic arbitrage platforms**:

Specialized platforms recruiting high-quality evaluators in lower-cost regions:
- English-fluent raters in India, Philippines, Eastern Europe, Latin America
- College-educated, trained on Western cultural contexts
- Cost: $3-8/hour, quality approaching in-house US-based teams

**The trend**: more specialization, more quality guarantees, blurred lines between crowd/in-house/expert as hybrid models mature.

---

## Practical Decision Framework

**Use this decision tree**:

**Step 1: Can you share this data externally**?
- No → In-house or on-prem solutions only
- Yes → Continue

**Step 2: Does this task require domain expertise**?
- Yes → Expert panels or domain-certified raters
- No → Continue

**Step 3: What is the volume and urgency**?
- High volume, fast turnaround → Crowd
- Lower volume, quality-critical → In-house
- Occasional, very high stakes → Experts

**Step 4: What is your budget per evaluation**?
- $0.10-1 → Crowd
- $1-10 → In-house or specialized platforms
- $10-100+ → Experts

**Step 5: How important is consistency over time**?
- Critical → In-house
- Moderate → Hybrid with in-house core
- Less important → Crowd

**A simple template for evaluation task routing**:

```
Task: [Description]
Sensitivity: [Public / Internal / Confidential]
Domain expertise required: [Yes / No]
Estimated volume: [X per week]
Urgency: [Hours / Days / Weeks]
Quality bar: [Directional / Reliable / Mission-critical]

Recommended sourcing: [Crowd / In-house / Expert / Hybrid]
Rationale: [Why this model fits the task characteristics]
Escalation criteria: [When to move up to higher tier]
```

Use this template when planning a new evaluation initiative or reviewing existing sourcing decisions quarterly.

---

## Failure Modes

**Crowdsourcing gone wrong**:

- **Race to the bottom** — raters optimize for speed, accuracy collapses, you don't notice until downstream impact
- **Adversarial raters** — bots or bad actors submitting random responses to collect payment
- **Training collapse** — crowd learns to game your qualification tests without understanding the task
- **Context loss** — different rater every time, never develop intuition for your product
- **Data leak** — rater screenshots evaluation content and posts on social media

**In-house gone wrong**:

- **Groupthink** — team converges on shared biases, stops representing real user diversity
- **Burnout** — high turnover, constant retraining, evaluation quality unstable
- **Over-specialization** — team becomes too narrow, can't evaluate new product directions
- **Political capture** — evaluators align with product team's preferred outcomes rather than truth
- **Fixed cost trap** — you're paying for idle capacity but can't quickly reduce headcount

**Expert panels gone wrong**:

- **Credential fraud** — claimed experts lack actual credentials or expertise
- **Expert disagreement** — two board-certified doctors give opposite evaluations, now what?
- **Over-reliance** — treating expert opinion as ground truth when it's actually subjective or biased
- **Availability collapse** — experts ghost when workload increases or they get busy
- **Scope creep** — paying expert rates for tasks that don't actually require expertise

**Hybrid gone wrong**:

- **Quality inversion** — experts validating crowd labels find so many errors the entire dataset is suspect
- **Routing failures** — sensitive data accidentally sent to crowd, or simple tasks over-escalated to experts
- **Integration complexity** — three different platforms and workflows, high operational overhead
- **Cost explosion** — everything escalates to top tier, economics break down

**Prevention**:

- Monitor quality continuously, not just at launch
- Run blind expert audits of crowd and in-house work quarterly
- Maintain clear routing criteria and enforce them with automated checks
- Have backup sourcing for each tier in case primary fails
- Set evaluation budgets with 20% contingency for escalations

---

## Enterprise Expectations

**Leadership will ask**:

"How do we know our evaluation quality isn't degrading over time?"
→ You should have quarterly calibration exercises and blind expert audits

"What's our contingency if the labeling platform has an outage or quality issue?"
→ You should have backup crowd platform and in-house overflow capacity

"Can we evaluate this new regulated product feature with our current setup?"
→ You should have pre-cleared expert panels for each regulated domain

"How much would it cost to 10x our evaluation volume next quarter?"
→ You should have cost curves and capacity limits for each sourcing model

**Board-level evaluation sourcing maturity** (what investors look for in 2026):

**Level 1: Ad-hoc** — Sourcing decisions made per-project, no systematic approach
**Level 2: Standardized** — Clear policies for crowd vs in-house vs expert, but manual routing
**Level 3: Hybrid** — Multi-tier system with automated routing and escalation
**Level 4: Optimized** — Cost and quality continuously optimized, predictive models for task routing
**Level 5: Strategic** — Evaluation sourcing is competitive advantage, proprietary rater networks or methods

Most companies are at Level 1-2. Getting to Level 3 requires dedicated program management and tooling investment.

**Red flags in due diligence**:

- No in-house evaluation capacity, entirely dependent on one crowd platform
- Using crowd raters for regulated or sensitive domains without expert validation
- Can't produce inter-rater agreement statistics or quality trends
- No NDAs or data processing agreements with external raters
- Evaluation sourcing decisions made by engineers without security or legal review

**Compliance checklist for regulated industries**:

- [ ] Data sensitivity classification for all evaluation datasets
- [ ] Legal approval for external sharing of each data category
- [ ] NDAs and DPAs signed with all platforms and expert contractors
- [ ] Security assessment of platforms (SOC 2, penetration tests)
- [ ] Audit trail of who accessed what evaluation data
- [ ] Credential verification for all expert evaluators
- [ ] Regular compliance training for in-house evaluation team
- [ ] Incident response plan for data breaches or quality failures
- [ ] Quarterly review of sourcing decisions with legal and security

---

## Interview Q&A

**Q: When should a startup invest in an in-house evaluation team vs using crowd platforms?**

When evaluation quality becomes a competitive differentiator or a security requirement, not before. Early stage, use crowd platforms to iterate quickly and cheaply. As you approach product-market fit and quality expectations stabilize, consider hiring your first evaluation program manager and 2-3 specialists. The trigger is usually when you're spending $50K+ annually on crowd evaluations and noticing quality inconsistency. At that point, redirecting that budget to 1-2 full-time people can improve quality while keeping costs roughly flat.

**Q: How do you prevent your in-house evaluation team from becoming an echo chamber that drifts from real user perspectives?**

Three mechanisms. First, rotate new evaluators through customer support or user research periodically so they hear unfiltered user feedback. Second, run quarterly calibration sessions where external raters (crowd or users) evaluate the same examples, and compare judgments. Large divergence is a signal your team has drifted. Third, tie evaluation quality to downstream product metrics—if your in-house team rates something highly but users churn, that's a wake-up call. The key is external feedback loops that prevent insularity.

**Q: We're in healthcare and everything is PHI. How do we get enough evaluation volume without using external raters?**

You have three options. One, hire a large enough in-house team to handle the volume—this is what most health tech unicorns do. Two, use de-identification techniques (Safe Harbor or Expert Determination) to strip PHI, then send to external raters. This works if the clinical context survives de-identification. Three, use synthetic data generation to create realistic but non-PHI examples for evaluation, then validate with a small expert panel on real data. Many companies use a hybrid: in-house for real patient data, crowd for synthetic data, and physician panels for edge case validation.

**Q: How do you manage expert evaluators who disagree with each other on the same example?**

Disagreement is information. First, understand why they disagree—is it truly subjective (reasonable people differ), or is one expert misunderstanding the rubric? Run a calibration discussion where both explain their reasoning. If it's subjective, you may need to refine your rubric to clarify the standard, or accept that this dimension has inherent ambiguity and use majority vote. If it's rubric misunderstanding, provide better examples. For high-stakes cases, consider a third expert as tiebreaker, or escalate to a senior expert panel. Don't treat expert judgment as ground truth—it's high-signal input, but still fallible.

**Q: What's the biggest mistake companies make when choosing between sourcing models?**

Over-rotating on cost efficiency early, then hitting a quality ceiling later. I see companies use cheap crowd labor for 18 months, then realize their evaluation data is too noisy to actually improve the model. They've wasted time on low-signal feedback. The better approach is to start with high quality (in-house or expert) on a small scale to establish what good looks like, then scale to crowd for volume while maintaining quality checks. Frontload the quality investment, then optimize for cost. It's also common to underestimate data security risks—sending proprietary or sensitive data to crowd platforms without legal review, then facing compliance issues later.

---

## Bridge to Next Chapter

You now understand the three main sourcing models for human evaluation—crowdsourcing for scale, in-house teams for quality and context, and expert panels for specialized accuracy. You know when to use each model and how to build a hybrid system that combines them strategically. You've thought through data security, compliance, and the practical operational details of managing internal and external raters.

But even with the right sourcing strategy, human evaluation is expensive. As your evaluation program grows—more models, more features, more frequent releases—costs can spiral quickly. The next chapter addresses the financial reality: **cost management and budget allocation**. We'll explore how to optimize your evaluation spend, where to invest for maximum ROI, how to make trade-offs when budgets are constrained, and how to build a business case for evaluation investment that leadership will approve. Because no matter how good your evaluation strategy is, it needs to be economically sustainable.

---

*Next: [6.10 — Cost Management & Budget Allocation](/core/01-eval-strategy/6-10)*
