# 7.9 — Custom Metric Design (When Off-the-Shelf Fails)

You're three months into building your legal tech AI. You've got BLEU scores. You've got ROUGE scores. You've got a standard LLM-as-Judge scoring "helpfulness" and "harmfulness." All the metrics are trending up. Green dashboards everywhere.

Then your pilot customer — a law firm — calls. "The AI keeps giving advice in a tone that would never fly with our clients. It's too casual. And it's not flagging the jurisdictional nuances we need. According to your metrics, these responses are great. According to us, they're unusable."

Your off-the-shelf metrics aren't measuring what actually matters to your product.

This is the moment when you need **custom metrics**. Not generic quality scores. Not academic benchmarks. Metrics specifically designed for your product's unique definition of "good." Metrics that capture brand voice consistency. Regulatory compliance. Customer sentiment alignment. Action correctness in multi-step workflows. The things that make your AI succeed or fail in the real world, not in a research paper.

This chapter is about how to design those metrics. When to build them. How to validate that they're actually measuring what you think. How to avoid the traps that make custom metrics worse than no metrics at all. And how to connect them to business outcomes so they're not just vanity numbers on a dashboard.

---

## When Off-the-Shelf Metrics Fail

Let's be clear: off-the-shelf metrics are great when they work. BLEU and ROUGE for translation and summarization. Exact match for question answering. Perplexity for language modeling. Standard LLM judges for general helpfulness.

But they fail in three common scenarios:

**Your product has unique quality dimensions.** You're building a customer service AI for a luxury brand. "Helpfulness" isn't enough. You need "maintains brand voice" — warm but not casual, knowledgeable but not condescending, efficient but not transactional. No standard metric measures that.

**Your constraints are domain-specific.** You're building a healthcare AI. Standard safety metrics catch obvious harm (medical advice, self-harm). But you need "HIPAA compliance score" — does the response avoid asking for or revealing protected health information? Does it include appropriate disclaimers? Does it refuse high-risk requests correctly? That's domain-specific.

**Your success depends on workflow correctness.** You're building an agent that books appointments. "Fluency" doesn't matter if it books the wrong time. "Helpfulness" doesn't matter if it forgets to send a confirmation. You need "action correctness" — right tool called, right parameters, right sequence, right final state. That's task-specific.

The tell: **Your team keeps saying "the metrics look good but the output is wrong."** That means your metrics aren't aligned with your actual quality definition. You're optimizing the wrong thing.

Another tell: **User feedback and metric scores don't correlate.** Users rate a response 1-star, your LLM judge gives it 8/10. Users love a response, your ROUGE score is low. When metrics and reality diverge, reality wins. Your metrics are measuring something, but it's not what matters.

The 2026 shift: Teams are moving from "use standard metrics everywhere" to "use standard metrics as a baseline, build custom metrics for what makes your product unique." The companies shipping great AI products have 3-5 custom metrics that define their differentiated quality. The companies struggling are still optimizing BLEU scores.

---

## Examples of Custom Metrics in the Wild

Here's what teams are actually building in 2026:

**Brand voice consistency score.** An LLM judge trained on your brand guidelines and example communications. Scores 1-5 on how well a response matches your brand personality — tone, formality, word choice, sentence structure. Used by customer-facing AI for luxury brands, B2B SaaS with strong editorial voices, nonprofits with mission-specific messaging.

**Regulatory compliance score.** A specialized judge that checks responses against domain regulations. For finance: "Does this response include required disclosures? Does it avoid prohibited advice?" For healthcare: "Does this refuse to diagnose? Does it suggest seeing a doctor?" For legal: "Does this include appropriate disclaimers about not being legal advice?" Pass-fail or weighted scoring across multiple compliance criteria.

**Customer sentiment alignment score.** Measures whether the AI's response matches the emotional tone the customer needs. An angry customer gets empathy and resolution, not cheerful efficiency. A confused customer gets patience and clarity, not speed. An LLM judge that analyzes customer input sentiment, then scores whether the response tone is appropriate.

**Action correctness for multi-step workflows.** For agents, a composite metric: Did the agent call the right tools? In the right order? With the right parameters? Did it handle errors appropriately? Did it reach the correct end state? Scored per-step and overall. Critical for booking agents, data analysis agents, workflow automation.

**Citation quality score.** Beyond "is there a citation?" — does the citation actually support the claim? Is it from a trustworthy source? Is it the most relevant source available? Used by RAG systems in research, legal, and medical domains where citation quality matters as much as answer quality.

**Conversation coherence score.** For multi-turn chat, does the AI maintain context across turns? Does it remember what the user said three turns ago? Does it avoid contradicting itself? Does it pick up on implied information? Measured over full conversations, not single turns.

**Edge case handling score.** How well does the AI handle ambiguous inputs, conflicting constraints, missing information, and policy edge cases? Not "average case quality" but "worst case quality." Used as a floor metric — you can't ship until edge case handling is above threshold, regardless of average scores.

The pattern: These metrics are **task-specific, product-specific, or domain-specific.** They capture something unique about what makes your AI good or bad. They're not generalizable to all AI products. That's the point.

---

## The Metric Design Process

You can't just decide "we need a brand voice metric" and start measuring. Custom metrics require deliberate design. Here's the process that works:

### Step 1: Define what you're measuring (the quality dimension)

Be specific. Not "quality" but "brand voice consistency." Not "safety" but "HIPAA compliance." Not "accuracy" but "citation quality."

Ask: **What specific aspect of output quality are we trying to capture? Why does it matter to our users or business?**

Write it down in plain English: "This metric measures whether the AI's tone and word choice match our brand guidelines, specifically the warmth and professionalism we use in customer communications."

If you can't explain what you're measuring in one clear sentence, the metric will be fuzzy and unreliable.

### Step 2: Define the scale

How will you score this dimension? Common scales:

**Binary (pass/fail).** Is it compliant or not? Did the action succeed or not? Use when there's a clear line — either it meets the criteria or it doesn't.

**Ordinal (1-5 or 1-10).** Low to high quality. Use when there are degrees — a response can be somewhat on-brand, mostly on-brand, perfectly on-brand.

**Continuous (0-100 or 0.0-1.0).** Finer granularity. Use when you need to rank outputs precisely or when the metric is composite (combining multiple signals).

**Multi-dimensional.** A vector of scores, one per sub-dimension. Example: citation quality might be (relevance: 4/5, trustworthiness: 5/5, coverage: 3/5).

The choice matters. Binary is easiest to score reliably but loses nuance. Continuous is most flexible but harder to score consistently. Start simpler (binary or 1-5), add complexity only if needed.

### Step 3: Write examples at each scale point

This is the most important step. For each possible score, write 3-5 example outputs that should receive that score.

For a brand voice metric (1-5 scale):

**Score 1 (completely off-brand):** "Hey! So I checked and yeah, we can totally help with that. Just hit me up with your order number and I'll sort it out for you!"

**Score 3 (somewhat on-brand):** "Thank you for contacting us. I can assist with that request. Please provide your order number and I will look into it."

**Score 5 (perfectly on-brand):** "Thank you for reaching out. I'd be happy to help you with this. Could you please share your order number so I can look into this for you right away?"

These examples are **anchor points**. They define what each score means in concrete terms. Without them, different evaluators (human or AI) will interpret the scale differently. With them, you get consistency.

The test: Can a new team member or an LLM judge look at these examples and correctly score new outputs? If yes, your scale is well-defined. If no, add more examples or clarify the distinctions between score levels.

### Step 4: Validate against human judgment

Before you deploy a custom metric, validate that it actually measures what you think it does.

The validation process:

1. Select 100-200 diverse outputs from your system
2. Score them with your custom metric (using an LLM judge or automated check)
3. Have domain experts (humans who understand the quality dimension) score the same outputs
4. Measure agreement (correlation for continuous scores, Cohen's kappa for categorical)
5. Look at disagreements — where does the metric fail? What patterns is it missing or misjudging?
6. Refine the metric definition, scale, or examples based on what you learned
7. Repeat until agreement is above 80% (for most tasks) or 90% (for high-stakes)

This is non-negotiable. A custom metric that hasn't been validated against human judgment is just a number. You don't know if it means anything.

Common validation failures:

- Metric correlates with surface features (length, formality) instead of actual quality
- Metric works on easy cases, fails on edge cases
- Metric is systematically biased (always harsher or more lenient than humans)
- Metric has high variance (same output scored differently on different days)

Fix these before you use the metric for real decisions.

---

## Composite Metrics: Combining Multiple Signals

Sometimes your quality definition isn't one thing, it's several things combined. That's when you need **composite metrics**.

Example: For a customer support AI, overall quality might be:

- Accuracy (does it solve the problem?): 40% weight
- Empathy (does it acknowledge the customer's frustration?): 30% weight
- Efficiency (is it concise?): 20% weight
- Brand voice (does it sound like us?): 10% weight

The composite score is a weighted combination: 0.4 × accuracy + 0.3 × empathy + 0.2 × efficiency + 0.1 × brand voice.

### Three ways to combine signals:

**Weighted average.** Each dimension contributes proportionally. Weights reflect importance. Use when all dimensions matter and trade-offs are acceptable (a response can be slightly less empathetic if it's very accurate).

**Worst-of (floor metric).** Overall score is the minimum of all dimensions. Use when every dimension must meet a threshold — if any one is unacceptable, the whole output is unacceptable. Example: "Safety must be 5/5, and quality must be 4/5 or higher. If safety is 3/5, I don't care if quality is 5/5, the output fails."

**Gated scoring.** One dimension is a gate. If it passes, score the others. If it fails, the output fails regardless of other scores. Example: "First check safety. If safe, then score quality. If unsafe, quality score doesn't matter." Common pattern: safety and compliance are gates, quality dimensions are scored afterward.

Which to use? Start with **gated scoring for non-negotiable constraints** (safety, compliance, policy). Use **weighted average for quality dimensions** where trade-offs are reasonable. Use **worst-of sparingly** — it's harsh and makes it hard to ship anything, but it's right for high-stakes domains where weak links break the product.

### Setting the weights:

How do you decide that accuracy is 40% and empathy is 30%? Three approaches:

**User research.** Ask users what matters most. Survey, interviews, or conjoint analysis. "Would you rather have a perfectly accurate response that feels cold, or a warm response that's 90% accurate?"

**Business impact.** What correlates with outcomes you care about? If empathy correlates with CSAT and CSAT correlates with retention, empathy gets higher weight.

**Expert judgment.** Product leaders and domain experts decide based on product strategy and brand positioning. "We're competing on empathy, not speed, so empathy gets 40%."

In practice, teams use all three. Start with expert judgment for a draft, validate with user research, refine based on business data.

And crucially: **The weights aren't permanent.** As your product evolves, your priorities shift. Re-calibrate weights quarterly or when strategy changes.

---

## Proxy Metrics: When You Can't Measure What You Really Want

Sometimes you can't directly measure the quality dimension you care about. So you measure something correlated. That's a **proxy metric**.

Example: You care about "response quality" but you can't afford to have humans score every response. So you use "user engagement" (did the user click the suggested link? did they ask a follow-up question?) as a proxy. The assumption: high-quality responses lead to higher engagement.

Another example: You care about "whether the AI solved the user's problem" but you can't observe that directly. So you measure "conversation length" as a proxy. Short conversations (1-2 turns) are likely successes (quick resolution). Long conversations (10+ turns) are likely failures (user struggling).

Proxy metrics are useful when direct measurement is too expensive or too slow. But they come with a massive trap: **Goodhart's Law.**

### Goodhart's Law: "When a measure becomes a target, it ceases to be a good measure."

Here's how proxy metrics fail:

1. You start measuring "conversation length" as a proxy for "problem resolution."
2. You use it in your eval pipeline. You optimize your AI to have shorter conversations.
3. The AI learns that shorter is better. It starts giving briefer responses.
4. Conversations get shorter — but not because problems are being solved faster. Because the AI is cutting corners, giving incomplete answers, or pushing users away.
5. Your metric (conversation length) improves. Your actual goal (problem resolution) gets worse.
6. The proxy stopped working because you optimized for it.

This happens constantly with proxy metrics. Response length as a proxy for quality leads to verbose fluff. Citation count as a proxy for groundedness leads to over-citation. User engagement as a proxy for satisfaction leads to clickbait.

### How to use proxy metrics safely:

**Never rely on one proxy alone.** Use multiple proxies that are hard to game simultaneously. If you optimize for short conversations AND high user ratings AND low repeat contacts, you can't just cut corners.

**Validate the proxy regularly.** Check that the proxy still correlates with the real thing. Monthly: sample 100 cases, measure both the proxy (conversation length) and the real metric (did the problem get solved?). If correlation drops below 0.6, stop trusting the proxy.

**Use proxies for monitoring, not optimization.** It's safer to use proxies to detect regressions ("conversation length spiked, let's investigate") than to actively optimize for them ("let's make conversations shorter").

**Have circuit breakers.** If the proxy moves in a direction you didn't expect (conversation length dropped by 50% overnight), pause automated decisions and investigate. It might be a real improvement, or it might be the AI gaming the metric.

The 2026 wisdom: **Proxy metrics are useful but dangerous. Treat them like financial derivatives — they can provide leverage but they can also blow up spectacularly if you're not careful.**

---

## Metric Validation: How Do You Know Your Metric Is Measuring What You Think?

You've designed a custom metric. You've written a scoring rubric. You've deployed an LLM judge. How do you know it's actually measuring what you intended?

### Validation check 1: Correlation with human judgment

Have domain experts score a sample of outputs. Compare their scores to your metric scores. Measure correlation (Pearson for continuous, Spearman for ordinal, Cohen's kappa for categorical).

**Target: 0.7+ correlation, 80%+ agreement.**

If correlation is low, your metric and human judgment disagree on what's good. Diagnose: Are humans wrong (they need calibration, Chapter 2.2)? Is the metric wrong (it's measuring something else)? Is the quality dimension fuzzy (the definition needs clarification)?

### Validation check 2: Sensitivity to real quality differences

Take a good output and intentionally degrade it. Does your metric score it lower?

Example: Take a perfectly on-brand customer service response. Rewrite it to be overly casual. Does your brand voice metric catch it? Rewrite it to be robotic and formal. Does the metric catch that too?

If your metric doesn't detect intentional quality degradation, it's not sensitive enough.

### Validation check 3: Stability over time

Score the same output on different days. Does it get the same score?

For automated metrics (exact match, BLEU), stability is 100% by definition. For LLM judges, stability is lower — language models have temperature, they're non-deterministic. But you should still see high consistency (variance below 5% of the scale).

If an output scores 80/100 on Monday and 60/100 on Tuesday with no changes, your metric is unreliable. Fix: lower temperature on your LLM judge, add more structure to the scoring prompt, or switch to a more deterministic method.

### Validation check 4: Agreement across different evaluators

If you're using human scoring, do different humans give similar scores? If you're using multiple LLM judges, do they agree?

Target: 85%+ inter-rater agreement for humans, 80%+ for LLM judges.

Low agreement means your scale or rubric is ambiguous. Common fix: add more anchor examples, clarify edge cases, calibrate raters (Chapter 6.5).

### Validation check 5: Business outcome correlation

This is the ultimate validation: does your metric correlate with real business outcomes?

If you're measuring "empathy" in customer support, does it correlate with CSAT? If you're measuring "action correctness" in an agent, does it correlate with task completion rate? If you're measuring "brand voice," does it correlate with brand perception surveys?

Target: 0.5+ correlation (moderate) with a business outcome you care about.

If your metric has high human agreement but zero correlation with business outcomes, it's measuring something real but not something that matters. Either refine the metric or question whether the dimension is actually important.

The validation is never one-and-done. Re-validate quarterly or after major product changes. Metrics drift as products evolve, as user expectations shift, as language models improve. A metric that was valid in Q1 might be invalid by Q4.

---

## Metric Evolution: Your Metrics Must Grow With Your Product

Here's a truth teams learn the hard way: **A metric designed for v1 may not capture what matters in v3.**

Example: You launch a simple FAQ chatbot. Your quality metric is "answer accuracy" — does the bot give the right answer from the knowledge base? It works great.

Six months later, you add multi-turn conversations. Now "answer accuracy" isn't enough. You need "context retention" — does the bot remember what the user said two turns ago?

A year later, you add agentic capabilities — the bot can look up account info and make changes. Now you need "action correctness" — does it call the right APIs with the right parameters?

Your product evolved. Your metrics must evolve with it. If you're still measuring only "answer accuracy" when your product is doing multi-step agentic workflows, your metrics are stale. You're optimizing for the product you used to have, not the product you have now.

### Signs your metrics are stale:

**Team intuition and metrics diverge.** Engineers say "this feels worse" but metrics say "this is better." Trust intuition first, investigate the metrics.

**User feedback doesn't match metric scores.** Users complain but metrics look good. Metrics are measuring the wrong thing.

**New features aren't covered.** You shipped voice support. Your metrics are all text-based. You're flying blind on voice quality.

**Metrics from six months ago haven't changed.** If your product is evolving but your metrics aren't, you're measuring the past.

### How to evolve metrics:

**Quarterly metric review.** Every quarter, ask: "Do our current metrics still capture what makes our product good or bad?" If the product changed (new features, new use cases, new quality standards), update the metrics.

**Version your metrics.** When you change a metric definition, version it (Brand Voice v1, Brand Voice v2). Track which version you used for each eval run. This prevents confusion when comparing scores across time.

**Deprecate metrics that no longer matter.** If you're no longer using a feature, stop measuring it. Metric sprawl is real — teams accumulate metrics over time and never remove them. Every metric has a cost (compute, cognitive load, dashboard clutter). Prune aggressively.

**Add metrics for new capabilities.** When you ship a new feature, design metrics for it within the first month. Don't wait for problems to emerge. Be proactive about defining and measuring quality for new surfaces.

The 2026 pattern: Teams have a **metric roadmap** alongside their product roadmap. "In Q2 we're shipping agentic workflows, so we need to design and validate action correctness metrics by end of Q1." Metrics are not an afterthought, they're part of the product planning process.

---

## The Metric Registry: Documenting Every Metric

By the time you have 5-10 custom metrics, you have a metric sprawl problem. Different teams use different metrics. Nobody remembers which metric was used in last quarter's eval. Someone says "brand voice score" but there are three different brand voice metrics in the codebase.

You need a **metric registry**. A single source of truth for every metric you use.

### What goes in the registry:

**Metric name and version.** "Brand Voice Consistency v2.1"

**Definition.** "Measures whether the response tone, formality, and word choice match our brand guidelines, specifically warmth + professionalism."

**Scale and anchors.** "1-5 scale. 1 = completely off-brand (overly casual or robotic). 3 = neutral/acceptable. 5 = perfectly on-brand. See linked examples."

**Validation data.** "Validated 2026-01-15 with 200 samples. Human-AI agreement: 84%. Correlation with brand perception survey: 0.62."

**Owner.** "Product: Sarah Chen. Eng: Alex Kumar."

**Usage.** "Used in: customer support eval pipeline, release gates, monthly brand audits."

**Last calibrated.** "2026-01-15. Next calibration due: 2026-04-15."

**Known limitations.** "Works well on standard queries. Struggles with highly technical responses where warmth is less relevant."

**Change log.** "v2.0 (2025-11): Added examples for technical responses. v2.1 (2026-01): Adjusted scale to reduce over-penalization of slightly casual tone."

Why this matters:

**Consistency.** Everyone uses the same definition. No confusion about which metric is which.

**Reproducibility.** Six months from now, someone can look up exactly which metric was used for a past eval run.

**Ownership.** Clear who to ask if the metric is behaving strangely or needs updating.

**Maintenance.** Scheduled recalibration, versioning, and deprecation. Metrics don't rot silently.

**Onboarding.** New team members can read the registry and understand the full metric portfolio.

Implementation: Most teams use a simple doc or wiki. Some teams use tooling (e.g., a YAML file in version control, or a dedicated feature in their eval platform). The format matters less than the discipline of keeping it updated.

The rule: **Every metric you use in production must be in the registry.** No exceptions. If it's not documented, you don't deploy it.

---

## Avoiding Metric Proliferation: Focus on 3-5 Primary Metrics

Here's the trap: You start with one metric. It's not quite right, so you add a second. That one has blind spots, so you add a third. Before long, you have 15 metrics and nobody knows which ones actually matter.

**Metric proliferation** is when you have so many metrics that the signal-to-noise ratio collapses. You can't tell what's important. You spend more time analyzing metrics than improving the product. Different metrics point in different directions, and decision-making becomes paralyzed.

The fix: **Ruthless prioritization. Focus on 3-5 primary metrics that define your product quality.**

### How to choose your primary metrics:

**What do users care most about?** If you could only optimize three things that users notice and value, what would they be?

**What are your product's unique strengths or differentiation?** If you're competing on empathy, empathy is a primary metric. If you're competing on speed, latency is a primary metric.

**What are your highest product risks?** If incorrect actions could cause real harm or cost, action correctness is a primary metric. If regulatory violations are an existential risk, compliance is a primary metric.

The primary metrics are your **North Star**. They're on your main dashboard. They're in your release gates. They're what leadership reviews. They're what you optimize for.

Everything else is a **secondary metric** — useful for debugging, useful for specific domains, but not part of the core quality definition. Secondary metrics live in deeper dashboards, in domain-specific eval pipelines, in research notebooks. They inform decisions but they don't drive them.

Example metric portfolio for a customer support AI:

**Primary:**
1. Problem resolution rate (did we solve the customer's issue?)
2. Brand voice consistency (did we sound like us?)
3. Safety and policy compliance (did we avoid harm and follow guidelines?)

**Secondary:**
- Response latency
- Empathy score (sub-dimension of brand voice)
- Citation quality (when advice is given)
- Escalation rate
- Multi-turn coherence

The primary metrics get 80% of the attention. The secondary metrics get 20%. This keeps you focused.

The discipline: **To add a primary metric, you must remove one.** This forces hard conversations about what actually matters. It prevents drift toward "let's measure everything."

---

## Business-Aligned Metrics: Connecting Eval to Outcomes

Here's the metric that doesn't matter: one that has no correlation with business outcomes.

You might have the most carefully designed, validated, and consistent metric in the world. But if it doesn't connect to something your business cares about — revenue, retention, CSAT, cost, task completion, escalation rate — it's just vanity.

The hard question: **For each of your metrics, can you draw a line from that metric to a business outcome?**

Example chain:

- **Metric:** Brand voice consistency (LLM judge, 1-5 scale)
- **Product outcome:** Brand perception (user survey, quarterly)
- **Business outcome:** Customer retention (measured, quarterly)
- **Economic outcome:** Revenue retention

If brand voice consistency improves → users perceive the brand more positively → they're more likely to stay → revenue retention improves.

You don't need direct causality (that's hard to prove). You need **plausible correlation** backed by some data. If you improve brand voice by 20% and see no change in any downstream metric, either your metric is measuring the wrong thing or the quality dimension doesn't actually matter.

### How to build business alignment:

**Map metrics to outcomes annually.** Once a year, review every primary metric. For each, identify the business outcome it should affect. Pull data. Measure correlation. If correlation is weak or zero, investigate.

**Run experiments.** Deliberately degrade or improve a metric (in a safe test environment). Do downstream outcomes change? If you make responses 30% more empathetic, does CSAT improve? If not, maybe empathy isn't as important as you thought — or maybe your empathy metric isn't capturing what matters.

**Involve business stakeholders in metric design.** When you design a custom metric, bring in CS leaders, product marketing, finance — whoever owns the outcome you're trying to affect. Make sure the metric definition aligns with how they think about quality and success.

**Report metrics in business terms.** Don't just say "brand voice score improved from 3.2 to 3.8." Say "brand voice score improved 19%, which correlates with a 12% improvement in brand perception surveys. We expect this to contribute to higher retention." Make the connection explicit.

The 2026 shift: Eval metrics are no longer just eng metrics. They're product metrics. They're business metrics. The best companies have eval dashboards that sit alongside revenue dashboards, CSAT dashboards, and OKR tracking. Eval is not a side process, it's core to how you measure product success.

---

## 2026 Patterns: Dynamic Weighting, Product-Specific Judges, Metric Marketplaces

Here's where the industry is heading:

### Dynamic metric weighting

Instead of fixed weights (accuracy: 40%, empathy: 30%), weights **adjust based on context**.

Example: For a billing question, accuracy is 60% and empathy is 20%. For a complaint, empathy is 60% and accuracy is 20%. The context (detected intent, user sentiment) changes the weights dynamically.

How it works: Your eval pipeline detects the context, selects the appropriate weight profile, scores with that profile. You maintain multiple weight profiles in your metric registry. More complexity, but much better alignment with what actually matters per scenario.

### Product-specific judge prompts

Instead of generic LLM-as-Judge prompts ("rate this response for helpfulness"), teams are building **judge prompts tailored to their exact product and domain**.

Example: A legal tech company's judge prompt includes:
- Specific legal terminology and concepts
- Examples of good and bad legal responses in their product
- Domain-specific quality criteria (jurisdictional awareness, cite-checking)
- Knowledge of their product's capabilities and constraints

The result: Much higher accuracy than generic judges. The judge "understands" the product's quality standard because it's been explicitly taught.

How to build: Start with a generic judge prompt (Chapter 7.2). Add 10-20 examples from your product (good and bad). Add domain-specific guidance. Add your brand guidelines. Validate against human judgment. Iterate until calibration is above 85%.

### Metric marketplaces

In 2026, companies are starting to share proven custom metrics — anonymized, generalized, but validated.

Example: "Brand voice consistency for B2B SaaS" — a judge prompt, rubric, and validation dataset that multiple B2B companies have used and refined. "Citation quality for RAG in healthcare" — a composite metric that healthcare AI teams have collectively validated.

The trend: As custom metrics mature, the best ones become reusable. Not as rigid as off-the-shelf benchmarks (they still require customization), but not built from scratch either.

Where this lives: Some in open-source eval frameworks (LangSmith, Braintrust, Patronus). Some in consulting/advisory firms that work across multiple companies. Some in informal sharing across companies (conference talks, blog posts, private Slacks).

The future: A library of proven custom metrics that you can adapt to your product, rather than designing every metric from zero. We're not there yet, but the foundation is being built.

---

## Failure Modes and Traps

**Trap 1: Too many metrics.** You measure 20 things and optimize none. Fix: 3-5 primary metrics. Everything else is secondary.

**Trap 2: Optimizing a proxy that stops correlating.** Goodhart's Law kills you. Fix: Validate proxy correlation monthly. Use multiple proxies. Use proxies for detection, not optimization.

**Trap 3: Metrics that aren't validated.** You assume your "brand voice metric" is measuring brand voice. It's actually measuring response length. Fix: Validate every custom metric against human judgment before deploying. Re-validate quarterly.

**Trap 4: Stale metrics.** Your product evolved but your metrics didn't. You're optimizing for the product you used to have. Fix: Quarterly metric review. Version metrics. Deprecate aggressively.

**Trap 5: Metrics without business alignment.** Your metric is technically sound but doesn't correlate with any outcome that matters. Fix: Map every primary metric to a business outcome. If you can't, question whether the metric should be primary.

**Trap 6: Using custom metrics too early.** You build a custom "brand voice" metric when you have 10 users and no validated brand guidelines. Fix: Start with standard metrics. Build custom metrics only when you have (a) clear quality requirements, (b) enough data to validate, and (c) evidence that standard metrics are failing.

**Trap 7: Not documenting metrics.** Six months later, nobody remembers which metric was used or how it was calibrated. Fix: Metric registry (see above). Document everything.

---

## Enterprise Expectations

- They design custom metrics for quality dimensions that differentiate their product (brand voice, domain compliance, workflow correctness)
- They validate every custom metric against human judgment (80%+ agreement) before using it for decisions
- They use composite metrics thoughtfully (weighted average for trade-offs, gating for non-negotiable constraints)
- They treat proxy metrics with caution (multiple proxies, regular validation, circuit breakers)
- They maintain a metric registry (definition, scale, validation, owner, version, last calibrated)
- They focus on 3-5 primary metrics and resist metric proliferation
- They connect every primary metric to a business outcome (revenue, CSAT, retention, cost)
- They evolve metrics as the product evolves (quarterly review, versioning, deprecation)
- They use metrics to make decisions, not to decorate dashboards

---

## A Practical Template: Custom Metric Design Doc

Use this template when designing a new custom metric:

**Metric name:** [Clear, descriptive name]

**Quality dimension:** [What are we measuring? Why does it matter?]

**Scale:** [Binary, 1-5, 0-100? Why this scale?]

**Anchor examples:** [3-5 examples per scale point, showing what each score looks like]

**Measurement method:** [LLM judge? Automated check? Human scoring? Hybrid?]

**Validation plan:** [How will we validate this metric? Sample size, target agreement, timeline]

**Owner:** [Who is responsible for maintaining and calibrating this metric?]

**Usage:** [Where will this metric be used? Release gates, dashboards, research?]

**Calibration schedule:** [How often will we re-validate? Quarterly? After major product changes?]

**Business alignment:** [What business outcome should this metric affect? Expected correlation?]

**Known limitations:** [What does this metric miss? When does it fail?]

**Primary or secondary:** [Is this a primary metric (top 3-5) or secondary (debugging/research)?]

---

## Interview Q&A

**Q: When should I build a custom metric instead of using standard metrics?**

A: Build a custom metric when your product has a quality dimension that standard metrics don't capture and that dimension actually matters to users or business outcomes. Three triggers: First, your team keeps saying "metrics look good but output is wrong" — standard metrics aren't aligned with your quality definition. Second, your product has domain-specific constraints (regulatory compliance, brand voice) that generic metrics can't measure. Third, your success depends on workflow correctness (multi-step agents, API calls) that standard NLG metrics ignore. Start with standard metrics as a baseline. Add custom metrics only when you have clear evidence they're needed and you can validate them properly.

**Q: How do I validate that my custom metric is actually measuring what I think it's measuring?**

A: Five validation checks. First, correlation with human judgment — domain experts score a sample, you compare to your metric, aim for 0.7+ correlation or 80%+ agreement. Second, sensitivity — intentionally degrade output quality, metric should detect it. Third, stability — same output scored on different days should get similar scores, variance under 5%. Fourth, inter-rater agreement — different evaluators should agree 85%+ of the time. Fifth, business outcome correlation — your metric should correlate (0.5+) with a real outcome like CSAT or retention. Run all five checks before deploying. Re-validate quarterly because metrics drift as products and expectations evolve.

**Q: What's the danger of using proxy metrics, and how do I avoid it?**

A: The danger is Goodhart's Law — when you optimize a proxy, it stops being a good proxy. Example: you measure conversation length as a proxy for problem resolution. You optimize for shorter conversations. The AI starts cutting corners, giving incomplete answers to end conversations faster. Conversation length improves but problem resolution gets worse. How to avoid: First, never rely on one proxy alone — use multiple proxies that are hard to game simultaneously. Second, validate the proxy regularly (monthly) — check that it still correlates with the real thing. Third, use proxies for monitoring (detect regressions) not optimization (don't actively try to improve the proxy). Fourth, have circuit breakers — if the proxy moves unexpectedly, pause and investigate before trusting it.

**Q: How do I decide the weights for a composite metric?**

A: Three inputs. First, user research — what do users say matters most? Survey, interviews, or conjoint analysis. Would they rather have perfect accuracy with no empathy, or 90% accuracy with great empathy? Second, business impact — what correlates with outcomes you care about? If empathy correlates with CSAT and CSAT predicts retention, empathy gets higher weight. Third, expert judgment — product leaders decide based on strategy and differentiation. If you're competing on speed, latency gets higher weight. In practice, start with expert judgment for draft weights, validate with user research, refine based on business data. And remember: weights aren't permanent. Re-calibrate quarterly or when priorities shift. To keep focus, use weighted average for quality dimensions, but use gating for non-negotiable constraints like safety.

**Q: How many custom metrics should I have, and how do I avoid metric proliferation?**

A: Focus on 3-5 primary metrics that define your core product quality. Everything else is secondary. Primary metrics are on your main dashboard, in your release gates, what leadership reviews, what you actively optimize. Secondary metrics are for debugging and domain-specific analysis — useful but not driving decisions. To choose primary metrics, ask: what do users care most about? What differentiates your product? What are your highest risks? To avoid proliferation, enforce a rule: to add a primary metric, remove one. This forces hard prioritization conversations. Also, quarterly metric review — deprecate metrics that no longer matter, version metrics when you change them, maintain a metric registry so everyone knows what exists. The trap is measuring everything and optimizing nothing. The discipline is measuring a few things that actually matter and optimizing relentlessly.

---

This is the reality in 2026: off-the-shelf metrics get you started, but custom metrics are what separate good AI products from great ones. The companies shipping AI that users love have spent real time designing metrics that capture their unique definition of quality, validating those metrics rigorously, and connecting them to business outcomes. It's not glamorous work. But it's the work that ensures you're actually measuring — and improving — what matters.

Next, we'll cover safety and policy compliance in automated evaluation, including how to build guardrails that actually work.
