# Chapter 4 — Ground Truth Design

**What we're doing in Chapter 4:**
We're designing "ground truth" — what counts as the correct reference answer (or correct behavior) for evaluation.

This is the backbone of every eval program. If ground truth is messy or inconsistent, then:
- your rubrics become subjective,
- your datasets become noisy,
- your metrics become misleading,
- and your team argues forever.

**Important mindset (2026 enterprise reality):**
Ground truth is not always "one perfect answer."
Often it's:
- a set of acceptable answers,
- a preferred answer,
- or a correct behavior like "abstain" or "ask a clarifying question."

---

## Chapter 4.1 — What Counts as Ground Truth Per Task (Chat vs RAG vs Agents vs Voice)

### Mechanics (how it works)
Different task types require different forms of truth:

- **Chat:** truth can be a *good answer pattern* (clarity + correctness + fit), not a single sentence.
- **RAG:** truth must be *evidence-backed* by the provided sources.
- **Agents:** truth is *the correct action outcome* plus correct tool behavior and safety gates.
- **Voice:** truth includes *what was said* (transcript accuracy), *what should happen* (task outcome), and *how it should feel* (turn-taking, confirmations).

So we define ground truth in a way that matches the task.

---

## 1) Ground truth types (use the right one)

### Type A — Single-answer truth (rare, but easiest)
Use when there is one correct answer:
- "What is the API endpoint name?"
- "Which policy applies to X?"

**Risk:** If reality changes, it becomes stale fast. Version it.

---

### Type B — Multi-answer truth (common in real products)
Use when multiple responses are acceptable:
- "Write a polite email"
- "Suggest marketing ideas"
- "Summarize this in a friendly tone"

Here ground truth is:
- a set of acceptable outputs, OR
- a checklist of required elements (must include / must avoid)

---

### Type C — Behavior truth (critical for modern AI)
Use when the "correct" result is behavior, not content:
- "Ask one clarifying question"
- "Abstain because sources are missing"
- "Refuse safely and redirect"
- "Confirm critical fields in voice"
- "Request approval before sending/charging/deleting"

Behavior truth is huge in enterprise systems because it reduces risk.

---

### Type D — Trace truth (agents and tools)
Use when the correctness depends on steps:
- tool sequence
- parameters used
- state updates
- idempotency behavior
- retries and fallback path

You evaluate the trace, not only the final text.

---

## 2) Task-by-task: what ground truth looks like

### 2.1 Chat ground truth

**What it is**
- A "reference answer outline" (what must be included)
- Allowed variations
- A rubric-based score target

**What good ground truth includes**
- Key facts / constraints that must appear
- Tone constraints (if required)
- "Do not do" list (don't invent, don't overpromise)

**Example ground truth format**
- Must answer X
- Must mention Y
- Must not claim Z
- If uncertain, must say how to verify

---

### 2.2 RAG ground truth

**What it is**
- The answer must be supported by retrieved documents
- Ground truth includes:
  - which sources contain the answer
  - what parts are relevant
  - what should be cited (if citations are required)

**Key rule**
If the docs do not contain the answer, correct behavior is:
- "I can't find this in the provided sources" (abstain)

This is ground truth too.

---

### 2.3 Agents ground truth

**What it is**
- The final state/outcome is correct (task completed)
- The tool behavior is correct:
  - correct tool
  - correct params
  - correct order
  - correct safety checks
  - no duplicate actions

**Ground truth includes**
- success condition (done criteria)
- required confirmations
- allowed retries and fallback paths

---

### 2.4 Voice ground truth

**What it is**
Voice is multi-layer:
- transcript accuracy (did we understand?)
- dialogue behavior (turn-taking, barge-in)
- task outcome (did we resolve?)
- safety/privacy behavior (no leaks)
- critical-field confirmation compliance

So voice truth often includes:
- expected transcript elements
- required confirmations (numbers/dates)
- expected final outcome (appointment booked, ticket created)
- escalation rule if unsure

---

## 3) Knobs & defaults (what you actually set)

### 3.1 Ground truth "strictness" level
Default by task:
- Chat: medium strict (allow variation)
- RAG: high strict (must be evidence-backed)
- Agents: high strict (actions and traces)
- Voice: high strict on critical fields + safety, medium on phrasing

### 3.2 Allowed answer set size
- For creative tasks: many acceptable variants
- For policy/compliance: narrow set or abstain

### 3.3 Ground truth versioning
Always store:
- version number
- date created/updated
- why it changed
- which product/policy release it matches

---

## 4) Failure modes (symptoms + root causes)

### 4.1 Ground truth is too vague

**Symptoms**
- reviewers disagree constantly
- metrics swing depending on rater

**Root cause**
- truth defined as "be helpful"
- no required elements

**Fix**
- add must-include checklist + anchors

---

### 4.2 Ground truth is too strict

**Symptoms**
- good outputs are punished
- creativity is scored as wrong
- model looks worse than it is

**Root cause**
- expecting one perfect wording
- no allowed variation set

**Fix**
- define acceptable variants or scoring rubric instead

---

### 4.3 Ground truth goes stale

**Symptoms**
- model is "wrong" because policy changed
- retrieval docs updated but truth didn't

**Root cause**
- no version control or ownership

**Fix**
- ground truth must be tied to policy/doc versions + reviewed regularly

---

### 4.4 Hidden leakage and contamination

**Symptoms**
- eval looks amazing but production fails
- model "memorized" eval answers

**Root cause**
- eval prompts leaked into training or prompt templates
- no separation between training and eval sets

**Fix**
- strict dataset hygiene (we'll cover this deeply in Chapter 5)

---

## 5) Debug playbook: how to design ground truth correctly

1. Identify task type: chat / RAG / agent / voice
2. Choose ground truth type: single / multi / behavior / trace
3. Write:
   - must-include list
   - must-not list
   - abstain/refusal rules
4. Add 2–5 anchors:
   - great example
   - acceptable example
   - fail example
5. Add edge-case notes:
   - what if docs are missing?
   - what if user is ambiguous?
   - what if tool fails?
6. Version it and assign an owner

---

## 6) Enterprise expectations (what serious teams do)

- Ground truth is treated like a controlled artifact:
  - versioned
  - reviewed
  - auditable
- They maintain:
  - gold set (human-labeled truth)
  - adjudication process for disputes
  - link to source-of-truth docs and policies
- They enforce "correct abstain" as truth for missing evidence cases

---
