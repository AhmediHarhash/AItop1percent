---
title: "6.6 — Scaling Human Review Without Losing Quality"
seoTitle: "Scaling Human Evaluation Programs"
description: "How to go from 5 raters reviewing 100 cases to 50 raters reviewing 10,000 cases without quality collapsing"
date: "2025-01-29"
section: "core"
parent: "Chapter 6: Human Evaluation"
order: 6
---

# 6.6 — Scaling Human Review Without Losing Quality

Let me tell you about a medical device company that learned this lesson the hard way.

They had a team of three senior clinical specialists reviewing AI diagnostic suggestions. Each case took about 15 minutes. They carefully discussed edge cases. Quality was excellent. Then the product launched.

Volume went from 100 cases a week to 10,000. The VP of Quality hired 47 new reviewers from a contractor firm, gave them a 90-minute training session, and turned them loose. Within two weeks, quality had collapsed. Some reviewers were rubber-stamping everything. Others were flagging obvious correct answers. The senior specialists spent all their time firefighting inconsistencies instead of reviewing hard cases.

Six months later, after rebuilding the program from scratch, they had learned something crucial: **scaling human evaluation is not a hiring problem, it's a system design problem.**

This chapter walks you through how to build that system. How to go from 5 raters reviewing 100 cases to 50 raters reviewing 10,000 cases without everything falling apart.

---

## Why This Matters

Human evaluation is the gold standard. It's also expensive, slow, and hard to scale. Every AI team eventually hits this wall.

At small scale, you can use heroic effort. Senior engineers review every output. You discuss edge cases in Slack. You fix inconsistencies ad-hoc. This works for 100 cases. It breaks at 1,000.

At large scale, you need **infrastructure**. You need systems that maintain quality even when most of your raters are contractors you hired last week. You need real-time quality monitoring, smart routing, embedded checks, and automation assists.

The difference between teams that scale successfully and teams that collapse:

- **Failing teams** treat scaling as a hiring problem. They hire more people and hope for the best.
- **Successful teams** treat scaling as a system design problem. They build quality control into every step.

Let me show you how the successful teams do it.

---

## The Scaling Paradox

Here's the fundamental tension: **more raters = more throughput, but also more inconsistency.**

With one rater, you have perfect consistency (with themselves). With two raters, you get some disagreement. With fifty raters, you get chaos unless you have very strong systems.

The problems compound:

**Training burden multiplies.** With 3 raters, you can spend a day training them personally. With 50 raters, you need documented processes, video tutorials, and tiered training programs.

**Quality variance explodes.** Even if your average rater quality is good, variance matters. One bad rater can pollute hundreds of labels. You need real-time quality monitoring to catch this.

**Communication overhead scales non-linearly.** With 3 raters, you can all talk. With 50 raters, you need formal escalation paths, FAQ documents, and office hours.

**Context degrades.** Your first 3 raters understand the product deeply. Your 50th rater started yesterday and is working from a rubric. The rubric needs to be extremely good.

The way you handle this paradox: **you don't fight it, you design around it.**

You accept that large teams will be less consistent than small teams. Your job is to build systems that maintain acceptable quality despite this reality.

---

## Batch Design — The Foundation

The atomic unit of scaled human review is the **batch**. Not the individual case. The batch.

A batch is a set of cases assigned to one rater for one review session. Batch design determines everything else.

**Typical batch size:** 20-50 cases, depending on complexity.

Too small (5 cases) and you waste time on context switching and training overhead. Too large (200 cases) and you lose real-time quality feedback.

**What goes in a batch:**

1. **Mix of difficulty levels.** Don't give a rater 50 hard cases in a row. They'll burn out and quality will drop. Mix easy, medium, and hard. This keeps engagement high and gives you signal on whether they can handle the easy stuff correctly.

2. **Embedded gold questions.** 10-20% of every batch should be pre-labeled gold standard cases that you already know the right answer to. These are invisible to the rater. They think it's a normal case. You use their answers to track quality in real time.

3. **Coverage of case types.** If your data has 5 different case types (customer support, refund, complaint, question, feedback), make sure each batch has a representative sample. Don't let one rater only see easy questions while another only sees hard complaints.

**Example batch structure:**

- 40 unlabeled cases (what you actually need labeled)
- 5 easy gold cases (90%+ of raters should get these right)
- 3 medium gold cases (70% of raters should get these right)
- 2 hard gold cases (50% of raters should get these right)

Total: 50 cases per batch, 20% gold, mix of difficulty.

The rater reviews all 50. They don't know which 10 are gold. As soon as they submit, you instantly know if they passed your quality bar by checking their gold question accuracy.

---

## Gold Question Monitoring — Real-Time Quality Control

Gold questions are your early warning system. They let you catch quality problems immediately, not two weeks later.

**How it works:**

You have a set of 200-500 pre-labeled cases that you're extremely confident about. These are your **gold set**. You embed 5-10 of these in every batch. The rater doesn't know which ones are gold.

As soon as the rater submits their batch, you calculate their **gold accuracy**:

- Gold accuracy = (gold questions answered correctly) / (total gold questions in batch)

If their gold accuracy is below your threshold (usually 70-80%), you take action immediately.

**Action ladder:**

- Gold accuracy 80%+: Good, no action needed
- Gold accuracy 70-79%: Warning flag, review their work manually
- Gold accuracy 60-69%: Immediate feedback and retraining required
- Gold accuracy below 60%: Block further work, escalate to manager

This is real-time. The rater submits batch 1 at 9:00 AM. By 9:01 AM you know if they're passing the quality bar. If they're not, you don't let them do batch 2.

**Gold set maintenance:**

Your gold set needs care. Cases that get less than 90% agreement from expert raters should be removed. Cases that are too easy (100% of raters get them right) are less useful for quality control. Cases that are too hard (only 30% get them right) might not be good gold questions because the "right" answer isn't actually clear.

The sweet spot: gold questions where 70-90% of trained raters get the right answer. Hard enough to catch bad raters, easy enough that good raters consistently pass.

Refresh your gold set every quarter. Over time, your product changes, edge cases evolve, and old gold questions become stale.

---

## Tiered Review — Match Raters to Difficulty

Not everything needs expert review. In fact, routing everything to experts is wasteful and slow.

**The tiered model:**

- **Tier 1: General raters.** Handle 70% of volume. Clear-cut cases. Fast turnaround. Lower cost.
- **Tier 2: Specialist raters.** Handle 25% of volume. Domain-specific cases (medical, legal, technical). Higher expertise required.
- **Tier 3: Expert reviewers.** Handle 5% of volume. Edge cases, disputes, escalations. Highest cost, highest quality.

The key is **smart routing**. You need a system that automatically routes cases to the right tier.

**How to route:**

1. **Rule-based routing.** If the case involves medical terminology, route to Tier 2 medical specialists. If the case is a user complaint about billing, route to Tier 2 finance specialists. If the case is a standard question, route to Tier 1.

2. **Model-based routing.** Train a simple classifier that predicts case difficulty based on features (length, topic, sentiment, etc.). Easy cases go to Tier 1. Hard cases go to Tier 2 or 3.

3. **Escalation-based routing.** Start everything in Tier 1. If the rater clicks "unsure" or "escalate," the case automatically moves to Tier 2. This is how you discover which cases are actually hard.

**Cost-quality tradeoff:**

- Tier 1 rater: $15/hour, reviews 20 cases/hour = $0.75 per case
- Tier 2 specialist: $40/hour, reviews 10 cases/hour = $4 per case
- Tier 3 expert: $100/hour, reviews 5 cases/hour = $20 per case

If you route intelligently, your blended cost is around $1.50 per case (70% at $0.75, 25% at $4, 5% at $20). If you route everything to Tier 3, your cost is $20 per case.

This is a 13x cost difference for the same volume.

---

## Review Queues and Routing Logic

At scale, you need a **queue system**. Raters don't pick their own cases. The system assigns cases to raters based on their tier, expertise, and current workload.

**Queue design:**

Each rater has a personal queue. When they log in, they see their next batch. They review it, submit it, and the next batch appears. They never see the full backlog. This reduces choice paralysis and gaming.

**Routing logic:**

The system maintains several pools:

- **General pool:** All new cases start here
- **Tier 1 queue:** Cases routed to general raters
- **Tier 2 queue:** Cases routed to specialists (by domain)
- **Tier 3 queue:** Cases routed to experts
- **Escalation queue:** Cases flagged as "unsure" by lower tiers
- **Dispute queue:** Cases with disagreement between raters

**How a case flows:**

1. New case arrives → enters general pool
2. Routing classifier runs → assigns to Tier 1, 2, or 3 based on predicted difficulty
3. Case enters appropriate queue
4. Next available rater in that tier gets the case
5. Rater reviews case:
   - If confident: submits label
   - If unsure: clicks "escalate" → case moves to next tier up
6. If case requires multi-rater review (see below), it goes to a second rater
7. If raters disagree, case goes to dispute queue for expert adjudication

**Queue monitoring:**

You need dashboards that show:

- Queue depth (how many cases in each queue)
- Average wait time (how long until a case gets reviewed)
- Throughput (cases reviewed per hour, per tier)
- Quality metrics (gold accuracy per tier)

If Tier 2 queue is backing up, you need to hire more specialists or route fewer cases there. If Tier 1 quality is dropping, you need to retrain or adjust the routing to send harder cases to Tier 2.

---

## Throughput vs Quality — Finding the Right Speed

How fast should raters work? This is a critical question with no universal answer.

**Throughput varies by task complexity:**

- Simple binary classification (safe/unsafe): 40-60 cases per hour
- Short text rating (1-5 stars with explanation): 20-30 cases per hour
- Long text review with detailed rubric: 5-10 cases per hour
- Multi-step evaluation with research required: 2-5 cases per hour

If your raters are going faster than these benchmarks, they're probably not doing quality work. If they're going much slower, they might need retraining or the task is too complex.

**The speed-quality curve:**

Quality is high when raters work at their natural pace. Quality drops when you push them to go faster. But quality also drops when they go too slow (fatigue, boredom, overthinking).

The sweet spot: **comfortable but focused pace.**

**How to find it:**

1. During calibration, track how long expert raters take on average
2. Set that as your baseline (e.g., 3 minutes per case)
3. Allow 50% variance (1.5 to 4.5 minutes per case is acceptable)
4. Flag raters who are consistently outside this range

If a rater is reviewing 60 cases per hour when your baseline is 20 cases per hour, they're rushing. Check their gold accuracy. It's probably low.

If a rater is reviewing 5 cases per hour when your baseline is 20, they might be overthinking or distracted. Talk to them. Maybe the rubric is unclear. Maybe they need more training.

**Quality kills to watch for:**

- **Speed pressure.** If you tell raters "we need 1000 labels by Friday," quality drops. Don't create artificial urgency unless it's real.
- **Quota systems.** "You must review 200 cases per day." This incentivizes speed over quality. Better: "Review as many cases as you can while maintaining 80%+ gold accuracy."
- **Piece-rate payment.** Paying per label (not per hour) creates perverse incentives to rush. Avoid if possible.

The teams with the best quality pay hourly, set quality bars (gold accuracy thresholds), and let raters work at their natural pace.

---

## Multi-Rater Redundancy — When to Use It

For some cases, one rater isn't enough. You need two or three independent ratings.

**When to use multi-rater review:**

1. **High stakes cases.** Medical diagnosis, legal risk, safety violations. Get it wrong and real harm occurs. Use 2-3 raters, require agreement.

2. **Ambiguous cases.** If your task has a lot of subjective judgment, use multiple raters to measure agreement. If raters disagree often, your rubric is too vague.

3. **Quality auditing.** Even for low-stakes tasks, randomly sample 10-20% of cases for multi-rater review. This lets you measure inter-rater agreement (see Chapter 6.5) and catch drift.

4. **Training and calibration.** When onboarding new raters, have them review the same cases as experienced raters. Compare their answers. This reveals gaps in understanding.

**When NOT to use multi-rater review:**

- Low-stakes, high-volume tasks where cost matters more than perfection
- Cases where the answer is objective and clear (does this image contain a cat? yes/no)
- When you have strong gold question monitoring and high rater quality

**How to decide:**

Use this formula: **Expected cost of error × probability of error × volume.**

If one wrong label could cost you $10,000 (lawsuit, user harm), and your single-rater error rate is 5%, and you're reviewing 1,000 cases, your expected loss is $10,000 × 0.05 × 1,000 = $500,000.

If adding a second rater costs $1 per case ($1,000 total) and drops your error rate to 1%, your expected loss is now $10,000 × 0.01 × 1,000 = $100,000. You spent $1,000 to save $400,000.

Multi-rater review is worth it.

On the other hand, if one wrong label costs you $0 (you're just gathering preference data for model training), and you're reviewing 100,000 cases, multi-rater review doubles your cost for no benefit.

**Adjudication process:**

When you use multi-rater review, you need a plan for disagreement.

- **2 raters, full agreement:** Accept the label
- **2 raters, disagreement:** Send to Tier 3 expert for adjudication
- **3 raters, 2-1 agreement:** Accept the majority label
- **3 raters, 1-1-1 disagreement:** Send to expert adjudication or discard the case (the ground truth is too ambiguous)

Track your **agreement rate**. If raters agree less than 70% of the time, your task is poorly defined or your raters need retraining.

---

## Escalation Workflows — What Happens When Raters Are Unsure

A good human evaluation system has a clear **escalation path**.

Raters should never feel forced to guess. If they're unsure, they should be able to escalate the case to someone with more expertise.

**Escalation button:**

Every case should have an "I'm unsure / escalate this case" button. When clicked:

1. The case is removed from the rater's queue
2. The case is added to the escalation queue for the next tier up
3. The rater gets credit for completing the case (you don't penalize them for escalating)
4. The system tracks escalation rate per rater

**Escalation rate monitoring:**

If a rater escalates 50% of their cases, they're under-trained or mis-tiered. They should be reviewing easier cases or should get more training.

If a rater escalates 0% of their cases, they might be overconfident or guessing. Check their gold accuracy.

Healthy escalation rate: **5-15%** for Tier 1 raters, **2-5%** for Tier 2 raters.

**Escalation reasons:**

When a rater escalates, they should select a reason:

- Ambiguous input (the user's message is unclear)
- Outside my expertise (requires domain knowledge I don't have)
- Edge case not covered in rubric
- Conflicting guidelines
- Potential safety/legal issue

This feedback is gold. It tells you where your rubric is weak and where you need to add guidance.

**Fast escalation response:**

The worst thing you can do: let escalated cases sit in a queue for days. The rater who escalated will never get feedback. They'll stop escalating and start guessing.

Best practice: **Expert adjudicators review escalations within 4 hours.** They resolve the case and send feedback to the rater who escalated: "Good catch, this was ambiguous. Here's how to handle cases like this in the future."

This creates a learning loop. Raters improve over time because they get feedback on their edge cases.

---

## Quality Control Dashboards — What to Track

You can't manage what you don't measure. At scale, you need dashboards that show rater-level quality in real time.

**Per-rater metrics:**

1. **Gold accuracy.** Percentage of gold questions answered correctly. Track per batch and cumulative. This is your primary quality signal.

2. **Agreement with peers.** For cases that get multi-rater review, how often does this rater agree with others? Low agreement means they're interpreting the rubric differently.

3. **Speed.** Cases per hour. Flag raters who are much faster or much slower than the median.

4. **Escalation rate.** Percentage of cases escalated. Too high = under-trained. Too low = overconfident.

5. **Skip rate.** If raters can skip cases, track how often they do. High skip rate might mean they're cherry-picking easy cases.

6. **Consistency over time.** Is their gold accuracy stable, or does it fluctuate? Fluctuation suggests fatigue or distraction.

**Traffic light system:**

For each metric, define green/yellow/red thresholds:

- Gold accuracy: 80%+ green, 70-79% yellow, below 70% red
- Escalation rate: 5-15% green, 2-5% or 15-25% yellow, below 2% or above 25% red
- Speed: Within 50% of median green, 50-100% of median yellow, more than 100% off median red

Each rater gets a dashboard that shows their current status. If any metric is yellow, they get a warning. If any metric is red, they're blocked from new work until they talk to a manager.

**Team-level metrics:**

1. **Overall throughput.** Cases reviewed per day, by tier.
2. **Overall quality.** Average gold accuracy across all raters.
3. **Queue depth.** How many cases are waiting for review.
4. **Turnaround time.** How long from case creation to case reviewed.
5. **Cost per label.** Total cost / total labels. Track over time.

These metrics tell you if your program is healthy or if you need to hire, retrain, or adjust routing.

---

## Automation Assist at Scale — Humans Review What Matters

The smartest way to scale human review: **don't make humans review everything.**

Use automation to filter out the easy cases. Humans only review what's ambiguous or high-stakes.

**Pre-filtering with automated checks:**

Run every case through a fast automated evaluation first:

- **Obvious failures.** If the model output is empty, contains profanity, or violates a hard constraint, flag it automatically. Don't send it to a human.
- **Obvious successes.** If the model output passes all automated checks and looks structurally correct, mark it as "likely good" and only send 10% to humans for spot-checking.
- **Ambiguous cases.** If the automated checks are unsure, send it to human review.

This is how you go from reviewing 10,000 cases to reviewing 2,000 cases while maintaining quality.

**2026 pattern: LLM pre-screening + human verification**

Use a strong LLM (Claude Opus 4.5, GPT-4, etc.) as a first-pass evaluator. The LLM reviews every case and assigns a confidence score.

- High confidence + pass → Auto-accept (spot-check 5%)
- High confidence + fail → Auto-reject (spot-check 5%)
- Low confidence → Send to human review (100%)

The LLM handles 70-80% of cases automatically. Humans focus on the 20-30% that are genuinely ambiguous.

**Important:** The LLM is not the ground truth. It's a filter. Humans are still the source of truth. You use the LLM to reduce human workload, not replace humans.

**Active learning for human review:**

Instead of reviewing cases at random, use **active learning** to select the most informative cases.

Informative cases are:

- Cases where the model is uncertain (low confidence)
- Cases near the decision boundary (score close to your threshold)
- Cases that are representative of underrepresented slices of your data

By focusing human review on these cases, you get more signal per label. This is especially useful when training or fine-tuning models based on human feedback.

**Automation assist in the UI:**

Give human raters tools that speed up their work without reducing quality:

- **Pre-filled suggestions.** Show them the model's output and the automated check results. They can accept, edit, or override.
- **Similar case lookup.** "Here are 3 similar cases and how they were labeled." Helps with consistency.
- **Auto-highlighting.** Highlight potential issues in the model output (factual claims, dates, numbers, etc.). Raters know where to focus.

These tools can 2x rater throughput without hurting quality.

---

## Cost Modeling at Scale — Budgeting for Human Eval Programs

Human evaluation is expensive. You need to budget realistically.

**Cost components:**

1. **Rater labor.** Hourly rate × hours worked. This is 70-90% of total cost.
2. **Training overhead.** Time spent training new raters, creating training materials, running calibration sessions.
3. **Tooling and infrastructure.** Building and maintaining your review platform, dashboards, queue system.
4. **Quality control.** Expert time spent on adjudication, gold set curation, rater feedback.
5. **Program management.** Project managers who coordinate the program, hire raters, track metrics.

**Example budget for 10,000 labels:**

- 10,000 labels, average 3 minutes per label = 30,000 minutes = 500 hours
- Tier 1 raters ($15/hour) handle 70%: 350 hours × $15 = $5,250
- Tier 2 raters ($40/hour) handle 25%: 125 hours × $40 = $5,000
- Tier 3 raters ($100/hour) handle 5%: 25 hours × $100 = $2,500
- Total rater labor: $12,750

- Training overhead (10% of labor): $1,275
- Tooling (one-time build cost amortized): $500
- Quality control (expert time, 5% of labels): $1,000
- Program management (20% of labor): $2,550

**Total: $18,075 for 10,000 labels = $1.81 per label.**

This is realistic for a medium-complexity task with tiered review.

**Cost per label varies widely:**

- Simple binary classification: $0.10 - $0.50 per label
- Medium complexity rating: $1 - $3 per label
- Complex evaluation with research: $5 - $20 per label
- Expert medical/legal review: $20 - $100 per label

**How to reduce cost without killing quality:**

1. **Better routing.** Send fewer cases to expensive tiers.
2. **Automation assist.** Pre-filter easy cases.
3. **Invest in training.** Better-trained raters work faster and make fewer errors.
4. **Gold question monitoring.** Catch bad raters early before they pollute hundreds of labels.
5. **Tooling improvements.** Better UI, keyboard shortcuts, auto-suggestions can increase throughput 20-30%.

**The false economy of cheap labor:**

Some teams try to save money by hiring the cheapest raters possible. This backfires.

A $5/hour rater who produces 50% accurate labels is worse than a $15/hour rater who produces 90% accurate labels. The cheap labels are worthless. You'll have to throw them out and redo the work.

Better strategy: **Hire good raters, pay them fairly, invest in training, and use automation to reduce volume.**

---

## Failure Modes When Scaling

Let me show you the common ways human evaluation programs collapse at scale.

**Failure mode 1: No real-time quality monitoring.**

You hire 50 raters, they work for two weeks, and only then do you check their work. You discover 20 of them were doing it wrong the entire time. You've wasted two weeks and thousands of dollars.

Prevention: Gold question monitoring from day one. Check quality after every batch.

**Failure mode 2: One-size-fits-all routing.**

You send every case to every rater type. Your expensive experts waste time on trivial cases. Your cheap generalists mess up complex cases.

Prevention: Tiered routing. Match case difficulty to rater expertise.

**Failure mode 3: No escalation path.**

Raters encounter edge cases and have to guess. They guess wrong. Quality drops.

Prevention: Clear escalation workflow with fast expert response.

**Failure mode 4: Training once, never again.**

You train raters on day one, then never give them feedback. They develop bad habits. They misinterpret the rubric. Quality drifts over time.

Prevention: Continuous feedback. Weekly calibration sessions. Feedback on escalated cases.

**Failure mode 5: Optimizing for speed over quality.**

You set aggressive quotas. You pay per label. Raters rush. Quality collapses.

Prevention: Pay hourly. Set quality bars, not speed targets.

**Failure mode 6: Ignoring rater feedback.**

Raters tell you the rubric is unclear or the task is too hard. You ignore them. They keep struggling. Quality stays low.

Prevention: Regular office hours. Read escalation reasons. Update rubric based on feedback.

**Failure mode 7: Stale gold sets.**

Your gold questions don't reflect current edge cases. Raters pass gold checks but still produce low-quality labels on real cases.

Prevention: Refresh gold set quarterly. Add new gold questions based on recent edge cases.

**Failure mode 8: No cost tracking.**

You don't track cost per label. You don't notice when costs are spiraling. Your budget explodes.

Prevention: Dashboard with cost per label, updated daily.

---

## Enterprise Expectations — What Great Programs Look Like

When I work with AI teams at scale, here's what I expect to see in a mature human evaluation program:

**Documented and versioned rubrics.** Your evaluation criteria are written down, examples included, version-controlled. When the rubric changes, you track it.

**Multi-tier rater structure.** General raters, specialists, experts. Clear routing logic. Cost and quality optimized.

**Real-time quality dashboards.** Every rater can see their gold accuracy, speed, and escalation rate. Managers can see team-level metrics.

**Gold question monitoring.** 10-20% of every batch is pre-labeled gold. Raters below 70% gold accuracy are blocked from new work.

**Fast escalation loop.** Raters can escalate unclear cases. Experts respond within 4 hours with feedback.

**Continuous training.** Weekly calibration sessions. Feedback on edge cases. Rubric updates based on rater questions.

**Automation assist.** LLM pre-screening or automated checks filter easy cases. Humans review the ambiguous 20-30%.

**Cost tracking.** You know your cost per label, broken down by tier. You can forecast budget for new projects.

**Inter-rater agreement monitoring.** For multi-rater cases, you track agreement rates. Low agreement triggers rubric review.

**Rater feedback loops.** Raters can submit questions, suggest rubric improvements, flag confusing cases. You act on this feedback.

**Onboarding process.** New raters go through structured training, review practice cases with known answers, shadow experienced raters, then review real cases under supervision before going solo.

**Quality audits.** 10% of all labels are spot-checked by experts, even for cases that passed gold checks. This catches subtle drift.

This is what world-class looks like. You don't need all of this on day one. But if you're running a human eval program at scale (10,000+ labels per month), you should be working toward this.

---

## Interview Q&A

**Q: We're scaling from 5 raters to 50. What's the single most important thing to get right?**

Real-time quality monitoring with gold questions. Everything else can be fixed over time. But if you don't catch bad raters immediately, they'll pollute thousands of labels before you notice. Embed 10-20% pre-labeled gold cases in every batch. Check gold accuracy after every batch. Block raters below 70% accuracy from doing more work until you retrain them. This one system prevents 80% of scaling disasters.

**Q: How do I know if I should use multi-rater review or single-rater review?**

Use this formula: expected cost of error × probability of error × volume. If one wrong label could cost you $10,000 and your single-rater error rate is 5%, and you're reviewing 1,000 cases, your expected loss is $500,000. If adding a second rater costs $1,000 and drops error rate to 1%, you save $400,000. Multi-rater is worth it. For low-stakes tasks where wrong labels don't cause real harm, single-rater with strong gold monitoring is usually fine.

**Q: What's a realistic throughput target? How many cases per hour should raters complete?**

It depends entirely on task complexity. Simple binary classification: 40-60 cases per hour. Short text rating with explanation: 20-30 per hour. Long text review with detailed rubric: 5-10 per hour. Complex multi-step evaluation: 2-5 per hour. The key is to measure what your expert raters do naturally during calibration, then use that as your baseline. Don't push raters to go faster than that baseline. Speed pressure kills quality.

**Q: When should we use LLM pre-screening vs just having humans review everything?**

When you have high volume (1,000+ cases per week) and budget constraints. Use a strong LLM to review every case first. High-confidence passes and fails get auto-decided with 5% spot-checking. Low-confidence cases go to humans. This typically reduces human review load by 70-80% while maintaining quality. The LLM isn't ground truth, it's a filter. Humans are still the source of truth. Just don't make them review obvious cases.

**Q: Our raters' quality was good initially but has degraded over time. What's happening?**

This is called rater drift. Three common causes: one, they're not getting feedback so bad habits develop. Solution: weekly calibration and feedback on edge cases. Two, the rubric is ambiguous so they're inventing their own interpretations. Solution: clarify rubric, add examples. Three, they're fatigued or bored from repetitive work. Solution: rotate raters between tasks, mix easy and hard cases, ensure reasonable workload. Track gold accuracy over time. When it drops, investigate immediately. Chapter 6.7 covers rater drift, fatigue, and bias in depth.

---

The difference between 5 raters and 50 raters isn't just volume. It's infrastructure. The teams that scale successfully treat human evaluation as a system with quality controls, not just a hiring problem.

Next, let's talk about what happens when that system starts to degrade: rater drift, fatigue, and bias.

