# Rule-Based & Heuristic Checks

A senior engineer at a fintech startup once told me about their first AI deployment disaster. They'd built a customer service chatbot that answered questions beautifully in testing. Two hours after launch, it leaked a customer's social security number in a support transcript. The model had simply echoed back information from the conversation history, trying to be helpful. No one had written a single rule to check for PII patterns in outputs.

That's when they learned the hard lesson: **before you ask whether an AI response is good, you need to verify it's safe**. And before you spend money on expensive human reviewers or LLM judges, you need to catch the obvious failures that should never have happened.

Rule-based evaluation is the unsexy foundation of production AI. It's the bouncer at the door, the smoke detector in the ceiling, the spell-checker that runs before your essay gets graded. Rules don't tell you if your response is insightful or empathetic or clever. They tell you if it's structurally valid, compliant with basic constraints, and free of known failure patterns.

In 2026, every production LLM system runs rules first. They're fast, cheap, deterministic, and catch 60-80% of failures before anything else runs. This chapter is about building that first line of defense.

---

## Why Start With Rules

**Rule-based checks** are deterministic algorithms that evaluate outputs against explicit criteria. They're the simplest form of automated evaluation: if condition X is true, then pass; otherwise, fail. No machine learning, no subjective judgment, no cost per call.

Here's why rules come first in every eval pipeline:

**Speed and cost.** A regex pattern match runs in microseconds and costs nothing. An LLM judge call takes 500ms and costs $0.001-0.01 per evaluation. When you're evaluating millions of responses, those differences compound. Rules let you filter out obvious failures before spending money on expensive evaluation methods.

**Determinism and debuggability.** Rules produce the same result every time for the same input. If a rule fails, you know exactly what triggered it. You can write a unit test for it. You can explain it to legal and compliance teams in plain English. There's zero ambiguity about what passed and what didn't.

**Immediate feedback.** Rules can run synchronously in production at inference time. The moment your model generates a response, you can validate it before showing it to the user. You don't need to wait for batch evaluation jobs or human reviewers. You catch failures in real time.

**Failure prevention, not just detection.** When rules run at inference, they're not just measuring quality—they're acting as **guardrails**. If a response fails a critical rule, you can block it, regenerate, or fall back to a safe default. Rules prevent bad outputs from reaching users.

**Zero training or tuning required.** Unlike machine learning classifiers, rules don't need labeled training data or hyperparameter optimization. You write the rule once, test it on examples, and deploy it. No model drift, no retraining cycles.

The limitation is that rules can't judge quality, nuance, or semantic correctness. They can tell you that a response contains a citation, but not whether that citation is relevant. They can verify JSON structure, but not whether the data inside makes sense. That's fine—rules aren't meant to replace human or model-based evaluation. They're meant to catch mechanical failures before the expensive stuff runs.

---

## What Rules Can Catch

Rule-based checks excel at validating **structural properties** and **surface-level constraints**. Here's the taxonomy of what rules are good at:

### Format Compliance

The most common rule category: validating that outputs conform to expected formats.

**JSON structure validation.** If your LLM is supposed to return structured data, a rule checks whether it's valid JSON before you try to parse it. Does it have matching braces? Are all strings quoted? Are there any syntax errors?

Beyond basic validity, you check for **schema compliance**: does the JSON contain the required fields? Are the data types correct? Is the "confidence" field a float between 0 and 1? Is the "category" field one of the allowed enum values?

**Field presence and completeness.** Even if the JSON is valid, rules verify that all required fields are populated. If your chatbot is supposed to return a response with "answer", "sources", and "confidence_score", a rule ensures none of them are null or empty.

**Length and boundary constraints.** Rules enforce that text fields fall within acceptable ranges. An email subject line should be 50-100 characters. A product description should be 200-500 words. A SQL query should be fewer than 1000 characters. These constraints prevent truncation bugs, UI overflow, and runaway generation.

**Encoding and character set validation.** Rules check for unexpected characters—non-ASCII in fields that should be English-only, emoji in formal business text, null bytes that break downstream systems, control characters that mess up logs.

### Required Content Elements

Rules that verify specific content is present in the output.

**Citations and source attribution.** If your RAG system is required to cite sources, a rule checks that every response contains at least one citation in the expected format. For example, every answer must include at least one markdown link or a "Source:" section.

**Disclaimers and legal boilerplate.** For regulated industries, certain outputs must include specific language. Medical advice needs "This is not professional medical advice, consult a doctor." Financial recommendations need risk disclosures. Rules verify these disclaimers appear verbatim.

**Required structural elements.** If your model generates reports, rules verify the expected sections are present: Executive Summary, Analysis, Recommendations. If it generates code, rules check for docstrings, type hints, or error handling blocks.

**Keyword or phrase inclusion.** For brand voice or compliance reasons, certain terms might be mandatory. A customer support response might be required to thank the customer. A sales email might need to mention the product name. Rules verify these elements are present.

### Forbidden Content Patterns

Rules that detect content that should never appear in outputs.

**PII and sensitive data.** Pattern matching for social security numbers, credit card numbers, email addresses, phone numbers, API keys, passwords. If your model shouldn't be emitting these, rules catch them before they leak. This is the most critical category for most enterprises.

**Banned words and phrases.** Profanity filters, slur detection, competitor mentions, prohibited medical claims, legally problematic terms. You maintain a blocklist and scan outputs for exact or fuzzy matches.

**Unsafe instructions or recommendations.** Rules that flag dangerous advice: instructions to harm oneself, bypass security measures, engage in illegal activity. These are often keyword-based triggers that escalate to human review.

**Hallucination markers.** Certain phrases are red flags that the model is making things up: "I don't have access to real-time data but here's what I imagine…", "I'll simulate the result…", "Let's pretend that…". Rules can catch these hedging patterns.

**Brand and competitor mentions.** If your chatbot shouldn't name competitors or specific products, rules scan for those terms. If you have trademark restrictions, rules enforce them.

### Response Structure Validation

Rules that check the organization and formatting of responses.

**Markdown or HTML correctness.** If your model outputs markdown, rules verify that links are properly formatted, lists are structured correctly, headers follow a hierarchy, code blocks have closing tags.

**Paragraph and sentence structure.** Rules that count paragraphs, check for excessive run-on sentences, verify that responses are broken into readable chunks. A 500-word wall of text might fail a readability rule.

**Bullet point and list formatting.** If your prompt asks for a bulleted list, a rule verifies the output actually contains list markers. If it asks for numbered steps, a rule checks for sequential numbering.

**Table or data structure formatting.** If the model generates tables, rules validate column alignment, header rows, and cell formatting. If it generates CSVs, rules check for consistent delimiter usage and row length.

### Threshold and Boundary Checks

Quantitative rules that verify outputs fall within acceptable ranges.

**Response length limits.** Maximum and minimum character or word counts. A customer support response should be at least 50 words but no more than 500. A tweet should be under 280 characters. These prevent both insufficient and excessive verbosity.

**Latency constraints.** If your SLA requires responses in under 2 seconds, a rule checks generation time. If a request takes longer than 5 seconds, it's flagged for investigation even if the output is valid.

**Token count and cost.** Rules that track how many tokens were consumed in generation. If a response uses 10,000 tokens when the prompt asked for a short answer, something went wrong. Cost-per-request rules flag outliers for cost control.

**Repetition detection.** Rules that check for repeated phrases, sentences, or n-grams within a response. If the model says the same thing three times, it's likely stuck in a loop. Common in early 2025 with long-context models.

**Confidence thresholds.** If your model outputs a confidence score, rules enforce minimum thresholds. If confidence is below 0.6, the response might not be shown to users or is escalated to human review.

---

## Pattern Matching and Regex

The workhorse of rule-based evaluation is **regular expressions**—compact patterns that match text structure without understanding meaning.

### Why Regex Works for Evaluation

Regex excels at checking **surface form** without semantic understanding. You can verify that an email address is formatted correctly without knowing who owns it. You can check that a date follows ISO 8601 format without knowing if it's historically valid. You can find all occurrences of a phone number pattern without a database lookup.

For LLM evaluation, regex is perfect for:

- **Detecting forbidden patterns:** PII, profanity, specific phrases
- **Validating formatting:** URLs, dates, IDs, codes
- **Counting structural elements:** How many times does the model cite a source? How many bullet points are in the list?
- **Extracting content for downstream rules:** Pull out all numbers, then check if they're in range; extract all links, then validate them

### Common Regex Patterns in Production

**Email addresses:** Match standard email format to detect potential leaks.

**Phone numbers:** Various formats, country codes, extensions.

**Credit cards and SSNs:** Digit patterns with optional separators.

**URLs and API endpoints:** Protocol, domain, path structure.

**Code patterns:** Function calls, variable names, SQL injection attempts.

**Citation formats:** Academic citations, markdown links, footnote markers.

### Regex Pitfalls in LLM Outputs

Models don't always generate perfectly formatted text, so strict regex can have high false positive rates.

**Inconsistent whitespace.** The model might add extra spaces, tabs, or newlines around elements. Your regex needs to be forgiving: match one or more whitespace characters instead of assuming a single space.

**Partial or malformed patterns.** A phone number might be missing digits or have unconventional separators. Decide whether partial matches should fail the rule or just trigger a warning.

**Context matters for false positives.** A string of digits might look like a credit card number but actually be a product ID or random example. Regex can't distinguish context—you might need to combine it with other signals (e.g., only flag if it appears in a field labeled "payment information").

**Escaping and special characters.** LLM outputs can contain regex metacharacters like parentheses, brackets, asterisks. Make sure your pattern matching library escapes properly or uses literal string matching where appropriate.

**Performance on large outputs.** Complex regex with backtracking can be slow on long text. If you're scanning 10,000-word documents, test performance and consider simpler patterns or pre-filtering.

The best practice: **start with strict patterns, then relax them as you see false positives in real data**. It's easier to loosen a rule than to tighten one that's already deployed.

---

## Keyword and Blocklist Checks

The simplest rule-based approach: scan for presence or absence of specific terms.

### Building and Maintaining Blocklists

A **blocklist** is a list of words, phrases, or patterns that should never appear in model outputs. Common categories:

**Profanity and slurs.** Language that violates content policy. Usually starts with a standard list (there are open-source profanity lists) and grows as edge cases are discovered.

**Competitor names.** If your company policy is to never mention competitors by name, you blocklist them. Sometimes brands, sometimes products.

**Legally restricted terms.** Medical claims like "cure" or "treat" if you're not allowed to make those statements. Financial terms like "guaranteed returns." Anything that could create legal liability.

**Internal jargon or codenames.** Terms that should stay internal—project names, unreleased features, employee nicknames. If your model was trained on internal docs, it might leak these.

**PII patterns that are hard to regex.** Common names, place names, company identifiers. "John Smith's account" might not match a PII regex but could still be a leak if it's too specific.

### The Allow-List Alternative

The inverse approach: instead of blocking bad words, you **only allow known-good words**. This is practical for constrained domains.

If your chatbot should only discuss five product categories, you can validate that the response only uses vocabulary from those domains. If the model starts talking about unrelated topics, it fails the rule.

Allow-lists are brittle—they break when legitimate use cases need new vocabulary—but they're powerful for high-stakes, narrow domains like medication dispensing or financial transactions.

### Fuzzy Matching and Variations

Simple substring matching misses variations: "f***", "fսck" (with Unicode substitution), "F U C K" (spaced out), "fook" (misspelling). Attackers and jailbreakers constantly evolve their techniques.

Modern blocklist checkers use:

- **Levenshtein distance:** Allow up to 2 character edits from a blocked term.
- **Phonetic matching:** Block words that sound like banned terms.
- **Unicode normalization:** Detect lookalike characters from other alphabets.
- **Character-level pattern matching:** Match "s3cur1ty" variations of "security."

The tradeoff is false positives: "Scunthorpe" contains a profanity substring, "bass" is close to a slur. You need manual review of flagged terms to tune sensitivity.

### Contextual Keyword Checks

Not all keyword detection is about blocking. Some rules check for **required presence** of terms:

- Customer support responses should mention "thank you" or "appreciate."
- Medical advice should include "consult a doctor" or "seek professional care."
- Legal documents should contain "terms and conditions" or "privacy policy."

These checks ensure compliance with policies that require specific language. The challenge is that models can paraphrase, so exact keyword matching misses semantically equivalent phrases. This is where keyword rules start reaching their limits and you need semantic similarity checks (covered in Chapter 7.2).

---

## Threshold Checks and Quantitative Rules

Many failures are detectable through simple measurements of output properties.

### Length Constraints

**Character count limits.** Mobile UIs truncate at 140 chars. Database fields have max lengths. Rules enforce these before outputs hit downstream systems.

**Word count ranges.** If you prompt for a "short answer" and get 2,000 words, something's wrong. If you ask for a summary and get 5 words, it's incomplete. Rules flag outliers.

**Token count budgets.** Each LLM call has a token cost. If your average response is 200 tokens and one uses 5,000, it's either a failure or a cost anomaly. Rules alert you to investigate.

**Sentence count.** Some outputs should be a single sentence (headlines, product names). Others should be multiple paragraphs. Rules verify the expected structure.

### Latency and Performance Thresholds

**Generation time limits.** If a request takes more than 10 seconds, it's a user experience failure even if the output is perfect. Rules track and flag slow responses.

**Time-to-first-token.** For streaming interfaces, you want to start showing output quickly. If TTFT exceeds 1 second, it feels laggy. Rules measure this.

**Per-request cost ceilings.** If your economics assume $0.01 per request and one costs $1.00, you need to know. Rules flag cost outliers for investigation.

### Repetition and Redundancy Detection

Models sometimes get stuck repeating themselves—especially in long-context or multi-turn scenarios.

**N-gram repetition.** Count how many times the same 3-word or 5-word sequence appears. If it's more than twice, flag it.

**Sentence-level duplication.** If the exact same sentence appears multiple times, it's likely a generation artifact.

**Paragraph structure repetition.** Some models fall into templates: "In conclusion… In conclusion… In conclusion…". Rules detect these patterns.

### Statistical Outlier Detection

For systems generating many outputs, you can set rules based on **statistical norms**:

- Flag responses more than 3 standard deviations from mean length.
- Alert when 95th percentile latency exceeds SLA by 20%.
- Detect when cost per request spikes above historical average.

These aren't fixed thresholds—they adapt to your traffic patterns. Useful for catching performance regressions or model behavior changes after updates.

---

## Deterministic Assertion Testing

The most straightforward rule form: **assert that condition X is true**. If it's not, the test fails.

### Positive Assertions

Checks that verify required content or structure is present.

- **Output must contain substring:** "The response must include the phrase 'cited from'."
- **Output must match format:** "The response must be valid JSON with a 'result' key."
- **Output must meet threshold:** "The response must be at least 100 characters long."

These are pass/fail checks with no ambiguity. Either the assertion holds or it doesn't.

### Negative Assertions

Checks that verify prohibited content is absent.

- **Output must not contain substring:** "The response must not include the phrase 'I don't know'."
- **Output must not exceed limit:** "The response must not be longer than 500 words."
- **Output must not include patterns:** "The response must not contain email addresses."

### Combining Assertions Into Rule Sets

In practice, you don't run one rule at a time. You build **rule sets**—collections of assertions that all must pass for an output to be considered valid.

For example, a "customer support response" rule set might include:

- Must be 50-500 words
- Must contain at least one of: "thank you", "appreciate", "glad to help"
- Must not contain PII patterns
- Must not contain profanity
- Must be valid markdown with no broken links
- Must not mention competitors by name

Each rule is independent. If any single rule fails, the entire output fails the rule set. You log which specific rules triggered so you can debug.

### Rule Set Versioning

As your system evolves, rules change: you add new constraints, remove overly strict ones, adjust thresholds. To avoid breaking existing evaluations, you version your rule sets.

**Rule set v1.0** might have 10 rules. **Rule set v1.1** adds 2 new PII patterns. **Rule set v2.0** loosens length constraints. When you evaluate historical data, you specify which rule set version to use so results are comparable over time.

This is critical for regression testing (Chapter 12): you want to know if a model change causes failures under the same rules, not whether rules changed between test runs.

---

## When Rules Break Down

Rules are powerful, but they have hard limits. Understanding where they fail is essential for knowing when to escalate to other evaluation methods.

### Rules Can't Judge Semantic Quality

A response can pass all structural rules and still be terrible.

- **Factual correctness.** Rules can't verify that "Paris is the capital of France" is true. They can check that a response cites a source, but not whether the source supports the claim.
- **Relevance to the question.** Rules can't tell if an answer is on-topic. A response might be grammatically perfect, properly formatted, and completely irrelevant to what the user asked.
- **Helpfulness and user satisfaction.** Rules can't measure whether a response actually solves the user's problem. A chatbot could generate a compliant response that frustrates the user.
- **Tone and style appropriateness.** Rules might enforce word count and keyword presence, but they can't judge if the tone is condescending, the style is too formal, or the voice is off-brand.

### Rules Generate False Positives and Negatives

**False positives:** Rules flag outputs that are actually fine.

- A blocklist triggers on "Scunthorpe" or "bass" because they contain substrings of blocked words.
- A length rule rejects a concise, perfect answer because it's 45 words instead of the required 50.
- A PII regex matches a product serial number that looks like a credit card.

False positives create alert fatigue. Engineers stop trusting rules and disable them.

**False negatives:** Rules miss outputs that are actually problematic.

- A model leaks PII in an unusual format the regex doesn't catch.
- A response is subtly biased or harmful but uses acceptable keywords.
- A factual error slips through because the rule only checks structure, not correctness.

False negatives erode trust in your safety guarantees. Users encounter bad outputs that supposedly passed all checks.

### The Brittleness Problem

Rules are **brittle**—they work perfectly for the exact cases they were written for, but break when inputs vary slightly.

If you write a rule that expects citations in the format "[1]", it fails when the model switches to "(Smith 2024)" format. If you require bullet points with asterisks, it fails when the model uses hyphens.

Models are creative. They generate outputs in formats you didn't anticipate. Rules require constant maintenance to keep up with model behavior drift.

### The Coverage-Precision Tradeoff

To reduce false negatives, you make rules more permissive. To reduce false positives, you make them stricter. You can't optimize both simultaneously.

A loose PII regex catches more leaks but flags more innocuous strings. A tight regex misses edge cases but has fewer false alarms. The right balance depends on the cost of each error type in your domain.

For safety-critical rules (PII, profanity, legal compliance), you bias toward false positives—better to block a good response than allow a bad one. For quality rules (length, formatting), you bias toward false negatives—better to let through a minor imperfection than block too much.

---

## The Rule Layer as a Foundation

Despite their limitations, rules are the **foundation layer** of every production eval pipeline. Here's how they fit into the broader system.

### Multi-Stage Evaluation Architecture

Modern LLM systems run evaluation in stages, from cheapest to most expensive:

1. **Rules run first** (microseconds, free): Filter out structural failures, PII leaks, formatting errors, obvious safety violations.
2. **Model-based judges run second** (seconds, low cost): Evaluate quality, relevance, tone for responses that passed rules.
3. **Human review runs last** (minutes to hours, high cost): Spot-check high-risk outputs, resolve edge cases, label data for future rules.

This cascade is efficient: 70% of failures get caught by rules, 25% by model judges, 5% need human eyes. You only pay for expensive evaluation on the outputs that made it through cheaper filters.

### Rules as Real-Time Guardrails

In production inference, rules run **synchronously**—they evaluate the response before it's shown to the user. If a rule fails, you can:

- **Block the output entirely** and show a generic fallback ("I'm unable to answer that question").
- **Regenerate with a modified prompt** (e.g., if a response is too long, retry with "Please answer in under 200 words").
- **Redact or filter content** (e.g., if a PII pattern is detected, replace it with "REDACTED" before showing).
- **Log the failure and allow the response** (if the rule is a warning, not a blocker).

This real-time enforcement is only practical with rules. Model-based judges are too slow and expensive to run in the hot path. Human review is obviously too slow. Rules give you instant safety guarantees.

### Rules as Regression Test Suites

When you update prompts, change models, or deploy new features, you run your rule sets on a regression test dataset (Chapter 12). If pass rates drop, you know the change broke something.

Rules are perfect for regression testing because they're deterministic. Unlike human reviewers or even model judges (which can be inconsistent), rules produce the same pass/fail result every time. This makes it easy to detect regressions automatically in CI/CD pipelines.

### Rules as Training Data Filters

Before you send outputs to human annotators or model judges, rules filter out garbage. If a response is malformed JSON, there's no point asking a human to rate its quality. If it's 5,000 words when it should be 200, it's obviously wrong.

Rules save annotation budget by only escalating outputs that plausibly passed structural checks. This is especially important when annotation is expensive (domain experts) or slow (hourly contractors).

---

## Building a Rule Library

As your system matures, you accumulate hundreds of rules. Organization and governance become critical.

### Categorizing Rules by Purpose

**Safety rules:** PII detection, profanity filters, unsafe instruction blocking. These are usually blockers—if they fail, the output never ships.

**Compliance rules:** Legal disclaimers, required language, industry regulations. Often blockers for regulated industries, warnings otherwise.

**Quality rules:** Length, formatting, citation presence. Usually warnings or soft gates—they flag issues but don't always block outputs.

**Performance rules:** Latency, cost, token usage. Monitoring rules that alert engineers but don't affect user-facing behavior.

**Brand and style rules:** Keyword presence, tone markers, competitor mentions. Often configurable per use case.

### Rule Ownership and Versioning

Each rule should have:

- **A unique ID and version number** (e.g., `pii_ssn_detection_v2.1`)
- **An owner** (team or individual responsible for maintaining it)
- **A description and rationale** (why this rule exists, what it catches)
- **Test cases** (example inputs that should pass or fail)
- **Change history** (when it was added, modified, or deprecated)

This metadata makes rules maintainable at scale. When a rule generates false positives, you know who to talk to. When you need to understand why something was blocked, you have documentation.

### Enabling and Disabling Rules by Context

Not all rules apply to all use cases. A rule that blocks competitor mentions might apply to marketing content but not to internal analysis reports. A length limit might apply to customer-facing chatbots but not to internal summaries.

Your rule engine should support **context-based rule sets**: specify which rules to run based on task type, user role, or output destination. This prevents rule proliferation (where you create slightly different versions of the same rule for each use case) and keeps the library manageable.

### Rule Configuration as Code

Rules should be defined in version-controlled config files, not hard-coded in application logic. This allows:

- **Non-engineers to update rules** (e.g., a legal team adding a new compliance phrase)
- **A/B testing of rule sets** (deploy stricter rules to 10% of traffic, measure impact)
- **Rollback if rules break** (revert a config change that caused false positives)
- **Audit trails** (every rule change is logged in version control)

In 2026, common formats are YAML or JSON schemas. Here's an example structure:

```yaml
rule_id: response_length_check
version: 1.2
category: quality
enabled: true
severity: warning
description: Ensure responses are between 100-500 words
parameters:
  min_words: 100
  max_words: 500
test_cases:
  - input: "Short answer."
    expected: fail
  - input: "This is a response with exactly 250 words..."
    expected: pass
```

---

## False Positive Management

The biggest operational challenge with rules: they trigger too often on legitimate outputs, causing alert fatigue and eroding trust.

### Measuring Rule Precision

For each rule, track:

- **True positives:** Rule correctly flagged a bad output
- **False positives:** Rule flagged a good output
- **Precision:** TP / (TP + FP)

If precision drops below 80%, the rule is too noisy. Engineers start ignoring alerts or disabling the rule.

### Tuning Thresholds

Many rules have tunable parameters: minimum word count, maximum latency, fuzzy matching distance. When false positives spike, adjust thresholds:

- **Relax length constraints** if too many good responses are slightly outside bounds.
- **Tighten PII regex** if it's matching non-sensitive patterns.
- **Increase confidence thresholds** for fuzzy blocklist matching.

This is an iterative process: deploy a rule, monitor precision, tune, redeploy.

### Adding Allow-Lists for Known False Positives

If a rule consistently flags specific phrases that are actually fine, add them to an **allow-list**—exceptions that override the rule.

Example: A profanity filter blocks "Scunthorpe." Add "Scunthorpe" to an allow-list so the rule skips it. A PII regex matches employee ID format "E123456." Add that pattern to an exception list if it's not actually sensitive.

Allow-lists prevent rule sprawl (where you create increasingly complex logic to avoid false positives). They're easier to maintain than rewriting the core rule.

### Sampling and Spot-Checking

For low-severity rules (warnings, not blockers), use **sampling**: only evaluate a random 10% of outputs instead of 100%. This reduces noise while still catching trends.

For high-severity rules (safety, compliance), run on 100% of traffic but **spot-check false positives** weekly. Review a sample of blocked outputs to verify they were actually bad. If too many are good, tune the rule.

### Rule Deprecation

When a rule generates too many false positives and can't be fixed, **deprecate it**: disable it in production but keep it in the library for historical analysis.

Document why it was deprecated so future engineers don't recreate the same problem. Sometimes old rules become relevant again when model behavior changes, so you want them available.

---

## 2026 Patterns in Rule-Based Evaluation

The last few years have seen rule-based checks become deeply integrated into the LLM development lifecycle.

### Rules in CI/CD Pipelines

Every prompt change, model update, or config tweak triggers automated rule-based testing before deployment (Chapter 12.4).

Your CI pipeline runs:

1. **Unit tests with rule assertions:** Verify that fixed test cases pass all rules.
2. **Regression tests on evaluation datasets:** Ensure rule pass rates don't drop below baseline.
3. **Rule coverage analysis:** Confirm that all critical rules are exercised by test cases.

If any rule fails, the build doesn't deploy. This prevents regressions from reaching production.

### Guardrail Frameworks

Open-source libraries have emerged to standardize rule-based guardrails:

**Guardrails AI** (guardrailsai.com): A Python framework for defining validators (rules) in code. Supports regex, length checks, PII detection, custom validators. Integrates with LangChain, runs synchronously at inference.

**NVIDIA NeMo Guardrails:** A toolkit for conversational AI safety. Define input and output rails (rules) in a config language. Supports semantic similarity checks (next chapter) in addition to pattern matching.

**AWS Bedrock Guardrails:** Cloud-managed rule enforcement for LLM APIs. Configure content filters, PII detection, topic blocking, word filters. Runs at the API layer, no custom code needed.

These frameworks make it easier to deploy and manage rules at scale. You define rules once and apply them across multiple models or endpoints.

### Real-Time Rule Enforcement at Inference

In 2026, most production LLM platforms run rules **before showing outputs to users**, not just in post-hoc evaluation.

Inference flow:

1. User sends a request
2. LLM generates a response
3. **Rule engine evaluates** (synchronously, milliseconds)
4. If rules pass: show response to user
5. If rules fail: block, regenerate, or fallback

This "guardrail as middleware" pattern is now standard in enterprise deployments. It's the only way to guarantee that outputs meet compliance and safety standards before they leave your infrastructure.

### Rule Observability and Dashboards

Modern rule systems expose metrics in real time:

- **Rule trigger rates:** How often each rule fires (overall and per task type)
- **False positive estimates:** Based on manual review samples
- **Rule latency:** How long each rule takes to evaluate
- **Rule coverage:** Percentage of outputs checked by each rule

Dashboards show trends: Is a rule suddenly triggering 10x more often? Did a model update change output structure, breaking rules? These signals help you maintain rule health over time.

### Integrating Rules With Model-Based Judges

Rules and model judges are complementary. In 2026, hybrid pipelines are common:

- **Rules run first** to filter structural failures.
- **Model judges run on passing outputs** to evaluate quality.
- **If a model judge contradicts a rule**, flag for human review. Maybe the rule is wrong, or the judge hallucinated.

Some systems even use model outputs to **generate new rules**: if a judge consistently flags a pattern, you codify it as a rule for faster future detection.

---

## Failure Modes and Enterprise Expectations

Even in a mature rule-based system, things go wrong. Here's what to watch for.

### Rules That Never Trigger

Dead rules—ones that haven't fired in months—clutter your library and slow down evaluation. Audit periodically and remove rules that:

- Were written for edge cases that never happened in production
- Duplicate other rules
- Are made obsolete by model improvements (e.g., early GPT-3 models needed length rules that GPT-4 never violates)

### Rules That Are Too Strict

When pass rates drop below 50%, rules are over-tuned. Models can't generate anything that satisfies them. This often happens with:

- **Overly specific formatting requirements** (e.g., expecting exact markdown structure the model rarely produces)
- **Conflicting rules** (e.g., "must be under 200 words" and "must include examples, citations, and disclaimers")
- **Keyword requirements** that are too rigid (e.g., must contain "thank you" when paraphrases like "we appreciate" are equally valid)

The fix: relax constraints, use OR logic instead of AND, or replace keyword rules with semantic similarity checks (Chapter 7.2).

### Rules That Drift With Model Behavior

Models change: you update to a new version, fine-tune on new data, or switch providers. Suddenly, rules that worked perfectly start triggering false positives because the model generates text in a slightly different style.

This is especially common with:

- **Formatting conventions** (bullet point syntax, citation styles)
- **Length distributions** (new model is more concise or verbose)
- **Keyword usage** (new model paraphrases where old model used exact phrases)

Solution: **Re-validate rules after every model change**. Run rule sets on a holdout dataset with the new model, measure precision, and adjust before deploying.

### Rules as Gaming Targets

When developers know their outputs will be evaluated by rules, they optimize prompts to pass rules rather than produce quality responses.

Example: If there's a rule that responses must be longer than 100 words, prompts might instruct "Pad your response to at least 100 words" even when concise answers are better.

This is **Goodhart's Law** applied to LLM evaluation: when a measure becomes a target, it ceases to be a good measure. Rules that developers can easily game lose their value as quality signals.

Mitigation: Don't share all rule details with prompt engineers. Use rules as safety nets, not optimization targets. Combine rules with human or model-based evaluation so gaming one doesn't game the whole system.

### Enterprise Compliance Expectations

In regulated industries, rule-based checks aren't optional—they're **legally required**. Enterprises expect:

- **Audit trails:** Every rule evaluation logged with input, output, rule version, result, timestamp. Stored for 7+ years.
- **Explainability:** If an output was blocked, a human-readable explanation of which rule failed and why.
- **Governance:** Rules approved by legal/compliance teams, version-controlled, with change approval workflows.
- **Certifiability:** Evidence that rules run on 100% of production traffic, not just samples.

Building this level of rigor requires treating rules as first-class infrastructure, not ad-hoc scripts.

---

## Template: Rule-Based Evaluation Config

Here's a lightweight structure for defining a rule set as configuration:

```yaml
rule_set:
  name: customer_support_response_rules
  version: 2.1
  description: Validation rules for customer support chatbot outputs
  owner: support-eng-team
  rules:
    - id: response_length
      type: length_check
      severity: error
      params:
        min_words: 50
        max_words: 500
      message: "Response must be 50-500 words"

    - id: required_politeness
      type: keyword_check
      severity: warning
      params:
        must_contain_one_of:
          - "thank you"
          - "appreciate"
          - "glad to help"
      message: "Response should include polite language"

    - id: no_pii
      type: regex_block
      severity: error
      params:
        patterns:
          - '\b\d{3}-\d{2}-\d{4}\b'  # SSN
          - '\b\d{16}\b'              # Credit card
          - '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'  # Email
      message: "Response contains potential PII"

    - id: no_competitor_mentions
      type: blocklist_check
      severity: warning
      params:
        blocklist:
          - CompetitorA
          - CompetitorB
          - CompetitorC
      message: "Response mentions a competitor"

    - id: valid_markdown
      type: format_validation
      severity: warning
      params:
        format: markdown
        check_links: true
      message: "Response contains invalid markdown"
```

This config defines rules as data, making them easy to version, test, and update without code changes.

---
