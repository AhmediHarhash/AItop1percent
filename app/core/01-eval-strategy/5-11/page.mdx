# Chapter 5.11 — Dataset Evolution & Maintenance

**What we're doing here:**
Six months ago, your eval dataset was perfect. Balanced difficulty, clean labels, great coverage. Today? Your product added three new features that aren't tested. Your user base shifted — you're getting queries you never saw before. Half your "hard" cases are now "easy" because the model improved. And nobody noticed because nobody owns dataset maintenance.

Eval datasets are living things. Build them once and forget them, and within two quarters they're testing yesterday's product against yesterday's problems. This chapter covers how to keep your datasets evolving with your product.

---

## 1) Why datasets decay

**Product drift.** Your product ships new features, retires old ones, changes workflows. Your eval set still tests the old version. The coverage map has blind spots that grow with every release.

**User drift.** Your user base changes — new markets, new personas, new use patterns. The queries in your eval set reflect who your users were six months ago, not who they are today.

**Model drift.** You upgrade models. What was "hard" becomes "easy." Your difficulty distribution skews. Your adversarial cases no longer challenge the model. Your eval scores go up but you're not learning anything new.

**Rubric drift.** Quality standards evolve. "Good enough" last quarter isn't good enough now. Your rubric hasn't been updated, so you're scoring against outdated expectations.

**World drift.** Facts change. Regulations update. Products and pricing change. Any eval case testing factual knowledge becomes stale as the underlying reality shifts.

If you're not actively maintaining your dataset, all five of these forces are working against you simultaneously.

---

## 2) The maintenance cadence

### Weekly: production signal integration
Review production logs for new failure patterns. When you see a new type of failure, add it to your dataset backlog. Don't wait for quarterly reviews to catch emerging issues.

Track new query types that don't fit existing taxonomy categories. If your classification says "other" more than 10% of the time, your taxonomy has gaps.

### Monthly: coverage and difficulty audit
Run your coverage map (Ch 3.2) against current product features. Flag any feature or intent that has fewer than 5 eval cases. Check difficulty distribution per slice — if any difficulty bucket has shifted more than 15% from target, rebalance.

Review production metrics against eval predictions. If eval says quality is 90% but production satisfaction is 70%, your dataset isn't testing what matters.

### Quarterly: deep refresh
This is the big one. Every quarter:

**Retire stale cases.** Cases testing deprecated features, outdated facts, or removed workflows. Don't delete them — archive with a "retired" tag and reason. You might need them if you re-introduce a feature.

**Recalibrate difficulty.** Re-run difficulty heuristics (Ch 5.3) on the full dataset. Cases the model now handles easily get downgraded. New hard cases get added to replace them.

**Rotate holdouts.** Swap which cases are in the hidden holdout vs visible set (Ch 5.6). This prevents eval overfitting.

**Refresh adversarial cases.** Adversarial techniques evolve. Prompt injection patterns that worked last quarter might not be relevant anymore — new ones have emerged. Update your adversarial suite.

**Add new coverage.** Fill gaps identified in monthly audits. Add cases for new features, new user personas, new languages, new edge cases discovered in production.

### Annually: full rebuild assessment
Ask: is this dataset still fundamentally sound, or does it need a ground-up rebuild? If more than 40% of cases have been retired or replaced in the past year, you might be better off rebuilding with fresh production data as the foundation.

---

## 3) Who owns maintenance

Dataset maintenance fails when nobody owns it. It's not glamorous work, so it gets deprioritized unless there's clear ownership.

**The eval team owns the process** — setting cadence, running audits, enforcing quality standards. **Product teams own coverage requests** — they know what features are shipping and what needs testing. **Domain experts own content accuracy** — they catch factual staleness and rubric misalignment.

If you don't have a dedicated eval team yet, assign a rotating "dataset steward" who runs the monthly audit. Make it a real responsibility with time allocated, not a side task.

---

## 4) Version management for evolving datasets

Every change to your dataset creates a new version. This isn't optional — it's how you trace which dataset produced which eval result.

**Semantic versioning for datasets:**
- **Major version** (v2.0): Schema changes, rubric overhaul, full rebuild. Scores from v1 and v2 are not directly comparable.
- **Minor version** (v1.3): New cases added, coverage expanded, difficulty rebalanced. Scores are comparable with caveats.
- **Patch version** (v1.3.2): Bug fixes — wrong labels corrected, broken cases removed. Scores should be comparable.

**Always log what changed and why.** A changelog entry like "Added 50 cases for new billing feature, retired 20 cases for removed plan types, recalibrated 30 difficulty labels" tells future-you exactly what happened.

**Never modify a released version in place.** If you find errors in v1.3, fix them in v1.3.1. Don't silently patch v1.3 — someone might be comparing results against it.

---

## 5) Knobs & defaults

**Production signal review:** Weekly, 30-minute scan of new failure patterns.

**Coverage audit frequency:** Monthly. Flag features with fewer than 5 eval cases.

**Difficulty recalibration:** Quarterly. Downgrade cases the model handles above 95% pass rate.

**Holdout rotation:** Quarterly. Swap 30-50% of holdout cases.

**Adversarial refresh:** Quarterly. Replace at least 20% of adversarial cases with current attack patterns.

**Stale case retirement:** Quarterly. Archive cases testing deprecated features or outdated facts.

**Full rebuild threshold:** If more than 40% of cases have been modified in a rolling 12-month window, consider rebuilding.

---

## 6) Failure modes

**"Our eval scores keep going up but users aren't happier."**
Dataset hasn't evolved with the product or users. You're testing old patterns that the model has already mastered. Fix: run production-eval alignment check. Add cases from recent production failures. Refresh difficulty labels.

**"Nobody maintains the dataset — it's stale."**
No ownership. Fix: assign a dataset steward. Make maintenance a scheduled task with clear deliverables, not a best-effort side project.

**"We added cases but broke comparability with last quarter."**
Major changes without version management. Fix: use semantic versioning. When adding cases changes the difficulty distribution significantly, bump the minor version and document the impact on score comparability.

**"We retired too many cases and lost coverage."**
Aggressive cleanup without checking the coverage map. Fix: before retiring cases, run coverage analysis. Only retire if replacement cases exist or the feature being tested is truly gone.

**"Our adversarial suite hasn't changed in a year."**
Nobody refreshes adversarial cases. The model has effectively memorized them. Fix: quarterly adversarial refresh is non-negotiable. Draw from current prompt injection research, red-team exercises, and production abuse patterns.

---

## 7) Enterprise expectations

- They have a named owner (person or team) responsible for dataset maintenance
- They run monthly coverage audits against current product features
- They recalibrate difficulty and rotate holdouts quarterly
- They version every dataset change with semantic versioning and changelogs
- They track production-eval alignment — if eval scores and production quality diverge, they investigate
- They refresh adversarial suites quarterly with current attack patterns
- They never silently modify a released dataset version

---

## 8) Interview Q&A

**Q: How do you keep eval datasets from going stale?**
A: Structured cadence. Weekly production signal review to catch new failure patterns. Monthly coverage audits against current features. Quarterly deep refresh — retire stale cases, recalibrate difficulty, rotate holdouts, refresh adversarial suite. Every change is versioned with a changelog.

**Q: Who owns dataset maintenance on your team?**
A: The eval team owns the process — cadence, audits, quality standards. Product teams own coverage requests because they know what's shipping. Domain experts own content accuracy. If there's no eval team, I assign a rotating dataset steward with real time allocation.

**Q: How do you handle dataset versioning?**
A: Semantic versioning. Major versions for schema or rubric changes — scores across major versions aren't comparable. Minor versions for new cases or rebalancing. Patch versions for bug fixes. I never modify a released version in place — fixes go in a new patch version so anyone comparing against the old version isn't silently affected.

---

*This completes the Dataset Design & Management section. Next: Chapter 6 covers Human Evaluation — how to design, run, and scale human eval programs that produce reliable quality signals.*
