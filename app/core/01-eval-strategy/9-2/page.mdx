# Retrieval Quality Metrics

A friend once told me about their experience at a research library. They needed a specific 1970s engineering paper for a project. The librarian, an expert with decades of experience, listened carefully to the request and disappeared into the stacks. Twenty minutes later, she returned with a stack of twelve journals. "The article you need is in one of these," she said.

My friend spent three hours going through all twelve volumes. The article wasn't there.

The librarian had failed at the most fundamental task: retrieval. It didn't matter that she was knowledgeable, experienced, and well-intentioned. She didn't find the right document. Everything downstream—reading, synthesizing, writing—became impossible because the foundation was missing.

**This is RAG in a nutshell.** Your language model can be brilliant. Your prompts can be perfectly crafted. Your generation can be flawless. But if your retrieval system doesn't find the right documents, everything else fails. Garbage in, garbage out. You cannot generate good answers from bad retrieval.

This chapter is about how to measure whether your retrieval system actually works. Not whether it feels good, not whether it returns something plausible, but whether it consistently finds the right information when users ask questions.

---

## Why Retrieval Quality Is the Foundation

In traditional software, we talk about the "garbage in, garbage out" principle. Bad inputs produce bad outputs, no matter how good your processing logic is.

In RAG systems, retrieval quality is your input quality gate. **The retrieval stage determines the ceiling for your entire system.** If the right documents aren't in the top results, your language model has no chance of generating a correct answer. It doesn't matter if you're using Opus 4.5 with a perfect prompt—if the context is wrong, the answer will be wrong.

I've seen teams spend months optimizing their generation prompts, tuning temperature settings, and experimenting with different models, all while ignoring the fact that their retrieval system only surfaces relevant documents forty percent of the time. They're polishing the wrong part of the pipeline.

**Retrieval is also where most RAG failures happen in production.** When users complain that "the AI gave me the wrong answer," the root cause is usually not generation—it's that the retrieval system surfaced outdated documentation, similar-but-wrong examples, or completely irrelevant context.

This is why retrieval metrics deserve their own dedicated chapter. Before you evaluate generation quality, faithfulness, or hallucination rates, you need to know: did we even find the right documents?

---

## Precision at K — What Fraction of Results Are Actually Useful?

Let's start with the most intuitive metric: **precision at K**.

Imagine you search your company's documentation for "how to reset a customer password." Your retrieval system returns ten documents. You look through them:

- Document 1: Password reset procedure (relevant)
- Document 2: Password policy requirements (relevant)
- Document 3: Customer account creation guide (not relevant)
- Document 4: Password reset troubleshooting (relevant)
- Document 5: Security best practices (marginally relevant)
- Document 6: Database backup procedures (not relevant)
- Document 7: API authentication guide (not relevant)
- Document 8: Previous version of password reset doc (relevant but outdated)
- Document 9: Customer support escalation process (not relevant)
- Document 10: Password encryption technical spec (marginally relevant)

**Precision at 10** asks: of these ten results, how many are actually relevant?

If you count documents 1, 2, 4, and 5 as clearly relevant, that's 4 out of 10. Your precision at 10 is 0.4 or forty percent.

**Higher precision means less noise.** When precision is high, users don't have to wade through irrelevant results. The system gives them mostly good stuff. When precision is low, users waste time reading documents that don't help.

Precision at K is typically measured at multiple cutoff points. **Precision at 1** is the most critical—is the very first result relevant? If yes, many users stop there. **Precision at 5** matters for conversational interfaces that might show top results. **Precision at 10** or **precision at 20** matters for traditional search interfaces where users scan multiple results.

The challenge with precision: it doesn't tell you if you missed anything important. You could have perfect precision by returning only one highly relevant document—but if there were twenty other relevant documents you didn't surface, your users get an incomplete picture.

This is where recall comes in.

---

## Recall at K — Did We Miss Anything Important?

**Recall at K** asks a different question: of all the relevant documents that exist in your corpus, what fraction did we find in the top K results?

Going back to the password reset example: imagine your documentation actually contains eight relevant documents about password resets scattered across different sections. Your retrieval system returned ten documents, but only four of them were relevant (the ones we counted for precision).

That means you found 4 out of 8 possible relevant documents. Your **recall at 10 is 0.5** or fifty percent.

**You missed half the relevant information.**

This is the "did we miss anything important?" metric. High recall means you're capturing most of the relevant information that exists. Low recall means you're leaving critical documents on the table.

In practice, there's always a **precision-recall tradeoff**. If you return only the single best document, you might have perfect precision but terrible recall. If you return a thousand documents to make sure you capture everything, you might have perfect recall but terrible precision—users are drowning in noise.

Real-world example: a legal research team I worked with had a retrieval system with ninety percent precision at 20 but only thirty percent recall at 20. Lawyers loved that the results were clean and relevant, but they kept missing important case precedents because the system would only surface a narrow slice of relevant documents. In legal work, missing a key precedent can be catastrophic.

We tuned the system to return fifty documents instead of twenty, accepting lower precision (seventy percent) in exchange for much higher recall (eighty percent). Lawyers preferred this—they'd rather skim more results than risk missing something critical.

**The right precision-recall balance depends on your use case.** Customer support chatbots need high precision—users want one good answer fast. Research tools need high recall—users want comprehensive results and are willing to filter.

---

## Mean Reciprocal Rank — How Far Down Do Users Have to Scroll?

Here's a truth about human behavior: **users don't scroll.**

If the answer they need is in position one, great. Position two or three, okay. Position ten? Most users have already given up or tried a different query.

**Mean Reciprocal Rank (MRR)** measures how high up in the results the first relevant document appears. It's a user-experience metric disguised as a technical metric.

Here's how it works. For each query, you identify the **rank of the first relevant document**:

- If the first relevant document is in position 1, the reciprocal rank is 1/1 = 1.0
- If the first relevant document is in position 2, the reciprocal rank is 1/2 = 0.5
- If the first relevant document is in position 5, the reciprocal rank is 1/5 = 0.2
- If no relevant documents appear in the top K, the reciprocal rank is 0

Then you average these reciprocal ranks across all queries. That's your **Mean Reciprocal Rank**.

Example: you evaluate five queries.

- Query 1: first relevant doc at position 1 → reciprocal rank = 1.0
- Query 2: first relevant doc at position 3 → reciprocal rank = 0.33
- Query 3: first relevant doc at position 1 → reciprocal rank = 1.0
- Query 4: first relevant doc at position 7 → reciprocal rank = 0.14
- Query 5: no relevant docs found → reciprocal rank = 0

Average: (1.0 + 0.33 + 1.0 + 0.14 + 0) / 5 = 0.49

Your **MRR is 0.49**. This tells you that on average, users have to scroll past a couple of irrelevant results before finding what they need.

**Higher MRR is better.** An MRR above 0.8 means most queries have a relevant document in the top two positions. An MRR below 0.3 means users are consistently scrolling through garbage before finding anything useful.

MRR is particularly important for **question-answering systems** where you expect one good answer. It's less critical for exploratory search where users want to see multiple perspectives.

One limitation of MRR: it only cares about the first relevant document. If you return one relevant document at position 1 and nine irrelevant documents after it, your MRR is perfect (1.0), but your precision is terrible (0.1). This is why you need multiple metrics.

---

## Normalized Discounted Cumulative Gain — Rewarding Better Ranking

**Normalized Discounted Cumulative Gain (nDCG)** is the most sophisticated retrieval metric we'll cover. It solves several limitations of precision, recall, and MRR:

1. It considers the **position** of relevant documents (like MRR)
2. It considers **all** relevant documents, not just the first one (unlike MRR)
3. It handles **graded relevance**—some documents are more relevant than others

Here's the intuition. Imagine three scenarios where your retrieval system returns five documents:

**Scenario A:** Highly relevant doc at position 1, four irrelevant docs after
**Scenario B:** Moderately relevant doc at position 1, highly relevant doc at position 2, three irrelevant docs after
**Scenario C:** Highly relevant doc at position 3, moderately relevant docs at positions 1 and 2, two irrelevant docs after

Precision and recall treat these scenarios almost identically. But users experience them very differently. Scenario A is best—you get the perfect answer immediately. Scenario C is worst—you have to wade through mediocre results to find the best answer.

**nDCG captures this difference.**

Without diving into the mathematical formula (which involves logarithms and discount factors), here's how it works:

1. Each document gets a **relevance score**. Highly relevant = 3, moderately relevant = 2, marginally relevant = 1, not relevant = 0.
2. Documents higher in the ranking get **weighted more heavily**. The exact weighting decreases logarithmically as you go down the list.
3. You calculate the **ideal ranking**—what if you had perfectly sorted documents by relevance?
4. You compare your actual ranking to the ideal ranking. Perfect match = nDCG of 1.0. Terrible ranking = nDCG close to 0.

In practice, **nDCG at 10 above 0.7 is considered good** for most enterprise retrieval systems. Above 0.8 is excellent. Below 0.5 means your ranking is barely better than random.

The key advantage of nDCG: **it rewards systems that put highly relevant documents at the top.** Two systems might have identical precision at 10, but the one that surfaces the best documents first will have higher nDCG.

When I evaluate retrieval systems for clients, nDCG is usually my primary metric because it aligns with user experience. Users care about getting the best results first, not just getting some relevant results somewhere in the list.

---

## Hit Rate — The Simplest Binary Check

Sometimes you don't need sophisticated metrics. You just need to answer: **did we find at least one relevant document?**

That's **hit rate** (also called **success rate at K**).

For each query, check if at least one relevant document appears in the top K results. If yes, count it as a hit. If no, count it as a miss. Then calculate the percentage of queries that hit.

Example: you evaluate 100 queries at K=5 (top 5 results). For 83 of those queries, at least one of the top 5 documents was relevant. Your **hit rate at 5 is eighty-three percent**.

**Hit rate is the minimum bar for usability.** If your hit rate at 10 is only sixty percent, that means forty percent of the time, users get ten irrelevant results and give up. That's a broken system.

For question-answering chatbots, **I recommend a minimum hit rate at 5 of ninety percent**. Below that, users lose trust. For more exploratory systems, you might tolerate hit rates in the seventy to eighty percent range if you're compensating with good reranking or user feedback mechanisms.

Hit rate is useful as a **high-level health metric**. It's easy to explain to non-technical stakeholders: "Our system finds at least one useful document ninety-two percent of the time." But it doesn't tell you about ranking quality (MRR, nDCG) or completeness (recall).

Use hit rate as your **canary metric**—if it drops suddenly, something is broken. But don't use it as your only metric for optimization.

---

## Context Relevance — Does the Document Actually Help?

Here's a subtle but critical distinction that trips up many teams: **document relevance is not the same as context relevance.**

**Document relevance:** "Is this document about the topic the user asked about?"

**Context relevance:** "Does this document contain the specific information needed to answer the user's question?"

Example query: "What's the maximum file size for document uploads?"

A document titled "Document Upload Guidelines" is clearly **relevant to the topic**. But if that document discusses file types, naming conventions, and security scanning without ever mentioning size limits, it doesn't contain the **information needed to answer the question**.

Your retrieval system might surface it (good topic match), but your generation system can't use it to answer the question. From the user's perspective, this is a retrieval failure.

**Context relevance metrics ask: could a human (or LLM) answer the question using only this retrieved document?**

This is harder to measure than simple relevance. You can't just look at keyword overlap or semantic similarity. You need to actually check if the answer is present in the text.

In 2026, most teams measure context relevance by:

1. **LLM-based evaluation:** Give an LLM the query and the retrieved document. Ask: "Does this document contain enough information to answer the question? Yes/No."
2. **Manual spot-checking:** Periodically sample queries and have humans verify that retrieved documents actually contain answers.
3. **Answer extraction testing:** Try to extract the answer from the retrieved document. If you can't extract it, the document wasn't truly relevant.

I've seen systems with ninety percent document relevance but only sixty percent context relevance. The retrieval system was good at finding topically related content, but bad at finding content that actually contained answers. This shows up as high precision but low downstream answer quality.

**Track both metrics.** If document relevance is high but context relevance is low, your embedding model is matching topics but missing semantic nuance. You might need better chunking, better embeddings, or query expansion to capture the actual information need.

---

## Retrieval Diversity — Avoiding Redundant Results

Imagine searching for "best practices for database indexing" and getting back ten documents that all say the exact same thing—"add indexes to frequently queried columns." Technically all ten documents are relevant. Your precision is perfect. But you've learned nothing beyond what the first document told you.

**Retrieval diversity** measures whether your results provide **varied perspectives, information sources, or types of content**—not just the same information repeated ten times.

Diversity matters for:

- **Research and exploration:** Users want to see different viewpoints, not echo chambers
- **Comprehensive answers:** Complex questions often require synthesizing information from multiple distinct sources
- **Avoiding bias:** If all retrieved documents come from the same author or time period, you're missing important context

Measuring diversity is tricky. Common approaches:

**Semantic diversity:** Measure how different the document embeddings are from each other. If all ten retrieved documents have nearly identical embeddings, they're probably saying the same thing.

**Source diversity:** Count how many unique sources (authors, document types, publication dates) appear in the top K results. Five documents from five different authors is more diverse than five documents from one author.

**Content diversity:** Use LLMs to classify the "type" of information in each document (tutorial, reference, opinion, example, etc.) and check if you're getting a mix.

In practice, **diversity often trades off with relevance**. The most relevant documents for a narrow query might naturally be similar to each other. You don't want to sacrifice relevance just to force diversity.

The right approach: **optimize for relevance first, then add a diversity boost as a secondary signal**. Modern rerankers (which we'll discuss in the 2026 patterns section) can do this—rerank primarily on relevance, but slightly penalize documents that are too similar to higher-ranked results.

I've seen this make a real difference in customer support systems. Early versions would retrieve five articles that all basically said "restart the device"—technically relevant, but frustrating for users who wanted more depth. Adding a diversity signal meant the system would return one "restart" article plus troubleshooting steps, known issues, and escalation procedures.

---

## Retrieval Latency — Speed Matters in Production

You can have perfect precision, perfect recall, perfect nDCG—and still have a completely unusable system if retrieval takes five seconds.

**Retrieval latency** is how long it takes from query submission to getting results back. It's a production metric, not a quality metric, but it directly impacts user experience.

In 2026, enterprise expectations for retrieval latency:

- **Sub-100ms:** Excellent for simple sparse retrieval (BM25, keyword search)
- **100-300ms:** Good for dense retrieval with small corpora (under 100K documents)
- **300-500ms:** Acceptable for hybrid retrieval or medium corpora (100K-1M documents)
- **500ms-1s:** Maximum acceptable latency for synchronous user-facing systems
- **Above 1s:** Only tolerable for batch processing or offline evaluation

If your retrieval takes more than one second, users will perceive the system as slow. In chatbot interfaces, delays above 500ms create noticeable lag between user input and response.

**Latency often trades off with quality.** Dense retrieval with large embedding models is slower but more accurate than sparse retrieval. Reranking improves quality but adds latency. Retrieving more candidates for better recall increases processing time.

The practical approach: **set a latency budget and optimize quality within that constraint.** If your budget is 300ms, you might:

- Allocate 150ms to initial retrieval (fast sparse or cached dense retrieval)
- Allocate 100ms to reranking the top 50 candidates
- Allocate 50ms for network and processing overhead

In production, **track latency percentiles**, not just averages. The average might be 200ms, but if the 95th percentile is 1.5 seconds, some users are having a terrible experience. Use p50, p95, and p99 latency to understand the full distribution.

Also watch for **latency regressions**. I've seen teams add a new feature (like query expansion or hybrid search) that improved quality metrics by five percent but increased p95 latency from 300ms to 900ms. The quality gain wasn't worth the user experience degradation.

---

## Building Retrieval Ground Truth — Labeled Query-Document Pairs

All the metrics we've discussed require one critical input: **ground truth labels** that tell you which documents are actually relevant for each query.

Without labeled data, you can't calculate precision, recall, MRR, or nDCG. You're flying blind.

**Building retrieval ground truth means creating labeled query-document pairs:** a set of queries where you know which documents should be retrieved and how relevant they are.

### Manual Labeling

The gold standard is **manual labeling by domain experts**. Have humans look at queries and documents and label relevance on a scale (not relevant, marginally relevant, relevant, highly relevant).

Process:

1. Collect representative queries from production logs or create synthetic queries covering key use cases
2. For each query, retrieve candidate documents using your current system
3. Have labelers review each document and assign relevance scores
4. Collect labels from multiple labelers and resolve disagreements

**Manual labeling is expensive and slow** but produces high-quality ground truth. A financial services client I worked with spent six weeks having compliance officers label 500 query-document pairs. That dataset became the foundation for evaluating every retrieval improvement over the next two years.

For enterprise systems, I recommend starting with **at least 200-300 labeled queries** covering your main query types. That's enough to get reliable metrics. As you mature, grow to 1,000+ labeled examples.

### LLM-Assisted Labeling

In 2026, **LLM-assisted labeling** is common for bootstrapping ground truth faster.

Process:

1. Collect queries and candidate documents
2. Use an LLM (like Opus 4.5) to label relevance: "Given this query and this document, rate relevance from 0-3"
3. Have humans spot-check a sample (ten to twenty percent) to verify quality
4. Use LLM labels for the bulk of your dataset, human labels for high-stakes or ambiguous cases

**LLM labeling is faster and cheaper than pure manual labeling** but introduces some noise. The LLM might miss subtle relevance or apply inconsistent standards. The spot-checking step is critical—if human-LLM agreement is below eighty percent, your LLM labels aren't reliable enough.

I've seen teams use a **two-stage process**: LLM labels everything, humans review cases where the LLM was uncertain (e.g., predicted relevance score of 1 or 2 out of 3, indicating borderline relevance). This combines speed with human judgment on hard cases.

### Click-Through and Interaction Data

If you have an existing production system, **user interaction data** provides implicit labels.

Signals:

- **Clicks:** If a user clicked on a document, it was probably relevant to their query
- **Dwell time:** If they spent two minutes reading a document, it was likely useful
- **Explicit feedback:** Thumbs up/down, "this helped" buttons
- **Follow-up behavior:** Did they ask a clarifying question (suggesting irrelevant results) or move on to another task (suggesting success)?

**Interaction data is noisy** but abundant. A clicked document isn't always relevant—users might click and immediately bounce. An unclicked document isn't always irrelevant—it might be in position eleven where users didn't scroll.

The best approach: **use interaction data to generate candidate labels, then validate with manual or LLM labeling**. For example, if a document was clicked for a query fifty times and had high dwell time, that's a strong signal of relevance worth validating.

One challenge: **cold start**. If you're building a new system, you don't have interaction data yet. You have to start with manual or LLM labeling, deploy to production, then augment with interaction data over time.

---

## 2026 Retrieval Evaluation Patterns

The retrieval landscape has evolved significantly. In 2026, enterprise teams are dealing with more complex retrieval architectures—**hybrid search**, **reranking**, **multi-hop retrieval**—and the evaluation patterns have evolved to match.

### Hybrid Retrieval Metrics — Dense Plus Sparse

Most production systems in 2026 use **hybrid retrieval**: combining dense retrieval (semantic embeddings) with sparse retrieval (keyword matching like BM25).

Hybrid retrieval gives you the best of both worlds: dense retrieval handles semantic similarity and paraphrasing, sparse retrieval handles exact matches and rare terms.

**Evaluating hybrid systems requires measuring both components separately:**

- **Dense-only metrics:** What's precision/recall if we only use semantic search?
- **Sparse-only metrics:** What's precision/recall if we only use keyword search?
- **Hybrid metrics:** What's precision/recall when we combine both?

You want to see that hybrid outperforms either component alone. If it doesn't, your fusion strategy (how you combine scores from dense and sparse) isn't working.

Also track **contribution metrics**: what percentage of top results came from dense vs sparse retrieval? If ninety-five percent come from one component, you're not really getting benefit from being hybrid.

I worked with a legal tech company that found their sparse retrieval was contributing less than ten percent of top results. After investigating, they realized their BM25 configuration was too conservative. They tuned the sparse weights, and suddenly sparse retrieval was contributing thirty percent of results—and overall nDCG improved by 0.08.

### Reranker Evaluation

**Rerankers** are models that take an initial set of retrieved documents and reorder them to improve relevance. They're slower but more accurate than first-stage retrieval.

Typical flow: retrieve 100 candidates with fast dense/sparse search, then rerank the top 20 with a cross-encoder model.

**Evaluating rerankers:**

- **Pre-reranking metrics:** Calculate precision, recall, MRR, nDCG on the initial retrieved set (before reranking)
- **Post-reranking metrics:** Calculate the same metrics after reranking
- **Reranker lift:** The difference between post and pre metrics

Good rerankers improve **nDCG by 0.1 to 0.2** and **MRR by 0.05 to 0.15**. If your reranker isn't improving metrics by at least 0.05, it's not worth the latency cost.

Also measure **reranker latency** separately. If reranking adds 500ms but only improves nDCG by 0.03, that's not a good tradeoff.

One subtle point: rerankers can hurt recall if they're too aggressive. I've seen rerankers trained to maximize precision at 5 that pushed relevant documents out of the top 10 entirely. Always check recall at K after adding reranking.

### Multi-Hop Retrieval Metrics

**Multi-hop retrieval** is when you need to retrieve documents in multiple stages to answer complex queries. For example: "What was the revenue impact of the product launch mentioned in the Q3 earnings call?"

Step 1: Retrieve Q3 earnings call transcript
Step 2: Find mention of product launch in the transcript
Step 3: Retrieve revenue data related to that product

Single-hop metrics (precision, recall at K) don't capture this. You need **multi-hop success rate**: did the system successfully complete all retrieval hops and find the final answer?

Also track **intermediate hop quality**: at each stage, were the intermediate retrievals relevant? If step 1 retrieves the wrong document, steps 2 and 3 are doomed.

In practice, multi-hop retrieval is still challenging in 2026. Most teams treat it as an agentic workflow (Chapter 8) and evaluate the full pipeline end-to-end rather than optimizing each hop independently.

### Semantic vs Lexical Recall

A useful diagnostic metric: **semantic recall vs lexical recall**.

**Lexical recall:** Of the relevant documents, how many share keywords with the query?

**Semantic recall:** Of the relevant documents, how many are semantically similar even without keyword overlap?

If your dense retrieval has high lexical recall but low semantic recall, it's not actually doing semantic search—it's just memorizing keyword patterns. This is a sign your embedding model is undertrained or your queries are too narrow.

Conversely, if you have high semantic recall but low lexical recall, you might be missing documents with exact terminology matches. This suggests you need better hybrid fusion.

I've used this diagnostic to identify embedding model problems. One client had semantic recall of only forty percent despite using a supposedly strong embedding model. Turned out the model was trained on web data but their documents were full of domain-specific terminology. We fine-tuned the embeddings on their data and semantic recall jumped to seventy-five percent.

---

## Common Retrieval Failure Modes

Even with good metrics, retrieval systems fail in predictable ways. Knowing the failure modes helps you diagnose problems faster.

### The Vocabulary Mismatch Problem

User queries use different words than documents. Query: "How do I delete my account?" Document title: "Account Termination Procedures."

Dense retrieval should handle this via semantic similarity, but it often fails on domain-specific terminology. Solution: query expansion, synonym injection, or fine-tuning embeddings on domain data.

### The Recency Problem

Your corpus contains fifty versions of the same document across five years. The retrieval system surfaces an outdated version from 2022 instead of the current 2026 version.

Solution: boost recent documents in ranking, filter out deprecated content, or use metadata filters to prefer latest versions.

### The Specificity Problem

User asks a very specific question. Retrieval system returns high-level overview documents because they have broader keyword coverage.

Example: Query: "What's the timeout value for webhook retries?" Retrieved document: "Webhook Configuration Guide" (which mentions retries but not timeout values).

Solution: improve context relevance measurement, use answer-extraction signals in ranking, or add specificity boosting for queries with precise information needs.

### The Chunk Boundary Problem

The answer spans two chunks. Chunk 1 ends with "The maximum file size is" and Chunk 2 starts with "50MB for standard accounts." Neither chunk alone answers the question "What's the maximum file size?"

Solution: overlapping chunks, larger chunk sizes, or post-retrieval stitching of adjacent chunks.

### The Popularity Bias Problem

Your retrieval system always surfaces the same popular documents because they're linked frequently or appear in many training examples, even when niche documents would be more relevant.

Solution: diversify training data, add explore/exploit randomness to retrieval, or use fairness-aware ranking.

---

## Enterprise Expectations for Retrieval Quality

What does "good enough" look like in production systems?

Based on 2026 enterprise deployments:

**Customer support chatbots:**
- Hit rate at 5: minimum ninety percent
- Precision at 5: minimum seventy percent
- nDCG at 10: minimum 0.65
- Latency p95: under 500ms

**Internal knowledge search:**
- Hit rate at 10: minimum eighty-five percent
- Recall at 10: minimum sixty percent
- nDCG at 10: minimum 0.60
- Latency p95: under 1 second (users tolerate slightly higher latency for comprehensive search)

**Legal/compliance research:**
- Recall at 20: minimum seventy-five percent (missing relevant precedents is risky)
- Precision at 20: minimum sixty percent (lawyers will tolerate noise to ensure completeness)
- nDCG at 20: minimum 0.55
- Latency p95: under 2 seconds (accuracy matters more than speed)

**Code search:**
- Hit rate at 10: minimum ninety percent
- MRR: minimum 0.60 (developers want the right file in top results)
- Precision at 10: minimum sixty-five percent
- Latency p95: under 800ms

These are **minimum bars for production launch**. High-performing systems often exceed these significantly—hit rates above ninety-five percent, nDCG above 0.75, latency under 200ms.

But don't over-optimize before launch. Shipping a system with seventy percent precision that improves over time via user feedback is better than spending six months trying to hit ninety percent precision in the lab.

**Launch at minimum viable quality, then iterate.**

---

## Pulling It Together: A Retrieval Evaluation Template

Here's a lean template for evaluating retrieval quality:

```yaml
retrieval_eval_config:
  dataset:
    name: "customer-support-retrieval-v1"
    queries: 250
    source: "production logs + synthetic queries"
    labeling: "LLM-assisted with 20% human validation"

  metrics:
    primary:
      - nDCG_at_10  # Overall ranking quality
      - hit_rate_at_5  # Minimum usability bar

    secondary:
      - precision_at_5
      - recall_at_10
      - MRR

    diagnostic:
      - context_relevance  # Are retrieved docs actually useful?
      - semantic_vs_lexical_recall
      - diversity_at_10

  performance:
    - latency_p50
    - latency_p95
    - latency_p99

  quality_gates:
    launch_minimum:
      hit_rate_at_5: 0.90
      nDCG_at_10: 0.65
      latency_p95_ms: 500

    production_target:
      hit_rate_at_5: 0.95
      nDCG_at_10: 0.75
      latency_p95_ms: 300

  evaluation_frequency:
    regression_suite: "every deployment"
    full_eval: "monthly"
    user_feedback_analysis: "weekly"
```

**Use this structure to track retrieval quality over time.** When you make changes—new embeddings, different chunking, reranker updates—run the full eval and compare metrics. Did nDCG improve? Did latency regress? Did hit rate stay stable?

Treat retrieval evaluation like unit tests for traditional software. You wouldn't deploy code without running tests. Don't deploy retrieval changes without running evals.

---

## Interview Questions on Retrieval Quality Metrics

**Q: We're building a RAG chatbot for customer support. Leadership wants one number to track retrieval quality. Which metric should I choose and why?**

A: I'd choose **hit rate at 5** as your primary executive metric, with **nDCG at 10** as your team's internal optimization target.

Here's why: hit rate at 5 is dead simple to explain. "Ninety-two percent of the time, we find at least one useful document in the top five results." Non-technical stakeholders immediately understand it. It's also a clear quality gate—if hit rate drops below ninety percent, something is broken and users are getting frustrated.

But hit rate alone doesn't tell you about ranking quality. You could have hit rate of ninety-five percent but terrible ranking where relevant documents are always in position five. That's why your team should optimize for nDCG at 10 internally—it rewards putting the best documents first, which directly improves user experience.

Track both. Report hit rate to leadership monthly. Use nDCG to guide optimization decisions. If you have to choose only one, start with hit rate—it's the minimum bar for a functional system. Once you're consistently above ninety percent, shift focus to improving nDCG.

Also track latency p95 as a third metric. Speed matters. A system with ninety-five percent hit rate but 2-second latency will feel broken to users.

**Q: Our precision at 10 is seventy-five percent but recall at 10 is only thirty percent. Users are complaining the system feels incomplete. How should I tune this tradeoff?**

A: Your users are telling you they value completeness over cleanliness. Seventy-five percent precision means one in four results is irrelevant—that's pretty good. But thirty percent recall means you're missing seventy percent of relevant documents. In knowledge work, missing information is often worse than seeing some noise.

I'd recommend increasing K—retrieve more documents. Try precision/recall at 20 or even 50. You'll accept lower precision (maybe dropping to sixty percent) but gain much higher recall (hopefully sixty to eighty percent).

Practically, this might mean:

1. Increase initial retrieval from top 10 to top 50 candidates
2. Add a reranker to sort those 50 by relevance
3. Present the top 20 after reranking
4. Use UI design to help users scan results quickly (summaries, highlight matching terms)

You can also explore **query expansion**—rewrite the user's query to capture related concepts—which often improves recall without drastically hurting precision.

Finally, measure **context relevance** separately from document relevance. Sometimes low recall is because your chunks are too small and each chunk covers only part of the answer. Larger chunks or better chunk overlap might help.

The key insight: precision-recall tradeoffs are domain-dependent. In customer support where users want one fast answer, optimize for precision. In research, legal, or technical documentation where users want comprehensive results, optimize for recall.

**Q: We're using hybrid retrieval—dense embeddings plus BM25. How do I know if the hybrid approach is actually better than using just one method?**

A: Run **ablation tests**. Measure metrics three ways:

1. Dense-only retrieval (turn off BM25, use only embeddings)
2. Sparse-only retrieval (turn off dense, use only BM25)
3. Hybrid retrieval (both enabled)

Calculate precision, recall, nDCG, and MRR for each configuration on the same evaluation set. Hybrid should beat both individual methods. If it doesn't, your fusion strategy is broken.

Example results I've seen:
- Dense-only: nDCG at 10 = 0.68, MRR = 0.52
- Sparse-only: nDCG at 10 = 0.61, MRR = 0.58
- Hybrid: nDCG at 10 = 0.74, MRR = 0.61

Here, hybrid is clearly winning. Dense is better at ranking (higher nDCG), sparse is better at surfacing relevant docs in top positions (higher MRR), and hybrid gets the best of both.

Also measure **contribution rates**: what percentage of top-10 results came from dense vs sparse? If ninety-eight percent come from dense, you're not really getting value from sparse. Tune your fusion weights to ensure both components contribute meaningfully.

One more diagnostic: **stratify by query type**. Dense often wins on natural language questions ("How do I configure X?"), sparse often wins on exact matches ("error code E4021"). If you see this pattern, you might use different strategies for different query types rather than always doing hybrid.

**Q: Our nDCG at 10 is 0.58—is that good or bad? How do I know if I should invest in improving retrieval or move on to optimizing generation?**

A: Context matters, but 0.58 is **mediocre for modern enterprise systems**. It's not broken—users are getting some relevant results—but there's significant room for improvement.

Here's how I'd assess whether to invest:

First, check your **hit rate**. If hit rate at 5 is above ninety percent, you're at least surfacing relevant documents consistently. The 0.58 nDCG means ranking isn't great, but you're not totally failing. If hit rate is below eighty-five percent, you have a bigger problem—fix that first.

Second, measure **end-to-end answer quality** with your current retrieval. If users are happy with answers despite mediocre retrieval, maybe your generation is compensating. But if users complain about wrong or incomplete answers, improving retrieval is probably the highest-leverage fix.

Third, estimate **marginal gains**. Retrieval improvements tend to have outsized impact on downstream quality. I've seen systems where improving nDCG from 0.58 to 0.72 (via better embeddings and reranking) improved final answer correctness by fifteen percentage points. That's huge ROI.

Rule of thumb: if nDCG is below 0.65 and you're seeing quality complaints, invest in retrieval before generation. If nDCG is above 0.70 and you're still seeing quality issues, the problem is likely in generation, faithfulness, or prompting.

For 0.58 nDCG, I'd recommend a focused two-week sprint on retrieval: try better embeddings, add a reranker, tune hybrid fusion weights, and improve chunking. If that gets you to 0.68-0.72, great—move on to generation. If you're stuck at 0.60 after trying multiple things, you might have a data quality problem (bad chunks, outdated docs) that requires deeper investment.

**Q: How do I build ground truth labels for retrieval evaluation when we're just starting out and have no production data yet?**

A: Start with a **hybrid manual-and-LLM labeling approach** on a small high-quality dataset.

Step 1: Create 50-100 representative queries covering your main use cases. If you don't have production queries yet, brainstorm with domain experts or use synthetic query generation from your documents.

Step 2: For each query, retrieve 10-20 candidate documents using your initial system.

Step 3: Use an LLM (Opus 4.5 or similar) to label relevance on a 0-3 scale for each query-document pair. Prompt: "Rate how relevant this document is to answering the query. 0=not relevant, 1=marginally relevant, 2=relevant, 3=highly relevant."

Step 4: Have a human expert review twenty percent of the LLM labels. Check agreement rate. If human-LLM agreement is above seventy-five percent, the LLM labels are probably good enough for bootstrapping.

Step 5: Use the LLM labels for the bulk of your dataset. Have humans hand-label the hard cases (where the LLM was uncertain or where stakes are high).

This gives you **100-200 labeled queries** in a few days rather than weeks of pure manual labeling. It's enough to start measuring metrics and making optimization decisions.

Once you deploy to production, start collecting **interaction data**—clicks, dwell time, thumbs up/down. After a few weeks, you'll have implicit labels from real user behavior. Use those to expand your ground truth dataset.

Over six months, aim to grow to 500-1000 labeled queries. Prioritize labeling queries where your system performs poorly (low confidence, user complaints) and queries that represent high-value use cases.

Don't let perfect be the enemy of good. A small high-quality labeled dataset is infinitely better than no ground truth. Start small, iterate, and grow your eval set over time.

---

## What's Next: Generation and Faithfulness Evaluation

We've covered how to measure whether your retrieval system finds the right documents. That's the foundation. But retrieval is only half of RAG.

Once you've retrieved documents, your language model has to generate an answer. And that answer needs to be **faithful**—grounded in the retrieved content, not hallucinated or made up.

**Chapter 9.3 — Generation and Faithfulness Evaluation** covers:

- How to measure whether generated answers are actually supported by retrieved documents
- Hallucination detection and attribution metrics
- Answer correctness, completeness, and relevance
- Balancing faithfulness with usefulness—when strict grounding makes answers worse
- 2026 techniques for automated faithfulness checking using LLMs as judges

Retrieval gets you the right context. Generation turns that context into useful answers. Both need rigorous evaluation. Let's move to the next stage of the RAG pipeline.

---

**Next Chapter:** [9.3 — Generation & Faithfulness Evaluation](/core/01-eval-strategy/9-3)

**Previous Chapter:** [9.1 — RAG Evaluation Overview](/core/01-eval-strategy/9-1)

**Related:** [Chapter 4 — Labeling & Annotation](/core/04-labeling-annotation) • [Chapter 5 — Quality Metrics](/core/05-quality-metrics) • [Chapter 7 — Automated Evals](/core/07-automated-evals)
