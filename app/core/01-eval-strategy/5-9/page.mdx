# Chapter 5.9 — Synthetic Data Generation for Evaluation

**What we're doing here:**
You need 500 adversarial eval cases for your RAG system. Your production logs have 12 that qualify. Expert annotators can write maybe 20 per week. At that rate, you'll have your dataset in six months.

Or you use synthetic generation and have a draft set in two days.

Synthetic data generation — using LLMs to create eval cases — has become one of the most powerful dataset construction techniques in 2026. But it comes with traps. If you generate carelessly, you get a dataset that looks diverse but actually tests the same narrow patterns over and over. This chapter covers how to generate synthetic eval data that's genuinely useful and how to avoid the pitfalls that make it worse than useless.

---

## 1) Why synthetic generation matters in 2026

Three forces make synthetic generation essential:

**Coverage gaps are expensive to fill manually.** Your taxonomy has 200 task types across 4 channels, 3 languages, and 4 difficulty levels. That's thousands of cells in your coverage map. Production logs cover the head well but leave the long tail empty. Expert-written cases are high quality but slow and expensive.

**Adversarial cases are hard to find organically.** Real users don't typically send prompt injections, multi-constraint traps, or policy edge cases in high volume. You need to create these deliberately.

**Model capabilities evolve faster than datasets.** When you upgrade from one model to the next, your "hard" cases may become "easy." You need to generate fresh hard and adversarial cases quickly — faster than manual authoring allows.

---

## 2) The generation pipeline

Think of synthetic generation as a three-stage process: **seed → generate → validate.**

### Stage 1: Seed design
Seeds are the inputs that guide generation. Good seeds produce diverse, realistic output. Bad seeds produce repetitive, unrealistic output.

**Seed types:**
- **Production log seeds** — real user queries that you want variants of. "Generate 10 harder versions of this customer question."
- **Taxonomy seeds** — your task taxonomy cells. "Generate an adversarial RAG query for the billing intent in Spanish."
- **Failure pattern seeds** — known failure modes from production. "Generate 5 cases that would trigger this hallucination pattern."
- **Constraint seeds** — specific difficulty factors. "Generate a query with 3 conflicting constraints and ambiguous intent."

**The diversity rule:** If you use only one seed type, your dataset will be narrow. Mix seed types. Vary the generation prompt. Use different source materials. The goal is coverage breadth, not volume.

### Stage 2: Generation
Use an LLM to produce eval cases from your seeds. The generation prompt matters enormously.

**What makes a good generation prompt:**
- Specify the channel (chat, RAG, agent, voice)
- Specify difficulty level and why it should be that difficulty
- Specify the task type from your taxonomy
- Include the expected output format (what fields the eval case needs)
- Include constraints — what makes this case hard, what edge case it tests

**What makes a bad generation prompt:**
- "Generate 100 test cases" — no guidance, you'll get 100 similar cases
- Using the same prompt template for every batch — mode collapse
- Not specifying difficulty — you'll get 90% normal cases

**Model choice for generation:** Use a strong model (frontier-class) for generation. Weaker models produce less diverse, less realistic output. The generation model should ideally be different from the model you're evaluating — otherwise you're testing a model on cases it would naturally produce, which biases toward its strengths.

### Stage 3: Validation (non-negotiable)
Synthetic cases must be validated before entering your eval set. Never trust raw LLM output as ground truth.

**Validation checks:**
- **Realism check** — would a real user plausibly send this? Synthetic cases often sound "too perfect" or use patterns no real human would.
- **Difficulty verification** — is the labeled difficulty actually correct? Run your difficulty heuristics (Ch 5.3) on generated cases.
- **Ground truth accuracy** — is the expected output actually correct? Have a domain expert verify a sample.
- **Deduplication** — run against your existing dataset (Ch 5.6). Synthetic generators often produce near-duplicates across batches.
- **Diversity audit** — cluster generated cases by embedding. If they cluster tightly, your seeds weren't diverse enough.

**Default validation rate:** Human-review at least 20% of synthetic cases before they enter the eval set. For adversarial and Tier 2-3 cases, review 50%+.

---

## 3) Common synthetic generation strategies

**Gap-filling:** Identify empty cells in your coverage map (Ch 3.2), then generate cases specifically for those cells. This is the highest-ROI use — you're filling real gaps, not adding volume for its own sake.

**Difficulty scaling:** Take existing normal-difficulty cases and ask the LLM to create harder versions. "Take this billing query and add two conflicting constraints and make the intent ambiguous." This preserves the topic while pushing difficulty.

**Adversarial generation:** Prompt the LLM to create cases designed to break specific behaviors — prompt injections, retrieval traps, policy edge cases, safety boundary tests. These are hard to find organically and essential for Tier 2-3 evaluation.

**Persona-based generation:** Generate cases from specific user personas — a confused elderly user, an impatient power user, a non-native speaker, a malicious actor. This creates natural diversity in phrasing, complexity, and intent.

**Cross-lingual generation:** Generate the same scenario in multiple languages to build multilingual coverage (more in Ch 5.10). Verify with native speakers — LLM translations can be fluent but culturally wrong.

---

## 4) Avoiding mode collapse

**Mode collapse** is when your synthetic dataset looks diverse on the surface but actually tests the same narrow patterns. This is the biggest risk of synthetic generation.

**How it happens:**
- Same generation prompt used for every batch → similar structure and phrasing
- Same model used for everything → reflects that model's biases and style
- Seeds drawn from the same source → narrow topic coverage
- No diversity audit → nobody checks whether generated cases are actually different

**How to prevent it:**
- Vary generation prompts between batches (different phrasings, different constraint mixes)
- Use multiple models for generation when possible
- Draw seeds from diverse sources (production logs + expert cases + taxonomy cells)
- Run embedding-based diversity analysis on every batch — if cases cluster too tightly, regenerate with different seeds
- Set a minimum unique intent coverage threshold — if 500 generated cases only cover 30 unique intents, that's too narrow

---

## 5) Knobs & defaults

**Generation model:** Use frontier-class models. Ideally different from the model being evaluated.

**Batch size:** Generate in batches of 20-50 cases per prompt, not 500. Smaller batches with varied prompts produce more diversity.

**Validation rate:** 20% human review for normal cases, 50%+ for adversarial and high-stakes. 100% for gold set additions.

**Diversity threshold:** After generation, cluster by embedding. If more than 30% of cases fall in a single cluster, seeds weren't diverse enough — regenerate.

**Synthetic-to-organic ratio:** No more than 60% of your eval set should be synthetic. Production logs and expert-written cases provide realism that synthetic data can't fully replicate. For safety/adversarial suites, synthetic can go higher (up to 80%) because organic adversarial cases are rare.

---

## 6) Failure modes

**"Generated cases all sound the same."**
Mode collapse from using one prompt template. Fix: vary prompts, vary models, vary seeds. Run diversity analysis before accepting a batch.

**"Synthetic cases are easy — model scores 95% on them."**
Generator produced cases that match its own strengths. Fix: use a different model for generation than evaluation. Explicitly prompt for hard factors (multiple constraints, ambiguity, edge cases). Verify difficulty labels with heuristics.

**"Expected outputs in synthetic cases are wrong."**
LLM generated the case and the answer, but the answer is hallucinated or incorrect. Fix: never trust synthetic ground truth without validation. Have domain experts verify expected outputs on at least 20% of cases. For factual tasks, cross-check against your source-of-truth (Ch 4.3).

**"Synthetic cases don't look like real users."**
Too polished, too structured, no typos or conversational mess. Fix: add persona seeds (confused user, rushed user). Include real production log examples in the generation prompt as style references. Post-process to add realistic noise.

---

## 7) Enterprise expectations

- They use synthetic generation to fill coverage gaps and build adversarial cases — not as a replacement for production data
- They validate every synthetic batch (20%+ human review, 100% automated QA)
- They track synthetic-to-organic ratio per slice and keep it below 60% for general quality
- They run diversity audits on generated batches before accepting them
- They use different models for generation vs evaluation to avoid self-reinforcing bias
- They version synthetic generation prompts alongside datasets (Ch 5.8 lineage tracking)
- They regenerate synthetic cases quarterly to keep up with model capability changes

---

## 8) Interview Q&A

**Q: How do you use synthetic data in your eval pipeline?**
A: I use synthetic generation to fill coverage gaps — empty cells in my taxonomy map, adversarial cases, and difficulty-scaled variants. I generate in small diverse batches with varied prompts and seeds, validate 20-50% with domain experts, and never let synthetic exceed 60% of the eval set. Every batch goes through dedup and diversity analysis before entering the dataset.

**Q: How do you prevent synthetic data from making evals unreliable?**
A: Three controls. First, I use a different model for generation than evaluation so I'm not testing a model on its own output patterns. Second, I validate ground truth on at least 20% of cases — synthetic answers are often wrong. Third, I run diversity audits to catch mode collapse — if generated cases cluster too tightly, I regenerate with different seeds.

**Q: When do you prefer synthetic over organic data?**
A: Adversarial cases — real users rarely send prompt injections in volume, so I generate them. Long-tail coverage gaps — production logs cover common queries well but miss rare intents. Difficulty scaling — I take real normal-difficulty cases and generate harder variants. For head queries and realistic user behavior, I always prefer production data.

---

*Next: Chapter 5.10 covers multilingual and cross-cultural dataset construction — how to build eval sets that work across languages and cultural contexts.*
