# Chapter 3.2 — Coverage Map (What's Tested vs Untested)

**What we're doing here:**
Now that we have a task taxonomy (3.1), we build a **coverage map**: a clear view of which tasks are tested well, which are barely tested, and which are not tested at all.

**Why this matters:**
Most teams don't fail because they have "no evals."
They fail because they have evals that only cover:
- easy examples,
- happy paths,
- and low-risk tasks.

A coverage map makes gaps obvious and forces smart prioritization.

---

## 1) Mechanics: what a coverage map is

A **coverage map** is a matrix that connects:

- **Tasks** (from your taxonomy)
- **Test types** (human eval, automated, red-team, regression, monitoring)
- **Slices** (risk tier, language, channel, customer segment, difficulty)

It answers 5 important questions:
1. Which tasks have strong tests?
2. Which tasks are missing tests?
3. Which high-risk tasks have weak gates?
4. Which user segments/languages are not represented?
5. Where are regressions most likely to slip through?

---

## 2) The 5 kinds of coverage you must track

Enterprise teams track coverage in these dimensions:

### 2.1 Task coverage
Are all **leaf tasks** represented in evals?

### 2.2 Risk coverage
Do Tier 2–3 tasks have stronger tests and hard gates?

### 2.3 Difficulty coverage
Do you test:
- easy
- normal
- hard
- adversarial/edge cases

### 2.4 Slice coverage
Do you test across:
- languages
- regions
- device types
- customer tiers
- domains (finance, healthcare, B2B support, etc.)

### 2.5 Failure-mode coverage
Do you test for known failures:
- hallucinations
- tool loops
- over-refusal
- PII leaks
- citation errors
- voice mishearing critical fields

---

## 3) Knobs & defaults (what you actually set)

### 3.1 "Minimum viable coverage" defaults (good starting point)

For each **leaf task**:
- **10–30 examples** minimum in an eval set
- At least:
  - 60% normal cases
  - 20% easy
  - 20% hard/edge

For **Tier 2–3 tasks**:
- Add a dedicated safety/abuse subset
- Add regression gating in CI (must pass to ship)

### 3.2 Coverage scoring (simple, practical)

Give each task a coverage score:

- **0 = Uncovered** (no tests)
- **1 = Weak** (a few examples, no gates)
- **2 = Medium** (enough examples + some regression checks)
- **3 = Strong** (good dataset + hard gates + monitoring)

### 3.3 Where teams usually set gates

- Tier 0–1: monitor + basic regression suite
- Tier 2: regression gates + human spot checks + monitoring
- Tier 3: strict gates + red-team suite + approval workflows

---

## 4) Failure modes (symptoms + root causes)

### 4.1 "Great offline scores, bad production"
Root causes:
- eval dataset doesn't match real traffic
- missing long-tail intents
- missing tough edge cases

### 4.2 "Safety incidents despite safety policy"
Root causes:
- safety tests not included in release gates
- no red-team suite
- no slice coverage by language or phrasing style

### 4.3 "We keep fixing the same bug"
Root causes:
- no regression tests added after incidents
- no failure-mode coverage tracking

### 4.4 "One big customer is unhappy but dashboard looks fine"
Root causes:
- no slice coverage by tenant/customer segment
- averages hide localized failures

---

## 5) Debug playbook: how to build the coverage map (fast)

### Step 1 — Start with your taxonomy table (3.1)
You need a list of leaf tasks with:
- channel
- risk tier
- complexity
- owner

### Step 2 — Add columns for test coverage
For each task, record:
- Human eval set exists? (Y/N)
- Automated regression exists? (Y/N)
- Safety/red-team tests exist? (Y/N)
- Monitoring signals exist? (Y/N)

### Step 3 — Add volume and freshness
- Number of examples in dataset
- Last updated date
- Source type (prod logs, synthetic, expert-written)

### Step 4 — Add slices
Record whether the dataset includes:
- multilingual
- noisy/voice
- adversarial prompts
- long context
- tool errors and timeouts

### Step 5 — Score coverage 0–3
Then sort:
- highest risk + lowest coverage = highest priority

---

## 6) Enterprise expectations (what serious teams do)

- Coverage map is reviewed weekly or per release
- Every production incident must lead to:
  - a new regression test
  - updated dataset examples
  - updated coverage score
- High-risk features require:
  - pre-launch sign-off
  - safety suite pass
  - canary release plan
- Coverage is tracked by:
  - task
  - risk tier
  - top customers
  - major languages

---

## 7) Ready-to-use templates

### 7.1 Coverage map table (copy/paste)

| Task ID | Leaf Task | Tier | Channel | Dataset Size | Human Eval | Auto Regression | Safety Suite | Monitoring | Coverage Score (0–3) | Notes / Gaps |
|---|---|---:|---|---:|---|---|---|---|---:|---|
| T-001 | Retrieve refund policy | 2 | RAG | 18 | Y | N | Y | Y | 2 | Need CI gate + more hard cases |
| T-002 | Book appointment | 2 | Agent | 12 | Y | Y | Y | Y | 3 | Add tool-timeout cases |
| T-003 | Answer general FAQ | 1 | Chat | 25 | N | Y | N | Y | 1 | Add human eval + edge cases |

---

### 7.2 "Coverage gap" checklist (fast)

High priority gaps:
- Tier 2–3 tasks with Coverage 0–1
- Tasks with no regression tests after past incidents
- No multilingual coverage for top markets
- No adversarial / jailbreak-style safety coverage
- No tool-failure coverage for agents
- No "must abstain" coverage for RAG
- No critical-field confirmation tests for voice

---

## 8) Interview-ready talking points

> "I maintain a coverage map: tasks x risk x test type x slices, scored 0–3."

> "I prioritize by risk: Tier 3 tasks must have strict gates and safety suites."

> "Every incident creates a regression test and updates the coverage map."

> "I slice coverage by customer and language because averages hide real failures."
