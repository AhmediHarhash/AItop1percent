# 12.9 — Data & Context Change Regression Testing

A financial services company had a beautiful RAG system. Answer quality was stellar. Their evaluation pipeline caught every prompt regression, every model change issue. Then one Monday, customer complaints spiked. The AI was giving outdated product information, missing recent policy changes, and contradicting the company website.

What happened? Their knowledge base team had updated hundreds of documents over the weekend. New compliance documents. Revised product descriptions. Updated FAQ entries. All necessary changes. But nobody ran the evaluation suite afterward. The team had robust regression testing for code changes, prompt changes, model changes. But data changes? Those just… happened.

This is the silent killer of AI quality. Everyone tests when they change the model or the prompt. Almost nobody tests when they change the data.

---

## The Invisible Regression Surface

Your AI system has multiple regression surfaces. Most teams guard the obvious ones: model updates, prompt changes, code deployments. But there's a larger, more volatile surface that changes constantly: **data and context**.

In a RAG system, data changes include:
- New documents added to the knowledge base
- Existing documents updated or deleted
- Changes to chunking strategy or chunk size
- Updates to the embedding model
- Re-indexing of the vector store
- Changes to retrieval parameters or ranking algorithms

For agent systems, add:
- External API schema changes
- Tool definition updates
- Parameter validation changes
- Data source accuracy drift

Each of these changes can silently degrade quality. The model still works. The code still runs. But the answers get worse.

The insidious part: **data changes feel safe**. Engineers know that code changes need testing. But updating documentation? Adding more knowledge? That feels like an improvement, not a risk. It usually is an improvement. But without testing, you don't know.

---

## RAG Regression Patterns

RAG systems are especially vulnerable because their behavior depends entirely on what context gets retrieved. Change the retrieval, change the output.

**Knowledge Base Updates**

Three scenarios:

*New documents added*: More content should improve coverage, right? Sometimes. But new documents can shadow existing ones. A poorly-written new document about Product A might rank higher than your carefully-curated Product A FAQ. Suddenly, quality drops for Product A queries.

*Documents updated*: Facts change. Products evolve. Policies update. You must update your knowledge base. But each update changes the embedding space. A document that previously ranked third might now rank seventh. Queries that worked perfectly now retrieve different context.

*Documents removed*: You deprecate a product, remove its documentation. Good practice. But if users still ask about that product, your system now fails where it previously succeeded. Maybe you want that failure (redirect to new product), but you should know it's happening.

**Chunking Strategy Changes**

You optimize your chunking strategy (see Chapter 9). You move from fixed 512-token chunks to semantic chunking. Or you adjust overlap. Or you change how you handle section headers.

Every chunking change is a complete re-index. Every chunk embedding changes. The entire retrieval surface changes. This is a major regression risk that teams often treat as a minor tweak.

One team changed their chunk size from 512 to 768 tokens to reduce retrieval calls. Cost went down 30%. But answer quality dropped on questions requiring multiple facts, because the relevant facts were now spread across different chunks that didn't get retrieved together.

**Embedding Model Updates**

You switch from text-embedding-ada-002 to text-embedding-3-large. Better embeddings, better retrieval, better quality. Probably. But you need to verify. The new model might have different biases, different semantic understanding, different distance distributions.

After embedding model changes, run your full RAG golden set. Don't assume improvement. Measure it.

**Vector Store Re-indexing**

Even without changing anything else, re-indexing can cause differences. Floating-point precision. Index optimization. Random initialization in approximate nearest neighbor algorithms. Most differences are tiny. But in edge cases, they can flip which documents get retrieved.

For reproducibility: version your vector stores. Keep the old index running while you test the new one. Cut over only after verification.

---

## API Data Source Regression

Your AI calls external APIs for real-time information. Weather data. Stock prices. Product inventory. Calendar availability. These APIs change:

**Schema Changes**

The API adds a new required field. Or removes a field your prompts reference. Or changes field types (string to number). Your AI's tool calling breaks or degrades.

One team's travel assistant used a flight search API. The API added a new "airline alliance" filter that became required. The AI's existing queries started failing silently—the API returned empty results instead of errors. The AI told users "no flights available" for routes that had dozens of options.

**Coverage Changes**

The API adds new data sources or removes old ones. A weather API starts covering more international cities—good. But it also drops support for small towns, returning errors instead—bad. Your AI's reliability profile changes.

**Accuracy Drift**

APIs have quality issues too. Their data freshness degrades. Their accuracy drops. Your AI is only as good as its data sources.

Testing approach: **monitor API reliability separately**. Track API error rates, response times, data freshness. When API metrics degrade, test your AI's quality with that degraded data. Does it fail gracefully? Does it cite the data uncertainty? Or does it confidently hallucinate?

---

## Context Assembly Changes

Your context assembly logic determines what information the model sees. Change that logic, change the behavior.

**Context Window Management**

You used to include the full conversation history. Now you truncate to the last 10 messages to save cost. Does quality drop? For which types of conversations?

You change your truncation strategy from "drop oldest" to "drop middle, keep first and last." This can improve quality (preserve initial context and recent exchanges) or harm it (lose important middle details).

You switch from truncation to summarization. The model sees summaries of old messages instead of full text. More cost, but potentially better continuity. Does it actually improve quality? Test it.

**Context Prioritization**

You implement smart context ranking (see Chapter 22). Instead of chronological order, you rank context by relevance. Retrieved documents, conversation history, user profile data, system instructions—you order them by importance.

This is a major behavioral change. Test it thoroughly. Your golden set examples might retrieve different context in different orders, leading to different outputs.

**Metadata Changes**

You add timestamps to retrieved chunks. Or source citations. Or confidence scores. These metadata additions change the context structure. Sometimes models ignore metadata. Sometimes they over-rely on it. Test to see which.

---

## Tool Schema Regression

For agent systems using tools (see Chapter 8), tool changes cause behavioral regression.

**Tool Definition Changes**

You rename a tool. Change its description. Add or remove parameters. Change parameter types or validation rules. Each change affects tool selection and usage.

One team added an optional "reason" parameter to their database query tool, thinking it would help with debugging. The AI started spending tokens explaining its reasoning in the parameter instead of just querying. Tool usage patterns shifted. Quality on some tasks dropped because the AI became more verbose and slower.

**Tool Availability Changes**

You add new tools. The AI might over-use them, ignoring better approaches. You remove tools. The AI might fail at tasks it previously succeeded at, or find creative workarounds that work less reliably.

You change tool access permissions based on user roles. The same query from different users now routes to different tools. Your eval set needs to cover these permutations.

**Tool Output Changes**

The tool schema stays the same, but the data it returns changes. Your database schema evolves. Your API endpoints add new fields. The AI sees different information in tool results, potentially changing its reasoning and outputs.

---

## Testing Methodology

Here's how to actually test data and context changes:

**Before/After Snapshots**

Before any significant data change:
1. Run your golden set and record current metrics
2. Save the baseline results and scores
3. Make the data change
4. Re-run the golden set on the new data
5. Compare metrics and investigate any degradations

This is straightforward for planned changes (knowledge base updates, embedding model swaps). For continuous changes, you need a different approach.

**Continuous Monitoring**

For knowledge bases that update daily:
- Run a quick regression suite (10-50 critical examples) after each update
- Run a full regression suite (500+ examples) weekly
- Alert on any metric drops above threshold (e.g., 5% decline in answer quality)

**Change Attribution**

When quality drops, you need to know why. Did a specific document change cause it? Did the new chunking strategy hurt this query type?

Debugging process:
1. Identify failing examples
2. Check what context was retrieved (before and after)
3. Compare retrieved chunks—what changed?
4. Trace back to the data change that caused the retrieval change

This is where **retrieval logging** becomes essential. Log every retrieval: query, retrieved chunks, relevance scores, timestamps. When quality drops, you can diff retrievals before and after the change.

**Canary Testing**

For major data changes: don't cut over all at once. Route 5% of traffic to the new data source while 95% stays on the old one. Compare quality metrics between cohorts. If the new data performs well, gradually increase the percentage.

This only works if you have production traffic volume. For smaller systems, stick with golden set testing.

---

## Data Change Frequency Tiers

Not all data changes need full regression testing. Tier your approach:

**Tier 1: Critical Changes (Full Regression)**

Run your complete eval suite:
- Embedding model changes
- Major knowledge base overhauls (50%+ documents changed)
- Chunking strategy changes
- Context assembly logic changes
- Tool schema breaking changes

**Tier 2: Significant Changes (Targeted Regression)**

Run a focused eval set (100-200 examples covering affected areas):
- Knowledge base updates (10-50% documents changed)
- New tools added or removed
- API schema changes
- Context window size changes

**Tier 3: Routine Changes (Smoke Tests)**

Run a quick smoke test (10-20 critical examples):
- Single document updates
- Minor API updates
- Metadata additions
- Tool parameter additions (backward compatible)

**Tier 4: Monitoring Only (No Immediate Testing)**

Rely on production monitoring:
- Individual document additions (less than 1% of total)
- Tool output data changes (same schema)
- Minor metadata updates

The key: **match testing effort to change risk**. Don't skip testing. Don't over-test. Calibrate.

---

## RAG-Specific Testing Patterns

For RAG systems, implement these specific checks:

**Retrieval Quality Tests**

Before/after every major KB update:
- **Recall test**: Do your golden queries still retrieve the relevant documents?
- **Precision test**: Are the top-K retrieved documents still relevant?
- **Ranking test**: Are the most relevant documents still ranking highest?

You can measure retrieval quality independently from answer quality. A document might be retrieved but not used, or used but misinterpreted. Separate retrieval testing helps isolate where quality drops.

**Coverage Tests**

- Query your eval set and track retrieval success rate
- Identify queries that retrieved relevant docs before but don't now
- Identify queries that now retrieve relevant docs but didn't before

The second point is important: data changes can improve quality. You want to detect improvements too, to validate that the update achieved its goal.

**Semantic Drift Tests**

Over time, your knowledge base evolves. New terminology. Different writing styles. Your embeddings might drift away from your query distribution.

Test: take queries from 6 months ago. Do they still retrieve relevant content? If not, your knowledge base language has drifted from your users' language. You might need query expansion or embedding fine-tuning.

---

## Vector Store Quality Monitoring

Your vector store is infrastructure, but it has quality characteristics:

**Index Health**

Monitor:
- Index build times (increasing might indicate scaling issues)
- Query latencies (degradation affects user experience)
- Memory usage (indexes growing too large might need optimization)

**Retrieval Consistency**

Run the same query multiple times. Do you get the same results? You should, unless you're using randomized sampling. If you get different results, your index might have issues or your ranking might be too dependent on near-ties.

**Distance Distributions**

Track the distribution of cosine similarities (or other distance metrics) between queries and retrieved documents. If the distribution shifts significantly after a data update, investigate. It might indicate:
- Embedding space has changed (new embedding model)
- Document distribution has changed (very different content)
- Query distribution has changed (users asking different questions)

These distributions help you understand whether changes in quality metrics are due to the data change itself or to underlying shifts in user behavior or content.

---

## Enterprise Patterns (2026)

Leading organizations have automated this:

**KB Update Pipelines with Eval Gates**

workflow:
kb_update:
  steps:
    - ingest_new_documents
    - validate_format
    - run_chunking
    - generate_embeddings
    - create_canary_index
    - run_regression_suite:
        golden_set: rag_golden_set_v12
        metrics:
          - retrieval_recall
          - answer_correctness
          - citation_accuracy
        thresholds:
          min_recall: 0.85
          min_correctness: 0.80
          max_regression: 0.05
    - deploy_to_production:
        condition: all_tests_passed

The KB update doesn't go live until it passes quality gates. Treat data like code.

**Vector Store Versioning**

Keep multiple versions of your vector store:
- current_production (serving live traffic)
- previous_production (rollback target)
- canary (testing updates)
- dev (for experimentation)

When you update, you build canary, test it, and promote it to production. Previous production becomes the rollback target. This lets you roll back data changes just like code changes.

**Automated RAG Regression**

Systems that run RAG regression automatically:
- After every KB update (quick suite)
- Weekly (full suite)
- Before every deployment (even if KB didn't change, code changes might affect retrieval)
- On-demand (when quality alerts fire)

**Data Change Attribution**

Advanced systems track which data changes affected which quality metrics:

data_change_log:
  change_id: kb_update_2026_01_29
  timestamp: 2026-01-29T10:30:00Z
  changes:
    - added: product_docs/new_feature_x.md
    - updated: faq/pricing.md
    - removed: deprecated/old_product.md
  eval_results:
    retrieval_recall: 0.87 (was 0.89, -0.02)
    answer_correctness: 0.82 (was 0.81, +0.01)
    affected_queries:
      - query: "How much does X cost?"
        before_score: 0.9
        after_score: 0.95
        reason: updated_faq_improved_answer
      - query: "What features does old product have?"
        before_score: 0.8
        after_score: 0.0
        reason: document_removed_intentional

This level of attribution helps you understand causality and make informed decisions about data changes.

---

## Context Change Testing

Testing context assembly changes requires careful experimental design:

**Ablation Testing**

Change one context element at a time:
- Test with full conversation history vs. truncated
- Test with retrieved docs in original order vs. reranked
- Test with metadata vs. without
- Test with system instructions at the top vs. bottom

Measure quality for each configuration. This isolates the impact of each context element.

**Context Window Experiments**

Systematically vary context size and measure quality:
- 2k tokens of context
- 4k tokens
- 8k tokens
- 16k tokens

You might find diminishing returns (quality plateaus after 8k) or sweet spots (performance peaks at 4k, drops at 16k due to lost-in-the-middle effects).

**A/B Testing Context Strategies**

In production, route different users to different context assembly strategies. Compare:
- User satisfaction metrics
- Task success rates
- Latency (more context = slower + more expensive)
- Cost per conversation

Choose the strategy that optimizes your target metric.

---

## Failure Modes and Mitigation

Common failure patterns:

**Silent Degradation**

Data changes, quality drops, nobody notices. Users complain weeks later or just stop using the system.

*Mitigation*: Automated regression testing. Production quality monitoring. Alert on metric drops.

**Catastrophic Retrieval Failure**

A chunking change or embedding update causes retrieval to fail for a major query category. Users suddenly get "I don't have information about that" for previously-working queries.

*Mitigation*: Canary testing. Quick smoke tests before cutover. Rollback capability.

**Knowledge Gaps After Deletions**

You remove outdated documents without adding replacements. Users who ask about those topics now get no answer.

*Mitigation*: Track query coverage before/after. If coverage drops significantly, investigate whether it's intentional.

**Embedding Space Mismatch**

You update embeddings for documents but not for cached query embeddings (if you cache). Or you update your embedding model but not your reranker. The systems get out of sync.

*Mitigation*: Atomic updates. When you change embedding models, rebuild all indexes and clear all caches at once.

**Over-Reliance on New Data**

You add high-quality new documents. The system over-retrieves them (they rank high for everything) and under-retrieves older but still relevant content.

*Mitigation*: Monitor document retrieval distribution. If new documents dominate all queries, your embeddings or ranking might be biased by recency or writing style.

---

## Cost-Quality Tradeoffs

Data change testing has costs:

**Testing Cost**

Running eval suites after every KB update adds compute cost and time. A 500-example RAG eval suite might cost five to twenty dollars and take 10-30 minutes.

For daily KB updates, that's $150-600/month just in testing. Plus engineering time to review results.

Optimization: tiered approach. Quick tests on every update (low cost), full tests weekly (higher cost but less frequent).

**Data Versioning Cost**

Keeping multiple vector store versions requires storage and memory. Embeddings are large. A 10GB document corpus might produce a 50GB vector index.

If you keep three versions (current, previous, canary), that's 150GB. For large organizations with hundreds of gigabytes of documents, this multiplies.

Optimization: only keep current and previous. Build canary on-demand. Compress or downsample dev indexes.

**Rollback Complexity**

Rolling back data changes is harder than rolling back code. Your vector store is stateful. If you rolled back to yesterday's index, but users interacted with today's system, you might create inconsistencies.

Mitigation: short rollback windows (only roll back within hours of deployment) or accept inconsistency and rebuild forward.

---

## Enterprise Expectations

In high-stakes environments:

**Financial Services**

Zero tolerance for giving outdated financial advice. Every KB update must pass compliance review and quality regression. Full eval suite, human review of changes, staged rollout.

Some firms run parallel systems: old KB and new KB side-by-side for a week, comparing outputs. Only cut over when quality is proven.

**Healthcare**

Medical knowledge bases update constantly (new research, updated guidelines). But outdated medical information can harm patients.

Approach: version control for medical content. Each update tagged with version number. AI cites version (e.g., "According to the 2026-01-15 treatment guidelines…"). Clinicians know when information is outdated and can check current guidelines manually.

**Legal**

Law changes constantly. Regulations, case law, statutes. Legal AI must reflect current law, but also must be able to answer questions about past law ("What was the rule in 2024?").

Solution: temporal knowledge bases. Documents tagged with effective dates. Queries specify the relevant time period. Regression testing covers both current law and historical law retrieval.

---

## The Testing Mindset

Data changes are code changes. Treat them the same way.

When an engineer updates a prompt, everyone agrees it needs testing. When a content team updates 200 knowledge base documents, people think "it's just content, it'll be fine."

It won't always be fine. Content changes the behavior of your AI as much as code changes. Test it.

This requires cultural change: content teams, data engineers, and AI teams must collaborate. Content teams need to understand that their updates have quality implications. AI teams need to provide easy testing tools that content teams can use.

Best practice: give content teams a self-service eval dashboard. They make KB updates, click "Test," and see a quality report in minutes. Red scores = investigate before deploying. Green scores = deploy with confidence.

This democratizes quality. Everyone becomes responsible for it.

---

## Practical Testing Template

Here's a minimal viable approach for RAG systems:

**Golden Set for Data Changes**

Build a 100-example set covering:
- 30 examples: factual recall (tests whether key facts are still retrievable)
- 30 examples: multi-hop reasoning (tests whether related docs still co-retrieve)
- 20 examples: edge cases (unusual queries, niche topics)
- 20 examples: negative cases (queries that should return "I don't know")

Run this after every significant KB update (Tier 1 or Tier 2 changes).

**Metrics to Track**

Minimum set:
- Retrieval recall (are the right docs retrieved?)
- Answer correctness (is the final answer correct?)
- Citation accuracy (are citations valid and relevant?)

Optional additions:
- Precision (are retrieved docs relevant?)
- Latency (did retrieval slow down?)
- Cost (did retrieval cost increase?)

**Alert Thresholds**

Define acceptable regression:
- Recall drops more than 5% → investigate
- Correctness drops more than 3% → block deployment
- More than 10% of examples regress → block deployment

Thresholds depend on your risk tolerance. Safety-critical systems might use 1% thresholds. Experimental chatbots might tolerate 10%.

**Weekly Review**

Even if individual updates pass testing, review aggregate trends weekly:
- Is quality gradually declining despite passing individual tests?
- Are certain query types consistently regressing?
- Is the knowledge base growing in ways that hurt retrieval?

Long-term trends matter as much as individual changes.

---

## Looking Forward: Data Quality as Infrastructure

The future of AI quality is treating data quality as infrastructure quality.

Just as you monitor server uptime, request latency, and error rates, you'll monitor data freshness, retrieval quality, and semantic drift. Just as you have CI/CD pipelines for code, you'll have CD pipelines for knowledge (continuous data deployment with eval gates).

The best organizations in 2026 have already built this. Vector stores with integrated quality monitoring. Knowledge base platforms with built-in regression testing. Content management systems that trigger evals on every publish.

For everyone else, it's a competitive advantage waiting to be claimed. Your competitors are probably not testing their data changes. If you do, your quality will be more consistent and reliable than theirs.

Start simple: pick your 50 most important queries. Run them after every KB update. Track whether they still work. Expand from there.

Data change regression testing isn't glamorous. It's not LLM research or prompt engineering wizardry. It's plumbing. But good plumbing is what separates hobbyist systems from production systems that enterprises trust.

---

## Connection to Chapter 12.10

You've built regression testing for model changes, prompt changes, code changes, and now data changes. You have comprehensive quality protection across all dimensions.

But when something goes wrong in production—and something will go wrong—how do you investigate? How do you communicate what happened, what you learned, and how you fixed it?

In **Chapter 12.10: Release Documentation and Post-Deployment Review**, we'll cover the practices that close the loop: documenting what you shipped, reviewing how it performed, learning from incidents, and building institutional knowledge about what works and what doesn't in your AI system.

Quality protection isn't just about preventing problems. It's also about learning from problems when they happen, and getting better over time.

---

## Interview Questions: Data & Context Change Regression Testing

**Q1: Your knowledge base gets updated twice per week with 50-100 document changes each time. Your eval suite has 500 examples and takes 45 minutes to run. How do you balance testing coverage with deployment speed?**

Use a tiered approach. Build a quick smoke test with 20-30 critical examples covering the most important query types and run it immediately after each KB update (takes 2-3 minutes). If that passes, deploy. Run the full 500-example suite nightly, covering all updates from that day. If the nightly run finds regressions, investigate and potentially roll back or patch.

Additionally, categorize KB changes by risk. Small updates (single document, minor edits) only trigger the smoke test. Large updates (new document categories, major rewrites) trigger the full suite before deployment. Over time, analyze which types of changes historically caused regressions, and tune your risk categorization accordingly.

Consider building a faster eval set: maybe you only need 200 examples to catch 95% of regressions. Profile your eval set to identify redundant coverage and prune it.

**Q2: After switching embedding models, your retrieval recall dropped from 0.89 to 0.84. But when you manually review the results, the new model seems to retrieve more semantically relevant documents—users like it better. What's happening, and what should you do?**

Your golden set is probably labeled based on lexical or keyword-based relevance, but the new embedding model is optimizing for semantic relevance. The "better" documents aren't in your ground truth, so recall drops even though actual quality improves.

This is a labeling mismatch. Your ground truth reflects old retrieval assumptions. The new model has different (better) assumptions.

Action: re-evaluate a sample of queries where recall dropped. Have human raters compare old vs. new retrieved documents. If the new ones are actually more relevant, update your ground truth to include them. This requires treating ground truth as living data that evolves with your system.

Alternatively, add a "user satisfaction" metric alongside recall. Run an A/B test with real users, measure which embedding model leads to better task success or satisfaction. If users prefer the new model despite lower recall, trust the user signal and accept that your eval set needs updating.

**Q3: You add 1000 new documents to a 10,000 document knowledge base. A few days later, users report that the AI is "giving generic answers" and "not using the detailed docs anymore." What might have happened, and how do you diagnose it?**

Possible causes:

1. **Retrieval dilution**: The new documents are ranking high (new content, different writing style, maybe more keyword overlap) and pushing older detailed docs down in the ranking. The top-K retrieved chunks now include more generic content.

2. **Chunking issues**: The new documents might be chunked differently (different structure, formatting) leading to less informative chunks.

3. **Embedding bias**: The new documents might have different language or structure that embeds more similarly to user queries, even if they're less relevant.

Diagnosis steps:
- Sample failing queries and log their retrievals before and after the update
- Compare what was retrieved: are the old detailed docs still being retrieved? At what rank?
- Check retrieval scores: are the new docs scoring much higher than old docs?
- Manually review a few new documents: are they well-written, specific, and relevant? Or are they generic/marketing content?

Fix: If new docs are dominating retrieval inappropriately, either improve their chunking, adjust ranking/reranking, or add metadata to differentiate detailed technical docs from generic content and prioritize technical docs for technical queries.

**Q4: Your AI uses an external API for product pricing. The API changed its response format, breaking your tool schema. Your tests didn't catch it until production. How do you prevent this in the future?**

Immediate fix: Add API contract testing. Write tests that validate the API response schema matches your tool's expectations. Run these tests in CI/CD before every deployment, even if you didn't change the tool code (the API might have changed).

Better solution: Implement API versioning. Use a specific version of the API (e.g., `/v2/pricing`) rather than the unversioned endpoint. This way, the provider can't break your integration by changing the default version.

Best solution: Build resilience into your tool code. Parse API responses defensively. If a field is missing, log a warning and degrade gracefully (return partial data, or return an error message the AI can communicate to the user). Add retry logic for transient failures. Monitor API error rates and alert when they spike.

Additionally, add API response validation to your eval suite. Include examples that exercise every tool, and assert not just that the final answer is correct, but that the tool calls succeeded and returned expected data structures. This catches API contract breaks during regression testing.

**Q5: You're designing a regression testing strategy for a RAG system in a highly regulated industry. Changes to the knowledge base require legal review and can't be rolled back easily. What's your testing and deployment process?**

Multi-stage process:

1. **Pre-deployment testing (development environment)**:
   - Content team prepares KB updates in a staging environment
   - Run full regression suite (500+ examples) against staging
   - Generate a quality report comparing old vs. new metrics
   - If any regressions, content team revises updates

2. **Legal and compliance review**:
   - Legal reviews content changes for accuracy and compliance
   - Compliance reviews eval results to ensure no regulatory violations
   - Both must approve before proceeding

3. **Canary deployment**:
   - Deploy new KB to a canary environment serving 5% of users
   - Monitor quality metrics, user feedback, and error rates for 24-48 hours
   - Compare canary metrics to main production metrics
   - If canary performs worse, halt rollout and investigate

4. **Full deployment**:
   - After successful canary period, deploy to full production
   - Run post-deployment regression suite to confirm quality
   - Monitor production metrics closely for the first week

5. **Documentation and audit trail**:
   - Document what changed, why, what the test results were, who approved it
   - Keep logs for regulatory audit purposes
   - Version the KB and eval results for reproducibility

6. **Rollback plan**:
   - Even though rollback is difficult, define conditions that would trigger it (e.g., user complaints above threshold, correctness drops below X%)
   - Keep the previous KB version available for emergency reactivation
   - Communicate rollback process to stakeholders so everyone knows the plan

This is heavyweight, but in regulated industries, the cost of an error (legal liability, regulatory fines, loss of trust) far exceeds the cost of thorough testing.

