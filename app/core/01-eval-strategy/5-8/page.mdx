# Chapter 5.8 — Versioning, Lineage & Dataset Registry

**What we're doing here:**
Six months after launch, your VP asks: "Why did our Q2 metrics drop compared to Q1?"
You pull up the eval results. They used different datasets. One had 300 examples, the other 500. The labels changed. The rubric version changed. The model version changed.

You can't answer the question because you don't know which dataset produced which result.

**Versioning, lineage, and a dataset registry** solve this. They make one thing non-negotiable: **"Which dataset produced this result?" must always be answerable.**

**Enterprise outcome:**
Every eval result is traceable to an exact dataset version, every dataset version has a complete changelog, and every dataset has tracked lineage from raw source to released eval set. You can reproduce last year's eval, compare apples-to-apples across quarters, and confidently say "this regression is real" or "this is a dataset change artifact."

---

## 1) Why versioning is non-negotiable

### 1.1 The problem: invisible dataset drift

Most teams start with a folder of examples. Over time:
- Someone adds 20 new cases to improve coverage
- Someone fixes a label that was wrong
- Someone updates the rubric from 3-point to 5-point scale
- Someone removes a deprecated task type
- Someone swaps out the retriever corpus version

Each change is reasonable. But if you don't version these changes, you get:
- **False regressions** — scores drop because the dataset got harder, not because the model got worse
- **False improvements** — scores rise because bad examples were removed, not because the model got better
- **Unreproducible results** — "I can't replicate the eval from last month" becomes a weekly complaint
- **Broken comparisons** — you compare Q1 to Q2 metrics but the datasets are incomparable

### 1.2 The 2026 reality: compliance and auditability

Regulated industries (finance, healthcare, government) now require:
- Provenance tracking: "Show us how you validated this model"
- Reproducibility: "Re-run the eval that approved this release"
- Change justification: "Why did you modify the ground truth?"

Without versioning and lineage, you cannot pass these requirements. Versioning is not a nice-to-have. It's a compliance and operational necessity.

---

## 2) What to version (the full checklist)

Versioning isn't just "lock the dataset file." You need to version every input that affects eval results.

### 2.1 Dataset content (examples and structure)

Track changes to:
- **Examples added** — new cases inserted
- **Examples removed** — cases deleted or deprecated
- **Examples modified** — input text changed, context edited, multi-turn adjusted
- **Split changes** — moving examples between train/val/test or dev/prod
- **Slice distribution** — rebalancing difficulty, tiers, languages

### 2.2 Labels and ground truth

Track changes to:
- **Ground truth updates** — reference answers corrected, citations fixed, expected tool calls updated
- **Rubric version** — scoring dimensions changed, scale modified (3-point → 5-point), anchor definitions updated
- **Quality tier reassignment** — example moved from Tier 1 to Tier 2
- **Annotator pool changes** — new reviewers added, old reviewers deprecated

### 2.3 Metadata schema

Track changes to:
- **New fields added** — e.g., adding difficulty_label or tenant_id
- **Field definitions changed** — e.g., redefining "adversarial" category
- **Enum values changed** — e.g., adding a new language code or risk tier
- **Required vs optional fields** — changing validation rules

### 2.4 Associated configurations (system under test)

Track versions of:
- **Prompt version** — the prompt template used when this dataset was created or last validated
- **Model version** — the model(s) this dataset was designed for or validated against
- **Retriever version** — for RAG evals, the corpus version and retriever config
- **Tool schemas** — for agent evals, the API definitions and tool versions available

Why track these? Because dataset validity depends on system config. A dataset designed for GPT-4 may not be appropriate for Claude Opus. A RAG dataset validated against corpus v1.2 may break on corpus v2.0.

---

## 3) The dataset registry (central catalog of all eval datasets)

### 3.1 Purpose: one source of truth

The **dataset registry** is a centralized catalog — typically a database or structured file — that tracks every eval dataset in your organization.

It answers:
- What datasets exist?
- Which one should I use for this eval?
- What's the lineage of this dataset?
- Who owns it? When was it last updated? Is it still active?

### 3.2 Required fields (minimum viable registry)

Every dataset entry must have:

| Field | Description | Example |
|---|---|---|
| dataset_id | Unique identifier | rag_general_v2.3 |
| version | Semantic version string | 2.3.1 |
| purpose | What this dataset tests | General RAG quality, Tier 1–2 |
| channel | Chat, RAG, Agent, Voice | RAG |
| split | dev / test / prod / regression | test |
| owner | Team or person responsible | ai-quality-team@company.com |
| created_date | When this version was created | 2026-01-15 |
| status | active / deprecated / archived | active |
| parent_version | Previous version (for lineage) | 2.3.0 |
| changelog_url | Link to changelog doc | wiki.company.com/datasets/rag_v2.3_changelog |
| artifact_url | Storage location | s3://evals/datasets/rag_general_v2.3.jsonl |

### 3.3 Optional but recommended fields

| Field | Description |
|---|---|
| size | Number of examples |
| difficulty_mix | % easy / normal / hard / adversarial |
| slice_coverage | Languages, tiers, tenants covered |
| annotation_status | % labeled, % QA'd, % anchored |
| last_validated_date | Last time this dataset was spot-checked |
| deprecated_reason | Why this dataset is no longer active |
| successor_version | If deprecated, which version replaces it |

### 3.4 Access control and permissions

Track who can:
- **Read** the dataset (most teams)
- **Write/modify** the dataset (dataset maintainers only)
- **Approve new versions** (eval eng + domain experts)
- **Deprecate/archive** (eval eng lead only)

This prevents unauthorized changes that break reproducibility.

---

## 4) Versioning strategies (how to actually version datasets)

### 4.1 Semantic versioning for datasets (major.minor.patch)

Adopt semantic versioning rules:

**major.minor.patch**

- **Major version bump (1.0 → 2.0):**
  - Breaking changes: incompatible schema, different rubric, task taxonomy changed
  - Use case: "This dataset is no longer comparable to v1.x"

- **Minor version bump (2.1 → 2.2):**
  - Backward-compatible additions: new examples added, new slices added, non-breaking metadata fields added
  - Use case: "We expanded coverage but didn't change existing cases"

- **Patch version bump (2.2.0 → 2.2.1):**
  - Bug fixes: typos corrected, labels fixed, duplicate examples removed
  - Use case: "Small corrections that don't affect comparability"

### 4.2 Immutable snapshots (never mutate, always create new version)

**Rule:** Once a dataset version is published and used in an eval, it is **immutable**. You cannot modify it in place.

Why?
- Reproducibility: If v2.3 changes after an eval run, you can't reproduce that run
- Trust: Teams stop trusting results if datasets silently mutate
- Compliance: Auditors require immutable records

**How it works:**
- To fix a label, create v2.3.1 with the fix. Don't modify v2.3 in place.
- Keep old versions accessible (archived storage) for at least 12–24 months
- Clearly mark deprecated versions in the registry

### 4.3 Changelog per version (what, why, who)

Every version must have a changelog. Minimum required info:

```
Version: 2.3.1
Date: 2026-01-20
Author: alice@company.com
Approved by: bob@company.com

Changes:
- Fixed 3 label errors in adversarial pack (cases 450, 487, 502)
- Removed 1 duplicate example (case 310 duplicate of 89)
- Updated metadata schema: added tenant_id field (optional)

Why:
- Label errors discovered during QA review caused false negatives
- Duplicate case was inflating coverage metrics

Impact:
- Patch-level change, backward compatible
- Expected metric change: +0.5% on adversarial subset due to label fixes
- No re-eval required unless targeting adversarial quality specifically

Validation:
- Spot-checked 50 cases, no other issues found
- Difficulty distribution unchanged
- Slice balance unchanged
```

Store changelogs in:
- Version control (Git)
- Wiki or docs system
- Registry changelog_url field

---

## 5) Lineage tracking (from raw source to released dataset)

### 5.1 The lineage chain (typical flow)

A production-grade dataset goes through multiple stages. Track each transformation:

1. **Raw source** — prod logs, expert-written drafts, synthetic generation output
2. **Sanitized** — PII removed, sensitive data redacted (Chapter 5.5)
3. **Labeled** — ground truth assigned, rubrics applied
4. **QA'd** — spot-checked, inter-annotator agreement validated
5. **Released** — versioned, registered, locked

Each stage is a transformation. Lineage tracks:
- What was the input?
- What transformation was applied?
- What was the output?
- Who performed it? When?

### 5.2 Lineage metadata (required tracking)

For each dataset version, store:

```yaml
lineage:
  source_type: production_logs | expert_written | synthetic | hybrid
  source_version: prod_logs_2026-01-10_to_2026-01-15
  raw_count: 12000 examples

  transformations:
    - step: sanitization
      tool: pii_redactor_v3.1
      date: 2026-01-16
      output_count: 11800 (200 removed for privacy)

    - step: labeling
      annotator_pool: internal_qa_team
      rubric_version: v2.1
      date: 2026-01-18
      output_count: 11800 (all labeled)

    - step: QA_review
      reviewer: alice@company.com
      date: 2026-01-19
      corrections: 47 labels fixed

    - step: balancing
      method: stratified_sample
      target_mix: 60% normal, 20% hard, 15% adversarial, 5% easy
      date: 2026-01-20
      output_count: 1000 (sampled from 11800)

  dependencies:
    prompt_version: v4.1
    model_version: gpt-4-2024-11-20
    retriever_version: v2.3 (for RAG datasets)
    tool_schema_version: v1.5 (for agent datasets)
```

### 5.3 Dependency graph (what this dataset depends on)

Track external dependencies so you know when to re-validate:

**Example dependency graph:**

```
Dataset: rag_general_v2.3
  ↓ depends on
Retriever: v2.3
  ↓ depends on
Corpus: wiki_2026-01 + docs_v5.2
  ↓ depends on
Prompt: rag_prompt_v4.1
```

If any dependency changes (e.g., retriever upgraded to v2.4), you know:
- This dataset may need re-validation
- Results are no longer directly comparable
- You may need to create a new dataset version for the new config

---

## 6) Reproducibility (the ultimate test)

### 6.1 The reproducibility requirement

**Definition:** Given a dataset version + config version, you must be able to reproduce the exact eval run that produced historical results.

**What this requires:**

1. **Immutable dataset snapshots** — v2.3 is always v2.3, never changes
2. **Artifact storage** — datasets stored in long-term accessible storage (S3, GCS, etc.)
3. **Config snapshots** — the exact model version, prompt version, tool versions used
4. **Deterministic eval harness** — if you re-run the eval with the same inputs, you get the same outputs (or within known variance bounds for stochastic models)

### 6.2 Access controls (who can read/write/delete)

Without access control, reproducibility breaks:
- Someone accidentally deletes a dataset version
- Someone modifies a "locked" dataset in place
- Someone uses a dataset they're not authorized to see

**Recommended access control model:**

| Role | Permissions |
|---|---|
| Eval eng team | Read all, write new versions, deprecate old versions |
| Product team | Read active/deprecated, cannot write/delete |
| Compliance/audit | Read all including archived, cannot write/delete |
| External contractors | Read only specific datasets, time-limited access |

Implement via:
- IAM policies (for cloud storage)
- Database roles (for registry DB)
- API access tokens (for programmatic access)

### 6.3 Backup and archival policy

**Retention rules (recommended):**

- **Active datasets:** replicated storage, daily backups
- **Deprecated datasets:** moved to archival storage (cheaper tier), retain for 24 months
- **Archived datasets:** cold storage, retain for compliance period (varies by industry, often 5–7 years)

**Deletion policy:**
- Never delete a dataset version that was used in a production release gate
- Only delete after retention period expires and compliance approves
- Always log deletion events (who, what, when, why)

---

## 7) Knobs & defaults (what you actually set)

### 7.1 Versioning scheme

**Default:** Semantic versioning (major.minor.patch)

**Alternative:** Date-based versioning (2026-01-20) for rapidly evolving datasets

**Choose semantic if:**
- You have stable, long-lived datasets
- You need clear backward-compatibility signals
- You compare across versions frequently

**Choose date-based if:**
- You refresh datasets weekly or faster
- Compatibility is less critical (e.g., monitoring sets)
- Simplicity matters more than semantic meaning

### 7.2 Registry implementation

**Lightweight option (startup):**
- YAML or JSON file in Git
- Fields: dataset_id, version, purpose, owner, status, artifact_url, changelog_url
- Works for fewer than 20 datasets

**Production option (enterprise):**
- Database (Postgres, MySQL) with schema enforcement
- API layer for programmatic access
- UI for browsing and search
- Works for 100s–1000s of datasets

### 7.3 Retention policy for old versions

**Default:**
- Active: indefinite retention
- Deprecated: 24 months
- Archived: compliance-driven (5–7 years typical)

**Customize based on:**
- Regulatory requirements (finance = longer, consumer SaaS = shorter)
- Storage cost tolerance
- Litigation risk (some industries keep everything indefinitely)

---

## 8) Failure modes (symptoms + root causes + fixes)

### 8.1 "We can't reproduce last quarter's eval results"

**Symptoms:**
- Re-running an eval from 3 months ago gives different scores
- Team says "the dataset changed but we don't know how"
- Audit asks for proof and you can't provide it

**Root causes:**
- Dataset was mutated in place without versioning
- No immutability enforcement — anyone could edit the file
- Config versions not tracked (prompt changed, model changed)
- No artifact storage — dataset file lost or overwritten

**Fixes:**
- Implement immutable versioning starting now (version everything from this point forward)
- Store all dataset versions in append-only storage (S3 with versioning enabled, GCS with object versioning)
- Create a registry entry for every dataset currently in use, backfill versions as best you can
- Snapshot current configs (prompts, models, tools) and store alongside datasets
- Add a policy: "no eval results without version metadata"

---

### 8.2 "A dataset changed but nobody noticed until production broke"

**Symptoms:**
- Metrics stayed green through release
- Production quality dropped post-release
- Investigation shows eval dataset was silently modified weeks ago — 50 hard cases were removed "to speed up CI"

**Root causes:**
- No changelog requirement — changes weren't documented
- No approval gate — anyone could modify datasets without review
- No monitoring of dataset health metrics (size, difficulty mix, slice balance)

**Fixes:**
- Require changelog for every version bump (no exceptions)
- Require approval from eval eng + domain expert before publishing new version
- Add dataset health checks to CI: flag if size drops >10%, difficulty mix shifts >15%, any slice drops to 0
- Implement access control: only dataset maintainers can write, everyone else is read-only

---

### 8.3 "An old, deprecated dataset is still being used in production evals"

**Symptoms:**
- Team reports eval results that don't match current quality definitions
- Investigation shows they're using dataset v1.5 from 2 years ago, which is deprecated
- Current standard is v3.2

**Root causes:**
- No deprecation enforcement — old datasets still accessible and runnable
- No migration path — team didn't know how to upgrade
- No notification — deprecation wasn't communicated

**Fixes:**
- Mark deprecated datasets in registry with clear status + deprecated_date + successor_version
- Add warnings to eval harness: "WARNING: You are using deprecated dataset v1.5. Migrate to v3.2. See migration guide: [link]"
- After 6-month deprecation period, make old datasets read-only or require explicit override flag
- Send quarterly reminders to dataset consumers: "You are using X deprecated datasets. Please migrate."

---

### 8.4 "We lost the lineage — we don't know where this dataset came from"

**Symptoms:**
- Someone asks "where did these 500 RAG examples come from?"
- No one remembers. No documentation exists.
- Can't determine if they're prod-based (privacy risk) or synthetic (safe to share externally)

**Root causes:**
- Lineage was never tracked
- No source metadata fields
- No process for capturing provenance during dataset creation

**Fixes:**
- Add lineage metadata to every new dataset version (source_type, source_version, transformations, dependencies)
- Backfill lineage for existing datasets: interview dataset creators, check Git history, check old Slack threads
- Make lineage a required field in registry: cannot publish dataset without filling in lineage section
- Add lineage to changelog template: every version bump must document "what changed and where it came from"

---

## 9) Debug playbook: setting up versioning and registry from scratch

**If you have no versioning today, start here:**

### Step 1: Audit current state (1 day)
- List all datasets currently in use
- Identify storage locations (local files, S3 buckets, shared drives)
- Check if any version info exists (folder names, Git history, filenames)

### Step 2: Create registry (1 day)
- Choose implementation (lightweight YAML for under 20 datasets, DB for more)
- Define required fields (dataset_id, version, purpose, owner, status, artifact_url)
- Backfill registry: one entry per dataset currently in use, version = v1.0 if unknown

### Step 3: Establish immutability (1 day)
- Move all datasets to versioned storage (S3 with versioning, Git LFS, or similar)
- Lock write permissions: only eval eng team can write
- Document policy: "never mutate in place, always create new version"

### Step 4: Create changelog template (1 hour)
- Use template from section 10.2 below
- Require changelog for every future version bump

### Step 5: Track dependencies (1 day)
- For each dataset, document: prompt version, model version, retriever/corpus version (if RAG), tool schema version (if agent)
- Add to registry or as separate config file stored with dataset

### Step 6: Communicate and train (ongoing)
- Announce new versioning process to all dataset consumers
- Run training session: "how to request a new dataset version, how to find datasets in registry"
- Add versioning checks to CI: flag eval runs that don't specify dataset version

---

## 10) Enterprise expectations (what serious teams do)

- They version every dataset and config used in evals, no exceptions
- They maintain a central registry with lineage, ownership, and status for every dataset
- They enforce immutability: published versions cannot be modified, only deprecated and replaced
- They require changelogs and approval for every version bump
- They track dependencies: dataset → prompt version → model version → retriever version
- They test reproducibility quarterly: pick a random eval from 6 months ago, re-run it, verify results match (within stochastic tolerance)
- They archive old versions but keep them accessible for compliance and debugging
- They monitor dataset health: size, difficulty mix, slice balance, annotation quality — treat these as SLIs (service level indicators)

---

## 11) Ready-to-use templates

### 11.1 Dataset registry schema (YAML example)

```yaml
datasets:
  - dataset_id: rag_general_v2.3
    version: 2.3.1
    purpose: General RAG quality eval, Tier 1-2
    channel: RAG
    split: test
    owner: ai-quality-team@company.com
    created_date: 2026-01-20
    status: active
    parent_version: 2.3.0
    changelog_url: https://wiki.company.com/datasets/rag_general_v2.3.1_changelog
    artifact_url: s3://evals-prod/datasets/rag_general_v2.3.1.jsonl
    size: 1000
    difficulty_mix:
      easy: 10%
      normal: 60%
      hard: 20%
      adversarial: 10%
    slice_coverage:
      languages: [en, es, fr]
      tiers: [1, 2]
      tenants: [acme_corp, beta_inc]
    lineage:
      source_type: hybrid
      source_version: prod_logs_2026-01-10 + expert_pack_v1.2
      transformations:
        - sanitization (pii_redactor_v3.1)
        - labeling (rubric_v2.1, internal_qa_team)
        - balancing (stratified_sample)
    dependencies:
      prompt_version: rag_prompt_v4.1
      model_version: gpt-4-2024-11-20
      retriever_version: v2.3
      corpus_version: wiki_2026-01 + docs_v5.2
    access_control:
      read: [ai-quality-team, product-team, compliance-team]
      write: [ai-quality-team]
      deprecate: [eval-eng-lead]

  - dataset_id: agent_toolcall_safety_v1.5
    version: 1.5.0
    purpose: Agent tool safety, adversarial red-team suite
    channel: Agent
    split: test
    owner: security-team@company.com
    created_date: 2026-01-18
    status: active
    parent_version: 1.4.2
    changelog_url: https://wiki.company.com/datasets/agent_safety_v1.5_changelog
    artifact_url: s3://evals-prod/datasets/agent_toolcall_safety_v1.5.jsonl
    size: 300
    difficulty_mix:
      easy: 0%
      normal: 10%
      hard: 30%
      adversarial: 60%
    slice_coverage:
      tiers: [2, 3]
      attack_types: [prompt_injection, social_engineering, policy_bypass]
    lineage:
      source_type: expert_written + synthetic
      transformations:
        - expert_curation (red_team)
        - synthetic_generation (adversarial_pack_generator_v2.0)
    dependencies:
      tool_schema_version: v1.5
      prompt_version: agent_system_prompt_v3.2
    access_control:
      read: [security-team, eval-eng]
      write: [security-team]
```

---

### 11.2 Changelog template

```markdown
# Dataset Changelog: [dataset_id] v[X.Y.Z]

**Version:** X.Y.Z
**Date:** YYYY-MM-DD
**Author:** name@company.com
**Approved by:** approver@company.com
**Version type:** major | minor | patch

---

## Summary
[1-2 sentence summary of what changed]

---

## Changes

### Added
- [List new examples, fields, slices added]
- Example: Added 50 adversarial cases for prompt injection (Tier 3)

### Modified
- [List examples or metadata modified]
- Example: Updated 12 ground truth labels in Spanish slice (cases 201-212)

### Removed
- [List examples or fields removed]
- Example: Removed 5 duplicate examples (cases 89, 134, 200, 305, 421)

### Schema changes
- [List metadata schema changes]
- Example: Added optional field "tenant_id" (string, default null)

---

## Rationale

**Why these changes:**
[Explanation of why changes were needed]

**Root cause (if fixing issues):**
[If this is a bug-fix version, what was the root cause?]

---

## Impact

### Metrics
[Expected impact on eval metrics]
- Example: +0.5% overall accuracy due to label fixes
- Example: -2% pass rate due to added adversarial cases (expected)

### Compatibility
[Backward compatibility notes]
- Breaking change: Yes | No
- Comparable to previous version: Yes | No | Partially

### Re-eval required?
[Do teams need to re-run evals?]
- Yes, for: [which use cases]
- No, unless: [conditions]

---

## Validation performed

- [ ] Spot-checked 50 random examples, all correct
- [ ] Difficulty distribution matches target mix
- [ ] Slice balance maintained (no slice below 5% or above 80%)
- [ ] No PII detected (privacy scan passed)
- [ ] Inter-annotator agreement above 0.80 (for labeled changes)
- [ ] Changelog reviewed and approved

---

## Migration notes (if applicable)

**For teams using v[previous]:**
[How to migrate, what to watch for]

**Breaking changes (if major version):**
[List breaking changes and required updates]

---

## Lineage (for this version)

**Source:**
[Where this version's changes came from]

**Transformations applied:**
[What processing steps were applied]

**Dependencies:**
- Prompt version: vX.Y
- Model version: model-name-YYYY-MM-DD
- Retriever version (RAG): vX.Y
- Tool schema version (Agent): vX.Y
```

---

### 11.3 Version bump checklist

```markdown
# Version Bump Checklist

**Dataset:** [dataset_id]
**Current version:** vX.Y.Z
**New version:** vX.Y.Z
**Version type:** major | minor | patch

---

## Pre-version steps

- [ ] All changes documented in draft changelog
- [ ] Source data identified and lineage recorded
- [ ] PII scan passed (if using prod data)
- [ ] Labels reviewed by domain expert (if labels changed)
- [ ] Metadata schema validated (if schema changed)

---

## Validation steps

- [ ] Size check: dataset size within acceptable range (not below min, not above max)
- [ ] Difficulty check: difficulty mix matches target distribution
- [ ] Slice check: all required slices represented, no slice below 5%
- [ ] Quality check: spot-check 50 examples, no errors found
- [ ] Dependency check: all dependencies (prompt, model, retriever, tools) documented

---

## Versioning steps

- [ ] Determine version number (major/minor/patch) per semantic versioning rules
- [ ] Copy dataset to new versioned artifact (e.g., dataset_vX.Y.Z.jsonl)
- [ ] Update registry entry: version, parent_version, created_date, changelog_url, artifact_url
- [ ] Finalize and publish changelog
- [ ] Lock artifact (make read-only)

---

## Post-version steps

- [ ] Notify dataset consumers (Slack, email, or announcement channel)
- [ ] Update CI configs to reference new version (if auto-updating)
- [ ] If replacing deprecated version: mark old version as deprecated in registry
- [ ] Archive previous version to long-term storage if >12 months old
- [ ] Add entry to dataset release log

---

## Approval

- [ ] Reviewed by: [name, date]
- [ ] Approved by: [name, date]
- [ ] Published: [date]
```

---

### 11.4 Lineage tracking template (per transformation)

```yaml
lineage:
  source:
    type: production_logs | expert_written | synthetic | hybrid
    version: [source identifier, e.g., prod_logs_2026-01-01_to_2026-01-07]
    count: [number of raw examples]
    date_collected: YYYY-MM-DD

  transformations:
    - step: sanitization
      tool: pii_redactor_v3.1
      config:
        redact_pii: true
        redact_emails: true
        redact_phone: true
      date: YYYY-MM-DD
      input_count: 12000
      output_count: 11800
      removed_count: 200
      removed_reason: PII detected

    - step: labeling
      rubric_version: v2.1
      annotator_pool: internal_qa_team
      annotation_tool: labelbox_enterprise
      date_start: YYYY-MM-DD
      date_end: YYYY-MM-DD
      input_count: 11800
      output_count: 11800
      inter_annotator_agreement: 0.85

    - step: QA_review
      reviewer: alice@company.com
      date: YYYY-MM-DD
      corrections_made: 47
      correction_types:
        - label_error: 30
        - typo: 10
        - duplicate: 7

    - step: balancing
      method: stratified_sample
      target_mix:
        easy: 10%
        normal: 60%
        hard: 20%
        adversarial: 10%
      slice_stratification: [tier, language]
      date: YYYY-MM-DD
      input_count: 11800
      output_count: 1000
      sampling_seed: 42

  dependencies:
    prompt_version: v4.1
    model_version: gpt-4-2024-11-20
    retriever_version: v2.3
    corpus_version: wiki_2026-01
    tool_schema_version: v1.5

  audit_trail:
    created_by: alice@company.com
    approved_by: bob@company.com
    approval_date: YYYY-MM-DD
    compliance_reviewed: yes | no
    compliance_reviewer: compliance@company.com
```

---

## 12) Interview-ready talking points

> "I version every eval dataset using semantic versioning — major for breaking changes, minor for expansions, patch for fixes."

> "Once a dataset version is published, it's immutable. To make changes, I create a new version with a changelog."

> "I maintain a dataset registry with lineage, ownership, status, and dependencies for every dataset we use."

> "Every dataset has tracked lineage from raw source through sanitization, labeling, QA, to final release."

> "I track dependencies: which prompt version, model version, retriever version, and tool versions this dataset was designed for."

> "I can reproduce any eval from the last 24 months because datasets and configs are versioned and stored immutably."

> "If a dataset is deprecated, I mark it in the registry, notify consumers, provide a successor version, and archive the old version after a grace period."

> "I treat dataset health as an SLI: I monitor size, difficulty mix, slice balance, and annotation quality per version."
