# Chapter 5.8 — Versioning, Lineage & Dataset Registry

**What we're doing here:**
Six months after launch, your VP asks: "Why did our Q2 metrics drop compared to Q1?"

You pull up the eval results. They used different datasets. One had 300 examples, the other 500. The labels changed. The rubric changed. The model changed. You can't answer the question because you don't know which dataset produced which result.

That's the problem versioning solves. One rule is non-negotiable: **"Which dataset produced this result?" must always be answerable.**

---

## 1) Why versioning is non-negotiable

Most teams start with a folder of examples. Over time, someone adds 20 new cases. Someone fixes a label. Someone updates the rubric from 3-point to 5-point. Someone removes a deprecated task type.

Each change is reasonable. But if you don't version them, you get:

- **False regressions** — scores drop because the dataset got harder, not because the model got worse
- **False improvements** — scores rise because bad examples were removed, not because the model got better
- **Unreproducible results** — "I can't replicate last month's eval" becomes a weekly complaint
- **Broken comparisons** — Q1 vs Q2 metrics are meaningless because the datasets are different

Think of it like this: if someone secretly swapped your bathroom scale between weigh-ins, your weight chart would be useless. Same thing with eval datasets. If anything changes between runs, the comparison means nothing.

**2026 compliance reality:**
Regulated industries (finance, healthcare, government) now require provenance tracking, reproducibility, and change justification. The EU AI Act requires evaluation data to be auditable. Without versioning, you cannot pass these requirements. This is not a nice-to-have — it's an operational and legal necessity.

---

## 2) What to version

Versioning is not just "lock the dataset file." You need to version **every input that affects eval results.**

**Dataset content** — examples added, removed, or modified. Split changes (moving examples between dev/test). Slice rebalancing (difficulty mix shifts).

**Labels and ground truth** — reference answers corrected. Rubric version changed (3-point to 5-point). Quality tier reassignments. Annotator pool changes.

**Metadata schema** — new fields added (like difficulty_label or tenant_id). Field definitions changed. Enum values changed. Required vs optional fields changed.

**Associated configs** — the prompt template version, model version, retriever version (for RAG), and tool schemas (for agents). Why? Because dataset validity depends on system config. A dataset designed for one model may not be appropriate for another. A RAG dataset validated against corpus v1.2 may break on corpus v2.0.

If any of these change and you don't track it, you lose the ability to compare results across time.

---

## 3) The dataset registry

The **dataset registry** is your central catalog — one place that tracks every eval dataset in your organization.

It answers four questions:
- What datasets exist?
- Which one should I use for this eval?
- What's the lineage of this dataset?
- Who owns it, when was it last updated, and is it still active?

**Minimum fields every entry needs:**

| Field | Example |
|---|---|
| dataset_id | rag_general_v2.3 |
| version | 2.3.1 |
| purpose | General RAG quality, Tier 1–2 |
| channel | RAG |
| split | test |
| owner | ai-quality-team@ |
| created_date | 2026-01-15 |
| status | active / deprecated / archived |
| parent_version | 2.3.0 |
| changelog_url | link to changelog |
| artifact_url | storage location |

**Recommended extras:** size (example count), difficulty mix, slice coverage, last validated date, deprecated reason, successor version.

**Access control:** Track who can read (most teams), write (dataset maintainers only), approve new versions (eval eng + domain experts), and deprecate (eval lead only). This prevents unauthorized changes that break reproducibility.

---

## 4) Semantic versioning for datasets

Borrow the same convention software uses: **major.minor.patch**.

**Major bump (1.0 → 2.0):** Breaking changes. Incompatible schema, different rubric, task taxonomy changed. This dataset is no longer comparable to the previous major version.

**Minor bump (2.1 → 2.2):** Backward-compatible additions. New examples added, new slices added, non-breaking metadata fields. Coverage expanded but existing cases unchanged.

**Patch bump (2.2.0 → 2.2.1):** Bug fixes. Typos corrected, labels fixed, duplicates removed. Small corrections that don't affect comparability.

**The immutability rule:** Once a dataset version is published and used in an eval, it is **immutable**. You cannot modify it in place. To fix a label, create a new patch version. Keep old versions accessible for at least 12–24 months. Mark deprecated versions clearly in the registry.

Why immutability matters: if v2.3 changes after an eval run, you can't reproduce that run. Teams stop trusting results if datasets silently mutate. And auditors require immutable records.

**Every version gets a changelog.** At minimum: what changed, why, who made the change, who approved it, and expected metric impact. Store changelogs in version control or your docs system, and link them from the registry.

---

## 5) Lineage tracking

Lineage answers: "Where did this dataset come from, and what happened to it along the way?"

A production-grade dataset goes through stages:

1. **Raw source** — prod logs, expert-written drafts, synthetic generation
2. **Sanitized** — PII removed (Chapter 5.5)
3. **Labeled** — ground truth assigned, rubrics applied
4. **QA'd** — spot-checked, agreement validated (Chapter 5.7)
5. **Released** — versioned, registered, locked

For each stage, track: what was the input, what transformation was applied, what was the output, and who performed it.

**Dependencies matter too.** Your dataset depends on external things — the prompt version, model version, retriever version, corpus version, tool schemas. Track these so you know when to re-validate. If the retriever gets upgraded, any RAG dataset validated against the old retriever may need a new version.

Think of it like a recipe. The dataset is the finished dish. Lineage tells you: what ingredients were used, what steps were followed, who cooked it, and what equipment they used. If the dish tastes different, you can trace back to figure out what changed.

---

## 6) Reproducibility

**The test:** Given a dataset version plus a config version, you must be able to reproduce the exact eval run that produced historical results.

This requires:
- **Immutable snapshots** — v2.3 is always v2.3
- **Artifact storage** — datasets stored in long-term accessible storage
- **Config snapshots** — the exact model, prompt, tool versions used
- **Deterministic harness** — same inputs produce same outputs (or within known variance bounds for stochastic models)

If you can't reproduce an eval from 6 months ago, you can't prove your quality claims, you can't debug regressions, and you can't pass compliance audits.

---

## 7) Knobs & defaults

**Versioning scheme:** Default is semantic versioning (major.minor.patch). Alternative is date-based (2026-01-20) for rapidly-evolving datasets that refresh weekly. Choose semantic if you compare across versions frequently. Choose date-based if simplicity matters more.

**Registry implementation:** Lightweight option (startup): a YAML or JSON file in Git. Works for fewer than 20 datasets. Production option (enterprise): a database with API layer and UI. Works for hundreds or thousands.

**Retention:** Active datasets — indefinite. Deprecated — 24 months. Archived — compliance-driven (typically 5–7 years for regulated industries). Never delete a version that was used in a production release gate.

---

## 8) Failure modes

**"We can't reproduce last quarter's eval."**
Datasets were mutated in place. No immutability enforcement. Config versions not tracked. Fix: implement immutable versioning now, store all versions in append-only storage, snapshot configs alongside datasets.

**"Dataset changed but nobody noticed."**
No changelog requirement. No approval gate. Anyone could modify datasets. Fix: require changelogs and approval for every version bump. Add dataset health checks to CI — flag if size drops more than 10% or difficulty mix shifts more than 15%.

**"Old deprecated dataset still in production evals."**
No deprecation enforcement. No migration path. Fix: mark deprecated datasets in the registry with successor version. Add warnings to the eval harness. After 6 months, require explicit override to use deprecated datasets.

**"We lost the lineage — nobody knows where this data came from."**
Lineage was never tracked. Fix: make lineage a required field in the registry. Backfill for existing datasets by checking Git history and interviewing dataset creators. Going forward, no dataset publishes without lineage.

---

## 9) Enterprise expectations

- They version every dataset and config, no exceptions
- They maintain a central registry with lineage, ownership, and status
- They enforce immutability — published versions cannot be modified
- They require changelogs and approval for every version bump
- They track dependencies (dataset → prompt → model → retriever)
- They test reproducibility quarterly — pick a random old eval, re-run it, verify results match
- They archive old versions but keep them accessible for compliance
- They monitor dataset health (size, difficulty mix, slice balance) as service-level indicators

---

## 10) Template: dataset registry entry

```
dataset_id: rag_general
version: 2.3.1
purpose: General RAG quality eval, Tier 1-2
channel: RAG
split: test
owner: ai-quality-team@
created_date: 2026-01-20
status: active
parent_version: 2.3.0
size: 1000
difficulty_mix:
  easy: 10%
  normal: 60%
  hard: 20%
  adversarial: 10%
lineage:
  source: prod_logs_2026-01 + expert_pack_v1.2
  sanitization: pii_redactor_v3.1
  labeling: rubric_v2.1, internal_qa_team
  balancing: stratified_sample
dependencies:
  prompt: rag_prompt_v4.1
  model: claude-opus-4-5-20251101
  retriever: v2.3
  corpus: wiki_2026-01 + docs_v5.2
access:
  read: [eval-team, product-team, compliance]
  write: [eval-team]
  deprecate: [eval-lead]
```

---

## 11) Interview Q&A

**Q: How do you version evaluation datasets?**
A: Like code — semantic versioning with major, minor, and patch bumps. Every published version is immutable. Every bump gets a changelog documenting what changed, why, who approved it, and expected metric impact. I store versions in append-only storage so nothing gets silently mutated.

**Q: How do you ensure reproducibility of past evals?**
A: I maintain a central dataset registry where every eval set has a version, lineage, owner, and dependency list. Given a dataset version plus config version, I can reproduce any eval from the last 24 months. I test this quarterly by picking a random old eval and re-running it.

**Q: What happens when a dataset needs to be retired?**
A: Deprecated datasets get marked in the registry with a successor version. Consumers get notified. The eval harness shows warnings if someone tries to use a deprecated set. After 6 months, old versions require explicit override. Nothing gets silently removed.

**Q: How do you track where a dataset came from?**
A: Every dataset has lineage metadata — source type, transformations applied, who performed each step, and what external dependencies it was validated against. If the retriever gets upgraded, I know which RAG datasets need re-validation because the dependency graph tells me.

---

*Next up: Chapter 6 begins Labeling & Calibration — how to build the annotation infrastructure that makes all this dataset work reliable.*
