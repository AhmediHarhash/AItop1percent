# Chapter 10.3 — Speech Recognition (ASR) Evaluation

**What we're doing here:**
You launch a voice assistant for customer support. Users love it. Then one Monday morning, your Slack fills with escalations. The system is canceling accounts when users say "I want to check my balance." It's hearing "cancel" instead of "check." Your ASR is making mistakes, and every mistake cascades — wrong transcription means wrong intent detection means wrong action means angry users.

If the system doesn't hear correctly, nothing else matters. Perfect reasoning on misheard input is still failure.

Speech recognition is the **foundation** of every voice AI pipeline. It's where audio becomes text. It's where accents, background noise, and bad microphones meet your model. It's where mistakes get locked in before your LLM ever sees the input.

Evaluating ASR isn't optional. It's the first gate everything else depends on.

---

## 1) Why ASR quality is foundational

In text-based AI, the input is clean. The user types "refund my order" and that's exactly what your model receives.

In voice AI, the input is **interpreted**. The user says "refund my order" but your system might hear "refuse my order" or "review my order" or "return my order."

Every downstream component — intent detection, retrieval, reasoning, response generation — operates on transcribed text. If the transcription is wrong, everything built on it is wrong.

**ASR errors cascade:**
- Misheard word leads to wrong intent
- Wrong intent leads to wrong tool call
- Wrong tool call leads to wrong action
- Wrong action leads to user harm

You can have the best LLM, the best prompt engineering, the best safety filters. If your ASR hears "delete my account" when the user said "update my account," all of that sophistication produces harm.

**ASR is a gate, not a metric to average.** One critical misheard word in a medical or financial context can cause real damage. You don't measure ASR quality by averaging across all utterances. You measure it by finding the errors that matter.

---

## 2) Word Error Rate — the standard metric

**Word Error Rate (WER)** is the fundamental ASR accuracy metric. It measures how many words were transcribed incorrectly.

The formula counts three error types:
- **Substitutions:** Wrong word appears (user said "cancel," ASR heard "counsel")
- **Deletions:** Word is missing (user said "cancel my order," ASR heard "cancel order")
- **Insertions:** Extra word appears (user said "cancel order," ASR heard "cancel the order")

WER equals the total errors divided by the total words spoken:

WER = (Substitutions + Deletions + Insertions) / Total Words Spoken

A WER of 5% means 5 out of every 100 words are wrong. That sounds good until you realize that's **one mistake every 20 words** — in a 200-word conversation, that's 10 errors.

**What's acceptable?** Enterprise benchmarks in 2026:
- WER below 5% is production-ready for most use cases
- WER below 3% is excellent for general conversation
- WER below 1% is required for safety-critical domains (medical, legal, financial)

But WER alone doesn't tell you what you need to know. A 5% WER where all errors are filler words ("um," "uh") is fine. A 5% WER where errors are critical nouns ("buy" vs "sell") is catastrophic.

**Track WER but slice it.** Measure WER separately for:
- Domain-critical keywords (product names, action verbs, numbers)
- High-risk utterances (financial transactions, account changes, cancellations)
- Demographic slices (accents, age groups, gender)
- Acoustic conditions (quiet office, noisy street, poor phone connection)

WER gives you a number. Slicing WER tells you where the real risk lives.

---

## 3) Character Error Rate — for languages without clear word boundaries

Some languages don't separate words with spaces. Chinese, Japanese, Thai — the concept of a "word" is ambiguous. Counting word errors doesn't make sense.

**Character Error Rate (CER)** measures errors at the character level instead. Same formula as WER but applied to characters:

CER = (Substitutions + Deletions + Insertions) / Total Characters Spoken

For English, WER is standard. For Asian languages, CER is often more meaningful.

**When to use CER:**
- Languages without word delimiters (Chinese, Japanese, Thai)
- Character-level quality matters (spelling-sensitive tasks, proper name accuracy)
- Comparing ASR systems across multiple languages (CER is more comparable cross-linguistically than WER)

For multilingual products, track both. WER for word-delimited languages, CER for character-based languages. Don't average them together — they measure different things.

---

## 4) Semantic accuracy vs verbatim accuracy

WER measures **verbatim accuracy** — did the system transcribe exactly what was said?

But users care about **semantic accuracy** — did the system understand the intent?

"I wanna cancel" and "I want to cancel" have different WER (one substitution) but identical meaning. Penalizing the first transcription is technically correct but practically irrelevant.

Conversely, "I want to cancel" vs "I want to proceed" has a WER of just one word but completely opposite meanings. That single error is catastrophic.

**Elite ASR evaluation measures both:**
- **Verbatim accuracy (WER/CER):** How closely does the transcription match the spoken words?
- **Semantic accuracy:** Did the system capture the correct intent?

Semantic accuracy is harder to measure. You can't compute it from edit distance. You need:
- Human annotators judging whether the transcription preserves meaning
- Intent classification agreement (does the transcribed text trigger the correct intent?)
- Downstream task success (did the system take the right action?)

For production voice AI, **semantic accuracy matters more than verbatim accuracy.** But you still track WER because it's objective, reproducible, and catches regressions.

The goal is high WER **and** high semantic accuracy. Low WER with low semantic accuracy means your ASR is precise but not useful. High semantic accuracy with high WER means you're getting lucky — fragile and will break on edge cases.

---

## 5) Accent and dialect robustness

ASR systems are trained on data. If that data is mostly American English speakers, the system will struggle with British accents, Indian accents, Southern US dialects.

This isn't a technical problem. It's a **fairness problem**. Your system shouldn't work better for some users than others based on how they speak.

**Accent bias manifests as:**
- Higher WER for non-dominant accents
- Misrecognition of common words spoken with regional pronunciation
- Complete failure on code-switched utterances (mixing languages mid-sentence)

Testing for accent robustness requires **stratified test sets:**
- Collect audio samples across demographic groups (regional accents, age groups, native vs non-native speakers)
- Measure WER separately for each group
- Flag disparities (if WER is 3% for Group A but 12% for Group B, you have a bias problem)

**2026 ASR systems are better at this than ever** — models trained on diverse multi-accent datasets (Whisper v3, Deepgram Nova-2, AssemblyAI Universal-2) handle accents more robustly. But you still need to validate on **your user population**.

Synthetic accent testing helps. Use voice cloning or accent transfer tools to generate test utterances in different accents. But validate with real speakers — synthetic accents miss subtle phonetic patterns.

**Enterprise expectation:** WER variance across accent groups should be within 2 percentage points. If one demographic experiences 10% WER while another experiences 3%, you're not production-ready for that demographic.

---

## 6) Noise robustness — testing in real conditions

Your ASR works great in a quiet office. Then users call from a busy street, a car, a coffee shop, or a construction site. Suddenly WER triples.

Background noise is the **most common real-world ASR degrader:**
- Traffic noise
- Overlapping conversations
- Music
- Wind on mobile microphones
- Echo and reverb in large rooms

Testing ASR in studio conditions tells you nothing about production performance.

**Noise robustness testing requires augmentation:**
- Inject realistic noise into clean test audio (use public noise datasets — MUSAN, AudioSet)
- Test at varying Signal-to-Noise Ratios (SNR): clean (30+ dB SNR), moderate noise (15-20 dB), heavy noise (5-10 dB)
- Track WER degradation as noise increases

Elite systems maintain below 10% WER even at 10 dB SNR. Mediocre systems collapse below 15 dB.

**Microphone quality matters too.** Studio-quality audio vs smartphone vs Bluetooth headset vs speakerphone — each introduces different degradation. Test across device types if your users aren't on controlled hardware.

**Crosstalk (overlapping speakers) is a special case.** If your system handles multi-party calls or meetings, it needs to separate overlapping speech. That's not just ASR — it's **source separation + ASR**. Test with deliberately overlapping test cases.

---

## 7) Domain-specific vocabulary

General-purpose ASR is trained on common language. It handles everyday conversation well. But specialized domains break it:
- Medical terminology ("metformin" becomes "met foreman")
- Technical jargon ("Kubernetes" becomes "communities")
- Product names ("Salesforce" becomes "sales force")
- Proper names ("Nguyen" becomes "win")

Generic ASR models don't know your domain. They transcribe what sounds plausible in general English, not what's correct in your context.

**Custom vocabulary testing:**
- Build a test set of domain-critical terms (product names, medical terms, technical jargon, common proper names)
- Measure **keyword accuracy** — what percentage of domain-specific terms are transcribed correctly?
- Track separately from overall WER (a system can have great WER on general speech but fail on domain terms)

Many ASR providers support **custom vocabularies** or **domain adaptation**:
- Whisper: Fine-tune on domain audio
- Deepgram: Upload custom word lists with phonetic hints
- AssemblyAI: Domain-specific models (medical, finance)

But custom vocabularies introduce risk — boosting rare words can cause false positives. If you add "Kubernetes" to your vocabulary, the system might hear it even when the user said something else.

**Test both directions:**
- **Recall:** Do domain terms get recognized when actually spoken?
- **Precision:** Does the system avoid false positives (hearing domain terms that weren't said)?

Track keyword-level metrics separately from WER. A 5% overall WER is meaningless if critical product names are transcribed wrong 30% of the time.

---

## 8) Real-time ASR vs batch processing

Most ASR benchmarks assume **batch processing** — you have the full audio file and transcribe it in one pass.

Production voice AI uses **real-time streaming ASR** — transcription happens as the user speaks, with partial results updated continuously.

Real-time ASR is harder:
- Decisions are made with incomplete context
- No future audio to disambiguate
- Latency constraints force tradeoffs (accuracy vs speed)

**Real-time ASR produces two types of output:**
- **Partial transcripts:** Intermediate results while the user is still speaking (may change)
- **Final transcripts:** Locked-in results after a silence or end-of-turn

Evaluation needs to test both. Partial transcripts affect real-time understanding (interruption detection, live captioning). Final transcripts affect downstream reasoning.

**Partial transcript stability matters.** If partials change wildly as more audio arrives, downstream systems can't trust them. Measure:
- **Edit distance between partials and final:** How much do partial results change?
- **Stability threshold:** How many words of context are needed before transcripts stop changing?

High partial stability means you can act on streaming transcripts confidently. Low stability means you need to wait for final transcripts, adding latency.

**Latency vs accuracy tradeoff.** Faster ASR models make decisions sooner (lower latency) but with less context (lower accuracy). Measure WER separately for:
- Real-time streaming transcripts (what users experience)
- Batch re-transcription of the same audio (upper-bound accuracy)

Gap between the two tells you how much accuracy you're sacrificing for speed.

---

## 9) Endpointing evaluation — detecting when the speaker finished

**Endpointing** (also called Voice Activity Detection or turn-taking detection) determines when the user has stopped speaking.

Get it wrong and:
- **Cut off too early:** User is mid-sentence, system interrupts, loses words
- **Wait too long:** Awkward silence, system feels slow, user experience degrades

Endpointing is part of the ASR pipeline but evaluated separately.

**Metrics:**
- **Premature cutoff rate:** How often does the system interrupt mid-sentence?
- **Excessive latency rate:** How often does the system wait more than 1 second after the user finishes?
- **Missed turn rate:** How often does the system fail to detect the end of an utterance?

Elite systems endpoint within 500-700ms of silence after speech. Waiting 2+ seconds feels broken. Interrupting mid-word is worse.

**Endpointing is context-dependent.** Long pauses mid-sentence (thinking pauses) shouldn't trigger endpoint. Short pauses at sentence boundaries should. Testing requires realistic conversational audio with natural pauses.

**Language and speaking style matter.** Some languages and cultures use longer pauses. Some speakers trail off. Some speak in bursts. Endpointing thresholds tuned for American English might be wrong for Japanese or Spanish.

Measure endpointing accuracy across:
- Speaking styles (fast, slow, hesitant)
- Languages and accents
- Turn types (short confirmations vs long explanations)

---

## 10) Speaker diarization — who is speaking

In multi-speaker scenarios (meetings, call center calls with agent + customer, conference transcription), you need to know **who said what**.

**Speaker diarization** labels each segment of transcribed audio with a speaker ID.

Evaluation measures:
- **Diarization Error Rate (DER):** Percentage of time attributed to the wrong speaker
- **Speaker confusion rate:** How often are two speakers mixed up?
- **Missed speech:** How much speech is not attributed to any speaker?

DER is the standard metric, analogous to WER but for speaker identity.

**Why it matters for voice AI:**
- Call center QA (which statements came from the agent vs customer?)
- Meeting AI (attributing action items and decisions to the right people)
- Multi-party voice assistants (tracking who made each request)

DER below 10% is production-ready for most use cases. DER above 20% means diarization is unreliable — better to not use it than to use it wrong.

**Speaker diarization is hard when:**
- Speakers have similar voices
- Speakers overlap (talking at the same time)
- Background noise obscures voice characteristics
- Short utterances (not enough audio to identify the speaker)

Test on realistic multi-speaker audio, not clean studio recordings. Real calls have interruptions, crosstalk, and varying audio quality.

---

## 11) 2026 ASR landscape and patterns

**Leading ASR systems in 2026:**

**Whisper v3 (OpenAI):** Open-weights model trained on 5 million hours of multilingual audio. Excellent for batch transcription, decent for streaming. Strengths: multilingual coverage, accent robustness. Weaknesses: latency (not optimized for real-time), hallucinations on silence or music.

**Deepgram Nova-2:** Commercial streaming ASR optimized for real-time. Strengths: low latency (sub-300ms), good noise robustness, custom vocabulary support. Weaknesses: cost at scale, less multilingual coverage than Whisper.

**AssemblyAI Universal-2:** Commercial model with strong domain adaptation (medical, finance, legal). Strengths: high accuracy on technical vocabulary, good speaker diarization. Weaknesses: higher latency than Deepgram, premium pricing.

**Google Cloud STT / Azure Speech / AWS Transcribe:** Managed services with multilingual support and custom model training. Strengths: enterprise integration, compliance, scaling. Weaknesses: vendor lock-in, less control over model behavior.

**On-device ASR (Apple Siri, Google Assistant):** Privacy-first models running locally. Strengths: zero-latency (no network round-trip), privacy. Weaknesses: limited model size, lower accuracy than cloud ASR.

**2026 ASR trends:**
- **Streaming-first architectures:** Real-time ASR is table stakes, not a specialty
- **Multilingual models:** Single model handling 50+ languages with code-switching support
- **Distillation for latency:** Smaller models distilled from frontier ASR for sub-200ms response
- **Noise robustness as default:** Models trained on augmented noisy data, not clean studio audio
- **Emotion and prosody detection:** ASR outputs not just words but tone, sentiment, emphasis

**Code-switching handling** is a major 2026 improvement. Users who speak multiple languages naturally mix them mid-sentence. Legacy ASR forced language selection upfront. Modern ASR detects language switches automatically.

Test code-switching explicitly if your users are multilingual. Measure WER on utterances that mix languages (e.g., "I want to hacer un pago" — English + Spanish).

---

## 12) Failure modes to watch for

**Hallucinations on silence.** Some ASR models (especially Whisper) generate plausible-sounding text when there's no speech — just noise or silence. This is catastrophic for voice AI. Test explicitly: feed silence or music into your ASR and verify it outputs empty transcripts, not hallucinated words.

**Semantic drift in long audio.** ASR models can lose context over long sessions. Test with 10+ minute audio samples and measure whether WER degrades over time.

**Confidence calibration.** Many ASR systems output confidence scores. Are they meaningful? Test: do low-confidence transcripts actually have higher WER? If confidence doesn't correlate with accuracy, ignore it.

**Homophone confusion.** "To," "too," "two" — ASR gets these wrong because they sound identical. In voice AI, this causes intent misclassification. Track homophone errors separately if they affect downstream behavior.

**Proper name failures.** Rare names, non-English names, and newly created product names have high error rates. Test with a proper name dataset (user names, company names, location names).

**Gender and age bias.** Some ASR systems perform worse on women's voices or children's voices. Measure WER separately by demographic group and flag disparities.

---

## 13) Enterprise expectations

Enterprises shipping production voice AI expect:

- **Demographic WER parity:** No user group experiences more than 2 percentage points higher WER than others
- **Domain vocabulary testing:** Critical keywords (product names, commands, industry terms) tested separately with above 95% accuracy
- **Noise robustness validation:** WER measured at multiple SNR levels (clean, moderate noise, heavy noise)
- **Real-time stability:** Partial transcripts don't change more than 10% between interim and final results
- **Endpointing precision:** Less than 5% premature cutoffs, less than 1 second average latency to endpoint
- **Multilingual coverage:** If serving multiple languages, per-language WER tracked independently
- **Failure mode testing:** Explicit tests for silence hallucination, homophone errors, code-switching

ASR isn't tested once and forgotten. It's monitored continuously in production. WER regressions indicate data drift, microphone changes, or user population shifts.

---

## 14) Practical ASR evaluation workflow

**Step 1: Build a stratified test set**
- 500+ utterances minimum
- Balanced across: domain vocabulary, accents, noise conditions, utterance lengths
- Include known failure cases from production logs

**Step 2: Measure baseline WER**
- Overall WER across all test cases
- Per-slice WER (domain terms, accents, noise levels)
- Confidence score calibration

**Step 3: Test real-time performance**
- Partial transcript stability
- Endpointing accuracy
- Latency to first word, latency to final transcript

**Step 4: Stress test edge cases**
- Silence and music (verify no hallucinations)
- Overlapping speakers (if applicable)
- Long-form audio (check for drift)

**Step 5: Monitor in production**
- Track WER on labeled production samples
- Alert on WER regressions
- Collect user corrections (when users repeat or rephrase, that's a signal ASR failed)

ASR evaluation isn't a one-time gate. It's an ongoing quality signal.

---

## Template: ASR Evaluation Config

```yaml
asr_evaluation:
  test_set:
    total_samples: 500
    sources:
      - domain_vocabulary: 100  # Product names, technical terms
      - accent_diversity: 150   # Regional and non-native accents
      - noise_conditions: 100   # SNR 5-30 dB
      - edge_cases: 50          # Silence, music, crosstalk
      - general_conversation: 100

  metrics:
    primary:
      - word_error_rate  # Overall WER
      - keyword_accuracy # Domain-specific terms

    sliced:
      - wer_by_accent
      - wer_by_snr
      - wer_by_utterance_length

    real_time:
      - partial_stability  # Edit distance between partial and final
      - endpointing_accuracy
      - latency_to_first_word

    fairness:
      - wer_by_demographic_group
      - max_group_disparity  # Flag if difference exceeds 2pp

  thresholds:
    production_gate:
      - overall_wer: "below 5%"
      - keyword_accuracy: "above 95%"
      - max_demographic_disparity: "below 2pp"
      - hallucination_on_silence: "zero tolerance"

    monitoring_alerts:
      - wer_regression: "above 1pp increase"
      - endpointing_degradation: "above 10% premature cutoffs"
```

---

## Interview Q&A

**Q: How do you evaluate ASR quality for a production voice AI system?**

A: I start with Word Error Rate as the baseline metric, but I don't just measure overall WER. I slice it by critical dimensions — domain-specific keywords, accents, noise conditions, and demographic groups. I track keyword accuracy separately because a 5% overall WER is meaningless if product names are wrong 30% of the time. For real-time systems, I also measure partial transcript stability and endpointing accuracy. And I test failure modes explicitly — feeding silence to check for hallucinations, testing code-switching if users are multilingual, and measuring WER disparity across accents to catch bias.

**Q: What's the difference between verbatim accuracy and semantic accuracy in ASR?**

A: Verbatim accuracy is what WER measures — did the system transcribe exactly what was said, word for word. Semantic accuracy asks whether the system captured the correct meaning. "I wanna cancel" vs "I want to cancel" has a WER penalty but identical meaning. Conversely, "I want to cancel" vs "I want to proceed" differs by one word but has opposite intent. For production voice AI, semantic accuracy matters more. But I still track WER because it's objective and catches regressions. The goal is high WER **and** high semantic accuracy — precise transcription that preserves intent.

**Q: How do you handle domain-specific vocabulary in ASR evaluation?**

A: I build a separate test set of domain-critical terms — product names, technical jargon, medical terminology, proper names. Then I measure keyword accuracy on those terms independently. Many ASR providers support custom vocabularies or fine-tuning, but that introduces precision-recall tradeoffs. Boosting rare words can cause false positives. So I test both: recall (are domain terms recognized when spoken?) and precision (does the system avoid hearing them when they weren't said?). I track keyword-level metrics separately from overall WER because general WER doesn't tell you if your critical terms are failing.

**Q: Why is noise robustness testing important for ASR?**

A: ASR tested in quiet studio conditions tells you nothing about production performance. Real users call from cars, streets, coffee shops, construction sites. Background noise is the most common real-world ASR degrader. I test by injecting realistic noise at varying Signal-to-Noise Ratios — clean audio (30+ dB SNR), moderate noise (15-20 dB), heavy noise (5-10 dB). Elite systems maintain below 10% WER even at 10 dB SNR. If your system collapses below 15 dB, it's not ready for real-world deployment. I also test across microphone types — smartphone, Bluetooth, speakerphone — because each introduces different degradation.

**Q: What are the most common failure modes in ASR systems?**

A: First, hallucinations on silence — some models generate plausible text when there's no speech, just noise. That's catastrophic. Second, domain vocabulary failures — generic ASR transcribes technical terms phonetically wrong. Third, accent bias — higher WER for non-dominant accents because training data was skewed. Fourth, homophone confusion — "to," "too," "two" sound identical but have different meanings. Fifth, poor endpointing — cutting off mid-sentence or waiting too long after the user finishes. And sixth, code-switching failures — users who mix languages mid-sentence break single-language ASR. I test all of these explicitly because they don't show up in overall WER averages.

---

**Next:** Chapter 10.4 covers Natural Language Understanding (NLU) evaluation for voice AI — how to measure intent detection, entity extraction, and contextual understanding accuracy when working with spoken language and real-time constraints.
