# Chapter 5 — Dataset Construction

**What we're doing in Chapter 5:**
We're building the datasets used to evaluate (and sometimes improve) AI systems.

This is not "collect some examples." In enterprise settings, dataset construction is a discipline:
- you choose the right sources,
- remove sensitive or leaking data,
- balance difficulty and slices,
- prevent contamination,
- and version everything so results stay meaningful.

**Core idea:**
Your dataset is your measurement instrument.
If the instrument is biased or noisy, every metric downstream becomes unreliable.

---

## Chapter 5.1 — Data Sources (Prod Logs, Synthetic, Expert-Written)

### Mechanics (how it works)
A strong evaluation program uses multiple sources because each source has strengths and weaknesses.

The 3 primary sources are:

1. **Production logs** (real user interactions)
2. **Expert-written cases** (crafted by domain experts)
3. **Synthetic cases** (generated to cover gaps and edge cases)

In 2026, the best enterprise datasets are **hybrid**:
- production-based for realism,
- expert-written for correctness and policy alignment,
- synthetic for coverage and adversarial testing.

---

## 1) Source type A — Production logs (real traffic)

### What it's best for
- Real phrasing and real user behavior
- True long-tail requests
- Real failure patterns (complaints, escalations, retries)
- Actual slice distribution (languages, devices, tiers, tenants)

### What usually goes wrong
- Contains PII and sensitive data
- Contains user confusion, sarcasm, incomplete context
- Biased toward current product usage (may miss new features)
- Can include "contaminated" text that overlaps with prompts or training data

### Enterprise default rules
- Always sample from prod logs for **realism**
- But treat them as **high-risk data** requiring strict privacy handling (covered more in 5.5)

---

## 2) Source type B — Expert-written cases (gold-quality inputs)

### What it's best for
- High-stakes workflows (Tier 2–3)
- Compliance and policy accuracy
- Clear "must include" requirements
- Edge cases that are rare but dangerous

### What usually goes wrong
- Experts write in a "clean" style unlike real users
- Too few examples because experts are expensive
- Experts accidentally encode their own preferences

### Enterprise default rules
- Use expert-written cases for:
  - critical workflows
  - refusal and safety boundaries
  - "must abstain" RAG cases
  - tool safety guardrails for agents
- Mix expert-written with real user phrasing (use paraphrases carefully)

---

## 3) Source type C — Synthetic cases (coverage and stress testing)

### What it's best for
- Filling gaps in long-tail coverage
- Creating adversarial and jailbreak variants
- Generating multilingual variants
- Simulating tool failures for agents
- Simulating noisy/interrupt conditions for voice (paired with audio sets where possible)

### What usually goes wrong
- Synthetic text can be unnatural or repetitive
- It can overfit to the generator's style
- It may not reflect real business constraints
- Synthetic datasets can accidentally become "too easy" or "too weird"

### Enterprise default rules (2026 reality)
- Synthetic is powerful, but must be:
  - reviewed (spot check),
  - diversified (multiple styles),
  - grounded in real failures,
  - and labeled with "synthetic" metadata so you don't confuse it with production distribution.

---

## 4) A modern enterprise dataset mix (recommended starting ratios)

These ratios are practical and used widely:

### For general assistants (Tier 0–1 heavy)
- 50–70% production
- 20–30% synthetic
- 10–20% expert-written

### For high-stakes assistants (Tier 2–3 heavy)
- 30–50% production (carefully sanitized)
- 20–30% synthetic (adversarial, coverage)
- 20–40% expert-written (policy, workflows, safety)

### For agents (tools + actions)
- production traces where possible (sanitized)
- expert-written workflow cases
- synthetic tool-failure packs

### For RAG
- production queries + real docs
- expert-written "must abstain" and "must cite" cases
- synthetic "retrieval trap" cases (similar wording, wrong doc nearby)

### For voice
- real call snippets where permitted and safe
- scripted scenarios for coverage
- synthetic variants for phrasing + noisy conditions

---

## 5) Knobs & defaults (what you actually set)

### 5.1 Source metadata (must track)
For every example, store:
- source_type = prod / expert / synthetic
- channel = chat / RAG / agent / voice
- risk_tier (0–3)
- intent/task_id (taxonomy)
- slice labels (language, region, tier, tenant)
- difficulty label (easy/normal/hard/adversarial)
- date_added + dataset_version

This is how you prevent silent dataset drift.

### 5.2 "Realism enforcement"
If you use a lot of expert-written cases, add:
- paraphrase variants that mimic real users
- incomplete/short queries
- messy multi-turn context

But never distort policy facts.

### 5.3 "Synthetic control"
For synthetic cases:
- cap near-duplicate patterns
- ensure multiple writing styles
- ensure it covers real failure modes, not fantasy edge cases

---

## 6) Failure modes (symptoms + root causes)

### 6.1 "Our evals are too optimistic"
Root causes:
- too much expert-written clean text
- not enough long-tail production
- not enough adversarial/safety packs

Fix:
- increase prod + long-tail packs
- add difficulty mix + adversarial packs (3.6)

### 6.2 "Our evals are too pessimistic"
Root causes:
- synthetic cases are unrealistically hard or unnatural
- dataset overweighted with edge cases

Fix:
- rebalance with realistic production distribution
- label synthetic difficulty carefully

### 6.3 "We can't use production logs because privacy"
Root causes:
- no anonymization pipeline
- unclear consent/retention rules

Fix:
- build a safe sanitization process (5.5) and use expert-written proxies until ready

---

## 7) Debug playbook: building a dataset the right way

1. Start from taxonomy + coverage map (Chapter 3)
2. Choose dataset purpose:
   - regression gate
   - broad quality tracking
   - safety red-team
3. Select sources:
   - production sample + expert pack + synthetic gap filler
4. Tag everything with metadata
5. Balance:
   - risk tiers
   - difficulty mix
   - slices
6. Add anchors:
   - gold items and incident cases
7. Version the dataset and lock it

---

## 8) Enterprise expectations (what serious teams do)

- They maintain multiple datasets, not one:
  - regression suite (stable, gated)
  - monitoring set (fresh, production-aligned)
  - safety suite (adversarial, strict)
  - tenant packs (top enterprise customers)
- They treat production logs as regulated data:
  - strict access control
  - anonymization
  - retention rules
- They track dataset drift and label drift with versioning

---
