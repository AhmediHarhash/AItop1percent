# Chapter 3.5 — Keeping Taxonomy Updated (Drift + New Intents)

**What we're doing here:**
Your task taxonomy and coverage map are not "set and forget."
In production, user behavior changes, product features change, and model behavior drifts.

This chapter shows how enterprise teams keep the taxonomy **alive**, so your eval program stays relevant and doesn't slowly become fake.

---

## 1) Mechanics: what changes over time (the drift problem)

There are 3 kinds of drift you must plan for:

### 1.1 Intent drift (users ask new things)
Examples:
- New user needs show up (new feature, new market, new season)
- Users start using different phrasing (trends, slang, new workflows)
- Different language mix increases

### 1.2 Data drift (your sources change)
Examples:
- Policy docs updated
- Knowledge base reorganized
- RAG corpus grows and chunking changes
- Tool/API responses change shape

### 1.3 Model drift (behavior changes without you noticing)
Examples:
- You update the model
- You change prompts/tools
- You adjust safety rules
- Vendor updates change output style

**Key point:**
A taxonomy that doesn't evolve becomes a "museum."
Your eval scores will look stable while reality changes.

---

## 2) The update loop: how enterprise teams keep it current

A simple, repeatable loop:

1. **Collect signals** (what changed in production?)
2. **Detect new intents / shifts**
3. **Update taxonomy**
4. **Update coverage map + datasets**
5. **Add regression tests**
6. **Review and sign off**
7. **Version everything**

This is basically "product management," but for evaluation.

---

## 3) Knobs & defaults (what you actually set)

### 3.1 Review cadence (default)
- **Weekly:** check drift signals + top complaints + new intents
- **Monthly:** taxonomy review + slice review + dataset refresh plan
- **Per release:** update regression suite for new features and incidents

### 3.2 Change thresholds (practical)
Trigger a taxonomy update if:
- a new intent shows up in top traffic
- a support complaint cluster repeats
- a new feature launches (always)
- safety/PII incident occurs (always)
- a top tenant reports a new workflow

### 3.3 Ownership model
- Each major taxonomy branch has an owner
- One "eval owner" (or small group) approves changes for consistency

### 3.4 Versioning defaults
- Taxonomy version: v1.0, v1.1, v1.2…
- Coverage map version aligned with taxonomy
- Datasets versioned and tagged to releases

---

## 4) Detecting new intents (simple, reliable methods)

### 4.1 The "top unknown bucket"
In your intent classifier (human or automated), keep a label:
- **UNKNOWN / OTHER**

Weekly, inspect:
- the top 100–500 UNKNOWN examples

Then either:
- map them to an existing intent, or
- create a new leaf task.

### 4.2 Complaint-driven discovery
Look at:
- escalations
- low CSAT conversations
- "agent fallback" events
- repeated user rephrases ("no, I mean…")

These are gold for finding missing intents and failure modes.

### 4.3 Product change triggers
Every new feature must declare:
- new intents created
- new risk tiers
- new slices (tenants, languages)
- new tests required

No feature ships without an eval plan.

---

## 5) Updating the taxonomy without breaking everything

### The goal
You want to improve the taxonomy while keeping comparisons meaningful.

### 5.1 Safe change rules
- Don't rename tasks casually (rename breaks trendlines)
- Prefer:
  - adding new leaf tasks, or
  - splitting a task into two (with a migration note)

### 5.2 Deprecation rules
If a task becomes obsolete:
- mark it as DEPRECATED
- keep it for trend history
- remove it from "active coverage requirements" after a set period

### 5.3 Migration notes (must exist)
When you split or change tasks:
- record what changed
- record why
- record how old data maps to new tasks (best-effort)

---

## 6) Failure modes (symptoms + root causes)

### 6.1 "We keep missing new problems"
Root causes:
- taxonomy reviews are rare or informal
- no unknown bucket review
- no incident → regression discipline

Fix:
- weekly drift review
- incident workflow forces updates

---

### 6.2 "We can't compare month to month"
Root causes:
- tasks renamed or restructured constantly
- datasets replaced without stable anchors

Fix:
- keep a stable regression suite
- version the taxonomy and store mapping notes
- maintain "core tasks" that almost never change

---

### 6.3 "The taxonomy exploded into 500 tasks"
Root causes:
- no standard for what a leaf task is
- people add hyper-specific tasks for every variation

Fix:
- keep leaf tasks at "testable + meaningful" level
- push variations into:
  - slices
  - difficulty tags
  - examples within the same leaf task

---

## 7) Debug playbook: the "taxonomy maintenance SOP"

Use this as your standard operating procedure.

### Weekly (30–60 minutes)
- Pull:
  - top UNKNOWN/OTHER items
  - top low-CSAT interactions
  - top escalations
  - safety/PII near-misses
- Decide:
  - map to existing tasks or create new leaf task
- Add:
  - 5–20 examples to eval sets for each new issue
- Update:
  - coverage map gaps
  - regression suite if it's critical

### Monthly (1–2 hours)
- Review:
  - top 20 tasks by volume
  - top 20 tasks by risk
  - worst 10 slices
- Confirm:
  - tasks still match real user behavior
  - dataset freshness
  - owners still correct

### Per release
- Review changes:
  - prompts/tools/models
  - policies and safety updates
- Run regression suite and compare slices
- Add tests for newly introduced workflows

---

## 8) Enterprise expectations (what serious teams do)

- Taxonomy is treated like a product artifact:
  - owned
  - versioned
  - reviewed
  - tied to releases
- Every production incident produces:
  - taxonomy update (if needed)
  - new eval cases
  - new regression tests
  - coverage map update
- They keep a permanent "core benchmark":
  - stable tasks
  - stable datasets
  - stable trendlines

---

## 9) Ready-to-use templates

### 9.1 Taxonomy change request (copy/paste)

**Change type:** Add / Split / Deprecate / Rename
**Requested by:**
**Date:**
**Reason:** (new feature / drift / incident / customer request)
**Old task(s):**
**New task(s):**
**Risk tier(s):**
**Channels affected:**
**Slices affected:**
**Eval updates required:** (datasets, regression tests, safety suite)
**Owner:**
**Approval:**

---

### 9.2 Drift review checklist
- Top UNKNOWN intents reviewed
- New feature intents added
- Incidents converted into regression tests
- New slices added (language/tenant/tier)
- Coverage map updated
- Datasets refreshed or expanded
- Taxonomy version bumped with notes

---
