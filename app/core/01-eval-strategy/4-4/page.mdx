# Chapter 4.4 — Gold Sets and Adjudication (Final Truth Decisions)

**What we're doing here:**
Even with great rubrics and source-of-truth rules, humans will still disagree sometimes.
So enterprise eval programs create:

- **Gold sets:** small, high-quality, "best possible" labeled examples
- **Adjudication:** a formal process for resolving disagreements and locking the final truth

These two things are what turn evaluation from "opinions" into a stable system.

**Enterprise goal:**
Make labels consistent, auditable, and reusable — so your metrics actually mean something over time.

---

## 1) Mechanics: what a Gold Set is (and why it's special)

A **Gold Set** is not "a big dataset."
It is a carefully curated set of examples where the correct outcome is locked with high confidence.

Gold sets are used for:
- rater calibration
- regression testing ("never break")
- benchmarking model versions
- dispute resolution (what does "good" mean here?)
- validating automated judges (LLM-as-judge)

Gold sets should include:
- typical cases (normal)
- hard cases (edge)
- high-risk cases (Tier 2–3)
- "must abstain" and "must refuse" cases
- known historical incident cases

---

## 2) Gold set vs regular eval set (simple difference)

### Regular eval set
- larger
- refreshed often
- more noise
- used for broad tracking

### Gold set
- smaller
- updated carefully
- extremely consistent
- used as your "truth anchor" across time

**Enterprise truth:**
If you don't have a gold set, your evaluation will drift with every new rater and every new month.

---

## 3) Knobs & defaults (what you actually set)

### 3.1 Gold set size (practical)
- Small product: **200–500** gold examples
- Mid product: **500–2,000**
- Large enterprise: **2,000–10,000** (across tasks and slices)

### 3.2 Composition rules (good defaults)
Include:
- **40–60% head intents**
- **20–40% long-tail**
- **10–20% safety/adversarial**

And ensure Tier 2–3 tasks are heavily represented.

### 3.3 Update cadence
- Gold set changes should be controlled:
  - monthly or per major policy/product change
  - never "daily churn"

### 3.4 Label strictness
Gold labels must be:
- consistent across raters
- tied to sources/policies
- documented with rationales

---

## 4) What "adjudication" means (enterprise version)

Adjudication is a structured process to decide:
- what the final label is when raters disagree
- what ground truth rules need to be clarified
- how to update rubrics and anchors

It is not:
- one person guessing
- "majority vote" without reasoning
- "whatever the loudest reviewer wants"

---

## 5) The adjudication workflow (step-by-step)

### Step 1 — Detect disagreement
Trigger adjudication when:
- rater scores differ beyond a threshold (example: 0 vs 3)
- "unsafe vs safe" is disputed
- "abstain vs answer" is disputed (critical in RAG)
- repeated confusion appears in calibration sessions

### Step 2 — Gather evidence
Adjudicator collects:
- the user request
- the model output
- allowed sources
- relevant policy/doc version
- tool traces (if agent)
- audio/transcripts (if voice)

### Step 3 — Decide final truth
Adjudicator selects:
- final score per dimension
- pass/fail safety outcome
- rationale (1–3 sentences)
- what would have been an acceptable variant

### Step 4 — Update the system
If disagreement happened, something is unclear. So you update:
- rubric wording
- anchor examples
- "must include" checklist
- source-of-truth mapping
- new regression tests if needed

### Step 5 — Store it (auditable)
Gold set items must store:
- label + rubric scores
- ground truth version
- policy/doc references
- adjudicator
- timestamp
- change history

---

## 6) Who adjudicates (and how to avoid bias)

### 6.1 Common enterprise setup
- **Primary adjudicators:** senior reviewers / eval lead
- **Domain adjudicators:** legal/security/finance for Tier 3
- **Rotation:** avoid one-person bias

### 6.2 Bias controls (very important)
- Use 2-person adjudication for high-risk tasks
- Keep written rules for:
  - abstain vs answer
  - refusal boundaries
  - what counts as evidence
- Run periodic "adjudicator audits":
  - sample decisions and check consistency

---

## 7) Failure modes (symptoms + root causes)

### 7.1 Gold set becomes stale
Symptoms:
- model "fails" because policies changed
- gold set no longer reflects product behavior

Root causes:
- no version control
- no owners for policy updates

Fix:
- tie gold labels to policy versions
- update gold set in controlled releases

---

### 7.2 Adjudication becomes a bottleneck
Symptoms:
- too many disputes
- slow label turnaround

Root causes:
- rubric too vague
- raters undertrained
- too many tasks marked "gold" too early

Fix:
- improve rater training and anchors
- restrict gold to high-impact tasks
- automate dispute routing (only big disagreements go to adjudication)

---

### 7.3 "Majority vote" hides correctness
Symptoms:
- wrong truth gets locked because most raters guessed the same way

Root causes:
- no evidence-based adjudication
- no source-of-truth enforcement

Fix:
- require evidence for Tier 2–3 decisions
- keep "abstain is correct" rules strict for RAG

---

### 7.4 Adjudicator drift over time
Symptoms:
- adjudication standards slowly change

Root causes:
- no audits
- no calibration for adjudicators

Fix:
- monthly adjudicator calibration sessions
- periodic sampling and consistency checks

---

## 8) Debug playbook: how to build a gold set from scratch

1. Start with your top 10 intents + Tier 2–3 tasks
2. Pull real logs and select representative cases
3. Label with 2+ raters
4. Adjudicate disagreements
5. Add:
   - must-abstain cases
   - safety/refusal cases
   - tool failure cases
   - voice critical-field cases
6. Freeze the gold set version
7. Use it as a release gate benchmark

---

## 9) Enterprise expectations (what serious teams do)

- Gold sets are treated like production code:
  - versioned
  - reviewed
  - auditable
- They maintain separate gold sets for:
  - safety
  - Tier 3 workflows
  - major enterprise tenants
  - multilingual support
- They validate automated judges using gold sets:
  - "LLM judge must match gold outcomes to be trusted"

---

## 10) Ready-to-use templates

### 10.1 Gold item record (copy/paste)

**GoldItemID:**
**Task:**
**Channel:** Chat / RAG / Agent / Voice
**Risk Tier:**
**Difficulty:** Easy / Normal / Hard / Adversarial
**Input:**
**Expected behavior:** Answer / Ask / Abstain / Refuse / Escalate
**Required elements:**
**Forbidden elements:**
**Allowed sources:**
**Rubric scores:** (Correctness, Relevance, Clarity, Grounding, Safety gate, etc.)
**Rationale (1–3 sentences):**
**Adjudicator:**
**Policy/Doc version:**
**Created date:**
**Last updated:**
**Change log:**

### 10.2 Adjudication decision log (copy/paste)

**Case:**
**Disagreement:** (who scored what)
**Final decision:**
**Evidence used:**
**Rule applied:**
**Updates required:** (rubric/anchors/tests)

---
