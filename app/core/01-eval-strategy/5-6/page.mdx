# Chapter 5.6 — De-duplication & Near-Duplicate Control (Anti-Gaming)

**What we're doing here:**
You think you have 5,000 test cases. But after running deduplication, you discover you actually have 2,000 unique scenarios tested 2.5 times each. Your coverage map has phantom entries. Your quality scores are artificially inflated by repetition. And your team may be unconsciously overfitting to patterns they see over and over.

De-duplication is fundamental dataset hygiene. If you skip it, every metric downstream is built on a lie.

---

## 1) Why duplicates poison evaluation

Most teams discover duplication too late. They sample from production logs (which naturally contain repeated queries), generate synthetic variants (which accidentally produce paraphrases), merge datasets from multiple sources (without checking overlap), or copy cases across slices (changing only a label).

What goes wrong:
- **Inflated coverage** — you think you test 50 intents, but 20 are duplicates of 10 core intents
- **Biased metrics** — repeated cases get overweighted, skewing aggregate scores
- **Wasted labeling budget** — you pay annotators to label the same scenario multiple times
- **False confidence** — passing the same test repeatedly doesn't prove robustness

Even when teams remove exact duplicates, a subtler problem remains: **eval overfitting.** When developers can see the eval set, they naturally optimize for what they see — tuning prompts to specific phrasing, fixing edge cases that appear in the eval but ignoring similar cases that don't. This isn't malicious — it's human nature. But it breaks the eval's ability to measure generalization.

---

## 2) Types of duplication

### Exact duplicates
Identical text after normalization (lowercasing, whitespace removal, punctuation standardization). Examples: "What are your hours?" vs "what are your hours?" or production logs sampled from overlapping time windows.

**Detection:** Hash-based. Normalize text, compute SHA-256, group by hash, keep first occurrence. Most eval platforms include this as a default preprocessing step.

### Near-duplicates (paraphrases)
Different surface text, but semantically equivalent or trivially different. "What time do you open?" vs "When do you open?" or "I need to reset my password" vs "I want to reset my password."

If 30% of your dataset is paraphrases of the same 100 core queries, you're not measuring 1,000 scenarios — you're measuring 100 with style variations.

**Detection:** N-gram overlap plus MinHash. Compute character or word n-grams, use MinHash to estimate similarity, flag pairs above threshold (typically 0.70–0.85). Always sample flagged pairs for human review — automated detection produces false positives.

### Semantic duplicates (different surface, same test intent)
Different phrasing and structure, but testing the exact same capability. "What's the return policy?" and "How do I return an item?" both test return policy knowledge. Two different agent scenarios that test the same tool sequence.

**Detection:** Embedding-based similarity. Encode each case with a sentence embedding model, compute cosine similarity, flag pairs above threshold (typically 0.85–0.92). Review flagged clusters manually — semantic similarity doesn't guarantee functional redundancy.

**When to keep semantic near-duplicates:** If they test different slices, different difficulty levels, or represent different user personas. **When to remove:** If they're in the same slice and difficulty bucket with no strategic purpose.

### Cross-split duplicates
The same case appears in multiple dataset splits (training, dev, test). This is the most dangerous type — covered in depth in Chapter 5.4. Zero tolerance for leakage between eval and training data.

---

## 3) LLM-assisted semantic deduplication

For high-stakes datasets, embedding similarity isn't always enough. You need human-level judgment of "does this test the same thing?"

The method: use embedding similarity to find candidate pairs (cosine above 0.80), then prompt an LLM to judge whether each pair tests the exact same capability. The LLM classifies each pair as DUPLICATE, VARIANT (meaningfully different), or DISTINCT. Keep VARIANT and DISTINCT. For DUPLICATE, keep the higher-quality or earlier-created case.

Use this for Tier 2–3 datasets, safety/red-team suites, and when deduplicating 10,000+ cases where manual review of all pairs is impractical. To control cost, only run on candidate pairs flagged by cheaper methods first.

---

## 4) Anti-gaming strategies

### Why teams accidentally game their own evals
Developers see the eval cases during debugging. They fix specific failures without generalizing. They tune prompts to the visible eval phrasing. Over time, the model/prompt becomes **overfit to the eval set**, and scores inflate without real quality improvement.

The signal: eval scores improve, but production quality stays flat or gets worse. You pass the eval, ship, and immediately see new failure modes.

### Rotation strategies
Periodically swap out a portion of the eval dataset with new cases, keeping the rest stable for trend tracking.

Maintain a core stable set (30–50%) for regression and trend analysis. Rotate the remaining 50–70% on a fixed cadence — monthly for high-velocity products, quarterly for standard, 70% quarterly for safety suites (adversarial tactics evolve fast). Draw new cases from fresh production logs, updated expert cases, or adversarial red-team exercises.

What stays stable: anchor cases (gold examples that define quality), incident cases (regressions you must never repeat), and cross-version benchmarks.

### Hidden holdout
Designate 20–30% of your eval set as hidden holdout — never shown to developers, only run in CI or by the eval team. Developers see scores on the visible set only. At release gates, both visible and hidden scores must pass.

This is the single most effective anti-gaming control. When visible scores are high but hidden scores lag, that's overfitting.

Practical challenge: developers will want access to debug failures. Solution: for hidden set failures, provide aggregate error categories ("10% failed on multi-constraint RAG"), not specific cases. Release a small sample periodically (10% every quarter) to help teams improve, then replace them with new hidden cases.

---

## 5) Knobs & defaults

**Deduplication thresholds:**

| Method | Threshold | Catches | Review needed |
|---|---|---|---|
| Exact (hash) | 100% match | Identical text | No |
| N-gram (MinHash) | 0.70–0.85 | Paraphrases | Sample review |
| Embedding (cosine) | 0.85–0.92 | Semantic duplicates | Always |
| LLM-as-judge | N/A | Functional redundancy | Spot-check |

Run exact dedup first (always, no review needed), then n-gram or embedding similarity, then review flagged pairs manually or via LLM. For Tier 2+ datasets, use LLM-assisted review on borderline cases.

**Deduplication frequency:** On dataset creation (always). On dataset merge (always). On expansion (incremental — new vs existing). Quarterly audit for production log datasets.

**Rotation and holdout parameters:** 40% stable core, 40% rotated set, 20% hidden holdout. Quarterly rotation cadence. Adjust based on release velocity (faster releases = faster rotation) and dataset size.

---

## 6) Failure modes

**"We have 5,000 cases but scores feel inflated."**
Dataset contains many near-duplicates. Duplicate cases overweight certain intents. Fix: run embedding-based similarity analysis, cluster similar cases, sample and review clusters, remove redundant cases. Expect scores to drop — that's honest metrics.

**"Near-duplicate clusters are biasing slice scores."**
One language or slice has suspiciously high scores because synthetic generation created many paraphrases or production sampling over-sampled a popular query. Fix: run dedup separately per slice. Cap near-duplicate variants per intent (max 3 paraphrases). Track "unique intent coverage" per slice, not just raw case count.

**"Dedup removed too much — we lost important variants."**
Threshold too aggressive, didn't distinguish between redundant tests and strategic variants. Fix: label cases with strategic intent before dedup. Protect coverage-essential, adversarial, and cross-slice cases. Run dedup only on general/normal cases. Never auto-delete without inspection.

**"Scores improved but production quality didn't."**
No eval rotation. No hidden holdout. Prompt tuning is overfitting to visible eval phrasing. Fix: implement rotation (swap 30–50% quarterly). Create hidden holdout (20–30%, never exposed). Track visible vs hidden scores — divergence equals overfitting.

---

## 7) Enterprise expectations

- They run deduplication as part of every dataset build and merge — automated, not optional
- They maintain exact dedup + near-dup + cross-split validation in CI pipelines
- They track "unique intent coverage" separately from raw case count
- They implement eval rotation (30–50% quarterly) to prevent overfitting
- They maintain hidden holdouts (20–30%) that developers never see
- They monitor visible vs hidden scores — divergence triggers an overfitting investigation
- They treat cross-split contamination as a critical bug, not a warning

---

## 8) Interview Q&A

**Q: How do you handle duplicates in eval datasets?**
A: I run three layers — exact hash dedup (automatic), embedding-based near-dup detection (flagged for review), and cross-split validation (zero tolerance for train/eval overlap). For Tier 2+ datasets, I add LLM-assisted semantic dedup where an LLM judges whether candidate pairs test the same capability. I also track unique intent coverage, not just raw case count.

**Q: How do you prevent teams from gaming their own evals?**
A: Two controls. First, I rotate 30–50% of the eval set quarterly so the team can't memorize the test. Second, I maintain a hidden holdout — 20–30% of the eval that developers never see, only run in CI. If visible scores are high but hidden scores lag, that's overfitting and we investigate.

**Q: What's your dedup threshold?**
A: It depends on the method. Exact hash is binary. For embedding-based semantic dedup, I start at 0.87 cosine similarity, calibrate by manually reviewing 50–100 pairs, and adjust based on false positive rate — targeting under 10% false positives. Higher stakes means lower threshold (catch more duplicates).

---

*Next: Chapter 5.7 covers dataset QA — spot checks, audits, and label consistency controls that ensure your data quality before it enters production evals.*
