# Chapter 4 â€” Ground Truth & Gold Standards

### Plain English

Ground truth is the answer you compare your AI system against.

**What is the correct answer, and how do you know?**

In traditional software, ground truth is clear:
- input: 2 + 2
- expected output: 4

In AI systems, ground truth is messy:
- some tasks have one correct answer
- some tasks have many correct answers
- some tasks have no objectively correct answer

Without clear ground truth, your evals measure nothing.

You cannot score a response as correct or incorrect if you do not know what correct means.

This chapter teaches you how to define ground truth per task type, handle ambiguity, prevent label rot, and maintain gold standard datasets that your entire eval system depends on.

---

### Why This Chapter Exists

AI outputs are rarely exact matches.

You cannot use simple string equality to judge quality.

Examples:
- User asks: "What is the capital of France?"
- Acceptable answers: "Paris", "The capital is Paris", "Paris, France", "It is Paris"
- Unacceptable answers: "Lyon", "I do not know", "France has many cities"

Ground truth must define:
- what counts as correct
- what counts as acceptable variation
- what counts as incorrect
- how to handle ambiguity

This chapter exists to:
- make ground truth explicit, not implicit
- prevent annotator confusion
- create reproducible evaluation
- handle tasks with multiple correct answers
- prevent datasets from degrading over time

In 2026, **ground truth is not obvious**.

It must be engineered, documented, and maintained.

---

### What Ground Truth Actually Is (2026 Meaning)

Ground truth is **not**:
- whatever the model outputs
- the first answer an annotator writes
- a single correct string

Ground truth **is**:
- task-specific
- versioned and documented
- validated by multiple reviewers
- maintained over time
- sometimes a set of acceptable answers, not one answer

Technically, ground truth defines:
- the reference answer for correctness scoring
- acceptable variations
- edge case handling rules
- confidence levels (certain, likely, uncertain)
- adjudication rules when reviewers disagree

---

### Core Components of Ground Truth & Gold Standards

#### 1. Ground Truth Per Task Type

Different tasks require different ground truth strategies.

**Extraction tasks**:
- Ground truth: the exact entities, dates, numbers from the source
- Acceptable variation: formatting (e.g., "January 1, 2025" vs "2025-01-01")
- Unacceptable: wrong values, hallucinated entities

**Classification tasks**:
- Ground truth: the correct category label
- Acceptable variation: none (labels are discrete)
- Edge case handling: what to do when the example is ambiguous

**Summarization tasks**:
- Ground truth: a reference summary written by experts
- Acceptable variation: same meaning, different wording
- Evaluation method: compare against reference using semantic similarity or LLM judge

**RAG tasks**:
- Ground truth: the correct answer grounded in provided documents
- Acceptable variation: different phrasing, same grounding
- Evaluation method: check both correctness and grounding

**Open-ended creative tasks**:
- Ground truth: there is no single correct answer
- Evaluation method: score on dimensions like coherence, relevance, tone
- Reference examples: show what good looks like, not what correct is

**Agent tasks**:
- Ground truth: the correct sequence of actions or final state
- Acceptable variation: different paths to the same goal
- Evaluation method: check both final state and path quality

Your ground truth strategy must match the task type.

---

#### 2. Handling Ambiguity & Multiple Correct Answers

Many tasks have multiple correct answers.

Example: "Summarize this article"
- Multiple valid summaries exist
- Summaries can emphasize different aspects
- Length and detail vary

Strategies for handling multiple correct answers:

**Strategy 1: Accept sets**
- Define a set of acceptable answers
- Score as correct if the output matches any answer in the set
- Example: "Paris", "The capital is Paris", "Paris, France"

**Strategy 2: Semantic equivalence**
- Use embedding similarity or LLM judges
- Score based on meaning, not exact wording
- Example: "Paris" and "The capital is Paris" are semantically equivalent

**Strategy 3: Rubric-based scoring**
- Do not define a single correct answer
- Define scoring criteria (0-3 scale)
- Example: Does the summary capture the key points? Is it concise? Is it accurate?

**Strategy 4: Comparative evaluation**
- Compare outputs pairwise
- Ask: which output is better?
- Build rankings instead of absolute scores

Ambiguity is not a failure.

Ambiguity is a property of the task, and your ground truth strategy must handle it.

---

#### 3. Source-of-Truth Rules

Where does ground truth come from?

**Human expert annotation**:
- Domain experts write reference answers
- Used for complex, subjective, or creative tasks
- Requires clear rubrics and training

**Source documents**:
- For RAG and grounding tasks, the source is the truth
- Ground truth is what the document says, not what is factually true

**Factual databases**:
- For fact-based tasks, use trusted databases
- Example: Wikidata, enterprise knowledge bases

**User intent**:
- For conversational tasks, user satisfaction is the truth
- Ground truth is what the user wanted, not what is objectively correct

**Consensus annotation**:
- Multiple annotators label the same example
- Ground truth is the majority vote or adjudicated answer

**Model-generated with human validation**:
- Use a strong model to generate reference answers
- Humans review and correct
- Faster than pure human annotation, more reliable than pure model generation

Your source-of-truth rules must be documented and consistent.

---

#### 4. Gold Sets & Adjudication

A gold set is a curated, high-quality dataset with verified ground truth.

Gold sets are used for:
- calibrating human annotators
- evaluating model performance
- benchmarking changes over time
- training LLM judges

**Creating a gold set**:
- Start with 100-500 examples covering key task categories
- Have multiple annotators label each example
- Measure inter-annotator agreement
- Adjudicate disagreements with a domain expert
- Lock the gold set (no further changes without versioning)

**Adjudication process**:
- When annotators disagree, a third expert reviews
- The expert decides the correct ground truth
- Adjudication rules are documented (majority vote, defer to domain expert, mark as ambiguous)

Gold sets are **expensive to create** but **invaluable for eval quality**.

Without a gold set, you have no anchor for correctness.

---

#### 5. Preventing Label Rot

Label rot happens when ground truth becomes outdated.

Common causes:
- Product behavior changes (new features, new policies)
- User expectations change
- Annotator understanding drifts over time
- Source documents are updated

Preventing label rot:

**Version your ground truth**:
- Tag ground truth with version numbers
- Track when and why ground truth changed

**Review gold sets regularly**:
- Quarterly review of gold set examples
- Check if ground truth still matches product expectations

**Re-annotation audits**:
- Periodically re-annotate a sample of examples
- Measure drift from original labels

**Triggered reviews**:
- When product behavior changes, review affected examples
- When model performance suddenly changes, check if ground truth drifted

Label rot is silent and deadly.

It makes your evals lie to you without you noticing.

---

#### 6. Open-Ended & Creative Task Ground Truth

Creative tasks have no single correct answer.

Examples:
- "Write a poem about spring"
- "Design a logo concept"
- "Generate a marketing tagline"

Ground truth strategies for creative tasks:

**Strategy 1: Reference examples**
- Provide high-quality examples of good outputs
- Score by comparing to reference quality, not reference content

**Strategy 2: Dimension-based rubrics**
- Score on dimensions: creativity, relevance, coherence, tone
- Ground truth is the rubric, not a reference output

**Strategy 3: Human preference**
- Show outputs to users
- Ground truth is what users prefer

**Strategy 4: Constraint satisfaction**
- Define constraints (length, tone, safety)
- Ground truth is whether constraints are met

Creative tasks are scored on **quality**, not **correctness**.

Your ground truth must reflect this.

---

### Enterprise Perspective

Enterprises need:
- documented source-of-truth rules for audits
- adjudication processes for consistency
- version-controlled gold sets
- regular reviews to prevent label rot

Ground truth allows:
- defensible evaluation outcomes
- consistent quality standards across teams
- evidence for compliance reviews
- reproducible results over time

In regulated industries, ground truth creation is reviewed by compliance teams.

It is treated as a critical system artifact.

---

### Founder / Startup Perspective

Startups need:
- fast but rigorous ground truth creation
- strategies for handling ambiguity without over-engineering
- gold sets that are small but high-quality
- processes that scale as the team grows

Good ground truth practices:
- clarify what success means
- reduce arguments about eval results
- allow you to delegate annotation work
- create institutional knowledge even with turnover

Early-stage teams that skip ground truth eng end up with unreliable evals.

They discover this only when evals contradict user feedback.

---

### Common Failure Modes

- Assuming ground truth is obvious
- Using model outputs as ground truth
- No process for handling ambiguity
- No adjudication when annotators disagree
- Gold sets are never reviewed or updated
- Label rot is not monitored
- Creative tasks are evaluated like extraction tasks
- Source-of-truth rules are implicit, not documented

Recognizing these failures is the foundation of reliable evaluation.

---
