# Chapter 5.7 — Dataset QA (Spot Checks, Audits & Label Consistency)

**What we're doing here:** We're implementing systematic quality assurance for evaluation datasets—because your eval is only as good as your data. This chapter covers the three layers of dataset QA (schema, content, and label validation), spot check protocols, automated QA pipelines, and audit trails that prevent bad examples from corrupting your metrics silently. You'll learn how to catch schema drift before it breaks your pipeline, detect label inconsistencies that inflate disagreement scores, and establish sign-off workflows that ensure every dataset version meets production standards before it's used to make model decisions.

**Enterprise outcome:** Teams ship eval datasets with confidence, knowing that schema validation catches broken fields before runtime, content spot checks ensure examples actually test what they claim to test, and label consistency audits prevent annotator drift from skewing metrics. When your VP asks "how do we know this eval is valid," you point to audit logs showing who reviewed what sample when, automated checks that run on every commit, and sign-off records proving domain experts validated the dataset before it went live. Dataset QA transforms evals from "we think this works" to "we have evidence this measures what we need."

---

### 1. Why Dataset QA Is Mandatory

**Your eval is only as good as your data.** A single batch of mislabeled examples can make a worse model look better, causing you to ship a regression. Schema drift can break your eval pipeline at 3am when you're running release candidate tests. Inconsistent labels inflate inter-rater disagreement and make it impossible to tell if annotators are confused or the task is genuinely ambiguous.

Dataset QA is **not optional.** It's the difference between:
- **Without QA:** Eval runs complete, metrics look plausible, you ship based on numbers that are silently wrong because 15% of your dataset had swapped labels or malformed JSON that your parser handled inconsistently.
- **With QA:** Every dataset version passes schema validation, content spot checks, and label consistency audits before it's used in production evals. You catch bad examples before they corrupt decisions.

**Core principle:** Treat eval datasets like production data. You wouldn't deploy a database migration without schema validation—don't run evals on unvalidated datasets.

---

### 2. Three Layers of Dataset QA

Dataset QA operates at three levels, each catching different failure modes:

**Layer 1: Schema QA (Structure & Types)**
- **What it checks:** Correct fields present, valid data types, no unexpected nulls, arrays have expected length, enums match allowed values.
- **Why it matters:** Schema violations break eval pipelines. A missing `expected_output` field means you can't score that example. A `difficulty` value of "hard" when the enum expects "difficult" causes silent filtering failures.
- **Tools:** JSON Schema validation, TypeScript type checks, Pydantic models, database constraints.

**Layer 2: Content QA (Semantics & Accuracy)**
- **What it checks:** Inputs make sense for the task, expected outputs are actually correct, metadata (difficulty, category, source) accurately describes the example, edge cases are valid edge cases not just broken data.
- **Why it matters:** Structurally valid data can still be semantically wrong. An example labeled "easy" that requires domain expertise, an expected output that's factually incorrect, or an input that's just random text all corrupt your metrics.
- **Tools:** Domain expert spot checks, automated heuristics (length checks, format validation), LLM-based content review.

**Layer 3: Label QA (Consistency & Agreement)**
- **What it checks:** Labels match the rubric definition, same/similar inputs get consistent labels, inter-rater agreement is within acceptable bounds, no stale labels from outdated rubric versions.
- **Why it matters:** Label inconsistency destroys eval reliability. If two annotators label the same type of response differently, your metrics become noise. If half your dataset uses an old rubric version, you're mixing incompatible data.
- **Tools:** Inter-rater agreement metrics (Cohen's kappa, Fleiss' kappa), duplicate detection, label distribution analysis, rubric version tracking.

**Run all three layers.** Schema QA is automated and runs on every commit. Content QA requires human review on samples. Label QA combines automated consistency checks with periodic deep audits.

---

### 3. Spot Check Protocols

**Spot checks are structured human reviews of dataset samples.** They catch issues that automated checks miss—subtle semantic errors, ambiguous examples that confuse annotators, edge cases that don't actually test what they claim to test.

**Sample Size Rules**
- **Minimum per slice:** Review at least 20 examples per meaningful slice (difficulty level, category, source type). For small slices (under 50 examples), review 50%.
- **Stratified sampling:** Sample proportionally from each slice to ensure you review the full distribution, not just the most common cases.
- **New vs. changed:** For dataset updates, review 100% of new examples and 20% of changed examples. For unchanged examples, sample 5% to verify no upstream data corruption.

**Who Reviews**
- **Not the original author:** Spot checks must be done by someone who didn't create the examples. Authors are blind to their own errors.
- **Domain expert when possible:** Content QA requires subject matter expertise. A lawyer reviewing legal eval data catches errors an engineer would miss.
- **Documented reviewer:** Audit logs must record who reviewed what, so you can trace decisions and assess reviewer reliability over time.

**Frequency**
- **Per dataset version:** Every versioned release (v1.0, v1.1, v2.0) requires a full QA cycle before use in production evals.
- **Per major update:** Adding a new category, changing the rubric, or importing data from a new source triggers a QA review of affected examples.
- **Continuous monitoring:** Even stable datasets get quarterly spot checks to detect drift (e.g., real-world data no longer matches your examples).

**Document everything.** Each spot check produces a QA report: sample reviewed, issues found, severity (blocking vs. advisory), resolution (fixed, accepted risk, deferred).

---

### 4. Automated QA Checks

**Automated checks run on every dataset commit,** catching the most common errors before humans waste time reviewing broken data.

**Schema Validation**
- **JSON Schema enforcement:** Define expected structure (required fields, types, nested objects) and validate every example on load.
- **Field type checks:** Ensure `difficulty` is a string from the allowed enum, `expected_output` is not null, `metadata.timestamp` is a valid ISO 8601 date.
- **Referential integrity:** If your dataset references external entities (user IDs, product SKUs), verify they exist and are current.

```yaml
# Example JSON Schema for eval dataset
schema:
  type: object
  required: [id, input, expected_output, metadata]
  properties:
    id:
      type: string
      pattern: "^[a-z0-9-]+$"
    input:
      type: string
      minLength: 1
    expected_output:
      type: object
      required: [response, explanation]
    metadata:
      type: object
      required: [difficulty, category, source, rubric_version]
      properties:
        difficulty:
          type: string
          enum: [easy, medium, hard]
        rubric_version:
          type: string
          pattern: "^v\\d+\\.\\d+$"
```

**Distribution Checks**
- **Slice balance:** Verify difficulty levels are within expected ratios (e.g., 30% easy, 50% medium, 20% hard). Flag datasets that deviate >15% from target.
- **Difficulty mix:** Ensure you have enough hard examples to differentiate top models, enough easy examples to catch basic failures.
- **Source type ratios:** If you sample from prod, synthetic, and handcrafted sources, verify the mix matches your design (e.g., 60/20/20).

**Anomaly Detection**
- **Outlier lengths:** Flag examples with unusually long/short inputs or outputs (e.g., >3 standard deviations from mean). Often indicates truncation, duplication, or garbage data.
- **Unusual patterns:** Detect repeated phrases (copy-paste errors), all-caps text (encoding issues), special characters (JSON escaping failures).
- **Empty fields:** Catch null or empty string values in fields that should never be empty (even if not schema-required).

**Label Consistency Checks**
- **Same input → same label:** If the same input appears multiple times (exact match or near-duplicate), verify it has the same expected output and difficulty.
- **Similar input → similar label:** For near-duplicates (>90% text similarity), flag if labels differ significantly—either you need to differentiate the inputs or fix inconsistent labeling.
- **Temporal consistency:** If you re-label old examples with a new rubric, track both versions and flag if the new label is drastically different (suggests rubric drift or misunderstanding).

**Run these checks in CI/CD.** Dataset commits that fail schema validation or exceed anomaly thresholds are blocked from merge until issues are resolved.

---

### 5. Audit Trails

**Every QA action must be logged.** Audit trails answer: What was checked? When? By whom? What issues were found? What was the resolution?

**What to Log**
- **Timestamp & version:** When the QA check ran, which dataset version was checked (git SHA, version tag, or dataset ID).
- **Reviewer identity:** Who performed the spot check or approved the automated QA results. For automated checks, log the tool/version.
- **Sample details:** For spot checks, log which examples were reviewed (IDs or selection criteria). For automated checks, log how many examples passed/failed each rule.
- **Issues found:** Severity (blocking, high, medium, low), category (schema, content, label), description, affected example IDs.
- **Resolution:** Fixed (link to commit), accepted risk (justification), deferred (target version), false positive (rule updated).

**Issue Tracking**
- **Link to backlog:** QA issues become tickets in your issue tracker (JIRA, Linear, GitHub Issues). Blocking issues must be resolved before dataset release.
- **Severity levels:**
  - **Blocking:** Schema violations, factually incorrect expected outputs, label inconsistencies >20%.
  - **High:** Metadata errors that affect slicing, missing edge cases, unclear rubric application.
  - **Medium:** Suboptimal example phrasing, minor metadata inaccuracies, style inconsistencies.
  - **Low:** Typos in comments, formatting inconsistencies that don't affect scoring.

**Sign-Off Before Production**
- **Explicit approval:** Before a dataset version is used in production evals, a designated reviewer (eval lead, domain expert) must sign off in the audit log.
- **Checklist completion:** Sign-off requires all blocking issues resolved, spot check sample size met, automated checks passed, distribution targets achieved.
- **Version lock:** Once signed off, the dataset version is immutable. Any changes require a new version and new QA cycle.

```yaml
# Example audit log entry
audit_log:
  - timestamp: "2026-01-15T14:32:00Z"
    dataset_version: "v2.3"
    git_sha: "a7f3d2e"
    check_type: "spot_check"
    reviewer: "jane.doe@company.com"
    sample_size: 50
    issues_found:
      - severity: "blocking"
        category: "content"
        description: "Expected output incorrect for example eval-1234"
        affected_ids: ["eval-1234"]
        resolution: "fixed in commit b8e4f1a"
      - severity: "medium"
        category: "metadata"
        description: "Difficulty labeled 'easy' but requires domain knowledge"
        affected_ids: ["eval-5678"]
        resolution: "accepted_risk - borderline case, will monitor"
    sign_off: "approved"
    notes: "All blocking issues resolved, ready for production use"
```

---

### 6. Knobs & Defaults

**Spot Check Sample Size**
- **Default:** 20 examples per slice, 50% of slices under 50 examples, 100% of new examples, 20% of changed examples.
- **When to increase:** High-stakes evals (release gates, model selection), datasets with history of errors, new annotation team.
- **When to decrease:** Stable datasets with long track record, low-risk exploratory evals, resource constraints (but never below 10 per slice).

**Automated Check Frequency**
- **Default:** On every commit (schema + distribution + anomaly checks), nightly full scan (including expensive similarity computations).
- **When to increase:** Datasets with frequent updates, early in eval program maturity, after a major quality incident.
- **When to decrease:** Stable datasets updated monthly or less, mature programs with strong QA culture, compute cost concerns.

**Severity Thresholds**
- **Default:** Schema violations = blocking, >10% label inconsistency = blocking, >3 SD outliers = high, >15% distribution deviation = medium.
- **When to tighten:** High-stakes evals, mature programs with good tooling, after shipping a bad dataset.
- **When to relax:** Early-stage programs (too strict blocks progress), exploratory evals, low-risk use cases.

**Sign-Off Requirements**
- **Default:** Eval lead or domain expert must approve before production use, all blocking issues resolved, automated checks passed.
- **When to add approvers:** Cross-functional impact (product + legal + compliance for user-facing evals), high-risk domains (medical, financial).
- **When to streamline:** Low-risk internal evals, mature datasets with no recent issues, automated-only QA for minor updates.

**Calibration tip:** Start with defaults, then tighten based on observed failure modes. If you ship a bad dataset despite QA, add checks to prevent that class of error. If QA is blocking progress without catching real issues, relax low-value checks.

---

### 7. Failure Modes

**Symptom:** Eval metrics look normal but model performance in production is different than eval predicted.
- **Root cause 1:** Bad examples in the dataset (incorrect expected outputs, mislabeled difficulty) corrupted metrics silently.
- **Root cause 2:** Label inconsistency inflated disagreement scores, making it hard to distinguish true model variance from data noise.
- **Root cause 3:** Schema drift caused certain examples to be scored incorrectly or skipped entirely, biasing metrics.
- **Fix:** Run full QA audit on the dataset. Check schema compliance, spot check 10% of examples for content correctness, measure label consistency on duplicates. Re-run eval on cleaned dataset and compare metrics—if they shift significantly, investigate why the original dataset was used despite being broken.

**Symptom:** Eval pipeline breaks at runtime with parsing errors or missing fields.
- **Root cause:** Schema validation not enforced in CI, allowing malformed examples to merge.
- **Fix:** Add JSON Schema validation as a blocking CI check. Ensure every dataset commit is validated before merge. For existing datasets, run schema validation and fix violations before next eval run.

**Symptom:** Annotators complain that examples are ambiguous or the rubric doesn't apply cleanly.
- **Root cause 1:** Content QA not catching semantically broken examples—inputs that are edge cases but not useful edge cases, expected outputs that require judgment calls the rubric doesn't address.
- **Root cause 2:** Rubric version drift—dataset was created under v1 rubric but annotators are using v2, causing apparent inconsistency.
- **Fix:** Increase spot check sample size and ensure domain experts review new examples before annotation. Add rubric version to metadata and validate that all examples in a batch use the same version. When updating rubric, re-label a sample of old examples to verify the new rubric still makes sense for existing data.

**Symptom:** Inter-rater agreement is low (kappa below 0.6) despite clear rubric.
- **Root cause 1:** Label inconsistency from different annotators interpreting edge cases differently. The rubric is clear in theory but doesn't cover the long tail of real examples.
- **Root cause 2:** Annotator drift over time—early examples labeled one way, later examples labeled differently as annotators' understanding evolved.
- **Root cause 3:** Stale labels—examples were labeled months ago under a different rubric or before annotators were properly trained.
- **Fix:** Run duplicate detection to find same/similar examples with different labels. Bring annotators together to resolve disagreements and update rubric to clarify edge cases. Re-label a stratified sample with the updated rubric to establish a new baseline. Track rubric version and annotator ID in metadata so you can isolate drift.

**Symptom:** Automated QA flags hundreds of "issues" but most are false positives, leading to alert fatigue.
- **Root cause:** Thresholds too sensitive for your data distribution. Outlier detection tuned for normal distributions flags legitimate edge cases. Similarity checks flag intentional near-duplicates (e.g., testing robustness to small input changes).
- **Fix:** Tune thresholds based on your actual data. Review flagged "issues" with domain experts and adjust rules to reduce false positives. Add allow-lists for known edge cases (e.g., certain examples are intentionally long). Document exceptions in the QA config so future reviewers understand why certain patterns are allowed.

**Symptom:** Dataset passes all QA checks but still has quality issues in production.
- **Root cause:** QA checks focused on schema and surface-level content, missing deeper semantic issues like examples that don't actually test the intended capability or expected outputs that are technically correct but misleading.
- **Fix:** Enhance content QA with domain expert review, not just automated checks. Add LLM-based semantic validation (e.g., does this example actually test the stated capability?). Run pilot evals on small model/dataset samples and manually inspect failures to find systematic dataset issues.

---

### 8. Debug Playbook

**When an eval produces unexpected metrics:**

1. **Check dataset version:** Verify you're using the signed-off dataset version, not a draft or outdated snapshot.
2. **Run schema validation:** Ensure the dataset you actually loaded matches the schema (no silent parsing errors).
3. **Spot check failures:** Manually review 10 examples where the model failed. Are the expected outputs actually correct? Is the difficulty label accurate?
4. **Check for label drift:** If this eval uses a dataset you've run before, compare label distributions and inter-rater agreement to historical baselines.
5. **Validate slicing:** Ensure examples are being grouped into the intended slices (check `category`, `difficulty`, `source` metadata).

**When automated QA checks fail:**

1. **Review flagged examples:** Don't just fix to pass the check—understand why the example was flagged. Is it a real issue or a false positive?
2. **Check recent changes:** If a dataset that previously passed QA now fails, diff the versions to see what changed (new examples, metadata updates, schema changes).
3. **Validate thresholds:** If distribution checks fail, verify the target ratios are still appropriate (requirements may have changed).

**When inter-rater agreement is low:**

1. **Isolate disagreement:** Break down kappa by annotator pair, category, and difficulty. Is disagreement uniform or concentrated in specific slices?
2. **Review disagreements:** Manually examine examples where annotators disagreed. Are they genuinely ambiguous, or is one annotator misapplying the rubric?
3. **Check rubric version:** Ensure all annotators are using the same rubric version and have been trained on recent updates.
4. **Run calibration session:** Bring annotators together to label a shared sample, discuss disagreements, and align on edge case handling.

**When datasets pass QA but production metrics diverge:**

1. **Distribution shift:** Compare the dataset's input distribution to production traffic. If they don't match, your eval isn't representative.
2. **Scoring implementation mismatch:** Verify the eval scoring code matches production scoring logic (same thresholds, same tie-breaking rules).
3. **Hidden data leakage:** Check if production system has access to information not present in eval examples (user history, real-time data).

---

### 9. Enterprise Expectations

**For early-stage programs (months 0-6):**
- Implement schema validation as a blocking CI check.
- Spot check 20% of every new dataset version before use.
- Track issues in a shared doc or simple issue tracker.
- Manual sign-off by eval lead before production use.

**For scaling programs (months 6-18):**
- Full automated QA pipeline (schema, distribution, anomaly, label consistency).
- Spot check protocol with minimum sample sizes per slice.
- Audit log integrated with version control (git tags + metadata).
- Severity levels and resolution workflows for QA issues.

**For mature programs (18+ months):**
- Multi-layer QA (schema, content, label) with role-based reviewers (engineers for schema, domain experts for content).
- Automated QA checks run on every commit with configurable thresholds.
- Comprehensive audit trails with issue tracking, sign-off workflows, and compliance reporting.
- Continuous monitoring even for stable datasets (quarterly spot checks, drift detection).

**Red flags in any program:**
- Datasets used in production evals without documented QA review.
- Schema validation only run manually or not at all.
- No tracking of who reviewed what or what issues were found.
- Same person creating examples and doing QA spot checks (no independence).

---

### 10. Ready-to-Use Templates

**Dataset QA Checklist**

```yaml
# Eval Dataset QA Checklist — Run Before Production Use

dataset_version: "v2.3"
git_sha: "a7f3d2e"
reviewer: "jane.doe@company.com"
review_date: "2026-01-15"

# Schema QA
schema_validation:
  - [ ] All examples pass JSON Schema validation
  - [ ] No unexpected null or empty fields
  - [ ] Enums match allowed values (difficulty, category, source)
  - [ ] Metadata includes rubric_version for all examples
  - [ ] Referential integrity verified (external IDs exist)

# Content QA (Spot Check)
content_spot_check:
  sample_size: 50  # 20 per slice, 3 slices
  - [ ] Inputs are clear and unambiguous
  - [ ] Expected outputs are factually correct
  - [ ] Difficulty labels match actual task complexity
  - [ ] Edge cases are valid and useful (not just broken data)
  - [ ] Metadata (category, source) accurately describes example

# Label QA
label_consistency:
  - [ ] Duplicate examples have consistent labels
  - [ ] Inter-rater agreement measured (kappa > 0.7)
  - [ ] All examples use current rubric version
  - [ ] No stale labels from outdated rubric

# Distribution QA
distribution_checks:
  - [ ] Difficulty mix within 15% of target (30/50/20)
  - [ ] Source type ratios match design (60/20/20)
  - [ ] All required categories represented (min 20 examples each)
  - [ ] No slices under minimum size (10 examples)

# Automated QA
automated_checks:
  - [ ] CI schema validation passing
  - [ ] Anomaly detection (outliers, empty fields) clean
  - [ ] Label consistency checks passing
  - [ ] Distribution checks within thresholds

# Sign-Off
issues_found: 2
blocking_issues_resolved: true
sign_off: "approved"
notes: "Minor metadata fixes applied, ready for production eval use"
```

**Audit Log Schema**

```yaml
# Audit Log Entry Schema — Track All QA Actions

audit_log_entry:
  timestamp: "2026-01-15T14:32:00Z"
  dataset_id: "customer_support_eval"
  dataset_version: "v2.3"
  git_sha: "a7f3d2e"

  check_type: "spot_check"  # spot_check | automated | full_audit
  reviewer_id: "jane.doe@company.com"
  reviewer_role: "domain_expert"  # domain_expert | eval_engineer | annotator

  scope:
    total_examples: 500
    sample_size: 50
    sampling_strategy: "stratified_by_difficulty"
    slices_reviewed: ["easy", "medium", "hard"]

  issues_found:
    - issue_id: "QA-2024-001"
      severity: "blocking"  # blocking | high | medium | low
      category: "content"  # schema | content | label | distribution
      description: "Expected output factually incorrect"
      affected_example_ids: ["eval-1234"]
      resolution: "fixed"  # fixed | accepted_risk | deferred | false_positive
      resolution_commit: "b8e4f1a"
      resolution_notes: "Corrected expected output, verified with product team"

    - issue_id: "QA-2024-002"
      severity: "medium"
      category: "metadata"
      description: "Difficulty mislabeled (easy vs medium)"
      affected_example_ids: ["eval-5678", "eval-5679"]
      resolution: "accepted_risk"
      resolution_notes: "Borderline cases, will monitor in next review"

  automated_checks:
    schema_validation: "passed"
    distribution_check: "passed"
    anomaly_detection: "passed"
    label_consistency: "passed"

  sign_off:
    approved: true
    approver_id: "eval_lead@company.com"
    approval_timestamp: "2026-01-15T16:00:00Z"
    notes: "All blocking issues resolved, ready for production use"
```

**QA Issue Tracker Fields**

```yaml
# QA Issue Ticket Template — For Tracking Dataset Quality Issues

issue_ticket:
  id: "QA-2024-001"
  created_at: "2026-01-15T14:32:00Z"
  created_by: "jane.doe@company.com"

  dataset:
    dataset_id: "customer_support_eval"
    version: "v2.3"
    git_sha: "a7f3d2e"

  classification:
    severity: "blocking"  # blocking | high | medium | low
    category: "content"  # schema | content | label | distribution
    subcategory: "incorrect_expected_output"

  description: |
    Expected output claims product supports feature X, but product docs
    confirm feature X was deprecated in 2025. This will cause model to
    be penalized for correct refusal.

  affected_examples:
    count: 1
    example_ids: ["eval-1234"]
    slice: "medium_difficulty"

  impact:
    eval_runs_affected: ["eval-run-2026-01-10", "eval-run-2026-01-12"]
    metrics_impact: "Accuracy score for 'medium' slice artificially low by ~5%"

  resolution:
    status: "fixed"  # open | in_progress | fixed | accepted_risk | deferred | wont_fix
    assigned_to: "dataset_owner@company.com"
    resolution_commit: "b8e4f1a"
    resolution_notes: |
      Corrected expected output to reflect feature deprecation.
      Verified with product team that refusal is correct response.
      Re-ran affected eval runs with corrected dataset.
    resolved_at: "2026-01-15T15:30:00Z"

  linked_issues:
    - "QA-2024-002"  # Other examples with same root cause

  tags: ["product_feature", "expected_output", "requires_domain_expert"]
```

**Automated QA Check Configuration**

```python
# Dataset QA Configuration — Automated Check Rules & Thresholds

qa_config = {
    "schema_validation": {
        "enabled": True,
        "json_schema_path": "schemas/eval_dataset_v1.json",
        "strict_mode": True,  # Fail on any schema violation
        "required_fields": ["id", "input", "expected_output", "metadata"],
    },

    "distribution_checks": {
        "enabled": True,
        "difficulty_targets": {
            "easy": 0.30,
            "medium": 0.50,
            "hard": 0.20,
        },
        "difficulty_tolerance": 0.15,  # ±15% deviation allowed
        "min_examples_per_slice": 20,
        "source_type_targets": {
            "production": 0.60,
            "synthetic": 0.20,
            "handcrafted": 0.20,
        },
    },

    "anomaly_detection": {
        "enabled": True,
        "outlier_threshold_sd": 3.0,  # Flag if >3 SD from mean
        "check_input_length": True,
        "check_output_length": True,
        "detect_repeated_phrases": True,
        "repeated_phrase_min_length": 20,  # words
        "detect_empty_fields": True,
        "empty_field_severity": "blocking",
    },

    "label_consistency": {
        "enabled": True,
        "duplicate_threshold": 0.95,  # 95% text similarity = duplicate
        "inconsistency_severity": "blocking",
        "max_label_disagreement": 0.10,  # 10% of duplicates can differ
        "check_rubric_version": True,
        "flag_mixed_rubric_versions": True,
    },

    "spot_check_requirements": {
        "min_sample_per_slice": 20,
        "new_examples_coverage": 1.0,  # 100% of new examples
        "changed_examples_coverage": 0.20,  # 20% of changed
        "unchanged_examples_coverage": 0.05,  # 5% of unchanged
        "require_domain_expert": True,
        "require_non_author_reviewer": True,
    },

    "severity_levels": {
        "blocking": {
            "blocks_production_use": True,
            "requires_immediate_fix": True,
            "examples": [
                "schema_violation",
                "incorrect_expected_output",
                "label_inconsistency_high",
            ],
        },
        "high": {
            "blocks_production_use": False,
            "requires_fix_before_next_version": True,
            "examples": [
                "metadata_error",
                "missing_edge_cases",
                "unclear_rubric_application",
            ],
        },
        "medium": {
            "blocks_production_use": False,
            "requires_tracking": True,
            "examples": [
                "suboptimal_phrasing",
                "minor_metadata_inaccuracy",
            ],
        },
        "low": {
            "blocks_production_use": False,
            "advisory_only": True,
            "examples": ["typos", "formatting_inconsistency"],
        },
    },
}

# Usage: Run automated checks on dataset commit
def run_automated_qa(dataset_path, config):
    results = {
        "schema_validation": validate_schema(dataset_path, config),
        "distribution_checks": check_distribution(dataset_path, config),
        "anomaly_detection": detect_anomalies(dataset_path, config),
        "label_consistency": check_label_consistency(dataset_path, config),
    }

    blocking_issues = [
        issue for check in results.values()
        for issue in check["issues"]
        if issue["severity"] == "blocking"
    ]

    if blocking_issues:
        raise Exception(f"QA failed with {len(blocking_issues)} blocking issues")

    return results
```

---

### 11. Interview-Ready Talking Points

> **"How do you ensure your eval datasets are high quality?"**
> We run three layers of QA on every dataset version before production use. Schema QA validates structure and types using JSON Schema—catches missing fields and malformed data. Content QA uses domain expert spot checks to verify inputs make sense and expected outputs are correct—we sample at least 20 examples per slice using stratified sampling. Label QA checks consistency across duplicates and measures inter-rater agreement—we block datasets with kappa below 0.7. All QA actions are logged with audit trails showing who reviewed what when. We've caught incorrect expected outputs, label drift, and schema violations that would have silently corrupted metrics.

> **"What's the most common dataset quality issue you've seen?"**
> Label inconsistency from annotator drift over time. Annotators start with a clear rubric, but as they see edge cases, their interpretation evolves—early examples get labeled one way, later examples differently. We catch this by running duplicate detection to find similar inputs with different labels, then measure inter-rater agreement within time windows. Fix is to bring annotators together for calibration sessions to re-align on rubric application, then re-label a stratified sample. We also track rubric version in metadata so we can isolate examples labeled under outdated rubrics.

> **"How do you balance thorough QA with velocity?"**
> Automated checks run on every commit in CI—schema validation, distribution checks, anomaly detection—these are fast and catch 80% of issues. Spot checks are where we invest human time, but we use risk-based sampling: 100% of new examples, 20% of changed examples, 5% of unchanged. For low-stakes exploratory evals, we run automated-only QA. For release gates or model selection, we add domain expert review. The key is making QA incremental—don't re-review the entire dataset every time, only review what changed plus a stability sample.

> **"What happens if you ship a dataset with quality issues?"**
> We did this once—shipped an eval dataset where 15% of examples had incorrect expected outputs due to stale product knowledge. The eval showed our model regressing on customer support accuracy, so we delayed a release. After investigating, we found the dataset was wrong, not the model. Now we require domain expert sign-off before production use, track when product docs were last synced, and run pilot evals on small samples before full eval runs. We also added audit logs so we can trace back how bad data got approved and fix the process gap.

> **"How do you handle disagreements in QA reviews?"**
> If an automated check flags an issue but the reviewer thinks it's a false positive, we document the exception in the QA config and update the rule. If two spot check reviewers disagree on whether an example is correct, we escalate to a subject matter expert for tie-breaking and update the rubric to clarify that edge case. All disagreements and resolutions are logged in the audit trail so future reviewers understand why certain patterns are allowed. The goal is to continuously refine QA rules based on real data, not just apply rigid thresholds.

> **"What metrics do you track for dataset quality?"**
> Schema compliance rate (100% required before merge), inter-rater agreement (kappa >0.7 target), spot check issue rate (issues found per 100 examples reviewed), and time-to-resolution for QA issues. We also track QA coverage—percentage of dataset reviewed in the last 90 days—to ensure stable datasets don't go stale. If issue rate increases or kappa drops, we investigate for annotator drift or rubric ambiguity. These metrics go into our quarterly eval health report shown to leadership.

