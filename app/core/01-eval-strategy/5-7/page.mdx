# Chapter 5.7 — Dataset QA (Spot Checks, Audits & Label Consistency)

**What we're doing here:**
Your eval is only as good as your data. A single batch of mislabeled examples can make a worse model look better, causing you to ship a regression. Schema drift can break your pipeline at 3am during release tests. Inconsistent labels make it impossible to tell if annotators are confused or the task is genuinely hard.

Think of it like quality control in manufacturing. You wouldn't ship a product without inspecting samples off the line. Dataset QA is that inspection — structured, documented, and non-negotiable before any dataset goes into production evals.

---

## 1) Three layers of dataset QA

### Layer 1: Schema QA (structure and types)
Does every example have the right fields, valid data types, no unexpected nulls? Does the difficulty field use allowed values? Is the rubric version present?

Schema violations break eval pipelines. A missing `expected_output` field means you can't score that example. A difficulty value of "hard" when the enum expects "difficult" causes silent filtering failures.

### Layer 2: Content QA (semantics and accuracy)
Do the inputs make sense for the task? Are the expected outputs actually correct? Does the metadata accurately describe the example? Are the edge cases valid tests or just broken data?

Structurally valid data can still be semantically wrong. An example labeled "easy" that requires domain expertise. An expected output that's factually incorrect. An input that's just random text. All of these corrupt your metrics without triggering any schema error.

### Layer 3: Label QA (consistency and agreement)
Do labels match the rubric definition? Do same or similar inputs get consistent labels? Is inter-rater agreement within acceptable bounds? Are there stale labels from outdated rubric versions?

Label inconsistency destroys eval reliability. If two annotators label the same response differently, your metrics become noise. If half your dataset uses an old rubric, you're mixing incompatible data.

**Run all three layers.** Schema QA is automated and runs on every commit. Content QA requires human review on samples. Label QA combines automated consistency checks with periodic deep audits.

---

## 2) Spot check protocols

Spot checks are structured human reviews of dataset samples. They catch what automation misses — subtle semantic errors, ambiguous examples, edge cases that don't test what they claim to.

**Sample size rules:**
- At least 20 examples per meaningful slice (difficulty level, category, source type)
- For small slices (under 50 examples), review 50%
- For dataset updates: 100% of new examples, 20% of changed, 5% of unchanged
- Sample proportionally from each slice so you review the full distribution

**Who reviews:**
- Not the original author — authors are blind to their own errors
- Domain expert when possible — a lawyer reviewing legal eval data catches errors an engineer would miss
- Documented reviewer — audit logs must record who reviewed what

**Frequency:**
- Every versioned release requires a full QA cycle before production use
- Adding a new category, changing the rubric, or importing from a new source triggers review
- Even stable datasets get quarterly spot checks to detect drift

**Document everything.** Each spot check produces a QA report: sample reviewed, issues found, severity (blocking vs advisory), resolution (fixed, accepted risk, deferred).

---

## 3) Automated QA checks

These run on every dataset commit, catching common errors before humans waste time on broken data.

**Schema validation:** Define expected structure (required fields, types, nested objects) and validate every example. Ensure difficulty is from the allowed enum, expected_output is not null, timestamps are valid. Block commits that fail.

**Distribution checks:** Verify difficulty levels match expected ratios (e.g., 30% easy, 50% normal, 20% hard). Flag datasets that deviate more than 15% from target. Check source type ratios match design.

**Anomaly detection:** Flag unusually long or short inputs or outputs (often truncation or garbage data). Detect repeated phrases (copy-paste errors). Catch empty fields that should never be empty.

**Label consistency:** If the same input appears multiple times, verify it has the same expected output and difficulty. For near-duplicates (over 90% text similarity), flag significant label differences. Track temporal consistency — if you re-label old examples with a new rubric, flag drastic changes.

---

## 4) Audit trails

Every QA action must be logged. Audit trails answer: what was checked, when, by whom, what was found, and what was the resolution.

**What to log:**
- Timestamp and dataset version checked
- Reviewer identity (or tool/version for automated checks)
- Which examples were reviewed
- Issues found: severity (blocking, high, medium, low), category (schema, content, label), description, affected example IDs
- Resolution: fixed (with commit link), accepted risk (with justification), deferred, or false positive

**Issue severity:**
- **Blocking:** Schema violations, factually incorrect expected outputs, label inconsistency over 20%. Must be resolved before release.
- **High:** Metadata errors affecting slicing, missing edge cases. Fix before next version.
- **Medium:** Suboptimal phrasing, minor metadata inaccuracies. Track but don't block.
- **Low:** Typos in comments, formatting. Advisory only.

**Sign-off before production:** A designated reviewer (eval lead or domain expert) must approve in the audit log. All blocking issues resolved, automated checks passed, spot check sample size met. Once signed off, the version is immutable — any changes require a new version and new QA cycle.

---

## 5) Knobs & defaults

**Spot check sample size:** Default 20 per slice, 50% of slices under 50 examples, 100% of new examples. Increase for high-stakes evals or datasets with error history. Never drop below 10 per slice.

**Automated check frequency:** Every commit for schema, distribution, and anomaly checks. Nightly full scan for expensive similarity computations. Increase after quality incidents.

**Severity thresholds:** Schema violations are always blocking. Over 10% label inconsistency is blocking. Outliers beyond three standard deviations are high. Distribution deviation over 15% is medium. Start with defaults, tighten based on observed failures.

**Sign-off requirements:** Eval lead or domain expert approval before production use. Add approvers for cross-functional impact (product plus legal plus compliance for user-facing evals).

**Calibration tip:** Start with defaults, then tighten based on real failure modes. If you ship a bad dataset, add checks to prevent that class of error. If QA blocks progress without catching real issues, relax low-value checks.

---

## 6) Failure modes

**Eval metrics look normal but production quality is different.**
Bad examples corrupted metrics silently — incorrect expected outputs, mislabeled difficulty. Fix: run full QA audit. Spot check 10% for content correctness. Measure label consistency on duplicates. Re-run eval on cleaned data and compare.

**Pipeline breaks at runtime with parsing errors.**
Schema validation not enforced in CI. Fix: add JSON schema validation as a blocking CI check. Validate every commit before merge.

**Annotators complain examples are ambiguous or rubric doesn't apply.**
Content QA didn't catch semantically broken examples, or rubric version drifted (dataset created under v1 rubric, annotators using v2). Fix: increase spot check size with domain expert review. Add rubric version to metadata. When updating rubric, re-label a sample to verify compatibility.

**Inter-rater agreement is low despite clear rubric.**
Different annotators interpret edge cases differently. Or annotator drift — early examples labeled one way, later ones differently. Fix: run duplicate detection for same-input-different-label cases. Bring annotators together for calibration. Track rubric version and annotator ID in metadata.

**Automated QA flags hundreds of false positives — alert fatigue.**
Thresholds too sensitive for your data. Fix: tune thresholds from real data. Review flags with domain experts. Add allow-lists for intentional edge cases. Document exceptions in QA config.

---

## 7) Enterprise expectations

**Early-stage (months 0–6):** Schema validation as blocking CI check. Spot check 20% of new versions. Track issues in shared doc. Manual sign-off by eval lead.

**Scaling (months 6–18):** Full automated QA pipeline (schema, distribution, anomaly, label consistency). Spot check protocol with minimum sample sizes per slice. Audit logs integrated with version control. Severity levels and resolution workflows.

**Mature (18+ months):** Multi-layer QA with role-based reviewers (engineers for schema, domain experts for content). Comprehensive audit trails with sign-off workflows and compliance reporting. Continuous monitoring even for stable datasets.

**Red flags at any stage:** Datasets used in production without documented QA. Schema validation only run manually. No tracking of who reviewed what. Same person creating examples and doing QA.

---
