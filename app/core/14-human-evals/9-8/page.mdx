# 9.8 — Ground Truth Updates from Production Review

Production review data is the best source of ground truth your system will ever have. It's real user inputs, real model failures, and real expert corrections on cases that matter. It's better than synthetic data because it's grounded in actual usage. It's better than pre-labeled datasets because it reflects current model behavior and current user needs. It's better than crowdsourced annotations because it's validated by your domain experts in production context.

When a reviewer corrects a production output, they're not just fixing one case. They're defining ground truth for a class of inputs your original training set missed. When a reviewer flags a failure mode, they're identifying a pattern your model needs to learn. When a reviewer resolves a dispute about what the right answer should be, they're refining your understanding of correctness itself. All of that is ground truth evolution, and it should flow back into your training data, your evaluation sets, and your quality definitions.

Most teams treat production review as a quality gate. Outputs get reviewed, corrected, and shipped to users. The corrections sit in a review database and never get used again. That's a waste. The corrections are training data waiting to be harvested. The failure patterns are eval cases waiting to be formalized. The edge cases are schema updates waiting to be integrated. Production review isn't just a quality process. It's a continuous ground truth refinement process.

## Why Production Data is Better Ground Truth

Your original training set was created before your model existed. It was built on assumptions about what users would ask, what outputs would look like, and what "correct" meant. Many of those assumptions were wrong. Production review reveals the real distribution of inputs, the real failure modes of outputs, and the real expectations of correctness.

Production inputs are different from training inputs. Users ask questions you didn't anticipate. They use phrasing you didn't include in your dataset. They reference context you didn't model. They combine features in ways you didn't test. If your training set was built by imagining user needs, production inputs are actual user needs. When a reviewer corrects a production output, they're labeling data from the true input distribution, not a synthetic approximation.

Production failures are more informative than random examples. When your model fails in production, it's failing on cases where the training set was insufficient. Those are exactly the cases the model needs to learn from. A production correction tells you: here's a scenario the model gets wrong, and here's the right answer. That's higher-signal than a random example the model might already handle well.

Production corrections are validated in context. When a reviewer corrects an output, they see the full user session, the previous outputs, the user's follow-up questions. They understand what the user was trying to accomplish. That context informs the correction. A standalone labeled example might say the answer is X, but a production correction says the answer is X given this user's goal and this interaction history. That contextual grounding makes production corrections more accurate than decontextualized labels.

Production review also captures evolving ground truth. User expectations change. Domain knowledge updates. Policies shift. Your original training set reflects ground truth from six months ago or a year ago. Production corrections reflect ground truth from this week. If you continuously propagate production corrections into training, your model stays current. If you only train on static datasets, your model fossilizes.

The result is that a thousand production corrections are worth more than ten thousand synthetic examples. Production corrections are scarce, but they're dense with signal. They should be treated as premium training data and propagated back into the training pipeline with high priority.

## Correction Flow from Review to Training Pipeline

Production corrections need a pipeline that moves them from the review system to the training system automatically, quickly, and cleanly.

The pipeline starts with review completion. When a reviewer finishes a case and submits their correction, the correction is written to the review database. The database emits an event: correction-created. A stream processor listens for that event. When it receives one, it pulls the correction record, validates it, transforms it into training format, and writes it to the training data store. Latency from correction submission to training availability is minutes.

Validation ensures the correction is training-worthy. The pipeline checks that the correction was approved, not just saved as a draft. It checks that the reviewer's confidence score is above a threshold. It checks that the correction passed any dispute resolution if applicable. It checks that the output doesn't contain prohibited content or personally identifiable information. Only corrections that pass all checks proceed.

Transformation converts the correction from review schema to training schema. The review database stores the original input, the model's output, the corrected output, the reviewer ID, the timestamp, the confidence score, and various review-specific metadata. The training system needs input-output pairs with optional metadata fields. The transformation extracts the input and the corrected output, maps the metadata fields, and writes the result in the format the training system expects.

The pipeline also handles deduplication. If the same input was reviewed twice and both corrections passed validation, the pipeline keeps the most recent one or the highest-confidence one, depending on your policy. It doesn't add duplicate inputs to the training set.

Once transformed and deduplicated, the correction is written to the training data store. That could be a database table, a file in a cloud bucket, a feature in a feature store. Wherever your training pipeline reads from, that's where the correction lands. The next training run picks it up automatically.

The entire pipeline is automated. Reviewers submit corrections. Corrections propagate to training. No manual export, no weekly batch job, no CSV files emailed between teams. The loop is closed by infrastructure.

## Building Production-Grounded Eval Sets

Production corrections don't just feed training. They also feed evaluation. The cases that reviewers correct most often are the cases your eval set should include.

Every month, you pull the top 200 corrected cases. These are the cases where your model failed in production. You anonymize the inputs if necessary. You add the corrected outputs as reference answers. You now have 200 new eval cases grounded in real failures. You add them to your eval set. The next model version is tested on cases that the previous version failed on. That's how you ensure you're fixing real problems, not optimizing for synthetic benchmarks that don't matter.

Some teams build entire eval sets from production corrections. They start with zero curated eval cases. They deploy a model. They review production outputs. They collect corrections. After a month, they have 1,000 corrections. Those become the eval set. Every subsequent model is evaluated on the 1,000 cases that real users triggered and real reviewers corrected. The eval set is perfectly aligned with production reality because it is production reality.

Production-grounded eval sets also capture temporal shifts. If your domain changes over time, production corrections from this quarter reflect current ground truth. Corrections from a year ago reflect outdated ground truth. You can version your eval sets by time period. Eval-set-2025-Q4 contains corrections from Q4 2025. Eval-set-2026-Q1 contains corrections from Q1 2026. You track model performance on both. If performance on 2025-Q4 stays high but performance on 2026-Q1 is low, you know the model hasn't adapted to recent changes.

Building eval sets from production data also reveals blind spots. If your synthetic eval set has 500 cases and your production corrections have 1,000 cases, you compare them. You find that 300 production cases are similar to synthetic cases, but 700 are novel. Those 700 cases represent scenarios your synthetic eval didn't anticipate. They're blind spots. You add them to the eval set permanently.

The key is to treat production corrections as a continuous stream of eval candidates, not a one-time data source. Every week, you sample the best corrections and add them to the eval set. The eval set grows and evolves alongside your model and your users.

## Flagging Edge Cases for Formal Integration

Production review surfaces edge cases that weren't in your original training data. Some of those edge cases are one-offs. Others are systematic gaps that need formal integration.

A one-off edge case is a correction for a highly unusual input that's unlikely to recur. A user asked a question that required domain knowledge from a niche subfield that 99.9% of users will never ask about. The reviewer corrected it. That's a valid correction, but it's not worth adding to the training set because it's not representative. You log it, you serve the corrected output to the user, and you move on.

A systematic edge case is a correction that represents a class of inputs your model handles poorly. A reviewer corrects an output because the model misunderstood medical abbreviations. Over the next week, five more corrections involve medical abbreviations. That's not a one-off. That's a systematic gap. The model doesn't understand medical abbreviations, and that's affecting multiple users. You flag it for formal integration.

Formal integration means treating the edge case as a new data requirement. You go back to your data labeling pipeline and create 200 examples involving medical abbreviations. You add them to the training set. You retrain the model. You test it on medical abbreviation cases and verify that performance improves. You update your eval set to include medical abbreviation cases so future models are tested on them. The edge case becomes a permanent part of your training and evaluation infrastructure.

Flagging edge cases for formal integration requires pattern detection. You can't manually read every correction and decide if it's systematic. You need automated analysis. One approach is clustering. You embed all corrected inputs into a vector space. You cluster them. Clusters with more than 10 corrections in a week are flagged as systematic. A human reviews the cluster, confirms it's a real pattern, and kicks off data labeling for that pattern.

Another approach is keyword or entity extraction. You extract key terms from corrected inputs. You count how often each term appears. Terms that appear in more than 5 corrections in a week are flagged. A human reviews the term, sees that it's medical abbreviations or legal jargon or product names, and decides whether to invest in labeling more examples.

Edge case flagging also feeds back into your task taxonomy. If production review reveals a new class of inputs that your original task taxonomy didn't account for, you add a new task type. If you thought you were building a general question-answering system and production review shows that 20% of inputs are actually multi-step reasoning tasks, you split multi-step reasoning into its own task type with its own eval set and its own training data requirements.

## Versioning Ground Truth as Understanding Evolves

Ground truth isn't static. What you thought was correct six months ago might be wrong today. Production review captures that evolution, but only if you version ground truth and track changes over time.

When a reviewer corrects an output, that correction reflects the current understanding of correctness. If policies change next month, the correction might become outdated. If domain knowledge evolves, the correction might need to be revised. If user expectations shift, the correction might no longer match what users actually want. You version every correction with a timestamp and a policy version so you know what "correct" meant at the time the correction was made.

Versioning lets you filter training data by recency. If you want to train on only the last three months of corrections because older corrections are stale, you filter by timestamp. If you want to train on only corrections made under the current policy version because old policies are deprecated, you filter by policy version. Versioned corrections give you control over which ground truth you train on.

Versioning also lets you track how ground truth changes. You pull all corrections from six months ago. You re-review them with today's reviewers. You see how many of the old corrections are still considered correct. If 95% are still correct, your ground truth is stable. If only 60% are still correct, your ground truth is shifting rapidly. That tells you how often you need to retrain and how much weight to give to historical corrections.

Some teams actively re-version corrections when policies change. When a new policy is released, they run a batch job that re-reviews old corrections against the new policy. Corrections that are still correct get re-tagged with the new policy version. Corrections that are now wrong get deprecated and removed from the training set. This keeps the training set aligned with current ground truth without manually re-labeling everything.

Versioning also applies to your reference answers in eval sets. If an eval case has a reference answer from 2025 and your policies changed in 2026, the reference answer might be outdated. You re-review eval cases periodically. You update reference answers if ground truth changed. You version the eval set so you can compare model performance on 2025 ground truth versus 2026 ground truth. That tells you whether performance changes are due to model improvements or ground truth shifts.

## Propagating Definition Changes to Specs

Production review sometimes reveals that your original definition of "correct" was incomplete or wrong. When that happens, you don't just update training data. You update the specification documents, the reviewer guidelines, and the quality rubrics.

If reviewers consistently correct a certain type of output in a certain way, that's evidence that your spec is misaligned with actual expectations. Maybe your spec says answers should be concise, but reviewers consistently expand terse outputs into more detailed explanations. That means "concise" isn't the right quality dimension. You update the spec to say "detailed enough to be actionable" instead of "concise."

If reviewers consistently disagree about what's correct for a certain input type, that's evidence that your spec is ambiguous. Maybe your spec says to prioritize recent information, but half your reviewers interpret "recent" as last six months and half interpret it as last two years. That ambiguity causes inconsistent corrections. You update the spec to define "recent" as information from the last 12 months. Disagreement drops.

If a new failure mode appears in production that your spec didn't anticipate, you add it to the spec. Maybe your spec defined hallucination, refusal, and formatting errors, but production review reveals a new failure mode: overly cautious hedging where the model gives a correct answer but buries it in disclaimers. That's a new quality dimension. You add "clarity without over-hedging" to your spec. Future reviewers and future models are trained to optimize for it.

Propagating definition changes is a formal process. Every quarter, the review team analyzes correction patterns. They identify systematic disagreements, ambiguous spec language, and emerging failure modes. They draft spec updates. The updates are reviewed by domain experts, product, and engineering. Once approved, the updated specs are published. Reviewers are retrained on the new specs. New training data is labeled using the new specs. The model is retrained and evaluated against the new specs. Everyone aligns on the updated definition of correctness.

This closes the loop. Production review doesn't just generate training data. It refines the definition of what training data should represent. Ground truth evolves from a static concept into a dynamic, continuously updated understanding of what your system should do.

## Infrastructure for Continuous Ground Truth Refresh

Continuous ground truth refresh requires infrastructure that supports versioning, filtering, propagation, and monitoring.

You need a versioned training data store. Every correction gets a timestamp, a policy version, a model version, and a quality score. You can query by any of those dimensions. You can pull all corrections from the last 90 days. You can pull all high-confidence corrections from policy version 3. You can pull all corrections where the model version was 18 or later. The store supports time-range queries and version-range queries.

You need a propagation pipeline that runs continuously or near-continuously. Corrections flow from review to training with minimal latency. The pipeline is monitored. If propagation latency exceeds one hour, an alert fires. If the pipeline fails, the on-call engineer gets paged. Corrections never sit in the review database for days waiting to be manually exported.

You need deduplication and conflict resolution logic built into the pipeline. When multiple corrections exist for the same input, the pipeline applies your conflict resolution policy automatically. It doesn't require manual intervention. It doesn't propagate duplicates.

You need dashboards that show ground truth drift. You track how many corrections from six months ago are still valid. You track how often reference answers in eval sets get updated. You track how often specs get revised. You see whether your ground truth is stable or shifting. That informs how often to retrain and how much to weight recent data.

You need automated re-review jobs that periodically re-evaluate old corrections against current policies. When a new policy version is released, the job pulls all active corrections, re-scores them, and deprecates any that no longer align. This keeps the training set clean without manual labor.

You need alerts for systematic edge cases. When a cluster of similar corrections exceeds a threshold, an alert notifies the data team. They investigate, decide if it's a systematic gap, and kick off a labeling project if needed. Edge cases don't languish as scattered corrections. They get escalated to formal integration.

All of this infrastructure treats ground truth as a living dataset, not a frozen artifact. It assumes that what's correct today might be wrong tomorrow. It builds the machinery to detect changes, propagate updates, and retrain models as understanding evolves. That's what separates a static model from a model that learns continuously from production.

This chapter covered integration with ML pipelines — feedback loops, correction propagation, versioning, review triggers, and ground truth evolution. The next chapter will cover reviewer performance and quality assurance — how to measure reviewer accuracy, how to identify and correct reviewer drift, how to maintain consistency across a distributed review team, and how to ensure the humans in your human review loop are producing reliable signal.

